text
"a typical hierarchy of ecc consists of four successive levels as shown in figure 1 . the first level contains finite field arithmetic such as modular addition, subtraction, multiplication, and inversion. the second level comprises elliptic curve group operations such as point addition and point doubling, which accommodate a number of modular arithmetic. the third level relates to elliptic curve point/scalar multiplication (ecpm/ecsm) that integrates the elliptic curve group operations in a sequential manner. the top-level includes ecc protocols such as elliptic curve digital signature algorithm (ecdsa) and eddsa."
"rivest-shamir-adleman (rsa) [cit] and elliptic curve cryptography (ecc) [cit] . rsa, proposed by rivest, shamir, and adleman, is based on integer factorization, whose encryption strength depends on the key sizes taken. ecc, first introduced by koblitz [cit] and miller [cit] independently, is based on discrete logarithms, whose encryption strength is much difficult to break. to provide the same level of security, ecc requires a shorter key length than rsa. for example, 160-, 224-, and 256-bit ecc encryption keys provide equivalent security as 1024 [cit] -, and 3072-bit rsa encryption keys, respectively. the advantage of smaller key sizes is that they make ecc the best suited for the high-speed cryptographic processors as well as resource-constrained iot devices. the authentication protocols for wireless sensor volume 7, 2019 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/ nodes are adopting ecc primitives to provide a high level of security with optimal hardware resources. edwards curves, a family of elliptic curves, introduced by edward [cit], have gained much interest among cryptography researchers because of their fast group operations [cit] and high immunity to side-channel attacks (scas) [cit] . edwards curves can offer strongly unified addition [cit] formulas that can be used for both point addition and point doubling, ensuring side-channel security. an edwards curve based cryptographic processor can be developed with low area utilization and low power consumption, providing high computational speed and high level of security. edwards25519 is a twisted edwards curve [cit], which is the edwards form of the elliptic curve ''curve25519'' [cit] . it is mainly used for highspeed key generation as well as in edwards curve digital signature algorithm (eddsa) [cit] ."
"in this paper, a high-speed, area-efficient, sca-resistant ecc processor is developed for fast point multiplication exploiting edwards25519 curve with its projective representation. a radix-2 interleaved modular multiplier is adopted for modular multiplication that requires n + 1 clock cycles to multiply two n-bit integers. novel hardware architectures for point addition and point doubling are proposed that require 4n + 5 and 2n + 4 clock cycles, respectively, to accomplish n-bit operations. the montgomery scalar multiplication algorithm is used to perform ecpm as it offers fast computation with high resistance against scas. all the designs are implemented on the xilinx virtex-7 and virtex-6 fpga platforms individually over a prime field of 256 bits. the processor performs single point multiplication in 262,650 clock cycles and it takes 1.48 ms with a throughput of 173.2 kbps, consuming 8,873 slices on the virtex-7 fpga. it offers higher efficiency in terms of area-delay product and throughput without degrading the security level. based on the overall performance analyses, it can be concluded that the proposed ecc processor can be a good choice for high-speed data encryption as well as for the privacy and security of resourceconstrained iot devices."
"projective or jacobian coordinates are used to avoid the most expensive modular inversion operation, which is essentially used in affine coordinate systems. in a projective coordinate system, each point (x, y) on the curve e a,d is represented in a triplet form (x, y, z ) that corresponds to the affine point"
"in this paper, fpga implementation of a time-area-efficient 256-bit ecc processor over gf(p) is presented. the number of clock cycles, as well as computation time for point multiplication, is aimed to reduce as far as possible. the minimization of the hardware resources required for the group and modular operations is emphasized to reduce the occupied area of the processor. the major contributions of this paper can be summarized as follows:"
"the proposed ecc processor is implemented using the xilinx ise 14.7 design suite software and simulated by the xilinx isim simulator. the simulation results are verified by the maple software. all the designs are synthesized, mapped, placed, and routed on the xilinx virtex-7 (xc7vx690t) and virtex-6 (xc6vhx380t) fpga platforms, separately. the design goal is set to ''balanced'' and the design strategies are set to the default values. the implementation results of the proposed ecc modules over a prime field of 256 bits are summarized in table 2 . on the virtex-7 fpga, the proposed modular multiplier, pa, pd, and ecpm modules run at a maximum frequency of 177.7 mhz. the latencies of the multiplier, pa, pd, and ecpm modules are 257, 1029, 516, and 262,650 clock cycles, respectively. to perform modular multiplication, the multiplier takes 1.45 µs with 177 mbps throughput on the virtex-7 fpga, occupying 427 slices equivalent to 1311 look up tables (luts). on the same platform, the pa module takes 5.79 µs with a throughput of 44.21 mbps to add two points on the curve e d, consuming 4459 slices equivalent to 15,619 luts. the pd module occupies 1801 slices equivalent to 6687 luts and takes 2.90 µs with 88. 16 mbps throughput for doubling a point on the curve. the final ecpm scheme combines both the pa and pd modules, utilizing 8873 slices equivalent to 32,781 luts. it requires 1.48 ms with a throughput of 173.2 kbps to perform single point multiplication for a 256-bit key."
"the acronyms used in this paper are enlisted in table 1 . the mathematical background of the twisted edwards curve is described in section ii. the hardware architectures for the ecc operations are proposed in section iii. the implementation and simulation results of the proposed ecc designs are presented in section iv. a performance comparison of our ecpm design with other available designs is shown in section v. finally, in section vi, this research work is summarized and concluded."
"an elliptic curve cryptosystem can be implemented with either a hardware or software approach. in this research, our focus is only on the hardware approach because the hardware implementation offers considerably faster operations compared with the software implementation. however, the hardware implementation of ecc with low hardware consumption and low time complexity is a very challenging task. area and time are two contradictory parameters of an ecc processor, one of which has to be compromised to achieve high efficiency in terms of the other one. for better performance, the area-time (at) product of the processor should be as small as possible. this research aims to develop an ecc processor well-suited for high-speed, low-power cryptographic devices by reducing the computation time and area required for ecpm."
this section presents the twisted edwards curve and group law of this curve. the point addition and point doubling formulas for the edwards25519 curve in projective coordinates are also presented in this section.
"the addition of the affine points a (x 1, y 1 ) and b (x 2, y 2 ) on the curve e a,d is given by the formula [cit] a ("
"modular multiplication is one of the most time-consuming arithmetic operations of an ecc processor over a prime field on which the efficiency of an ecpm scheme entirely depends. although higher radix modular multipliers offer fewer clock cycles as well as less computation time to perform modular multiplication, these require more hardware resources that increase occupied area. to minimize the area required for ecpm, a radix-2 interleaved modular multiplier is adopted to implement the ecpm scheme, which requires n + 1 clock cycles to perform modular multiplication of two n-bit integers. the modular multiplication of the two n-bit integers a and b over the prime field gf(p) can be defined as"
"the doubling of the affine point a (x 1, y 1 ) on the curve e a,d is given by the formula [cit] 2a ("
"the same ecpm design can be implemented with fewer clock cycles by the montgomery ladder method than the double-and-add method, which makes the ecc processor faster. we take the advantages of high-speed computation and high resistance against probable scas offered by the montgomery ladder algorithm for our ecpm module."
"the analysis of these results shows the importance of the service discovery and prediction mechanisms. we believe that the proposed mechanisms allow the selection of the service that fulfills the user's immediate needs and the anticipation of her/his future need. this is thanks to both our intentional approach, which is more transparent to the user, and our contextual approach that restricts services to those that are valid. however, it is important to note that such good results cannot be obtained if the system designer does not establish, from the beginning, a rich description of the available services and of the different ontologies, as well as an appropriate threshold setting."
"a similar trend could be observed for the classification algorithm (see fig. 10 ), in which we evaluate the time the algorithm takes to update the user's behavior model. we vary the number of available observations from 10 to 200 observations, which represents a variation of 20x in the number of observations, and we obtained an execution time varying from 39ms up to 398ms, representing a variation of only 10x in the execution time."
"pervasive information systems (pis) have to face complex pervasive environments, in which heterogeneous technologies cohabit and interact with each other and with the system itself. besides, a pis is supposed to behave as an information system (is), allowing its users to accomplish their business goals through the services it proposes. a pis can thus be characterized by this dual nature: it must be considered in terms of is and as a pervasive system. as a pervasive system, a pis must handle dynamicity and heterogeneity of pervasive environments. [cit], context information determines the environment in which a system operates. according to these authors, because different contextual conditions can provoke different system responses, suitable runtime adaptation mechanisms and intelligent decision making may enable systems to successfully handle the demands and the complexity of dynamic variability and adaptation. the context-aware nature of pis is then ineluctable."
"we advocate that new successful pis will have to integrate competitive bi (business intelligence) techniques, adapted to the big data reality in order to deal with the explosion of context data brought by iot. [cit], to remain competitive, all companies will invest heavily in analytics and business intelligence [cit] . new improvement on these techniques will then be necessary for fully explore context information as big data on such iot environment."
"moreover, we believe that the development of iot will bring an important swift on the way context awareness is conceived. until now, context information is established on design time: system designers have to anticipate what context information they need (or want) to observe at design time and integrate on their systems design the corresponding sensing mechanisms. however, thanks to iot technologies, pis will have available in their environment several new context information that was not necessarily predefined or known at design time. sensing context information is not the challenge anymore. the challenge will be how to opportunistically explore context information collected and discovered from the environment at execution time, instead of using only predefined context elements. the question of relevance of context information becomes central for service discovery and prediction, since the volume and the nature of available context information will exponentially grow. we strongly believe that it is necessary to go further on today's clustering and classification techniques in order to efficiently identify context elements that have a real impact on service discovery and prediction, and more generally, on the overall use of pis."
"evaluation results presented earlier in this paper (cf. section 6) demonstrate that, by observing the user's intention and combining it with the notion of context, it is possible to propose users better suited services, which are able to satisfy this intention considering current user's context conditions. results considering the proposed prediction mechanism demonstrate that by analyzing user's traces, it is possible to achieve a proactive behavior, anticipating future user's needs. besides these quality considerations, evaluation results also demonstrate the feasibility of the proposed mechanisms, which show good performance behaviors."
"pervasive information systems (pis) intend to increase user's productivity by making is services available anytime and anywhere. these systems changed the interaction paradigm from desktop computing to new technologies. they evolved from a fully controlled environment (the office) to a dynamic pervasive one [cit] . contrary to traditional is, pis have to support a multitude of heterogeneous devices and service types, challenging its design. we argue that pis should be designed for helping user to better satisfy her/his needs according to her/his environment. pis must not only consider the goals they must reach as an is, but they must also handle heterogeneity of pervasive environment. they should hide this heterogeneity from the user, allowing her/him to concentrate on her/his needs, instead of on the technology itself."
"we believe that in order to handle such dual nature, a pis must reduce the user's understanding effort, augmenting its transparency, allowing users to totally focus on the goals they want to achieve. a pis must hide the complexity of the multiple available services it offers. this transparency will be only possible thanks to a user-centered vision. in this paper, we adopt a user-centric vision of pis that is based on a close relationship between the notions of intention, context and service. this vision allows, on the one hand, to consider the user's real needs through an intentional approach, and on the other hand, to manage the heterogeneity and dynamics of the pervasive environment through a contextual approach."
"besides, the possibility of dynamically composing services thanks to observed user's behavior patterns represents thus a necessary step towards transparent pis on a iot environment. different from current composition mechanisms, which consider often only service input and output information in addition to context information, such new composing mechanisms should consider user's intention associated with these services in addition to the context itself. it is then necessary to go beyond our current prediction mechanism, towards advanced learning algorithms, combining both intention and context information on more complex patterns."
"for each new situations stored in the history, we proceed by selecting for each situation sti (identified by its cluster), its successor stj. the successor stj represents the situation that is directly stored after the situation sti. indeed, by observing timestamp associated with each observed situation individually, it is possible to determine the situation that follows this one, indicating a possible transition between the clusters containing these situations. then, we calculate the number of the existing transitions from sti to stj (nstistj). next, for each situation sti, we determine the entire possible next situation stk (nstistk). we note that, in the history database, the former number of transitions from sti to stj (nstistj) and the number of all the possible transitions from sti (nstistk) are already stored. this information is updated, and all the states and transition probability are represented and calculated accordingly. the markov chains model resulting from this process represents the user's behavior model (mc) that is used during the prediction process."
"a major challenge of this approach is balancing internal and external sources. the impact of external sources should be limited in order not to jeopardize the creation of domain ontologies, which reflect the terminology used in the domain corpus (13) . this might be achieved by assigning relatively high weights to in-corpus data while still including external data (e.g., from social evidence sources) reflecting the latest trends in the field. the following sections outline the balancing of evidence sources and the composition of the source impact vector in greater detail."
"several classification techniques exist. among these techniques, the markov chains [cit] represents one of the well-known classification algorithms that can be used in a pis. it represents a method for representing a stochastic process in discrete time with discrete state space. at a given moment t, the user is in a state sti. in the pis, the user's intention and/or her/his context may change. therefore, the user moves from the state sti to a state stj. the state stj represents the successor of sti with a certain probability p. this transition probability represents the ratio of the transitions from sti to stj divided by the number of all the possible transitions from sti. this probability is represented as follow:"
"two main processes compose the service prediction mechanism, as illustrated in fig. 4 : the learning process and the prediction process. indeed, to realize anticipatory and proactive behavior of pis, we need first to dynamically learn about the user and her/his behavior in a frequently changing environment. this represents the learning process, in which similar situations are grouped into clusters, during the phase of clustering. in the next step, these clusters are interpreted as states of a state machine. this phase is called classification. it aims at representing, from the recognized clusters, the user's behavior model (mc) based on her/his situations (si). by interpreting situation changes as a trajectory of states, we can anticipate her/his future needs. therefore, the prediction process will try to anticipate the user's next situation based on the user's behavior model (mc), resulting from the learning process, on the current user's intention (iu), as well as on the current user's context (cxu)."
"finally, the execution time of the prediction algorithm is measured by varying the number of states in the user's behavior model, between 8 and 168 states. as illustrated in fig. 11, the execution time also follows a polynomial trend of degree three, like the service discovery algorithm, varying from 1.63s for 8 states up to 4.16s for 168 states. in other words, we increased the number of states over 25x, while the execution time has only increased by 2.5x. all these results allow us to validate the feasibility of our proposed algorithms. in order to measure the result quality of the prediction process, we use a quality metric inspired from the precision used to evaluate the service discovery. this metric is used to check whether the predicted service is the one that is expected or not. we determined previously the service that the prediction algorithm should predict for a given user's request, based on the user's behavior model. then, we compared this service with the service returned by the algorithm."
"a similar situation can be observed when considering service prediction. anticipating user's needs represents an important step towards transparent pis. a proactive behavior can offer new possibilities to enhance available informational services or construct new currently unavailable ones [cit] . indeed, when interacting with information systems, users develop often work habitudes that can be understood as work patterns. discovering and anticipating user's actions based on such patterns may lead to a more proactive behavior, necessary to obtain transparent pis."
"by conceptualizing an application domain (1), ontologies facilitate a common understanding of domain concepts and relations among different stakeholder groups. such ontologies require ongoing updates and refinements to keep track with evolving domain knowledge, tasks that are both laborintensive and costly. by supporting and guiding these processes, automated ontology learning improves productivity, reduces the human input required, and paves the way for applying semantic technologies to real-world problems."
"to sum up, we advocate that future pis will have to integrate techniques from multiple domains, such as big data and data analytics, in order to be successful. and inversely, new pis offer interesting challenges for these domains, by confronting information system needs to a new environment offered by iot and pervasive technologies."
"for the spreading activation network. the translation heuristics reflect the evidence source's importance in the extension process and have proven their usefulness in related research on ontology extension and evolution (3; 14; 15; 16). the following example illustrates the integration process. three evidence sources suggest a relation between the terms climate change (cc) and fuel, resulting in the following evidence vector: applying this process to all seed candidate concept pairs (c s, c c ) yields the spreading activation network used to determine the candidate concepts to include in the domain ontology."
"we illustrate in fig. 12 the quality percentage achieved by the algorithm by varying the number of states. this percentage represents the average quality obtained for all the user's requests. the results presented in fig. 12 indicate that the prediction algorithm has a good quality around 60%. similar to the discovery mechanism, these results can be explained by the evaluation of certain situations described by our requests, which can significantly degrade the results quality obtained. for example, in some requests using intentions in which the verb and/or the target are fairly generic or specific, we obtain a poor quality that is, in some cases, below 45%. besides, these results also demonstrate the impact of the threshold on the prediction mechanism. indeed, when considering high threshold settings in the prediction algorithm, some clusters or states that could meet the immediate user's intention in her/his current context will not be selected, and this contributes to degradation of the results quality."
"as indicated above, we consider the relation contextconditionmatching that individually matches the observations cxj, obtained from the user's context description cxu, and the context conditions cxi, indicated in the service required context cxrsvi. the observable context elements can be divided into several types. their values can be distinguished between numerical or non-numerical types. in order to take into account this diversity, the relation contextconditionmatching identifies the nature of the context element and accordingly triggers the suitable measure to compare them. these measures evaluate if the user's context element satisfies the service context condition, based on a specific operator (equal, not-equal, between, higher-than, etc.) . for example, a service context condition can be having the device bandwidth higher than 12500. from the user's current context, if the observed value of the user's device bandwidth is really higher than 12500, then evaluating measure will return an exact match."
"the ontology extension framework presented in this paper draws upon unstructured evidence sources (e.g. archives of web documents) and social evidence sources such as del.icio.us, flickr, technorati and twitter. the results demonstrate the benefits of integrating multiple evidence sources for ontology learning from a multi-stakeholder view. two evaluation metrics (pointwise mutual information, domain expert assessments) measure the quality of ontological concepts as well as the impact of including social evidence sources in the ontology extension process."
"we consider that a service offered by a pis is proposed in order to satisfy a given user's goal, corresponding to the user's needs. in other words, we consider that services should be associated with the intentions they allow users to satisfy. such intentions emerge in a given context, which should be observed in order to fully satisfy such an intention. indeed, we consider that a user does not require a service just because she/he is located in a given place or under a given context. for us, the user does require a service because she/he has an intention that a service can satisfy in this context. the context in which an intention emerges has an important impact on its satisfaction. a given intention may be satisfied by different means according to the context in which it emerges, since different services can be proposed according to the available environment, representing an important source of variability for a pis. this vision, illustrated by fig. 1 [cit], consider that a service is proposed in order to satisfy a given intention i under a given context cxr. this later represents the context in which the service is supposed to be invoked in order to work properly. in other terms, the required context cxr represents a set of contextual conditions under which the service is more likely to reach its goals. therefore, the better the matching between the observed user's context and the required context cxr is, the higher the chances of adapting it to the current situation and of satisfying the user. moreover, we also consider that a service is performed under a context cx, which indicates the execution conditions of the service in the provider side. we present, in this paper, two mechanisms that explore this user-centric vision. first, we propose a discovery mechanism (cf. section 4) that is able to discover most appropriate service capable of satisfying current user's intention under a given context. secondly, we propose a prediction mechanism (section 5) that will try to anticipate user's future intention, based on current user's intention and context, in order to proactively offer her/him the next service according her/his habits in the current context."
"finally, as discussed previously in this paper, pis open new research perspectives for is. the development of iot and the potential integration of a large volume of data, and notably, context data, and services will durably impact pis. successful pis in the future will be those capable of not only handle big volumes of data, but also to explore these data, with appropriate mining and data analytics techniques. service discovery and prediction mechanism are only a few examples of the numerous mechanisms and services offered by pis that can beneficiate from these new developments."
"new technologies transform the way we interact with information systems (is) and the services they offer, expanding the frontiers of is outside the companies' environment. the byod (bring your own device) illustrates this tendency: employees bring their own devices to the office and keep using them to access the is even when they are on the move. byod lets employees use their personal device to work seamlessly across their personal user space and enterprise workspace instead of using multiple devices depending on business need, location, and circumstances [cit] ). this tendency does not concern only laptops. employees are increasingly using tablets, smartphones and other \"thin clients\" to access network (lan, wan and vpn), and hosted applications, as well as applications that run locally on (some of) the devices. there is also a growing opportunity for businesses to exploit the capabilities of wearable devices, such as microsoft hololens, apple iwatch or bluetooth headsets, among countless other devices that are changing the way we search, navigate, transact, and live [cit] ."
"each context condition cxi is compared to the context observations cxj, resulting in a context matching score cxscore. this score is calculated as the sum of the scores of each context condition, as follows:"
"predic2on\"quality\" patterns describing complex strategies users apply in order to reach bigger goals. however, identifying such complex behavioral patterns is a harder task than identifying only simple sequences, which may demand more complex learning techniques, such as bayesian networks for instance. besides, dynamically identifying such behavior patterns may contribute to improve transparency of pis, since these patterns allow composing fine-grained intentions (and services) on coarse-grained ones, easiest to understand by the users."
"the method presented in this article extends existing seed ontologies based on a corpus of domain documents. figure 1 outlines the ontology extension process. unstructured sources (section ii-a) such as web documents are enriched by data from social evidence sources such as del.icio.us, flickr, technorati and twitter (section ii-b). evidence sources (e) process the seed ontology and corpus text to yield relations between seed ontology concepts (c s ) and candidate concepts (c c ). the evidence source determines the kind of this relation (e.g., \"co-occurs\", \"related twitter tag\", \"related delicous tag\", etc.). an rdf triple store collects the evidence, as outlined in table i reification is used to add meta-data on the extracted relations such as significance levels, dice coefficients, and counts of the relations in the triple store. applying transformation heuristics and spreading activation integrates the collected evidence into the ontology building process. per evidence source heuristics transform the relation between the seed and candidate concepts into a numerical weight used in a spreading activation network. these weights reflect the expected contribution of the evidence source to the final ontology and consider source-specific annotations (section ii-c). the neural network method of spreading activation integrates the typically heterogeneous data from multiple evidence sources and computes a ranked list of candidate concepts for inclusion in the extended ontology. a combination of noun phrases, subsumption analysis and spreading activation then determines the new concepts' positions in the extended ontology (3). this work focuses on terminology and therefore only determines relations between concepts, but not the relation type."
"in order to make this ubiquitous access possible, pervasive information systems (pis) have to conciliate two different worlds: the dynamic and heterogeneous pervasive environments with the predictable and expected behavior necessary to an information system. from this analysis, a set of requirements applying to pis can be delineated, including heterogeneity (being able to handle heterogeneity of devices and services), predictability (being able to satisfy user's goals in a predictable and expected manner) and context awareness [cit] . context-awareness can be seen as the capability pervasive artifacts have to collect, to process, and to manage environmental or user-related information on a real-time basis [cit] as the ability of a system to adapt its operations to the current context, aiming at increasing its usability and its effectiveness by taking environmental context into account. in contrast to desktop computing, where user's action precedes system response, pis promote system pro-action based on environmental stimuli [cit] . thus, pis should be able to observe changes in the execution environment and to adapt consequently their behavior [cit] ."
"table iii outlines the method's performance based on the pointwise mutual information (pmi) measure, which determines how well the terms participating in a relation are associated to each other based on the counts of the source (n ts ) and the candidate tag (n tc ) and the number of times they occur with each other (n tsc ):"
"as part of our experiments, we deployed the proposed mechanism on a machine intel core i5 1.3 ghz with 4 gb memory. the purpose of these experiments is to evaluate the validity of our algorithms and their feasibility. two criteria retained our attention: the algorithms scalability and the quality of their results. by the scalability, we intend to evaluate if the execution time is reasonable and if it scales up in a reasonable way. by evaluating the quality of the proposed, we intend to evaluate whether the proposed algorithms can effectively reach their goals. in order to perform such an evaluation, we formulate a set of user's requests relatives to the travel domain. these requests represent the user's intention and her/his current context. these requests are formalized according to three different distributions. the first distribution considers requests that are very similar to one or more available services (for the service discovery mechanism) or to a clusters centroid (for the service prediction mechanism). then, the second distribution illustrates situations in which the elements describing the intention and/or the context are not described in the ontologies, while there are services or clusters that could be considered as similar to this request. finally, the third distribution shows the influence of the threshold by presenting in this distribution requests that are within the limits of the threshold and others that are beyond the threshold."
"we propose a new user-centric approach for service discovery and prediction considering pis. this approach is based on both user's intentions, representing the goals she/he wants to achieve without saying how to perform it [cit], and on the context in which these intentions have been formulated. the notion of context can be seen as any information that can be used to characterize the situation of an entity [cit] . we consider that context information can influence service execution and, consequently, what service can be chosen to satisfy a given intention. in our opinion, both concepts should be considered during service discovery, since the main purpose is providing user with a service that can fulfill her/his goals in a fairly understandable and non-intrusive way. thus, we propose a new service discovery mechanism that intends discovering the available service that can satisfy the immediate user's intention in the current context. based on the discovery results, we propose a new prediction mechanism that identifies common situations representing usage patterns, i.e., recurrent context and intentions observed during the pis usage. by analyzing these patterns, the prediction mechanism learns user's behavior when using a pis, and therefore anticipates future intentions and the most appropriate services that may satisfy it. this paper is organized as follows: section 2 presents related works on service discovery and prediction. section 3 introduces the user-centric approach we propose for pis. based on this approach, section 4 presents the proposed service discovery mechanism, while section 5 introduces the service prediction mechanism. section 6 presents an evaluation of both mechanisms. finally, section 7 discusses future directions, before concluding in section 8."
"by extending its influence outside traditional enterprise perimeter, pis are able to integrate services that are made available by pervasive environments frequented by users, wherever they are. available services, as well as their nature, may vary according to the environment they integrate. this complexity is not necessarily understood by users, who are not interested in knowing service alternatives, but only on having an appropriate service. discovering best-suited service becomes then a challenging task since it is necessary to observe not only user's goals, but also the circumstances under which such goals appear. besides, a more proactive behavior should also be considered. indeed, user expects a pis that is able to anticipate her/his needs, in a transparent way, for more productivity. thus, transparency and proactivity become key aspects on pis, which require offering user appropriate services considering her/his goals and the context in which such goals appear, as well as the capability of anticipating future goals in this context. new service discovery and prediction mechanisms that cope with such requirements are then necessary."
"the prediction process is thus mainly based on the results of the classification phase in order to predict the next user's intention and service. this prediction process, represented by the algorithm in table 3, is based on the semantic matching between the user's immediate intention and context and those of the states represented in the user's behavior model. similar to the discovery mechanism, the semantic matching is based on ontologies in order to calculate the intentional and contextual matching scores. the final matching score represents the sum of the intention matching score and the context matching score. this information is stored with the state identifier. and going through all the states of the model, we can determine the state the most similar to the current user's situation. subsequently, if a state is identified, then the next state is selected based on the transition probabilities. this transition probability must exceed a certain threshold. if several successor states are retrieved, then the one having the highest transition probability is chosen. by this choice, we can anticipate the user's future needs by offering her/him the most appropriate service that can interest her/him."
"to create the corpora for the evaluation, we mirrored 156 news media sites from the newslink.org, kidon.com and abyznewslinks.com directories. the weblyzard suite of web mining tools (www.weblyzard.com) crawls those sites in regular intervals, gathering around 200,000 documents per week. [cit] . since the number of documents in each corpus was restricted to 1250, the domain corpora represent a broad overview of monthly media coverage on the seed concepts. [cit] corpus. the column unstructured includes terms generated solely based on corpus data, the remaining columns the first 17 of the related tags collected from social evidence sources."
the remainder of this paper is structured as follows: section ii presents a method for learning domain ontologies which considers social evidence sources in the ontology building process and describes the social sources used by the architecture. section iii presents an evaluation of our approach which has been performed using an evaluation measure and domain experts. the paper closes with an outlook and conclusions drawn in section iv.
"different service discovery mechanisms have been proposed in the literature [cit] . main goal of such works is to propose services that are better adapted to the user's requirements by observing real conditions under which such services are invoked. [cit], context-aware service discovery can be defined as the ability to make use of context information in order to discover the most relevant service for the user. most of the proposed context-aware service discovery mechanisms consider context information as a non-functional aspect of a service [cit], or as a condition for service selection and execution [cit] ). on both cases, a matchmaking, using semantic matching [cit] or similarity measures [cit], is performed between context information related to the service and the one related to the user or to the execution environment."
"in this paper, we have proposed a user-centric approach for service discovery and prediction considering pis. this approach is needed in order to hide the complexity of these systems and to achieve the transparency required by their users. indeed, new technologies are transforming the way users interact with information systems (is). new trends, such as byod and iot transform is environment on a pervasive environment, full of complex and heterogeneous technologies. new pervasive information systems have to integrate this environment, without penalizing the user, who should focus on her/his own activities and not on the environment. a new approach for conceiving pis is then necessary in order to reach context-awareness and transparency needed for these systems. we have introduced in this paper a user-centric approach that focus on both user's intention and execution context. we advocate that pis transparency and proactivity can be enhanced throw the proposed service discovery and prediction mechanisms, defined considering the user's point of view. these allow us not only offering user the most suitable services given her/his current intention and context, but also to anticipate the future user's needs in a fairly understandable way. by this approach, we believe contributing to the improvement of pis transparency and proactivity through a user-centric perspective focusing on the intentions that services satisfy in a given context."
"the proposed service discovery mechanism allows determining the most appropriate service considering immediate user's intention and her/his current context. nevertheless, this mechanism corresponds to a reactive behavior, since discovery process is launch by an active service request made by the user. user actively demands to the service discovery mechanism to satisfy a given intention. this reactive behavior has to be completed by a more proactive one, in which the system anticipates the user's next intention. such a proactive behavior is offered by the prediction mechanism, that we explain in the next section."
"applications and services are no longer limited to the strict boundaries set by the desktop computer. users are interested in having access to those services independently of whether these services are provided via a desktop computer, a mobile phone, or an information kiosk located in a public place [cit] ."
a transformation function t transforms ontology concepts c into tags t for comparing tag popularities. we apply the dice coefficient to determine the similarity (s d ) between these tags:
"unfortunately, the influence of context on the intention satisfaction is merely considered on the literature, context being often seen as a simple input on intention-based mechanisms [cit] )."
"our first experiment concerns the evaluation of the proposed service discovery mechanism. we measure the performance of the discovery algorithm by varying the number of services between 100 and 400. as illustrated in fig. 7, the execution time follows a polynomial trend of degree three varying from 2.79 s for 10 services to 11.82 s for 400 services. indeed, this trend correspond to a complexity o (n 3 ), where n represents the set of available services. we consider that this set will be greater than the set of contextual conditions that is supposed to be match by a service, and than the set of context observations available on the user's context. this means that, in its worst case, discovery algorithm executes respecting such a polynomial complexity. such polynomial complexity explains the observed execution time. even if this time seems still high, we can observe that despite the fact that we have increased the number of services over forty times, the response time has only increased by four times. besides, in order to measure the quality of the result, we cover the two most useful quality metrics: precision and recall [cit] ). as indicated in table 4, precision represents the capacity of a system to return only relevant items, while recall indicates the capacity of a system to discover all relevant items. through the experiments, we observe that the precision and recall are interesting factors when considering the intention and the context in service discovery. the result presented in fig. 8 shows that we obtained a higher precision percentage, about 80%. this indicates that our service discovery algorithm has a greater chance to retrieve the most appropriate service according to user's intention and context. however, the good results of precision are accompanied by less interesting results concerning the recall, as illustrated in fig. 8 . we can observe that the average recall approximates the 67%. these results can be explained by the different request distribution we prepared for these experiments. indeed, in our second distribution, we have described some user's requests in which the elements of the intention are not described in ontologies, while it exists a set of services able to satisfy this intention in the current user's context. the evaluation of the requests belonging to this second distribution leads to the evaluation of situations that can potentially harm the results quality."
"however, several other questions raise from these mechanisms and from future directions on pis. the first of these questions consider the complexity of intentions related to the pis services. the prediction mechanism we propose allows identifying only simple sequences of 1+1 services, representing the current intention and the following one, without considering whenever such sequences of intentions correspond to a bigger strategy. [cit], high-level intentions can be decomposed on lower level ones. this means that, in order to satisfy a complex high-level intention, user may have to first satisfy several low level intentions, simplest and consequently easiest to operationalize. currently, our prediction mechanism allows the dynamic composition of sequences of two intentions, but different other forms of composition are possible. [cit] have identified several different operators, including parallel intentions, i.e. intentions related by a and construct, in which the satisfaction of a bigger intention demands the satisfaction of other smaller intentions simultaneously, and variability, in which multiple alternative intentions can be used in order to satisfy a high-level intention (like an or construct). today, such composition of intentions is only available at design time: services encapsulating such compositions should be predefined and described as such [cit] for details); no other dynamic composition of intentions is possible."
"as an information system, a pis must consider goals users want to achieve by executing services it offers. indeed, an is is designed to support precise business goals, proposed as part of the overall organization strategy. these goals are independent from the technology used to implement them. users should focus on these goals and not on the technology itself."
"our second experiments concern the evaluation of our service prediction mechanism. we measure the performance of our algorithms with respect to the number of clusters, situations and states, by measuring the average processing time. concerning the clustering algorithm, we measure the time this algorithm takes to determine the belonging cluster of a new observation. results, presented in fig. 9, demonstrate that the execution time observes a polynomial trend, varying from 2.56s for 7 clusters up to 5.1s for 186 clusters. in other words, by increasing about 6x the number of clusters, execution time has augmented only about 2x."
"the service discovery and prediction mechanisms presented in this paper were implemented using java language, jena23 2 and the reasoner pellet 3 . in order to evaluate the proposed mechanisms, we use the test collection owls-tc 4, a very popular set of owl-s service descriptions available in different domains. we focus, for evaluation purposes, on the traditional travel domain, selecting the services descriptions concerning this domain. we extend these service descriptions proposed by owls-tc, using our api owl-sic (owl-s intentional & contextual) [cit], which includes, on each service description, intentional and contextual information. besides, we create a database containing user's traces, recognized clusters and the user's behavior model. we mixed a set of arbitrary traces with others following a scenario representing a well-defined user behavior. thereafter, we launch the clustering algorithm on the set of traces in order to determine all the recognized clusters. then, we execute the classification algorithm on all traces and clusters in order to update the user's behavior model stored in the database."
"querying web 2.0 services (e.g., tagging, social networking and micro-blogging applications) with seed ontology terms provides candidate concepts for the extended ontology. the taginfoservice interface of the easy web retrieval toolkit (ewrt; www.semanticlab.net/index.php/ewrt) helped capture these social evidence sources. the interface allows determining a tag's popularity, and retrieving tags related to the input tag."
"the pmi measure is complemented by domain expert assessments. four domain experts rated each relation identified by the ontology learning components on a discrete scale from not relevant (0), slightly relevant (1) and very relevant (2) to the domain. the average expert rating per relation serves as measure to evaluate relations learned from unstructured sources (column 2) and unstructured combined with social sources (column 3). we applied the measures to five ontologies based on corpus data from the time periods indicated in column 1. the values in parenthesis refer to the number of relations unique to the respective ontology and therefore omits relations shared between the learned ontologies as the evaluation focuses on the differences in the terminology yielded by both methods."
"from these recognized clusters and the user's history, the classification phase determines and maintains a user's behavior model, as illustrated in fig. 6 . this model represents the user's behavior as a set of states with a transition probability. each state is represented by the centroid of the recognized cluster. each probability is calculated based on the history and determines the probability of moving from one state to another."
"future work will address the issue of extracting n-grams from social sources by combining statistical approaches toward tag relatedness such as co-occurrence analysis, distributional measures and folkrank (17) with thesauri and lexicons. additional and refined evaluation metrics will be able to detect and assess shifts in terminology caused by integrating social evidence sources, for example the inclusion of implicit domain terminology or latest trends. incorporating evidence structured sources such as swoogle, dbpedia and freebase in the extension process represents another promising research avenue."
"n tsc denotes to the number of times the tags t s and t c have been used together to tag a blog or a web site, (n ts + n tc ) refers to the number of times a web site has been tagged using any of these tags. an inherent problem of consulting external evidence sources is the introduction of unrelated terms due to linguistic ambiguities. currently, we utilize wordnet data (wordnet.princeton.edu) for word sense disambiguation in order to minimize the negative impact of such ambiguities."
"unfortunately, at the best of our knowledge, none of these works has proposed combining intention aspect with context information in a proactive or anticipatory behavior. in general, the notion of intention, representing concrete user's goals, remains totally unexplored by these works. this may represent an important drawback, since prediction is performed based only on technical solutions, ignoring user's intentions behind (and guiding) user's actions. indeed, by ignoring user's goals, these approaches may improve the risk of recommending irrelevant services for users."
"the overall context matching process proceeds as follows: for each condition cxi and an observation cxj, we first match the corresponding subjects, using the context ontology. this match is calculated, like the verb and target matching, based on the distance between both concepts in the context ontology. if this matching score is higher than a given threshold, then we match the context elements associated with this subject. this last matching takes also into account a weight assigned to the context elements. if the matching score between them is higher than a given threshold, only at this moment we evaluate the satisfaction of the context condition regarding to the user's context observations value one by one."
"these new usages are changing the panorama of is (information systems) and it departments. several challenges raise from these, such as the management of heterogeneous hardware and software, or the enforcement of security policies on this heterogeneous environment [cit] . nevertheless, this new panorama also offers advantages and opportunities for business companies, such as reducing technology costs and increasing employees' productivity [cit] ."
"often on those works [cit], context information is considered as required capabilities imposed by the user's request. context information and the service capabilities are semantically described thanks to multiple ontologies. such ontologies allow describing service functional properties (mainly input and output information) and describing context elements and values. based on these ontologies, different semantic matching mechanisms have been proposed [cit] . most of them are based on simple relations of subsumes and plugins between concepts on an ontology. [cit], for example, consider exact, subsumes and plugin relations between capabilities offered by the services and those required by the user, both represented as ontology concepts. [cit] adopts the same relations in order to discover services that propose exactly the required capabilities (exactcapabilitymatch), or services that offer more general capabilities than those required by the user (inclusivecapabilitymatch) or more specific ones (weakcapabilitymach). [cit] also consider to calculating a semantic distance, still based on ontologies, between concepts used to describe offered and required capabilities."
"besides, other improvements on the proposed algorithms might be considered. [cit], could be considered on the discovery mechanism. [cit], could be considered for calculating the centroid of a cluster on the prediction algorithms. such improvements should be considered and evaluated in the future."
"moreover, evaluation results we have presented in this paper are promising and encourage us to go further on our approach. proposed algorithms have demonstrated to scale up and have presented interesting quality results. these encourage us to improve current implementation of these algorithms, by considering different optimization techniques, such as parallelizing segments of code. besides, an improved evaluation of the algorithms is also needed. indeed, evaluating the user acceptation of the proposal requires applying it in a real case study. such evaluation should consider the final user's point of view. it should consider the user acceptance, mainly considering the prediction mechanism, as well as the level of transparency perceived by these users. as a future work, we expect to evaluate our approach in a large-scale in order to validate its usefulness and compare it with the existing techniques."
"the terms extracted from corpus data (column 1) vary from very relevant (e.g. \"carbon dioxide emissions\") to hardly relevant at all (e.g. \"levels\"). this also applies to terms gathered from social sources, where the percentage of non-relevant entries is even higher. the fact that some of the input terms generated in previous ontology learning steps are themselves not domain-relevant might help explain this observation, but even for domain-relevant input social evidence sources return terminology of different levels of relevance. social sources are still helpful when combined with evidence from unstructured sources as described in section ii, which decreases the influence of irrelevant terms and enforces relevant terminology."
"more precisely, the input of the clustering phase consists of a set of vectors representing user's situations stored in the history. for each new observed situation in the user's history, the clustering phase determines closest cluster by comparing intention and context from this situation with those coming from the centroid of the identified clusters. in order to compare them, we use the same intention matching and context matching used for the discovery phase. if this matching is below a given threshold, the situation is included in the closest identified cluster. if no existent cluster is enough similar to the situation, a new cluster is created. thus, after a clustering phase, the corresponding cluster identifier is attached to each new situations stored in the history."
"dynamically composing coarse-grained services will become a priority for pis in a near future thanks to the integration of iot technologies on future pis. with the development of new iot technologies, pis will have at their disposal in the future new multiple devices (sensors, actuators, etc.), offering multiple services and data. indeed, the cost of bandwidth and processing power have declined as well as the cost of sensors, that become so cheap they can be used in many devices [cit], \"within the next 25 years the world around us will consist mainly of input and output components to networked computers; most of the things and devices we interact with will be linked to a global computing infrastructure\". the iot vision considers a full new world of physical and virtual 'things' that are seamlessly integrated into the network and fully communicating. these 'things' are supposed to interact and to communicate among themselves and with the environment by exchanging data and information 'sensed' about the environment while reacting to physical world events [cit] . a full new panel of collected data and services will be then possible thanks to the integration of such 'things' into pis, opening on those systems new perspectives of advanced features and services."
"during the last decade, a lot of research has been conducted concerning pervasive systems, and notably several works concerning context-aware services [cit] . on the one hand, context-awareness becomes a necessary feature for providing adaptable services, for instance, when selecting the best-suited service according to the relevant context information or when adapting the service during its execution according to context changes [cit] ). on the other hand, loose coupling offered by services fits the requirements of high dynamic pervasive environments, in which entities are often mobile, entering and leaving the environment at any moment [cit] )."
"the comparison in table iii shows that the results obtained with social evidence sources clearly outperform corpus-only data for both evaluation metrics, and for each of the ontologies evaluated. the differences in the average evaluation score of the two methods are significant, exceeding 99.9% for a welch two sample t-test (for the pmi) as well as a wilcoxon rank sum test with continuity correction (for the discrete expert ranking). the average standard deviation among expert assessments is 0.45. a substantial portion of disagreement was caused by a single evaluator, the standard deviation among the remaining experts amounts to 0.34. table iv contrasts a selection of terms included and removed in accordance with social evidence sources. most of the removed terms (as well as the added ones) are relevant, but n-grams are often removed due to the bias of tagging towards unigrams. user-generated tags typically consist of unigrams, although users often indicate n-grams by concatenating words or using underscores to separate them. since there is no agreed notation for n-grams, such tags are rare and hard to extract. many users also use abbreviations such as agw (anthropogenic global warming), cprs (carbon pollution reduction schema) and epa (environmental protection agency). the included and removed concepts demonstrate the impact of social sources on the extension process, currently emphasizing unigrams and therefore causing the removal of some relevant n-grams such as climate change policy or reduce greenhouse gas. depending on the actual application of the technology, deployed systems should consider this effect by compensating n-gram resources for the lack of support from tagged resources."
"one of the biggest criticisms levelled at clhs is the rigidness around the sampling configuration. [cit] remarked on the lack of guidance on what to do when a site cannot be accessed. they arrived at the same conclusion, that rather than continuing with clhc sampling, the more flexible sampling approach of fuzzy stratified sampling be used. [cit] demonstrated that this alternative sampling approach was comparable to clhs in their work. nevertheless, once a sampling location selected by the clhc algorithm is unable to be visited for any number of reasons, a replacement site needs to be selected. the introduction of replacement sites can potentially degrade the objective of clhs if the replacement sites are arbitrarily selected in nearby locations that are accessible."
"2) utility model of vehicle for the vehicle, if the computation resources are contributed and the resources are utilized by other vehicles, the reward corresponding to contract of the type n,(ξ n, r (ξ n )) can be obtained, the utility function of the vehicle i in the contract of n is as follows"
"a maximum phase angle delay of 11.4° occurs at a heart rate of 65 bpm and a pulse pressure of 60 mmhg, as shown in fig. 14b . this phase angle delay is small enough to be 3.2% when converted to a percentage error in one cycle (360°). as a result, the phase delay effect caused by using air instead of incompressible liquid in the proposed simulator is sufficiently small, thus proving that a very accurate pulse pressure waveform can be reproduced using the air-based three-peak cam simulator."
"as shown in fig. 10, when comparing the upstroke slopes of early systolic pressure in the three measured waveforms, it can be seen that the slope of the representative pressure waveform of the human is the steepest. this is because the proposed simulator generates a pressure waveform by compressing and tensioning air instead of an incompressible liquid similar to blood. owing to the nature of the air, which is a compressible fluid, the slope of the upstroke becomes less steep, resulting in a phase delay. to ensure that this phase delay effect is small enough to be ignored, an error analysis of the phase delay was performed among the representative pulse waveforms of a human, the pulse wave measured by the rts on the skin above the simulator's wrist, and the pressure sensor outputs built into the simulator's vessel, as described in fig. 10 . figure 12 shows the amplitude of fourier transform at each frequency in the frequency domain when a discrete fourier transform was applied to the pulse waveforms measured in the human and the simulator. the results of the discrete fourier transform showed almost no difference between the amplitudes obtained from the human waveform and simulator waveform. the results also showed that the pulse waveforms measured in the human and the simulator had a dominant amplitude in the low-frequency range. thus, we investigated the difference in phase angle at low frequencies of 1 hz and 2 hz to determine the phase delay between pulse waveforms. if y h is the pulse shape of a human, and y c is the waveform generated by the cam simulator, the discrete fourier transform is defined as follows:"
"environment feedbacks the reward value r k to the agent according to the action a * and the current system state and the next state s k+1 based on the action a *, besides r k will be used to update the q value table in the agent. the update equation is as follows"
"as the sdd will scale from 0 to 1, values close to 1 means at least one observation point is similar to the ancillary data for the selected pixel."
"ironically this question arose from a situation where we examined data from a ∼100 ha field. within this field 238 soil samples were collected by regular grid sampling. soil samples were then analyzed for key soil properties. also collected from the field were ancillary data including soil conductivity (from an em38 sensor), elevation, and crop yield data. our interest was in determining whether the 238 soil samples adequately covered the ancillary data space and if an alternative sampling configuration using clhs could be used to determine a specific sample number that would achieve the same data coverage as the 238 sites. we acknowledge that for most contexts, practitioners will not experience the same situation we have just described, but will only have a suite of ancillary data and will need to estimate an optimal sample number."
"where else can i sample when a clhs location cannot be visited because of difficult terrain, locked gate, safety reasons, etc.? table 1 presents the kl divergences for each ancillary variable for both the original sample configuration and the alternative sample configuration for each of the 341 sites. overall, the mean kl divergence is equal for both sampling configurations which is a desired outcome of the algorithm. [cit] it becomes possible to derive alternative sample locations within the field at the time of sampling. this delivers to the field technician an objective way to select an alternative sample site (or possibly even a general area) during the survey campaign."
"we implemented experimental simulations under different parameters, such as the number of vehicles, the size of minibatch, the size of the reply memory, and the learning rate, and compared and analyzed the simulation."
"in this work, the r package clhs [cit] was used for performing the clhs. the inputs into the clhs function included a table form (r data.frame) of the ancillary data that was collected in the field of interest, that is, the proximal sensed data and yield data. we used the default 10,000 iterations of the annealing process to run the function. a schematic of the sample size optimization algorithm is given in fig. 1 ."
"the algorithm will terminate once s sample locations have been selected. a caveat to using the ahels algorithm is that it is not immediately amenable to optimizing sample size as described in the earlier section. essentially, one needs to select a sample size, then run the algorithm."
"this example uses the same ancillary data and the 341 sample sites from the hwcpid study area described earlier. for this example, we want to add an additional 100 samples while accounting for the prior 341 sample sites. we call this algorithm, adapted hels (ahels). a schematic of the ahels algorithm is shown in fig. 3 and described in the following: 1a. construct a quantile matrix of the ancillary data. if there are k ancillary variables, the quantile matrix will be of (s + 1) â k dimensions. the rows of the matrix are the quantiles of each ancillary variable."
"and r (ξ n ) represents the bonus value corresponding to the resource contribution ξ n . the more vehicles contribute the computation resources and the greater the probability that the resource will be utilized by other vehicles, the higher the reward value will be."
"hunter valley region, of new south wales, approximately 140 km north of sydney, australia. [cit] . for this example, we used the following ancillary variables derived from a 25 m digital elevation model: elevation, slope, terrain wetness index, multi-resolution valley bottom flatness, and potential incoming solar radiation. we also used categorical rasterized data from a 1:100,000 geological unit map [cit], and a 1:250,000 legacy soil map depicting the surveyed soil units [cit] ) that cover the hwcpid. in this example, each of the 341 sampling locations were assumed to be inaccessible and alternative sites were selected for each. we then compared the kl divergence between the original and alternative sample sites and assess whether the alternative sample sites capture the same environmental information using the following algorithm (also illustrated in fig. 2) ."
"in this paper, we propose a new contract theory that incents vehicles contribute their computation resources and get the rewards so that they can use them to exchange additional resources from the iov to improve the qos of their application, such as task processing delay, energy consumption, etc. we also use drl method to reduce the system implementation complexity, which based on our proposed contract theory offloads the tasks of vehicles and reasonably allocates system resources to achieve better system performance. our main contributions of this work are summarized as follows:"
"where p k i represents the wireless transmission power of the vehicle i, h ij represents the wireless channel gain between the vehicle i and the vehicle j, and n 0 represents the noise power [cit] . the gain of the wireless channel can be expressed as"
"in the drl model proposed in this paper, dqn outputs the q values of all actions corresponding to the state feature vector. with the increase of the number of vehicles, the space size of the action a for representing the offloading decision and resource management will be very large, so the output value space of the dqn network will be very large, which will affect the implementation effect of the action selection strategy."
the purpose of this technical note is to describe solutions to each of these questions. we first begin with a brief overview of the clhs algorithm and then address each question separately.
"the agent contains a dnn. the input of the neural network is the state feature vector s k of the current environment, and the output of the neural network is the q value q (s, a) obtained from all the actions taken under the state s k . the action a * is selected to act on the environment through the action selection strategy π . different from reinforcement learning, on the one hand, drl introduces a dnn instead of the q value table, and on the other hand, a reply memory is set up to store parameters s k, a k, r k, s k+1 during the interactions between the agent and environment, periodically sampling ω data from the reply memory for training the dnn, and updating the weight parameter θ dnn in the dnn to minimize the loss function. the loss function is as follows"
"at the same time, we will use the adam optimizer to optimize the loss function in the neural network [cit] . other detailed parameters about neural networks and default application scenarios [cit], which are shown in table 1 . volume 8, 2020 according to the setting of the scene and the construction of the model, we will compare the system performance from the following aspects:"
"where t is the time interval of each time slot. at the same time, the vehicle will also generate new tasks (t k+1 i, d k+1 i ) that needs to processing."
"1a. calculate the multivariate distance to every other location in the stacked ancillary data. we use the mahalanobis distance because it preserves the correlation between variables, but other distance or similarity metrics can be considered."
1. matching the sample with the empirical distribution functions of the continuous ancillary variables 2. matching the sample with the empirical distribution functions of the categorical ancillary variables; and 3. matching the sample with the correlation matrix of the continuous ancillary variables.
"in this chapter, we propose a contract theory-based incentive mechanism for the current iov scenario to encourage vehicles to contribute their own idle computation resources."
"this metric could also be useful for optimization of a sample size too, but the difference between it and the kl divergence is that it penalizes all departures from the population values equally. with the kl divergence, the penalty of missing a sample from the tail of a distribution is larger than the penalty of missing a sample close to the mode."
"we assume that vehicles within the rsu communication range are classified into n types of vehicles according to their shareable resources, each type corresponding to a contract (ξ n, r (ξ n )), where ξ n represents the type of resource contribution of vehicles, which is given by"
"this article is organized as follows. the next section describes the target pulse pressure waveform of the radial artery that the proposed simulator is trying to reproduce. the following design and development section explains the schematic diagram and the developed platform of the radial pulsation simulator based on the physiological behavior of the human body. finally, after describing the process of the pressure data measurement from the developed simulator, an analysis and discussion of the experimental results conclude the article."
"to estimate an optimal sample size, we derived a cumulative density function for the fitted line on fig. 5a, and identified the sample size required to capture 95% of the cumulative probability (fig. 5b) . the optimal sample size was 110. the kl divergence for this sample size was 0.033, slightly lower (better) than that achieved with the actual 238 grid samples."
"current pulse simulators, including simulators developed in above mentioned studies, are complex, bulky, and expensive. in most simulators, the method of controlling the fluid to generate the blood pressure waveform is a quite complicated. even if sophisticated and expensive simulators, they have limitations in generating a wide range of radial pulse waveforms, which may be hundreds depending on human race, sex, age, and health conditions, by adjusting several valves and flow rates. note that it is extremely difficult to control the reflected waves sporadically generated in the liquid. in addition, liquid-based simulators, in particular for portable ones, pose potential problems such as distortion of pressure waveforms due to cavitation and the leakage of liquid."
"in this section, we will introduce a task offloading and resource allocation mechanism based on drl, and explain the key parts of reinforcement learning and the architecture of deep q-networks (dqn) in detail."
"although the rsu has rich computation resources and strong computing capacity, it is also limited in computation resources. when the task of offloaded to the rsu exceeds the total computation resources of the rsu, some tasks processing would fail, which is denoted as"
2a. calculate the multivariate distance of the ancillary data between the pixel i and each of the legacy data points. we call this the data distance (dd).
"the objective of the minimization problem, eq. (1), is to find the fourier series equation that most closely matches the human measured data while minimizing the relative error of the radial ai. therefore, when the evolution of e ri (u) is plotted in fig. 3 while solving the minimization problem using iterative methods. as shown in fig. 3, 150 iterations were performed so that the relative error of the radial ai could be sufficiently reduced to 0.00247%. through this iteration process, the coefficients of the fourier series function of eq. (2) are obtained as shown in table 1 . in addition, the l 2 error of the total waveform, e l 2 (u), is 3.64%, which means that the representative waveform is well matched to the average waveform of u i . the coefficients of table 1 determined through the minimization problem are applied to the representative waveform eq. (2), and the equation is plotted as shown in fig. 4 . in fig. 4, the value of the radial augmentation index defined by eq. (3) is calculated as 73.3%, which is similar to the average radial ai of men (69.5% ± 16.3%, [cit] ). therefore, it was confirmed that the representative waveform obtained by the fourier series preserved the average radial ai of men."
"in the following, we demonstrate the usage of kl divergence to help identify an optimal sample size in the ∼100 ha field previously described. the clhs algorithm was run sequentially with increasing sample sizes beginning at 10 and finishing at 500. a step size of 10 samples was used. each sample size was repeated 10 times to assess the dispersion about the mean kl divergence estimate. kl divergence was estimated using a bin number of 25 for each ancillary variable and then aggregated for all variables by calculating the mean. note that the justification for selecting a bin number of 25 [cit] . our own small investigations indicated this bin number to be adequate for our data, but would recommend for other studies and contexts to evaluate the kl divergence response to various bin numbers via iteration. too few bins may not adequately capture the main features of a distribution, giving misleading low divergence values. while too many bins maybe overly sensitive to noise and provide high kl divergence values in otherwise near matching distributions."
"in the dqn architecture, we set up a four-layer neural network, one of which is an input layer, one is an output layer, and two hidden layers. the number of neurons in the two hidden layers is 600, 300. we will use the relu function as the activation function and relu function of this neural network is as follows"
"because the state feature vector space of this scenario increases with the number of vehicles in the environment, the agent needs to maintain a large q value table, and it takes a lot of iterations to make the q value converge, so we will use drl to get convergent q values faster."
"2. calculate the multivariate distance between the inaccessible site and all ancillary data in the sampling zone. if ancillary data is categorical, remove all areas of the sampling zone that do not match the category at the initial sampling location. if ancillary data is continuous calculate a mahalanobis distance."
"the kl divergence decreases towards zero as the population and sample distributions converge [cit] and can be defined for both categorical and continuous ancillary data. signal processing is probably the most common use case for kl divergence, such as the characterization of relative entropy in information systems where the related shannon's information criterion [cit] ) is widely applied. although [cit] clhs algorithm, where instead comparison is made using the quantity:"
"the remaining parts of the paper are summarized as follows. the system model is introduced in section ii. in section iii, a new mechanism based contract theory is presented. we describe a framework of drl based task offload and resource allocation in section iv. in section v, the simulation presented. conclusions and future work are drawn in section vi and section vii, respectively."
"3. set a similarity threshold close to 1, then count the number of legacy observations that are equal to or better than that criteria. save this number and pixel location together, then move to the next pixel, that is, go back to step 2a."
where e and t represent the change of energy and task processing time when the task t is processed by the rsu and the vehicle as
1c. calculate the data density of ancillary information from the existing legacy samples. this is the same as 1b except the number of existing observations are tallied within each quantile (quantile matrix from 1a) and the density is calculated by dividing the tallied number by o instead of r.
"the following information was supplied regarding data availability: bitbucket: https://bitbucket.org/brendo1001/clhc_sampling/src/master/. proportions of original and additional sampling sites in the hwcpid selected using the adapted hels algorithm that occur within groupings of the sample data coverage, determined using the coobs algorithm. we note that coobs means at the pixel level, the count of observations estimated to be similar in terms of the given ancillary data."
"one purpose of vehicle contribution computation resources is to ease the burden of rsu computing tasks, while rsu is also responsible for resource scheduling. therefore, we assume that the rsu utility function can be improved by reducing task processing latency, reducing the energy consumed by task processing, increasing the task completion ratio within the system, and reducing the utilization of system frequency resources, which is denoted as"
"the reference input signal of a radial pulse waveform was acquired from the clinical data of a healthy young adult because the second peak of the pulse waveform was apparently observed in young people [cit] . as shown in fig. 1, the robotic tonometry system (rts), developed by the authors at korea institute of oriental medicine in south korea, was utilized to obtain the radial pulse waveform with high precision by autonomously detecting the exact pulsation positions and precisely pressurizing the radial artery using a 6-dof robotic manipulator including one redundant actuator [cit] . a pulse sensor array with six pressure sensory channels was attached to the end effector of the rts to maintain a constant posture and contact force on the radial artery [cit] . a 3-dof motorized stage moved the center position of the pulse sensor to the exact pulsation position. the contact directions between the pulse sensor and the skin surface were controlled by two harmonic-driving actuators without gear backlash. a ball-screw typed linear actuator was used to precisely control the contact force of the pulse sensor. figure 2 shows the raw data of the pulse wave measured from the time that the pulse sensor reached for the wrist skin surface to pressurize the artificial radial artery. the pulse sensor incrementally pressed the radial artery until it found the pulse pressure (pp) that was the maximum value of the first peak magnitudes. when the pp was detected, the tonometry device maintained the contact force for about 60 s to reliably record the raw signals of the radial artery pulse waveform at the pp. the final reference signal of the radial artery pulse waveform was obtained by averaging the 40 pulse waveforms recorded in the steady-state region."
"comparing the empirical distribution functions of the samples and the population can be done in a number of ways. [cit] compared the overall variance of the ancillary data from an entire region, hereinafter referred to as the population, with that of the sample. the application of this approach for categorical data was not established, but for categorical data it is only necessary to match the relative proportions of the categories. other metrics for continuous variables could involve comparison of the quantiles of the empirical distributions. this would require comparing an absolute deviation between the quantiles (which could be absolute difference or euclidean distance) for each ancillary variable, then deriving a unified value by combining the distances (weighted averaging) from each variable. alternatively, comparison could be made of the principal components of the ancillary variables [cit] ), which would alleviate the need to first compute distances for each variable and then combine into a single measure of distance."
"how do i account for existing samples when designing a new survey? figure 6a shows a map with the original 341 sites and the additional 100 sites selected using the ahels algorithm. the coobs algorithm created the map on fig. 6b . a caveat to the coobs algorithm though is that it is computationally heavy. some of this burden can easily be diffused via compute parallelization, however. alternatively, and in order to take advantage of spatial correlation, it could be possible to take a representative sample from the provided ancillary data of a specified size, calculate magpd for each of these locations, then interpolate this target variable across the whole area. future investigations would need to assess the viability and efficiency of this approach. overall, both the ahels and coobs algorithms allow one to understand which areas in a spatial domain are adequately and under-sampled. the ahels algorithm is explicit in selecting sites preferentially to locations in the environment that are not captured by the existing site data. relatedly, the coobs algorithm provides a visualization of general areas where under sampling is prevalent. as an aside, it may be possible to use the coobs derived map to design an additional survey by constraining the clhs algorithm to areas where the coobs value is below some specified threshold. nevertheless, both ahels and coobs are somewhat comparable in their objectives and ultimately generate similar outcomes as shown in table 2 where the proportion of sample sites selected using the ahels is much higher in the areas where the coobs algorithm determined relatively low existing data coverage. for example, 87% of additional sites appear to be allocated in areas where the existing data coverage is 10 sites."
"for the various comparative analyses shown in fig. 10, the heart rate was adjusted to 65 bpm and 75 bpm by changing the rotational speed of the built-in motor in the simulator. the pulse pressure was regulated to 50 mmhg and 60 mmhg, respectively, by adjusting the internal volume of the simulator. in each case, the measured pulses were stored using the rts on the skin of the wrist of the simulator. at the same time, the pulses measured by a pressure sensor built into the simulator's silicone vessel were stored. in each case, the average waveforms were generated from the stored pulses, and then the magnitude and period were normalized to 1 and compared with the representative waveform of a human (fig. 4) as shown in fig. 10 . figure 10a shows the results obtained by measuring the heart rate and pulse pressure of the simulator at 65 bpm and 50 mmhg, respectively. in the figure, the measured data with the rts and pressure sensor were compared with the representative waveform of a human. figure 10b shows the comparison results when the simulator heart rate and pulse pressure were set to 65 bpm and 60 mmhg, respectively, and fig. 10c shows the comparison results at 75 bpm and 50 mmhg. figure 10d shows the comparison results at 75 bpm and 50 mmhg."
"the first peak of the radial artery pressure waveform was used to reconstruct the early systolic shoulder of the aortic pressure wave through a generalized transfer function [cit] . since the upstroke slope of the early systolic shoulder is related to the left ventricular contractility whose abnormality can initiate a clinically significant heart failure syndrome [cit], the slope of the first peak as well as the magnitude ratio (radial ai) were evaluated to be in good agreement with the upstroke slope of the representative human waveform."
"modification of the clhs objective function can be done so that accessibility is considered. [cit] where the cost of getting to sampling locations was incorporated into the annealing schedule of the clhs algorithm. such modifications, however, do not guarantee accessibility but do increase the probability that a site will be accessible. [cit] selected alternative sample sites by deriving numerous test sample sets based on the original clhs sites with all possible combinations of the covariate data from the clhs strata. they selected the test sample set that most closely represented the original clhs sites via quantile matching. [cit] was based on terrain slope and selected land use classes."
"for the vehicle i which offloads the task, the bonus value is exchanged for additional bandwidth resources, and the time taken for the processing task can be reduced. so the utility function is expressed as"
"where i indicates the rsu performance improvement, which means that the performance of the task offload to the vehicle processing is compared with the performance of task offload to the vehicle processing, and is given by"
"this is the situation where existing soil samples exist within a sampling domain, and the user wants to derive a sampling configuration that captures environmental variation that the existing samples fails to achieve appropriately. [cit] goes some way to addressing this problem through forming n1+n2 marginal strata per covariate. here n1 represents existing site data, and n2 represents marginal strata where new sampling locations can be derived. essentially one selects a sample size of n2, adds in the existing sample data (n1) that have previously been collected from the study area, and then runs the clhs algorithm. this appears a useful solution but probably needs to be tested or compared with other approaches such as described below. possibly a more explicit approach to dealing with the aforementioned problem, is to first assess the environmental coverage of the existing samples, then look for gaps (where there is no coverage in the data space), and then prioritizing new samples to those areas as they occur in the field. [cit] . a more thoroughly described approach is the hypercube evaluation of a legacy sample (hels) algorithm and its companion the hisq algorithm described in carré, [cit] . the hels algorithm is used to check the occupancy of the existing samples in the hypercube of the quantiles of given ancillary data, and to determine whether they occupy the hypercube uniformly or if there is over-or underobservation in partitions of the hypercube. the companion hisq algorithm is used to preferentially select additional samples in areas where the degree of under-observation is figure 2 selection of alternate sites. illustrated process of the algorithm for selecting an alternative sampling site when a clhs site is inaccessible. (a) a sample site (as indicated by a marked point) within the hwcpid area that has been determined to be inaccessible. (b) a circular buffer area is created around the site. if categorical data are being used, those categories that do not match that at the sampling greatest. a limitation of the hels algorithm is that the full hypercube of all ancillary variables needs to be computed (i.e., if there are k number of variables, k-cubes need to be formed). we present an algorithm that is an adaptation and simplification of the combined hels and hisq algorithms."
"after the action a k is transmitted to the environment, the environment will generate a reward value r k and feed it back to the agent. the r k here can use the results discussed in section iii as"
"in this article, we first designed a resource management scheme based on contract theory. the contract theory contains two aspects of the contract. one is a resource contribution contract. the vehicle contributes resources and obtains a reward value corresponding to the amount of resources it contributes, which motivates vehicles to contribute computation resources to the rsu. the other is a contract for resource utilization. the vehicle uses its accumulated reward in exchange for additional wireless transmission bandwidth to improve the qos of the vehicle application. for the entire system, while improving the overall performance of the system (delay, task completion rate, energy consumption), it is also necessary to minimize the utilization of system resources (spectrum resources). we use this as a basis to establish an optimization function for system performance."
"in this study, a radial pulsation simulator equipped with a cam mechanism was developed and tested. the developed simulator employed a pneumatic-driven mechanism to avoid the problems of liquid-driven devices, such as sporadic reflections of pressure waves, bubbles, and leakage. to design the cam profile, human pulse waveforms measured by a robotic tonometry system were mathematically modeled as one representative waveform. the representative waveform for a 20-year old was then converted into the circular cam profile. a cam design with three peak points was machined and mounted on a simulator, consisting of a rotating motor, a cylinder/piston module, an artificial wrist, and an lcd display module. the experimental results show that the proposed cam simulator can reproduce human representative waveforms with considerably small errors for the radial augmentation index and the phase delay effect with a maximum of 4.9% and 3.2%, respectively."
"the solution to choosing an optimal number of samples is to compare the empirical distribution functions of incrementally larger sample sizes with those of the population. the resulting output will display a typical exponential growth (or decay) curve, depending on what comparative metric is used, that plateaus at some point with increasing sample size. this allows one to invoke a diminishing returns-like rule to derive an optimal sample number. [cit] for evaluating an optimal sample size to calibrate a dsm model."
"in a practical setting, the 110 samples would be used as the minimum recommended sample size. however, one caveat is that other metrics of comparison are likely to result in differing optimal sample sizes because the underlying calculation is different. in situations where multiple comparison metrics are used, it might be pragmatic to assess what the minimum and maximum optimal sample sizes are, then select a recommended sample size in the middle. though likely not optimal, this approach would be an improvement on the arbitrary selection of sample size that currently occurs in most studies."
"to design a device capable of regenerating human-like pulse pressure using the fabricated three-peak cam, the cam was mounted on a dc motor (maxon motor, dcx 26 l), and a cylinder/piston module capable of repeating compression and tension according to the shape of the cam during rotation was mounted in connection"
"where α is the learning rate and β is the attenuation factor. during the interaction between the agent and the environment, the q value table maintained by the agent is continuously iteratively updated until the q value in the q value table converges. the agent can select the action with the greatest value to act on the environment according to the greedy algorithm."
"to monitor the air pressure inside the cylinder in real time, a small pressure sensor (honeywell, 40pc006g) was connected to the cylinder by a tube, and the measured pulse pressure value was displayed on the lcd screen in real time. the microprocessor was built in the housing and was used to calculate the pulse pressure, diastolic pressure, and mean pressure from the measured pulse pressure waveform value in real time. these values were displayed on the screen."
"secondly, we use the drl method to implement the proposed contract-based resource management and task offloading scheme. we use the previously established system performance optimization function as the reward in reinforcement learning. we set a reply memory, and use the data sampled from the reply memory to update the parameters in the deep neural network to optimize resource management policy and the decision of tasks offloading and improves system performance."
"this technical note provides some solutions to common questions that arise when the clhs algorithm is used for designing a soil or any other environmental survey. we have collated solutions that others have come up with to deal with such questions, and also presented new and/or modified solutions too. we have presented a solution to optimize a sample size number and to take into consideration existing soil samples. importantly we have introduced a relatively simple approach to relocate a site for soil sampling in situations of inaccessibility without deteriorating the original clhs site configuration. r scripts with associated data examples for each of these solutions is shared in a data repository at: https://bitbucket.org/brendo1001/clhc_sampling."
"for each vehicle, they can handle the task by processing it locally or offload the task to other place to process. in local processing, the time t c required by the vehicle to process the task in the k th time slot can be expressed as"
"we compare the performance of the proposed system model with the conventional model. in the conventional model, when the vehicle cannot complete the task within the maximum delay of the task, it will directly offload the task to the rsu for processing. although the rsu has relatively strong computing power, it can smoothly handle all tasks offloaded to the rsu in a scenario with a small number of vehicles. however, as the number of vehicles increases, the number of tasks offloaded to the rsu gradually increases, and the task processing pressure on the rsu increases too. as the rsu is in a full load state, the rsu has no free computation resources for other offloaded tasks to utilize. hence, these tasks will not be processed successfully. therefore, the larger the number of vehicles, the higher the failure rate of task processing, and the lower the performance level of the conventional offloading policy. as shown in fig. 8, with the increase in the number of vehicles, the task offloading scheme proposed in this paper can achieve performance improvement earlier, and the system performance has not received much impact, because with the increase in the number of vehicles, the number of tasks that vehicles offload to the rsu increases, but at the same time, the number of vehicles with idle computation resources also increases, so the computation resources contributed by vehicles also increase. however, the performance of the conventional offloading method in fig. 8 decreases as the number of vehicles increases. therefore, the resource management scheme proposed in this paper can motivate vehicles to contribute resources, and it is robust to the increase in the number of vehicles. as the number of vehicles increases, the system can maintain good performance."
"where dist is the mahalanobis distance of the ancillary data i and the inaccessible site and dist med is the median mahalanobis distance of the available data within the buffer area. we used the median because it is less susceptible to outliers than the mean. the similarity is expressed on a range between 1 and 0 with numbers approaching 1 being highly similar, and is akin to a membership function."
"as the task is offloaded, the vehicle i can offload the task to the vehicle j for processing and transmit task data through the v2v link. in order to improve the spectral efficiency, we assume that the allocated channels are orthogonal to each other and do not interfere with each other."
"we demonstrate this approach using a collection of 341 soil samples that cover the hunter wine country private irrigation district (hwcpid), a 220 km 2 region in the lower"
"4. select an alternative site where the similarity is above a given threshold. in our example this threshold was set to 0.975. alternatively, one could rank the sites (smallest to highest distance) and select a given number from the top for possible consideration. whichever the approach, an alternative site may be selected at random from the sites that pass the threshold similarity value, or alternatively the nearest one to the inaccessible site could be selected and so on until it is found that access to the sampling location is possible. in our example, we selected the former option."
"3. rank the density ratios from smallest to largest. across all elements of the (s+1) â k matrix of the ratios from step 2, rank them from smallest to largest. the smallest ratios indicate those quantiles that are under-sampled, and should be prioritized for selecting additional sample locations. in this ranking, it is important to save the element row and column indexes."
"in order to convert the fourier series equations (fig. 4) obtained from the human data continuously measured by rts into a three-peak circular shape, the normalized period (fig. 4) is converted to 360°, and the shape of the cam is schematized as shown in fig. 5a . this schematized three-peak cam design was fabricated through a wirecutting machining process of nonmagnetic and high-rigidity material, stainless-steel 304, as shown in fig. 5b ."
"from the above described coobs algorithm, and depending on how it is implemented, the first step will create a raster of the target area depicting the spatial pattern of magpd. this map is then interrogated pixel-by-pixel in step 2 to estimate the associated coobs number. alternatively, the two steps of the coobs algorithm could be implemented as a single workflow whereby the maps of magpd and coobs are derived at near the same time as each other, pixel-by-pixel. the resulting output is shown in fig. 5a which is a plot of the mean kl divergence with increasing sample size and is a classical exponential decay with increasing sample size. we do not show the standard deviation bars on this plot as they are very small with increasing sample size and are not visible on the plot. however, relatively larger standard deviations were observed at smaller sample sizes which decreased with increasing sample sizes. we also calculated the kl divergence for the 238 samples already collected from the field of interest. this was found to be 0.039, and as can be seen on fig. 5a as the \"+\" symbol. this mark is above the fitted line for the same sample size using clhs. this indicates that (for this example), clhs is superior to grid sampling for capturing the variation in ancillary data for the same sample size."
"neither of the approaches described above accommodate situations where issues of accessibility are found in the field, that is, site inaccessibility is unplanned. [cit] grappled with this issue and proposed a solution that involves simulated annealing for optimally selecting accessible sites from a region. this method involves multiple criteria such as the kl divergence, ease of access, and geographical coverage, which are all combined into a single criterion. while useful, this approach is computationally demanding to the extent that it would be difficult to apply directly in a field setting. to obviate the need for this, an ordered list of alternative sites close to each of the primary target sites (should the primary target site prove inaccessible) is produced prior to entering the field. [cit] would be to calculate a similarity measure to an inaccessible clhs site within a given buffer zone. in their example, a gower's dissimilarity index [cit] ) was used. with this approach, areas with high similarity to the inaccessible clhs site could then be identified and the new sample location moved to these areas if needed. such an approach would allow alternative sites to be determined when in the field, given an appropriate computation device or if areas with high similarity to each location were generated before field sampling begins. [cit] ."
"where p i denotes the transmission power of the vehicle i, and g t i, g r j denote the transmission antenna gain of the vehicle i and the reception antenna gain of the vehicle j, respectively. l ij represents the transmission loss, which we assume is the path loss of free space, and l ij is given by"
"in addition to a growing demand of pulsatile simulators for calibrating blood pressure sensors, they can make significant contributions to the scientific advancement of oriental medicine (om), such as modernizing or standardization of pulse diagnosis techniques. om or traditional chinese medicine is a long-established traditional medical practice in asia, but it is being widely used nowadays in western countries in the form of alternative medicines. om practices include pulse diagnosis, acupuncture, and herbal medicine. the pulse diagnosis is one of the most important diagnostic methods in om. it is based on the 3-finger technique that sense radial pulses at the terminal region of radial artery on a wrist by index, middle, and ring fingers to diagnose health conditions of internal organs. unfortunately, the pulse diagnosis technique is ambiguous, and it is not standardized. it depends on the pulse characteristics (intensity, patterns, etc.) and location of the figures. furthermore, it heavily relies on om doctors' subjective experiences. thus, there exists an urgent need for quantification or standardization of pulse waveforms to modernize and teach the pulse diagnosis. pulsatile simulators capable of reproducing standardized radial pulse waveforms reliably can play an important role in order to train om students and professionals and to meet the urgent need."
"the primary goal of this study is to develop a cost-effective and portable blood pulse simulator that can accurately and repetitively generate a human radial pulse waveform. to this end, it proposes to use a cam-follower mechanism to generate radial artery waveform. the proposed simulator adopts a pneumatic-driven mechanism to avoid the problem of pressure wave reflection, bubbles, and leakage produced in a liquid-driven device. in this study, a cam profile is determined based on a \"standard\" radial pulse waveform obtained by in vivo testing of a healthy young man in his 20 s. to demonstration purpose, only one cam is fabricated, but the proposed simulator is designed to easily replace the cam with other types of cams to generate other radial pulse waveforms. the design includes a dc motor connected to the cam and follower mechanism that pushes a piston into a cylinder to simulate the heart beat rate with its speed. the design also includes a \"diastolic\" chamber to adjust the pulse pressure of the waveforms. using the prototype simulator, a serious of testing was performed to evaluate its performance in generating radial pulse waveforms. these waveforms were compared with the human pulse profile."
"1. how many samples should i collect? 2. where else can i sample when a clhs location cannot be visited because of difficult terrain, locked gate, safety reasons etc.?"
"therefore, in future work, we can use other drl methods to improve the efficiency of the algorithm, such as deep deterministic policy gradient (ddpg). this method is a policy-based drl algorithm, that is, in ddpg, a action network can directly generate corresponding output actions based on the input state vector. a critic network in ddpg will evaluate the actions generated by the action network and continuously optimize the action selection strategy of the action network [cit] ."
"with the cam, as shown in fig. 6 . here, in order to measure and display the heart rate, a hall sensor capable of measuring the rotational speed of the dc motor was installed. an additional small cylinder/piston module was installed to control the diastolic pressure by adjusting the amount of air in the cylinder. in order to facilitate the rts or human to detect the pulse pressure wave generated when the air in the cylinder is compressed by the piston connected to the cam, a silicon artificial blood vessel was connected to the end of the cylinder, and the blood vessel was supported by an artificial wrist bone for tonometry and was surrounded by silicone skin (3b [cit] 0)."
"in order to analyze how accurately the developed cam-based pulsation simulator can regenerate the human representative radial pulse waveforms shown in fig. 4, error analyses were performed among the representative pulse waveforms of the human (fig. 4), the pulse wave measured by the rts on the skin above the simulator wrist (fig. 9), and pressure sensor outputs built into the simulator's vessel. first, these error analyses were performed by comparing the radial ai calculated from each radial pressure waveform. this is because the radial ai has a significantly high correlation with the central aortic ai, which is a very important indicator for predicting cardiovascular diseases such as atherosclerosis and vascular aging [cit] . next, these error analyses were also conducted by comparing the phase delay between the first peaks of the representative human pulse wave and the simulator's measured pulse data."
"however, our own experience, and from personal communication with other researchers and field technicians, a common set of methodological questions arise when using clhs. these questions are:"
"in order to be able to traverse the entire state space in this state instead of just choosing the action that maximizes the q value, during the dnn training, our action selection strategy π is defined as the -greedy method, represents the action exploration rate, and -greedy method indicates that the agent has a probability of 1− to choose the action a * that maximizes q (s, a), and has the probability of to randomly choose the action a. as the number of iterations increases, the value of gradually decreases, which accelerates the convergence of the q value."
"with the rapid increase in the research and development of wearable blood pressure sensors, the demand for securing the measurement accuracy of the wearable sensors has also been increased considerably. the accuracy of blood pressure measurement is critically important for the commercial use of such wearable sensors. in the case of hypertension, a 5-mmhg error in blood pressure measurements may double the number of patients diagnosed with hypertension, or even reduce it by half [cit] . despite the importance of measurement accuracy, few studies exist on the evaluation and improvement of wearable sensors' measurement accuracy. ideally, for such studies, clinical trials with large numbers of patients are the best way to examine the accuracy of wearable sensor measurements. however, often times, a large-scale human subject testing is limited due to high cost and time constraints. as an alternative to clinical testing, mechanical simulators capable of accurately regenerating standardized radial pulsation waveforms with a variety of different pulse features can be a good means of investigating and improving the measurement accuracy of wearable sensors."
"where e i is the distribution of an ancillary variable for the population (i is a histogram bin), and o i is the distribution of the same ancillary variable from a sample."
"as shown in fig. 10a, it was confirmed that the waveform measured by the rts on the wrist of the simulator and the waveform measured by the pressure sensor are in good agreement with the human representative waveform. this result implies that the proposed three-peak cam generating the pressure waveform in the simulator is designed to accurately regenerate the human pulse waveform. as shown in fig. 10b-d, similar trends were observed when the same comparisons were made by changing the heart rate and pulse rate. 10 comparison of pulse waveforms measured on a human's wrist using rts, on a simulator's wrist using rts and inside simulator's vessel (tube) using a pressure sensor with varying heart rate and pulse pressure: a case with heart rate of 65 bpm and pulse pressure of 50 mmhg, b case with heart rate of 65 bpm and pulse pressure of 60 mmhg, c case with heart rate of 75 bpm and pulse pressure of 50 mmhg, d case with heart rate of 75 bpm and pulse pressure of 60 mmhg figure 11a shows the error between the representative waveform of a human (fig. 4 ) and the pressure waveform measured by a pressure sensor inside the simulator's vessel in terms of the radial ai. here, since the error value of the radial ai is very small at less than 8.14e−3, it was confirmed that the radial ai values of both waveforms were matched well. on the other hand, in fig. 11b, the error value of the radial ai between the representative waveform of a human (fig. 4) and the waveform measured by the rts on the skin of the wrist of the simulator was about 4.85e−2, which is relatively large. the reason why the error value (waveform measured by the rts) is relatively large is that the proposed simulator generates the pressure waveform using air pressure instead of an incompressible liquid similar to blood. because the compressibility of air is different from that of blood, the pressure waveform measured by the tonometry method using the rts has a slightly different radial ai value from that of the human radial ai. although this error value is larger than the value in fig. 11a, this error value is small enough to conclude that the developed simulator can reproduce a pulse waveform similar to the human waveform while ensuring the value of the radial ai."
"possibly a much more visual way-yet also complementary to the ahels algorithm-to assess relatively adequate and under-sampled areas is to create a map that could show such patterns. we developed an algorithm called count of observations (coobs) that can achieve this objective. the coobs algorithm is implemented on a pixel basis and requires a stack of ancillary data pertaining to the environmental variables for a given spatial domain, and the associated legacy data points from the same spatial domain. the legacy points will have been intersected with ancillary data. we demonstrate this coobs algorithm using the hwcpid example. a schematic of the coobs algorithm is presented on fig. 4."
"where ω rsu and ω veh represent the energy consumed by the task of processing the unit data amount on the rsu and the vehicle i, respectively, while t 0 and t i represent the total time that the task is offloaded to the rsu and the vehicle i for processing, respectively. for n types of m vehicles covered by the rsu communication range, the rsu utility function is as follows"
"we hope that the system model can pay attention to long-term benefits rather than immediate benefits, which is reflected in the gamma value in equation (34) . a large gamma value indicates that there is a large part of the value from the next state in current q-value, which means that the model will consider more about the reward the next state will obtain. as shown in fig. 7, for a system model with a large gamma value, its performance can also be quickly improved."
"1b. calculate the data density of the ancillary data. for each element of the quantile matrix, tally the number of pixels within the bounds of the specified quantile for that matrix element. this number is divided by r, where r is the number of pixels of an ancillary variable."
"with the rapid development of the internet of vehicles(iov) technology and 5g communication technology, more and more functional technologies are applied to vehicles, such as augmented reality(ar), real-time video streaming, automatic driving(ad), etc. [cit] . in these applications, some of them need to transmit a large amount of data, others do not need to transmit a lot of data, however, there would be rigorous delay constraints to transmit them, so these applications have a relatively large demand for the iov resources, such as spectrum resources, storage space, etc. but local vehicle capability is limited and vehicle would be too hard to accomplish its task. therefore, cloud computing with strong computing capability has become a more efficient way to implement the associate editor coordinating the review of this manuscript and approving it for publication was amr tolba . these applications [cit] . the data that needs to be processed by these applications is transmitted to the cloud server for processing, and the cloud server sends the processing result to the vehicle end to complete the task processing [cit] ."
"conditioned latin hypercube sampling is one of the many environmental surveying tools available for understanding the spatial characteristics of environmental phenomena. [cit] . clhs has its origins in latin hypercube sampling (lhs) [cit] . lhs is an efficient way to reproduce an empirical distribution function, where the idea is to divide the empirical distribution function of a variable, x, into n equi-probable, non-overlapping strata, and then draw one random value from each stratum. in a multi-dimensional setting, for k variables, x 1,x 2, : : :,x k, the n random values drawn for variable x 1 are combined randomly (or in some order to maintain its correlation) with the n random values drawn for variable x 2, and so on until n k-tuples are formed, that is, the latin hypercube sample [cit], but they recognized that some generalization of lhs sampling was required so that selected samples actually existed in the real world. subsequently, they proposed a conditioning of the lhs, which is achieved by drawing an initial latin hypercube sample from the ancillary information, then using simulated annealing to permute the sample in such a way that an objective function is minimized. [cit] comprised three criteria:"
"the importance of monitoring artery-related factors such as arterial pressure waveform and pulse wave velocity has steadily increased in the medical science and healthcare fields [cit] . among the factors for health monitoring, the radial pressure waveform is a surrogate marker for estimating the central aortic pressure and predicting cardiovascular diseases [cit] . thus, in recent years, the need for radial artery monitoring sensors is rapidly increasing in order to measure radial pulsation waveforms, which can vary according to human race, sex, age, and health conditions, such as arterial stiffness [cit] . to effectively measure the radial artery pulse waveforms, there have been numerous research studies on flexible and wearable sensing technologies. these studies aimed at developing skin-attachable blood pressure sensors with superior sensing properties along with mechanical flexibility and robustness, thus enabling real-time blood pressure measurement or monitoring. recently, numerous nanomaterials including nanowires [cit], carbon nanotubes [cit], polymer nanofibers [cit], metal nanoparticles [cit], and graphene [cit] were tested in the design of wearable blood pressure sensors."
"brendan p. malone conceived and designed the experiments, performed the experiments, analyzed the data, contributed reagents/materials/analysis tools, prepared figures and/or tables, authored or reviewed drafts of the paper, approved the final draft. budiman minansy conceived and designed the experiments, analyzed the data, contributed reagents/materials/analysis tools, authored or reviewed drafts of the paper, approved the final draft. colby brungard authored or reviewed drafts of the paper, approved the final draft."
"we assume that the size of computation result data fed back from the vehicle j to the vehicle i is very small, so the transmission delay of the transmission processing result is negligible."
"where ω p k t k p k represents the energy consumed by wireless transmission. the greater the transmission power, the more the energy consumed by wireless transmission, and the smaller the corresponding reward value."
"1. create a sampling zone (i.e., buffer) from which an alternative site can be selected and extract all the ancillary data from inside this zone. we used a circled sampling zone of radius 500 m."
"6. using the same row and column position of the quantile matrix from 1a, select from the grid data all possible locations that meet the criteria of the quantile at that position in the matrix. select at random from this list of possible locations, m sampling locations."
"assuming that there are x vehicles with task offloading requirements within the coverage of the rsu communication range, the rsu utility function is as follows"
"however, long-distance data transmission may create some potential challenges, such as large data transmission delay cannot guarantee the quality of service (qos). extending the computation resources of the cloud computing to the mobile edge computing (mec) close to mobile users is proposed [cit] . the mec puts the cloud services to the radio access network (ran) and offers the cloud-computing capability in close proximity to mobile users [cit] ."
"in the dqn test process, we choose the strategy π as the greedy method, that is, the action a * that maximizes the q value is selected as the output and acts on the environment. the average performance of the feedback is used to measure the system performance level after iteration."
"more than a quarter century ago a review was done, which concluded that traditional lecture methods were not as effective as interactive discussion approaches in significantly improving retention, interest, and participation among students [cit] . for a while now there has been a shift in favor of incorporating more dynamic or active techniques categorized as active learning. active learning techniques are activities designed to get students involved in doing things and thinking about what they do [cit] ."
"the author's response to criticism about his \"recipe\" of sequence pair optimization being too rigid and mechanical to be practical is best understood with an analogy to culinary training. the critics should know that \"recipes\" are to be considered to be structured, sequenced steps that provide roadmaps to guide aspiring chefs. these established protocols have been tested and sequenced based on the individual ingredients and what result is desired. for instance, when preparing a dish that consists of a liquid ingredient and a finely ground starch ingredient, there is an enormous difference between adding the liquid to the starch and adding the starch to the fluid. one sequence with the pair of ingredients results in a clumpy consistency while the other leads to a smooth consistency, which is why knowing that one wants to achieve a certain result requires that a specific order be followed"
"the optimal sequence was born of the idea that, like ingredients for a recipe that are added in a specific order to yield optimal results, there must be an optimal order to introduce active learning techniques. more specifically, there must be an optimal order or sequence for different types of educational outcomes, various kinds of subjects, and different time constraints at the least. that notwithstanding, the key was to determine which natural biological process or design could be superimposed on the learning process successfully."
"as instructors who are tasked with putting structure in students, utilizing a recipe to provide a structured approach is the most efficient method convey information. with our hypothetical chef analogous to instructors who have at their disposal n number of actual evidence-based active learning techniques, given n techniques and an outcome in mind, they have n! many ways in which to sequence the techniques overall using n(n-1) sequence pairs to achieve their goal. the example used eight ingredients intentionally to draw a comparison to an existing binary system in common use. the same way zeros and ones are employed in groups of eight to encode an alphanumeric character, each node of any eight ingredient sequence will be yes-no, or zero-one, such that active learning sequences may be conceived of as strings of a combination of decisions incorporating anywhere from no techniques (i.e., all no's) to all eight (i.e., all yes's). utilizing a binary system of visualization, a row of eight nodes in numeric order left to right would indicate sequence pair one-four by yes under one and yes under four. to represent four-six, a yes would be in the node underneath four and six as well. when combined, nodes for one, four and six would result. to represent the inverted pair fourone, for instance, the nodes under one and four would be no and all others yes. in this manner, a pair and its inversion could be represented correctly. the reason that the author mentions how this may be done is to show how easily data obtained from experiments could be used to devise an algorithm for active learning decision-making programs to assist educators in choosing the most appropriate sequences for their desired purpose."
"undergraduates found that 45 percent of students show no significant improvement in the key measures of critical thinking, complex reasoning and writing by the end of their sophomore years [cit] . something definitely in need of revisiting, according to this statistic, is how the process of educating students occurs in the classroom."
"the author believes that characteristics of an optimal sequence of active-learning techniques should include that which minimizes the likelihood of students becoming stuck in a metaphoric \"reentrant loop.\" additional features that should be built into the optimal sequence is that, in the unfortunate event that students do become stuck having to \"reenter the loop,\" the optimal sequence should allow correction of weaknesses before students progress. incorporating features like employing demonstration [cit], think, pair, share [cit], and classroom assessment techniques [cit] will ensure that students will have the opportunity to comprehend."
"theoretical biomimetics led to the abstraction of the biological process, design, and therapeutics involved in svt to act as a framework for conceptualizing how one might best sequence evidence-based active learning techniques to optimize student learning, achievement, and educational outcomes. analogous to individual ingredients each of which has individual properties when combined in a variety of sequences that yields specific results, the author believes that the particular arrangement or sequence of steps implemented in an optimized instructional method will lead to different outcomes."
"the author was unable to pinpoint what triggered them due to seemingly unpredictable occurrences. he was to avoid anything that increased heart rate because the faster the rate, the increased likelihood and episode would occur. the author had to wear a holter monitor continuously for a period to be able to record an event before an official diagnosis was confirmed. eventually, while wearing the monitor 24hrs daily, the author had an episode that was recorded for the author's cardiologist who confirmed the diagnosis of supraventricular tachycardia."
"for the first demonstration of how theoretical biomimetics may be used in creative problem-solving, the author attempted to address the issue with student learning in higher education after reading a disturbing fact. research conducted on more than 2,300"
"many studies advocate active learning techniques or strategies individually. nevertheless, though there are lists that indicate the types of things educators are recommended to incorporate, the author found nothing close to an established protocol or flowchart detailing a sequence of steps in which to implement the techniques. therefore, the author set out to integrate some of the best techniques to include from the literature and figure out how an optimal sequence might be modeled."
"theoretical biomimetics: the new field nature does not operate in terms of right, or wrong, with respect to processes or design; man interprets them as such relative to subjective experience. we have studied things that are good or work well in nature and applied the underlying principles to the development of human-made physical processes and designs successfully with good reason. nevertheless, as much can be learned from how biological processes and designs that are deemed inadequate or ineffective operate."
"after much consideration to natural processes whose design could provide the scaffolding necessary to implement an optimal sequence, the author determined he would model the instructional and learning process after his personal experience with a type of cardiac arrhythmia known as supraventricular tachycardia (svt)."
"the approach that the author took was to conceptualize how to establish a sequence for implementing well-accepted, evidence-based active learning techniques to synergize the educational process. the goal was to devise a \"recipe\" for instructors to use in the classroom, which could provide guidance based on desired outcomes, materials taught, or time constraints. nevertheless, there will be some who oppose implementing such a thing, and the author has already expected naysayers to anticipatorily criticize this approach as being too mechanistic and rigid to be of any practical use to educators. the reader will be able to see that this could not be any more inaccurate a statement if they understand the what natural process inspired the idea."
"all components of an optimal sequence will be substantial. nevertheless, the analogous reentrant loop design overlaying steps six to eight are the heart of the sequence and continues indefinitely until the material is mastered by the student. each cycle of reentry reinforces learning the material such that subsequent cycles become less likely while permitting progression in the most efficient manner possible. this design will ultimately lead to the successful application of newly mastered material to address new problems by ensuring identified weaknesses are corrected before progressing."
"multidisciplinary journal for education, https://doi.org/10.4995/muse.2017 .7078 social and technological sciences issn: 2341 -2593 la the field of theoretical biomimetics as envisioned by the author allows individuals to become intellectual chefs of creative problem-solving. the examples from which to draw inspiration abound and their longevity are a testament to their brilliance. despite conceptually being in its infancy, there is so much potential for theoretical biomimetics to blossom into a full-fledged field with the capabilities to create and develop both humanmade intangibles directly, as well as human-made tangibles (i.e., biomimetics) indirectly."
"after the author had finished writing sections of this paper that he had not started, he began to format the completed sections he had not yet written. next, the preceding sections that followed were what motivated him to come up with the idea to start writing about theoretical biomimetics in the first place. if the reader finds something not quite right or is having difficulty comprehending the previous two sentences, then author's point has been made: sequence is everything. moreover, if those same two sentences made sense to the reader, the author's point has also been achieved. he or she was presented with a group of individual words serving as information inefficiently juxtaposed that the mind rearranged in the most appropriate sequence to result in the information being intelligible and useful. whether it is dna, coding for amino acids, speech, and conversation, or logic and mathematics, sequences determine economy, effectiveness, and efficiency. even with culinary arts like cooking and baking, creative arts like dance and drawing, construction, architecture, or simply daily activities like getting dressed and ready to work, there is not a single aspect of existence that has not been affected, inspired, or optimized by sequencing."
"the utilization of an algorithmic approach to incorporating active learning and evidencebased techniques will help instructors select optimal sequences of any length based on desired outcomes, time constraints, and material covered using available evidence-based data from individual pairings. contrary to what critics may believe, although particular \"recipes,\" or sequences, result in unique outcomes that can only be achieved with one order, there is no rigidity at all since sequence pairings are the units of which exact recipes are comprised and may be rearranged in nearly any order imaginable. in fact, sequencing is the opposite of rigidity; it provides flexibility as well as structure instead of guesswork and its associated unpredictability."
"as an example, for scientists to study the underlying biological principles of the \"pathological\" processes of metastasis in malignancies and apply them analogously to create, develop, or advance a human-made cascading system responsible for interception, encoding, encryption, and dissemination of information would be considered biomimetics since the resultant application would be tangible. the point that the author is making is that when biological processes and design work well using them as inspiration is a good idea. however, it is when these biological processes and designs are violated then studied, as in the case of malignancies, that we have the best opportunity to capitalize on the potential for the greatest breakthroughs. mother nature does not often have design flaws, and when she does, they are not many. to study and learn about how they affect her and how she handles them reciprocally the author feels would provide so many answers to problems that we are yet to encounter and is why \"conceptual\" problems should be explored with such abstract intellectual framework. it is with this approach in mind that the author employed theoretical biomimetics to conceive of an abstract yet creative approach to establishing a sequence for implementing evidence-based active learning [cit] techniques to synergize student learning and achievement in educational settings., https://doi.org/10.4995/muse.2017 .7078 social and technological sciences issn: 2341 -2593 la"
"when this occurs, the nerve impulse becomes trapped in this \"reentrant loop\" resulting in tachycardia and the associated symptoms that are experienced [cit] ). trying to \"break\" the episodes when the author first had them, the author noticed that it seemed as though the longer the author had an episode, the easier it was to break it. there was something about spending more time in the reentrant loop that the author believed made escaping or \"breaking\" it easier. the author had success utilizing the valsalva maneuver as needed with a daily regimen of beta-blockers. a valsalva maneuver is one method to break the svt episode that involves trying to forcibly expel air while bearing down against a closed glottis to slow the heart rate and break the reentrant loop [cit] . beta-blockers are a useful class of drug routinely used to prevent the onset of the reentry phenomenon that occurs with svt by keeping the heart rate low by depressing nodal automaticity and inhibiting function [cit] ."
"although the author cannot be credited with the concept of using biological design or processing approaches to creating human-made tangibles also known as biomimesis, the author is contributing the idea of using biomimetic principles as an approach to creating, developing or advancing human-made intangibles such as theories. researchers have studied biological processes and design in order to understand them and explain, which we know as science. many have certainly taken science a step further by exploring natural processes and designs and applied principles to create human-made things or tangibles, which is known as biomimetics. the author has taken biomimetics one step further by implementing the principles of biological processes and designs to create, develop, or advance human-made intangibles."
"the author has not come across anything in the literature. whether it will come to be known as a branch of biomimetics, or be to biomimetics what biomimetics is to science is yet to be known. nonetheless, the author describes and defines his contribution as the"
"it must be stressed that the author is not advocating mechanical rigidity as some may contend. active learning does require some flexibility, and the author has conceptually accounted for that in the process of establishing a sequence. one must remember that all that is minimally necessary for a sequence are two techniques where one must precede the other. in this fashion, the overall optimal sequence of techniques will be determined by combining the results from a series of individual sequence experiments performed on pairs of techniques at a time. conducting the experiments this way not only will allow an overall optimal sequence, but technique-pair results individually can be organized such that algorithms may be formulated to suggest sequence choices for which techniques work best when immediately following others. ultimately, these flow diagrams for use by both educators and students alike will facilitate instruction and study while simultaneously providing the sort of flexibility required to deal with ever changing classroom dynamics."
"in this fashion, theoretical biomimetics not only possesses a \"meta-\" aspect that will result in theory about theorizing and concepts on how to conceptualize, but the author conceived"
"trinity,\" and (10) the demonstration-deconstruction of material to allow students to comprehend and engage themselves in doing serves a dual purpose: to display material knowledge for assessment while building confidence, and to reinforce understanding of the material, which also builds confidence."
"gives students a chance to allow for teacher feedback, modification, and refinement before misconceptions are set in. this step displays the ability students have to teach one another (paul) and collaborate with each other about what they have learned [cit] [steps 6 thru 8 are the reentrant loop that gets repeated and students only move to the next step after full comprehension, and students' total commitment is positively reinforced by comprehension resulting in a continued effort to thoroughly master material], (9) the educator tests for students' mastery, which they may readily exhibit by applying what they have newly mastered to address novel problems. mastery is defined by knowing what to do, how to do it, and why it is done the author coined \"the interrogative"
"important to realize, however, is that a recipe containing five ingredients consists of units composed of four sequenced ingredient pairings. eventually, through understanding why the ingredient pairs in a recipe are sequenced a certain way the novice chef will transition from the apparent rigidity of an exact recipe to the next level of improvisational cooking."
"the suggested sequence of techniques composed of sequence pairings serves only as an example and has not been validated experimentally yet. once data is obtained, it may be found that group collaboration yields the best results when following demonstrations as a sequence pair, in which case the sequence above can be further optimized by rearranging the techniques to reflect this. the sequencing process is dynamic and as different variables are discovered needs to be flexible enough to accommodate such newly discovered information. the author's concept is structured, yet flexible enough to meet such demands."
"man are modeled with nature in mind. it is no surprise why biomimesis [cit], or mimicry of natural processes and designs to create structures and materials (tangibles), has been recognized and widely adopted as both an effective and efficient approach to human-made things [cit] . with this in mind, the author wondered with the completely logical application to vehicles, buildings, aircraft, and other human-made tangibles, \"why couldn't biomimetic principles be applied to the creation of human-made intangibles with as much effectiveness?\" from this, natural and biological phenomena serving as the basis for conceptual problem-solving was born."
"although there exist varieties, svt may be explained as a reentrant loop in which nerve impulses occasionally in the heart are conducted through accessory pathways at different rates allows the faster one to travel back up on the path used by the slower one instead of continuing in the original direction [cit] . the author has had this condition his entire life. those unmistakable rapid heartbeats were more annoying and frightening than anything. what made having an episode or svt event so frustrating, yet intriguing, was how it occurred."
"the steps involved in the optimal sequence that the author suggests are as follows: (1) the teacher administers a diagnostic assessment to determine individual and class baseline levels with respect to the material to be covered in the course, (2) the teacher must assume students are capable; initially assuming the opposite in the absence of proof is selfdefeating and self-fulfilling, (3) based on the assumption they are capable, he or she establishes high expectations for the students, (4) the teacher must be sure that expectations are clearly communicated (teaching) and incorporate students teaching into course goals, (5) the high expectations imply confidence in students who then feel obligated to meet these expectations and fully commit to the educational experience, (6) having obtained student commitment, the educator begins instruction with ice-breaker activity that ties into the topic followed with an optimized sequence pair of demonstration [cit] and deconstruction [cit], or any other appropriate pairing of active learning techniques determine using an algorithm for the allotted time, material or educational outcome desired, (7) students are encouraged to ask questions to solidify understanding of the material immediately afterward making the process interactive or dynamic. the instructor will reciprocate with purposeful questions appropriate to ability level, (8) students may then have an opportunity to demonstrate they understand, which"
"of it after what inspired its development yet it will be the inspiration for the creation of that which the idea that inspired its own creation was supposed to do thereby surpassing it! it is hoped that the author's ideas and article have inspired the reader. if there is one thing that the reader should take from this paper, it should be to understand the importance of sequence in that coming after does not preclude the possibility to take one further than that which has come before."
"the author believes that as important as individual active learning evidence-based techniques may be, without established sequences in which to implement them they have nothing more to offer. unfortunately, their full potential to collectively become greater than the sum of individual parts will never be realized. theoretical biomimetics aims to prevent that from occurring and is a prime example of how concept abstraction and application works."
"previously, [cit] introduced acmi (automated crystallographic map interpretation), a three-phase, probabilistic method for determining protein structures from electron-density maps (see figure 3) . empirical results show that acmi outperforms other methods in the field on difficult protein structures, producing complete and physically feasible protein structures where other methods fail [cit] . acmi models the probability of all possible configurations of a protein structure (i.e., full-joint probability of each amino acid's location) using a pairwise markov random field (mrf) [cit] a type of undirected graphical model where vertices represent random variables and edges imply dependencies between these variables. acmi's mrf combines visual evidence of protein fragments from the electron-density map with biochemical constraints in order to effectively identify the most probable locations for each amino acid in the electron-density map. unfortunately, exact inference of the optimal configuration is computationally infeasible in this model. to overcome this, acmi employs belief propagation (bp) [cit] to produce marginal probability distributions for the three-dimensional location of each amino acid in the electron-density map."
"this is problematic for acmi-pf. to find a good solution, acmi-pf's sequential sampling must adequately explore the conformation space of an amino acid. with limited samples, this requires a restricted space of non-negligible locations to search, which is what bp and dobp provide. rbp, however, contains more locations of non-negligible probability than acmi-pf can sample in an efficient manner, causing the algorithm to fail. in fact, across all ten proteins, nine failed to produce any portion of the protein structure when using rbp marginals, and the tenth only extended 5% of the total protein. the results in figure 7 and a histogram of the distributions reflect that rbp excelled at preventing the true solution from having neglible probability (i.e., a rank of 1) and thus looked better on average. rbp, however, did not eliminate enough portions of the density map from consideration for acmi-pf to succeed. this explains why the rank was much better for rbp, but the log-likelihoods were slightly better for dobp."
"this convolution occurs over the entire distribution, denoted edm (for electron-density map). the denominator inside the integral removes the influence of the previous message sent across the edge. in essence, a message from vertex i to j is amino acid i stating, \"based on my current belief, i would expect you to be located (with probability) here.\""
"calculation methods that provide the composition of multi-ingredient foods are different (infoods method, british method, yield factor method, retention factor method, method used in epic-european prospective investigation into cancer and nutrition, usda method, etc. [cit] in major part of cases based on the nutrient contents of the ingredients (values taken from national fcdbs or imputed from other databases), the amount of each single ingredient in the recipe, information on the cooking techniques (time, temperature, utensils, etc.) and the sensitivity of foods and nutrients to thermal treatments."
"acmi-pf (sample all-atom structure) a priori probability of cα i 's location marginal probability of cα i protein model 's location figure 3 : the three-phase acmi pipeline. given an electron-density map and primary amino-acid sequence, acmi-sh performs a local-match search independently for each amino acid. the resulting priors are fed to the second phase, acmi-bp, which applies global constraints to create posterior marginal probabilities of each amino acid's location. finally, acmi-pf uses these marginals to sample physically feasible, all-atom protein structures."
"the model in equation 1 represents the full-joint probability distribution over all possible configurations (location and orientation) for all residues in the target protein. calculating this probability exactly, however, is intractable in large graphs with loops. acmi, instead, employs loopy belief propagation [cit], a fast approximate-inference algorithm, to calculate an approximate marginal probability distribution for the location of each amino acid's cα atom. this algorithm, acmi-bp, is the second phase of acmi and is the crux of this paper. we explore acmi-bp in section 3."
"when the stress placed on the body is due to illness, the tendency for the body to accumulate lactate is prolonged, perhaps resulting in lactic acidosis. it is therefore commonplace in contemporary medicine for lactate to be used as a means to evaluate the severity of acute illness, diagnose disease states, predict mortality, and assess response to resuscitation [cit] . furthermore, in sport, lactate is one of the most often measured parameters when performance testing athletes and prescribing exercise intensities [cit] ."
"this section of the paper describes the sensor used during the work, the testing regime employed using cyclists to test the sensor response to lactate levels, and detail regarding placement of the sensor itself on participants."
"1) begin with a rest period after fitment of sensors and other preparation for 5 minutes to enable stabilisation of a baseline lactate level; 2) warmup for a period of 5 minutes at 80 w, encouraging participants to maintain a constant cadence (approx. 70-80 rpm) throughout; 3) increment resistance every 2 minutes by 20 w, maintaining similar cadence, and maintain resistance increment regime until cyclist cadence falls below 60 rpm, indicating exhaustion. 4) conclude with a 10-minute rest period to observe falling lactate post-exercise. throughout this test regime, measurements were taken with various devices as follows: 1) em wave sensor measurements, comprising an s 11 and s 21 spectra, every 30 seconds (see sections iv-b and iv-c for detail of the sensor and placement). 2) blood lactate measurements using a lactate pro v2 electrochemical analyser, drawing blood samples from the tip of a finger on the left hand. in respect of the test regime, measurements were taken at the beginning and end of phase 1, the end of phase 2, every minute during phase 3, and then every 2 minutes during phase 4. this device was chosen not only due to its use in research work noted by other authors, but also due to it being one of the only such devices with medical approval. 3) temperature measurements using a thermocouple taped to the arm and leg of participants. 4) heart rate via a polar v800 chest strap and watch combination. all data was date and time stamped so that it could be retrospectively synchronised for comparison and analysis. [cit], with 34 participants being recruited for the trial. the majority of the participants were male and aged between 25 and 40; 20% of the test subjects were female. there is no significant difference noted in expected blood lactate levels in these groups [cit] . in total, from all participants, 367 lactate measurements were taken using the lactate pro v2 device, which acted as the reference method in this study."
"given an electron-density map and the linear amino-acid sequence of a protein, acmi builds a pairwise markov-field model (mrf) [cit] to model the location of each amino acid's cα atom. a pairwise markov field, a type of undirected graphical model, defines a probability distribution on a graph. vertices (or nodes) are associated with random variables, and edges enforce pairwise constraints on those variables. mrf's are a compact representation of a full-joint probability, allowing for the joint probability function to factorize into smaller functions which can then undergo inference. in acmi, each vertex corresponds to an amino acid i, and random variables describe the location, u i, of each cα i . figure 4 shows the mrf associated with an example protein."
"-k. huang and l. zhang, &quot;cardiology knowledge free ecg feature extraction using generalized tensor rank one discriminant analysis,&quot; eurasip journal on advances in signal processing, vol. 2014 processing, vol., pp. 1-15, 2014 -j. l. garvey, &quot;ecg techniques and technologies,&quot; emergency medicine clinics of north america, vol. 24, pp. 209-225, 2006 medicine and biology magazine, ieee, vol. 20, pp. 45-50, 2001 ."
"as seen in fig. 5 and in prior studies, the standard approach involving measurement of oct psf based on a specular surface is simple and provides useful quantitative and qualitative information on spatial resolution. while the ctf-based approach introduced here involves a more complicated process for phantom development and data analysis, it also provides direct visual access to unique details related to spatial resolution, and concomitant image quality, not apparent from the psf. the effects of critical system and sample parameters like signal-tonoise ratio and bulk scattering are present in the recorded signal from which contrast is determined. differences between oct system configurations under particular imaging conditions, such as low spatial frequency content, are readily observed and quantified. when a simple test is needed, such as for quality assurance in a clinical setting, even a single spatial frequency could suffice to provide a qualitative visual assessment or \"pass/fail\" criterion. such a test can also readily incorporate the effects of logarithmic intensity transformation and display scaling of the oct device."
"chemical analysis of foods, recipes, manufactured foods etc. by producing new experimental values with other national collection of composition data, also for traditional and certified foods, must be carried out to increase also the total quality of food composition databases."
recipe calculations must be carried out by persons with basic knowledge of food chemistry and appropriate skills for considering the complex changes occurring during processing of foods.
"there are a variety of improvements in thin-film fabrication which may benefit phantom development for optical imaging. we plan to investigate enhanced fabrication techniques, such as a modified pdms formulation [cit] and coating procedure [cit], to realize thinner layers (~2 μm) with suitable thickness precision and control of optical properties. such phantoms are needed to fully challenge devices with high axial resolution. additionally, increased consistency in executing particular steps in the fabrication process can offer improvement of thickness precision and homogeneity of scatterer distribution, both of which would reduce variability in contrast measurements. the current study utilized separate phantoms for each spatial frequency as a proof-of-principle for the fabrication and ctf analysis; however, it would be straightforward to create a single phantom with all the spatial frequencies by simply taking a small piece from each phantom and affixing them onto a single substrate, with each spatial frequency labeled for ease of use."
"in a clinical environment blood gas analysis has become an integral part of patient monitoring, particularly in the case of acute illness (i.e., in emergency wards or intensive care units), with clinical staff relying upon inclusion of blood gas analysers (bgas) to assist in diagnostic workups and development of treatment plans [cit] . a bga, such as that shown in fig. 1(a), can directly measure ph, partial pressure of oxygen (po 2 ) and carbon dioxide (pco 2 ), a variety of electrolytes, and various metabolites including glucose, lactate, blood urea nitrogen, and creatinine [cit] . compared with laboratory analysis, a bga offers rapid measurement time (approx. 1 minute, excluding sampling and transit times) and a wealth of information upon which assessment of patient condition can be made. it is no surprise therefore that the bga has become the gold standard against which clinicians compare emerging point of care technologies."
"recently, this issue has drawn attention in italy and has led to a reviewed sensitivity in europe towards the implementation and standardization of the procedures for producing new and reliable data for composite dishes [cit] ."
"one avenue of future work is to apply a similar domain knowledge function to acmi-pf, which utilizes particle filtering -an approximate inference algorithm in which each iteration also requires a choice of what amino acid to next sample in the electron-density map. in addition, we would like to investigate the use of domain-specific heuristics to guide loopy belief propagation when applied to other tasks, particularly other large-state space problems in the computer vision field. many of these tasks could benefit from our domain-knowledge message-passing protocol, where rule-ofthumb heuristics can be encoded into a priority function to guide bp."
"as shown in algorithm 1, acmi-bp utilizes an asynchronous message-passing protocol. in particular, nodes are treated in a simple round-robin fashion where a pass begins at one end of the protein's primary sequence and works toward the other. at each step along the way, amino acid i first updates its belief based on equation 2 and then updates its outgoing messages using equation 3; this sequence is then repeated by amino acid i + 1, and so on. once all amino acids are processed, the next pass works in the reverse direction. one disadvantage to this protocol is that it does not prioritize"
"current off-the-shelf point of care (poc) technologies (further detailed in section ii) necessitate a blood sample. while steps have been taken to speed up the process of measurement and analysis, the requirement of extracting blood is still considered a major inconvenience. in a hospital environment, this carries significant infection control risks, and the frequency of sampling is rarely sufficient for clinicians to understand whether intervention is necessary. leading clinicians at alder hey childrens hospital (liverpool, uk) suggest that even if patient blood is sampled and measured 4-6 times per day, as may be the case in intensive care environments, this does not readily enable one to understand if the lactate level is rising (i.e., worsening condition) or falling (i.e., recovery). furthermore, in cases where the patient is an infant, the amount of blood available is small and so extraction of even 1-3 ml of blood represents a significant percentage of overall blood volume if sampling frequently and leads to considerable stress to the patient."
"bgas offer a broad range of measurements, but a number of devices have been released to the market that offer single metabolite measurement. these are typically based on an electrochemical principle, using an electrochemically sensitised strip which, when exposed to blood, changes its electrical properties. when inserted into a device designed to interface with these strips, users are able to obtain a lactate reading within 15-60 seconds. while these devices still require blood to be extracted from a subject, the volume requirement is significantly lower than a bga-for example the lactate pro v2 lt-1730 system (see fig. 2 ) used regularly by the authors requires only 5 μl of blood."
"this research has been supported by nlm grant r01-lm008796 and nlm training grant t15-lm007359. in addition, support for our collaborators at the university of wisconsin center for eukaryotic structural genomics (cesg) has been provided by nih protein structure initiative grant gm074901. the acmi system is a collaboration of this paper's authors and frank dimaio, george n. phillips, jr., and members of the phillips laboratory at the university of wisconsinmadison. acmi and the data set of experimentally-phased density maps is available online at http://pages.cs.wisc.edu/∼dimaio/acmi/get acmi.htm."
"while implementations vary, this section introduces the general outline and notation for belief propagation in acmi-bp. at each iteration, a vertex computes an estimate of its marginal probability distribution as a product over all associated potential functions, marginalizing out other random variables. the vertex then calculates outgoing messages to each of its connected neighbors by combining its marginal probability estimate with the edge potential function shared with that particular neighbor. acmi-bp, at iteration n for each vertex (i.e., amino acid) i, computes an estimate,p n i ( ui), of amino acid i's marginal distribution (or belief ) over locations in the unit cell by combining its local probability and incoming messages:"
"for nutrition labeling, which follows precise procedures but also permits greater tolerance of the nutritional declared values, the calculated data should be considered reasonable and of great utility to the food business operators (regulation (eu) no. 1169/2011)."
"for this reason, studies aimed at comparing calculated data and data obtained from chemical analyses are needed to optimize and asses the correction factors (retention factors and yield factors)."
"the last step in the crystallographic process is interpreting the electron-density map, whereby a crystallographer fits a protein molecular model to the density map. this phase is alternatively referred to as tracing the protein. a crystallographer's task is: given the sequence of the protein and an electron-density map, trace the chain of amino acids through the 3d map. this end goal is the same as in automated ab initio protein-structure prediction, with the difference being that a crystallographer also possesses a fuzzy image of the protein structure. figure 2 shows a sample density map and the resulting interpretation. figure 2a) is a contoured electron-density map, similar to what a crystallographer would see at the beginning of interpretation. in b), we see the resulting protein structure with all nonhydrogen atoms in a stick representation. the main chain of atoms is known as the backbone of the protein and the variably sized groups hanging off of the backbone are called side chains. amino acids (or residues) form the building blocks of proteins, linking end-to-end to form the backbone. each amino-acid type has a unique side chain molecule, but all connect to the backbone via the cα atom -the central atom in an amino acid's structure."
"therefore, our present work focuses on fabrication, characterization, and testing of multilayer thin-film phantoms specifically for determining oct axial resolution characteristics via the ctf. this type of resolution evaluation is recommended in performance standards for medical imaging modalities like ct [cit] . when using a multilayer phantom for oct, images can be captured in the native format without oversampling, and no knowledge about the system's spatial calibration is required. with a collection of phantoms covering a range of oct-relevant spatial frequencies, a ctf curve can be generated, providing a more thorough analysis of system performance than previously possible."
"for athletes, the issue of blood volume is less challenging since they are typically adult and in a good state of health. however, blood sampling is still cumbersome in sport since athletes typically have to reduce exercise intensity (or stop altogether) to provide a measurement which prohibits continuous high resolution monitoring during exercise."
"an important design decision for belief propagation is to define a protocol for ordering the passing and receiving of messages. in exact inference, the ordering of messages only affects the rate of convergence, not the final solution. in graph models with loops, however, the message-passing protocol can affect the speed and accuracy of inference [cit] . message-passing protocols fall into two categories: synchronous or asynchronous. synchronous message passing is where all outgoing messages are calculated at the same time followed by a synchronous reception of messages by all nodes in the graph. asynchronous message passing, instead, prioritizes messages, sending and updating one at a time. [cit] showed that asynchronous message passing demonstrates faster convergence tendencies and lower error bounds."
"we have fabricated and characterized novel multilayer thin-film phantoms for systemindependent assessment of oct axial resolution characteristics, thus enabling the determination of the axial ctf, a fundamental figure of merit of imaging systems. the resolution capabilities of an oct device can be readily visualized and quantitatively characterized with these phantoms. the availability of stable, well-characterized layered phantoms for axial oct resolution evaluation can offer device developers and users an effective means to critically assess and compare one of the most important aspects of oct device performance."
"a testing regime was designed to enable the development of a lactate profile in participants. the regime was based on the use of a lode excalibur sport ergometer, which enables increase in pedal resistance up to 1500 w. the protocol adhered to was phased as follows:"
"the sensor is manufactured via a standard etching process, and the substrate is fr4 epoxy glass coated with a biocompatible mask that helps to prevent leeching of the copper conductor when worn by test subjects. the sma connector contacts, shown as exposed in fig. 4 (a) were also masked with insulating tape when in use to prevent direct conductor contact with the skin. the rear of the sensor has a discontinuous ground plane that isolates ports 1 from port 2, as pictured in fig. 4(b) . this is to enable resonance of the device, while also ensuring that the generated em energy is directed toward the test material and providing the hairpin pattern with shielding from outside sources of interference."
"nowadays, a lot of countries have their own national food composition databases, whose updating should include a large amount of food, by reflecting the main nationally consumed foods and food habits of the population. the level of certain nutrients in some foods available worldwide varies greatly between countries, because of differing cultivars, soils, climates, agricultural practices, processing procedures and analytical methods, as well as many composite dishes with the same name show different characteristics between countries. this is the main reason why data from other countries cannot be used in all situations and why it is essential to develop a national food composition database."
"the measured thicknesses of the key ctf layers are presented in table 1 . the thinnest layer we achieved was 8 μm, which may be too thick to challenge ultrahigh resolution oct systems but is near the resolution limit of many commercially-available clinical devices. the thickness precision is reasonable, with all standard deviations ~10% or less of the mean. variations in thickness are likely due to uncontrolled steps in the fabrication process, such as the amount of uncured pdms placed on the substrate for spin coating. to get a more detailed understanding of the thickness variation across a phantom, we used the stylus profilometer to also obtain a high resolution 2d thickness profile over the entire phantom surface, shown in fig. 3 . the image consists of horizontal scans, with 2.1 μm sampling within each scan and 100 μm between scans. in general, the spin coating process tends to produce films which are thinner and relatively flat in the central region, with thicker beads near the substrate edges. some point defects are noticeable, due to dust particles on the surface and local aggregation of baso 4 particles within the phantom. but the phantom is quite uniform within a 10 mm x 10 mm area near the cut edge, thereby defining the optimum region for oct imaging."
"edge potential functions, denoted ψi,j( ui, uj), represent one of two conformation potentials which define global constraints on the protein structure. edges between neighboring amino acids in the linear sequence are represented by the adjacency potential function, which encodes the restraint that adjacent residues must maintain an approximate 3.8å spacing as well as proper angles. since amino acids distant in the linear sequence can still neighbor each other in the threedimensional conformation, edges between non-neighboring amino acids contain an occupancy potential. this function reflects the chemical constraint that no two residues can occupy the same space. acmi, in practice, uses an aggregator function to collect and disseminate occupancy messages efficiently [cit] ."
"therefore, the authors have proposed the use of an electromagnetic (em) wave sensor system, operating at microwave frequencies, to provide a chemical-free sensor for real-time monitoring of athletes. although the main aim of the authors has been to develop the system for medical use, it is clear also that the technology has relevance to sport science, namely the monitoring of athletes to ensure applicability of training regimes, as well as to assist in their prescription."
"a marginal probability represents the posterior probability of a single variable. the terms arises from calculating the full-joint probability and then summing out (or marginalizing) all other variables. in tree-structured graphs, this inference is exact and efficient. in cyclical graphs, such as the model for acmi, convergence to the exact solution is not guaranteed. in practice, belief propagation in graphs with cycles (loopy belief propagation) tends to produce good approximations, particularly under certain conditions [cit] . this section first introduces belief propagation, and the particular notation used for acmi. section 3.2 introduces the topic of message scheduling and discusses its impact on the quality of bp's approximate marginal probabilities. sections 3.3 and 3.4 discuss informed message-passing protocols for producing improved marginals, including our new domain-knowledge based priority function."
"currently, the fcdbs include different typologies of foods: simple, processed, traditional or \"innovative\", certified and local foods, as well as those originating from other countries that have become of more common use; the availability of data depends mainly on economic resources."
"the options for non-invasive lactate monitoring remain limited for practitioners in either healthcare or sports, and perhaps the best example to reach the market is the bsx insight lactate prediction system (see fig. 3 ). this is a validated [cit] wearable system to predict lactate threshold, the point at which the concentration of blood lactate begins to exponentially increase during exercise. this system uses near infrared (nir) sensors to monitor oxygenation in the gastrocnemius muscle and, via a patented algorithm, detects inflection points in the muscle oxygenation curve at increased workloads."
"this need has been carried out in several projects and led to the development of several co-operations including infoods-international network of food data system, cost action 99, \"project committee-food composition data\" (cen/tc 387), european food information resource (eurofir) network of excellence. the latter projects, at european level, have produced a consensus document on standardized procedures for production and compilation of fcdbs and developed a european food data platform: foodexplorer tm tool (http://www.eurofir.org/foodexplorer/login1. php) [cit] . the eurofir activity is currently being supported by the non-profit association eurofir aisbl and now it offers a set of tools for the exchange and information of validated and standardized data: food explorer, food basket, ebasis and plantlibra database [cit] ."
"for a transparent film, we mixed the base and curing agent thoroughly with a glass rod, and then placed the mixture in a degassing vacuum chamber for 20 minutes to remove air pockets within the sample prior to spin coating and curing. the process for a scattering film involved the addition of barium sulfate powder (baso 4, b-3758, sigma-aldrich st. louis, mo) at 10% by weight to the pdms base prior to mixing with the curing agent. the powder does not have a controlled particle size which has been characterized by the manufacturer, but the polydisperse particles are nominally micron-sized, as determined by comparison to 0.75 μm diameter polystyrene microspheres with brightfield microscopy. to maximize homogeneity, the particles were dispersed in the pdms through 3 minutes of sonication with a probe tip and 10 minutes in a sonicator bath. then, the sample was placed in the vacuum chamber to remove air pockets. the sonication and degassing steps were repeated 8-12 times to provide a suitable level of homogeneity, verified visually with brightfield microscopy. the stock pdms-baso 4 mixture was diluted with an equal quantity of additional pdms base (by weight) to achieve a mixture with 5% baso 4 concentration by weight. the diluted sample and pdms curing agent were then mixed in a 10:1 ratio, followed by a single round of probe and bath sonication and a final degassing step in the vacuum chamber prior to spin coating and curing."
"the various calculation methodologies can be corrected for preparation factors: loss or gain in weight, usually considered as yields and nutrient changes, usually considered as retention factors [21, 22, [cit], providing therefore \"more or less representative estimates\" of the chemical-nutritional characteristics of the recipe under consideration."
"to date, the need for more nutritional data on cooked foods and composite dishes in fcdbs has emerged, beside a marked gap in their composition information. at the time, there are still too few studies in literature that take into account the complex food formulations (ingredients, preparations, cooking method) on nutritional properties of composite dishes [cit] ."
"each sensor was connected to a separate rohde and schwarz zvl13 vector network analyser (vna), and s 11 and s 21 measurements were recorded every 30 seconds via a bespoke labview interface. the equipment was configured to capture data between 10 mhz and 4 ghz, with 4000 discrete data points recorded. the equipment was set to output 0 dbm (1 mw) power. the system configuration was selected based upon discussion with the project partners, as well as knowledge obtained in prior published (e.g., [cit] and unpublished work. previous work suggested that lactate and similar metabolites were quantifiable within this selected frequency range, although some uncertainty of the precise response frequency was present due previous work being ex-situ. from a commercial perspective, it was desirable to have an upper limit of 4 ghz to limit unit cost and size of a future \"all-in-one\" wearable solution."
"(1) assessing the dietary intake of individuals or groups, carrying out epidemiological studies and clinical research; (2) formulating diets at the individual and/or population level; (3) formulating agri-food policy in relation to public health and educational-information and (4) supporting industrial and handicraft companies for their products' nutrition labeling [1, [cit] ."
"the increasing consumer's attention on knowing about what we eat and the mandatory of nutrition labeling by the industry, has increased the need to produce more complete composition data [cit] . on the other hand, the poor availability of data for specific food formulations, often on the market for a short time, clearly indicates the need to study alternative methods to analytical methods to know their nutritional properties."
"this paper describes the use of a microwave-based sensor for the measurement of lactate non-invasively by simple application to the skin of a subject. the authors have worked in this area for some years [cit] mainly considering in-vitro measurement of lactate and the varying types of microwave-based sensor design depending on specific applications. this work takes a considerable step forward, and shows the potential of in-vivo application of the sensor technology with human participants in a controlled environment."
"cult maps. first, acmi simultaneously ties local density information and global constraints to infer possible locations of residues. second, rather than represent each residue as one or a set of possible locations in the map, acmi represents each residue's location as a distribution over the entire electron-density map. this allows the algorithm to overcome poor, early decisions while also allowing weaker evidence to persist and possibly be utilized in later stages."
"provide and assess specific correction factors, considering the times and appropriate ways of cooking, to ensure a reliable calculation of the composition of complex dishes."
"in addition, reliability of recipe calculations depends on several factors i.e., the presence of data missing, the lack of specific retention factors, the use of national food composition databases [cit] . the final result should always be carefully evaluated and considered more or less reliable depending on its use. for instance, with the regulation (eu) no. 1169/2011 [cit] for food labeling, the use of calculation procedure to formulate the nutritional labels of processed products, was consented as possible alternative to direct chemical analysis. in fact, the mandatory allows a discrete margin of tolerance of the values."
"for each of the tests in section 4, all methods use the same acmi pipeline with the only differences coming in the second phase. first, acmi-sh is run for each map in the test set"
"nodes based on any metric of evidence or information gain. even in the best case, this leads to a waste of resources on passing low-information messages along the chain. a more worrying problem arises when the ordering of nodes gives high priority to nodes with poor prior information -that is, false positives in the match search from acmi-sh. the rest of this section explores alternative scheduling approaches which attempt to identify important messages during the inference process."
"where γ(i) is the set of neighbors for node i. at each step of message passing, the node with highest priority is popped off the queue and all of its messages are sent out. this node's residual value is set to 0, while all neighboring nodes update their beliefs and messages as well as their residual factors if necessary. further work by sutton and mccallum [cit] approximated these residuals to eliminate unnecessary calculations of new messages that were never sent."
"after acmi-bp produces a set of marginal probabilities, acmi-pf is run to sample physically-feasible protein structures. we compare the accuracy of these protein structures in both completeness and accuracy of the final model for each test-set protein. we compare how acmi-pf performs with dobp produced marginals relative to bp produced marginals. as mentioned, rbp did not produce the sharp distributions needed to sample protein structures and thus the results for rbp are not shown below. figure 8 shows the results of our experiments, with the original acmi protocol being shown on the x-axis (bp) and the method using domain knowledge for guidance as in algorithm 2 on the y-axis (dobp). each point in the plot refers to one of the test-set proteins. figure 8a) shows the percent of the predicted protein structure correctly identified. this is akin to the precision of the predicted structures. precision is a measure of fidelity -that is, of all predictions by algorithm, how many are actually valid? here, we are describing the percentage of residues predicted that were within 2å of their corresponding true solution location. conversely, figure 8b) shows the completeness of the predictions. these are akin to recall -of all possible positive results, how many did the algorithm actually return? for this experiment, we are measuring the percent of residues available in the true (i.e, pdb) solution that were accurately predicted (within 2å). anything above the diagonal indicates dobp produced better structures. in general, dobp produced more complete and correct protein structures, particularly in the hardest maps. dobp did worse on 2 proteins in terms of recall (completeness) and once in terms of precision (correctness). the underperformance in correctness occurs on a structure acmi was already doing well; in fact, most of the proteins with high correctness did not change one way or the other based on the different marginals. of the three hardest proteins, however, the correctness was dramatically higher when using dobp, and in two of these the completeness also improved."
"section 2 provides background information on interpreting low-resolution electron-density maps, including an overview of acmi and other methods for automating this task. section 3 explores belief propagation and discusses protocols for guiding message-passing during the inference phase of acmi. this section also presents our new method of using domain knowledge to guide bp. lastly, in section 4, we compare our (a) (b) figure 2 : the last step in the protein x-ray crystallography pipeline takes a) the electron-density map (a 3d image) of the protein and finds b) the most likely protein structure that explains the map. here, the electron density is contoured and the chemical structure of the protein designated with a stick model showing all of the non-hydrogen atoms."
"where γ(i) is the set of vertices connected to vertex i. figure 5 shows a sample message being sent between two amino acids in acmi, and the resulting update to the receiving node's belief. messages from amino acid i to amino acid j are calculated by convoluting the edge potential ψ i,j ( u i, u j ) (i.e., adjacency potential or occupancy potential) with amino acid i's belief"
"as described in section 2.3, acmi-bp produces a marginal probability distribution for each amino acid, describing the probability of that amino acid's location at each point in the electron-density map. figure 6 shows the log-likelihood of the true solution for each message protocol's results. this is the probability for the true (i.e., manually traced, pdb deposited) (x,y,z) coordinates for each residue, according to the acmi-bp produced marginals. the higher this value, the more likely a final trace will place the residue in its correct location. each point in this figure represents one protein structure, and is an average of log-likelihoods over all residues in that structure. figure 6a ) compares bp to dobp, with the diagonal line designating equal performance. all points above the line represent maps where dobp produced higher average-log-likelihoods than bp. in all but two maps, dobp improved the accuracy of acmi-bp's marginal probabilities. in figure 6b ) we see a similar comparison with rbp on the y-axis and bp on x-axis. here, rbp outperforms bp in 7 of the maps. figure 6c) shows a mixed picture with rbp and dobp splitting on the performance across the test set. the overall average-log-likelihood across all maps was -14.5 for bp, -12.0 for dobp and -12.2 for rbp. in terms of likelihood of the true solution, on average, acmi-bp benefits from using either informed message-passing protocol."
"not specified in algorithm 2 is the source for the input p ord (i). this measure would ideally measure the amount of order for amino acid i in the final protein-structure solution. since we do not know this a priori, we approximate the concept using protein-disorder prediction [cit] . specifically, we use disembl [cit], a computational method for disorder prediction using a concept known as \"hot loops\" -residues without secondary structure (i.e., not a helix or strand) and with high temperature factors. temperature factors are a term in pdb entries describing the variance of the atom's location. a higher temperature factor indicates either low confidence by the crystallographer or the existence of multiple conformations. disembl provides reasonable predictions, identifying 60-70% of disordered residues while predicting about 80% of ordered residues [cit] . disembl prediction scores are probabilities measuring the likelihood that an amino acid is in a \"hot loop\" region. in our experiments, the complement of this score is taken to formulate p ord (i)."
"from these previous results, we can see an improvement in marginal probability accuracy by the rbp and dobp message-passing schemes. biologists, however, are more interested in seeing if this translates into better protein structures at the end of the acmi pipeline. the final phase of acmi, acmi-pf [cit], samples all-atom structures from the marginal probabilities produced by acmi-bp. unfortunately, this phase of acmi revealed a shortcoming of rbp. while bp (and dobp) tend to concentrate probabilities in a few peaks, rbp produced smoother distributions with smaller and more numerous peaks. this is seen in the entropy levels, where the average entropy for an rbp produced marginal was 28.48, over five times higher than the 5.16 and 5.31 averages for dobp and bp marginals, respectively. the prime culprit is that rbp is susceptible to non-convergent oscillations. that is, a small group of nodes cannot arrive at a stable probability state after a series of messages are passed within this cluster. in this case, the residual stays high in this cluster without resolution, thus choking resources for the other nodes. in fact, for each protein in our set, the median value for the number of times a message was popped off the queue and updated in rbp was either 5 or 6, while the mean was forty."
"images of the six phantoms acquired with the four oct configurations, along with average ascans, are presented in fig. 4 . the signals due to the specular reflections above layer 1 (airsilicone interface) and below layer 5 (silicone-glass interface) are cropped out of the images, though some evidence of the signals can be seen in the phantom e images. those reflections are not strong enough to corrupt the images, in part because the beam focus is axially positioned within the spatial frequency layers. for wt and nt, it is relatively easy to resolve the individual layers in phantoms representing lower spatial frequencies, up to 14 or 21 lp/mm opt . thicker scattering layers are more likely to introduce signal attenuation, clearly noticeable in the bottom layer of phantom f. occasionally the signal does appear to increase near the bottom of layers, as with the ws image of phantom d, but we attribute that effect to random intensity fluctuations induced by local scattering heterogeneity. reduced contrast is apparent at higher spatial frequencies, and the two layers are difficult to distinguish in the images of phantom a. the individual layers of all phantoms can be resolved visually in the images from ws and ns, with the main difference between the two caused by the increased speckle size for ns. it is worth noting that several standards, including the aforementioned ct standard [cit], also recommend visual distinction of individual layers for evaluating system resolution. while representation of resolution through a ctf graph is more rigorous and objective, visualization of the individual phantoms does provide intuitive insight into how resolution affects imaging performance in an image with tissue-relevant scattering and speckle."
"final ctf curves -contrast values as a function of spatial frequency -for the four oct configurations are shown in fig. 5 . in general, the ctf curves show the expected trend, with contrast levels close to 1 at low spatial frequencies and decreasing monotonically with increasing spatial frequency. a distinction among the four configurations becomes apparent at 14 lp/mm opt, a spatial frequency which all configurations can clearly resolve. such a distinction would be quite difficult to identify without this type of phantom. as expected, the wideband sdoct and tdoct configurations demonstrate markedly better contrast than their narrowband counterparts at 14 lp/mm opt and can resolve higher spatial frequencies. ws and ns ctfs tend to diverge from each other as spatial frequency increases, whereas wt and nt maintain a more constant separation. it is interesting to note that the ns ctf is superior to both tdoct curves even though its psf-based resolution is poorer, especially compared to wt's psf. one reason for this may be the improvement in signal-to-noise with sdoct over tdoct, since our tdoct instrument utilizes older analog circuitry to filter and amplify the raw interferogram captured with the photodiode. tdoct is also operating at a longer source wavelength, which yields less scattering and therefore less available signal."
"many methods attempt to automate the interpretation of low resolution density maps. the most commonly used method is arp/warp [cit], an algorithm which iteratively fits structure to a density map, followed by a step of refinement (or improvement) of the map. the algorithm begins by creating a free-atom model -a model containing only unconnected, unlabeled atoms -to fill in the density map of the protein. it then connects some of these atoms, creating a partially-connected backbone. arp/warp refines this model, producing a map with improved phase estimates. the process iterates, ending with rotamer search to place side chains and a loop-building phase to connect the chains of atoms [cit] . arp/warp efficiently finds solutions in maps with 2.7å resolution or better, but fails in lower resolution maps when fewer observations are available."
"one difficulty in comparing average log-likelihood values among different proteins comes from the fact that the size of the probability space for each protein varies. that is, a residue from a protein in a small unit cell has fewer possible outcomes than a protein in a large unit cell. instead of loglikelihood, we can look at the rank of the true solution since this can be normalized and compared between maps. figure 7 examines the normalized rank for the true solution of a residue, averaged over all residues in the protein. the rank of the true solution of one residue is the fraction of points in that residue's marginal probability above the probability of the true solution's location. values range from (0, 1] with a rank closest to 0 being the best. the plot in figure 7 compares bp to dobp in a), bp to rbp in b), and rbp to dobp in c). again, both rbp and dobp perform better than bp with a relative decrease in rank by 18 and 10 percentage points respectively. that is, bp on average ranks the true residue solution at the 33% mark across all maps while dobp ranks at the 23% level and rbp at 15%. according to this metric, rbp tends to produce better ranks than dobp."
"the fcdbs also provide the basis for defining dietary reference values [cit] and for formulating guidelines for healthy food and nutrition i.e., for italy \"linee guida per una sana alimentazione italiana\" [cit] ."
"the permittivity of a material is derived from a number of characteristics (e.g., temperature, chemical structure, molecular composition, etc.) and is a measure of various polarisation phenomena that occur over different frequency ranges when exposed to an alternating em field [cit] . this causes dipolar polarisation in polar molecules (such as lactate), which causes them to rotate over a time period proportional to their dipole moment and local conditions (e.g., viscosity). since there is a delay between the dipolar polarisation and the applied alternating em field, dispersions exist whereby the molecule does not have sufficient time to fully align to the field, giving rise to dielectric relaxation in the microwave region of the em spectrum. a number of mathematical models have been developed by cole and cole [cit], cole and davidson [cit] and havriliak and negami [cit] to explain relaxation phenomena. it is based upon these principles that em wave sensors, operating at microwave frequencies, can selectively detect molecules such as lactate."
"it should be noted that the intensity values used were the raw intensities in linear units, without any logarithmic transformation or scaling for visualization purposes. this approach gives us access to the inherent, unbiased resolution capabilities of the oct device. the contrast versus spatial frequency data were used to determine axial ctfs for the four different oct system configurations: wideband (unfiltered spectrum) sdoct (ws), narrowband (filtered spectrum) sdoct (ns), wideband tdoct (wt), and narrowband tdoct (nt)."
"the matrix material for the phantoms was polydimethylsiloxane (pdms; sylgard 182, dow corning, midland, mi) a transparent two-part silicone elastomer formulation. the elastomer base must be mixed with a curing agent at a ratio of 10:1 by weight and then heat cured, which results in a flexible material with high mechanical and optical stability over at least one year [cit] . the cured polymer's refractive index (n) is 1.41 ± 0.01 in the near infrared (800-1300 nm), determined by oct optical pathlength measurements of samples with known physical thickness. as compared to other materials like polyurethane or epoxy resin, silicone's refractive index is lower and uniquely close to that of tissue (n≈1.38)."
"in this regard, in the last years research strategies in europe have been addressed towards the study of the nutritional composition of traditional dishes commonly consumed throughout european countries (eurofir project-european food information resource network of excellence) [cit] in order to know what we eat, in comparison with dietary recommendations and to include their nutritional characteristics in each national fcdb. in this project, efforts in developing procedures for defining and establishing a standardized approach of study (description, consumption, selection, collection, preparation, references, analytical approach and/or calculation) have being carried out [11, 19, [cit] ."
"an in-depth study [cit] considered the reliability of such handheld electrochemical devices, concluding that although all devices tested exhibited varying characteristic (error, accuracy), all could be used for longitudinal studies and have particular relevance in prescribing exercise regimes. a smaller study [cit] also demonstrated that such electrochemical sensors give acceptable results in clinical settings, and some are approved for medical use, however there is little evidence to show significant uptake in this context. this is perhaps due to uncertainty regarding the unknown sources of error with point of care devices (e.g., temperature, operator training, equipment condition, etc.) when compared with clinical laboratory facilities [cit], and the remaining infection control risk due to extraction of blood, albeit in smaller volumes. in addition, some caution against the use of a fingertip test for lactate due to inferior accuracy. [cit] note that this may not be an issue in all patients, but compares the case of those undergoing intensive care with those presenting at emergency departments. in the former case, patients will be given significant volumes of intravenous fluid which, coupled with continued capillary leak and decreased intravascular osmotic pressure, can lead to diffuse tissue oedema [cit] . in the latter case however, patients are often hypovolemic, potentially decreasing the amount of extravascular fluid that enters a fingertip blood sample."
"although direct chemical analysis is the most accurate measure of nutrient composition, it is out of reach for high cost, its long and time-consuming analysis and requires also complicated procedures, adequate equipment, specialized personnel. furthermore, the abundance and the variability of preparations of the same recipe in the italian diet, such as the continued presence of new food products on the market, increase the difficulty to analyze most of the italian consumed composite dishes. for this reason, often the characterization of nutrients and the energy value of cooked foods and especially of complex food matrices should be obtained indirectly by specific calculation procedures. in recent years this topic has been very discussed and nutrient values of complex matrices and composite dishes have often been calculated based on the nutrient content of individual ingredients, considering the different preparation and thermal treatments and by using some correction factors [cit] ."
the primary motivation for this work is that well-structured regions of the protein sequence are likely to contain better information in their local match probabilities than disordered regions of the protein.
"in section 4, we compare several variations of belief propagation to determine the effect of a message-passing protocol on acmi's ability to produce accurate protein structures. first is the original version of acmi-bp [cit], which uses the round-robin scheduling algorithm in algorithm 1. in the results in section 4, this method is designated as bp. we also consider a scheduler based on residual belief propagation [cit] from section 3.4. algorithm 3 shows the details for applying residual belief propagation to acmi, where we prioritize nodes according to equation 4 . acmi-bp scheduled with this function is designated rbp in the results below. last, we consider a method based on algorithm 2, utilizing domain knowledge to guide acmi-bp. in the results below, we denote this heuristic as dobp (for disorder belief propagation)."
"the task of determining protein structures has been a central one to the biological community for several decades. while ab initio methods have received a great deal of recent attention, x-ray crystallography remains the method of choice for protein-structure determination, producing over 85% of models in the protein data bank (pdb) [cit] . creating a high-throughput protein crystallography pipeline is a key area of research in the field, and one major bottleneck in need of automation is the last step in this pipeline -determining a protein-structure model from an electron-density map. an electron-density map is a three-dimensional image of a molecule and is an intermediate product of x-ray crystallography."
"nowadays, several research studies are being carried out on the development and validation of the calculation method to know the nutritional contents of complex matrices. the comparison between analytical and calculated data allows to evaluate the quality of calculation procedure; its results can be considered a valid alternative to analysis or only a rough estimate. calculated values are not suitable to all uses and more attention must be paid to nutrient intake studies, especially for micronutrients, which exhibit greater variability, because they are present in very small content and are more sensitive to preparation and thermic treatments [17, [cit] ."
"algorithm 2 shows an overview of our new inference algorithm for guiding acmi-bp. the main difference from the description in algorithm 1 is the introduction of a priority measure p ord . this probability function is given to acmi-bp by a user. while specifically built for our task, this formulation applies to any instance of belief propagation. the function p ord should describe the relative importance of node i in influencing the network. for our task, it measures the probability that amino acid i in a protein's primary sequence will be well-structured in the final 3d solution. guided acmi-bp uses this information to decide, for a given iteration, which residue to next perform inference on. intuitively, acmi-bp now focuses the initial iterations on passing information along regions of the sequence likely to produce stable structure. this probability measure is steadily decayed to allow other amino acids to move to the top of the queue. this will allow the (predicted) ordered regions to refine their probabilities, essentially locking in their locations. when less reliable amino acids finally work up the queue, the ordered amino acids should contain more confident messages and thus have a larger influence on the final distributions."
"food composition databases (fcdbs) contain nutrient composition, energy and/or other bioactive components of foods largely consumed by a population [cit], mainly simple foods and non-cooked processed foods. these databases are used to make a series of interventions, including:"
"therefore, for this work, the authors applied the approach of pointwise mutual information (pmi), combined with neural networks (nns). pmi is a useful method for establishing the relationship between datasets and their supposed target data, and producing rankings that indicate the prominence of relationships. in this work, pmi was used to consider the relationship between the lactate value measured with the lactate pro v2, and the corresponding spectral data captured using the em wave sensor. by doing this, it was possible to rank the spectral data by frequency in order of its relevance, and therefore reduce the spectral dataset being provided to the nn. this has significance for two reasons since: 1) it reduces the amount of irrelevant information being provided to the nn, thereby improving the likelihood of a suitable model being generated and; 2) it assists in the commercial objectives of the work since limiting the frequency of operation reduces cost, size and power requirements, all of which are barriers to implementing a wearable system."
"thin-film fabrication approaches can also be applied to development of biologically realistic phantoms to simulate tissues with micro-scale layers of varying scattering coefficients. for example, it may be possible to replicate retinal layers which vary widely in both backscattering and thickness. furthermore, thin-film phantoms may be useful for assessment of a range of other optical devices from confocal or two-photon microscopy systems to fluorescence and reflectance spectroscopy or microscopic photoacoustic imaging systems."
"other optical based techniques for monitoring lactate are evident in the literature [cit], however little of that work appears to have made a significant presence on the poc market. largely speaking, these types of devices combine a chemical approach (e.g., a colour change) which then infers a lactate concentration. however, these suffer from the same drawback as current electrochemical methods, namely the limited reusability of the sensitive elements of the device themselves. boldt [cit] demonstrates that costs from such point of care devices depend on many factors which can be categorised in terms of pre-analytical, analytical and post-analytical costs which may vary from one organisation to the next thus making the cost benefit difficult to establish."
"belief propagation is an iterative, local message-passing algorithm which distributes evidence between nodes in a graphical model. figure 1 gives a high-level view of belief propagation on an mrf. a message is sent between two random variables (i.e, nodes), conveying the sender's belief in the recipient's state, with probability. in the case of asynchronous message-passing, each iteration requires bp to choose a message to be delivered, calculates that message, and updates the recipient's current marginal probability estimate. in graphs with loops (such as acmi), bp is not guaranteed to arrive at the exact solution, or even converge to any solution. while empirically successful in acmi, bp is often abandoned in large, complex tasks due to the inability or slowness to converge to a solution."
"we measured the thickness and surface profile of phantom layers with a stylus profilometer (dektak 150, veeco, plainview, ny), capable of measuring step height changes over a range of 50 nm to 1 mm. we were able to determine each layer's thickness by measuring the total height of all accumulated layers relative to the substrate surface after each film was deposited and cured. in this way, we had independent and highly accurate measurements of layer thickness, which are necessary for reliable ctf calculations. the surface profile provides important quality control information post-fabrication to locate any defective regions."
"several methods exist that attempt to improve bp performance by prioritizing nodes based on the amount of new information they expect to receive from their neighbors. [cit] formulated residual belief propagation (rbp), a scheduling function based on the intuition that messages which differ significantly from their previous value are more likely to push bp toward convergence. conversely, a message whose new value is similar to the value the last time it was sent is contributing relatively little new information to the recipient node and should have low priority. rbp calculates a residual factor, r i, for each node 1 . when a neighbor of i is updated, the residual factor is calculated, capturing the amount of new information available. if that value is larger than the current value for ri, it is updated. ri is defined:"
"s-parameter measurements for the sensor in air are illustrated in fig. 4(c), showing that the senor tends to resonate at approx. 2 ghz. the sensor is designed such that the em field closely coupled to the surface of the sensor, so that the field may penetrate through the skin of a target and interact with the fluids beneath. maintaining a field close to the sensor surface has some advantages, namely that of reducing interference from objects other than the surface to which it is directly attached. the hairpin configuration of the device supports this notion well, and has the primary reason for its use."
"several factors make tracing the protein a difficult and timeconsuming process, mainly by affecting the quality of the electron-density map. the first and most significant factor is the crystallographic resolution of the density map. crystallographic resolution describes the highest spatial frequency terms used to assemble the electron density map. resolution is measured in angstroms (å), with higher values indicating poorer-quality maps. another factor making automation difficult is the phase problem; crystallographer's can only estimate the phases needed to calculate the electron-density map, reducing the interpretability of the image."
"following the evolution in recent years of nutritional science, the study of the potential beneficial role of diet is one of the most topical and controversial. the health status and in particular the risk of certain diseases has been closely related to the diet which, in addition to the concept of correct and balanced, has also been enriched by functional diet (who). research on interactions between single food components and/or between the various ingredients of a composite dish, has a significant role and amplifies the meaning of the \"food synergy\" as fundamental unit: a concept for understanding the relationship between nutrition and health [cit] ."
"a number of reduced datasets were produced using the pmi method, based on the top 10, 20, 50, 100, 250 and 500 frequencies of interest per measurement with the em sensor, where originally data was acquired at 4000 discrete frequencies between 10 mhz and 4 ghz. this was replicated for data collected from both the arm and leg of each participant, as well as for each measurement mode, i.e., s 11 and s 21 ."
"several studies [cit] have shown that the retention of nutrients, in the same cooking conditions, for some foods is similar. for example, for different types of red meat roasted the same retention factors are applied to different foods but belonging to the same food group, or subgroup, when subjected to similar heat treatments. similarly, for complex food matrices [cit], retention factors are considered similar to those of the main ingredient and are therefore \"imputed\" to complex dishes."
"recently, our group [cit] and others [cit] have developed phantoms for three-dimensional characterization of the oct point spread function (psf) using a sparse distribution of nanoparticles or microparticles embedded in a transparent polymer. this type of phantom permits rapid and detailed measurements of spatial resolution in both lateral and axial dimensions, and indicates how the resolution varies over the imaged volume. in addition, the mapped psf information can be used for spatial deconvolution to improve image quality [cit] ."
"considering the growing globalization of the food market, national food composition databases are addressing to produce, collect and present data in a standardized format in order to \"speak a common language\" which allows to compare data from different national databases and use them interchangeably to collaborate between countries."
belief propagation is an inference algorithm that calculates marginal probabilities by utilizing a local message-passing scheme to propagate information across a graphical model [cit] .
"with 34 participants and a total of 367 blood lactate measurements, on average there were 11 blood samples taken per participant. naturally, this varied depending on the fitness level of participants, and thus their ability to maintain a steady cadence despite the increasing work rate. fig. 6 gives an indicative lactate profile, with markers denoting the four phases discussed in the methodology section."
"this work wants to give an updated overview of the food composition databases, as essential tools in nutrition research for a wide spectrum of applications; currently the main issues to be addressed are the following:"
"the sensor was placed on the left arm and leg of each participant; the leg due to this being the source of lactic acid production during exercise, and the arm due to blood being drawn from the finger tip for lactate measurement. specific placement on the leg was over the rectus femoris muscle and on the wrist approx. one-third distance between the wrist and elbow joints, where there would be significant blood flow owing to the arteriovenous fistula. the left side of each participant was chosen simply due to accessibility within the testing space itself; the setup is shown in fig. 5(a), with a closer view of the sensors adhered to a participant in fig. 5(b) ."
"it is emerging that experimental and epidemiological studies should consider the \"total diet\", as an important variable in risk assessment and the \"total quality\" of what is consumed to address towards better choices, more consistent with the specific individuality, with the aim to positively influencing physical, psychological and social well-being [cit] . therefore, particular attention should be directed to the study not only of individual foods or food components but also to the nutritional characteristics of all foods in the generally consumed forms of the whole diet."
"a number of techniques were considered for providing robust analysis and models to test the correlation between em wave sensor outputs and lactate level measured via lactate pro v2. typical linear models, which have proven successful for in-vitro laboratory based tests (for example, see previous work of the authors in this field [cit] yielded relatively low correlation across the complete data set."
"a major benefit of real-time on-patient monitoring noted earlier was the potential to be able to monitor live patient information. current blood sampling does not give enough resolution to understand whether a patient's lactate level is rising or falling, and therefore deciding on an intervention strategy can be challenging. thus, being able to track the direction of lactate change is perhaps as important as knowing its absolute value. the capability of the sensor to do this is illustrated in fig. 9, where all of the collected data from the 34 participants is overlaid with the predicted data from the nn model, trained using 100 discrete frequencies."
"measurement with a bga is not without its drawbacks however, since the process of extracting blood from a patient is an invasive procedure, with potential complications which include artery occlusion, digital embolisation leading to digital ischemia, sepsis, local infection, pseudoaneurysm, hematoma, bleeding, and skin necrosis [cit] . as a result of infection risk, resource availability, and patient capacity to provide blood, bga does not give a high resolution assessment of patient condition over time, which many clinicians argue would provide information relevant to understanding the necessity and form of intervention. furthermore, the drive toward more point of care monitoring equipment located at the patient bedside has clinicians looking toward smaller and more portable devices. some attempts to produce portable bgas, such as the abott i-stat device illustrated in fig. 1(b), have been commercialised and studies show they give levels of accuracy for lactate comparable with larger desktop systems [cit] . however, the required blood volume (65 μl), long sampling times (approx. 65 seconds) and skilled handling procedure preclude use at the bedside."
"a guideline for calculating nutrient content based on the recommendations of eurofir aisbl [cit] has recently been formulated. this document provides information for different users, especially, to food commercial operators, by including also several components specified for processed foods: list of ingredients; weight of input ingredients; total raw weight of input ingredients; weight of cooked food; food composition data of input ingredients; calculation-content of nutrients in cooked food without using retention factors; retention factors; calculation-content of nutrients in cooked food with retention factors; calculation of energy value and as additional step calculation of water. however, it is recommended to check outputs of a recipe calculation by chemical analysis of a product [cit] ."
"we used two multispectral satellite images (quickbird) represented in figure 4 (a), with respective size 240 by 360 pixels and 500 by 280 pixels. the idea for a good spectral classification of the pixels is to directly give the image's pixels value as input data of the classifier."
"graph cuts finds the optimal solution to a binary problem. however when each pixel can be assigned to many labels, finding the solution can be computationally expensive. for the following type of energy, a series of graph cuts can be used to find a convenient local minimum:"
"radio signal strength (rss) is widely used in fingerprinting positioning systems to signify the location-dependent characteristic, such as radar [cit] and horus [cit] . the striking point of rss-based fingerprinting lies in the simplicity of deployment with no specialized hardware required at the mobile station except the wireless network interface card (nic). however, we claim that the weakness of rss-based fingerprint stems from two aspects: first, rss varies with time at a fixed position [cit] due to the multipath effects, which including the reflection, diffraction and diffusion in indoor environments. second, rss is a coarse measurement of the received power at the radio frequency band. for several locations, the rss values may be reproducible because it lacks of the frequency information to capture the multipath property. therefore, this time-varying and duplicated rss value describes signal characteristics inaccurately and creates undesirable localization errors."
"the support vectors machines (svm) [cit] introduced by vapnik is a technique of construction of optimal hyperplanes separating n classes and which provides a continuous function of discrimination. data are projected in a deployed space of large dimension (space of hilbert) in order to define a linear separation in this space, before returning back to the initial space. the choice of separation: hyperplane which classifies the data correctly (when it is possible) and which is \"as far as possible from all the examples\" [cit] as shown in fig. 1 . we have the following property and decision function (4) where corresponds to the closest points not-null. as error of treatment one introduces variables ' ressort' to soften the constraints, and one penalizes by the going beyond the constraint. (5) in the not-separable case the dual problem will have the same form, the only difference is the upper limit on α. instead of seeking a hyperplane in input data, we access to an intermediate space of representation of great dimension. several cores are used with the most frequent svm methods. in our case we have used a polynomial kernel."
"we argue that a reliable metric provided by commercial nics to improve the accuracy of indoor localization is in need. such metric should be more temporal stable and provide the capability to benefit from the multipath effect. in current widely used orthogonal frequency division multiplexing (ofdm) systems, where data are modulated on multiple subcarriers in different frequencies and transmitted simultaneously, we have a notation from phy layer that represents the channel properties over all the subcarriers called channel state information (csi). the primary advantage of csi over rssi is that this fine-grained information estimates the channel on each subcarrier in the frequency domain. in contrast to only one rssi per packet, we can obtain multiple csi values at one time and the values stay fairly stable over time [cit] . in our pervious work [cit], we have built a propagation model based on csi. and then we use it for precise indoor localization by eliminating the multipath effect. however, we observe that csis over multi-subcarrier will have unique signatures in different locations. these unique features come from the frequency diversity of csi, which has different amplitudes and phases of each subcarrier. by exploiting the frequency diversity, we can construct a unique \"fingerprinting\" indicating each location on the radio map. motivated by this, it is favorable to leverage the csi for location fingerprinting and thus improve the localization accuracy."
"in this paper, we have presented a methodology which makes it possible to combine space and spectral information to refine the classification of multispectral satellite images. the application of this methodology gives more satisfactory results, however it remains to improve even more these results. in future work, to achieve this goal, we will be concentrating on the study of the cores in order to determine which is appropriate one for this type of classification."
"in summary, the main contributions are as follows. 1) to the best of our knowledge, it is the first time to take advance of the combination of the fine-grained phy layer information csi with frequency diversity and multiple antennas with spatial diversity for indoor location fingerprinting. 2) we carefully design the architecture of fifs which consists of two key components: fingerprints generation and position estimation. first, we process the raw csi values by leveraging the diversity in time and spatial dimensions and store them in the fingerprints database (radio map). second, to map the target object to the radio map, a coherence bandwidth-enhanced probability algorithm with a correlation filter is proposed. 3) we implement fifs in commercial ieee 802.11 nics."
"in fifs, the positioning server will response the mobile device with the estimated position when it sends out a location query message. in order to obtain the location information, the positioning block must be capable of 1)calculating the similarity between the csi value measured at the rx and the fingerprints database, and 2)determining the location of the rx indicated by the corresponding fingerprint, which has the highest possibility of the measured csi. in this manner, the location of the target device can be estimated by mapping the csi value and database. iv. methodology in this section, we describe the design terminology of fifs. the methodology of this csi-based location fingerprinting approach can be broken down into two following phases. 1) calibration phase: first, we need to effectively process the raw csi value to generate fingerprints, and then record the fingerprints corresponding to certain sample locations for building up a radio map. note that this is known as the prerequisite of the ongoing online phase. 2) positioning phase: second, on the basis of this effective csi value, in fifs, we compare it with the csi fields of the entries stored in the radio map, and the position of the object will be extracted from the radio map with the closest match afterwards."
the main idea of the alpha-expansion algorithm is to successively segment all α and non-α pixels with graph cuts and the algorithm will change the value of α at each iteration. the algorithm will iterate through each possible label for α until it converges.
"to generate a radio map, we first extract the statistic determine the number of detectable aps for a sample position. at each reference point, we will generate a unique fingerprint from the csi of all aps and antennas."
"to these images ( at first sight, we note that we have a great difference between the results; the classification resulting from combining svm-graph cut is visually more satisfactory than those of svm and svm-morphological operations since it is less noisy, while in the svm classification we have an effect of salt and pepper. the resulting svm-graph cut matches well with an urban land cover map in terms of smoothness of the classes, also it represents more connected classes. for the svm-graph cut method a total accuracy of 96.05% was achieved, slightly higher than the accuracy of 92.45% derived by the svm-morphological operations classification and of 87.22% derived by the svm classification (detailed in the following table)"
"generally, there are many sophisticated algorithms like maximum-likelihood (ml), minimum mean square error (mmse) to estimate the csi precisely. therefore, comparing with rssi, csi is a fine-grained value from the phy layer that describes the channel gain from tx baseband to rx baseband."
"to ensure the robustness of the location fingerprinting systems, temporal stability is a foremost criteria we need to validate. we thus set out to investigate the stability of the proposed new metric csi and the widely applied rssi value in time series. due to the coarse packet-level estimation and easily varied by multipath effect, rssi is well known to be a fickle measurement of the channel gain. in particular, this instability of rssi induces inevitable errors in localization. thus, we need to figure out whether the fine-grained phy layer information csi will remain in a stable manner in practical indoor environment. the temporal variance of rssi in corresponding traces is much larger within 30% as presented in fig. 6(b) . therefore, the relatively stability for csi is an essential advantage for localization while comparing with the traditional rssi in time domain."
"channel state information (csi) also knwon as channel status information is information that estimates the channel by representing the channel properties of a communication link. to be more specifically, csi describes how a signal propagates from the transmitter(s) to the receiver(s) and reveals the combined effect of, for instance, scattering, fading, and power decay with distance. in summary, the accuracy of csi greatly influences the overall ofdm system performance."
"this paper is organized as follows. in the second section, we give outlines on the used classifier: support vector machines (svm). in the section 3, the used graph cuts approach is described. in section 4, the results are presented with numerical evaluation. finally, conclusions are given in section 5."
"moreover, due to the multipath effect of indoor environment, the wideband channel of 802.11n can provide abundant diversity in the frequency domain. the metric to evaluate the frequency diversity is coherence bandwidth [cit] . the coherence of two arbitrary different subcarriers with distance δf is defined as"
"our previous fila [cit] system leverages the frequency diversity of fine-grained csi and the refined free space path loss propagation model to increase the accuracy of indoor localization. this work is an nature extension in that, beneficial from the multipath effects, we further investigate the property of csi that provides unique amplitude and phrase per location for applying fingerprinting. pinloc [cit] also exploits the physical layer channel information to enhance the area granularity accuracy by only considering the frequency diversity. however, the spatial diversity, one of the most important features of current commercial aps, is lack of investigation. moreover, in our work, we determine the position of object by correlation calculation augmented with a probability algorithm, where the coherence bandwidth is applied to reduce the computational complexity."
"with the commercial emergence of the optical satellite images of sub-metric resolution (ikonos, quickbird) the realization as well as the regular update of numerical charts with large scales becomes accessible and increasingly frequent. the classification of such images is similar to that of other types of images, it follows the same principle, and it is a method of analysis of data that aims to separate the image into several classes of interest, in order to gather the data in homogeneous subsets, which show common characteristics. it aims to assign to each image's pixel a label what it represents in a scene (a zone of vegetation, a water zone, etc) [cit] ."
"recently, a worldwide convergence has occurred for the use of orthogonal frequency division multiplexing (ofdm) as a bandwidth-efficient technology for high data rates wireless communications. it has been endorsed in leading standards such as ieee802.11a/g/n, wimax, lte. ofdm is a broadband multicarrier modulation scheme combined with multiplexing. at the ofdm transeiver, the incoming data stream is split onto multiple narrow and orthogonally overlapped subcarriers as depicted in fig. 1 . the data on each subcarrier is then modulated and converted back to the time domain by an inverse fast fourier transform (ifft). after parallel to serial (p/s) and digital to analog conversion (dac) process, the signals are sent through a frequency-selective channel."
"upon receiving the signals, the receivers sample them and pass them on to a demodulation block as well as digitize them using analogue-to-digital converter (adc). afterward fft procedure processes the data sample blocks to convert back into the frequency domain."
"channel measurement at the subcarrier level becomes available based on ofdm in wireless communication. nowadays, the measured channel state are widely utilized for adapting or allocating the transmitter resources [cit] ."
"the proposed method consists in combining space and spectral information to obtain a better classification, we start with a spectral classification with svm. then, we apply the graph cuts approach which takes in account the space dimensions of the classification. experimental results are provided and comparisons with morphological approach applied to the spectral classification are made to illustrate that the method is able to find better class."
"each pixel p ij in the image corresponds to a node v ij in the graph. two additional nodes are the source and the sink, respectively the object and the background. each node (pixel) is linked to its neighbors by edges n-link (neighbor), with connectivity chosen, and whose capacities depend on differences in intensity. each node (pixel) is also connected by edges t-link to the terminals (source and sinks) fig. 2 . each pixel corresponds to a node, and all pixel nodes are connected to the source and the sink."
"experimental results demonstrate that the csi-based fingerprinting provided by fifs can improve the localization accuracy, and outperform the corresponding traditional rssi-based approach. the rest of this paper is organized as follows. in section ii, we introduce some preliminaries. section iii presents the architecture of the fifs system. this is followed by the methodology of the csi-based fingerprinting in section iv. the implementation of fifs and experimental evaluations are presented in section v. we summarized the existing work on fingerprinting in section vi. finally, conclusions are presented and suggestions are made for future research in section vii."
"since the small-scale fading effect occurs at the level of several wavelengths (about 12cm at 2.4ghz, and 6cm at 5ghz), and the distance between multiple antennas is typically much larger, using multiple antennas presents the opportunity to smooth out the effect, while maintaining the same calibration workload required by the localization system. consequently, an important open question is if we should aggregate the csi measurements obtained from different antennas, or the localization algorithm should use the csi from each antenna independently. in this paper, a simple aggregation scheme is examined that we perform an averaging over all the antennas at each sample position, more sophisticated investigation and schemes are left to our future work."
"a novel rf-based indoor location system fifs is presented in this paper, which is based on fingerprinting through csi values obtained in real time from the phy layer. the characteristic of the propagation channel between the received object and each ap is dynamically estimated from these fine-grained csi values. we collected and proceeded the csi values as \"fingerprints\" by leveraging both the frequency diversity and spatial diversity to uniquely present the reference points and generate the radio map. moreover, we adapted a probabilistic model to accurately map the observed csi into the stored \"fingerprints\" and use the coherence bandwidth to reduce the complexity of the algorithm. experimental results show that a mean error slightly lower than 1m is obtained in an unmodified fifs wlan network deployment."
"based on csi, in this paper, we present an indoor location fingerprinting scheme. to achieve high accuracy and low complexity indoor localization, our approach composes of two parts: first, we process the raw csi values from measurement by integrating frequency diversity and spatial diversity, and then build up a radio map; second, we determine the position of object by correlation calculation augmented with a probability algorithm."
"at each iteration, the α region pα can only expand. this changes somehow the way to set the graph weights. also when two neighboring nodes do not currently have the same label, an intermediate node is inserted and links are weighted so they are relative to the distance to the α label."
"( 1 1 ) in our case, one is going to use the algorithm alphaexpansion, this algorithm permits to minimize the energy e on a finite set of labels l and a class v called penalty metric. v is metric on the space labeled l if it satisfies:"
"we have as at last, for each image, two files \"trainfile.dat\" and \"testfile.dat\" containing respectively 8 516 and 8 451 individuals (pixels) for learn, 86 400 and 140 000 individuals for test (to classify), and divided on the following classes:"
"since the commercial 802.11n nics in the current market are mostly equipped with multiple antennas, the instinctive spatial diversity of mimo lays solid foundations of further enhancing the localization accuracy."
"in this section, we first introduce our experimental scenarios and the data collection procedure of fifs. afterwards, we will show the performance of our proposed csi-based fingerprinting approach by comparing against best known rss-based horus system [cit] . for fair comparison, we consider the continuous space estimator in horus system without consideration of the temporal correlation."
"where y and x are the received and transmitted vectors, respectively, and h and n are the channel matrix and the additive white gaussian noise (awgn) vector, respectively. to successfully decode the message x from received signal y distorted by fading and noise, we should estimate the channel distortion first with some symbols known as preambles or pilots. thus, csi of all subcarriers h can be estimated according to (1)"
"in this section we present the results obtained, first, with the svm classifier. thereafter, we show the interest of the use of the graph cuts to refine the result of the classification. we have used the toolbox \"support vector machine toolbox version 2.51 [cit] of anton schwaighofer [cit] (institute for theoretical computer science of graz in austria)\" implemented in matlab, with rbf kernel; but the results were not satisfactory, so we used svm light wich is an implementation of support vector machines (svms) in c language [cit] with rbf kernel."
"two sets, such that the terminals belong to each set. in combinatorial optimization, the cost of a cut is usually defined as the sum of the weights corresponding to the edges it serves [cit] as is formulated in equation 11."
"in this section, we start with an overview of the widespread ofdm technique in wlan. then, we introduce the csi value as the fundamental component of our work."
"during the fingerprints database construction, there exists two important modules including csi collection module and csi processing module on the mobile device side. since fifs is built based on the current 802.11n communication system, the transmitter end (tx-the ap) induces no modification. once the receiver end (rx-the target mobile device) received a packet, it will first export the raw csi value after the normal demodulation process. in the designated processing module, the csi collected from 30 groups different subcarriers will then be processed. as mentioned in the previous section, csi value is the channel matrix from rx baseband to tx baseband which is needed for channel equalization. therefore, there is no extra processing overhead when obtaining the csi information. afterwards, we introduce a calibration condition to determine the outgoing of store the csi value after processing. the calibrated csi will be stored in the fingerprints database. otherwise, it will be accessed as the input of the mapping algorithm. this fingerprints generation block serves as the prerequisite of the positioning block."
the main idea of a graph cuts is to bring back the problem of minimization of energy to a problem of minimal cut in a graph.
"several techniques exist, in this work, we use svm. this method is actually no more than an algorithm of training and classification. it is about a problem of optimization that makes possible the separation of a field into two or several classes. the problem consists in transforming the space in which one works in another space known as large-sized and inside of which we will establish a separation in various classes. the passage from one space to another is carried out according to some mathematical transformations that use the nonlinear functions, called kernel functions."
"the advance of wireless technology has fostered the flourish of indoor location-aware applications, such as indoor navigation, warehouse management and health care, etc. with the advent of wireless communications, wireless local area networks (wlans) that increasingly being deployed in offices and homes recently become a means of wireless indoor localization technique. due to the open access and low cost properties, it opens an opportunity for leveraging the existing wlan ieee 802.11 [cit] infrastructure to provide precise location estimation in indoor environment. many wlan-based indoor positioning systems [cit] adopting \"fingerprinting\" technique have gain popularity due to higher accuracy. the fingerprintingbased approaches typically determine the location based on two phases: first, associating location-dependent characteristics to certain locations for constructing a radio map (offline training phase); then, mapping the characteristic of object to the radio map to infer the location (online positioning phase)."
"step 8: make the statistical decision. the research makes the statistical decision based on the result of statistical analysis: the factor either contributes to performance improvement with statistical significance or does not contribute to performance improvement with statistical significance. in the first case, the factor is considered an optimal solution for the optimisation problem. in the second case, the factor is not considered an optimal solution for the optimisation problem. the researcher should search for a new biological example and repeat the process from steps 2 to 8."
"step 6: conduct experiments on the biological example with the factors and on the artificial model without the factors. the biological example used in the experiment is an airfoil which models the wing shape of ruddy darter (sympetrum sanguineum), which is a kind of european species of dragonfly of the family libellulidae. the artificial model is a naca0012 airfoil."
"landmark sequences need to be selected. thus, the selection of landmark sequences can be performed very efficiently with computational complexity of oðn log 2 nþ. we next discuss the termination criteria that we use to control the height of an hd tree, which is a trade-off between solution accuracy and computational efficiency. three termination criteria are used, including the sub-cluster radius, the sub-cluster size, and the number of sub-clusters. among them, the sub-cluster radius is the most important one. in order not to introduce extra computational costs, we estimate the radius of a node as the median of the pairwise distances between landmark sequences in the node. the reasoning is that if the result of spectral clustering performed on the landmark sequences is a good approximation of that obtained by spectral clustering performed on all sequences in a node, the estimated radius should be a good approximation of the radius of the node. generally speaking, the probability of falsely separating sequences belonging to the same species increases as recursive bisection goes deeper. hence, we can effectively control clustering accuracy by preventing clusters with small radiuses from being partitioned. in section 4.2, we performed a parameter sensitivity analysis that demonstrated how to estimate a proper sub-cluster radius in order to achieve a good balance between solution accuracy and computational efficiency. besides, we also use two auxiliary termination parameters, namely, the sub-cluster size and the number of sub-clusters. these two parameters are highly dependent on the input data size, so it is difficult to use them to control clustering quality. however, they can be used to force an early termination in order to balance the time spent on the top-level partition and sub-clustering phases."
"in the proposed method, each internal split s i consists of three phases: i) adaptive landmark selection, ii) spectral clustering and iii) averaging assignment. denote as l i, p i and v i the possible error events in the three phases, respectively. our method fails if any of these error events occurs in an internal split. it is easy to see"
(1) the closed-form mathematical model for the optimisation problem is difficult to formulate; (2) the analytic solutions for the optimisation problem are impossible or too complicated; (3) validating the mathematical model that describes the biological system or mechanism is impossible or too expensive; (4) no knowledge or theories exist on the complex biological system/mechanism.
"step 9: interpret the reason. with engineering and science knowledge, the researcher attempts to determine why this factor contributes to performance improvement. if he or she succeeds, new knowledge and theories are explored and obtained. nevertheless, the result may not be fully explained because of the limited knowledge of the researcher in the relevant area."
"in future research, the two methods will be applied to more case studies. the optimal solution identification and reuse method will be integrated with other processes and developed into a new product development methodology. in addition, a database that is designed for the storage of optimal solutions from nature will be constructed, so that product designers can search for the optimal solutions by using the keywords of their engineering optimisation problems. since the wing with higher lift-to-drag coefficient implies better aerodynamic performance, the hypotheses about the lift-to-drag coefficient are stated as follows:"
"the shape of the dragonfly wing cross-section is considered in the design of the airfoil wing of the flapping micro air vehicles. the two-dimensional cross-section of the airfoil wing is sketched in figure 3 . the (x, y) coordinates of the points on the cross-section contour are displayed by table 2 . to fabricate the airfoil wing, we develop a regression model to describe the contour. since the contour is symmetric, we only need build a regression model for the upper part. the contour is approximated by two three-order polynomial functions as follows:"
step 4: state the hypotheses. the null and alternative hypotheses on the optimality of the biological solution are stated. the optimal solution from the biological organism (biological form) is defined as a factor. the researcher tests whether this factor contributes to the excellent performance of the biological organism.
step 4: they developed an analytical model for the tree branch loading system and conducted mechanics analysis of the system. a tree branch can be treated as a cantilever. the wind loading produces bending moments and the self-weight of the tree produces compressive forces and bending moments. the upper region of the tree branch resists tensile stress and the lower region resists compressive stress.
a case study is used to illustrate the procedure of the second method. we aim to design the wing shape of the flapping micro air vehicles (mavs) which fly near the ground or the water surface. the case study is described step by step as follows.
"microbes play an essential role in processes as diverse as human health and biogeochemical activities critical to life in all environments on earth. however, due to the inability of traditional techniques to cultivate most microbes, our understanding of complex microbial communities is still very limited. the advent of high-throughput sequencing technology allows researchers to study genetic materials recovered directly from natural environments and opens a new window to extensively probe the hidden microbial world. consequently, metagenomics, where the amplicon sequencing of 16s [cit] by the science magazine [cit] ."
step 2: they observed the biological example directly from nature: a tree branch. the tree branch can be treated as a cantilever that supports the bending moment because of the self-weight of the branch. it contains several efficient structural features.
"the first is an equation-based approach, which uses an explicit function to establish the relationship between the performance of a product and the design parameters. this method is often difficult, or even impossible in certain cases to implement, because it requires complete knowledge of the mechanism of the system. [cit] pointed out that a feasible and an efficient algorithm which guarantees to find globally optimal discrete design is not achievable within the limits of our existing mathematical knowledge."
"a method for obtaining, identifying, and reusing optimal forms of living organisms should contain procedures or iterations with these elements: observing the biological examples, formulating an optimisation problem, obtaining a possible optimal solution (biological forms, shapes and structures) and verifying the optimal solution. since a solution is only valid for a specific problem, the problem and the solution should be identified and expressed first. then the optimality of the solution is verified either through theoretical analysis or through experiments. only after verification, the optimal solution is eligible to be reused for similar engineering optimisation problems. a method for obtaining, verifying and reusing optimal biological solutions should provide detailed instructions for all of the above elements."
"step 10: apply the optimal solution to product design. the shape of dragonfly wing cross-section will be considered in the design of the flapping micro air vehicles (mav) which fly near the ground or the water surface. the two-dimensional cross-section of the airfoil wing is sketched in figure 3 . we develop a regression model to approximate the shape. the details of the model are found in appendix 1. the airfoil wing will be fabricated by a computer numerical control machine with the alloy of duralumin and titanium. however, more research work should be done before the airfoil wing is applied to the flapping mav. the research includes mechanics tests of the airfoil wing produced by the machine, the design of the hinge system which links the wing to the vehicle body, the design of the assembling process and aerodynamics tests of the vehicle, etc. we will conduct these tests and design in future research."
"this article studies the methods of obtaining, verifying, and reusing biological optimal solutions (refer to biological shapes, structures, and forms) to solve engineering optimisation problems. a product design for fulfilling a functional requirement can be viewed as a multiple-objective, multiple-constraint optimisation problem, which is to determine the detailed design and parameters for optimising the product performance. however, many optimisation problems in engineering are very difficult to solve."
"step 4: develop an analytical model for the biological organism/system. building an analytical model describes the working mechanism of a biological system. it is a mathematical expression constructed using well-known operations that are ready for calculation. the model should be a close approximation of the real system and incorporate most of its features; however, the model should not be too complex to understand and analyse. building an analytical model often requires a comprehensive understanding of the mechanism and abundant knowledge of the biological organism/ system."
"step 1: define an engineering optimisation problem. to design an engineering product, the researcher must solve an optimisation problem (called problem a). due to lack of knowledge of the forms, shapes, and structures of the components and the product, the problem definition (its objective functions, design variables, and constraints) is not explicit without explicit mathematical formulas. it relies on concept description. step 2: search for biological examples. living organisms in nature must solve optimisation problems similar to engineering problems for their survival. the following three methods are applicable in this step."
"we also compared the clustering results obtained by uclust with and without slad and the nmi scores are around 0.97-0.98 (fig. 4), which is consistent with the result observed in supplementary figure s1 ."
step 1: [cit] studied the optimality of the shape and size of the cross-section of a cantilever that supports the bending moment. they formulated the optimisation problem as follows:
"sometimes, an analytical model of the biological system and the explicit formulation of the optimisation problem are difficult or impossible to obtain. the second method is proposed for this situation. it is based on statistical techniques. the brief procedure is shown by figure 2 . the method consists of the following ten steps (the first three steps are the same as those of the first method; thus, their details are omitted)."
"step 5: theoretically analyse the model. theoretically analyse the analytical model and develop closed-form mathematical functions. formulate the optimisation problem faced by the biological example explicitly, which is called problem b."
"in our implementation, we used spark mllib [cit], which is a distributed framework built on top of spark core and provides a library of commonly used machine learning algorithms. due in large part to the distributed memory-based spark architecture, the implementations provided by spark mllib run much faster than disk-based counterparts. due to space limitation, other implementation details are presented in supplementary material."
step 5: they theoretically analysed the model and developed closed-form mathematical functions. the function requirement f is defined as the bending failure moment. the performance index p is expressed as a function of the shape of the cross-section (s) and the area of the cross-section envelope (d). [cit] for the details of the functions.
you may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. submit your manuscript to a cogent oa journal at www.cogentoa.com
"the ladc method partitions a sequence dataset recursively into clusters and represents them as an incomplete hierarchical divisive (hd) tree. an hd tree is a k-ary tree consisting of multiple layers of nodes with each node representing a cluster. it can be constructed by recursively partitioning a node into k children using a clustering method as one moves down the hierarchy. in a complete tree, each leaf node contains only one sequence. however, the partition process can stop intermediately so that each leaf node contains multiple sequences, thus forming an incomplete hd tree. the standard method for constructing an hd tree has a computational complexity of oðn 2 þ, and hence is computationally infeasible to process large sequence datasets. one possible way to address the issue is to randomly select a small number of sequences and perform clustering analysis only on the selected sequences in each partition operation [cit] . in this way, the number of pairwise sequence comparisons can be significantly reduced. one issue associated with random selection is that samples in small clusters are seldom selected, and thus it may not be able to recover small clusters. in order to address the issue, we propose to construct an incomplete hd tree by using the adaptive landmark selection method [cit] . the method was originally proposed for flat clustering, and to the best of our knowledge, it has never been used for constructing a data hierarchy. the proposed method consists of three major steps. the first step is to select s landmark sequences from a dataset"
definition 3. a landmark set s satisfies the landmark spread property if for any g i there exists a landmark in s with a distance smaller than 2d crit to a certain point in g i .
"instead of attempting to solve these heb problems with mathematical methods and optimisation skills (which is sometimes impossible), engineers can obtain the optimal solutions directly from nature. living organisms in nature often face optimisation problems similar to those of engineering products. to survive in a competitive environment, a living organism must optimise in many aspects. biological examples in nature display their forms explicitly, which are the optimal solutions for many optimisation problems."
"step 4: state the hypotheses. the optimal solution (the shape of dragonfly wing cross-section) is defined as a factor. we tests whether this factor contributes to the excellent aerodynamic performance. since there is only one factor, two sample hypothesis testing is conducted. in this case, the aerodynamic performance of the airfoil is measured by the lift-to-drag coefficient. two notations used in the hypothesis statements are defined as follows: μ 1 : the mean lift-to-drag coefficient of the dragonfly wing. μ 2 : the mean lift-to-drag coefficient of an artificial wing with different shape."
"a dozen of methods have been proposed in the last decade for de novo otu picking of 16s rrna sequences [cit] . yet, the computational burden of generating clusters from massive sequence data remains a serious challenge, and only a few algorithms are able to handle millions of sequences. based on the structure into which generated otus are organized, existing methods can be generally divided into two categories: hierarchical clustering (hc) and greedy heuristic flat clustering. hc is one of the most widely used approaches for sequence binning [cit] . it organizes sequences in a hierarchical tree, enabling researchers to examine otus at various similarity levels that may bear biological significance. a major drawback of hc is its extremely high computational complexity stemming mainly from the need of computing and storing a pairwise distance matrix, making it unsuitable for large-scale sequence analysis. various data preprocessing heuristics have recently been proposed that were proven to be very effective in reducing the computational complexity of a clustering process [cit] ). yet, these heuristics do not fundamentally change the nature that hc is an oðn 2 þ algorithm. as a trade-off between computational efficiency and accuracy, several heuristic methods including cd-hit [cit] and uclust [cit] were proposed that employ greedy flat clustering to reduce the computational complexity associated with sequence binning. the basic idea is to process input sequences sequentially, by either assigning each sequence to an existing cluster or designating it as the center of a new cluster if the distances between the sequence and the centers of all existing clusters are larger than a pre-defined threshold. as such, heuristic methods calculate only the distances between input sequences and cluster centers, and run much faster than hierarchical clustering, though at the cost of decline of clustering quality [cit] . however, the sizes of otus generally exhibit a long-tailed distribution [cit], meaning that there are a few large otus and a large number of small otus, and when processing massive sequence data, the number of otus is non-negligible. consequently, existing heuristic methods are still not sufficient to handle extremely large datasets. as high-performance computing systems are becoming widely accessible, it is highly desired that a clustering method can easily scale to handle massive sequence data by leveraging the power of parallel computing. however, efficient parallelization of sequence clustering is inherently difficult. for example, for uclust and cdhit, distance calculation in each iteration depends on cluster centers generated in previous iterations; for hierarchical clustering, each merging or dividing operation relies on the results of all previous merging or dividing operations. several attempts, including hpc-clust [cit], dace [cit] and subsampled open-reference clustering [cit], have been made to speed up a clustering process by utilizing the power of parallel computing. however, existing methods do not sufficiently address the computational issue associated with large-scale sequence analysis. hpc-clust takes a pre-aligned profile as input, which is computationally very expensive to calculate. for dace, data partition relies on locality sensitive hashing (lsh) for approximate nearest neighbor search. while there exist a number of hash functions designed for various similarity measures [cit], it remains an open problem to perform lsh on sequence alignment distances. subsampled open-reference clustering first generates cluster centroids by using randomly sampled sequences and then assigns remaining sequences to the centroids in parallel. however, it did not fundamentally address the issue of clustering parallelization, since the clustering process performed on sampled sequences remains a single-thread procedure and becomes the performance bottleneck when the number of sequences becomes excessively large."
we performed a large-scale experiment to demonstrate that the proposed framework can significantly speed up various commonly used methods for de novo otu picking and meanwhile maintain the same level of accuracy.
"this article studies the procedure and methodology for discovering and applying optimal biological solutions. although some procedures and methodologies for biologically inspired design have been presented in the literature [cit], most of them are methodologies designed for general biologically inspired design, not specifically for obtaining, verifying and reusing optimal biological solutions."
"a systematic biologically inspired design procedure for optimisation solution discovery and reuse is crucial because it helps designers comprehend biological forms as well as facilitates the development of new technical solutions and industrial products. because no procedure has been specifically proposed for optimal solution discovery and reuse in biologically inspired design, this study fills this research gap by presenting two methods for obtaining, verifying, and reusing biologically optimal solutions. the first method develops an analytical model, formulates an optimisation problem explicitly, and then theoretically verifies the optimal solution. when theoretical and analytical models are difficult to obtain, the second method is applied, which is based on experiments and statistical analysis. it uses experimental design and hypothesis testing to verify the optimal solution, and does not require the explicit mathematical formulas or full understanding of the biological system's mechanism. these two methods are applicable to different biologically inspired design situations. through the two methods, scientists and engineers can efficiently obtain, verify, and reuse the optimal solution from biological organisms."
"the third approach is information content assessment, for similar designs that use identical components [cit] . this approach associates each critical characteristic of a physical part with a value representing the information content, which is a measure of probability that the part will satisfy the performance requirement. then the performance of a physical part or a product is predicted. this method requires identical components to be used for the new design and relationships between the components and the product performance to be established."
step 6: conduct experiments on the biological example with the factors and on the artificial model without the factors. the biological example used in the experiment can be a real organism or a physical model built by the researcher that is analogous to the real organism and equipped with the investigated factors. the artificial model without the factors is a man-made model deriving from existing product designs or other common industry designs.
"we implemented the proposed method on apache spark v2.0.2 by using the scala programming language v2.11.8. apache spark is a fast and general engine for large-scale data processing, providing researchers with an interface for programming entire clusters with implicit data parallelism and fault-tolerance. it can run on hadoop, mesos, standalone, or in the cloud, and can access diverse data sources including hdfs, cassandra and hbase. most existing parallel de novo otu picking methods utilized message passing interface (mpi) for speed-up in a distributed computing environment [cit] . while mpi enables the message communication between computational nodes via network, it lacks job scheduling and fault recovery. since our method can be easily fit into the mapreduce model, the low-level flexibility offered by mpi becomes less appealing. by using high-level and portable apache spark, our method is scalable, fault-tolerant and compatible with different file systems. apache spark also supports several programming languages, including python, r and scala. we chose scala since apache spark focuses on data transformation and mapping concepts, which are flawlessly supported by functional programming languages including scala. moreover, scala is a jvm native language and thus is much more efficient than python and r in spark. another advantage of using apache spark is that it is equipped with a bunch of built-in libraries."
"we observe the dragonfly wing and also learned from previous studies on this object. many features in a dragonfly may contribute to excellent fluid aerodynamics of flapping wing in ground effect. among them, the shape of dragonfly wing cross-section is chosen as a possible optimal solution because it plays a significant role in aerodynamic performance."
"has a theoretical linearithmic time complexity with respect to the number of input sequences. in addition, we have demonstrated that the proposed method can efficiently process ultra-large-scale sequence datasets by taking advantage of parallel computing resources with the implementation on apache spark."
"step 9: interpret the reason. the drag and lift forces acted on the flapping foil are dependent on the shape of the airfoil. however, we have not fully understood the mechanism and could not build an analytical model yet. more detailed exploration of the force behaviour of the flapping foil in ground effect should be done in order to understand the mechanism."
"we developed a new computational framework for the parallel de novo clustering analysis of ultra-large-scale sequence data, a task currently computationally intractable with conventional methods. the basic idea is to first partition data into small parts by using an incomplete hierarchical divisive tree, then process each part by using a user-chosen otu picking method, and finally assemble individual clustering results to form the final output. figure 1 presents the flowchart of the proposed method, and the pseudo-code is given in algorithm 1."
"to further demonstrate the scalability of the proposed method, we conducted an experiment on the entire emp dataset. to our knowledge, this is the largest de novo 16s rrna sequence clustering analysis ever performed in a distributed computing environment. we first transferred the data to amazon web server (aws) s3 and requested a computing cluster consisting of 17 m3.xlarge (intel xeon e5-2680 v2 ivy bridge processors, 4 cores, 15 gb memory) amazon elastic compute cloud (amazon ec2) instances. the apache spark computing environment was then set up via aws elastic map-reduce service (emr) v5.6.0. the cluster was launched in a client mode, where 16 slave instances were used for computation and a master node was used for monitoring. the memory limit was set to 10 473 mb for the master node and 9 658 mb for the slave nodes. the termination radius parameter and the distancelevel parameter of uclust were the same as above. we also set the number of sub-clusters to 300 for an early termination. the toplevel partition phase took 533 min and the sub-clustering phase took 536 min. the total running time was $17.8 h. in contrast, it has been estimated that the running time of uclust applied to a subset of the emp dataset that contains $660m sequences is 150 days on a single computer [cit] . we have previously shown that the empirical computational complexity of uclust is oðn 1:2 þ $ oðn 1:3 þ [cit] . thus, if uclust were applied to the entire emp data, it would take $636 days."
"the rest of this article is organised as follows. sections 2 and 3 discuss the first and the second methods, respectively. these sections discuss the procedure, requirements, and advantages of each method; an application example of the first method and a case study of the second method are also provided. the final section summarises the contribution of this article and suggests future research directions."
"in this paper, we proposed a general-purpose computational framework referred to as slad (separation via landmark-based active divisive clustering) that can in principle be used to parallelize any single-thread de novo otu picking method. theoretical analysis was performed that showed that the proposed method has a linearithmic computational complexity and can recover the true clustering structure with a high probability under some mild assumptions. we implemented the proposed method on apache spark, which allows us to easily and fully utilize parallel computing resources. experiments performed on various datasets demonstrated that slad can significantly speed up a number of commonly used de novo otu picking methods while maintaining the same level of accuracy. finally, we conducted a scalability study on the earth microbiome project (emp) dataset [cit] ) ($2.2b reads, 437 gb). to our knowledge, this is the largest de novo otu picking analysis ever performed in a distributed computing environment. by using 17 computer processors provided by amazon cloud, our method coupled with uclust finished the analysis of the 2.2b sequences in $17.8 h. in contrast, it was estimated that it would take uclsut $636 days to finish the analysis on a single computer."
"method 1: learn from biologists. search biology journal papers, reports, books, and dictionaries for biological examples. to seek optimal biological solutions, the researcher should search by the keywords of the optimisation problem (the keywords are the objective function, the constraints, and the decision variables). sometimes, the keywords of optimisation problems differ from the terms that are used commonly in biology. in that case, the synonyms of the keywords should be used instead."
"step 7: collect data and compute the test statistics. the sample performance indices from both the biological example and the artificial model are collected. the researcher determines the level of significance α, appropriate test statistic and other critical values used for hypothesis testing. test statistics are calculated through statistic methods."
"the first method has the following advantages. first, it saves time and resources required in modelling and testing a product prototype by reusing the optimal solution of biological organisms. second, it can solve the heb problem effectively. third, it helps researchers understand the biological example/system and establish new scientific theories by developing an explicit analytical model. finally, the optimal parameters for a new product can be obtained by solving an explicit optimisation problem."
"compared with the first method, which uses theoretical analysis, the second method, with an experimental approach, has many advantages. first, it saves the time, resources, and effort that are required to study a complex biological system/mechanism. for example, in the case study, it is difficult to model the aerodynamics of the flapping wing in ground effect due to the complicated shape of the dragonfly wing. it may take years to fully understand the mechanism. alternatively, we have applied the second method to verify the optimal solution through experiments. the process only took several months. second, the second method helps designers reuse the optimal form of a biological example even if the mechanism and theories in this area are undiscovered or undeveloped. third, no explicit mathematical problem needs to be written, and no heb problems need to be solved; thus, an engineering designer without advanced mathematical knowledge or optimisation skills can apply this method. fourth, it directly proves the optimality of the solution through experimental data, and the conclusion is statistically significant. fifth, designers can learn and reuse the most complicated form without fully understanding the mechanism or deriving mathematical functions to describe them. finally, it directly tests the product performance on the basis of experimental data, which are more understandable than the optimal solution that is based on mathematical functions."
"under the following terms: attribution -you must give appropriate credit, provide a link to the license, and indicate if changes were made. you may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use."
"the second one is a meta-model-based approach (surrogate model), when the explicit functions are not available. a surrogate model based on a few real case data is built and simulation experiments are used to establish the relationship between the performance and input design parameters. then the optimisation is performed on the surrogate model. [cit] . however, the experiment is possible only when the shape and structure of the physical part have already been designed. even if such a relationship is established, the real global optimum may not be obtained with a small number of experiments."
"where p is a measure of mass efficiency, f is the function requirement, d describes the dimensions (width and height) of the cross-section envelope, s describes the shape of the cross-section, and m describes the material properties. f, d, s and m are four decision variables. the researchers attempted to determine efficient structures that can minimise mass and maximise p. due to lack of knowledge of the form, shape and structure of the cantilever, the initial problem definition was not explicit."
"method 2: search a nature-inspired solution database. [cit] . however, to the best of our knowledge, there is no database specialised for searching optimal solutions."
"we finally conducted a large-scale scalability study on the emp-755 and entire emp datasets. for computational considerations, only uclust was tested. to investigate how the running time of uclust with and without slad grows with respect to the number of input sequences, we randomly sampled various numbers of sequences (5m, 10m, 15m, 20m, 25m, 30m, 35m, 40m, 45m, 50m) from the emp-755 dataset. the termination radius parameter used in the top-level partition in slad was set to 0.20 as per the parameter sensitivity analysis, and the distance-level parameter of uclust was set to 0.03. the experiment was performed on a 4 â 2.40 ghz intel xeon e5645 processor machine. figure 4 reports the running time of uclust with and without slad. since uclust applied to 40m, 45m and 50m sequences did not finish in 72 h, the results are not reported. with only one exception (5m), slad accelerated uclust by more than one order of magnitude. also note that the running time of uclust with slad grows much more slowly than that without slad with respect to the input data size. this suggests that the proposed method has the potential to achieve even more speed-up on larger datasets, as shown below."
"nevertheless, applying this method generates some requirements for designers. first, they should be knowledgeable in biology in order to find biological examples in which optimisation problems similar to those of artificial products are faced. in addition, they should be able to efficiently develop an appropriate model with a design that is analogous to the real biological example, and design experiments to measure and test the performance of both the model and the biological example. furthermore, they should be knowledgeable in statistics and be able to select appropriate statistical approaches to analyse the experimental data as well as perform hypothesis testing or factorial design."
"step 7: apply the biological solution to product design. by using knowledge of the forms of the components or the product, the objective functions and constraints of the engineering problem (problem a) can be explicitly expressed as mathematical functions, which can substantially reduce the number of decision variables. apply the biological optimal solution to the engineering problem. some modification may be necessary due to the difference between problems a and b."
"the aforementioned three approaches are difficult to implement in practice. because the form and structure of a part of a product are unknown, engineers cannot write explicit forms of the objective and constraint functions for the optimisation problems. these problems are categorised as heb (high dimensionality, computationally expensive and black-box) problems, which widely exist in engineering design optimisation and feature high dimensionality (with many decision variables), are computationally expensive, and possess black-box properties (unknown function properties)."
"second screening: the keyword \"fluid aerodynamics of flapping wing in ground effect\" was used. however, this keyword is rarely used in the biology literature, so it was replaced by two synonyms \"flying efficiently\" and \"flying near the ground\". among the insects, dragonflies were chosen because they have an appropriate size and can fly efficiently near the ground and the water surface."
"the adjusted r 2 statistics for function (3) and (4) are 0.987 and 0.999 respectively. the large adjusted r 2 statistics imply that the regression models provide adequate approximation to the contour. you are free to: share -copy and redistribute the material in any medium or format adapt -remix, transform, and build upon the material for any purpose, even commercially. the licensor cannot revoke these freedoms as long as you follow the license terms."
the drag and lift coefficients in the forward flight mode are obtained through numerical simulations by the immersed boundary-lattice boltzmann method (ib-lbm). the drag and lift coefficients are given by these functions:
step 3: they observed that the tree branch develops a structure in response to mechanical loading. the optimisation of form occurs in the transversal shape: the lower region is larger than the upper region (more cells grow at the lower region).the shape of the cross-section of the tree branch is the possible optimal solution to the optimisation problem.
"however, because this method requires developing an analytical model for a biological system, it imposes the following requirements for the designers. first, they should be knowledgeable of optimisation (mathematics), biology/biomechanics, and relevant engineering areas (such as mechanics, aerodynamics, and hydrodynamics). in addition, they should be able to analyse the form of the biological example and prove the optimality of a solution for an optimisation problem theoretically."
"living organisms in nature display abundant optimal designs, many of which have been found and reused by humans. the application of these optimal designs results in many new and astonishing technical solutions, such as lightweight shape-optimised technical structures (modelled according to trees and bones) [cit] and streamlined shapes in technology for least resistance in travelling through air or water (modelled according to the trout or dolphin body) [cit] . the practice of using analogies to biological systems for deriving innovative solutions to difficult engineering problems is called biologically inspired design [cit], also referred to as biomimetics, biomimicry, biognosis, bionics, bioinspiration, biomimetic design, or bioanalogous design."
"in 16s rrna sequence analysis, the first major step after quality control is usually to bin sequences into taxonomic or genotypic units, which forms the basis for performing ecological statistics and comparative studies [cit] ."
"step 6: they used the functions and models to compare structures with different cross-section shapes. they proved theoretically that the cross-section of the tree branch has a better performance p than elliptical, rectangular, circular and any other cross-sections of man-made structures. for example, they proved that the tree branch saved 32% mass compared to a common rounded (circular) section for the same failure moment required. it is evident that the cross-section of the tree branch is the most efficient shape of the cantilever."
"in this paper, we have developed a novel two-stage parallel sequence clustering framework that addresses the computational issue of existing methods for ultra-large-scale sequence analysis. theoretical results have showed that our method can recover the true hierarchy with a high probability under mild assumptions and note: for the greengenes dataset, only uclust finished the analysis in 72-h wall-time limit. when a method was used with slad, the total running time is the sum of the time spent on top-level partition and sub-clustering. the experiment was performed on a 4 â 2.4 ghz intel xeon e5645 processor machine."
"step 10: apply the optimal solution to product design. the researcher applies the factors (the optimal forms, shapes, and structures of the biological example) to a new product. the factors may be modified according to some engineering requirements (such as cost, material, and manufacturing methods). the researcher develops a product prototype, tests the performance, and modifies the design until the design is satisfactory."
"step 5: design experiments. the researcher designs the experiments to measure the performance of the biological example with the factor and that of the artificial model without the factor. the performance is measured either through experimental instruments or through computer simulations. if there is only one factor, two sample hypothesis testing is used, which tests whether the means of the two populations differ. if there is more than one factor, then the experiment design is complex. in this case, the researcher studies the relationship between independent variables (factors) and their interactions with a dependent variable (the performance index). such an experiment design is called factorial design. for two factors, two-way anova is used, and for k factors, a 2 k factorial design, and more complicated methods are used. [cit] provided instructions and examples for factorial design."
"in the experiment, the reynolds number re is 730, the angle of attack is 10 degree, and the amplitudes of plunging and pitching motions of the flapping foil are fixed at 0.4c and π/4 respectively. 15 runs of the experiment are performed."
"lemma 5. spectral clustering can obtain a clustering over a landmark set, where landmarks whose nearest good points belonging to the same good set are grouped to the same cluster, and landmarks whose nearest good points belonging to different good sets are assigned to different clusters."
"existing methods can be generally classified into taxonomydependent approaches, where sequences are annotated against a reference database, and taxonomy-independent approaches [cit], where sequences are clustered into operational taxonomic units (otus) based on pairwise similarities without using external references (thus also called de novo otu picking). since the main goal of metagenomic studies is to explore uncharted biospheres where a significant portion of genetic material is contributed by previously unknown taxa, taxonomy-independent analysis is often the preferred, if not the only, choice."
"the optimal solutions (biological forms) of biological organisms can help solve difficult engineering optimisation problems. this article presents two methods for obtaining, verifying, and reusing the optimal designs from biological organisms. the first method develops an analytical model, formulates an optimisation problem explicitly, and then verifies the optimal solution theoretically. when theoretical and analytical models are difficult to obtain, the second method is applied, which is based on experiments and statistical analysis. engineers should select a suitable method according to their capabilities, resources and knowledge of the biological system. with the help of the two methods, engineers without much biologically inspired design training will be able to obtain, verify and reuse biological forms, shapes and structures to design high-performance products."
"step 6: verify the optimality of the solution for the biological optimisation problem. use scientific theories or mathematical algorithms to verify the optimality of the biological solution for problem b. if the optimality of the biological solution is not verified, then the researcher should search for a new optimal solution and repeat steps 2 to 6."
"the core component of slad is the procedure that partitions data into small sub-clusters (fig. 1), which has to meet two requirements. first, the partition process must be efficient; otherwise, the efficiency gain from parallelization is amortized. second, the partition result must be accurate, since all the downstream operations depend on the top-level partition. we developed a new method, referred to as landmark-based active divisive clustering (ladc), that achieves the above two goals simultaneously. below, we give a detailed discussion of the proposed method."
step 1: define an engineering optimisation problem. the objective of the problem is to design the wing shape of mavs to achieve excellent aerodynamic performance in ground effect (reduce the drag coefficient and enhance the lift coefficient) in the forward-flight mode.
"when evaluating an otu picking method for sequence analysis, clustering accuracy and computational efficiency are two major considerations. accordingly, four datasets were used in the experiment. the first dataset was generated from oral plaque samples that cover the v3-v4 hyper-variable regions of 16s rrna gene. to generate species-level taxonomic labels for the dataset, we performed blast search against the homd database [cit] and annotated each sequence by using a stringent criterion: the identity percentage !97% and the length of the aligned region !97% of the total length. a total of 410 600 sequences were confidently annotated at the species level. the second dataset is the greengenes database [cit], which is one of the most commonly used databases for 16s rrna gene sequence annotation and contains 1 269 986 taxonomically labeled sequences spanning over the v1-v9 hyper-variable regions. the third dataset, which contains 66 520 485 sequences of the v4 region, comes from a study of a water purification system [cit] . since it is one of the studies performed in the earth microbiome project (emp) [cit] (study #755), we refer to it as the emp-755 dataset. the fourth dataset is the whole emp dataset, consisting of 27 751 samples from 97 studies. the dataset has $2.2 billion v4 16s rrna sequences and is probably the largest publicly available 16s rrna sequence dataset."
"where f d and f l are the drag and life forces acted on the flapping foil, c is the chord of foil, ρ ∞ is the density of the fluid (air) and u ∞ is the free stream velocity. f d and f l are obtained by solving a mathematical equation system. [cit] for the details of the method."
"step 3: observe the biological example and select an optimal solution. select one champion from the biological examples. observe the form, shape, structure, and other features of this example. these are the possible optimal solutions for the optimisation problem."
"in this section, we present an analysis that provides theoretical guarantees for the proposed method on both accuracy and efficiency. we start by introducing some notations and definitions used in the analysis. definition 1. a hierarchical clustering t on a dataset d is a collection of non-empty clusters that satisfy the following three con-"
"this work was in part supported by 1r01ai125982 (ys, rg, jww), 1r01de024523 (jww, rg, ys, mb, wz) and national science foundation of china (yc, grant # 11471313)."
"empirical modelling is another commonly used statistical method in studies. through empirical modelling, scientists conduct experiments, collect data, and perform regressions to build an empirical model, which expresses a dependent variable (the performance index) as a function of several independent variables (the factors). the most common empirical model is a multi-regression linear model. although empirical modelling is widely used in physics, biology, and other fields, it is rarely applied in biologically inspired design. the reason may be as follows. regression modelling requires scientists to estimate the relationships between the dependent variable and independent variables before the experiment. the estimation is easy when the relationship forms are simple, such as linear, quadratic, and exponential. however, in most biological organisms, the relationship forms are very complicated and difficult to estimate correctly. because there are scarce examples of using regression modelling in biologically inspired design, no empirical modelling method is proposed in this article."
"observe the example directly from nature. [cit] observed that most of the birds soaring over the land exhibit characteristic slotted wingtips. to apply this method, the designer may begin with a broad search space, and then screen the examples and gradually reduce the search space. the screening aims to find candidates with high potential to display the optimal solution for the engineering problem."
"first screening: the keyword \"flapping wings\" was used. many living creatures, such as birds, insects, and mammals, have flapping wings. among them, insects were chosen because they are likely to result in more ornithopter-like wings and wing actuation."
step 7: the shape of the cross-section of a tree branch is the optimal solution to the engineering design problem and could be applied to design efficient cantilevers that support the bending moments.
3.1 adaptive landmark selection phase definition 2. an instance ðd; dþ satisfies the ð1 þ a; þ-property for the k-median objective function u with respect to the target clustering c t if any clustering c with
"step 5: design experiments. the drag and lift coefficient in the forward flight mode will be obtained through numerical simulations. the recently developed immersed boundary-lattice boltzmann method (ib-lbm) is adopted, which has been successfully employed to simulate various moving boundary problems. the details of the method are in appendix 1. in the experiment, the reynolds number re is 730. 15 runs of the experiment are performed."
"three commonly used criteria to assess clustering performance, normalized mutual information (nmi), random index (ri) and adjusted random index (ari), were also implemented and included in this package."
"first, for each gene set, we derive a local expression matrix, denoted as x i of size nxp i, from the user-provided gene expression data by extracting only those genes in the gene set. p i should be smaller than or equal to the number of all genes in the gene set, since some genes may not be expressed in the sample under investigation due to the known tissue-specific expression of genes. in single-cell studies, a large percentage of genes may be undetectable or undetected, leading to sparse datasets. then, we perform hierarchical clustering on x i using the hclust r function, and obtain the cluster membership of each sample using the cutree r function by setting the number of required clusters to user-specified k. often, users test several k values and select an optimum based on expert knowledge. based on the cluster membership, we can calculate a consensus matrix s i of size n x n with its element at the jth row and mth column set to 1 if sample j and sample m belong to the same cluster and 0 otherwise. then, we evaluated the class separating performance of the gene set using fisher discriminant analysis (fda). for each of the k(k-1)/2 pairs of clusters, we calculated the ratio of between-group to within-group variance. we then use the average of all the k(k-1)/2 ratios as a score (ar-score) to measure the cluster-separating importance of gene sets. since the local expression matrix x i may have many more genes than samples, the co-variance matrix will be ill-conditioned and fda models can not be accurately calculated. also, over-fitting can easily occur when there are more genes than samples. to make fda computable and reduce the risk of over-fitting, principal component analysis (pca) is applied to reduce data dimension by decomposing x i using pca and choosing only the top ranked principal components that explain a predefined percentage, say 90%, of total variance as the input of fda."
"gene sets in msigdb were used in our study. of the seven classes of gene sets (c1 to c7) in msigdb [cit], we mainly considered three classes of gene sets (c2, c5 and c6) in our study. the reasons are that: (1) c2 represents curated gene sets from the biomedical literature, online databases such as kegg and knowledge of domain experts, and are considered of high quality; (2) c5 collects gene sets in gene ontology which represent normal biology and are widely used in biological and bioinformatics studies; and (3) c6 is a collection of signatures of cellular pathways that are often dys-regulated in cancers and is therefore suitable to interrogate gene expression of cancer data. the reasons we do not consider c1, c3 and c7 are that: c1 (positional gene sets) and c3 (motif gene sets) do not describe biological functions, the focus of our study; c4 (computational gene sets) focuses on computationally derived gene sets and is thus less confident than experimentally validated or curated gene sets; c7 is specific to immunologic pathways. however, depending on interest and purpose, any gene set can be used for clustermine."
"summing up, clustermine is a competitive and complementary tool for clustering analysis. we expect that it will find many more applications in bioinformatics and biomedical research."
"clustermine also has limitations. firstly, since gene products, especially in higher organisms such as humans, are a mixture of isoforms generated by the alternative splicing mechanism [cit], isoform-level gene expression would promise more accurate clustering. because our method is dependent on function annotations and current functions are mainly only available at the gene level, clustermine can only analyze gene-level expression data and is not able to directly take isoform-level expression as input. other methods that are not dependent on gene function annotation such as the consensus clustering (cc) method can analyze both gene-and isoform-level expression data. with the accumulation of experimentally validated isoform function annotations or using predicted isoform functions [cit], clustermine can then be extended to isoform-level data analysis. secondly, compared to methods such as cc, clustermine is computationally more extensive since it computes a similarity matrix for each gene set. taking the c2 collection of 4378 gene sets as example, a total of 4378 similarity matrices are needed to be computed. as a result, clustermine often runs slower, though the time to run clustermine is not long (seconds to minutes in our analyzed data)."
"clustermine was implemented as an r package. all the seven classes of gene sets in msigdb (version 6.0) [cit], a rich compilation of gene sets about normal biology or cancers derived from go, kegg, literature mining, etc., were built into the package to facilitate users' interrogation of gene expression data with respect to their functional gene sets of interest."
"in addition to these two inputs, the number of clusters, denoted as k, needs be specified by the users based on their prior knowledge or optimization by computationally approaches to run clustermine. the algorithm of clustermine is detailed in the following."
"(1) gene sets that represent known knowledge about biological functions of genes, which can be obtained from commonly used databases such as go, kegg, msigdb [cit] . since both go terms (biological processes, molecular function, cellular component) and kegg pathways are included as part of the msigdb database, we here only consider the gene sets annotated in msigdb [cit] . in this database, gene sets were organized into seven functionally different collections [cit] : c1: positional gene sets; c2: curated gene sets; c3: motif gene sets; c4: computational gene sets; c5: go gene sets; c6: oncogenic gene sets; c7: immunologic gene sets. to facilitate the use of these gene sets, we have downloaded them and built them into the clustermine r package. users can choose which gene sets to analyze, without having to download them from the msigdb website."
"we developed clustermine as a new tool to perform cluster discovery, which features in the integration of known gene sets representing our knowledge about gene functions. another feature of this method is its ability to single out gene sets that contribute most to the separation between clusters by using fisher discriminant analysis. this method is implemented as an r package. all the gene sets in msigdb(v6.0) were built into this package to facilitate use of our method."
"we tested the performance of clustermine on three cancer datasets, two scrna-seq based cell differentiation datasets, one cell cycle dataset and two datasets of cells of different tissue origins."
"the clustering performance of cc and clustermine is shown in table 1 . for nmi, our method outperformed cc on the c2 and c5 classes of gene sets, with slightly lower nmi values on c6. in terms of ri values, our method showed significantly better results on all the three gene sets. this result suggests that our method is promising in distinguishing cells at different phases."
"further, for each of the above three datasets, we sorted all gene sets in c2, c5 and c6 by their ar-scores, and provided the top 5 ranked gene sets in table 2 . these gene sets are most likely associated with the disease under investigation, and may be a useful resource for the community."
"for the garber data (lung cancer), we first compared the performance of our method with state-of-the-art consensus clustering (cc) in terms of nmi and ri. the number of clusters for both methods was set to 3, corresponding to the three subclasses of sample. the nmi and ri by cc and clustermine (based on c2, c5 and c6) are listed in table 1 . compared to cc, clustermine achieved a 5%-8% positive increment of nmi on c2 and c6 datasets, but decreased by 0.6% on c5, with similar observations using the ri criterion. thus, clustermine achieved on average a better performance than cc."
"at the core of clustering analysis approaches is the calculation of sample similarity. in the above-mentioned general-purpose or scrna-seq specific approaches, the sample similarity is often calculated using holistic gene expression profiles or optionally a percentage (say 80%) of randomly selected genes, as in cc. based on the common assumption that only a small percent of genes are differentially expressed between conditions such as disease or cell subtypes [cit], the resulting sample similarity may be noisy because its calculation involves a large number of genes that are not relevant to the biological differences under investigation. therefore, the similarity based on holistic gene expression profiles may not accurately reflect the biological differences between sample clusters. another limitation of the holistic gene expression-based similarity is that clustering results are often hard to interpret, because all genes are used and genes that contribute most to sample clustering are not prioritized and supplied to users."
"(2) a gene expression matrix with n samples and p genes, which can be any gene expression data of interest. for example, they can be microarray or rna-sequencing data, can be from bulk-sample or single cells, can be from healthy or disease samples."
"to intuitively visualize the clustering of the samples, the heatmap of cluster together with the gene set weight plot was displayed in figure 2c ."
"as can be seen from our analysis, different gene sets could lead to different results. therefore, the performance of clustermine is dependent on the selection of gene sets. in general, users can choose to test gene sets of their interest. in the case of no prior knowledge, it is our recommendation that c2 (curated gene sets) and c5 (commonly used gene sets in gene ontology) be tested. in the case of cancer-related datasets, c6 (disturbed pathways in cancers) can be tested. in special cases of studying immunity-related biological questions, c7 can be used. as an option, users can also combine all the seven gene sets as input for clustermine, but using all gene sets does not necessarily or reliably improve clustering performance, since many gene sets irrelevant to the clustering are involved in the assessment of between-sample similarity and thus introduce noise. here, feature selection approaches, though beyond the scope of this report, can be used to select a subset from all the combined gene sets, followed by feeding the selected subsets again to clustermine for clustering analysis."
"by analyzing two cancer datasets and three scrna-seq datasets, we found that clustermine showed similar or better performance in terms of nmi (normalized mutual information) and ri (random index) compared to the commonly used consensus clustering (cc) approach. in addition to providing cancer subtypes or cell types in the given dataset, our method also analyzes gene sets that are biologically relevant. this feature is essential because it can guide us to infer the biology behind clustering and to propose hypotheses for further testing."
"given a gene expression dataset of n samples and p genes, the goal of clustermine is not only to detect clusters of samples sharing the same gene expression profiles but also to prioritize functional gene sets that are most likely to underlie the separation between clusters. as depicted in figure 1, two inputs are required to run clustermine, as detailed below:"
"clustering samples based on their gene expression profiles plays a key role in understanding complex biological data by grouping samples into clusters that show more nearly homogeneous biological pathways. existing approaches like consensusclustering often use the full gene expression profiles (or a randomly sampled proportion, say 80%, of them) for clustering analysis and do not report which gene sets most likely underlie the differences between clusters [cit], thus leading to limited interpretability of the resulting clustering."
"for the results listed in table 4, we can observe that number of embedded watermarks increases when the data size of the maps becomes larger, and all watermarks can be detected from the watermarked maps. the proposed algorithm is superior to the reference [cit] algorithm in terms of multiple watermark capacity, primarily because the proposed algorithm uses the dichotomy method to subdivide the embedding region, while the reference [cit] algorithm adopts the quad-tree method. however, due to the dichotomy method, the watermark number is not directly proportional to the map data size. for example, map 3 and map 4, which have different number of vertices, can be accommodated 4 watermarks. for the subdivision method shown in fig. 4, the fifth block is 1/8th of the domain, so the coordinates in the fifth block are the 1/8 of 1224, i.e., 153. in addition, 153 is fewer than the length of the watermarks, 200, which means that the fifth watermark can not be embedded in the fifth block. for this reason, map 4 accommodates only 4 watermarks instead of 6 watermarks. consequently, the multiple watermark capacity is related to the data size and the length of the watermark. additionally, the multiple watermark capacity of the proposed algorithm is sufficient for vector geographic data with common data sizes."
"in particular, we consider a decision support system that was developed using a support vector machine (svm), which is one of the machine learning tools which has been widely used to predict various diseases in biomedical engineering [cit] . typically, using an svm consists of two different phases, namely training and testing. during the training phase, a classifier will be trained using features of the training dataset belonging to different classes. in the testing phase, any unlabeled data sample can be classified and labeled to the corresponding matched class using the trained classifier. in the current setting, the available clinical dataset can be used to train a classifier and the trained classifier can be used as a clinical decision support system during the testing phase to make the decision for the patient data."
"in order to reduce the conservatism, it is worth noting that in this work, the l 2 performance problem is solved only when the input saturation is not activated. actually, this is relevant in reality because in the presence of actuator saturation, the main concern is to guarantee that the trajectories are bounded and that the state constraints are not violated."
"the recent advances in remote outsourcing techniques (i.e., cloud computing) can be exploited in healthcare to provide efficient and accurate decision support as a service. this service could be utilized by any clinician in a flexible manner such as on-demand or pay-per use [cit] . within this context, let us consider the following scenario: a third party server builds a clinical decision support system using the existing clinical dataset (i.e., assume that the server has a rich clinical dataset for a particular disease). now clinicians, who want to verify whether their patients are affected by that particular disease, could send the patient data to the server via the internet to perform diagnosis based on the healthcare knowledge at the server. this new notion overcomes the difficulties that would be faced by the clinicians, such as having to collect a large number of samples (i.e., a rich clinical dataset), and requiring high computational and storage resources to build their own decision support system. however, there is now a risk that the third party servers are potentially untrusted servers. hence, releasing the patient data samples owned by the clinician or revealing the decision to the untrusted server raises privacy concerns. this drawback can affect the adoption of outsourcing techniques in healthcare [cit] . furthermore, the server may not wish to disclose the features of the clinical decision support system even if it offers the service to the clinicians. hence, in this paper we propose a privacypreserving clinical decision support system which preserves the privacy of the patient data, the decision and the server side clinical decision support system parameters, so that the benefits of the emerging outsourcing technology can also be enjoyed in the healthcare sector."
"where sc is a parameter controlling the distortions to the cover data after embedding the watermark. % represents complementation. ⌊a⌋ is the greatest integer that is smaller than a. then, the mapping relationship between the logic domain in the kr_ith row and the kc_ith column and the watermark bit index, k, is"
"in the embedding process, the same watermark bit is repeatedly embedded into the coordinates that are in the block corresponding to a given index. additionally, correspondent watermarks are successively embedded in the blocks in every logic domain. these watermarks purportedly provide robustness against cropping attacks. for instance, in fig. 5, if the coordinates in the gray rectangles are cropped, the w 4 will not be removed. the embedding process is described as follows. first, the mapping relationship is established according to the section 2.2.1. second, every mapping logic domain is subdivided into n blocks according to the number of watermarks. finally, a quantization embedding algorithm is adopted in the program for embedding every watermark bit [cit] . the fig. 6 describes the embedding process for the watermark."
"for evaluation, we used a leave-one-out approach [cit], that is, one sample is removed from the dataset and all the remaining samples are used for training the svm. the removed sample will be used as patient data. this procedure will be repeated for a different left-out sample each time until all the samples are used. in order to analyze the proposed methods, we first conduct an experiment in the plain domain. later, we do the same experiment in the encrypted domain for various scaling factors."
"to preserve all watermarks in the watermarked data after cropping, we proposed a multiple watermarking algorithm to defend against cropping attacks for vector geographic data. the mapping relationships between the vertex coordinates, the logic domains, and the watermark bit indexes are first established. then, the logic domains are subdivided into blocks to embed multiple watermarks. since the mapping relationships are built before subdividing the blocks, the embedded watermarks are difficult to remove in a cropping attack. for the mapping and subdividing methods, high capacity is achieved as well."
"in this section, we analyze the performance of the proposed encrypted-domain algorithm. we compare the accuracy of the proposed encrypted-domain method with the conventional plain-domain method. for the experiment, we consider two datasets from the uci machine learning repository called the wisconsin breast cancer (wbc) and puma indian diabetic (pid) datasets [cit] . the wbc dataset contains 681 samples where 444 samples are benign (noncancerous) and 237 samples are malignant (cancerous), while the pid dataset contains 768 samples where 500 samples are malignant and 268 samples are benign. the number of features for each sample in wbc and pid datasets are nine and eight, respectively (excluding class label attribute). table ii shows some examples of training samples after normalization from the wbc and pid datasets."
"in this section, we develop an algorithm which utilizes the healthcare knowledge available in the remote location via the internet while preserving privacy. hence, we consider a clientserver scenario where the remote server uses (6) as a decision making tool. as shown in fig. 1, a clinician sends the patient data t over the internet and obtains support from the server to make a decision. however, the clinician is reluctant to reveal the patient data or the decision to the server due to privacy concerns. at the same time the server desires not to leak any parameter values of the classification function as this would be a breach of privacy of the training clinical data samples which relate to other patients. in this section, we show how to preserve the privacy of the patient data t and the decision from the server and the server side parameters from the clinician. first, let us explain the required building blocks in the next section."
"(9) although, the cs approach can be applied to many sparse signals, we concentrate on biomedical signals as a specific application for improving health care systems. the basic idea of cs approach is that when the biomedical signal of interest is sparse or compressible in some basis, relatively few wellchosen observations suffice to reconstruct the most significant non-zero components. thus, rather than measuring each sample and then computing a compressed representation, cs approach suggests that we can measure \"compressed\" random representation directly. consequently, the received vector in gw can be written as:"
". (9) ►the implementation of sensing matrix is simulated for gaussian distribution, sparse binary sensing, and uniform distribution. ►the sparse sensing matrix with nonzero entries equal to is used for sparse binary matrix [cit] . ►the permissible parameters were adopted of ieee802.15.3, ieee802.15.5, and ieee802.16e protocols which support low power communication in wbans [cit] . ►the random sensing matrix ф is applied to all the records of the mit-bih ecg database [cit] . ►the spgl1 (spectral projected gradient for l1 minimization) toolbox is used to determine large-scale onenorm regularized least squares in the following equation: subject to . (10) ►to validate the simulation results, the bpdq (basis pursuit dequantizer) toolbox is used for recovery of sparse signals from quantized random measurements to solve: subject to ."
"finally, to ensure that x(t) belongs effectively to s ρ (k, g, u 0 ) and that the state constraints are not violated, it must be proven that"
"from the results listed in tables 5, 6, and 7, the multiple watermark capacity is basically proportional to data size, which is accordance with the results in table 4 . according to the results in tables 4, 5, 6, and 7, multiple watermark capacity is inversely proportional to the length of the watermark. that is, the multiple watermark capacity becomes larger as the length of each watermark becomes shorter."
"the rest of this paper is organized as follows: in section ii, we describe the conventional svm, i.e., the steps involved in training the svm and classification in the plaindomain. particular focus is placed on the gaussian kernel method. in section iii, we first briefly describe one of the building blocks, i.e., the homomorphic encryption, and show how the svm classification can be extended to work in the encrypted domain. hence, the patient data can remain encrypted even when it is being processed by the server. in particular, the novel technique for scaling variables without deteriorating the performance and privacy is described in section iii-b. we analyze the performance of this encrypted-domain method in section iv. we review related works in section v. conclusions are discussed in section vi."
"the experiments were performed on different digital vector geographic maps in shapefile format, which were provided by geomarking company [cit] . the maps are organized as points, polylines, and polygons. figure 8 shows three examples of these maps."
"we have examined the benefit of cs theory, bsbl, and dta procedure for some records of normal and non-normal ecg signals. our future work involves developing the cs theory, bsbl, and dta procedure for other records of ecg signals."
"after watermark extraction, the watermark is detected based on correlated detection. let cor be the correlation coefficient between w j and w 0 j, which is shown in eq. (4)."
"in the setting considered in this paper, the clinician distributes a public key to the server while keeping his private key secret. the server is able to perform encryptions under this public key and exploits the homomorphic properties of the paillier cryptosystem to perform the required linear operations in the encrypted domain. however, only the clinician is able to decrypt any encrypted messages using his corresponding private key."
"note that, c 2 e c 3 −c 5, s −c 6, s, c 5,s, and c 2 e c 3 have, respectively, been used to scale the variables associated in (12)- (14) . variable c 6,s in (13) has been used to mask the value −γt t + 2γx s t. we generate fresh random values of c 6,s in the range of x s x s for different s values. this masking can be used to preserve the privacy of variables computed by the server. we explain this in detail later in this section. all the variables associated in this section are given in table i for convenience. now, the server needs to compute (15) followed by (16) to complete the decision-making process. the server knows all the variables associated with (12) and (14) in the plain domain; hence, it can easily compute (12) and (14) without interacting with the clinician. in order to obtain the whole decision function in (15), the server also needs to compute (13) . since the patient data, t in (13), are available to the server only in the encrypted domain, the server cannot directly compute (13) in the plain domain. to proceed, the server needs to normalize the patient data, then compute c 5,s + c 6,s − γt t + 2γx s t and finally the exponentiation. let us explain each step in the following sections."
"in this work, a lpv state feedback control was designed for the semi-active suspension control problem in order to ensure the stability in case of saturation and to improve the passenger comfort. moreover, thanks to the finsler's lemma, the proposed approach allows to use different lyapunov functions for multi-objective problems, which allows to reduce the conservatism. the simulation results show the effectiveness of this approach: the stability is kept in case of saturated input, the state constraints are not violated and the disturbance effects are minimized. for the future works, more performance objectives could be considered (road holding,...). moreover, the use of parameter dependent lyapunov functions could be a next step to improve the controller. for this, an appropriate bound on the derivative of the scheduling parameter,ρ, must be derived."
"there are three important conditions, which guarantee the correctness of this recovery. firstly, ф and θ must have the rip property with high probability. the rip property provides the theoretical guarantees to locate k-sparse. secondly, the number of random linear measurements, the number of coefficients, and the number of non-zero coefficients must satisfy the following equation:"
"as a robust watermarking algorithm, the robustness of the proposed algorithm was first analyzed with respect to the common attacks in the first experimental stage. following the robustness test, cropping attacks on the multiple watermarking algorithm were analyzed. in the second experimental stage the multiple watermark capacity of the algorithm was demonstrated."
"where a stands for matrices a, b 1,c 1, d 11 . then, provided that ρ is bounded in a polytope, the system s ρ can be written from a convex combination of the vertices s j ρ of a polytope of matrices as follows:"
"in the system, as shown in fig. 1, a clinician sends the patient data sample in the encrypted format to the server over the internet. then, the server exploits the paillier homomorphic encryption properties to perform the operations directly on the encrypted data, or if there are any operations that cannot be handled by homomorphic properties, then there will be a limited amount of interaction between the clinician and the server based on two-party secure computation protocols [cit] . we assume that both the parties will execute the protocol correctly to maintain their reputation; hence, we assume that they will behave in a semihonest manner, i.e., they are honest but curious, so privacy is a real issue."
that is why the cs theory and the collaboration from bsbl framework and dta procedure can provide new low sampling-rate procedure for normal and abnormal ecg signals due to the lake of research on this field. the main contribution of this paper lies in the use of cs approach and collaboration of bsbl framework and dta procedure to
"the emerging application of cs theory in medical areas has been potentially powerful to provide low sampling-rate ecg systems. however, the success of cs theory heavily relies on the sparsity of ecg signal. therefore, the cs approach is ineffective for non-sparse signals like abnormal ecg signals."
"(11) our simulation results show that by employing the cs the wbans can achieve a higher transmission, a lower time delay and higher probability of success of data transmission. therefore, a combination of cs theory to wbans is an optimal solution for achieving robust wban with low sampling rate and power consumption."
"the algorithm robustness for vector geographic data refers to the ability to detect the watermark after common operations [cit], such as data simplification, randomly adding or deleting vertices, deleting features (for example, a polyline in the map), and cropping."
"there are two drawbacks to the proposed algorithm. one drawback is that the multiple watermark capacity is decreased as the data size data becomes small, and another drawback is that the proposed algorithm lacks robustness against geometric transformation (such as rotation, scaling and translation). our intention for future work is to increase the multiple watermark capacity for vector geographic maps with few vertices and to improve the robustness against geometric transformation attacks."
"as it can be seen the biomedical signals are compressed by wireless sensors. the collected compressed biomedical data are then transmitted wirelessly to access points (aps) at hospital, ambulance, and helicopter [cit] . the aps are recovered compressed biomedical data for diagnostic and therapeutic purposes. the biomedical signal d has m-sparse representation in the proper basis and is expressed as:"
"we demonstrate the incorporation of a block sparse bayesian learning (bsbl) framework with cs and dta procedure to compress any ecg signal for normal and abnormal scenarios, including sparse and non-sparse signals to achieve better performance. the bsbl framework is partitioned the ecg signal into a concatenation of non-overlapping blocks, and a few of blocks are non-zero [cit] . more specifically, the number of non-zero blocks is the same as the number of random linear measurements in cs approach. the dta procedure can prune out blocks in abnormal ecg signals [cit] . therefore, if abnormal ecg signals have no clear block structure, the bsbl framework and the collaboration from cs theory is still effective to compress and recover abnormal ecg signals. the cs based on bsbl framework and dta procedure can compress normal and abnormal ecg signals with high probability and enough accuracy. in order to generate an approximate real-time transmission for collected the ecg signals the length of each block should be short [cit] . at the same time, we want to incorporate heartbeats in one block to recover the ecg signal with fewer samples. the compression ratio (cr), the structural similarity index (ssi), and percentage prd are employed as performance measures in our approach. the cr is found as follows [cit] :,"
"in order to complete the classification, the server needs to compute (16) . since d(t) is in the encrypted domain [i.e., (26) ], the server needs to obtain the sign of an encrypted number to complete this."
the state feedback gain k(ρ) that satisfies the stability condition for the saturated system (see section iv.a) and the disturbance attenuation for the unsaturated system (see section iv.b) can be derived by solving the following optimization problem:
"(19) note that every computation in (19) can be performed by the server without interacting with the clinician. now, the server can use the encrypted, normalized, and scaled patient data"
"► experiments are carried out over a 10-minutes ecg signal from mit-bih database. ► one hundred repletion's are averaged for our simulation results. to validate the simulation results ecg signals from records 100,107,115 and 117 of mit-bih are investigated. ►the mean of ecg blocks is rounded in the sliding window to the nearest multiple of 2 l, where l is the bsbl level. ►to simulate snr for ecg signals the following equation is used [cit] ."
"digital watermarking has been used to protect the vector geographic data for more than ten years. many watermarking algorithms have been proposed for vector geographic data [4, 9-12, 18, 19, 21, 22, 24, 25, 29] . with the rapid development of information technologies, more approaches are required for copyright protection. for example, data producers may update vector geographic data, and each version is marked with an identifier in the upgrade process. the data may be distributed to multiple parties in confidential environments, and every party marks the data to confirm authorization. therefore, it is necessary to embed multiple watermarks in the data. due to the data size of cover data, the number of embedding watermarks is restricted. meanwhile, cropping is a typical operation for vector geographic data. one or several watermarks in the data may be removed after a cropping attack. however, because of the requirement of multiple authorizations, any damage to any watermark is undesired. this means that all watermarks should be preserved after cropping the vector geographic data."
"in this work, we aim at enhancing the comfort evaluated in term of the vertical body acceleration. therefore, the minimization of the l 2 gain γ of the closed-loop transfer function from the disturbance w to the controlled outputz s is considered as the performance criterion. the control problem to be addressed can be therefore stated as follows: design a suspension control that improves the passenger comfort and satisfies the input saturation constraints (6) and the state constraints (7). to tackle this problem, we consider an lpv approach detailed in the sequel."
"depending on the separability of the available training data, the svm uses particular kernel functions such as linear and nonlinear kernels. if the number of features is larger than the number of instances, it is not necessary to map the data into higher dimensional space [cit] . it is because nonlinear mapping does not improve the performance. since medical datasets, in general, have less number of features than the number of instances, it is possible to get better classification results with nonlinear kernel-based svm. polynomial and gaussian kernels are nonlinear kernels. the polynomial kernel-based model is parametric, while the gaussian kernel-based model is nonparametric. in a way, a nonparametric model means that the complexity of the model is infinite, its complexity grows with number of instances. in contrast a parametric model's size is fixed, so after a certain point, the model become saturated, and giving more and more instances will not help. it means that the accuracy is dependent on the chosen degree of the polynomial. however, the gaussian kernel finds the best polynomial function in the infinite dimension for the given dataset. hence, we consider the gaussian kernel-based classification in this paper."
"this paper has presented a contribution of cs approach, dta procedure, and bsbl framework to establish a robust and low sampling-rate algorithm for ecg signals. as expected, the proposed algorithm exhibits better performance on snr, cr, and prd. our simulation results indicate that good level of quality of cr can be achieved when prd decreases by 35% and snr increases by 25% by employing the cs theory."
"2) computing (c 5,s + c 6,s − γt t + 2γx s t) in (13) : to do this, let us raise the power of (13) by c"
the remaining sections are organized as follows. section 2 presents the multiple digital watermarking algorithm. section 3 provides the experimental results of the algorithm. the conclusions are summarized in section 4.
"based on the experimental results listed in tables 1 and 2, we can see that the proposed algorithm has good robustness against the attacks, such as data simplification, vertex addition, vertex deletion, feature deletion, and cropping. in addition the algorithm is suitable for different types of vector geographic data."
"to the best of our knowledge, we present the first known privacy-preserving clinical decision support system for a gaussian kernel-based svm. in order to preserve privacy, we redesign the conventional gaussian kernel-based svm algorithm as an encrypted-domain algorithm using the paillier homomorphic encryption technique as one of its building blocks [cit] . since the paillier encryption supports only integers and the system variables are continuous and the gaussian kernel involves exponentiation of negative values, crucially we develop a novel technique to scale the variables, which overcomes these barriers without deteriorating the privacy and performance."
"in this paper, we have proposed a privacy-preserving decision support system using a gaussian kernel based svm. since the proposed algorithm is a potential application of emerging outsourcing techniques such as cloud computing technology, rich clinical datasets (or healthcare knowledge) available in remote locations could be used by any clinician via the internet without compromising privacy, thereby enhancing the decisionmaking ability of healthcare professionals. we have exploited the homomorphic properties of the paillier cryptosystem within our algorithm, where the cryptosystem only encrypts integer values. hence, we proposed a novel technique to scale the continuous variables involved in the process without compromising the performance and privacy. to validate the performance, we have evaluated our method on two medical datasets and the results showed that the accuracy is up to 97.21%. importantly, the benefit of our encrypted-domain method is that patient data need not be revealed to the remote server as they can remain in encrypted form at all times, even during the diagnosis process."
"we proposed a multiple watermarking algorithm for vector geographic data using vertex coordinates mapping and logic domain subdivision. the algorithm aims to improve the robustness against common attacks with an emphasis on cropping attacks. to address these issues, we first mapped the vertex coordinates to the logic domains. then, based on the dichotomy method and the proposed rules, each logic domain was subdivided into blocks according to the number of watermark, and multiple watermarks were embedded into the corresponding blocks one by one. in the experimental validation, the results showed that the proposed algorithm has good performance on the robustness and multiple watermark capacity."
". (11) the value of prd shows the quality of reconstruction approach. the relationship between the measured prd and diagnostic distortion is recognized on the weighted diagnostic data for ecg signals, which classifies the different values of prd based on the signal quality obtained by a specialist. table 1 illustrates the entire algorithm based on cs theory and bsbl framework. the main objective of this algorithm is to recover the original ecg signal with high probability and enough accuracy for patient monitoring purposes."
"the simulation results show the cs can reduce the effective ber by using adaptive sampling procedure to suit the transmission channel conditions which result in an increase in the higher packet transmission [cit] . higher packet transmission success probability reduces the packet delay as well as the power budget of an ecg signal [cit] . the power budget could be optimized by increasing successful packet transmission probability. the cs theory decreases ber for recovered ecg signal. our simulation results show this ability can minimize ber by 10%. fig. 3 ber for ecg signals in cs and non-cs scenarios. as depicted in figure 3 the ber with cs theory exhibits excellent robustness to be a convenient candidate for ecg signals. this capability allows the simplest operation and the smallest memory footprint in hardware designing. therefore, the use of cs theory ecg signals yields very good performance to recover original ecg signal for diagnostic and therapeutic purposes. the simulation results indicate that acceptable for ber can be achieved when cs is employed in ecg signals. figure 4 illustrates the sampling-rate for random binary matrix with cs theory. as depicted in figure 4, the sampling rate can be reduced by 75% of nr without sacrificing the performance. table 2 compares the simulation results on sampling rate and power consumption for random binary matrix with cs theory. table 2 indicates that satisfying quality on sampling rate can be achieved when cr does not exceed of 40."
where subtraction sets the least significant digits of z to 0 while the multiplication shifts the most significant digit down. since the z in (27) is in the encrypted domain the server needs to obtain thez in (29) in the encrypted domain. this can be performed as follows:
"to improve the multiple watermark capacity and the watermark detection reliability, we use pseudorandom binary sequence as the watermarks. the pseudorandom sequence builder is used to generate watermarks. in the watermark generation, the watermark seed is generated, usually a random integer, and then, the pseudorandom binary sequence to be used as a watermark is generated based on the watermark seed. figure 2 shows an example of the fig. 1 the proposed multiple watermarking algorithm relationship between the watermark seed, the watermark, the watermark bits, and the watermark bit indexes."
"in a multiple watermarking algorithm, the watermarks can be embedded into the cover data step by step or simultaneously. the approach that embedding the watermarks step by step can be used for multi-user tracking and is more flexible than embedding the watermarks simultaneously. therefore, we proposed a multiple watermark algorithm that embeds the watermarks step by step. figure 1 shows the whole procedure of the algorithm."
"svms have been widely used in machine learning for data classification [cit] . they have a high generalization ability which provides high reliability in real-world applications such as image processing, computer vision, text mining, natural language processing, biomedical engineering, and many more [cit] . the goal of an svm is to separate classes by a classification function, which is obtained by training with the data samples. we describe the classification function of an svm in the following section. this classification function is crucial to derive the privacy-preserving decision support system proposed in section iii."
"the range of vertex coordinates in the map changes with different maps. it is difficult to directly establish a stable relationship between vertex coordinates and watermark bit index. therefore, the logic domain is defined artificially, which means fixed domain where the vertex coordinates are mapped to. and the logic domain is used as mediation to establish the mapping relationships between vertex coordinates and watermark bit index. for embedding multiple watermarks in a 2d vector geographic map, the logic domains are subdivided into blocks to embed different watermarks. because the watermarks are embedded one by one, we use the dichotomy method to divide the logic domains for embedding watermarks as more as possible. the specific process is depicted as follows:"
"it should be noticed that under the input saturation, the state may become unbounded for large disturbances ( [cit] ). hence, in this work, we propose the design of a state feedback k(ρ) for the lpv system (13) in order to satisfy the following conditions:"
"in general, data classification is a combination of two phases: training phase and testing phase. the first phase, training a classifier, requires a large collection of data. there are various organizations which publish their customers' data for research and monetary purposes. publishing person-specific dataset (e.g., data related to patients of a cancer hospital) may reveal an individual's identity and breach the privacy of patients. however, there are various privacy preserving techniques (i.e., anonymization techniques and data perturbation techniques) have been well studied in the literature to preserve the privacy of individuals in the datasets (see [cit] and references therein). however, the proposed study in this paper considers the privacy in the second phase of the data classification task, where clinicians only require to send the test data of their patient to the remote server where the classifier is already established. since the proposed method preserves the privacy of the training dataset, it is possible for any organization with large data to provide a classification as a service to anybody through the internet rather than anonymize and publish the dataset in a plain domain. hence, our method is different from the data anonymization and data perturbation-based methods."
"the main problem in this architecture is lack of scalability. for example, to satisfy the growing demand by increased number of students, the administrator of the system will need to upgrade the server infrastructure and enable its robustness."
"regardless of serial or concurrent procedures, incorporating the lsm into gas faces two major challenges when applying to multi-objective reservoir operation under uncertainty. the first one is the exchange of objectives between the lsm and ga. on one hand, a multiobjective ga (e.g., nsga-ii) can directly handle multiple objectives of the reservoir operation by using the non-dominated concept to find the pareto optimal solutions [cit] . on the other hand, the lsm often scalarize multiple objectives into a single objective e.g., a weighted sum [cit] . it has been shown that most popular linear scalarizations fail to find global optima for non-convex problems [cit] . therefore, the linking of single objective in the lsm with multiple objectives in the nsga-ii is key for the success of hybrid optimization. the second challenge is that most of the lsm require first order or even second order derivatives to decide the search direction [cit] . this requirement largely limits the application of glsa for multi-objective reservoir operation under uncertainty as the derivative information may not always be available in the presence of discontinuous or noisy objective/constraints functions."
"we introduce resource allocation policy that determines required resources based on the number of students for particular assessment. moreover, it hosts only small amount of data required for particular e-assessment, i.e. teacher(s) and enrolled students of the course, exam questions and answers and other similar required data. this makes the system more efficient providing better overall performance."
"temporal inference is performed on the identified states, and prediction of the next possible state of the system can be achieved with the utilization of an n-order markov chain. prediction acquired in this way can be consequently used to identify specific patterns of behavior of the modelling problem under investigation. at the end of this process, the predicted vector can be compared with the actual vector, generated at each moment in a specific application context, in order to identify specific patterns or irregular behavior. this can be achieved with the use of distance function or basic ml techniques, like a multilayer perceptron."
"where v is reservoir storage; q in and q out are inflow to and outflow from reservoirs, respectively; δt is time step; h r is forebay elevation or reservoir water surface elevation; h rmin and h rmax are allowed minimum and maximum forebay elevations, respectively; qs is spill flow and qf is fish flow requirement; mop low and mop up are lower and upper boundary for the mop requirement on forebay elevation, respectively. q tb is turbine flow, q tb_min and q tb_max are minimum and maximum allowed turbine flows, respectively; q out_ramp_allow is allowed ramping rate for the outflow between any two consecutive time steps; h ramp_down is allowed ramping rate when reservoir water level is decreasing; tw ramp_down is allowed ramping rate for tailwater when tailwater level is decreasing; n d is power output, n d_min is minimum output requirement, and n d_max is maximum output capacity. the number of constraints is approximately 28,000 and many of them are highly nonlinear resulting in a complex search space."
"dementia is an age related, progressive, neurodegenerative condition, also considered to be one of the biggest global public health challenges that the current generation needs to face. the growing prevalence of diseases such as alzheimer's, and their impact on a society benefiting from greater longevity, is a critical health challenge of national importance. managing patients with dementia calls for improvements in effectively monitoring the progress of their condition, adjusting therapy interventions, and adapting to changing care needs . advanced deep learning approaches can be used to enable the development of context aware dementia monitoring, and predictive care recommendation solutions that can intelligently forecast behaviour changes of individual patients, from utilising contextual heterogeneous data, while handling uncertainties associated with incorporating qualitative data from stakeholders. human interpretable care recommendation decisions can provide care staff with the means of implementing evolving care and therapy plans, in response to the changing needs of patients due to the effects of cognitive decline."
"in the context of exploration and exploitation, search depth of the mads represents the extent of exploitation. in our study, the stopping criteria of bobjective difference^is implemented for determining the search depth of the mads. the mads search stops when the difference of the objectives between two consecutive iterations is smaller than 10 −3"
"assessment module is active only when e-assessment system is in active mode. it is devoted for assessments and its load varies depending on the number of assessments and assessed students. therefore, we propose this subsystem to be hosted on the cloud and to be dynamically allocated with resources."
"in this paper, we have discussed the importance of big data analytics and computational intelligence techniques. we provided a comprehensive survey of computational intelligence techniques appropriate for the effective processing and analysis of big data. we have presented a data modelling methodology, which introduces a novel biologically inspired universal generative modelling approach called hierarchical spatial-temporal state machine (hstsm). we investigated the benefits arising from the utilization of computational techniques namely deep learning neural networks, evolutionary algorithms, and fuzzy logic in big data analytics. we identified and highlighted potential novel real life cps applications arising from the vast amount of information on offer by modern high tech societies, the deployment of intelligent computational techniques, and state of the art solutions to address challenges in these application areas. in this work, a novel approach for big data modelling is presented. the proposed methodology relies on a hybrid method, which is based on the structure and functions of the mammalian brain. it incorporates different soft computing techniques and it has the potential to deal with large amounts of data, which are characterized by spatial-temporal correlations. this approach can tackle the high requirements and maximize the potential of dealing with big data and therefore can be considered as a state of the art tool for big data analytics. the potential benefits arising from this research are numerous and span over a large spectrum of application areas. utilizing this novel methodology to exploit big data's potential can lead to applications with significant impact to knowledge, society, economy, and individuals. scientific knowledge and research may benefit from revealing hidden patterns in big data or by delivering big data analysis results in ways, which can be easily visualized and interpreted. society could profit from the delivery of applications, which promote improved public transportations and health services. e-businesses and organizations could also be assisted through sentiment analysis tools, which contribute at the delivery of products and services which meet their customers' needs. finally, an individual may benefit through the development of personalized and contextualized products and services, which are able to account effectively for complex related notions, such as their cognitive/affective state. future work will involve the utilization of the proposed methodology to different application areas in order to create novel models and applications with significant commercial and scientific value and the further improvement of the developed systems."
in this section we describe several challenges of the third generation of e-assessment and the necessity to introduce a new fourth generation of e-assessment systems which we call eco-systems.
"every time when switching from the nsga-ii to the mads, the asf is formulated for each intermediate solution from the nsga-ii and the mads is started for optimizing the resultant single objective. it is noted that the number of intermediate solutions from the nsga-ii can be as many as the population number e.g., 50. therefore, for improving computational efficiency, a parallel computation approach was adopted by using the so-called bisland model^. each mads process is individually run until the stopping criteria i.e., a small difference in the objective is satisfied. each mads solution is collected and the corresponding result of decision variables is stored in a pool. the pool is then used to replace the current chromosome of the nsga-ii, i.e., solutions in decision space that were obtained by the nsga-ii."
"in order to facilitate the development of this fourth generation e-assessment system based on cloud computing, in this section we present the model of cloud eassessment system with organization of vm instances that host the previously described modules and the sequence diagram that presents activities related to provision and usage of cloud resources."
student agent based lms module is used for course student information gathering only. one agent is invoked for each course and this is then used for e-assessment via the vle subsystem and question bank lms. mis subsystem is used as manager to keep the assessment results temporarily. the main function is to keep temporary the results until they are saved within the main database reporting replication.
"e-assessment system works in two different modes, the active mode, when there is at least one active assessment and the inactive mode when there isn't any active assessment. inactive mode resource requirements are predictable since it depends mostly on the total number of all students and teachers. it can be easily determined if the load is simulated with some performance tester and the parameters of the previous years are used. active mode resource requirements are unpredictable since they depend not only from the total number of students and teachers, but mostly from the concurrent number of assessments and assessed students."
"big data analytics can also facilitate government's efforts towards delivering better services to their citizens. big data can aid governments in improving crucial sectors such as healthcare and public transport thus helping to shape a more efficient modern society. for example, big data analytics and computational intelligence techniques are able to provide intelligent solutions for challenging problems such as health shock prediction, or optimization of the public transport services delivered by the state to the population."
". other alternate options e.g., number of iterations can be used for determining the search depth; however, these are problem-dependent. the threshold on the objective difference is more general and flexible to different problem representations."
"for normalization purposes, where z max and z min are maximum and minimum objective values in the obtained solutions, respectively. it is usual and reasonable to start the nsga-ii first to avoid the local optima and then switch to the mads. therefore, the obtained solutions can be viewed as the current best solutions from the nsga-ii process. each point in the objective space of the obtained solutions is then selected as a reference point and consequently, the f r i becomes known."
"in big data analytics there is a growing need to accurately identify important features in the data affecting the outputs, and to determine the spatial co-relations between input variables at a given point in time as well as the causal or temporal co-relations between input parameters that change overtime. effective modelling to identify patterns from these data sources can be employed to produce accurate predictions of how a system is supposed to behave under normal operational conditions, and enable the detection of abnormalities. deep learning algorithms have attracted increasing attention by providing effective biologically inspired computational modelling techniques for addressing tasks such as speech perception and object recognition by extracting multiple levels of representation from various sensory inputs and signals [cit] ) [cit] ) [cit] . these approaches can offer the means to model large-scale data with significant dimensionality as well as spatial and temporal correlations for sequence modelling tasks. deep learning (dl) approaches are based on the principle of using anns with multiple hidden layers as shown in figure 1 . this allows both unsupervised (bottom-up) training to generate higher level representations of sensory data which can then be used for training a classifier (top down) based on standard supervised training algorithms [cit] . feature learning methods are based on supervised approaches such as deep nns, convolutional neural networks (cnns), and recurrent nns along with unsupervised techniques such as deep belief networks and cnns and provide deep architecture that combine structural elements of local receptive fields, shared weights, and pooling that aims to imitate the processing of simple and complex cortical cells found in animal vision systems [cit] ."
"the multi-objective from nsga-ii needs to be transformed to a single objective for mads optimization. for facilitating the exchange of objectives, an achievement scalarizing function (asf) is introduced. the asf is a reference-point based method that has shown the ability to produce proper pareto optimal solutions [cit] . the reference point is usually specified by a decision maker (dm) and contains dm's aspirations about desirable objective values. an example of constructing an asf is shown below:"
the rest of the paper is organized as follows. section 2 discusses computation intelligence for big data analytics. section 3 presents our novel methodology to provide solutions to data driven problems. section 4 presents few examples of application areas in which the data driven methodology is applied. section 5 concludes the paper.
"the mads belongs to the family of pattern search (ps) methods. similar to other members of the ps family e.g. generalized pattern search (gps), the mads does not require gradients for the optimization. the mads is an iterative algorithm which aims to minimize a function f over a set ω by evaluating f at some trial points. each iteration of the mads contains two steps, namely the search step and the poll step. the search step generates a finite number of mesh points and compare their objective function with the best feasible objective function value found so far (called current incumbent solution). the poll step is implemented whenever the search step fails to generate an improved mesh point. the poll step uses a parameter called poll size to dictate the magnitude of distance from the trial points generated in this step to the current incumbent solution. compared to the gps, the mads is not restricted to a finite number of search directions, and results in a much better local exploration of the space of variables, in particular for the problem with nonlinear constraints [cit] . recently, the mads has been increasingly used for complex engineering optimization problems, such as chemical processes and fluid structures [cit] . it should be noted that, other derivative-free local search methods such as hooke-jeeves direct search can be selected for hybrid optimization as well. the comparison between different local search methods for hybrid optimization is not within the scope of this study but can be explored in the future."
"the third generation, i.e. soa oriented e-assessment systems are still in use. but, issues like resource provisioning, optimal resource utilization and overall system cost still exist. therefore, the next generation eassessment systems must incorporate remedies and mitigations strategies for these issues. section iii describes the main challenges of modern e-assessment systems."
combining the nsga-ii and the mads is expected to improve the optimization performance by enhancing local refinement during the global search. alternating the use of the nsga-ii and the mads is similar to the concept of exploration and exploitation. the former helps to locate the global optimal by exploring the entire search space and the latter contributes to a faster convergence by exploiting the search within a small region. a step-by-step procedure for setting up the hybrid optimization model is presented below:
"even more, apart from its scalability and elasticity, our solution has another organizational advantage compared to third generation e-assessment systems providing better performance with the same hardware resources. that is, each student agent works with its own database containing only the necessary data for particular e-assessment. it is much smaller than the third generation e-assessment"
"capital expenditure costs (capex) are estimated for the following: 2 servers are used in the existing solution, each capable to handle at least 8.000 simultaneous concurrent accesses, or one server with 2 virtual machines installed on a more capable server that handles 16.000 simultaneous concurrent accesses. estimated value of two servers or one more advanced server is 15.000$. on top of this we add 2.000$ for other expenses, for example 500$ for network switch, 1.000$ for ups and 500$ for air conditioning cooling system. in reality we have to add also storage area network, but this demand will be not used in the model since it will also appear for the cloud solution."
"the problem is formulated as a robust optimization (ro) that accounts for uncertainty. the ro basically seeks for an optimal decision on the worst case scenario and the so-called robustness can be adjusted by reflecting dm's preference on the risk attitude towards uncertainty. in this formulation, we treat the optimization as a minimization problem with a weighted sum of the mean and standard deviation, in which the relative weighting is determined by the risk tolerance coefficient θ. this formulation can be written as min"
in case of a cloud solution there are no capital investment costs capex we calculate only the operating costs opex. the lowest available price (amazon [cit] ) for virtual machines with 8 cores is 0.52$ per hour and the maximum (google [cit] ) is 1.2$ per hour. therefore costs are estimated to be 1.123$ as minimum or 2.592$ as maximum.
"our solution is cost effective, i.e. the universities reduce their costs for buying the expensive hardware equipment in advance. even more, not only that the equipment will be underutilized most of the time, but it will spend a lot of electricity power unnecessary."
"computational intelligence (ci) is a subclass of ml approaches where algorithms have been devised to imitate human information processing and reasoning mechanisms for processing complex and uncertain data sources. ci techniques form a set of nature-inspired computational methodologies and techniques which have been developed to address complex real-world data-driven problems for which mathematical and traditional modelling are unable to work due to: high complexity, uncertainty and stochastic nature of processes. fuzzy logic (fl), evolutionary algorithms (ea) and artificial neural networks (ann) form the trio of core ci approaches that have been developed to handle this growing class of real-world problems."
"the discovery of correlations between individual inputs (bits) are determined through the spatial transformation of input space into a transformed feature space that is achieved through the use of deep belief networks, where this process is referred to as spatial pooling [cit] ). hierarchical clustering is performed on the transformed features derived from the deep belief network, to extract a number of possible states of the modelled system. the main purpose of this operation is for reducing the input space to a fixed number of the most probable states of the underlying system being modelled."
"the rest of the paper is organized as follows: section ii presents several development phases from traditional paper and pencil testing to modern e-assessment soa systems. the next section iii presents the challenges of a modern e-assessment system. our proposal for architecture and organization of e-assessment system to be hosted on the cloud is presented in section iv. the e-assessment cloud solution software engineering is elaborated in section v. section vi presents the cost model of our eassessment cloud solution, comparing it with the existing on-premise system. we discuss the benefits of the new proposed cloud e-assessment architecture and organization in section vii. related work in the area of scalable ijet -volume 8, special issue 2: \" [cit] \", [cit] 55 special focus paper e-assessment cloud solution: architecture, organization and cost model cloud architecture and e-assessment solutions are presented in section viii. the final section ix is devoted for conclusion and future work."
"the management role is realized with two agents: admin agent and infrastructure agent as depicted in figure 5 . 1) admin agent: the admin agent is the most versatile part of the system which provides the e-assessment content administration, data replication to the student agent in assessment module, authentication and authorization services, and assessment scheduling. the admin agent is service oriented and contains several parts:"
"we use the fact that particular student will be enrolled in maximum one assessment at a given time and therefore our system will utilize only minimum resources required to provide minimum performance. the traditional \"tight\" third generation e-assessment system utilizes constant resources during the assessments, but also before and after them."
"teacher's interaction with the e-assessment system consists of two activities: the first is content and grade authoring, and the second is test scheduling. we are not interested in the former since it represents common application scaffolding problem. however, we focus on the latter since it is a baseline to optimize resource utilization. that is, the e-assessment performance directly depends on the number of concurrent students enrolled in the scheduled assessment."
"this section discusses all benefits that our new eassessment solution provides to the universities. our solution is scalable and elastic and can be hosted on all three cloud service layers, i.e. iaas, paas and saas. communication with cloud controller is explained in corresponding sequence diagram by the infrastructure agent."
"recent advances in hardware and software technologies have enabled big data acquisition. this data can be harvested from a large number of diverse sources including emails and online transactions, multimedia information such as audio, video and pictures, large databases containing health records and other information. in addition, information can be captured during a user's interaction with social media such as posts, status updates etc., data derived from search queries or click patterns of a user, physiological data such as heart rate, skin conductivity etc. as captured from wearable sensors, data derived and extracted from our interaction with our mobile devices, embedded artifacts inside a smart home, data from production machines and industrial robots, scientific research and other sources [cit] . it is clear from the above that in the modern world data is being generated at an accelerated rate [cit] ."
in this section we propose an e-assessment system cloud architecture and organization. the solution is designed to provide sustainable performance with minimal costs for cloud hosting resources.
"the conventional testing process is organized as paper and pencil testing. evolution of computer technology enabled realization of variable-form testing approaches that utilize the interactive ability of a computer to administer a set of items that is determined at examination time, rather than a predetermined set of items [cit] . luecht and sireci [cit] present a brief history of computer based testing or etesting, although the most common used term lately is computer assisted assessment for processes that cover any use of computers in the process of assessing knowledge, skills and abilities of individuals [cit] ."
"in this paper we explain the implementation details of the broker module allocated in figure 3 . we introduce four agents that are part of a broker: admin agent, infrastructure agent, student agent and reporting agent, as depicted in figure 4 . next sections give detailed explanation of their purpose and features."
students' interaction with the e-assessment system consists also of two activities: preview of results and testing process. the former mostly consists of database scan and search operations. these resource requirements are predictive and therefore we model all these functions to be hosted on a separate vm for reporting node. the latter has major impact on the system performance due to complex and massive query executions. data write operations are realized during the assessment and they are stored in the corresponding vm instance identified as assessment node.
"assessment results have significant importance for proper student evaluation and introduce increased trustworthiness among the teacher and students via the eassessment system. however, increased load by increasing the number of assessments and assessed students usually decreases the system performance or even disables it."
"the new proposed e-assessment system organization model consists of three subsystems, i.e. the management, assessment and reporting subsystems, determined by ijet -volume 8, special issue 2: \" [cit] \", [cit] 57 special focus paper e-assessment cloud solution: architecture, organization and cost model figure 3 . e-assessment system organization working mode performance and resource requirements, as presented in figure 4 ."
"soa provides an environment for computing the services, while the cloud provides an environment that offers the services of computing. the combination of soa and cloud computing also provides potentially transformative opportunities [cit] ."
"the cost comparison of the given model is presented in figure 9 . the price is expressed in usd and the cases represent minimal workload with 160 students taking exams simultaneously, nominal with 320 students and maximal with 480 students. this figure also confirms the advantages of cloud solution over the on-premise. interestingly, for greater values of utilization of on-premise server we can expect lower costs in the beginning, but higher for increased processing demand. the average value of cost ratio for the given minimal workload is 0.23 if cloud is compared to the onpremise server."
"another important aspect of the algorithm is the computational cost. the computational cost of the nsga-ii can be evaluated based on the number of generations or function evaluations. however, this approach is not applicable to the mads because this method takes the objective difference as the stopping criteria. for a fair comparison, the central processing unit (cpu) time was recorded for all experiments under the same computing environment. the computational cost is expressed as a dimensionless ratio denoted as computational index, where the cpu time of the experiment with nsga-ii only is used as the benchmark (e.g., ratio denominator). the result of the computational index is shown in fig. 6 . the final hypervolume index for all experiments is also included for comparison. the results for experiment 1 (nsga-ii only) and 2 (one hybridization) in fig. 6 are used to produce a straight line (dashed line) as a representation of linear relationship. as shown in fig. 6, the hypervolume index increases almost linearly until the number of hybridization times is about 2. for a larger number of hybridization times, the hypervolume index still increases, but at a much slower rate. the computational cost also increases with the number of hybridization times. for instance, for ten hybridization times, the computational cost is increased by 30% compared to the case with no hybridization. also, as shown in fig. 6, the computational cost for the experiment with the mads only is the lowest, but its hypervolume index is also the lowest. the hybrid model with two hybridization times shows overall good performance in terms of hypervolume index, diversity of solutions and computational cost. for two hybridization times, the hypervolume index is improved by nearly 10% at an increase of computational cost of less than 0.1% compared to the nsga-ii."
"the first aspect in hybridizing the nsga-ii and the mads is the starting time of the mads. previous researches have shown that the nsga-ii is superior in finding feasible solutions in a complex search space [cit] . therefore, the nsga-ii is first utilized to efficiently handle the constraints. it normally takes a certain number of generations e.g., 100 to obtain feasible solutions and this number is expected to be increased for optimization problems with a complex search space. during the process of finding feasible solutions, the candidate solutions in the nsga-ii tend to be clustered around the one which has the least constraint violations. the solutions then begin to spread after a feasible one is obtained. since local search is applied to only good offspring from the nsga-ii [cit], it is reasonable to start the mads when the solutions of the nsga-ii have been spread out widely and evenly. in this case study, 1000 generations of the nsga-ii is found to be a good starting time for the mads. however, this timing may be problem-dependent and it is left for future work."
the data in this module is determined by the course and the number of students taking the exam. therefore the student agent is placed in this module to receive necessary assessment data from admin agent and send the results of the assessment to the reporting agent to store them in the main reporting database and to be published for the students and the teachers. figure 6 depicts the assessment module architecture and student agent with in.
"in this paper we define a specific organization and solution that will represent the fourth generation of eassessment systems called eco-systems. eco-systems are treated as biological living organisms. [cit] described the resource provisioning in the cloud as birth and death of the organism. the infants of the fourth generation systems use server clusters [cit] in order to gain better performance, optimal resource utilization and reduce the overall costs. as the cloud computing paradigm emerged, the next generation of e-assessment systems will be hosted on the cloud. figure 1 presents the methods, standards and technologies for the fourth generation of e-assessment and e-learning."
"similar to the admin agent, the student agent is service based. this subsystem is constructed from the same integrated parts as management agent, i.e. lms, vle and mis."
"gaining an insight of the user's/client's affective state, through the utilization of social media data, can fuel the generation and diversity of intelligent mechanisms able to deliver affective feedback in a wide spectrum of application areas. this feedback could extend from the digital to the physical world, by using standard hardware devices (e.g. smart phones, home appliances, lighting) in the context of intelligent environments. intelligent cars could modify their engine and cabin configuration to meet their driver's needs and preferences. e-learning systems could provide tailored learning material and tailored encouragements to the students. automated manufacturing systems could take into account the client's sentiment when designing and developing new products, in order to better satisfy their customer's needs. tele-home healthcare, robot companions, patient monitoring could be enriched by taking into account the patient's context, behaviors and emtions in order to deliver more efficient patient specific services. homes with the ability to modify their environment (lighting, temperature etc.) in order to suit their resident's mood."
"the present study proposes a hybrid optimization model which aims to overcome the two aforementioned challenges. to achieve this goal, a concurrent hybridization scheme is used together with the mesh adaptive direct search (mads), which is a pattern-based search method, as an additional step in the nsga-ii. instead of using the weighted sum approach, an achievement scalarizing function (asf) is introduced in the mads to handle the objective exchange and ensure a pareto optimal solution. a parallel optimization scheme is adopted for the mads to simultaneously refine the multiple intermediate solutions from the nsga-ii process, allowing an efficient exchange of the solution vector. the mads as well as the nsga-ii do not rely on derivative information and consequently, the hybrid model is entirely derivative-free. the superiority of the hybrid optimization model is demonstrated using a case study of a multiobjective reservoir system on the columbia river. the frequency of applying the mads in the routine of nsga-ii is investigated. the major contributions of the study are (1) development of a hybrid optimization model that couples the mads and the nsga-ii for multi-objective shortterm reservoir operation under uncertainty; (2) providing recommendation of a hybrid scheme that balances optimization performance and computational cost. the novelty of the paper is the development of a derivative-free model that has a relatively fast convergence, which could have broad applications in optimization of complex water resources systems."
"the main idea in our new e-assessment system is dividing the results of the exams in the new separate module, i.e. reporting module. after each assessment, the data is replicated into this module. then reporting agent provides the exam results to both the teachers and students. data privacy and security are obliged for this part of the system because of the present sensitive data."
"further on, the students take the assessment and after the assessment time has elapsed, data replication transfers the results to reporting module. at the end, the infrastructure agent releases the vm instance that hosted the corresponding assessment module."
the solutions of the objective functions at every ten generations were recorded for the experiments with nsga-ii only and for those using the hybrid model. each of these solutions contain 50 points (population is 50) and form a pareto front in the two-dimensional objective space.
"the initial stage of the approach performs hierarchical organization of multiple levels of data abstraction to identify correlations between temporal sequences of input patterns. temporal sequence learning is used to train the model on different temporal consequential relations between probable states of the system. this is used to infer the next predicted state of the inputs in comparison with the actual behaviour of the system, which is termed as temporal inference."
"a reservoir system on the columbia river in the united states, which comprises 10 reservoirs, is used as a test case. the sketch of the ten-reservoir system is shown in fig. 1 . the reservoir system provides multiple operational purposes including hydropower generation, ecological and environmental requirements and recreation [cit] ."
"to improve the efficiency of the ga, genetic local search algorithm (glsa) which combine local search methods with the ga, have been extensively reported in the literature [cit] . the general idea of the glsa is to incorporate a so-called local search method (lsm) as an extra optimizer in the process of the ga optimization. coupling a lsm and ga is normally done using a serial or concurrent procedure. serial approach applies the lsm by predefining a switching time [cit] . although this approach guarantees a local optimum with improved speed of convergence, the optimal switch timing is not known a priori on most practical problems. a recent study proposed a concurrent approach embedding a sequential quadratic programming (sqp) within nsga-ii [cit] ). in the concurrent approach, some or all of the intermediate solutions from the ga were regularly modified by the lsm during the process. the sqp is treated as another operator in the ga to avoid the switch timing."
"although the same sub systems are used, student agent operates on significantly smaller data and has only assessment access rights. working on relatively smaller database clearly improves the overall e-assessment performance since each query works on smaller data set."
"the model of cloud e-assessment initially requires one vm instance called administration node with goal to hosting the management module and another vm instance called reporting node with goal to hosting the reporting module, as presented in figure 7 . administration node dynamically instantiates and shuts down additional vms, identified as assessment node. the administration node hosts the management module. the specified functions in this module can be accessed by administrators and teachers. the module services communicate with cloud controller for resource provisioning via the infrastructure agent and to assessment module via admin agent. the reporting node contains tools to access the database for preview and analysis of e-assessment results and its functions can be accessed by all authorized users."
"because the short-term reservoir operation involves a time horizon of about two weeks, the operational horizon in our case study is set to two weeks. due to data availability, the period from august 25th to september 7th is considered herein. the decision variables are the hourly outflows from each reservoir during the optimization horizon, which results in a total of 3360 decision variables."
"http://www.i-jet.org special focus paper e-assessment cloud solution: architecture, organization and cost model figure 2 . the third generation e-assessment system architecture [cit] e-assessment systems must provide fast response after each student answer since the assessment time is limited. e-assessment performance depends on the total number of enrolled students. the most important factor that impacts the performance is the varying number of assessed students and concurrent assessments. all these requirements introduce a necessity to implement elastic and scalable system for e-assessment."
"a novel hybrid optimization model, which incorporates the mads into the nsga-ii, is developed to enhance local search in global exploration of the nsga-ii. an achievement scalarizing function is introduced in the mads to attain pareto optimal solutions. the major advantages of the model are (1) derivative-free, (2) fast convergence and (3) efficient exchange of solution vectors between local and global optimization methods. the case study for the optimization experiments is the short-term operation of a multiobjective reservoir system under uncertainty. in general, the results show that the hybrid model has a superior performance compared to both nsga-ii only and mads only. the hybrid model with two times of mads shows overall good performance in terms of hypervolume index, solution diversity and computational cost. for this case, the hypervolume index is improved by nearly 10% at an increase of computational cost of less than 0.1%, compared to the nsga-ii only model. overall, the proposed hybrid model is derivative-free and can be applied to a wide array of complex optimization problems in water resource planning and management."
"special focus paper e-assessment cloud solution: architecture, organization and cost model the assessment node hosts the assessment module and is accessed by authorized students and teachers. these instances are dynamically created and closed as required by the management module via the infrastructure agent and are usually managed by the system administrator. figure 8 presents the sequence diagram for the eassessment model from aspect how it communicates with cloud, i.e. how the activities for resource provision and releasing are asked by the modules and how the cloud controller instantiates and shuts down corresponding virtual machines with assessment module."
"the infants of e-learning, i.e. teaching machines date from the 19th century. although plenty of teaching machines were patented [cit] s despite huge interests about them. rapid development of microprocessors enabled personal computers to be used in classrooms which is introduced as the first generation of e-assessment systems. monolithic assessment applications are the first generation of eassessment systems [cit] . although the solution was widely adopted, it contained architectural problems. usually the e-learning systems were exclusively dedicated to single course or features for communication with external systems were excluded. additionally, re-usability of existing services was unavailable."
"our ongoing work is focused on optimizing taxi fleet distribution and routing in context of urban traffic conditions in order to enhance availability, reduce waiting and journey times, and promote fuel economy. in order to achieve the research objectives a novel deep learning based spatial modeling technique was developed and applied which was able to predict hot spots relating to locations of where taxis pick up and set down people in an urban area over the course of the day. the structure and ideas of the computational modelling approach is described in detail in section 4. the data used to train the predictive model were acquired from nyc open data webpage. [cit] . the data records include fields such as capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. the data used were collected and provided to the nyc taxi and limousine commission (tlc) by technology providers authorized under the livery passenger enhancement program (lpep). an example data sample can be seen in figure 8. the proposed model was utilized in the development of a commercial software application, and achieved above 95% prediction accuracy. process, the analysis engine builds a complex relational graph to determine the various relationships between identified parts of the query, including complex, multi-table relationships, from which an sql query is constructed, once all required relationships are identified."
"(1) determine parameters for nsga-ii e.g., population and generation; (2) determine parameters for mads e.g., poll size and stopping criteria; (3) set up interface between nsga-ii and mads, e.g., exchange of objectives; (4) start nsga-ii and run for a pre-determined number of generations; (5) pause nsga-ii and pass the intermediate results to mads; start the mads; (6) run mads until the stopping criteria is meet and pass updated results back to nsga-ii; (7) repeat steps (4) to (6) until the predefined frequency of running mads is meet; (8) run nsga-ii until the predefined generation number is meet."
"the total calculation of capex + opex is more than 30.000$ for a 3-year period for the estimated case that 160 students can have simultaneously the exams in lab. although we are aware of, in this calculation we do not add maintenance costs, or costs required for technical assistance or administration."
"the hypervolume index [cit] ) of the solutions is investigated for a more quantitative comparison. a higher value of the hypervolume index indicates better quality of the solution in terms of convergence and diversity. fig. 4 shows the hypervolume index at every ten generations for the experiment with nsga-ii only and those of the hybrid model. the improvement of the hypervolume index at early stages (e.g. first 1000 generations in fig. 4) is very similar for all five experiments. this is likely because the same initial population is used for all of these experiments. in a similar fashion to the first 1000 generations, the experiment with nsga-ii only resulted in a continuous increase of the hypervolume index during the last 3000 generations as the diversity and convergence of the solutions were gradually improved (see fig. 3 ). in contrast to the results with the nsga-ii only, the hypervolume index for all hybrid optimizations show sudden jumps after the implementation of the mads. the results also show that a larger number of hybridization times (e.g., number of times mads is implemented) gives a higher hypervolume index, which indicates a better quality of the solution in terms of convergence and diversity. for better visualization, the optimal solutions of the last generation are shown in fig. 5 . this figure also shows a reference solution, which was obtained with the nsga-ii only using 100,000 generations. this reference solution represent the best possible results that can be achieved for the two objectives in the reservoir operation optimization problem (i.e., minimize the power deficit to the demand and maximize extra power for heavy load hours). as the two objectives are conflicting with each other, without additional subjective preference information, all solutions from the reference pareto are considered equally good in the context of multi-objective optimization and selection of one solution depends on preference of the decision maker. the solutions for the fig. 5 shows the final optimal solutions in objective space for all experiments. the result for the experiment with mads only is inferior to the other experiments. this is because the mads, which focus is on convergence, may be trapped into local optima. contrary to the mads, diversity is created by the nsga-ii. it is worth mentioning that too many local exploitation decreases the diversity of the solutions in the process of seeking for convergence. the optimal solutions for the hybrid model with ten hybridization times is the closest to the reference solution. however, the diversity of the solutions is decreased compared to the hybrid model with lesser hybridization times."
"in the beginning of the modelling process, raw process data is generated by the relevant data sources and is transformed from its textual -human readable form, to binary sparsely distributed format. data encoding aims to map the inputs defined for a specific modelling problem to discrete signals that can be understood by the hstsm. the encoded vector is compressed, and an automatic process of feature extraction is performed with the use of deep belief neural networks (dbn). the trained restricted boltzmann machines (rbm) forming the dbn are used to initialize the deep auto-encoder. this unsupervised method of feature extraction enables us to acquire an improved and more compressed representation of the input space. because of the unsupervised nature of the process, there is no need for labeling the data at this stage. therefore, the algorithm is able to cope successfully with high volumes of data. the features extracted are used to automatically identify a set of possible patterns on the inputs to the model. this would correspond to a set of possible states of the monitored system. in order to achieve this we incorporate into the model agglomerative hierarchical analysis, which is an unsupervised method of cluster analysis. the basic metric used to conduct this analysis is defined to be the euclidian distance. this process can be considered as spatial pooling, where the original binary input that occur close together in space are pooled together. this operation enables the extraction of a set of individual system states, and the analysis of the temporal sequence of input activities in terms of these states."
"the cloud computing paradigm offers many benefits in the whole education, not just for e-assessment. it stores the educational resources, e-mails, educational applictions and tools, thus reducing the costs and improving the quality of current system of education [cit] . many e-learning cloud architectures are proposed to achieve this objective."
"big data analytics and computational intelligence techniques are also able to facilitate governments and organizations provide solutions to challenging problems. health shock is a health related event with a heavy impact on a household because of the cost of treatment, or the absence from work caused by the health related event. health shock especially affects individuals in developing and underdeveloped countries. government, organizations, policy makers, and individuals could largely benefit from the development of intelligent big data analytics techniques to battle health shock by facilitating accurate prediction and by highlighting the factors contributing to this phenomenon. these techniques will enable effective and timely mitigation and management of health shock events. in order to address this problem and contribute in this direction a big data analytics and visualization framework based on fuzzy logic was presented, which utilized a large-scale dataset towards health shock prediction purposes [cit] . the researchers utilized cloud amazon web services integrated with geographical information systems (gis) to facilitate the collection, storage, indexing, and visualization of data for different stakeholders using smart devices, and develop their framework [cit] . the dataset utilized comprised of data from questionnaires, and an online system where each data sample comprised of 47 features. data were gathered from 1000 households belonging to 29 villages in rural areas of pakistan. the collected data where pre-processed and analyzed by utilizing expert-opinion to extract meaningful measures related to living standards, health risk, accessibility to health facilities and income allocation labeled with a level of health shock incurred. a fuzzy rule summarization technique was utilized in order to develop a health-shock prediction model. the proposed predictive fuzzy model was evaluated by utilizing k-fold cross validation. an overview of the research methodology can be seen in figure 7 . the model achieved a very high performance of 89%, while at the same time retained a very high interpretability degree with the help of the extracted natural language fuzzy rules. these rules can enable all relevant stakeholders to understand the causal factors affecting this phenomenon, and make informed and effective decisions. moreover as demonstrated by the team's experimental results the proposed framework was also able to deliver results in competitive times with a low computational burden thus making it suitable for real-time and big data settings. furthermore, data were mapped to the iron triangle model [cit] under the socio-economic, geographical, and cultural norms, and factors that lead to health-shock. [cit] . understanding each driver's behaviour and information needs in terms of their intentions can be used to provide relevant information and services as they are needed which can be used to improve user satisfaction, vehicle efficiency, and energy utilization."
"armenski and gusev [cit] present a new high level of abstraction of the architecture of so-called ultimate eassessment system, as presented in figure 3 . the main accent is set on intra-domain and inter-domain interoperability, although both approaches to build a standalone system or a cloud solution can be build based on this approach. the innovation that this system defines is introduction of a broker module with purpose to communicate to different learning management systems (lmss) and acts a role of system service orchestrator using the service registry, a form of universal description discovery and integration (uddi) approach with descriptions about available service."
"the second generation of e-assessment system introduced modular system architecture with primary goal for sanitization of architectural issues. modular architecture allowed homogeneous interconnected components to be used. additionally, it was suggested that component management, customization and re-usability will be fairly simple. therefore, pedagogical diversity satisfying various needs can be achieved."
"management module is the core of the e-assessment system. it manages all resource provisioning in the cloud. this module can be hosted on a bare metal server or on a virtual machine (vm) with constant resources since it manages the system when it is in inactive mode. it is always active to provide users, courses, questions, exams, authentication, authorization etc."
"another issue is the cost of hosting the management and reporting module, since it can be realized as dedicated server and appropriate storage area network. of course, we can choose whether to install it on premise or host it as dedicated server on the cloud. estimated price for hosting of dedicated server including the communication traffic costs is (amazon) approx. 2.500$ to 5.000$ for a 3-year period of medium utilization reserved instances with medium cpu processing power and, medium memory demands and medium communication bandwidth."
"the resulting pareto front at every ten generations are shown in fig. 3 . as can be observed in fig. 3, the solutions using nsga-ii only gradually move from the left-upper region to the right-lower region, where the values of the objective functions are improved. this means that power deficit to the demand is decreased and extra power for heavy load hours is increased, which results are desirable for the decision maker. the extension of the solutions is also gradually increased as diversity is gained in the process. however, the extension increase may not be significant or even may decrease depending on the structure of feasible regions in the search space and the ability of the algorithm for maintaining diversity. for the experiments with the hybrid model, the optimization evolution was found to present clear gaps in the objective space as greater convergence was gained from the mads process. overall, it is observed that a larger number of mads implementation leads to a better convergence; however, the diversity of the solutions are decreased. the later means that the two objectives of the reservoir operation problem (i.e., power deficit to the demand and extra power for heavy load hours) are both improved but the number of options for decision-making is reduced."
"masud and huang [cit] present a framework which specifies a number of steps how the universities should adopt the cloud computing. the framework consists of several features: esaas (education software as a service), digital library for easy content download, online storage to maintain students' data, student collaboration, network access to cloud resources, interoperability with different clouds, provisioning and security and privacy."
"short-term operation of reservoirs normally considers multiple purposes and face various uncertainties e.g., forecasted inflow. the optimization of reservoir operation is inherently a complex multi-objective problem under uncertainty, in which high-dimension decision variables and highly nonlinear constraints are often involved. the objective and/or constraint functions may be nonconvex, discontinuous [cit] or noisy as uncertainty is incorporated [cit] . in this context, the derivative information on either objectives or constraints often become unavailable, unreliable or impractical [cit], which may present a difficulty to the optimization methods in particular for derivative-based algorithms e.g., steepest descent or newton method."
the solution is capable to instantiate as many vm instances as enabled by hardware resources in the computer center. it is platform independent and therefore paas can be used to implement this solution.
"on top of previous costs we have to add costs for elastic load balancer, elastic load balancer data processing, elastic block store volume, s3 storage for war file and access, bandwidth in and out, which is estimated to be 3.300$ for a 3-year period."
"universities can benefit if they migrate their e-learning systems onto cloud [cit] since the cloud offers good resources flexibility and scalability, storage, computational requirements and network access, and most important lower cost [cit] . cloud lowers the disk space requirements, and enables newest software versions and monitoring of the installation progress in each of the cloud service models, i.e. infrastructure-as-a-service (iaas), platform-as-aservice (paas) and software-as-a-service (saas) [cit] . our proposed architecture offers a solution for each cloud service layer from iaas to saas. [cit] present how to harness cloud technologies for e-learning purposes in order to achieve ease of use, scalability, and reduced power consumption. however, if the universities migrate their e-learning systems on the cloud, they must ensure that the users can access appropriate resources across systems without breaking the copyright of the resource [cit] . in this paper we propose an architecture of eassessment system that can be hosted onto the cloud and use infinite cloud resources dynamically to reduce the costs and to provide better performance."
"where pg is hydropower generated in the system (mwh), pd is power demand in the region (mwh), t denotes time in hours, t h is the optimization period in hours (336 h), the index i represents reservoir identification number in the system and n r is the total number of reservoirs. the variable h r represents heavy load hours (hlh), which in a typical day takes place from 06:00 am (06:00 h) to 10:00 pm (22:00). the variable t d corresponds to the optimization period in days (14 days in this case). the first objective (eq. 2) aims to minimize the power deficit during the operational horizon. the function min(0, *) expresses that the deficit is equal to 0 if the total power fig. 1 sketch of the ten-reservoir system in the columbia river generated is greater than or equal to the power demand at time t. the second objective (eq. 3) aims to maximize hydropower generation during heavy load hours for selling power to the electricity market at a higher price, which would increase the revenue. the function max(0, *) expresses that the objective value is equal to 0 if the total power generated is smaller than or equal to the power demand at heavy load hours. in the optimization model, the two objectives are transformed as minimization functions and normalized using a dimensionless index between zero and one. other purposes of reservoir operation such as flood control or mop (minimum operation level) requirements are considered as constraints on either reservoir water surface elevations or storage limits, as summarized below."
"two inflows are considered, one inflow to the gcl reservoir and another to the lwg reservoir (see fig.1 ). uncertainty in inflow can be caused by a number of reasons, including uncertainty in forecasting. typically, the forecasting uncertainty is smaller at the beginning of the forecast period as information is more abundant and accurate, and tends to grow over time as information become less available and precise. the uncertainty associated with inflows can be illustrated by perturbing the historical record with a random linear function in time, as shown in fig. 2 . six different time series data are used to represent 6 forecasting scenarios. it is noted that this inflow perturbation is solely for illustrating uncertainty. an ensemble of forecasts [cit], each of which is an output from a simulation model (e.g., weather model), is a more accurate representation of uncertainty, and should be used for uncertainty quantification whenever they are available."
"e-assessment reliability is another important challenge. situations that require additional re-assessment due to any kind of assessment obstructions are unacceptable. these scenarios are almost impossible to be fully avoided, but need to be mitigated deploying redundancy to single point of failure and removing bottlenecks. even more, particular e-assessment failure should not affect the others simultaneous assessments. all participants in e-assessment process (questions' authors, teachers, managers, system developers and students) suffer from some risks [cit] . e-assessment systems store sensitive data such as student personal data, assessment results and questions for each test. data privacy and data integrity are imperative requirements for each eassessment system. additionally, the system must provide authorization and authentication both for students and teachers in order to segregate their access rights. question banks are another sensitive part of e-assessment systems and have to be managed according to security policy. finally, assessment results' confidentiality must be provided."
there are several challenges for cloud e-assessment system that initiate motivation to replace the traditional eassessment. one of the most important challenge is to survive the peaks when particular number of assessments are taken with particular number of assessed students.
"there are several methods and modeling tools to quantitatively compare the cost of leasing cloud resources to that of purchasing and using a dedicated server system. for example, walker [cit] has analyzed mainly the real cost of a cpu hour and its impact on overall resources. [cit] address the total cost of ownership (tco) and utilization cost considering the elastic cloud features and virtualization approach. greenberg [cit] analyzes and calculates the cost of a cloud, as a research problem in data center networks. durke [cit] argues why cloud computing will never be free."
"on the other side, this solution can be offered as saas services, so the cloud provider will take care about the infrastructure agent and provide relevant number of resources. the e-assessment solution can be hosted on appropriate iaas or paas cloud service layers. the communication with cloud controllers is scalable and elastic. it instantiates and shuts down vm instances for each eassessment with particular resources that can provide minimum system performance depending on the number of concurrent students enrolled within the assessment. on the other side, the traditional third generation soa eassessment system will be either underutilized or over utilized during the assessment and it is more important that it will be always underutilized when there is not any active assessment which is most of the time during the semester."
"nowadays huge quantities of personalized and contextualized information are generated in platforms such as social networks extending to wearable devices, where millions of people interact and express their opinions and emotions. the development of advanced big data analytics and computational intelligence techniques enables the development of intelligent computerized solutions with the help of social and behavioral data based sentiment analysis."
"in addition to the conventional three layered architecture the offered system architecture model [cit] defines common services layer, e-assessment services layer and composite services layer (broker). frema project [cit] defines most of the services in the e-assessment layer and in common services layer."
"the main e-assessment challenges are described in this section. each e-assessment system must provide accurate and objective assessment as the traditional assessment. additionally, the system must be reliable and available during the assessments in order to finish the assessment in a proper time."
"machine learning (ml) approaches are used for modelling patterns and correlations in data in order to discover relationships and make predictions based on unseen data / events. ml approaches consist of supervised learning (learning from labelled data), unsupervised learning (discovering hidden patterns in data or extracting features) and reinforcement learning (goal oriented learning in dynamic situations) [cit] . as such, ml approaches can also be categorised into: regression techniques, clustering approaches, density estimation methods and dimensionality reduction approaches. nonexhaustive examples of these approaches are: decision tree learning, associate rule learning, artificial neural networks, deep learning support vector machines, clustering and bayesian networks."
"fl is an established methodology to deal with imprecise and uncertain data [cit] . fl provides an approach for approximate reasoning and modelling of qualitative data and adaptive control ) [cit] ) based on the use of linguistic quantifiers (fuzzy sets) for representing uncertain realword, data and user-defined concepts and human interpretable fuzzy rules that can be used for inference and decision-making. eas are based on the process of natural selection for modelling stochastic systems [cit] ) and approaches such as genetic algorithms, genetic programming and swarm intelligence optimisation algorithms [cit] ) [cit] can be used for optimising complex real-world systems and processes. finally anns enable feature extraction and learning from experiential data [cit] and are based on imitating the parallel processing and data representation structure of neurons in animal and humans brains. a nn is an interconnected assembly of basic elements (artificial neurons) that broadly speaking resemble the neurons existing in our brains. the analyzing ability of neural networks is hidden in the values of the weights that connect these basic elements. these weights are acquired by adaptation, or by learning from training data [cit] ."
"the optimization experiments were conducted using nsga-ii only, mads only and a hybrid model where the mads was implemented one, two, five and ten times during the global optimization. this resulted in a total of six experiments. the population for all experiments was the same and was set to 50. it is noted that larger populations normally result in better performance but lead to a heavier computational cost. the stopping criteria for the nsga-ii only and hybrid models are the number of generations, which was set to 4000. the stopping criteria for the mads only was set to be when the objective difference is smaller than 10 −3 . because of the random nature of ga, in a similar way to other random-based search algorithms, optimization results may vary for different runs, especially for problems with complex search space and multiple local optima. to provide a fair comparison, the initial population for all experiments were set to be the same. we first ran the experiment with the nsga-ii only and recorded the first population (i.e. solutions that are randomly generated). this recorded information was then used as the first population for the other experiments. for the experiments with the hybrid model, the nsga-ii was run for the first 1000 generations and then the mads was started at regular intervals between the 1000th and 3000th generations. for example, the hybrid model with 2 times of mads would start the mads at the 1000th and 3000th generations, respectively. after the 3000th generation, the optimization continued with the nsga-ii and stopped at the 4000th generation."
"the cloud approach based on demand is not a new idea. however, as they say, the devil is in implementation, so here we address series of implementation details, not just presenting the model specification and service description within the proposed architecture."
"there are two possible scenarios for resource allocation. the first scenario is to dedicate enough resources to host the e-assessment system capable to handle the assessment of all enrolled students in one moment. this solution will be very expensive and the overall system will be underutilized most of the time. also the total number of enrolled students varies during time generating additional scaling problems. the second scenario is to predict the average number of students that are concurrently assessed and to dedicate enough resources to host the e-assessment system as it can handle the assessment of predicted number of students in one moment. although this scenario is less expensive, the solution will still be over-utilized or underutilized most of the time."
"a combination of ci techniques can be used to extract insight and meaning from the data, offering integrated solutions, which can be applied to a variety of application domains. such solutions should be adapted to offline and online, hardware and software data processing and control requirements, which can be further optimised to domain dependent constraints and dynamics. hence these approaches can be used to provide effective multipurpose intelligent data analysis and decision support systems for a variety of industrial and commercial applications characterized by large amounts of vague or complex information requiring analysis to support operational and cost effective decision-making )."
"leading organizations in e-learning community were focused on creation of a common technical framework recently. few detailed frameworks were developed and consensus was made that e-learning system architecture should be based on service-oriented architecture (soa) [cit] . soa was widely adopted paradigm, and it was introduced as the architecture for the next generation of elearning systems [cit] . main benefit of this architecture is the ability to mix together services from different elearning frameworks."
"this generation has added value to the previous generations for the technologies that implement scalability, virtualization, elasticity and interoperability between different systems (including legacy systems) required to become a cloud solution. used methods refer to introducing cloud computing solution, where all required functionalities are offered as a service. although development of standards is still hot research topic, several of them have been acknowledged to be accepted for the new generation, like ims learner information package (ims lip) and public and private information (papi) for learners (papi learner) for user modeling and personalization, ims question and test interoperability (ims qti) for exchange of content and ieee learning object metadata (lom) or dublin core metadata for exchange of metadata."
"over the last several decades, a number of optimization methods e.g., linear programming, dynamical programming and heuristic search algorithm have been developed and applied to reservoir operation [cit] . the optimization methods can be mainly classified into two types: global and local methods in terms of the ability for finding global optima or local one, or derivative-based and derivative-free methods in terms of the requirements of derivatives in the optimization procedure. genetic algorithms (ga) and its variants e.g., nsga-ii are global and derivative-free methods and have been widely applied to multireservoir operation during the last two decades [cit] owing to its robustness, effectiveness and global optimality properties. other global and derivative-free methods such as particle swarm optimization (pso) and ant colony optimization (aco) have also been used for optimization of reservoir operation [cit] . the aforementioned methods can be seen as strong candidates for optimizing short-term operation of multi-objective reservoirs under inflow uncertainty. however, like other random search methods, ga works by iteratively moving to better positions in the search-space, which are guided by the random process of the ga operators i.e., selection and crossover. the embedded randomness of ga is a key element for global optimality, however, this method has a slow convergence compared to methods that use derivative information, especially when approaching to the optimal point [cit] ."
"the key parameter for hybridizing a local and global method is the search frequency of the local method, which is defined as the number of implemented local searches during the process of global optimization. too many local searches may significantly degrade the diversity of the solutions from global search and too few may result in a slow convergence [cit] . to investigate the effect of search frequency of the mads on the overall performance of the optimization, various frequencies, ranging from 0 (i.e., nsga-ii only) to 10 are simulated. the performance of the optimization results are then compared for various search frequencies."
"in the following analysis we will make further calculations for three cases based on expected minimal (existing situation), nominal (expected after one year) and maximum (expected after 2 years). it also means that the system should be dimensioned double for 320 students having simultaneous exams after one year (nominal case) or 480 students after two years (maximum case)."
"however, both implementations cannot offer sustainable performance since e-assessment system requires huge amount of unpredictable resources only in appropriate small periods during the e-assessments, while in most of the time this system requires predictable much less resources. using server-cluster architecture to create scalable and highly available solutions [cit] will only partially solve a performance peak problem since it is hard to be managed and administered. on the other hand, the software solution has to be prepared for scalability."
"gone are the days when the computation time to model the atomic nucleus data took 24 hours on a cray supercomputer, it now takes minutes or even seconds on a laptop. big data is now transmitted via hundreds of operational satellites where global positional resolution is expected to be 40 cm in a few years' time."
"the process of test scheduling consists of several steps. the teacher defines the course by specifying the questions in corresponding knowledge database and number of students that can apply for assessment. subsequently admin agent adds a new event in the scheduler component which concludes the process of test scheduling where user interaction is necessary. event data consists of the previously submitted user data. the scheduler component trig- figure 7 . vm instances organization for cloud e-assessment system gers the previously stored event at the specified time interval before the assessment. the number of students'value stored in the event is forwarded to the infrastructure agent in order to determine the required resources for the student agent vm instance. after the assessment scheduling, the students should authenticate to the system. access token is endorsed for each active student. additionally, each student's token is published in the active assessment module. token publishing is constrained only to student agents dedicated for courses where the student is enrolled. successful token creation provides new access token and address of modules where token is published."
"the potential utilization of this huge amount of information has catapulted big data and big data analysis as a central focus for modern research communities, modern businesses, and governments [cit] working towards delivering the promise of a plethora of new application areas and opportunities which can emerge under completely diverse application contexts from smart cities [cit] to digital health care [cit] . as a result the benefits arising from this wealth of knowledge and information can affect research in numerous ways. these can include : promoting medical advances by providing evidence in the identification of symptoms and patterns concerning diseases, pandemics and modern health issues; or aiding in the creation of large ground truth databases for scientific fields such as sentiment analysis, which are in desperate need of vast amounts of data in order to successfully create models of human affect and effective behavior recognition techniques. the businesses that constitute the modern economy can also greatly benefit from big data and big data analytics since they can utilize the data generated from the interaction of users with social network [cit] or smart devices in order to identify the users' preferences towards a product, recognize dissatisfaction of modern clients, or understand the relationships between competitive and collaborating organizations, thus creating better and more appealing products and services, or improving existing ones."
in this paper we propose an architecture of cloud eassessment system that improves the overall performance and reduces the costs. the contribution can be summarized as three-fold for the proposed architecture:
"there are three main user roles in e-assessment system: teachers, students and administrators. the users in teacher role manage the e-assessment system functions: creation and administration of questions and answers in the system, staring test generation, testing, analysis of results, etc. the students belong to a role that can take the exams and view their results. the administrator role is dedicated to those who administer the system managing with users, courses, exams, tests, configuration etc."
"nsga-ii is a widely used random search method for multi-objective problem (mop) and have increasingly received attention for reservoir operation [cit] . the nsga-ii is a member of the ga family and follows the primary principles of the classical ga. first, a set of candidate solutions (population) are randomly generated (first generation) that is essentially white noise. by using the selection operator, some candidate solutions from the population are selected. a so-called binary tournament is implemented and the chosen candidate solutions are compared in pairs based on the evaluation of constraints and objectives. the winners of the tournament reproduce the next generation by using recombination and mutation operators. the next generation can be viewed as random generation around the parent by some forms of distributions. the evolution process continues until meeting the stopping criteria. one of the most common stopping criteria is the number of generations. this criterion is problem-dependent, but generally a large number of generations can be used for ensuring solution convergence. the global optimality of the nsga-ii has been proved in many applications which are non-convex and even discontinuous problems [cit] ."
"in order to capitalize on the advantages of big data analytics in an increasingly knowledge driven society there is a need to develop solutions that reduce the complexity and cognitive burden on accessing and processing these large volumes of data in both embedded hardware and software-based data analytics [cit] ) . big challenges stem from the utilization of big data in the real world, since the implementation of real time applications is becoming increasingly complex. this complexity derives from a variety of data-related factors. one factor is the high dimensionality degree which a dataset may possess thus increasing the difficulty of processing and analyzing the data. the interactions, co-relations and causal effects of these high dimensional data parameters in relation to the behaviours and specific outcomes of these systems are often too complex to be analysed and understood by human users. additionally, data can be accumulated from diverse sources and input channels, making online processing very demanding due to the variety of signal inputs which need to be synchronized, and diverse data types which need to be analyzed simultaneously. furthermore, the collected data is often comprised of multiple types of inputs, which are also not always precise or complete due to various sources of imprecision, uncertainty, or missing data (e.g. malfunctioning or inaccurate sensors). moreover, there is an inherent need in real life applications for high-speed storage, processing of data and retrieval of the corresponding analysis results. another factor that should be taken into account is that the method utilized for big data analytics should extract knowledge from data in an interpretable way. the computational techniques deployed to perform this task should make the underlying patterns, which exist in the data, transparent to the person who wishes to utilize and understand them. finally, there is a need for techniques performing online adaptation to incorporate contextual and user-specific elements in their design, and decision-making mechanism, in a user friendly and computationally feasible manner. all the above factors should be reflected in the computational and machine learning techniques utilized in order to process and analyze big data so that successful applications and models can be constructed [cit] ."
here we propose an architecture and organization that can be hosted on the cloud capable to provide fully realtime scalable solution. it is also soa based architecture which uses all benefits of the cloud.
"the existing e-assessment solution is configured to handle at least 160 concurrent executions of exams for students. however, the problem appears when the eassessment system generates the tests."
"focusing on the interoperable data integration and sharing in all production phases for lifecycle data management, this section has covered the key technical issues including product unique identification, online and offline data integration, object data and process modeling and flexible data sharing. the reference system architecture and technical solutions presented in this section are well-suited for iot-enabled manufacturing."
"iot-based business models and manufacturing reference architectures (mras) based on service oriented architecture (soa), restful and cloud computing are also a key support for manufacturing management [cit] . an appropriate service architecture can potentially benefit manufacturing systems in resource scheduling, optimal machine collaborations, manufacturing strategy optimization, and exploration of the capability of manufacturing systems. the exemplar mras are etsi m2m service architecture, smlc smart manufacturing platform, onem2m harmonized reference architecture, and microsoft dira, etc. [cit] the common targets of the mras are openness, interoperability, inter-module collaborations and optimization of the information flow and industrial operations."
"as described in section iii, the machines on the picknpack line identifies the product items using uuids by calculating the encoder position which is circulated to the machines by the thermoformer in the beginning of the line. each machine on the line can timely identify the product item in its workspace and integrate the data generated in its operations with uuid."
"1) an efficient method was proposed to preprocess raw eeg data into a 3d image form suitable for a cnn, which integrate multi-channel information; 2) this is the first time that the deep cnn with 3d kernels was applied into the epileptic datasets. in addition, we proposed instructive settings to help the cnn perform well in the seizure detection task."
"people with uncontrolled epilepsy suffer uncertainty when a seizure occurs, the diagnosis of seizure was a lack in remote areas because of limited medical services [cit] . for examining epilepsy patients efficiently, we hope to develop an automatic seizure detection system to guide doctors. fig. 4 the architecture of 3d cnn. 3d cnn network has 4 convolution layers, 3 max-pooling layers, and 2 fully connected layers, followed by a softmax output layer. all conv3d kernels are 3*3*3 with stride 1 in both three dimensions; all pooling layer kernels are 2*2*2. the first fully connected layer has 4096 [cit] output units deep learning opens the new gate of intelligent diagnosis in medical healthcare, especially in eeg signal processing. the lstm network was able to predict all 185 seizures, providing high rates of seizure prediction sensitivity based on different pre-ictal time window in the public datasets [cit] . the proposed deep learning approach combined the time-frequency and cnn achieves a sensitivity of 81.4, 81.2, and 75% in public dataset [cit] . the deep learning applied to the hidden layer makes the expression of data as specific as possible so as to obtain a more efficient representation of eeg signals."
the pervasive sensing techniques are the key building blocks for iot applications. the sensors and embedded electronics interface the physical world with information systems by sensing and monitoring the physical variables and operations.
"through the m2m messaging protocols, the rfid system can talk to the pickrobot and packrobot to notify them the crates' ids in their workspaces. it can also receive the encoder position and calculate the uuids of product items that the rfid tags are placed on by the applicator."
"to allow the product items and components to be identifiable through the lifecycle of production is a prerequisite of plm, and the suitable models to describe the product data and operations are crucial technique issues. this section presents an object-centric modeling method for inter-phase data integration."
techniques pre-label identification/integration encoder position and virtual uuid pre-and post-rfid integration encoder position and rfid applicator inter-phase integration rfid and object-centric model outgoing logistics data integration rfid and object-centric model data sharing for trading phases rfid encoding and object-centric model
"to seamlessly combine the industrial processes, a product at any production phase is defined as an object. the objects can be integrated or processed to new objects at the later phases of production. as shown in fig. 4, each object has an object id, trade id, class attributes, and the features of objects are defined by the class of objects. it also has the properties of previous and next phases object ids to integrate with its previous and next phases. therefore, the features of the objects at all production phases can be accessed through the links between object ids, and the automatic generation of the links between objects can be achieved using the iot sensing technologies like rfid."
"rfid and qr code are the most suitable candidates for object identification, which have already been used in industry, logistics, and retail, etc. uhf rfid outperforms the other candidates in reading range, speed, data capacity, security, and therefore has gained wide acceptance. the electronic product code (epc) rfid standard developed by epc global in cooperation with auto-id laboratories provides more details for object identification [cit] . normal epc class 1 gen 2 tags have a 96-bit general identification number (gin-96), which contains a header, an epc manager, an object class and a serial number. further schemes define more powerful codes, such as sgtin-96, sscc-96, and sgln-96, where company prefix, item reference, etc. can be encoded."
"classification stage in this stage, each cnn branch can learn features from different stages. the input of the several branches is the data processed in 3d image reconstruction stage. after the feature extraction stage and reduce overfitting stage, the features obtained by each fig. 1 the single-channel eeg recordings illustrating typical brain states. the typical brain states of epilepsy patients include pre-ictal, ictal and interictal three states. an hour segment before each seizure was defined as a pre-ictal state. neurophysiology experts annotated ictal state. eeg data of the signal that were neither pre-ictal nor ictal defined as inter-ictal states. the figure represents the whole process of brain electrical signal seizure a b fig. 2 overview of the pipeline used for seizure detection using 3d cnn branch are merged. the outputs of the model are the predicted category labels."
"for on-manufacturing-line industrial operations, the rfid tags can only be labeled on the products at a certain stage. the pre-rfid stages also need to identify the product items to build a connection with the later stages. thus, a virtual uuid is applied for pre-rfid stage identification and integration."
"according to the related work, the concept of plm and its benefits in combining all lifecycle phases of manufacturing for product development has attracted interests in many studies. due to the complexity of product lifecycle processes and communication barriers, the unique identification and unified management of distributed and heterogeneous product data that cover all the lifecycle phases are still challenging tasks. most studies focus on the system architecture design and data modeling for supply chain data management [cit] . it lacks indepth investigations to design from a system perspective to bridge the gap between the on-manufacturing-line mechanical operations and the information technologies. therefore, to build a loose coupled interoperable information flows between concerned entities with iot sensing and messaging techniques covering the entire lifecycle of products especially onmanufacturing-line procedures and inter-phase connects is a critical task for plm in today's manufacturing industry."
"as shown in fig. 8, the integration of the online integrated data with offline data is achieved by combining the rfid with uuid in three intersections: input crates with raw food materials at pickrobot, rfid labelling at rfid applicator, and output crates with finished products at packrobot. 1) for the pickrobot module, it is notified the valid raw material input crate identified by antenna 1 of the rfid system. it also calculates the encoder information and identifies the uuid of the food package in its workspace. when picking up raw food materials from the input crate and placing it into the empty package in its workspace, the pickrobot registers the input crate rfid as the pre-phase object id (source material rfid) property of the package. the item's uuid is linked with its raw material rfid. 2) for the rfid applicator, it calculates the encoder information and determines the uuid of the product item in its workspace. it then places an rfid tag on the product item and registers the rfid recognized by the antenna 2 as the object id (item rfid) property of the product item. the online uuid is linked with the product item rfid. 3) for the packrobot module, it is notified the rfid of the valid output crate in its workspace recognized by antenna 3. it calculates the encoder information and identifies the uuid of the product item in its workspace. when picking up the finished product item in its workspace and placing it in the output crate, it registers the output crate rfid as the next-phase object id of the product item. with the above method, the data generated by the functional modules can be collected and integrated with uuid. the combination of uuids with input crate and output crate rfids integrates the online generated data with the source material information and outgoing information. by combining the uuids and rfids with rfid applicator, the online generated data can be easily accessed with a machine readable approach."
"the mission of this system architecture is to provide a platform to integrate the real-world lifecycle phases in a data collection and management framework with pervasive sensing techniques, process models and a data repository in the cloud."
this technical framework could potentially allow the data collection and sharing between the authorized business partners through the entire product lifecycle. the collected data could benefit the industrial operations and the system optimization.
"suppose the reader antenna is in a fixed position, and let (xi, yi, zi) denote the position of tag i, the rssi can be presented by:"
"the lifecycle data management requires the maintenance of a live information flow for the smart manufacturing machines, robotics, iot sensing and computing terminals through all lifecycle phases. therefore, the efficiency and reliability of network connection, messaging protocols and potential communication barriers are the main challenges to the success of the plm information flow. in addition, since entities in the plm are both data generators and data consumers, iot collected data should be visible to authorized stakeholders in an interoperable way. the security and authentication for data access, data collection and data sharing are also critical issues."
"since the industrial environment may introduce multi-path effect to the rf propagation, the linearity of the above relationship is not as good as ideal free-space environment. as rfid reader always recognize all the ambient rfid tags, it is an important step to filter the tags to determine the one that is located in the workspace of a machine. by installing rfid antennas in fixed positions in the feed-in tunnels of input/output crates, algorithm 1, a threshold-based rfid unique detection method using a pr-x pattern, is employed for the identification of moving rfid objects. by comparing the rssis of the n rfids with the threshold, the one with the minimum distance in a defined error interval is considered the rfid which has just passed by the antenna. this radiation pattern based rfid filter provides a cost-effective solution to simultaneously identify and localize the moving input/output crates among the ambient rfid objects in real time. the real pattern that fits the practical industrial scene can be obtained conveniently using priori experimental data, and the pattern matching is more lightweight compared to peer rfid localization approaches."
"the proposed techniques have contributed to a boundary-less information flow with iot sensing techniques covering onmanufacturing-line robotics operations before and after rfid labelling and offline phases data access between trading partners. the rssi pattern based rfid filter provides a costeffective solution to simultaneously identify and localize the moving rfid objects in real time, which can be conveniently customized to fit the practical industrial scenes using priori experimental data. the encoder position based virtual uuid and rfid dual-identifier identification technique allows itemspecific identification and data integration of pre-and post-rfid operations. the object-centric modeling then enables the inter-phase data integration with previous and next trading partners through the product lifecycle. the product objects then carry the information to the following lifecycle phases using the proposed rfid and qr code encoding method. the presented techniques and practices might be of interest to manufacturing researchers and practitioners for the innovation of their applications."
"recent advances in deep learning in the past decade have attracted more attention in detective and predictive data analytics, especially in health care and medical practice [cit] . it is a powerful computational tool that enables features to be automatically learned from data. previous studies have proven the deep multi-layer perceptron neural network performs better than the traditional methods such as logistic regression [cit] and support vector machine [cit] . related research has shown a 13-layer deep convolutional neural network(cnn) algorithm achieved an accuracy, specificity, and sensitivity of 88.67, 90.00, and 95.00% respectively in the small bonn university public data [cit] . the ensemble of pyramidal one-dimensional cnn models [cit] was proposed to reduce memory space and detection time. recurrent convolutional neural network learned the general spatially invariant representation of a seizure, exceeding significantly previous results obtained on cross-patient classifiers [cit] . the deep unsupervised neural network such as denoising sparse auto-encoder (dsae) was used in automatically detecting the seizures timely, but may miss important information because of sparse strategy [cit] . other technologies such as deep belief network, transfer learning and so on are also applied to seizure detection [cit] . these algorithms based on deep learning lay the foundation of seizure detection research [cit] ."
pre-ictal state: segment with an hour duration before each seizure was defined as the pre-ictal state [cit] . ictal state: neurophysiology experts labeled the clinical seizures. inter-ictal state: the eeg signal data of each patient which was neither pre-ictal nor ictal state were defined as the inter-ictal state.
"3) the performance of the 3d cnn methodology was validated by test data, compared to both 2d cnn and traditional machine learning techniques that have been previously evaluated in the literature."
"to gain the system level interoperability between all entities producing, storing, and consuming production information in a manufacturing system, a wide spectrum of enabling technologies is involved. this section briefly introduces the plm process in iot-enabled manufacturing industry, and then illustrates the building blocks and some related investigations."
"to evaluate our approach, we have measured the proposed algorithm against three studies using the same data, summarized in table 8 . the first method [cit] extracted pre-defined features from the eeg data and use conventional machine learning techniques to classify epileptic stages. this requires much time and it is possible that some information is fully or partly missed in the selected features. the next two deep learning method including 2d cnn and 3d cnn have introduced before, which could learn data patterns automatically. on average, the proposed 3d cnn method performs better than 2d cnn in terms of the multi-channel information, and it outperforms the hand-engineered method with less time and high accuracy. a recent competition on kaggle held the seizure detection contest, the top three winner algorithms [cit] includes the hand -engineered and deep learning methods, but they relied on complex features selected. therefore, the method presented here can be run on an online platform and tested on more data, satisfying the power, resource, and computation that can be implemented in the wearable device."
"the overall study design consists of typical blocks (see fig. 2 ). firstly, due to the multiple electrodes, the multi-channel eeg time series were constructed as 3d images by means of the position of electrodes on the brain. 3d convolutional kernels were tunable to suit the 3d images input. moreover, deep cnn automatically learned the patterns of different stages from the eeg signal, and then the training model was used to test in the held-out data. training and inference phase for 13 patients were calculated using a high-performance computer."
"in table 7, the accuracy of 3d cnn based on multi-channel was 92.37%, the fnr is 11.43%, and the fpr is 6.22%. while the accuracy of the 2d cnn was 89.91%, the fnr was 15.13% and the fpr was 7.57%. the overall recognition rate of the 3dcnn model was higher than that of the 2d cnn, and the recognition rate for the ictal time segment was the highest, followed by the recognition rate of the pre-ictal eeg data. table 8 lists the comparison of the 3d cnn based algorithm with traditional machine learning algorithms as well as the 2d cnn, all of the above methods were trained and tested with the data used in this study. according to the results, the method proposed in this paper not only achieved the best performance but also reduce the hand-engineered time."
"this study proposed a new approach for epileptic eeg classification, which constructed the 3d cnn for multi-channel eeg data. the main advantage of the method is fully utilizing the multi-channel signal information without hand-engineered. the 3d cnn model outperformed the previously heuristic detectors. to our best knowledge, this study is the first try of using 3d cnn algorithm for seizure detection. therefore, it may serve as a benchmark for new work exploring deep learning enabled seizure detection in terms of multi-channel eeg data. further studies need to carry"
"an epileptic seizure is a critical clinical problem [cit] and electroencephalogram (eeg) is one of the most prominent ways to study epilepsy and capture changes in electrical brain activities that could indicate an imminent seizure [cit] . the diagnosis of epilepsy relies on manual inspection of eeg, which is time-consuming and error-prone. research from elger and hoppe found that only less than half of epileptic seizures which patients document were able to record accurately, and more than half of the seizures captured in long-term video eeg monitoring were not reported [cit] . it is of great significance to develop practical and reliable intelligent diagnosis algorithm for automatic seizure detection. although many efforts have been taken to push the field, we must conclude that seizure detection analysis has not made its way into the clinical practice yet [cit] ."
"since a 3d cnn is built in this work, it is inevitable to convert the multi-channel eeg signal into a 3d array (just like the multi-channel image). the conversion must enable to keep most information from the original data. in total, the procedure was divided into two major steps. firstly, the time series were formed into 2d images. in order to suit the cnn kernels, the image was designed as a square, which resolution is equal to the number of points (like 5000*5000). and the image compression was used to reduce the image down to 256*256 for reducing the complexity of computation. then the successive relationship of the different electrodes was selected according to the adjacent degree of the electrodes [cit], and the corresponding 2d eeg images were fused to form a 3d multi-channel image. its structure is [cit], which is presented in the fig. 3 ."
"to deal with the addressed technical issues, this section describes a technical framework for plm of iot-based manufacturing. the corresponding technical solutions for product identification and data integration in iot-enabled highly adaptive manufacturing context are illustrated."
"the rfid traceability system is set up to monitor the input and output crates and the product items rfid labelling, which consists of rfid reader, rfid antennas, rfid applicator, handheld rfid reader and pc controller as listed in table 1."
"to this end, a suggested cloud based system architecture is as shown in fig. 2 . with iot pervasive sensing technologies such as rfid, qr code and barcode, the product objects can be labeled and identified with their ids. the industrial operations on the product objects and the data generated in the operations are recorded and sent to the data cloud, and the transitions between industrial processes are recorded as well. for the manufacturer, the data of source materials can be accessed from the data cloud with their id using iot terminals. it processes the raw materials and save the collected data to the data cloud during the manufacturing. the finished products are labeled with ids for the market to read and access the product data from the data cloud. therefore, with the ids and iot pervasive monitoring, all collected data are available to the source supplier, manufacturer and market for further use. the end users may also be allowed to access the interested information."
"the proposed technical solutions are proved to be feasible to deal with the technical challenges in smart manufacturing: (1) the rssi pattern based unique identification technique can determine a moving rfid labelled item in multiple recognized items in real time, (2) the rfid and uuid dual identifier method can integrate the on-manufacturing-line pre-rfid stages with offline stages, (3) the object-centric model integrates the manufacturing phases with previous and next phases by registering their object ids as properties, (4) the rfid and qr code encoding method allows to encode the data access information in the ids for ubiquitous data sharing between trading partners. the implementations in picknpack practice has demonstrated the feasibility and advantages."
"the machine connectivity and networking technologies build the connections between functional machines, which promise optimized execution of manufacturing strategies by enabling interactive collaborations between machines."
"with the object-centric model and id techniques, the data of a product item at all of its lifecycle phases could be integrated. as shown in fig. 5, suppose there are n phases in the manufacturing lifecycle, when the operations of object i at phase i is completed, machines of phase (i+1) can obtain the data of phase i (pi) by recognizing the trade id (tidi) and object id (oidi). therefore, the data of n phases of a finished product can be accessed by the link of ids, which can be expressed as:"
"product data collection and management has been widely used in the manufacturing industry. however, the challenges facing iot-enabled highly adaptive manufacturing systems to uniquely identify the whole lifecycle of products, especially for pre-label unmanned automations, and provide a through lifecycle boundary-less information flow have not been adequately addressed. the related investigations focus on the information modeling, while critical techniques and systematic strategies have not been reported. the major technical challenges are: (1) item-specific identification and data integration of on-manufacturing-line phases and offline logistics phases, (2) online real-time moving object identification and localization for data integration of highly adaptive operations, (3) manufacturing process modeling for inter-phase data integration and sharing between trading partners, and (4) efficient and flexible data dissemination for ubiquitous data sharing in the trading phases."
"the task of seizure detection includes distinguishing different stages of seizures, which are generally divided into inter-ictal, pre-ictal and ictal periods [cit] . in general, the seizure detection procedure is separated into two parts: feature extraction and classification. there are numerous technological researches based on artificial features and machine learning classifiers [cit] . on the one hand, the time-frequency analysis [cit], nonlinear dynamics [cit], complexity, synchronization [cit] and increments of accumulated energy [cit] methods were used as feature extraction method. on the other hand, the machine learning classifier includes a bayes network, traditional neural network and support vector machine (svm) etc. in fact, feature-classifier engineering techniques have been used successfully in seizure detection tasks [cit] . however, the features were extracted based on a limited and pre-fined set of hand-engineer operations. most importantly, given that seizure characteristics vary among different patients and may change over time, automatically extracting and learning informative features from eeg data is necessary."
"to prove the feasibility of the proposed solutions discussed in section iii, this section presents the practice of the proposed technical solutions in picknpack digital food manufacturing production line as a proof of concept demonstration."
"nevertheless, the deep neural network was well suited for time series classification [cit], it is difficult to learn the corresponding information of multiple electrodes simultaneously. one of the multi-channel analysis is to study different electrodes respectively and finally integrate them [cit] . another method is used by two-dimensional (2d) cnn to learn multielectrodes, neglecting the relationship between the electrodes [cit] . therefore, we present cnn for seizure detection with a three-dimensional (3d) kernel that is accurate and fully automated to an individual's need. this method was originally designed to solve the problem of ignoring the inter-frame information recognition of image sequences in the 2d cnn."
"numerous investigations have demonstrated a gradual transition between the inter-ictal state and ictal state, which is defined as the pre-ictal stage [cit] . thus, the seizure detection could be considered as the classification of three states. in this study, the eeg data collected from clinical patients were divided into three stages: inter-ictal, pre-ictal and ictal stage, as depicted in fig. 1 . the details are as follows respectively:"
"offline offline fig.10 . workflow of inter-phase data integration manufacturer's production batches, (4) collection of source information and registered product ids of a production batch, (5) collection of the product data generated on the manufacturing line. since the rfids of product items are combined with the corresponding uuids, the online generated data and offline data of the product items are integrated. the picknpack manufacturing line was successfully demonstrated in wageningen, netherlands, and holbeach, the uk, and the system on the demonstration site is as shown in fig. 12 ."
"since plm combines the industrial processes through the product lifecycle in iot-enabled manufacturing, each 'thing' including raw materials, machine parts, product components, and finished products are expected to be uniquely identifiable by the functional machines and the data is available to all interested machines in the information framework [cit] . however, traditional process-centric data management solutions are constrained by automatic and unobtrusive identification techniques and strategies for automatic operations when identification labels are not applied. it faces huge technical challenge to uniquely identify products through their entire lifecycles and provide a complete information flow between machines, sectors and enterprises [cit] . in other words, a seamless information integration context for smart manufacturing is not technically ready."
"in this investigation, the received signal strength indicator (rssi) of rfid reading is utilized as the means for unique identification of product items. in theory, the rssi reading is proportional to the biquadrate of the reciprocal of distance. the free-space propagation model can be expressed as follows:"
"time window selection a sliding window analysis usually split the raw eeg data into segments for feature extraction, including overlapping sliding window and non-overlapping sliding window [cit] . since eeg signals are non-stationary data, time window should ensure the stability of data. the overlapping sliding window can guarantee the continuity of data, but it is easy to cause information redundancy. depending on the pre-experiment, the sliding time window for the ictal data is 2500 points (5 s), while for the non-onset period, the sliding time window size is 10 s, and no overlap occurs."
"to determine the product items in their workspace, each machine builds a buffer (buf[0,…,m-1]) to accommodate the uuids and sizes of unprocessed items and calculate with algorithm 2 to obtain the uuids. in algorithm 2, the encoder data of new batches, each include 3 food packages, is put into a data buffer. the software continuously calculates the left and right edges of the front batch. if the left and right edges are both in the machine workspace, the product batch is considered the one in the workspace. the processed product items will be removed from the buffer. with this method, each machine can determine the virtual uuids of the product items in its workspace and register the data of manufacturing operations for integration before rfids are applied. once an rfid is placed on a product item, the rfid applicator calculates the encoder position and determine the uuid of the item which is in its workspace and link it with the rfid to integrate the online generated data. the encoder position based virtual uuids allow the smart machines to determine the product ids for data registration in the early manufacturing stages before product labels are applied. the encoder position technique and virtual uuid-rfid dual-identifier technique have creatively bridged the gap between pre-and post-rfid manufacturing stages for product data integration. the combination of virtual uuids and physical rfids is a novel solution empowered by iot techniques for on-manufacturing-line data integration of smart robotics based manufacturing applications."
"iot-based manufacturing is usually supported by multiple levels of applied technologies, including hardware, software, communications, service platforms, and data science. with respect to iot-enabled manufacturing industry, the building blocks can be classified into four levels: pervasive sensing techniques, machine connectivity and messaging, production management architecture, data analysis and optimization. the interests of this investigation are the former three levels of technologies to build the information framework for production monitoring, data integration and interoperable data sharing."
"flexible machine presence and discovery, ubiquitous m2m communication, and inter-machine understanding are underlying enablers to take advantage of the sensing techniques and put highly adaptive manufacturing into practice. in order to gain the flexibility of interactions between connected machines, many m2m messaging protocols, middleware, and apis are developed by standard development organizations (sdos) [cit], such as message queuing telemetry transport (mqtt), constrained application protocol (coap), advanced messaging queue protocol (amqp), extensible messaging and presence protocol (xmpp), data distribution service (dds), zero-mq (zmq), mtconnect, and open platform communications-unified architecture (opc-ua). they are based on different technologies and for different applications scenarios. the selection of m2m messaging techniques should be based on the in-depth understanding of the messaging technologies and the object iot system."
"in this paper, the 2d cnn model was used to test the single-electrode eeg data and multi-electrode eeg data respectively, and the 3d cnn model was tested for demonstrating the 3d kernels' effeteness compared to other methods. the results are shown in tables 6, 7 and 8. according to table 6, the accuracy rate of the network based on the single electrode data test was 89.95%, the fnr was 15.07%, and the fpr was 7.53%. while the accuracy of the multi-channel was 89.91%, the fnr was 15.13% and the fpr was 7.57%. it demonstrated that more channels from eeg data carried more information and could increase the specificity and sensitivity in medical analysis."
"iot-based plm, as a new manufacturing data management paradigm, is an important concept for future rich sensing smart manufacturing applications. this investigation provides a proof of concept demonstration of exploring iot sensing and networking techniques to bridge the technical gap and build a boundary-less information flow for data integration and ubiquitous access through the entire manufacturing lifecycle."
"with the presented techniques, the picknpack manufacturing line has achieved a lifecycle data collection, data integration and data sharing framework with the smart machines and iot techniques. the interface of the picknpack traceability software shown in fig. 11 gives some data collected in a test of the system, which can demonstrate the functionalities achieved, including (1) modeling of the manufacturing industrial processes, (2) observation the state of peer machines on the manufacturing line, (3) collection of the fig.11"
"in this study, the time series of each channel of eeg data are transformed into images. all channel images consequently were combined as 3d images. in addition, the cnn based on 3d kernels was constructed to perform the classification of different epileptic eeg stages of image datasets. the main contributions of this work are as follows:"
"this investigation aims to determine the underlying challenges for lifecycle data integration and sharing in multiplephase smart manufacturing, bridge the technological gap with iot techniques and object-centric methods, and gain system level interoperability for seamless and efficient data integration through the lifecycle of products. for this purpose, this investigation combines the efforts of the following aspects: (1) rfid and virtual uuid dual identifier for online item-specific product data integration; (2) rfid based online product localization and identification for seamless data integration; (3) object-centric manufacturing process model for inter-phase data integration; and (4) rfid/qr code encoding method for ubiquitous data sharing between product trading phases. finally, the implementation of the presented methods in picknpack food manufacturing line is reported, and the practice proves the feasibility and advantages."
"for highly adaptive manufacturing systems, industrial operations are fulfilled by the smart machines automatically. a key prerequisite is to allow the machines to identify the product items in their workspaces. for rfid labelled products, it raises challenges to uniquely identify an item due to the multiple tag reading property of ultra high frequency (uhf) rfid."
"as shown in fig. 1, with the pervasive sensing technologies, network infrastructure and data sharing architecture, the manufacturing phases such as design and manufacturing as the beginning-of-life (bol), use and maintenance as the middleof-life (mol), and recycling and disposal as the end-of-life (eol) can be observed and the collected data can be stored in a data repository [cit] . the collected data can be used in real time for online decision-making in manufacturing, or for authorized business partners to access the data to optimize their products and production procedures."
"since the rssi values vary with the reader radiation power, gain of antenna, rcs of tags and the influence of the ambient environment, the rssi pattern should be recalibrated for specific antennas, tags categories and new installations. in picknpack line, the motion track of input and output crates are shielded with metal covers to improve the quality of signals."
"training and inference phase a total of 36,000 images dataset was split into a training dataset (30,000 images), a validation dataset (3000 images) and a test dataset (3000 images). the training dataset was used to train the parameters of the model. the validation samples used to validate the model. the test dataset was used to evaluate the trained model. the classification procedure includes the training phase and inference (test) phase. in the training phase, we trained our model using a 10-fold cross-validation strategy. the dataset is randomly scrambled and divided into 10 equal parts. one is selected as the validation dataset to validate the model, and the rest is the training set to complete the training process, each fold data was verified. the aim of this method is to prevent overfitting of the cnn model during training. in the inference phase, the independent test data was used to evaluate the performance of the model."
"the remainder of this article is structured as follows: a review of the enabling technologies and related studies are introduced in section ii. the iot-based plm framework and the corresponding technical solutions are presented in section iii. then, the implementation of the proposed solutions in picknpack food manufacturing line is presented in section iv. finally, discussions are given in section v and conclusions are drawn in section vi."
"a critical step for lifecycle manufacturing data management is the data integration and sharing between different enterprises, including the source material provider, manufacturer and sales. the picknpack line implements the inter-phase data integration and ubiquitous data access with the proposed process model and rfid/qr code encoding method as shown in fig. 5 and fig. 6 ."
"he increasing convergence of iot technologies with the manufacturing industry has created new opportunities for smart manufacturing [cit] . the pervasive sensing techniques provide means for comprehensive monitoring of industrial operations, and ubiquitous machine-to-machine (m2m) communications leverage collaborative automations between machines [cit] . this new industrial computing paradigm featuring pervasive sensing, ubiquitous data access, intermachine understanding, data analysis and optimization has gained interests in both industry and academia, which is marked with industrial iot (iiot), industry 4.0, industrial internet, smart manufacturing, cyber-physical systems (cps), etc. [cit] this iot-enabled new computing paradigm can potentially combine the raw material supply, production operations and product trading, and forge a closed product data management (pdm) loop, where information from any phase is recorded to affect processes and decision-making in other phases. the interoperability introduced by iot technologies therefore promise a technical solution which could streamline the flow of information about products and related processes throughout the products' lifecycles such that the right information in the right context at the right time can be made available [cit] . therefore, the concerned entities could improve production development at all phases of product lifecycle. the collected data can be used for further analysis and the analytical insights can be applied to optimize the products and production processes [cit] . the concept of closed-loop plm (clplm) could benefit the stakeholders in the manufacturing, including raw material suppliers, manufacturers, logistics and supply chain. a wide spectrum of sensor technologies and information and communication technologies (ict) in the iot scope are applied to revolutionize the manufacturing industry with focus on sensor networks, m2m communications, production process modeling, manufacturing reference architectures (mra), and data analytics [cit] . a number of heuristic investigations are conducted to optimize the sensing, communication, and data management at network, software and system levels."
"with iot technologies, the physical devices, material and product items in manufacturing, logistics, and supply chain are expected to be uniquely identified and the corresponding data be integrated and shared in an open information framework."
"to evaluate the seizure detection performance, we used the metrics in table 5 [cit] . standard measurements including sensitivity, specificity, and accuracy were adopted to evaluate the model. according to the above performance parameters, the evaluation indexes are defined as:"
"however, limitations of this work have to be admitted. firstly, this method, all deep learning technology requires sufficient data to train the model and the design of the network is much harder to guarantee to be optimal. maybe other research gets better performance just tuning the small parameters. secondly, few clinical experts in one center labeled the model data. thirdly, the experiment just involves the eeg data type, which neglects other data types from a multi-scale perspective. in order to have a more generalizable clinical validation, the methods should be tested on an extensive and multi-center dataset. further relevant information sources can be readily incorporated into the deep neural networks, such as video, weather patterns, biomarkers, or clinical notes [cit] . detection algorithm which incorporates these additional inputs and the data types is the focus of ongoing work."
"... for some on-manufacturing-line phases, it is not practical to place an rfid label or a qr code on the product items. timing or mechanical encoding based virtual uuid could be used to identify the objects for data integration. when it is applicable to place a label, the label id can be linked to the uuid to access the generated data, which is as shown in fig. 5, the sub-phases 1 to j of phase i. this is a well-suited solution for smart manufacturing, where industrial operations are carried out by collaborative automations of some interactive machines."
"in order to seamlessly integrate the data produced through the production process, some sensory technologies are applied to identify the products and connect the industrial processes. the mainstream product identification techniques are rfid, qr code and barcode. according to the features in information capacity, reading speed, reading distance, directivity, multiple reading, reusability, security, and cost, etc., rfid outperforms visual identification solutions, especially in none-line-of-sight reading. in addition, mechanical encoder and laser trigger are also widely used for the synchronization of mechanical operations. in addition, rfid also offers the functionality of environment or product quality sensing by coupling sensors to rfid tracking [cit] . since the sensing technologies build the intermediary interfaces between physical operations and information systems, they play a critical role in the seamless data integration of iot-based smart manufacturing systems."
"this investigation provides a systematic plm technical framework for smart manufacturing with iot sensing and networking technologies and object-centric methods focusing on the product unique identification, dual-identifier online and offline data integration, object-centric modeling for inter-phase data integration and ubiquitous data access. the proposed solutions are a timely update for the data integration of emerging iot-based smart manufacturing, which have demonstrated the feasibility and potentiality of iot pervasive sensing and networking techniques in enhancing the data management and interactive automations of smart manufacturing. the designs and implementations are a successful practice to introduce iot sensing and computing techniques to innovate practical manufacturing applications."
"this investigation aims to gain system level interoperability in smart manufacturing for plm by taking advantage of iot sensing and networking techniques and object-centric methods, focusing on lifecycle product identification, process modeling, data integration and sharing. the techniques for product identification to combine the lifecycle phases, the manufacturing process models for inter-phase data integration, and the data dissemination methods for data sharing are the focal research topics. the proposed solutions are implemented in picknpack food manufacturing line for online data integration, ubiquitous data access for machine collaborations, and inter-phase data integration between source material supplier, manufacturer and product supply chain. the practice of iot-enabled product lifecycle data integration and ubiquitous data access in picknpack manufacturing line is reported to prove the feasibility of the proposed technical solutions."
"picknpack is a eu funded project which aims to integrate the emerging information and communication technologies (ict) and the state-of-the-art industrial devices such as robots, sensors, and controllers to build a flexible food manufacturing production line. for this rich sensing and communication system, data collection, data analysis, and data based optimization are the central concerns. this data-centric manufacturing flexibility is developed from the connected smart devices and the integration of iot technologies for flexible data exchange and collaborative automations. the underlying ict technologies have built a through-the-line information flow for product data management."
"the process modeling and information flow design for rfid based data management has been a widely studied area in the manufacturing industry. this investigation focuses on the exploration of iot sensing and networking techniques and object-centric methods to build connectivity between onmanufacturing-line robot operation phases and offline logistics phases. table ii gives the key product identification and data integration techniques covering the picknpack manufacturing phases and the corresponding functionalities achieved. compared to some peer rfid based data management solutions which mainly focus on supply chain logistics information collection and distribution, the primary advantage of the presented solution lies in the dual identifier technique and object-centric methods which make it well-suited for the data registration and ubiquitous data access of smart machine based highly adaptive manufacturing applications, especially the prelabel unmanned manufacturing operations performed by smart machines. this allows the automatic and unobtrusive integration of raw material information, product information and outgoing information with minimized human involvement, which is challenging for the manufacturing operations when product labels are not applied."
"however, most deep learning researches adopt the 2d network, which ignores the fact of multi-channel signal processing [cit], table 6 shows that the more channels eeg signal could improve the performance of fig. 5 the architecture of 2d cnn. 2d cnn network has 5 convolutions, 5 max-pooling and 2 fully connected layers with a dropout rate of 0.5, followed by a softmax output layer. conv2d kernels are 3*3 with stride 1or 5*5 with stride1; pooling layer kernels are 2*2 with stride 2 or 3*3 with stride 2. [cit] output units and the second fully connected layer has 1024 output units the network. we proposed the 3d image reconstruction approach to relate multi-channel information, just like in video processing [cit] . in addition, the group normalization, as well as the oversampling techniques were applied to overcome the overfitting of the limited datasets [cit] . compared with the 2d cnn shown in table 7, our strategy achieved a mean accuracy of more than 90%. it demonstrated that there was a reliable and automatic seizure detection system. this is the first study to introduce 3d kernel cnn's for seizure detection."
"as shown in fig. 3, suppose the outside frame is the manufacturing line and small boxes inside are product items moving on the line through the workspaces of different machines. in the coordinate system, the x-direction locations of each machine [xmi1,xmi2] is fixed. the position of a product item on the line can be measured with a precise encoder machine, which broadcasts the virtual uuids, the dimension of newly created product batch and the x-direction displacement."
"when the source material batches are received by the manufacturer, they undergo a subdivision process which divides the batches with rfid labelled input crates. the input crate then registers the batch id (rfid or qr code) as its previous-phase id, consisting of the supplier id and batch id. the source materials in the input crates are made into product items on the manufacturing line. when finished product items are picked up and placed into the output crates, the items are then assembled to a logistic unit registering the unit id as the next-phase id (rfid or qr code), consisting of the outgoing company id and unit id. since the trading partners have data link and permission of the other partners' data repository. the source provider, manufacturer and market can access the product information from their previous and next trading partners. the inter-phase data integration can be presented with fig. 10 . with the designed id encoding format and phase integration method, a lifecycle data collection and sharing framework is therefore achieved. for a product item, the source information, production information and outgoing information can be accessed conveniently via rfid or qr code reading."
"in order to gain interoperable data access from trading partners in the manufacturing chain, a gin-96 based data sharing method is designed as shown in fig. 6 . the trade id and object id are encoded in the object class and serial number sections of gin-96 digits. the trade id encoded in the object class section, which can be a company id, a global location number (gln) of a trade partner, or a global trade item number (gtin) of a product type, is linked to data access url of the object saved in local database or the data cloud. with the data links and object ids, the data of product objects can be accessed from the trade partners' data systems. this method simplifies the data sharing between trade partners by getting the link and object id automatically via rfid reading. in addition to rfid, qr code can also encode the company id and item id for data sharing."
"reduce overfitting stage for the sake of limited available datasets, it is important to prevent the cnn from overfitting and improve the performance of the model. firstly, the equal three stage datasets were adopted. then the dropout strategy was applied in the both of the fully connected layers. dropout strategy makes results in the dysfunction of the weight of some hidden layer nodes. thirdly, considering the size of the epoch, group normalization proposed by he [cit] have replaced batch normalization algorithm [cit] in 3d cnn. group normalization can divide the data into groups, then calculate the mean and variance in each group. it improves network generalization ability and accelerates the model convergence. the comparison results are shown in table 3 ."
"the techniques for product object identification, the models for inter-phase data integration and methods for ubiquitous data access are regarded as the key parameters of plm in smart manufacturing. the integration of the iot techniques with the manufacturing industry and the above-mentioned enabling techniques have provided sufficient technical support."
"the previous studies reveal the great necessity of conducting a hybrid path prediction and the lack of quantitative comparison between the effects of subjective and objective factors on path selection. therefore, this article will establish a hybrid path prediction model, which includes a pre-trip path prediction model and a during-trip link prediction model. the different role of subjective and objective factors in path selection will be examined. using the traffic conditions as representative of objective factor and habits subjective factor, we conduct a quantitative comparison of the effects of two types of factors."
"notably, all the optional links collected in the sp survey are reported by the respondents based on their preferences or familiarity with the road network. but, they may fail to report the potential links that they are unfamiliar with, inevitably. consequently, the model may have a certain deviation in predicting the selection probabilities of the habitual link and the non-habitual links based on such data. further survey may provide potential links completely. moreover, owing to the difference of selected paths from different travelers' different trips, the alternatives of the link prediction model are uncertain. therefore, the habitual and non-habitual links are defined as the alternatives in the binary logit model, which leads to a complicated calculation process. further study should focus on developing more suitable model for link prediction. in addition, rather than defining 9 scenarios, quantitative description of traffic conditions should be conducted in the further study."
"the study exams the different path selection mode in different prediction model, answering the second question in section 1. based on the comparison of the factors in the models, travel habits are top priority in the pre/during-trip path selection."
step 3: adjusted utility calculation the adjusted utility v n of path n is then obtained by multiplying the selection probability of path n in step 1 and the selection probability of each link in path n. the adjusted utility of path n is calculated as:
"moreover, in the absence of real-time traffic volume, the pre-trip path prediction model can predict path based only on the traveler's historical path selection data. then, the network traffic flow can be predicted by aggregating all the travelers' path prediction results to infer the possible congestion areas."
"the rsa assumption is that the rsa problem is hard to solve when the modulus n is a randomly chosen and sufficiently large modular number, and the ciphertext y is a random integer between 0 and n − 1."
"this work has investigated an iot network storage architecture based on mec. to achieve identity privacy and relieve the burden of the end-user, a non-interactive pairing-free id-based prs is proposed, which not only avoids expensive pairing operation and complex certificate-maintenance, but also is proven to be secure in the rom by theoretic analysis. by comparing with other two id-prs schemes, the proposed scheme has some advantages in terms of security properties and computation costs. it is upcoming work to extend it into multi-use prs scheme."
"being advanced computer-based modeling methods [cit], discrete selection modes are widely employed in analyzing travelers' behaviors [cit] . previous studies denote that the logit model has high prediction accuracy in travel behavior analysis such as travel mode, path, and departure time [cit] . therefore, this paper employs the logit model to investigate travelers' during-trip link selection behavior."
"by comparing the verification results of pre + duringtrip path prediction model and pre-trip prediction model, we answer the third question proposed in section 1. the hybrid model can provide higher accuracy. this suggests that the model we proposed is closer to the actual path selection behavior than pre-trip path prediction model by considering the during-trip link adjustment due to real-time traffic conditions. in addition to verify the validity of the modeling results, the research results can be applied in the vehicle or mobile navigation app to facilitate the recommendation of path to travelers. storing the multi-day path selection data, the program can automatically generate the habitual path, and dynamically adjust the link selection according to the real-time traffic conditions. thereby the path and surrounding facilities, such as the parking lot and gas station, etc., can be recommended quickly to travelers. in addition, under the background of big data, short-term traffic flow prediction and congestion area determination can be conducted by collecting the path and link prediction results of each traveler."
"according to the prediction results depicted in table 5, travelers are also affected by the traffic conditions in addition to travel habits. in this section, the habits and traffic conditions are taken as the representatives of subjective and objective factors respectively to establish the during-trip link prediction model. the influences of two factors on link selection are quantitatively calculated and compared."
"proof : for delegatee security, the goal of this property is to protect the delegatee from the collusion attack between the delegator and the proxy. because our proposed scheme is non-interactive and the delegator can independently generate re-signing key by its private key sk id b, re-signing key queries is unnecessary. for h-queries,h-queries, keyextract queries and signing queries, they are the same as those of theorem 1."
"we use the normalized feature vectors to train a two class random forest (rf) [cit] to classify each superpixel as brain or non-brain. our dataset has pixel-wise labels so we assign each superpixel n i the class label l i that corresponds to the label with the highest frequency inside the superpixel. after the initial classification we obtain a probability map and use this to train another second random forest along with 10 % of the most important features used for training the first classifier. this produces an auto-context classifier [cit] that can increase the classification accuracy. the output volume is then filtered by finding the largest 3d component, which is the brain mask in this case, followed by a convex-hull extraction [cit] to obtain a clean homogeneous segmentation."
"the selection probabilities of habitual link and nonhabitual links under different traffic conditions are calculated with the binary logit model presented in section 4.2. based on the probability of habitual link p u,1 n, we then calculated the probability of each non-habitual link, as depicted in (9) . then the ration conversion is obtained to determine the selection probability of each link of path n."
"assume that there exists an adversary adv 2 breaking the limited proxy security in our proposed scheme, then we can create an algorithm b 2 to solve the large number factorization problem. to do so, b 2 needs a series of queries."
"internet of things is the concept of connecting physical devices to the internet and to other connected objects. the iot devices and physical objects with embedded sensors constitute an enormous internet of things platform, which can offer or predict the most valuable information for some applications used to address specific requirements by integrating data from the heterogeneous devices and applying data analytics. although iot devices are resource-constrained, they produce a huge amount of data due to a mass of iot devices, large-scale iot deployments . to obtain the rich benefits from the iot, it is essential to provide adequate networking and computing resources to implement fast response for iot applications. cloud computing becomes the main booster"
"for habitual path 1 and non-habitual path 2, the nodes and the corresponding links are shown in tables 12 and 13, respectively. because there are several road segments composing a link, we choose the bottleneck value (maximum value) of tpi among the several segments from the link to represent the traffic conditions of the link. the real-time traffic conditions of the optional links on feb.9 [cit] are shown in table 14 ."
"re eventually, a 2 outputs a forged signature in a nonnegligible advantage. by the same analysis in the proof of theorem 1, b 2 can obtain the factorization of n in a non-ignorable probability."
"the results indicate that the selected probability of the nonhabitual link is higher than that of the habitual link only when the habitual link is extremely congested and the nonhabitual link is congested or uncongested. except, the probability of choosing the habitual link is higher than that of the non-habitual link under the other seven scenarios, suggesting that travelers tend to choose the habitual link when the traffic conditions of the two links are the same. even if the non-habitual link is uncongested, and the habitual link is congested, the traveler still chooses the habitual link. this denotes that travelers are more dependent on the habits to choose path under most of the traffic conditions."
"few studies have examined both objective and subjective factors. [cit] developed a path recommendation system (mapmobyrek) to provide personalized trip path for users, which was a system that the recommended items were shown on the map based on the space-time information and long-term preferences. however, the research did not compare the effects of space-time and long-term preferences on travel decision."
"based on the pre-trip path prediction and during-trip link adjustment presented in section 4.1 and 4.2 respectively, this section will propose an integrated process of path prediction."
"the final sample includes 936 records of 104 respondents, whose rp data are valid. according to the data statistics, we obtain the selected frequency of the habitual link and the non-habitual link in different scenarios (shown in table 4 )."
"to show the unforgeability of the proposed id-prs scheme, we divide security models into external security and internal security inspired by ateniese-hohenberger's security model [cit] ."
"the rp + sp survey data of the 104 respondents are employed for the model. in detail, the first 15 groups of actual path selection data per person among the former 52 respondents are utilized to calibrate the pre-trip path prediction model, while the last 10 groups are used as verification data. as for the during-trip link prediction model, the 468 sp samples of the former 52 respondents and the 468 sp samples of the latter 52 respondents are used as calibration data and verification data, respectively."
"based on the 468 sp samples of the former 52 respondents, the model is calibrated by using the maximum likelihood method. the results are depicted in table 7 ."
"internal security means that the scheme can be against the attacks which are from internal entities such as dishonest proxy, malicious delegators or malicious delegatees. for different entities, internal security is divided into three types of different security games. 1) limited proxy security: this security ensures that the dishonest proxy is unable to create a signature of messages on behalf of the delegatee or the delegator unless message had already been signed by the delegatees. the notion requires the following probability be neglected:"
"where, t is the average value of actual travel time, t d is the travel time under desire speed. there are w links in the network, the length of link w is l w, q w is the traffic flow, t w and v w are the actual travel time and speed respectively. t dw and v dw are the desire travel time and speed respectively."
"the results indicate that most respondents have 1 to 3 selected paths for each type of trips. especially, for hometo-work trips and work-to-home trips, most of travelers have 2 to 3 paths, only a few of them have 4 paths. whereas compared with the weekday trips, there are fewer optional paths for weekend trips, of which there is generally 1 to 2 paths. this indicates that weekend trips may tend to follow path selection habits comparing to weekday trips. the primary reason is that the traffic congestion is more serious for commute trips on weekdays, which makes the traffic conditions dominate in path selection. contrarily, the traveler considers the habit more in path selection for shopping trips due to the good traffic condition on weekends."
"in mec, terminal devices are resource-constrained. however, most of existing proxy re-signature schemes are based on pki and use time-consuming pairing operation. it will increase the burden of them if existing proxy re-signature algorithm is directly applied to terminal devices. thus, the goal in this work is to design a light-weight proxy re-signature scheme which avoids expensive pairing operation and intricate certificate management."
"where x u,i kn is the value of variable k for a traveler choosing alternative i at node u in path n. β u,i kn is the corresponding coefficient [cit] . k is the total number of variables. the probability of choosing alternative links is as follow:"
"a. system model for mec storage architecture in iot, its system model is illustrated as fig. 1 . this architecture is roughly divided into three layers, cloud computing layer, mobile edge computing layer and the end-user layer."
"of iot applications due to its adequate storage capacity and data processing capacity. in cloud computing-based storage architecture, all terminal data needs to be uploaded to the centralized cloud center, and after they are processed, the results are returned to the terminal devices. due to being far from the end-users, this kind of storage architecture brings tremendous pressures to network bandwidth and transmission cost. furthermore, network performance will worsen with increasing data size. it results in high latency of the networks. obviously, it is unacceptable for time-sensitive iot applications."
"where o sign (·) denotes an oracle which simulates the first-level signature algorithm which outputs a signature when inputting an identity id i, a message m and a condition c. q denotes a set of three triple (i, m, c). 2) delegatee security: the goal of this security property is to protect the delegatee to resist the collusion attack of the delegator and the proxy. for the sake of clarity, let the index of the delegatee be 0. in the security game, the adversary is allowed to issue a signing query to obtain the first-level signature of the delegatee. the proxy and the delegator are considered as the adversaries. they can generate re-signing key and obtain the second-level signature by re-signing key. the adversary's purpose to fabricate the first-level signature of a new message (or condition). the notion requires the following probability be neglected:"
"according to the rp survey data of multi-day trips, firstly, the selected frequency of each path for each type of trips is statistically calculated and taken as its initial probability. successively, the transition probability and the transition probability matrix among different states are then calculated."
"to understand drivers' path selection behaviors, we divide the process into two parts, i.e., the pre-trip path selection and the during-trip link adjustment. thus, the hybrid path prediction in this paper is distinguished between the pre-trip path prediction considering habits, and during-trip link prediction considering both habits and real-time traffic conditions."
"it means that if the simulation does not abort, when adv 1 creates a valid forgery, b 1 can factor n in a non-ignorable probability."
"for most of the existing studies on path selection, objective factors, referring to factors over which drivers have little control, such as real-time traffic volume and the number of intersections, etc., are the major basis for optimal path calculation. for instance, some path guidance systems, such as the american advance system [cit], the ali-scout system in europe [cit] and the vics system in japan [cit] primarily consider real-time traffic volume to determine the optimal path. besides objective factors, some travelers also follow their subjective intentions, such as the previous travel experience and travel habits, etc. for example, a traveler may remember his/her previous experience, learn from it. they sometimes choose a habitual path even if it is congested. the potential reason is that they are not familiar with the nonhabitual path, and consider it is too risky to choose the vicarious path. previous studies underscore that subjective factors and objective factors both play a role in travel decisionmaking [cit] . in addition, with the rapid development of automatic driving technology, many researchers simulated the driving behaviors of path choice based on self-learning mechanism. the subjective information, such as historical data of path selection and driving preference, is often taken as crucial factors in these studies [cit] . however, few studies conducted a quantitative comparison of the impacts of subjective and objective factors on path selection. understanding how subjective vs. objective information affects drivers' path choice behaviors and how drivers may choose driving path can have important applications on path prediction. therefore, the traffic conditions and habits will be taken as the objective and subjective factors in this paper and the effects of the two on path selection behavior will be quantitatively fitted and compared."
"in this work, we only discuss unidirectional and single-use non-interactive id-based prs since it sufficiently satisfies our storage architecture. because the non-interactive id-based prs scheme avoids the interaction between the end-user and the hospital and removes certificate maintenance, it reduces communication overhead and computation cost of the end-user, and is suitable to our storage architecture. for a non-interactive id-based prs scheme (id-prs), it is composed of the following algorithms:"
"in this paper, the path with the highest selection frequency in each type of trips is defined as a habitual path, and the other optional paths non-habitual paths. the selection frequency of habitual or non-habitual paths is calculated and shown in table 2 . the results indicate that the proportion of choosing habitual path for all the four types of trips is greater than that of non-habitual paths, meaning that the traveler will choose the habitual path for travel in most cases. moreover, travelers are more inclined to choose habitual path for weekend trips than for weekday trips. no matter whether it's a weekday trip or a weekend trip, the return-trip is more inclined to follow the habits than the go-trip. the underlying reason is that the go-trip is more limited by arrival time, so it is necessary to consider the traffic conditions to a greater extent."
step1: pre-trip path prediction considering habits the selection probability of each optional path p n is calculated with the habit-based markov prediction model based on the actual multi-day path selection data.
"the results indicate that for all the respondents, the impacts (the coefficient in the model) of driving age, occupation, departure time and real-time traffic conditions on link selection are significant. among these factors, driving age is negatively correlated with the habitual link selection, which suggests that the traveler's link adjustment behavior becomes more flexible with the increase of age. a traveler, who is a staff of state-owned enterprise/private enterprise, student, teacher/lawyer/doctor and retiree, is more valued on travel habits. while a traveler, who is a civil servant, service worker, farmer, or full-time housewife, is more concerned about realtime traffic conditions. in addition, travelers are more likely to follow travel habits when the departure time is in the morning peak period, contrarily, traffic conditions in the evening peak period."
"to overcome the problem of sparse descriptors for superpixels, we construct superpixel graphs using the distance between the centroids of superpixels as edges [cit] . these edges are weighted based on both spatial and geodesic distances. we first compute the centroid c i for each n i superpixel. then we identify r neighbors based on the similarity score between two superpixels n i and n o with centroids c i and c o . the similarity between two superpixels is defined:"
"so according to the tpi, for each link, the traffic conditions are divided into three states, namely, uncongested, congested and extremely congested. consequently, there are 9 scenarios of traffic conditions on the two types of links (table 3), which are more intuitionistic than the precise value of traffic condition for respondents. in order to present the traffic conditions to respondents visually, a traffic condition simulation is constructed by vissim. then the traffic flow operation of each link under different traffic conditions was made into gifs, and those were linked together according to the combination of traffic conditions (c 1 -c 9 ), making the respondents to choose link under different traffic condition within the scenario of dynamic traffic. for example, fig. 2 shows the screenshot of dynamic diagrams of two links, when the habitual link a is extremely congested and the non-habitual link b is congested."
"our results suggest that both habits and real-time traffic conditions affect travelers' link choice, although habits appear to have a stronger influence than traffic conditions. the results answer the first question that we proposed in section 1, and suggest that travelers tend to choose habitual link under most of the traffic conditions. they will select the non-habitual link only if its traffic condition is obviously better than that of the habitual link. transportation managers should consider both objective factors and subjective factors when establishing effective management measures to improve drivers' path selection behaviors, while subjective factors should be given more attention than objective ones."
"with the development of social economy and the increasing number of car ownership, traffic congestion is increasingly serious in urban area. on a micro level, effective path guidance enables drivers to find the optimal driving path, thus to increase running speed and reduce delay. while on a macro level, this is conducive to improving the utilization of road resources, alleviating traffic congestion and also helping to reduce environmental pollution and energy consumption caused by traffic congestion. therefore, it is important to identify ways to make path selection more efficiently."
"we have developed an automatic framework for localizing the brain in fetal mri scans using superpixel graphical models. superpixels have enabled the proposed detection algorithm to be faster and more efficient than using pixels for classification. also, extending the extracted features from the individual superpixels to include features from the neighbors using superpixel graphical models, have provided more information about the image context instead of using only the local information. the evaluation results achieved 94.55 % accuracy for the brain detection, which shows the potential of extending the proposed approach using superpixel graphs to segment other fetal organs such as the heart, lung, and placenta. according to the recent studies [cit], the placental functions affect the birth weight as the placenta controls the nutrients transmissions from the maternal to the fetal circulation. moreover, the extracted brain can be used for developing automatic motion correction and registration techniques for fetal mri."
"moreover, the impacts of real-time traffic conditions on the link selection cannot be ignored according to the estimation results. only under the combined traffic conditions of c 2, c 3 and c 6 (shown in table 3 ), travelers are unwilling to choose the habitual link, which indicates that the traveler will select the non-habitual link unless its traffic condition is obviously better than that of the habitual link. except for the three cases, under the other six scenarios of traffic conditions, travelers tend to choose habitual link, which suggests that travelers tend to choose the habitual link under most of the traffic conditions, implying that travel habits more strongly affect link selection than traffic conditions. based on the 468 sp samples of the latter 52 respondents, we calculated the prediction accuracy of the model. the results (shown in table 8 ) indicate that the overall hit ratio of the model is 81.4%. the hit ratio of the habitual link prediction (89.6%) is higher than that of non-habitual link (64.9%). the potential reason is that the influence of travel habits on the selection of non-habitual link is overestimated, and the influence of other factors, especially traffic conditions, is underestimated by the model."
"the rp + sp survey data of the 104 respondents are employed to verify the model. the first 15 groups' actual selection data of each respondents are used to calibrate model parameters, and the last 10 groups' data are used to verify the model. the next 10 days' path selection for each type of trips were predicted for 104 respondents, which were compared with the actual path selection results in the next 10 days. the prediction accuracy for each type of trips is shown in table 9 ."
"the remainder of this paper is organized as follows. in section 2, we present a review of the relevant literature in particular on the path selection. section 3 presents a description of the data and the study area. in section 4, we develop a pre + during-trip path prediction model and investigate the impacts of objective vs subjective factors on path selection. section 5 presents a case study using the developed model. the paper is concluded with section 6, in which we summarize our findings and discuss our study limitation and direction for future research."
"as is known to all, iot devices are in the open networks, data is transmitted in the open channel, and the user's identity privacy and data security are confronted with great security threats. in the iot environment, most iot devices have limitations in terms of energy capacity, storage capacity and computing power, it makes that conventional security measures cannot be directly implemented in resource-constrained iot devices. various types of light-weight digital signature schemes, such as id-based signature and online/offline signature, are proposed to ensure the integrity and authenticity of the iot data transmitted over open channels. according to digital signature's property, anyone can verify the validity of the signature by the signer's public key. in verify phase, these schemes suffer from private information leakage of the signer since the identity or public key of the signer is needed. thus, it becomes an important challenge how to ensure data integrity without leaking public key information since it contradicts non-reputation of digital signature."
"the during-trip link prediction model is then applied to calculate the selection probability of each optional link in path 1 and path 2 respectively. the results are demonstrated in table 15 and table 16 . (12) and (13) are employed to calculate the utility v n and the final selection probabilities of path 1 and path 2. the results indicate that for home-to-work trips, the selection probabilities of habitual path 1 (p 1 ) and non-habitual path 2 (p 2 ) for respondent #16 are 0.838 and 0.162 respectively. consequently, the probability of choosing habitual path increases after considering the traffic conditions. according to the selection probability of each link for the habitual path, the adjusted habitual path is calculated and illustrated in table 17 and fig. 6 . the prediction results are consistent with the actual records that the respondent #16 filled in for the home-to-work trips."
note that it is unnecessary for the adversary to issue the second-level signature query since it can be obtain by issuing re-signature query and the first-level signature query.
"the path selection data of respondent #16's home-to-work trips are chosen to perform the case study. the rp survey data of respondent #16 indicate that he has two paths for home-to-work trips, which are the habitual path 1 and the non-habitual path 2, as depicted in table 10 and fig. 5 . the actual multi-day path data of respondent #16 are shown in table 11 . the pre-trip path prediction model is utilized to calculate the selection probabilities of habitual path 1 (p 1 ) and nonhabitual path 2 (p 2 ), which are 0.786 and 0.214, respectively."
"the verification results outweigh those of the pre-trip prediction model (shown in table 5 ). the underlying reason may be that the during-trip link adjustment due to real-time traffic conditions is considered based on the pre-trip path prediction model. the results indicate that the improvement of path prediction accuracy on commute trips is much obvious, which increases from 68.27% and 65.38% to 82.69% and 78.85% for home-to-work trips and work-to-home trips respectively. the potential reason is that traffic conditions play a more important role in path selection for the commute trips than for the shopping trips. consequently, it is more possible that the traveler adjusts path owing to traffic conditions in commute trips than in shopping trips."
"and it can also provide limited computation and storage services. in our architecture, it servers as the proxy and is responsible for converting the end-user's signature into its signature in order to ensure the privacy of the end-user's identity. the end-user is capable of offloading its computation task to mec server in this layer."
"the majority of the studies on path selection are duringtrip path prediction. [cit] calculated the probabilities of alternative paths by considering the during-trip traffic information. [cit] discussed how to take into account en-route choices in utility-based route choice modeling, and conducted two numerical experiments to describe a driver's dynamic taste for different routes at different decision nodes. [cit] extended the linear programming cell transmission model-based dynamic traffic assignment (lp ctm-dta) model to account for travelers' consideration of uncertainty regarding traffic condition. [cit] analyzed drivers' path selection behavior under multi-source traffic information provided by advanced traveler information systems."
"the study area of this paper is the urban area of changchun, which is the provincial capital of jilin province, with a population of 7.489 [cit] . the built-up area of central city in changchun is 506.33 square kilometers. [cit] (autonavi software co.), the morning peak period and the evening peak period in changchun are 7:00-9:00 and 17:00-19:00 respectively. congestion delay indices of peak and off-peak period are 1.90 and 1.48 respectively. [cit] ."
"in this end, adv 3 outputs a forged signature in a non-negligible advantage. by the same analysis in the proof of theorem 1, b 3 can obtain the factorization of n in a non-ignorable probability."
"in this paper, we propose a hybrid prediction model for drivers' path selection, which composes of the pre-trip path prediction model and the during-trip link prediction model. the results indicate that the path selection behavior revealed by the hybrid model align with the behavior from the survey data collected in changchun. in addition, the impacts of subjective factors (habits) and objective factors (real-time traffic conditions) on driving path selection are examined. markov model is constructed to identify the pre-trip path selection behaviors with the consideration of habits. in addition, a binary logit model is developed for during-trip link prediction to consider the impact of both habits and traffic conditions."
"the results suggest that the prediction accuracy of the model is generally acceptable. compared with weekend trips, the prediction accuracy of weekday trips is relatively low. the underlying reason is that the commute trips on weekdays dominate during the morning and evening peak periods, when traffic congestion often occurs. travelers sometimes switch the path owing to real-time traffic conditions, which leads to some inaccurate cases in fully habit-based path prediction. while for shopping trips on weekends, travelers are more inclined to choose the path depending on their subjective habits due to the small possibility of traffic congestion occurring in this period, making the prediction accuracy relatively high. the results underscore that habits have a more significant impact on the path selection of shopping trips than on that of commute trips, while both habits and traffic conditions have considerable influence on path selection of commute trips."
"here d(·, ·) is the euclidean distance, and d is the length of the diagonal of the 2d image. this normalizes the score to be between [cit] . the closer f r (c i, c o ) is to 1, the smaller is the distance between these two centroids. we next assign weights w j for the extracted r neighbors candidates based on the geodesic distances between their centroids. the geodesic distance is estimated using:"
"external security means that the outside adversaries can launch security attacks. these outside adversaries are different from the proxy, the delegatee and the delegator. their target is to create the delegetee's signature or the delegator's signature on a new message m * . under this security model, the adversary can adaptively issue signature queries and re-signature queries. the security requires the following volume 7, 2019 probability be ignorable."
"previous studies indicate that the markov model has a high accuracy in multi-day travel behavior prediction [cit] . therefore, the markov model is utilized in this paper to predict path selection behavior by considering habits. different trips will be regarded as different systems in markov chain. hence, the trips of home-to-work, work-tohome, home-to-shopping and shopping-to-home are defined as four systems, which are independent and have no mutual influence on each other. in each system, there are different states representing different optional paths in accordance with this type of trips. for example, there are three optional paths for a traveler's home-to-work trip, i.e., habitual path i, nonhabitual path ii and non-habitual path iii. then this traveler's home-to-work trip is defined as a system, including three states."
"path selection patterns are the results of drivers' decisions regarding where to go. there are primarily three types of studies on path selection: pre-trip path prediction, duringtrip path prediction and hybrid path prediction. among them, during-trip path prediction occupies the main account of studies [cit], in which travelers make path selection decisions based on the real-time traffic conditions. the during-trip path prediction can compensate for the defect in the pre-trip path prediction (e.g., [cit] ), which usually limits to travel time or travel cost and does not timely adjust path according to real-time traffic conditions. either pre-trip path prediction or during-trip link prediction has important application in urban traffic planning and management. for example, the pre-trip path prediction can be used not only to recommend the best driving path for travelers, but also to predict road network traffic flow and infer possible congestion areas. the duringtrip link prediction can be utilized to recommend the path with the shortest travel time based on the real-time traffic conditions, and provide the location information of the traffic facilities around the path, such as the parking lot and the gas station. however, hybrid modeling, which combines pre-trip path prediction and during-trip link prediction, makes path prediction much efficient and accurate. one of the reasons is that travelers may both make a pre-trip path selection and adjust the path according to the traffic conditions. there is a vivid evidence underscoring that a hybrid path choice model can preferably describe travelers' path selection decisions [cit] . in the process of hybrid path prediction, the results of the pretrip path prediction can enhance the accuracy of the duringtrip link prediction. also, it can reduce the workload and improve the calculation speed of during-trip link prediction, such as determining the optional driving links, examining the previous traffic flow as well as collecting location information of traffic facilities."
"instead of migrating data to the cloud, it may be more efficient to transfer the applications, storage, and processing closer to the place in which the data is produced by the iot devices. to handle the aforesaid problem, mobile edge computing (mec) technology can be introduced [cit] . as one extension of cloud computing services, mec bridges the gap between the centralized-cloud architecture and the decentralised cyber-physical devices on the networking edge. the terminal user can offload its computation task to the edge servers/nodes via a wireless access node. thus, mec plays very important roles in promoting the applications of resource-limited devices in mobile cellular networks."
"to the best of the authors' knowledge, most prs schemes are built on traditional public-key-infrastructure (pki). in pki, when the signer's public key is employed, its validity needs to be authenticated by a certificate issued by a certifier authority (ca). whereas, maintenance of certificate might bring a heavy burden to the signer."
"in this paper, we exam the three types of path selection predictions: pre-trip path prediction, during-trip link prediction and hybrid path prediction. the first one is associated with habits whereas the second one involves both habits and traffic conditions. we further study path selection by integrating pretrip and during-trip together. we seek to answer the following three questions: 1) which one, subjective or objective information, plays a more important role in affecting path selection? 2) how is external or internal information associated with path selection? 3) what are the applications of the three models?"
"we build a bag of features using dense scale invariant feature transform (dsift) [cit] descriptors. this is done by first computing sift [cit] descriptors for each pixel in every 2d image in the training set at a fixed scale and orientation. a k-means clustering is then performed on these descriptors and their centers are used to form a dictionary of k words. when collecting dsift descriptors from pixels we can then find their closest matching word from this dictionary, aggregating the frequency of words in each superpixel n i into one histogram h i with k-bins. the k-dimensional histogram acts as the feature descriptor for each superpixel. however, the descriptor is constructed in such a way that only contains local information about the superpixel itself. this leads to a loss of the large-scale image context. also, due to the nature of the superpixels, their histograms of features tend to be sparse. most of the dsift descriptors within a superpixel are likely to be mapped to the same word."
"we demonstrate work in progress towards a framework that estimates 4d anatomical models from longitudinal tbi images. our framework is fully automatic and leverages information from a different domain (brain tumor) to generate appearance models via domain adaptation. in addition to the new 4d anatomical modeling, we also presented a new domain adaptation method for generative kernel density models, integrated with our anatomical model in a single objective function (eq. 2). results on 3 tbi subjects show that our automatic method yields segmentations that match ground truth of manual segmentations. furthermore, our method generates diffeomorphic deformation models as well as non-diffeomorphic probabilistic changes that have potential for analyzing and characterizing changes of normal appearing tissue and lesions. in the future, we will quantify temporal brain changes across a large set of tbi patients which were exposed to different treatment strategies. our approach has potential to significantly improve regional and connectivity analysis of individuals relative to a population [cit], by making use of the mapping of a normative template with associated parcellation labels to tbi subjects, without tedious manual input."
"3) relief algorithm kira and rendell proposed the original relief algorithm to estimate the quality of attributes according to how well their values distinguish between examples that are near to each www.ijacsa.thesai.org other [cit] . the algorithm steps are stated in fig. 3, where diff function calculates the difference between the same attribute value (a) within two different instances i1 and i2 as in (4 there are many evaluation measures used to evaluate the performance of the classifier based on its confusion matrix resulted from testing. we will describe in more details some of the commonly used measures to be used later in our experiment."
"id3 algorithm uses some splitting criterion function to select the best attribute to split with. in order to define this criterion, we need first to define entropy index that measures the degree of impurity of the certain labeled dataset."
"given a measure for the quality of distribution of points on the pareto front and a scalarizing function, generalized decomposition (gd) can be used to create a set of weighting vectors that will result in sub-problems that produce a set of solutions that are optimally distributed according to the given measure [cit] . the optimality of the resulting distribution is contingent on the convergence of the sub-problems and knowledge of the pareto front geometry a-priori to the solution of the problem. nevertheless, it has been shown that when the pf geometry is unknown a-priori, which is most often the case, assuming an affine pf geometry still produces results that are several orders of magnitude better, in the selected measure, than commonly used alternative methods [cit] ."
"using the same line of reasoning we expect that if we distribute samples so as to capture most of the variations in decision space, we should be able to obtain a better approximation of the mappingfp, defined in (3) and therefore produce a better estimate of the pf using pe. specifically, we break down our method into three parts."
"traumatic brain injury (tbi) is a critical problem in healthcare that impacts approximately 1.7 million people in the united states every year [cit] . the varying cause and degree of injury (falls, car accidents, etc.) presents significant challenges in the interpretation of image data but also in quantitative assessment of brain pathology via image analysis. determining effective therapy and intervention strategies requires the ability to track the image changes over time, which motivates the development of segmentation and registration methods for longitudinal 4d magnetic resonance (mr) images. such methods need to account for changes in brain structures due to deformation, as well as the formation and deletion of new structures (e.g., edema, bleeding) due to physiological processes associated with damage, therapeutical intervention, and recovery."
"id3 is a simple decision tree learning algorithm developed by quinlan [cit] . it simply uses top-down, greedy search over the set of input attributes to be tested at every tree node. the attribute that has the best split, according to the splitting criteria function discussed later, is used to create the current node. this process is repeated at every node until one of the following conditions is met:"
"where a is the tissue class probability that is initially associated with the healthy template, φ t is the diffeomorphic deformation from time t to the atlas, and q t is the non-diffeomorphic probabilistic change for time t. this approach follows the metamorphosis framework of trouvé and younes [cit] . our method estimates a common subject-specific atlas a for all time points. given the model and 4d multimodal images i t at timepoints t, we estimate model parameters that minimize the following functional:"
"fig .2 shows the pseudo code for id3 algorithm to construct a decision tree over a training set () s, input feature set () f, target feature () c and some split criterion () sc ."
"another measure can be used as a splitting criterion which is gain ratio. it is simply the ratio between information gain value gain(s, a) and another value which is split information sinfo(s, a) that is defined as in (3)."
"nevertheless, despite the apparent superior performance of the method presented in this work in comparison with the original version of pareto estimation [cit], there is a number of open questions that need to be further investigated. as mentioned in the introduction real-world problems are usually multi-objective, and, often more than 3 objectives (manyobjective problems) are involved. however, the scalability of pe to many-objectives has not been explored. this can have significant practical implications. another issue is that the statistical results suggest that the solutions from pe and pe with the presented improved sampling strategy seem to produce superior solutions to the original approximation of the pareto front generated from the algorithm. although this result is a positive by-product, it should be further investigated as better understanding of the root cause for this behavior can suggest currently unforeseen improvements to pe. lastly, we envisage that the obtained results could be further improved by exploring more better techniques for identifying high variation regions in decision space. these improvements can also lead to a method for identifying robust solutions, if the used robustness measures are based on the sensitivity of the inverse mapping."
"there are many algorithms proposed for learning decision tree from a given data set, but we will use id3 algorithm due to its simplicity for implementation. in this section we will discuss id3 algorithm for decision tree construction and some of the frequently used functions used for splitting the input space."
"a-posteriori preference articulation can be summarized as follows: problem formulation the analyst, using specifications described by the dm formulates an optimization model, which could be a mathematical programming formulation or a data-driven model, etc. solution process subsequently, based on the properties of the problem, the analyst identifies a suitable algorithm (e.g. gradient-based or evolutionary algorithms) to solve the optimization model. decision making based on the information provided from the algorithm, the dm can, i) make their decision if the solutions provided by the algorithm are desirable, ii) reformulate the model and repeat the process, or, iii) choose another algorithm and repeat the process. one caveat with a-posteriori preference articulation methods is that computational resources impose a practical limit on the size of the representative optimal set that can be produced by the algorithm. this reveals an underlying metatrade-off, namely, the size of the representative optimal set and the satisfiability of the dm, to wit, the larger the size of the representative set the more likely it is that a solution that will match the dms preferences will be identified, albeit simultaneously the more expensive the optimization process becomes."
"the proposed method brings the advantage of providing a mapping from a normative template to a tbi subject. in fig. 6, we show a parcellation label image, provided by the international consortium for brain mapping (icbm), that has been mapped to a tbi subject. the mapping of a normal anatomy to pathological anatomy will be potentially important to compare type, locality and spatial extent of brain damages in the context of anatomically relevant regions with associated brain function information."
"we model the anatomical changes over time as a combination of diffeomorphic image deformation and non-diffeomorphic changes of probabilities for lesion categories, accounting for temporally smooth deformations and abrupt changes, e.g., due to lesions appearing and disappearing over time. specifically, the spatial prior p c t for each class c at time point t is modeled as"
"the assumption in (4) is that g(w, f(x)) is a convex function with respect to the weighting vectors. this is the case for all p-norm based scalarizing functions which are most commonly employed in decomposition-based optimization algorithms (see for example [cit] ), a family that also includes the widely used chebyshev decomposition as a limiting case [cit] ."
"decomposition-based multi-objective optimization methods have steadily increased in popularity in the last decade, see for example [cit] . these methods transform (1) into a set of single-objective subproblems with the help of a scalarizing function and a set of weighting vectors. these subproblems are subsequently solved simultaneously to produce an approximation of the pareto optimal set."
"to select the best attribute for splitting of certain node, we can use information gain measure, gain (s, a) of an attribute a, bya set of examples s. information gain is defined as in (2) ."
"for a given projection π −1 and a meta-modeling method (e.g. rbfnn) for identifying the mappingfp, the quality of the identified relationshipfp from pe is mainly affected by the training data, i.e. the approximation of pareto optimal solutions generated from the algorithm that is employed to solve a given mop. therefore the question we seek to answer is the following: how to manipulate the placement of the population during the optimization, so as to produce better data samples for pe, and therefore improve the quality of the resulting pe model and its performance, whilst ensuring that the convergence rate of the algorithm is not reduced. in the following section, we reflect on these questions and explore methods to address this problem."
"as is the case with all methods that are based on datadriven models, the performance of pe is contingent upon the quality of the pareto set approximation produced by the optimization algorithm, for instance see [cit] . in this work, we present an initial numerical study on the effect that the quality of samples has on the resulting pe model and its performance. in principle, for any interpolation method, shannon's theorem (see [cit] for a review) places a lower bound on the number of samples required for the complete determination of the function. however, the placement of these samples in the domain of the function is also very important, this placement is the object of study of optimal sampling [cit] ."
"method i method ii method iii fig. 4 . segmentation results for subject 1 at acute (top) and chronic (bottom) stages using different methods. our proposed method (iii) has the best segmentation quality overall. red: white matter, green: gray matter, blue: cerebrospinal fluid, and yellow: lesion."
"as the performance of pe is contingent upon the quality of the pareto set approximation produced by the optimization algorithm, in this work, we present an initial numerical study on the effect that the quality of samples has on the resulting pe model,fp and its performance. given that in real world problems the objective function is usually computationally more expensive than the optimization algorithm, improvements in the pe model can result in better utilization of the available samples, therefore resulting in a more efficient use of computational resources."
"a decision tree is a classifier expressed as a recursive partition of the input space based on the values of the attributes. as stated earlier, each internal node splits the instance space into two or more sub-spaces according to certain function of the input attribute values. each leaf is assigned to one class that represents the most appropriate or frequent target value."
"the decision tree model is generated over training dataset records using orange data mining tool [cit] . the generated decision tree is a binary tree with \"one value against others\" option. the confusion matrix valuesare shown in table 3 . the values of confusion matrix are generated by applying a decision tree on testing datasets. the evaluation measures shown in table 4 shows that the proposed classifier achieved a high recall at the cost of moderate precision. this means that a filtering tool improved the efficiency and effectiveness of the admission process. the classifier is to filter out the low level candidates so the admission staffs can focus their energy on the most promising candidates to make a better selection. so, the workload on the administrative staff is much reduced and hence they may be able to make a better selection job. in fact missing some (i.e., with a recall slightly lower than 1) is not necessarily bad, as the administrative staffs may not always be able to identify the best candidates from a large pool. on the other hand, the same measures in case of \"rejected\" class are about 0.58. this midlevel value stated that the classifier performance is above average."
"consider the process of identifyingfp as the process of interpolating a function, whose domain is d, using the candidate pareto optimal solutions as samples. it was argued in the introduction that for a fixed number of samples their position can be an important in the quality of the resulting interpolations of the same function. to better understand the importance of sampling, let us consider the following example in fig. (2) . in the example shown in fig. (2), we assume that a piecewise linear interpolation function is used to obtain an approximation of the function f (black solid line). we compare the interpolations from two sample sets, sample set i and sample set ii. both sets contain 11 samples but the positions of the samples are different. it is clear that sample set ii better approximates the function f (x) when compared with sample set i. samples from sample set i are evenly spaced, but due to the location of the samples, in particular the first four points from the left-hand side, the variation of the function is not sufficiently captured. sample set ii is obtained by a left shift of the position of all samples within (a, b) in sample set i and a few other sample relocations. by doing this, sample set ii produces a model that is more representative of the underlying function f (x)."
"the above example illustrates that the topology of a function can be better interpolated if the high variation (and high amplitude) regions of the function are well sampled. the variation of a function can be measured by its frequency and amplitude. by amplitude of a function, we mean the distance from the 'top of a crest' or the 'bottom of a trough' to a baseline (or base hyperplane in a space of more than two dimensions); in the above example, we can consider a line that horizontally passes thorough the 'center' of the function as the base line which can be estimated by the mean of the samples. we consider the regions with high frequency and amplitude as the high variation regions. therefore, if we can identify the high variation regions of a function and allocate more samples there, we can produce better interpolation of a function."
"supervised learning methods attempt to discover the relationship between input attributes and target attribute. once the model is constructed, it can be used for predicting the value of the target attribute for a new input data. there are two main supervised models: classification models, which is our interest in this paper, and regression models. classification models build a classifier that maps the input space (features) into one of the predefined classes. for example, classifiers can be used to classify objects in an outdoor scene image as person, vehicle, tree, or building. while, regression models map the input space into real-vales domain. for example, a regression model can be built to predict house price based on its characteristics like size, no. of rooms, garden size and so on."
"one of the main advantages of the decision tree is that it can be interpreted as a set of rules. these rules are generated by traversing the tree starting from the root node till we reach some decision at a leaf. these rules also give a clear analytical view of the system under investigation. in our case, they will help kau admission system office to understand the overall process. the induced set of rules is stated in table 5. as shown in table 5, beside each rule there is the percentage of instances that have the predicted class by this rule. also, we can figure out that there are only two rules that lead to \"accepted\" state. the first occurs if the student area code is \"1007\" (which is \"jeddah\" city) and student's high school grade is \"a\" (which is excellent student). the second case when \"male\" student from area with code \"1001\" (which is \"rabigh\" city) with grade \"a\" in high school."
"in the identification of the inverse mapping points in the projected space,p, are used in lieu of p, and, the motivation for this is to simplify the use of the resulting inverse mapping [cit] . elements inp are easily obtained and manipulated and therefore new samples are obtained in this space rather than the actual pf, whose topology is mostly unknown. one potential projection is to first normalize the objective vectors in p by"
"we evaluate the performance of our new approach on 4d tbi image data containing two time points: acute and chronic (≈ 3 days and ≈ 6 months postsurgery). the performance of our proposed method is shown in tab. 1, where we compare our method against those that do not use 4d modeling, with and without domain adaptation. dice overlap values comparing automatic lesion segmentations against a human expert rater are relatively low, which is a well known fact when dealing with small objects with complex and fuzzily defined boundaries. however, our method not only provides improved lesion segmentation but also better overall segmentation, as shown qualitatively in fig. 4 . the estimated 4d spatial priors for tbi subject 3 are illustrated in fig. 5, incorporating template deformation to match image boundaries and non-diffeomorphic changes due to lesions. subject 3 provides an interesting and revealing example of longitudinal pathology progression. the acute scan reveals gross pathology in the left frontal region, which results in considerable atrophy in this region at the chronic stage. however, the subject's chronic scan features an additional large lesion in the mid-frontal region due to the occurrence of a large abscess between acute and chronic scans. this is an excellent example of the dynamic and complex longitudinal changes that can occur in tbi patients."
classification accuracy (acc) is the most used measure that evaluates the effectiveness of a classifier by its percentage of correctly predicted instances as in (5) .
"in this paper, we are provided by sample datasets from kau system database that represent applicant student information and his/her status of being rejected or accepted to be enrolled in the university in three consecutive years (2010, 2011 [cit] ) . the dataset contains about 80262 records, while each record represents an instance with 4 attributes and the class attribute with two values: rejected and accepted. the classes are distributed as 53% of the total records for \"rejected\" and 47% for \"accepted\" class. table 2 shows detailed information about datasets attributes. www.ijacsa.thesai.org"
"for all the defined measures above, their values range from 0 to 1. for a good classifier, the value of each measure should reach 1."
"we perform model parameter estimation by minimizing the overall objective function (eq. 2) with respect to each parameter. fig. 3 provides a conceptual view of the parameter estimation process, which incorporates gradient descent updates that are effectively image registration and segmentation operations. in particular, we use these gradient equations to optimize the data functional f:"
"step 1 initialization 1. generate n evenly distributed points on the (k − 1)-simplex, where n is the size of population. we refer to this set as reference pf. 2. utilize generalized decomposition, (4) )."
"in this work we explored a potential direction for improving the utilization of the information obtained from a moea to better estimate the pf using pe. the preliminary numerical results in sec. 5, i.e. the statistics of three different measures and also the density plots, suggest that the proposed method can be useful for the production of better samples for pe which can be used to create a higher quality model of the inverse mapping, i.e. the mapping from the objective space to the decision space."
"in fig. (3) the final distribution of solutions on the pareto front is illustrated for maea-gd and maea-gd/rd. at this point, it could be argued that the distribution of solutions from maea-gd/rd is obviously inferior. however, we argue that this is not in fact the case as better model for pareto estimation has been identified using this distribution (for this problem) of solutions, which in turn can be used to create any desirable distribution in certain regions or across the pf. furthermore, the distribution of the approximated pareto optimal set in maea-gd/rd gives us information about the location of regions of high variation. this information can be leveraged by the analyst to produce more robust solutions, for example by requesting points from pe that are below a certain sensitivity threshold."
the dataset is divided into two main parts: training dataset that holds about 51206 records (about 64%) and testing dataset that contains about 29056 records (about 36%). the decision tree classifier is learnt using a training dataset and its performance is measured on not-seen-before testing datasets.
"data mining, the science and technology of exploring data in order to discover unknown patterns, is an essential part of the overall process of knowledge discovery in databases (kdd). in today's computer-driven world, these databases contain massive quantities of information. the accessibility and abundance of this information make data mining a matter of considerable importance and necessity [cit] ."
"in data mining, a decision tree (it may be also called classification tree) is a predictive model that can be used to represent the classification model. classification trees are useful as an exploratory technique and are commonly used in many fields such as finance, marketing, medicine and engineering [cit] . the use of decision trees is very popular in data mining due to its simplicity and transparency. decision trees are usually represented graphically as a hierarchical structure that makes them easier to be interpreted than other techniques. this structure mainly contains a starting node (called root) and group of branches (conditions) that lead to other nodes until we reach leaf node that contain final decision of this route. the decision tree is a self-explanatory model because its representation is very simple. each internal node test an attribute while each branch corresponds to attribute value (or range of values). finally each lead assigns a classification. fig. 1 shows an example for a simple decision tree for \"play tennis\" classification. it simply decides whether to play tennis or not (i.e. classes are yes or no) based on three weather attributes which are outlook, wind and humidity [cit] . as shown in fig. 1, if we have a new pattern with attributes outlook is \"rain\" and wind is \"strong\", we shall decide not to play tennis because the route starting from the root node will end up with a decision leaf with \"no\" class. in this paper, we introduce a supervised learning technique of building a decision tree model for king abdulaziz university (kau) admission system to provide a filtering tool to improve the efficiency and effectiveness of the admissionprocess.kau admission system contains a database of records that represent applicant student information and his/her status of being rejected or accepted to be enrolled in the university. analysis of these records is required to define the relationship between applicant's data and the final enrollment status. this paper is organized into five sections. in section 2, the decision tree model is presented. section 3 provides brief details about commonly used methods for classification model evaluation. in section 4, experimental results are presented and analyzed with respect to model results and admission system perspective. finally, the conclusions of this work are presented in section 5."
"the plots in fig. (4) show the normalized local density of the estimated pf from pe for problems wfg7 and dtlz2. the density is estimated based on the average distance from each pareto optimal solution to its five nearest neighbors on the pf and is regularized between [cit], where 0 is the smallest relative distance and hence highest density (densest, marked with warm color), and 1 represents the sparest, marked with color of cooler tones. higher density translates to a larger number of pareto optimal solutions in a neighborhood. it can be observed that the density of pareto optimal solutions on the estimated pf obtained from the maeagd/rd is slightly more uniform across the estimated pf, as there are fewer color variations when compared with the results obtained using maea-gd. see for example the bright yellow areas on the edges for both test problems for maeagd/rd fade and dissolve into their neighborhood compared with these ares for maea-gd. this indicates that the resulting estimated solutions on the pf obtained by maeagd/rd are more evenly distributed."
"we propose a novel framework that models changes in 4d pathological anatomy across time and provides explicit mapping from a healthy template to tbi subject images. this aids analysis of tbi patients by enabling the mapping of parcellation labels describing anatomical regions of interest and quantitative comparison against a common reference space defined by the normative template. moreover, our framework uses transfer learning [cit] to leverage rich information from a \"known source\" domain, where we have a large collection of fully segmented images, to yield effective models for the \"input target\" domain (tbi images). this is essential as such a database does not exist for tbi imaging, and thus we explore and demonstrate the use of an existing database of multi-modal tumor imaging that serves as a well-studied source domain. the information in the learned tumor model are transferred to the domain of tbi images using importance weighting based domain adaptation [cit], a well known transfer learning technique, resulting in a fully automatic method that does not require user input. in this paper, we propose importance weighting based domain adaptation for generative kernel density models, thus extending its applications beyond standard discriminative models available in machine learning literature [cit] ."
"in general, pe attempts to identify a relationship from the objective space to the decision space (inverse mapping) and then utilize this relationship to produce more pareto solutions on the entire pf or in a specific region of the pf. to identify this relationship, the set of pareto optimal objective vectors, p, is first transformed into a projected set,p. one element in p is mapped exactly to one element inp, namely this transformation can be described as a function"
"the model stated that the most accepted students from \"jeddah\" region in ksa with excellent high school grade (more than 85%) or male students from \"rabigh\"."
"instances are classified by traversing the tree from the root node down to a leaf according to the outcome of the test nodes along this path. each path can be transformed then into a rule by joining the tests along this path. for example, one of the paths in fig. 1 can be transformed into the rule: \"if outlook is sunny and humidity is normal then we can play tennis\". the resulting rules are used to explain or understand the system well."
"1. manipulate the current population to identify a model that maps from the objective space to the decision space. this process is identical to the pe method, and can be considered as building a local approximate of the mappingfp using currently available information. this model is regarded as a local approximate because the current population may not be close enough to the actual pf and therefore can only provide partial information of the actual mapping. quality of the local approximate improves, as the algorithm approaches the pf. 2. utilize the local approximate offp to identify the areas in the decision space that are more 'important' for obtaining better topology of the pareto optimal solution set in the decision space. sincefp mapsp to d, the domain of the function is the projected spacep. this implies that we have to identify the areas offp that present more geometrical changes in the decision space and then find the corresponding regions in the projected objective spacep. this can be achieved by as follows: first we evenly sample the projected space, then use these samples and the local estimate offp to generate new decision vectors, and finally identify the regions according to certain measure of the geometrical changes (frequency and amplitude) offp . in this work, we simply use the density of decision vectors as the measure. alternative methods will be explored in future research. 3. reallocate the position of the samples in the projected objective space, with more samples presenting in the areas identified above and generate the corresponding weighting vectors to the new samples using generalized decomposition (see section 4.1). to have consistently more samples in the identified areas in successive generations, the change of samples' locations inp need to be transformed to affect the search of the algorithm. for decomposition-based algorithm, for instance, we could transform these changes to alter the direction of corresponding weighting vectors; see next section for an algorithm that incorporate the above idea into a decomposition based algorithm."
"recall (r) and precision (p) are measures that are based on confusion matrix data. recall (r) is the portion of instances that have true positive class and are predicted as positive. on the other hand, precision (p) is the probability of that a positive prediction is correct as shown in (6 precision and recall can be combined together to formulate another measure called \"f-measure\" as shown in (7) . a constant  is used to control the trade-off between the recall and the precision values. the most commonly used value for  is 1 that represents f1 measure."
"pe is a novel method of improving the density of a given approximate pareto optimal solutions on the entire pf or in a specific region in which the dm is interested. in this paper, we present a method for improving the quality of the estimated pareto optimal solutions from pe by allocating more samples in areas of high variation thus acquiring better topological information of the relationship between the projected objective vectors,p, and decision vectors."
"data mining includes many methods and techniques, but mainly we can divide them into two main types; verification and discovery. in verification-oriented methods, the system verify the user's input hypothesis like goodness of fit, hypothesis testing and anova test. on the other hand, discovery-oriented methods automatically find new rules and identify patterns in the data. discovery-oriented methods include clustering, classification and regression techniques."
"king abdulaziz university (kau) admission system in the kingdom of saudi arabia (ksa) is a complex decision process that goes beyond simply matching test scores and admission requirements because of many reasons. first, the university has many branches in ksa for both division male and female students. second, the number of applicants in each year is a huge which needs a complex selection criterion that depends on high school grades and applicant region/city."
"at the same time, dmsp satellite cannot directly store high-resolution information of fine pixels, so fine pixels are smoothed when storage, and fine pixels of 5*5 are regarded as a smooth pixel. however, the initial calculation position of dmsp satellite for fine pixel smoothing is not uniform, resulting in no complete overlap of smooth pixel images every night, which is also one of the reasons for the blurring of annual average image."
"program operational linescan system) image is a kind of noctilucent remote sensing data widely used, and has important value in social and economic data mining."
"oide is an application based on the eclipse framework, developed as a set of eclipse plug-ins, which supports the ooh4ria methodology for the development of rias. specifically, this application defines the ooh4ria meta-models using the emof/ecore meta-metamodel and, using the emf/gmf framework, facilitates the definition using a graphical concrete syntax of the ooh4ria models: domain, navigational and presentation-orchestration. moreover, this tool supports the generation processes that obtain most of the ria software components (both server and client modules). the generation rules are implemented as a set of xpand rules, which, at present, transform the models into c# code using the silverlight, wcf (windows communication foundation) and nhibernate frameworks. using oide as platform, the s m 4ria extension for oide implements the artifacts and processes of the s m 4ria methodology as a new functionality of eclipse. this section describes the elements developed and the modifications to the original tool that facilitate the design of the sria software components. more specifically, this extension includes the following features and components: a) new models. using the emf and gmf libraries, three new models have been implemented, whose meta-models have been defined over the ecore meta-model:"
we describe three examples that illustrate the method. we choose examples where the true density is known so that a mean squared error is computable as
"where g is a real-valued function that takes as input ω. the stochasticity of the system derives from the uncertainty about ω, which may be modeled by a random vector or stochastic process in general. in this paragraph, however, we assume that ω is a scalar-valued random variable. (while most stochastic simulations involve more than one input parameter, we here envision, for example, a study of a single input parameter of high importance, with other parameters fixed.) we assume that for a given ω, the simulation returns g(ω) and the derivative g (ω). [cit] for methods to generate derivatives such as infinitesimal perturbation analysis. if the function g is differentiable and bijective, then derivative information can be used to calculate the output density exactly at certain points. specifically, if the derivative of g and the density of ω (denoted as f ω (ω)) are known, the density of the output can be calculated exactly at certain points using the following change of variables formula:"
"in this paper, rtsvd algorithm combined with pct image filtering was adopted to solve the fuzzy problem of dmsp/ols data. based on the average light data test in taiwan in f142000, the following conclusions were drawn:"
"firstly, according to the rule that the luminous frequency of the light source pixels must be higher than that of the non-luminous pixels, pct data was used to compare the luminous frequency of the pixels, close all the non-luminous pixels in the image, and set the threshold of 15% luminous frequency to eliminate the accidental influence. then, the gaussian distribution is used to fit the point diffusion function (psf), and the reflexive boundary conditions are used to construct the fuzzy matrix [cit] . finally, l curve was drawn to calculate the truncation parameters, and the the international archives of the photogrammetry, remote sensing and spatial information sciences, volume xlii-3/w10, 2020"
"the selection of truncation parameters is very important for rtsvd algorithm, and gcv or l curve is generally used to determine the truncation parameters. however, it is difficult for gcv to calculate the minimum truncation error when the image is blurred. therefore, l curve is used to determine the truncation parameters and cubic spline curve is used to fit the discrete points. l curve is a curve drawn for a series of truncation parameters in the rectangular coordinate system with canonical solution norm as the vertical axis and the corresponding residual norm as the horizontal axis on the scale of log-log. the truncation parameter corresponding to the point of maximum curvature on l curve is the optimal solution. let, are the logarithm of the residual and solution respectively, and the curvature formula of l curve is expressed as follows:"
"finally, adding derivative information contributes to an additional reduction in mse, as seen in table 2 . even though x and its gradient information are estimated (so technically the soft information constraints are inexact), we still see a great reduction in mse. the standard deviation also decreases with increasing soft information. the soft information constraints of unimodality, lower and upper bounds, and decreasing, and appears to be qualitatively closer than the kernel estimate. the right plot shows the inclusion of gradient information to constrain the density at the sampled values of x. we see that the epi-spline matches the true density closely in the regions where there are sample points, but not as closely between 1.75 and 2, where there are no sample points."
"the size of fine pixels at the center of mass of the field of view is 0.56km*0.56km, much smaller than the field of view area. in the process of field of view movement, the same fixed light source will fall into several overlapping fields of view, forming a fuzzy ellipse with the field of view as the center of the light source."
"the remainder of the paper is organized as follows. section 2 [cit] . section 3 discusses different types of soft information. section 4 presents computational experiments, and section 5 concludes and discusses future applications."
"according to the imaging principle of light image, the luminous frequency of the light source pixel must be higher than that of the non-light source pixel. the pct image can be used to close the pixel whose luminous frequency is less than that of the surrounding pixels to eliminate the pixel error of the light source:"
"the s m 4ria methodology extends the original ooh4ria methodology modifying some of the existing tasks and including new ones. the development process is divided into three main activities, which include tasks with the same aim: 1) design the components of the sria server; 2) design the components of the sria client; and 3) generate the sria by means of a collection of model-to-text transformations. the first activity starts when the server designer defines the domain model, which specifies the data structures used in the application and the operations over these structures. from this model, the ontology designer builds the domain ontology aligning the concepts extracted from the data structures with concepts of other sources or applications. as a result, the designer obtains the extended domain model (edm), which is a requisite of the next task of the activity: define the extended navigation model (enm). in this task, the designer specifies which data and ontology instances of the sria will be employed in the application. the emn specifies the manner in which users navigate these elements by means of a set of navigational classes, which refer to the concepts defined in the edm. furthermore, using the enm the designers can specify the access to external knowledge bases, using sparql queries, and the manner in which this information will be gathered and managed."
"finally, the last activity of the method is aimed at generating the software components of the sria using the information captured in the s m 4ria models by means of a collection of model-to-text transformation processes."
"simulation models are often analyzed to estimate the mean performance of a proposed system, with confidence intervals quantifying the uncertainty in the estimate. quantile estimates of simulation output at various probability levels provide further information about the system performance [cit] ). estimates of the density of the output yield an even more comprehensive picture that allows the computation of these and many more quantities. while kernel methods and other traditional nonparametric density estimation methods can in principle be used to construct density estimates, they do not easily account for known structural properties and tend to require a large sample to reduce the effect of outliers. in contrast, parametric density estimation tends to impose excessive restrictions on the shape of the densities and important characteristics of the output thereby may remain undetected. [cit] introduce a nonparametric method for estimating density functions using exponential epi-splines that easily incorporates soft information about a stochastic system, its inputs, and its outputs. soft information reduces the space of candidate density estimates substantially in many applications and may come in the form of knowledge of the nonnegativity of the support as well as monotonicity, unimodality, continuity, and smoothness of the density. for example, waiting times in queueing systems are nonnegative, and possibly bounded from above if entities renege after a certain amount of time. under approximate normality, the density can reasonably be assumed to be unimodal. exact and approximate density function values at particular points, cumulative distribution function values, and derivative information may also be available as we exemplify further below. [cit] to estimate densities of simulation output and illustrate the use of soft information in this context. we construct density estimates to assess the variation in output from a stochastic simulation with fixed input parameters. for example, the input parameters may define the distribution of input random variates. however, these input parameters may be uncertain and varying them is also important for sensitivity analysis. while this is a situation which is typically computationally demanding, we demonstrate that when including soft information, such as knowledge of derivatives, highquality estimates of the output density are achieved with as little as 10 runs of the simulation model. [cit] applies in any finite dimension, but we here focus on a single output quantity of interest."
"consequently, for a given mesh, an epi-spline of order p is uniquely defined by its epi-spline parameter r as the 'basis' function c is simply determined by the mesh and p."
"the above nonparametric density estimation method applies broadly, but the simulation context poses special challenges. for example, suppose that we have a stable single-server queueing simulation where the average customer time in system (tis) is a function of the mean service time parameter ω with the arrival parameters fixed. the output x is the steady-state average customer tis. we seek the density of x under uncertainty in ω given by some distribution. in principle, one way to estimate this density is to sample values of ω, simulate the corresponding values of x, and compute an exponential epi-spline estimate of the true density. however, since each value of ω requires, most likely, a long simulation to obtain x, the computational cost may become high and effective estimates from small sample sizes become especially pertinent. in addition, the resulting output will be corrupted by simulation error. as an illustration, if the support of the density of ω is bounded, then the corresponding values of x should also be bounded, but sampling error may lead to values of x outside those bounds. [cit] that the probability measure generated by a sample tends to the true measure in a probability metric almost surely. however, we do not address this in further detail, but include a numerical example in section 4."
", the international archives of the photogrammetry, remote sensing and spatial information sciences, volume xlii-3/w10, 2020 international conference on geomatics in the big data era (icgbd), 15-17 [cit], guilin, guangxi, china"
"the development of rich internet applications (rias) has lead to the improvement of the user interfaces in web applications increasing the interoperability of their components by means of an event-driven paradigm, and providing an appearance and user experience similar to a desktop interface. nevertheless, due to technological issues, rias act as black boxes that show their contents in a user-friendly manner but complicate the access to the data to some types of web clients, which require accessibility, such as, the search engines. this drawback is shared both by browser-oriented rias, whose data is visualized according to a list of events triggered by users, and plugin-oriented rias, which, in addition to being event-driven, are implemented as binary objects whose information can be only visualized using a plug-in specific for each technology and browser. in this context, the s m 4ria approach (semantic models for ria) [ 4ria approach, the tool also includes mechanisms for the generation of ria interfaces from ontologies and the generation of administration views for the designed applications."
"the international archives of the photogrammetry, remote sensing and spatial information sciences, volume xlii-3/w10, 2020 international conference on geomatics in the big data era (icgbd), 15-17 [cit], guilin, guangxi, china the condition that the overall shape of the curve remains unchanged. therefore, cubic spline curve was used to approximate the discrete point of l curve, and the maximum curvature point on the continuous curve was calculated to determine the truncation parameter."
"all results are computed using common random numbers for 100 replications, with different levels of soft information applied. we compare exponential epi-spline estimates of order two to those calculated using kernel estimation. [cit] with a gaussian kernel, and do not modify the default (optimized) calculation of the bandwidth. mlp n is solved using fmincon of matlab, typically in under one minute of run time on a dual-core 2.2 ghz processor."
"however, pct image filtering cannot completely turn off the blurred pixels in the light edge and cannot remove the noise in the image. therefore, on the basis of pct image filtering, a regularized truncated singular value decomposition (rtsvd) method was proposed to further process the average light image. according to the blur reason, the boundary conditions of fuzzy matrix are set as reflexive boundary conditions . then, the singular value decomposition is used to further improve the de-fuzzy model. for order n m  fuzzy matrix, there is a decomposition such that:"
"the second activity of the s m 4ria process continues by transforming the extended navigational model into a skeleton of the presentation and orchestration models using two model-to-model transformations: nav2pres and navpres2orch. the presentation model describes the structure of the user interface (components and visualization) which is complemented by the orchestration model, which defines the behavior of the interface."
"epi-splines with soft information can be used to construct density functions from limited data samples. knowledge of soft information constrains the feasible space of possible epi-splines to provide better estimates. we show how simulation analysis can uniquely benefit from the availability of soft information, as the shape or bounds of the density functions for simulation output are often known. simulation runs can also be expensive, and soft information can help reduce the number of data points needed to construct an estimate. when derivative information is available, the reduction in mse can be even greater."
"in truncated singular value decomposition, l curve is composed of several discrete points. to calculate the optimal solution, a differentiable and smooth curve associated with discrete points must be defined under"
"moreover, due to satellite miscalculation, the center of sources in different parts of the world. all the 28 points measured were migrated northward with an average deviation of 2.9km, which was about the width of a smooth pixel [cit] further speculated that the deviation standard deviation was about 1km to 1.12km."
"in order to verify the effectiveness of regularized processed by direct frequency filtering, the peak signalto-noise ratio was increased by 3.4435 and the information entropy was reduced by 0.4828, indicating that although the method effectively reduced noise and blur, the image details were seriously lost. it is shown that the rtsvd algorithm combined with pct image filtering can not only eliminate the blur but also retain the detailed information of the image."
"other more explicit information about the density might be known. if we have a reference density that we know will be close to the true density, we can place bounds on the divergence between the estimated density and this reference density. this may be useful in sensitivity analysis, where we have a good estimate for the density for a particular set of input parameters and wish to estimate the density for a slightly different set of parameters. in order to calculate this new density quickly (with a much smaller sample size than that used to calculate the reference density) we can bound the divergence from the old density to be small."
"where, is error. in order to weaken the error interference, the regularized truncated singular value decomposition method is used to replace the original singular value matrix with the truncated singular ∑ value matrix, and the formula is as follows: ∑"
"exponential epi-splines are functions that are compositions of the exponential function with epi-splines. epi-splines are piecewise polynomial as related to classical splines, but instead of focusing on interpolation aim at approximation. they also relax the typical requirement of continuity and smoothness of splines. exponential epi-splines approximate to an arbitrary accuracy essentially any density function encountered in practice and also easily allow for the consideration of soft information. [cit] develop a nonparametric maximum-likelihood based density estimator using exponential epi-splines and show asymptotic and finite sample-size results, including almost-sure convergence to the true density as the sample size tends to infinity. by restricting the shape of possible densities to consider, fewer samples are often required to construct an estimate that meets the requirements of the analyst. simulation experiments are usually costly and time consuming to run, and therefore this estimator appears especially well suited to that context. the use of exponential epi-spline estimators of output densities differs substantially from current analysis of simulation output; [cit] for density estimation methods using histograms."
"it should be noted that epi-splines can also be used to construct density functions from small data samples to generate input to a simulation model. the presence of soft information can be used to construct epi-spline estimates with more flexibility in shape than a parametric distribution. additionally, simulation from epi-splines can be performed using acceptance/rejection techniques, where the majorizing function is a piecewise constant function which takes the maximum value of the epi-spline over each segment. epi-spline density function values are easy to evaluate given that they are piecewise polynomial. we anticipate many future related applications for epi-splines."
"in this paper, the new pssldm is proposed and described. to develop the methodology, an interactive approach (ellström, 2007), based on the analysis of both available literature and companies' needs has been performed. the methodology was developed and validated by three manufacturing companies, which have been involved in the concept definition providing useful input and feedback to improve its development. the methodology developed has been used as a basis to develop a collaborative environment for product service design. for this purpose, an engineering environment and a set of software tools have been developed in order to translate theoretical methodology and methods. therefore, on this basis, formerly in section 2 the paper introduces the research context, unveiling which are the related gaps and explaining the basic principles used to fill them jmtm through such methodology. then, section 3 provides the results from business cases and requirements analysis, from concept definition to proof of concept. therefore, section 4 provides an overview of the methodology and of the related methods and tools developed to support the definition of a pss following a lean approach, while in section 5 an application case is reported. finally, section 6 discusses and concludes the paper, evidencing its strengths and weaknesses from managerial and practical perspective, and introduces the future research developments, a new comprehensive engineering environment able to conceptualise, design and monitor pss along its entire lifecycle."
"to adequately support companies in following the methods and therefore in identifying needs (level 1) and wishes (level 2) driving to the conceptualization of new pss(s), hints supporting each phase of the tree development by use of a brainstorming process are included in the psct tool developed ."
"in the previous phase, the conceptual level of the pss design has been carried out, in the following phases and related methods for guiding the pss design from the concept selection to the most detailed levels of the solution definition are provided."
"lean design methodology definition of lean content design rules that guide the product design phase starting from the service, manufacturing and environmental features the company wants to include in the product. in particular, since the pssldm wants to take into account the pss perspective along with all the design process, dfx techniques such as design for serviceability, design for maintainability and design for lifecycle are strongly integrated into the product design process [cit] in the limited context of remanufacturing). in this direction, the authors have been focussing on identifying and formalizing the initial lean content guidelines/rules, based on literature review and industrial practice, proposing a new more suitable approach to their research context, named design for product service supportability [cit] ."
"as previously explained, a final validation has been conducted through a face-to-face workshop in the industrial context in order to verify the value of the pssldm as a whole and to evaluate its consistency with the adopted and developed methods and tools. to do this, mould-making industry, one of the three companies that already contributed to the development of the methodology during the previous \"theory building and tool development\" phase, was chosen: the main advantage coming from this choice has been that its employees already knew the different single methods and approaches constituting the methodology. this interactive final test session was led by two academics expert in product development and se and involved two additional academics with which the company has a long-term relationship: researchers interacted with the production monitoring employee and one product designer."
"the company, mould-making industry, is a b2b greek sme designing and manufacturing moulds. in its product design process, the use of software tools and design methods is consolidated, supported by a reliable and experienced design and engineering division. indeed, in the company pss are not offered so far: services are provided in a disjoint way from the products (i.e. the mould) without considering integrated psss. however, mould-making industry is willing to go through the servitization process, since its actual intent is to increase competitiveness and income by improving not only customer involvement in the design phase of their offer but also his satisfaction during the following stages: in this way they would also manage to get access to new market sectors."
"going more into the detail, service blueprinting has five components (physical evidence, customer actions, onstage/visible contact employee actions, backstage/invisible contact employee actions, support processes) shown in figure 4 . the service delivery process is designed considering:"
"wrapping up all the principles and steps belonging to the above-mentioned research traditions, the design of this research follows the schema traced in detail in figure 1 and can be described as following. the first stage is the observation phase: research object is interpreted, gaps and research questions are detected, and the pssldm is conceptualised. the second phase, namely theory building and tool development, was carried out with the aim to progressively improve the methodology through company experts' feedbacks and obtain the final consolidated methodology design. finally, the third phase is aimed at validating in an industrial environment the pssldm integrated with the tools needed for its adoption, with the aim of obtaining the system development full prototype."
"in particular, the design research methodology (drm) framework [cit] due to its interactive approach has been adopted as a primary reference. in addition, drm wants to provide understanding and improving design research with the final aim of enhancing the design process, in both theory and practice. likewise, the here proposed research methodology guided the authors to develop the pssldm in order to improve the pss design and raise designers and engineers' consciousness in designing pss in a systematic and integrated way."
"furthermore, from a practical point of view, this paper has provided an overview of the pssldm, explaining through the description of a validation case, how the different methods supporting its conduction should contribute to the methodology in order to properly design an integrated pss. indeed, the methodology has been validated through three application cases and related to manufacturing company's needs. therefore, it is very likely that the methodology with related methods and tools, developed satisfying needs in these business cases, will be applicable to wide scope of industrial companies in various sectors. furthermore, as shown in table iii, companies highlighted several managerial implications along the pss lifecycle deriving from the systematic adoption of the pssldm."
"too often, in fact, product and/or service design takes place through inefficient and under-effective processes that generate a considerable amount of wastes and reworks at any levels and stages of the pss design process."
"finally, the entire pss development process has been guided by a set of development process rules aimed at avoiding wastes and increasing the value brought along it. an example, generated during this validation case, is here reported: \"before starting any pss development, identify a technical person in charge of the overall pss development ( from concept to launch), who will be your chief engineer\"."
"using as inputs these needs and wishes, the team, through the creation of the psct (figure 6 ), brainstormed on several possible products or services implementations that could help in satisfying them. this led to the generation of five possible solutions: analysis of each repair instance; digital history of repairs for the mould; supporting tool to handle maintenance communication/interactions; accurate delivery time estimation; predictive maintenance."
"in order to fill these gaps, some improvements are considered by authors in each of the phases encompassed by seem: from customer needs analysis to process prototyping, from process validation to offering identification and analysis. among them, a particular attention is given to: the definition of guidelines and rules, enabling a better km and working as a trait d'union between product and service design as well between concept and detailed design phases, and the introduction of the lean development concepts by defining lean development process rules, aiming at identifying and reducing wastes along the entire pss development process."
"4.3.1 service delivery process design. task 1: process prototype. this task involves representing the service delivery process(s) for the selected solution. in particular, to facilitate the description of the relationship between the customer/consumer and the organisation, the service blueprinting technique has been adopted for simultaneously depicting the customer's journey and the company's processes [cit] . since the service blueprinting itself does not provide a taxonomy, the bpmn one has been adopted within the pssldm."
(1) the possible impact that the implementation of a solution can have on the company's value; and (2) the difficulty that the company could encounter during the implementation.
"the pssldm should be adopted according to a cyclical process headed to continuous improvement of the pss: moreover, when available, also sentiment analysis can be strategic in conducting both the offering analysis and customer analysis phases. this paper detected the leading gaps dealing with pss design, evidencing the lack of methodologies able to support the integration of the product and service components since the early stage of the design phase. this is also justified by: a limited development of supporting tools able to bridge customer needs into technical constraints, a scarce adoption in the manufacturing context, the necessity of integrating actors from both product and service fields of knowledge to consider all the pss features, a lack of an entire lifecycle perspective along the pss development, a lack of dedicated methods able to enhance the coordination between the back-end and the front-end capabilities, the lack of a systematic design of product and service features as an integrated system. to overcome these limitations, the pssldm, a methodological framework and related supporting methods and tools for the systematic development and design of psss, has been introduced in this paper. it aims at creating a structured methodology for the design process and at fostering the collaboration between the different actors of the process. grounded on already existing methodologies, the pssldm introduces new innovative aspects, from customer needs analysis to process prototyping, from process validation to offering identification and analysis, contributing to the pss development context from both a theoretical and a practical perspective. in particular, in order to fill the gaps of the available methodologies, in the definition of the pssldm a particular attention has been given to: defining a method enhancing a better km and supporting a matter integration between product and service design as well between concept and detailed design phases, and the introduction of the lean development concepts aiming at identifying and reducing wastes along the entire pss development process. indeed, it promotes lpd theories to manage, use and re-use knowledge: design rules are proposed as best practice to solve the issue of integrating both service and product features in the pss design process, enabling the respect of both companies internal constraints and customer needs since the early design phase. this leads to a strong integration of the design process of the product and service features through the lean content design rules. moreover, through the lean development process rules, it fosters a pss design process focussed on waste reduction. it will hence guarantee, from the practical point of view, more effective pss design thanks to a better link with the front-end, the service and the product design, and more efficient pss design by anticipating reworks and revisions at the early phases of the design process with the ultimate objective of reducing time to market and costs."
"in such a context, if the pss engineering models methodologies, the related methods above mentioned and the most recent literature reviews on the pss research area [cit] are analysed in details, many gaps continue to reside. in the following, the main gaps identified by the analysis of the actual pss literature are reported:"
"reasonably, the first one to be implemented should be the one that requires lower effort (limited difficulty) and produces the higher impact. once the solution is selected, the final design can start."
"starting from the analysed customer's needs, to identify promising pss concept's solutions and to evaluate them, the product service concept tree (psct) method has been adopted. this method aims at suggesting a possible way to:"
"4.3.2 product design. task 1: product prototype. this phase refers to the design and/or re-design of new product features or a new product enabling the pss defined in the psct. this is a phase, in fact, in which companies have been already well-defined tools and design methods, such as product lifecycle management (plm). a plm is a strategic business approach that supports all the phases of the product lifecycle, from concept to disposal. integrating people, processes and technologies and assuring information consistency, traceability, and long-term archiving the plm enables organisations to collaborate within and across the extended enterprise [cit] ."
"all the activities that do not add any value to the product/service and generate waste of knowledge, time and resources should be eliminated from the process, which should be able to flow efficiently, step by step. the more standard is the process and the more \"instructions\" designers, engineers and people involved in the development team have, the higher their ability to learn how to deal with process development and to avoid mistakes and wastes. basically, if we say that a lean content rule indicates \"what to do\", a development process rule indicates \"how to do it\"."
"the final task of the customer analysis has been the solution concept selection: the team evaluated the identified solutions under the aspects of impact and difficulty (table i), choosing the \"digital history of repairs of the mould\"."
"in order to achieve these goals, factors as environmental impact, wastes in material, energy consumption, design and machining time, time to market and frequency of failure are considered. thus, the company started to strategically consider new pss projects as mould delivery time estimation as a service, maintenance history per customer, joint provider-customer proactive production planning for mould modifications or opinion mining offered to customers as a service. based on these business expectations, the pssldm was applied starting with customer analysis. the first step has been the detection of the customer, i.e. thrace plastic, a company working in the plastic industry. from the feedback collected and the brainstorming carried out during the workshop, the team derived two main specific needs of this customer: monitoring and control mould lifecycle; shorter mould downtime. starting from them, two wishes were detected: increased information availability; collaborative maintenance operations planning."
"4.3.3 content design rules. pss lifecycle is characterised by several phases from the initial concept to the final disposal. however, as for conventional products, the profit generation and the market success of psss critically depend on the decisions taken during the initial lifecycle stages, when psss are conceptualised, designed, developed and engineered. to adequately support an integrated design of product and service, specific design for x (dfx) techniques are used within the pssldm. the aim is to drive the"
"all the levels must be connected to each other according to their relationship. this method and the related tool are going to support the manufacturing companies both in the concept generation and evaluation phases. 4.2.1 task 1: solution(s) concept generation. starting from the information collected about the customer, as a first step, the psct method supports the concept generation. the aim of this sub-phase is to identify new solutions that can answer to customers' latent or declared needs and wishes and associate the resources required to deliver the product service. the customers' needs [cit] and wishes identified in the first phase (either through social network, surveys to define personas or brainstorming) are used as input. the output of this phase is a high-valuable concept."
the aim of this phase is the analysis of the customers' information with the aim to identify customers' needs. the customer analysis starts due to:
"the two factors are evaluated on a likert scale from 1 to 5. concerning the solution's possible impact on the company business, a score of 1 refers to low impact while a score of 5 implies a significant change in the company that can be in lean design methodology terms of market increase (revenues should be improved as well), innovations in the network, and technology that can positively affect buyers' preferences. difficulty in the implementation refers to the effort that the company encounters during the implementation of a solution. a score of 1 means that no relevant changes are required to implement the solution while a score of 5 means that the company needs a profound change in the organisation or a high investment in order to implement this change (e.g. a new organisational structure, an entirely new product with unknown technology)."
"so far, one of the leading gaps dealing with pss design can be originated in the lack of methodologies able to support the integration of the product and service components since the early stage of the design phase [cit] . this may be caused by a limited development of supporting tools with a consequent scarce adoption in the manufacturing context [cit] and by the necessity of integrating actors from both product and service fields of knowledge to consider all the pss features [cit] ."
"in the following, a detailed description of all the steps composing the pssldm is reported, specifying the supporting methods and tools needed to perform each of them."
"to overcome these limitations, lean product development (lpd) [cit] ) . in particular, lpd promotes strong attention on the way companies manage, use and reuse knowledge [cit] . the use of design rules represents a best practice to support the integrated pss design process considering both service and product features, enabling the respect of both companies internal constraints and customer needs since the early design phase [cit] : however, so far, none of the methodologies proposed in literature follows this approach. indeed, a methodological framework and related supporting methods and tools for the systematic development and design of psss are needed. based on this analysis and on the potential detected in lpd to cover pss development gaps, among the several methodologies existing in literature [cit], the service engineering methodology (seem) [cit] ) was selected to be further improved to cover the gaps detected. seem only aids pss designers to comply with both consumers' needs and companies' necessities; however it mainly focusses on the service features avoiding the design of the product. in this sense, it is not able to adequately integrate product and service components in a systematic way and to properly bridge customers' needs feedback into the designed solution."
"these guidelines/rules aim to create a lean process guiding managers, workforce, suppliers and customers through km. qualitative lean design guidelines help designers to choose the right product design decisions, they do not provide concrete instructions on an adequate level of detail. therefore, an integration of dfx approaches with the concept of lean design is necessary [cit] . indeed, while lean design focusses on an integrated methodology for an optimised product design, dfx approaches deliver specific recommendations for a specific virtue or stage in the product lifecycle. in pssldm, the adoption of lean design and dfx approaches is fundamental for the definition of lean guidelines and rules. in order to start identifying and categorising lean guidelines/rules it is essential to define: what a lean design rule is and in which forms can be declined, how the style of formalization will be, and what technique will be used to address the level of importance of each rule."
"task 2: product validation. through this task, the design product is checked and aligned with the pss feature identified. a check of the lean design content rules linked to the product to enhance the abilities enabling the pss is performed. through this validation, it is possible to understand if the designed product has included all the main relevant aspect of supporting the serviceability, manufacturability and the eco-friendliness. if some features have not been included in the product, design changes could be asked in order to create a product, which is aligned with the customer and company requests."
"lean development process rules, developed by use of the mywaste methodology [cit], are used to guarantee a waste-free pss development process. they affect all the phases of the pss development process also leading the designers and engineers through their pss development activities."
"the result is the here proposed pssldm: this methodology includes new innovative aspects, comprising the previously mentioned aspects coming from lpd theories. it will hence guarantee more effective pss design thanks to a better link with the front-end, the service and the product design, and more efficient pss design by anticipating reworks and revisions at the early phases of the design process with the ultimate objective of reducing time to market and costs. in this sense, along with the entire pssldm, a critical role is played by lean rules coming from lean thinking philosophy. in general, a lean rule is defined as a set of explicit principles, governing the procedure within an enterprise, in order to eliminate waste, amplify profit, reputation and satisfaction and abridge cost, energy and lead time. lean rules assume a supportive role in the pss development process, crucial to ensure that customer needs and requirements are respected and embedded in the pss to be designed and delivered; and the company internal performance is optimised."
"three areas have been identified to properly design a pss: a product design, a service design, and an integrated-view. in this last view, lean content design rules are created and used to act as trait d'union between service and product design."
"the pss design guru methodology procedure has been adopted at this stage: the current design approach of the company was assessed, and the solution was enhanced through the generation of new content design guidelines and rules. besides the aim of guiding the pss provider in the creation of a product consistent with the customer's needs, the design guidelines and rules are useful to limit the reworks, since they are thought to give . \"2 cavity, 1 litre seal lid\" mould jmtm precise information on how to design the product. moreover, they constitute a bridge among the two dimensions of product and service, of engineering and sales divisions: they become the knowledge belonging to the company enabling collaboration between engineering and sales to integrate the product and the service dimensions. content design guidelines and rules are aimed at solving product design issues also with a service perspective, thus fostering and easing the delivery of the pss, the digital history of repairs, through the integration of the mould with the improved maintenance service along its entire lifecycle."
"for the lean content design rules creation and validation the pss design guru methodology, composed of one preliminary step plus 5 additional phases, has been created [cit] ."
"all these elements, and others described in the next sections, concur to compose the here presented pssldm and the set of methods and tools supposed to be called in its conduction. one of the main scopes of the pssldm is to be industrial oriented and able to answer to manufacturing companies' needs facing the servitization transformation. for this reason, the pssldm has been developed considering both literature gaps and industrial requirements."
"(1) the customer's journey; (2) the company's ongoing processes; (3) the product features of a new pss, identified in the psct, and modelled as physical evidence; and (4) the pss content design rules that help the exchange of knowledge between the product and the service designers and guarantee the balance between the customer and the company value."
"in order to allow better integration of the service delivery process and the product either developed by the same provider or by different providers, a predefined service granularity level must be agreed among all the actors involved in the design team. the granularity-level selected must consider the kind of service, the degree of integration needed with the product and the information needed to implement the service later along the lifecycle properly [cit] . process description granularity is one of the most sensitive issues in business process modelling [cit] . in general, coarse-grained process description is used to describe the service delivery process model. task 2: process validation. the aim is to understand how the designed service works. it is the step in which the customer and consumer can evaluate the designed service. there are many ways to evaluate the service developed depending on the kind and the degree of intimacy with the customer/consumer. two qualitative methods, not supported by any specific tool, are suggested: cognitive walkthrough and wizard of oz. through the adoption of these methods, a qualitative evaluation in terms of possible benefits and unplanned problems is performed. the output of this phase is (are) the service delivery process prototype(s)."
"4.3.4 kpi modelling. the pss design loop is closed with the definition of an appropriate set of kpis that will be used to monitor the performance of the designed pss throughout its lifecycle . the identification of the appropriate set of kpis follows a precise methodology consisting of three main steps [cit] . its final aim is to monitor the pss status during the entire pss lifecycle stages. in detail, the selected kpis should be connected with the data repository to be updated at the end of each pss lifecycle stage."
"(2) no customer data are available. this option is less structured since no information from the customer is available. the needs and related wishes are identified \"manually\" through traditional brainstorming or focus group."
"to develop the pssldm according to the here proposed research methodology, three in-depth business cases and requirements analysis have been carried out. indeed, during the first phase, these three cases were used to define general requirements upon both the methodology and the related methods and tools to be developed. additional requirements gathered from other manufacturing companies have been collected, in addition, two ict vendors have provided insights into the market available solutions, including own products, and into the corresponding state-of-the-art. furthermore, during the second phase, the overall methodology has been implemented in the three application cases also to understand its completeness, its ability to collect the needed information and its usability in a real context. all the feedback collected during the implementation have been included in the methodology with the aim to increase the industrial orientation and applicability. it is remarkable that this research methodology also supports the development of the methods and the tools involved in each stage of the methodology. to do this, the pssldm has been used as a starting point, resulting in being strategically relevant to define, through brainstorming and workgroups with scientists and managers, the related engineering environment architecture and the tools' features. indeed, requirements collected in business cases were analysed and detailed and then data model, functional specification, external interfaces, and technical specification were derived [cit] with the aim of finding the features that could better allow the creation of compelling but intuitive tools for the conceptualization, definition and monitoring of new psss. concerning the sample, the three cases are very heterogeneous, this leads to a methodology able to satisfy different kind of needs. these three business cases (bcs) are typical european smes business-to-business (b2b) manufacturing companies in the addressed sectors: one in machines for consumer goods production, the second in development of control systems for air-conditioning and refrigeration and the third in moulds production. thus, they are characterised by a different level of technology even if they have both a consolidated design process and product portfolio. their main business is to design and deliver pure products, but their willing is to move towards servitization in order to improve their portfolio in a service perspective and become more competitive, also discovering new market shares. in the following section, the pssldm is presented, describing in detail the different steps composing it. furthermore, the final validation of the entire methodology, conducted in one of the three bcs, i.e. the mould shop, is reported to show the applicability and accuracy in supporting manufacturing companies in design pss in a systematic way, taking into consideration both customers and companies' needs."
"with these results, the team moved to the second phase of the pssldm, the solution final design. as first, the service delivery process model ( figure 6 ) was created by use of the service blueprinting and the bpmn nomenclature: through it, the company explored and defined how the service resources identified in the psct should interact during the delivery process to provide the service to the customers. lean design methodology therefore, once defined the pss concept to be engineered and modelled the service to be delivered with it, the product design phase can begin through the support of the pss design guru methodology [cit] . the product chosen to be redesigned, referring to the customer operating in the plastic industry, is the \"2 cavity, 1 litre seal lid\" mould. the product design phase has the aim to make the selected product more suitable to support the service to be added and integrated on it, better addressing, as a result, the pss digital history of repairs of the mould. the product is composed of four main components (shown in figure 7 ) whose main issues are shown in table ii ."
"the lack of methodologies enabling the collaborative design of product and service features in an integrated way [cit] along its entire lifecycle is acknowledged [cit] . in this perspective, the product service system lean design methodology (pssldm) has been developed."
"the first step, towards the identification and the selection of pss concepts, is the identification of customers' needs and wishes. from a methodological point of view, the customer's needs identification can be done in two different ways according to the company's business/market and considering data availability."
"lean content design rules intervene in this phase, guiding the design team in solving pss design issues, consistently integrating the involved service resources (used in the service delivery process) and product components (listed in pdm and plm tools)."
"thus, the research methodology [cit] has been defined and declined according to different traditions or philosophies: it retrieves concepts from interpretative jmtm [cit], interactive (ellström, 2007) and system development [cit] traditions, resulting thus in being purposive and theoretical [cit] ."
"4.2.2 task 2: solution(s) concept selection. the solution(s) identified through the psct previously developed should be evaluated, and finally the one(s) that is worth to be implemented is(are) selected."
"the product service development process is not easy to perform effectively and efficiently. pss development process improvement is getting more and more attention from scholars and practitioners, with the aim to increase efficiency. too often, in fact, product and/or service development takes place through inefficient processes that generate a considerable amount of wastes at any levels and stages of the development process."
"once the service delivery process and the product have been validated, and the set of kpis to monitor the pss has been created, the pss can be launched in the market."
"(1) customer's information available: users posts, collected through social networks (public or internal) or communication channels and related to a specific pss offer are analysed to obtain feedback that can be used for the definition of the concept. in this way, all the actors involved in the whole lifecycle of psss are considered."
"as stated in section 2, one of the main gaps in the pss design and service engineering (se) models and methods previously described, is the absence of a methodology which supports manufacturers in focussing on both customer's and company's perspective and that at the same time adequately supports the integration of service and product design. this rather myopic view can lead either to the development of services fulfilling customer's needs entirely, but that can potentially undermine the company's economic sustainability in the long term, or vice versa to an inefficient pss design since the product and service designers have not appropriately interacted in the early phase of design, implying reworks and revisions later in the design process. for these reasons an industrial-oriented methodology, the pssldm is proposed, constructed considering both the theory and the analysis of industrial requirements collected during the application in the three bcs. to support companies in their pss offering and engineering, it is fundamental to consider both company and customer(s) perspectives: [cit] ) has been used as the base to develop the pssldm, since it is the only one developed with that specific focus. in this perspective, two main areas constitute the core elements of the pssldm: customer and company. moreover, in order to allow a better integration of product and service features since the early phase of development and avoiding reworks, ce [cit] and lpd [cit] concepts are integrated into the methodology, as shown later on. the pssldm encompasses four phases, namely: customer analysis, solution concept design, solution final design, and offering analysis. as shown in figure 2, the first and the fourth phases belong to the customer area, aiming at collecting and analysing customer needs, while the central phases belong to the company area, aiming at managing the integrated design process in the jmtm lean design methodology most efficient and cost-effective ways. in addition, some of these phases are further decomposed into tasks, and for each of them, a specific method has been adopted. since the product/service development process is not easy to perform effectively and efficiently, along with the entire pssldm, a critical role is played by lean rules, occurring at two different levels: the content design level and the development process level, and referring to product, service, or the design process itself. both content design and development process rules serve as guidelines to be followed by designers and engineers to guarantee the respect of customer requirements and technical constraints from one side (effective pss) and waste-free pss development process from the other side (efficient pss development). in practice, such rules become precise instructions for engineers, designers and project managers to be followed during their daily activities."
"systematic support in the detection of needs from customer inputs generation of new product-service ideas and selection of the best one support to achieve new market sectors, improving competitiveness and customer satisfaction design definition and improvement of the product/service features needed to create the new pss reduction of time to market to update the product thanks to a better knowledge of the customer needs and improved use of knowledge reduction of environmental impact, wastes in material, energy consumption and frequency of failure involvement of all the stakeholders in the pss design thanks to effective knowledge management reduction of design time thanks to effective design knowledge sharing use and service delivery monitor the pss feature thanks to a systematic collection of kpis end of life support of reconfiguration and re-design of the product, based on customer feedback and kpis output is an improvement of the company performance both internally (e.g. on the operational side) and externally (e.g. gaining a certain competitive advantage over the competitors). on the short term, companies will be provided with an innovative engineering environment and a set of methods/tools to support the concurrent collaborative design of psss based on the knowledge captured and shared across the value chain actors and the pss lifecycle. on the long-term, the functionalities of the tools will be applicable to build different product service solutions and collaborations within extended enterprise, opening new business opportunities and making manufacturing industry less prone to failure. for this reason, it is important to stress that the adoption of the pssldm and therefore of the platform foresees many challenges since the manufacturing transition \"from products to services\" entails a significant change in the company business models and organisations. in particular, in relation to the design phase of a pss, the definition of a proper alignment of the product and the service design processes and responsibilities is one the main challenge to create a coherent offer and therefore effectively respond to customer needs. this represents the main risk against a proper adoption of the methodology and of the related design support tools. to reach such an alignment, cultural change in the company and in the design department is needed, even if this requires a new service culture quite distant from the ones of traditional manufacturing companies. in particular, product designers, generally focussed on the creation of high technology products, have to change their focus from the pure-product performance to the solution ones. internal processes and capabilities must also be revised to align the company design procedures to the ones suggested by the methodology and to allow a fruitful relationship between the product and the service company functions. the km features of the methodology aim to support cooperation inside the companies and make everyone aware of the main feature/s needed to answer to customer needs. in this sense, new service-oriented skills along all the company functions are needed and must be involved along with all the design phases."
"lean content design rules are distinguished in guidelines and rules. guidelines provide a proper basis for considering generic, non-company-specific, lifecycle-oriented information to be followed during the design phases [cit] . they are a reliable way to foster effective km in terms of knowledge formalization, representation, sharing, use and re-use [cit] . such guidelines can evolve and be applied to specific company issues of either a pss or a specific component, leading to the creation of the relative rules that become concrete and quantitative instructions for engineers, designer and project managers to be followed during their daily specific design activities. the set of design rules represents hence the knowledge characterizing and belonging to the company."
"to demonstrate the utility of our models, we applied phiscs to two real scs datasets from recent studies. one of these data sets also provides additional bulk sequencing data with vaf values."
this can be viewed as a kind of multi-variate negative binomial distribution. univariate and bivariate instances of this formula can be found by restricting the number of terms in the summation to be one or two. the reason for introducing this approach here is that the same reasoning will be applied when considering a bivariate p.g.fl. which has an exponential form constructed with poisson processes to determine an alternative bivariate p.g.fl. that is able to retain second-order information.
"the paper is structured as follows: point processes are described in the next section in terms of functionals, namely the probability generating functional the factorial cumulant generating functional, and statistics in terms of factorial moments and factorial cumulants and their connections. section iii describes point process models, including the poisson process and the negative binomial point process via the laplace-stieltjes transform, and the extension to the panjer point process. section iv presents the algorithm description in terms of propagation of the first two factorial cumulants. section v shows some experimental scenarios comparing with the phd filter. the paper concludes in section vi. the appendix presents the mathematical proofs of the algorithm specification. algorithm 1 presents the algorithm pseudo-code of a gaussian mixture implementation based on a similar approach to the gaussian mixture phd filter [cit] ."
"in figure 1 we present results for the case where isa violations are allowed, but no bulk data is used. we focus only on the mutations violating isa and provide the number of true positive (tp) and false positive (fp) calls for such mutations. as our results show, phiscs outperforms sifit, the only available alternative method operating under the finite sites assumption, especially in tp measure."
"all words disambiguation mode is illustrated in figure 3 . in this mode, the system performs disambiguation of all nouns and entities in the input text. first, the text is processed with a part-ofspeech and a named entity taggers. 10 next, each detected noun or entity is disambiguated in the same way as in the single word disambiguation mode described above, yet the disambiguation results are represented as annotations of a running text. the best matching sense is represented by a hypernym and an image as depicted in figure 3 . this mode performs \"semantification\" of a text, which can, for instance, assist language learners with the understanding of a text in a foreign language: meaning of unknown to the learner words can be deduced from hypernyms and images."
this section describes (1) how wsd models are learned in an unsupervised way from text and (2) how the system uses these models to enable human interpretable disambiguation in context.
"similarly, the compliance matrix of flexible limb n can be expressed as hence, the compliance of limb m in the coordinate system o-xy can be obtained as"
"case 1 : for this case, figures 1a-3b present, the mean ospa over time, and the cardinality mean and standard deviation over time for the phd, cphd and lc cumulant filters, where we can perceive the advantage of estimating second-order information on the target number. the lc cumulant filter maintains second-order information about the target number via the second-order factorial cumulant."
"to our model. now, for any eliminated column p we do not have to check whether it is in conflict with any other column q or vice versa. therefore, for each pair (p, q) of columns we replace the constraint 12 above with the following."
"the workpiece surface morphology with non-vibration and vibration polishing are given in figure 29 . because of the unstable control of polishing slurry, some marks can be observed at the workpiece with both methods. figure 29a shows the surface roughness was improved from 95 nm sa to 80 nm sa and 504 nm sz reduced with form deviation by non-vibration polishing. the polishing scratches can apparently be observed in the area. as shown in figure 29b, the scratches were increased compared with the former as a result of more abrasives involved in polishing (44 nm sa), and 856 nm sz smaller than non-vibration by applying vibration. the performance of the vibration assisted roll-type polishing was evaluated by experiment, indicating that the developed method could effectively improve the surface quality."
"in this paper, grey wolves optimization (gwo) is introduced for dimensional optimization. inspired by the leadership hierarchy and hunting mechanism of grey wolves. the applications of solving classical engineering design problems and the real problem in optical engineering have proven that the algorithm is suitable for challenging problems with unknown search spaces [cit] . compared with well-know heuristics genetic algorithms (ga), gwo is able highly competitive. gwo has a very high level of local optimal avoidance, which enhances the probability of finding proper approximations of the optimal weight and biases. moreover, the accuracy of the optimal values for weights and biases is very high because of the high exploitation of gwo [cit] . the optimization conditions are as follows:"
"the step and sine responses along the two directions were examined to inspect the xy micro-motion stage performance. a typical proportional-integral-derivative (pid) controller was implemented to position the flexure-based stage. according to the results in figure 26, the rise times of the x axis and y axis motions were approximately 29 ms and 24 ms, respectively. moreover, very small and steady errors and overshoots were observed thanks to the feedback control during the positioning process."
"ultrasound (us) images are used for clinical diagnosis of patients suffering from diseases related to internal organs of the human body. the real-time operation, non-ionization and non-invasiveness properties of this imaging modality make it popular among other imaging modalities, such as x-ray, ct-scan and mri. us images are generated by the recording of received echo signal from internal body parts when us waves are transmitted inside the human body. in its generation process, us images are corrupted by a noise, which arises due to interference between transmitted and received signal. this noise is of multiplicative nature and appears in granular pattern in the formed us image popularly known as \"speckle noise\". this noise degrades the visible quality of us images and hence makes diagnosis difficult. fine details and edges of us images are lost due to the presence of speckle [cit] . hence, speckle noise reduction is an important aspect and may also be the preprocessing step of various image processing algorithms. speckle denoising techniques are divided into two broad categories:"
"3 https://github.com/alvations/pywsd 3.1 induction of the wsd models figure 1 presents architecture of the wsd system. as one may observe, no human labor is used to learn interpretable sense representations and the corresponding disambiguation models. instead, these are induced from the input text corpus using the jobimtext approach [cit] implemented using the apache spark framework 4, enabling seamless processing of large text collections. induction of a wsd model consists of several steps. first, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. second, word senses are induced by clustering of an ego-network of related words [cit] . each discovered word sense is represented as a cluster of words. next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. [cit] patterns. finally, the obtained wsd model is used to retrieve a list of sentences that characterize each sense. sentences that mention a given word are disambiguated and then ranked by prediction confidence. top sentences are used as sense usage examples. for more details about the model induction process refer to . currently, the following wsd models induced from a text corpus are available: word senses based on cluster word features. this model uses the cluster words from the induced word sense inventory as sparse features that represent the sense."
"this section describes the models that will be used to form approximations in the point process prior used before the update. in particular, we describe the poisson point process and determine the over-dispersed negative-binomial point process via application of the laplace-stieljes transform. we then describe the relation between the negative-binomial process to the panjer point process [cit] which encompasses the negativebinomial, poisson and binomial processes within the same model."
"according to the simplified model of wu & zhou [cit], the compliance matrix of the right-circular hinge in its local coordinate system can be defined as follows: according to equation (5), k l and k α can be derived as follows:"
"note that we focus on trees t and g that have the same set of labels. without loss of generality, we can assume that no pair of labels x, y are simultaneously present in a single node in t and in g, since x and y can be iteratively replaced with a single new label v in both trees. we can also assume that both t and g have the same root node with identical labeling unique to the root."
"single word disambiguation mode is illustrated in figure 2 . in this mode, a user specifies an ambiguous word and its context. the output of the system is a ranked list of all word senses of the ambiguous word ordered by relevance to the input context. by default, only the best matching sense is displayed. the user can quickly understand the meaning of each induced sense by looking at the hypernym and the image representing the sense. [cit] showed that web search engines can be used to acquire information about word senses. we assign an image to each word in the cluster by querying an image search api 9 using a query composed of the ambiguous word and its hypernym, e.g. \"jaguar animal\". the first hit of this query is selected to represent the induced word sense. interpretability of each sense is further ensured by providing to the user the list of related senses, the list of the most salient context clues, and the sense usage examples (cf. figure 2) . note that all these elements are obtained without manual intervention."
", respectively, were embedded into the drive structures to achieve impact structure sizes. a power amplifier (pi, e-500) was utilized to amplify the excitation signals, which were generated by the power pmac controller. a four-channel capacitive displacement sensor (micro-sense de 5300-013) was chosen for dynamic position measurements. to reduce external disturbances on the sensing and measurement system, a vibration-isolated air-floating platform was used to mount the xy micro-motion stage."
"our contributions. in this paper, we introduce three novel combinatorial formulations for inferring tumor phylogenies via an integrative use of single-cell and bulk sequencing data. (1) our simplest formulation asks to minimize a weighted sum of potential false negative (which are common) and false positive (which are rare) mutation calls in genotypes of single cells, whose correction will result in a perfect phylogeny. (2) the goal of our more general formulation is to compute a sub-perfect phylogeny, which not only requires such mutation calls to be corrected but also needs the elimination of (at most a user defined number of) mutations that violate isa (e.g. due to loh -and are relatively rare). more specifically, this formulation asks to minimize a weighted sum of mutations to be corrected, given an upper bound on the number of mutations to be eliminated (due to isa violations) in order to achieve a perfect phylogeny. (3) our most sophisticated formulation has additional constraints imposed by the use of variant allele frequencies (vafs) of single nucleotide variants (from regions not affected by copy number aberrations) that can be estimated from bulk sequencing data (as a proxy to the cellular prevalence of a given mutation). these lineage constraints impose ancestor-descendant dependencies among mutation pairs (e.g. the prevalence of an ancestral mutation cannot be lower than that of a descendant) or triplets (e.g. the prevalence of an ancestral mutation cannot be lower than the sum of two descendant siblings) and improve inference accuracy. we describe solutions to each of the three formulations to address problems of varying complexity and data availability (i.e. some data sets have no isa violations and some do not come with matching bulk sequencing data)."
"in this section, we formulate our integrative tumor phylogeny reconstruction as a combinatorial optimization problem. we first discuss two special cases of the problem for the case when only single cell sequencing data is available. i.e. (i) a special case where the isa cannot be violated (section 2.1) (ii) the case where isa can be violated (section 2.2). we then describe the general integrative problem where both bulk and scs data are available. we present solutions for this problem using a novel integer linear program (ilp) as well as a constraint satisfaction program (csp)."
"where ax and ay are the signal amplitudes in the x and y directions, respectively; f1 and f2 are the signals frequencies in the directions of x and y, respectively;  is the phase difference between the initial phases in x and y directions; and  is the displacement loss rate induced by hinges 7, 8, 9, and 10 in the y direction. then, the stage trajectories were analyzed with different vibration parameters."
"several offline tests were carried out to verify the performance of the xy micro-motion stage. the experimental setup is schematically shown in figure 22 . two pzt actuators (40 vs. 12, core tomorrow science co., ltd., harbin, china), whose sizes are 12 mm 51.5 mm   and pzt k 35 n μm , respectively, were embedded into the drive structures to achieve impact structure sizes. a power amplifier (pi, e-500) was utilized to amplify the excitation signals, which were generated by the power pmac controller. a four-channel capacitive displacement sensor (micro-sense de 5300-013) was chosen for dynamic position measurements. to reduce external disturbances on the sensing and measurement system, a vibration-isolated air-floating platform was used to mount the xy micro-motion stage."
"the bridge-type mechanism often has an extremely small deformation, that is, the   is very small. therefore, the chord length produced by the rigid body rotation is approximately equal to the arc length corresponding to   [cit] . then, y  can be expressed as follows:"
"next, the finite-element analysis software was employed to measure the output displacements of the points on the sensor brackets. similarly, the input displacements can be obtained by measuring the displacement of the input end in the bridge-type mechanism. the input-output displacement relationship of the flexure-based stage is displayed in figure 19 . it can be seen that the displacement amplification ratio was around 6.78 along the x axis, 11% smaller than the theoretical value of 7.55; that along the y axis was around 5.53, 6.5% greater than the theoretical value of 5.89. the differences are attributable to the fact that the deflections of the lever model reduced the displacement of the flexure-based stage. the deviation about displacement ratio between the x and y directions resulted from the displacement loss of the flexure hinges 7, 8, 9, and 10. in the x direction, the output stiffness was 9.02 n μm, 18.3% smaller than the theoretical value 10.67 n μm . in the y direction, the output stiffness was 11.31 n μm, 18.6% smaller than the theoretical value 13.41 n μm . the deviation mainly comes from the accuracy of the compliance factors equations and the hypothesized rigidity of these links in the matrix model."
"finally, the system provides the reasons behind the sense predictions by displaying context words triggered the prediction. each common feature is clickable, so a user is able to trace back sense cluster words containing this context feature."
"note that even when we use vafs and allow isa violations, we obtain a similar topology as depicted in figure 7 . for this topology, the vaf values do not present a contradiction -which is a highly desirable feature when the vaf values are measured reasonably accurately. also, note that this particular solution has eliminated three mutations including those in cmtm8 and rrp8. cmtm8 has the mutation with a high vaf value that contradicted with its lower subclonal placement in figure 6 . phiscs has correctly eliminated it to avoid this contradiction. rrp8, on the other hand, has a mutation whose vaf is much lower than the sum of those in its two child nodes. phiscs is again correct in eliminating it and thus avoiding any contradiction. all of this points out that phiscs can potentially help detect previously underexplored copy number aberrations or identify inconsistencies in cnv calls. figure 6 with the exception of three mutations eliminated, possibly due to the inconsistent cellular prevalence and vaf values. coloring of subclones (nodes) is guided by figure 6 . subclones corresponding to blue and green nodes are each split into two subclones compared to figure 6 . this is due to the use of vafs that provide more refined tumor subclonal composition and separation of subclonal populations, otherwise not distinguishable by the use of scs data only."
"motivated by the above-mentioned problems, we target the development of an elliptical vibration-assisted roll-type polishing (earp) system considering the flexural mechanism to extend the capabilities of existing mechanisms for precision polishing. this system has the following advantages: it works in the non-resonant mode and can deliver large working stroke; benefiting from the principle of linear contact, ultrathin substrates without brittle fracture could be realized; the flexible system supports frequency adjustment and suits the continuous production line in new electronics production; high polishing accuracy can be realized through the integration of feed and machining modules."
"in this paper, a new speckle reduction method based on shearlet coefficient modelling of us image is presented. detailed band shearlet coefficients are modelled as a nakagami prior. the denoised shearlet coefficients are obtained using map estimation in each detail subband. subjective and objective evaluation is done for synthetic and real us images. from objective evaluation of the proposed method, it is evident that the higher values of all assessment parameters are obtained for both synthetic and real us images. the effectiveness of the proposed method can also be seen from the subjective evaluation of all denoised images obtained from various methods. hence, the proposed method outperforms other existing methods and is better in view of us image denoising and fine detail preservation."
"we present the results of phiscs-i, phiscs-b and scite based on mlted in figure 3 . in order to facilitate the interpretation of results, we use mlted similarity measure instead of directly presenting mlted values. in figure 3a, we show the results of phiscs-i and phiscs-b when only single-cell data is used as the input under infinite sites model. we observe that all three methods: phiscs-i, phiscs-b and scite have comparable accuracy in this case. next, we show results of phiscs when isa violations are allowed, but no bulk data is used. we can observe that the results of phiscs-i improve in comparison to the previous case, and overall it has slightly better average performance compared to scite. when both bulk and scs data are provided as the input to phiscs, we observe further improvements in the accuracy and in this case phiscs clearly dominates scite in terms of the similarity values. due to the nearly identical performance of phiscs-b and phiscs-i we include only results of phiscs-i in figure 3b and figure 3c . we do not include results of sifit in any of these plots due to its very poor performance in each of the cases."
"where d 22 is a compliance factor of matrix c d . according to equations (26)- (29), the input compliance for the micro-motion stage in the y-direction can be derived as follows"
"the computer-aided design assembly processes of the earp system are shown graphically in figure 1 . the components of the stage can be assembled together through two steps (figure 1a,b) . the fixing bolts in figure 1a are the fixed holes constrained in all directions, and the support plate in figure 1b is only limited by the contact area in the x direction. in the additional steps shown in figure 1c -e, the micro-motion stage, the ball-screw feed drive system, and the polishing roller processing system are assembled together and fixed at the base. the front, side, and top views of the assembled model are shown in figure 1f -h. and proposed a mathematical model for linear roll-cmp to disclose the effects of polishing pads on the mrr of cu, which laid a solid theoretical basis for the roll-cmp process [cit] ."
"because of the double symmetric property, a quarter of the bridge-type mechanism is modelled ( figure 10 ). in the coordinate system e-xy, the compliance of flexure hinge d can be expressed as follows:"
"while it is still expensive and experimentally challenging to robustly perform single cell library preparation, recent technological advancements in single-cell sequencing (scs) potentially provide higher resolution data for studying ith. single-cell datasets are, however, characterized by high levels of sequencing noise that includes both false positive (e.g. due to the read errors) and false negative (e.g. due to the allele dropout or variance in sequence coverage) mutation calls, as well as missing values for mutations from sites affected by dna amplification failure. 1 this necessitates the development of sophisticated computational methods that are sensitive to the noise characteristics of scs data, while incorporating the assumptions of the clonal theory of cancer evolution to tumor evolution modeling."
"in this paper, the aim was the generation of a high-quality surface based on the linear contact material removal mechanism. the theoretical model for the micro-motion stage's statics, compliance, and dynamic properties were given and validated by the gwo, followed by prototype fabrication."
it can be seen from the tab. 3 that highest average mvr value along with the lowest standard deviation value is obtained for the proposed method. this shows the effectiveness of the proposed method over other existing methods for speckle noise reduction.
(b) for all non-eliminated mutations q different from null mutation we must make sure that it has an ancestor mutation (which could be null mutation). this is achieved by imposing the following constraint:
"in order to simulate violations of isa, for each of the 60 simulations described above, we first generate simulations where exactly one of the mutations violates isa. in order to do this, we randomly choose mutation m i and assign it to node p different from root and n t (m i ). depending on the relation between p and n t (m i ) this violation might represent recurrence or loss of previously obtained mutation. in either case we update genotype of each node and value h(m i ) (in an obvious way) and repeat the above procedure of generating bulk and scs data."
"the step and sine responses along the two directions were examined to inspect the xy micro-motion stage performance. a typical proportional-integral-derivative (pid) controller was implemented to position the flexure-based stage. according to the results in figure 26, the rise times of the x axis and y axis motions were approximately 29 ms and 24 ms, respectively. moreover, very small and steady errors and overshoots were observed thanks to the feedback control during the positioning process. step responses of the apparatus in (a) the x direction and (b) the y direction."
"the earp system consists of a ball-screw feed drive system, a polishing roller processing system, and an xy micro-motion stage with a large workspace. in particular, the ball-screw feed drive system should be noted, which includes servo motors, ball-screws, couplings, supporting bearings, linear guides, and other machine structures. another part is the polishing roller processing system, which involves an aluminum roller, a hard polyurethane polishing pad (to ensure the flatness of substrate surface), and a dc motor. the polishing roller processing system could achieve its own rotation and translation in the xz plane. the resolution of motion of the ball-screw feed drive system is 1 μm in the z direction. the value of the roller scanning and rotation speed is calculated based on the requirement of the workpiece surface quality and the nc program is generated by the personal computer. the xy micro-motion stage is driven by two pzt actuators (pzt1 and pzt2), owing to its high input stiffness. considering that fixing holes can be easily machined with a tolerance of ±5 µ m, precise assembly of the components is not impossible nowadays."
"the bridge-type mechanism often has an extremely small deformation, that is, the ∆α is very small. therefore, the chord length produced by the rigid body rotation is approximately equal to the arc length corresponding to ∆α [cit] . then, ∆y can be expressed as follows:"
"because of the double symmetric property, a quarter of the bridge-type mechanism is modelled ( figure 10 ). in the coordinate system e-xy, the compliance of flexure hinge d can be expressed as follows:"
"modelling may be the suitable way to improve the use of the earp on several applications [cit] . before the kinematics analysis, it is necessary to simplify the bridge-type mechanism as an ideal multi-rigid body mechanism with ideal pivots, as shown in figure 5 . let la be the length of the arm and α be the angle between the connecting line of arm pivots and the horizontal line."
"then, the kinetic energy of the whole mechanism can be obtained as follows next, the kinetic energy should be substituted into the lagrange's equation below,"
"the earp system is designed with a xy micro-motion stage, a roller, and a pair of pzt actuators, as shown in figure 3 . the substrate should be placed on the stage and attached to the rubber backing layer. the polishing pad is tightly coiled around the high-stiffness roller, which can dampen the vibrations at a high rolling speed."
"we name our general formulation and the resulting program phiscs (phylogeny of tumors using integrated bulk and single cell sequencing data), which comes in two flavors: (i) phiscs-i expresses our formulation in the form of an ilp and efficiently solves it by the use of gurobi optimizer. (ii) phiscs-b expresses our formulation in the form of a boolean csp and solves it by the use of open source solvers for wmax-sat such as maxhs -many times more efficiently than phiscs-i."
"according to beam theory, the compliance matrix of the right-angle hinge can be obtained by the following [cit] : the equation would appear when the force f is applied at point a ( figure 7 ) [cit] ."
"to sum up, all the test results show that the proposed earp system can follow external commands with fast response speed and high accuracy, and thus can be applied to precision polishing."
as noise is zero-mean gaussian so absolute central moments for the noise-free shearlet coefficients can be calculated from noisy shearlet coefficients using the expressions given in eq. (23) and eq. (24):
"in this experiment, we gathered a dataset consisting of definitions of babelnet 3.7 senses of 1,219 frequent nouns. 11 in total, we collected 56,003 sense definitions each labeled with gold hypernyms coming from the isa relations of babelnet."
"ilp formulations for hip and related problems are routinely solved through commercial tools such as gurobi or ibm cplex -which have been developed over many years and provide reliable and fast solutions for relatively small sized optimization problems. these solvers aim to optimize a typically linear objective while satisfying a number of numerical constraints. as such, ilp is related to another fundamental problem, the boolean constraint satisfaction problem (csp) that can be used as an alternative for modeling many ilp problems encountered in practice."
"finally, we discuss results of all tools on our newly introduced hmlted measure. in figure 4a we present results of phiscs-b, phiscs-i and scite for the case where scs data is used as the only input and infinite sites assumption is made for all mutations (i.e. no mutation elimination is allowed). results for the runs where isa violations are allowed are shown in figure 4b, where only scs data was used as the input, and in figure 4c where phiscs-b was provided with both single-cell and bulk data. as our results illustrate, both implementations of phiscs and scite have comparable accuracy for the case where single-cell data is used as the only input (we suspect that most of the differences are due to non-convergence of mcmc chain in scite or multiple equally likely solutions for phiscs-b and phiscs-i), however phiscs significantly benefits from the use of bulk data showing same or improved performance over scite in all of the cases under finite sites model. due to its very poor performance in each of the three cases, we did not include results of sifit in any of these plots. also, due to the high similarity of results of phiscs-b and phiscs-i, in figure 4b and figure 4c we only show results of phiscs-i."
"(11) all other constraints used previously in the limited version of the problem are preserved. the latest objective contains quadratic terms of the form k(j)y (i, j) which can be transformed to the linear variables using standard linearization techniques. one can observe that mutation elimination never decreases data likelihood hence the global optimum in the above maximization problem is achieved when all variables k are set to 1. however, in real applications we expect only a limited number of isa violating mutations and therefore set the upper bound k max on the number of eliminated mutations which is implemented by the addition of the following constraint"
"childhood acute lymphoblastic leukemia. a more interesting dataset that we tested phiscs on is obtained from a lymphoblastic leukemia study where both single-cell and bulk sequencing data were made available [cit] . we focused on the second patient from this study which has multiple subclones with a non-linear tree topology. for this patient, 16 mutations in 115 single cells were identified. the estimated fn rate is 0.181749. since scs data presented in the study are affected by the presence of doublets, we first pre-processed scs data matrix by the use of single cell genotyper [cit] . the tree inferred by phiscs, interestingly by the use of scs data only and with no tolerance for isa violation (results are shown in figure 6 ) seem highly accurate and in good agreement with the tree topology published in the study. the most notable difference is mutation linc00052 was placed as a single mutation at branch descending from a clonal population. however, this placement is strongly supported by mutual mutation presence in single-cells where this mutation is rarely present together with some other mutations from the dataset, except for clonal mutations plec, rims and siglec as is expected according to the inferred topology. it is interesting to observe that the vafs of specific mutations (even though they have not been used by phiscs) seem to contradict with the tree topology (e.g. the mutation on cmtm8). this is likely due to the poor vaf estimation -likely due to an undetected cnv involving the gene."
"5. in sec. 6., results are obtained for the proposed method and are compared with other state of the art methods. finally, conclusion of the proposed work is presented in sec. 7."
"many of the available tools for studying intra-tumor heterogeneity formulate the problem as an ilp or quadratic integer programming (qip) and solve it via commercial tools such as gurobi or cplex. our csp formulation (specifically in wmax-sat) is the first to express a tumor phylogeny reconstruction problem combinatorially, but in a form other than ilp/qip. additionally, unlike many alternatives, phiscs has the ability to integrate single cell and bulk sequencing data, and can simultaneously infer tumor phylogeny and clonal composition of the tumor sample. furthermore, recent studies suggest that isa, that forms the basis for most of the above tools (with sifit being the main exception), could be violated to some degree in tumor phylogenies [cit] making it impossible to establish a perfect phylogeny. phiscs addresses this issue by eliminating (a small number of) mutations that violate isa (with a cost reflected in the objective) and solves the tumor phylogeny reconstruction problem for both simulated and real data, more efficiently and more accurately (clearly for simulations but also real data) than the available alternatives."
"where l is the total length of the lever and m is input length. thus, the actual amplification ratio of the displacement amplifier can be obtained as"
"this chapter optimizes the dimensions of the 2d xy micro-motion stage to maximize the natural frequency, the determinant of the dynamic performance of the stage. thus, the dimensions r, t, and w should be optimized. the analytical models on compliance and natural frequency show that the frequency has a nonlinear positive correlation with t and w, and a negative one with r. the increase of the frequency can significantly enhance the input stiffness and slightly reduce the quality. the parameters t a1, t a2 of the lever model and l a of the bridge-type mechanism are optimized to obtain the maximum amplification ratio. further, parameter l b is optimized to achieve a compact structure. however, the optimal values of these parameters must be determined by computational method."
"to allow correction of noisy genotypes in i (i.e. bit flips and bit assignments), for each cell i and mutation j, we introduce a binary variable y (i, j) which denotes the (unknown) true status (i.e. absence or presence) of the mutation j in the cell i. assuming that α and β respectively denote false positive and false negative error rates of single-cell data we use the following scoring scheme:"
"the earp system is designed with a xy micro-motion stage, a roller, and a pair of pzt actuators, as shown in figure 3 . the substrate should be placed on the stage and attached to the rubber backing layer. the polishing pad is tightly coiled around the high-stiffness roller, which can dampen the vibrations at a high rolling speed."
"the surface roughness, a key to precision polishing, is positively correlated with the stroke of xy micro-motion stage. in light of this, a stair control voltage with maximum displacement of 72 µm was given to the x direction actuator. similarly, the maximum displacement of 84 µm was adopted as the driving signal in the y direction. figure 24 records the displacements in the x and y directions. in the figure, cmd means command displacement, while act refers to the actual displacement. it can be seen that the stroke was 71 µm and 83 µm along x and y axes, respectively. moreover, figure 24b shows that the compliant mechanism cannot track the maximum input command immediately, which demonstrates the tested compliant mechanism is working in the maximum limitation of displacement output. there is a huge difference between simulated and actual displacements along the x axis. a possible reason lies in the friction between the flexure-based stage and fixed base plate, which is generated by manufacturing accuracy and assembly errors. if the contact surface is sufficiently smooth, the actual displacement could approximate the simulated results. the resolution is a major design criterion for the flexure-based stage. in theory, the piezo-electric-driven flexure-based stage can achieve a high resolution. to identify the resolution of the flexure-based stage, a stair-step command signal was generated from a digital computer, which was provided to the peas to drive flexure stage. in each step, chattering was observed mainly because of the inherent noise effect of the capacitance sensor. the responses in figure 25a,b show that the flexure-based stage achieved the resolution of 70 nm and 56 nm along the x and y axes, respectively. if the performance of the capacitance sensor could be improved and external disturbance is reduced, a higher output resolution (~50 nm) can be obtained [cit] ."
"under the isa, for a mutation m i we define its cellular prevalence h(m i ) as a sum of frequencies of cellular populations harboring m i . according to our notation, h(m i ) can be expressed as"
"methods for propagating higher-order information have been proposed [cit] . it has been shown that the popular cphd filter [cit] can have undesirable behaviour [cit], which has been shown to be due to strong negative correlations daniel being induced after bayes' rule [cit] . more recently, secondorder filtering solutions have been proposed that propagate the mean and variance in target number [cit] ."
"compared with vector equations, energy methods can easily derive the energy equations of compliance mechanisms [cit] . thus, the natural frequencies of the 2d micro-motion stage are determined by energy equilibrium-based lagrange equation, aiming to give a panorama of the free vibrations in the 2d micro-motion stage."
"knowledge-based and/or supervised systems ims [cit] ) is a supervised allwords wsd system that allows users to integrate additional features and different classifiers. by default, the system relies on the linear support vector machines with multiple features. the autoextend (rothe and schütze, 2015) approach can be used to learn embeddings for lexemes and synsets of a lexical resource. these representations were successfully used to perform wsd using the ims."
the compliance of point d with respect to input end a is figure 13 . the free body diagram of quarter model of bridge-type mechanism.
"the relationship between the actual and nominal maximum output displacement of the pzt in the steady state can be expressed as because of the symmetric property, the kinetic energies of limbs n in the x direction can be considered as"
"as reviewed in the introduction, some parameters can be arbitrarily altered by the non-resonant vibration device. it is simple and directly generates the ellipse with two orthogonal actuators, compared with those devices with two parallel actuators. the micro-motion stage aims to vibrate at a certain frequency and time about the center point o in the plane xoy. according to the basic vibration principles, the vibration process of the 2d micro-motion stage with variable parameters can be expressed as"
"in the x direction, the output stiffness was 9.02 n/µm, 18.3% smaller than the theoretical value 10.67 n/µm. in the y direction, the output stiffness was 11.31 n/µm, 18.6% smaller than the theoretical value 13.41 n/µm. the deviation mainly comes from the accuracy of the compliance factors equations and the hypothesized rigidity of these links in the matrix model."
"most of the existing approaches for studying ith are based on analyzing data from nextgeneration bulk sequencing experiments where only an average signal over a large number of cells is obtained. in the past few years, numerous computational methods for analyzing such signals with the aim of inferring tumor subclonal composition and evolutionary history have been developed [cit] . even though these methods employ a variety of computational approaches -each with a particular strength, all have theoretical limitations, mainly due to the limited resolution offered by bulk sequencing data."
"wsd performance is measured using the accuracy with respect to the sentences labeled with the direct hypernyms (hypers) or an extended set of hypernym including hypernyms of hypernyms most of the nouns come from the twsi [cit] dataset, while the remaining nouns were manually selected. table 2 : performance of the hypernymy labeling in context on the babelnet dataset."
molc method is used for parameter estimation of nakagami distribution from the observed noisy data. this method is widely used for parameter estimation because of its low variance and high computational efficiency. absolute values of shearlet coefficients are used for parameter estimation as molc method works only with a random variable having positive values ( + ). molc method is based on mellin transform defined as given in eq. (9) below.
"as the conventional roll-type polishing system performs polishing with line-contact material removal mechanism, the polishing contact area is much smaller than that of the rotary cmp system. accordingly, the abrasives directly involved in the polishing are insufficient, which impacts the surface roughness. in order to raise the amount of abrasives involved in polishing, an elliptical vibration is introduced in the processing, as shown in figure 4 . l is the roller length, d is the roller diameter, and a is the apparent area of contact between the pad and the substrate. accordingly, the abrasives directly involved in the polishing are insufficient, which impacts the surface roughness. in order to raise the amount of abrasives involved in polishing, an elliptical vibration is introduced in the processing, as shown in figure 4 . l is the roller length, d is the roller diameter, and a is the apparent area of contact between the pad and the substrate."
average mvr values with std. deviation noisy image 11.57 ± 5.89 wiener [cit] 14.21 ± 4.92 srad [cit] 14.98 ± 4.15 wavelet [cit] 15.18 ± 4.27 bayesshrink [cit] 15.01 ± 4.62 soft thresholding [cit] 15.59 ± 4.04 swt-cauchy [cit] 16.11 ± 3.92 proposed 17.13 ± 3.71
"super senses. in the case of the super sense inventory, the model based solely on the cluster words yielded better results that the context-based model. note here that (1) the clusters that represent super senses are substantially larger than word sense clusters and thus less sparse, (2) words in the super sense clusters are unweighted in contrast to word sense cluster, thus averaging of word vectors is more noise-prone. besides, the performance scores of the models based on the super sense inventories are substantially lower compared to their counterparts based on the traditional \"per word\" inventories. super sense models are able to perform classification for any unknown word missing in the training corpus, but their disambiguation task is more complex (the models need to choose one of 712 classes as compared to an average of 2-3 classes for the \"per word\" inventories). this is illustrated by the near-zero scores of the random and the mfs baselines for this model."
"where c oi is the compliance matrix of the flexure hinge in its local coordinate system o i − xy [cit] . note that the compliance factors in the matrices of the right-angle hinge and the right-circular hinge can be derived from equations (5) and (10), respectively."
"in order to implement this efficiently in conjunctive normal form (cnf) required by all available wmax-sat solvers, we need to introduce additional boolean variables a t (p, q)"
"we present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free wsd models. the contribution of this paper is the first system for word sense induction and disambiguation, which is unsupervised, knowledge-free, and interpretable at the same time. the system is based on the wsd approach of and is designed to reach interpretability level of knowledge-based systems, such as babelfy [cit], within an unsupervised knowledgefree framework. implementation of the system is open source. 1 a live demo featuring several disambiguation models is available online. 2"
"thanks to the symmetric property, half of the lever mechanism is simulated ( figure 11) . then, the compliance of the left limb m in the coordinate system c-xy can be described as hence, the compliance of limb m in the coordinate system o-xy can be obtained as"
"a set of real us images was collected from an online us image database [cit] . experiments were performed on these images and subjective quality assessment was carried out. for objective quality assessment, a parameter known as mvr is used. mvr is the ratio of mean to variance of a selected region in an image. a different value of mvr is obtained for a differently selected region in each image. a higher value of mvr parameter is desired in case of a better-denoised image. similarly, a total number of 120 different regions were chosen from a set of 50 real us images for mvr calculation. table 3 shows the average mvr values along with their standard deviations for 120 measurements. mvr values are calculated in all these regions for all the denoising methods under consideration."
"modelling may be the suitable way to improve the use of the earp on several applications [cit] . before the kinematics analysis, it is necessary to simplify the bridge-type mechanism as an ideal multi-rigid body mechanism with ideal pivots, as shown in figure 5 . let l a be the length of the arm and α be the angle between the connecting line of arm pivots and the horizontal line."
"4., parameters of nakagami distribution are estimated using molc and noise-free shearlet coefficients using absolute central moments. map estimation used for estimating noisefree shearlet coefficients is presented in sec."
"we present the first openly available word sense disambiguation system that is unsupervised, knowledge-free, and interpretable at the same time. the system performs extraction of word and super sense inventories from a text corpus. the disambiguation models are learned in an unsupervised way for all words in the corpus on the basis on the induced inventories. the user interface of the system provides efficient access to the produced wsd models via a restful api or via an interactive web-based graphical user interface. the system is available online and can be directly used from external applications. the code and the wsd models are open source. besides, in-house deployments of the system are made easy due to the use of the docker containers. 12 a prominent direction for future work is supporting more languages and establishing cross-lingual sense links."
the objective defined in 4 is also modified so that the eliminated mutations don't contribute to the objective score. this leads to the following objective for the case allowing for isa violations:
"according to the geometry of the micro-motion stage, the maximum angular deflection may occur on the hinge, which belongs to either bridge-type mechanism or level model. if the stage is actuated with a full stroke of the pzt, it could arrive at the maximum values [cit] . under such a case, we have"
"the swept excitation method is chosen to study the dynamic characteristic as it is an expedient method to use. a variable frequency sine sweep signal was applied to the pzt actuators for each axis, and the displacement response was recorded and analyzed by fast fourier transform (fft). the measured result along y and x axes are shown in figure 23a,b. it can be obtained from figure 23a that the first three natural frequencies were 237.51 hz, 361.43 hz, and 522.75 hz, respectively. figure 23b shows the first three natural frequencies were 205.39 hz, 294.80 hz, and 502.17 hz, which were coincident with the first three natural frequencies measured along the x axis as well. the marginal difference may be caused by manufacturing errors and additional mass. according to a higher increase in the equivalent masses compared with the increase in stiffness, the frequencies obtained are relatively smaller. the working bandwidth of the machining system is limited by the lowest first natural frequency. if the contacts between pzt actuators and input ends perfectly, a micro-motion in a high frequency range can be achieved [cit] . however, the resonance frequencies satisfy the requirements of precision polishing at medium or low roll speed. figure 23b shows the first three natural frequencies were 205.39 hz, 294.80 hz, and 502.17 hz, which were coincident with the first three natural frequencies measured along the x axis as well. the marginal difference may be caused by manufacturing errors and additional mass. according to a higher increase in the equivalent masses compared with the increase in stiffness, the frequencies obtained are relatively smaller. the working bandwidth of the machining system is limited by the lowest first natural frequency. if the contacts between pzt actuators and input ends perfectly, a micro-motion in a high frequency range can be achieved [cit] . however, the resonance frequencies satisfy the requirements of precision polishing at medium or low roll speed."
"the surface roughness, a key to precision polishing, is positively correlated with the stroke of xy micro-motion stage. in light of this, a stair control voltage with maximum displacement of 72 µ m was given to the x direction actuator. similarly, the maximum displacement of 84 µm was adopted as the driving signal in the y direction. figure 24 records the displacements in the x and y directions. in the figure, cmd means command displacement, while act refers to the actual displacement. it can be seen that the stroke was 71 µm and 83 µm along x and y axes, respectively. moreover, figure 24b shows that the compliant mechanism cannot track the maximum input command immediately, which demonstrates the tested compliant mechanism is working in the maximum limitation of displacement output. there is a huge difference between simulated and actual displacements along the x axis. a possible reason lies in the friction between the flexure-based stage and fixed base plate, which is generated by manufacturing accuracy and assembly errors. if the contact surface is sufficiently smooth, the actual displacement could approximate the simulated"
"experimental tests performed show that the vibration strokes in the x-and y-directions can reach 71 μm and 83 μm, respectively, while the resolution at points b and c of the micro-motion stage are 70 nm and 56 nm, respctively."
the p.g.fl. of the updated target process φ k is obtained from the differentiation of the joint p.g.fl. using bayes' rule [cit] with the following
"in bayesian shrinkage (bs) techniques, a prior distribution of transformed coefficients is to be assumed and original noise-free signal is estimated from noisy signal in bayesian environment. also, an accurate estimation of signal and noise characteristics is required to be achieved for both speckle suppression and edge preservation. thus, the performance of these filters is mainly dependent on the correct selection of prior for modelling of transformed coefficients. a large number of bs techniques based on wavelets and its extensions are available in literature."
"the computer-aided design assembly processes of the earp system are shown graphically in figure 1 . the components of the stage can be assembled together through two steps (figure 1a,b) . the fixing bolts in figure 1a are the fixed holes constrained in all directions, and the support plate in figure 1b is only limited by the contact area in the x direction. in the additional steps shown in figure 1c -e, the micro-motion stage, the ball-screw feed drive system, and the polishing roller processing system are assembled together and fixed at the base. the front, side, and top views of the assembled model are shown in figure 1f -h."
"where l is the total length of the lever and m is input length. thus, the actual amplification ratio of the displacement amplifier can be obtained as"
"as reviewed in the introduction, some parameters can be arbitrarily altered by the non-resonant vibration device. it is simple and directly generates the ellipse with two orthogonal actuators, compared with those devices with two parallel actuators. the micro-motion stage aims to vibrate at a certain frequency and time about the center point o in the plane xoy. according to the basic vibration principles, the vibration process of the 2d micro-motion stage with variable parameters can be expressed as"
"the stiffness model of the micro-motion stage with one limb actuated is established to calculate the input stiffness in the x-direction, as shown in figure 14 . the compliance of the point d can be calculated by"
word senses based on context word features. this representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores.
"the clonal theory of cancer evolution suggests that cancer is an evolutionary disease where multiple distinct cellular populations (i.e. subclones) emerge through successive rounds of mutation and selection. at the time of clinical diagnosis, most tumors are heterogeneous, consisting of multiple subclones harboring different sets of somatic mutations. an increasing evidence suggests that this phenomenon, better known as \"intra-tumor heterogeneity\" (ith), has a profound impact on treatment outcomes and that the existence of treatment resistant subclones is one of the main causes of treatment failures [cit] . deciphering intra-tumor heterogeneity and tumor evolutionary history thus represent some of the key challenges in designing efficiently combined therapies and better understanding of dynamics of cancer initiation and progression."
"we ask to find the minimum weighted number of bit flips (typically from 0 to 1 and rarely from 1 to 0) and bit assignments (assigning a 0 or 1 to an entry with value 2), where bit assignments are not a part of the objective, such that the resulting matrix provides a perfect phylogeny (pp). we recall that a binary matrix is a pp if the three-gametes rule holds, i.e. for any given pair of columns (mutations) there are no three rows (cells) with configuration (1, 0), (0, 1) and (1, 1) . bit flipping in the input matrix i is essential to building a pp as some mutation inferences in i are false positives and some mutations are not indicated in i (false negatives) as they do not have sufficient read support in sequenced single cells."
"word senses. all evaluated models outperform both random and most frequent sense baselines, see table 2 . the latter picks the sense that corresponds to the largest sense cluster . in the case of the traditional \"per word\" inventories, the model based on the context features outperform the models based on cluster words. while sense representations based on the clusters of semantically related words contain highly accurate features, such representations are sparse as one sense contains at most 200 features. as the result, often the model based on the cluster words contain no common features with the features extracted from the input context. the sense representations based on the aggregated context clues are much less sparse, which explains their superior performance."
"in other words, the random variable describing the number of objects within b has its mean equal to its variance. the factorial cumulant generating functional (f.g.fl.) is then"
"the step and sine responses along the two directions were examined to inspect the xy micro-motion stage performance. a typical proportional-integral-derivative (pid) controller was implemented to position the flexure-based stage. according to the results in figure 26, the rise times of the x axis and y axis motions were approximately 29 ms and 24 ms, respectively. moreover, very small and steady errors and overshoots were observed thanks to the feedback control during the positioning process. the resolution is a major design criterion for the flexure-based stage. in theory, the piezo-electricdriven flexure-based stage can achieve a high resolution. to identify the resolution of the flexure-based stage, a stair-step command signal was generated from a digital computer, which was provided to the peas to drive flexure stage. in each step, chattering was observed mainly because of the inherent noise effect of the capacitance sensor. the responses in figure 25a,b show that the flexure-based stage achieved the resolution of 70 nm and 56 nm along the x and y axes, respectively. if the performance of the capacitance sensor could be improved and external disturbance is reduced, a higher output resolution (~50 nm) can be obtained [cit] . the resolution is a major design criterion for the flexure-based stage. in theory, the piezo-electric-driven flexure-based stage can achieve a high resolution. to identify the resolution of the flexure-based stage, a stair-step command signal was generated from a digital computer, which was provided to the peas to drive flexure stage. in each step, chattering was observed mainly because of the inherent noise effect of the capacitance sensor. the responses in figure 25a,b show that the flexure-based stage achieved the resolution of 70 nm and 56 nm along the x and y axes, respectively. if the performance of the capacitance sensor could be improved and external disturbance is reduced, a higher output resolution (~50 nm) can be obtained [cit] ."
"the swept excitation method is chosen to study the dynamic characteristic as it is an expedient method to use. a variable frequency sine sweep signal was applied to the pzt actuators for each axis, and the displacement response was recorded and analyzed by fast fourier transform (fft). the"
"as we have already discussed in section 1, recent evidence suggests isa might be violated for a subset of mutations in the input data. to account for this, we introduce a more general version of what we discussed in the previous section where we allow elimination (i.e. deletion) from the input matrix of a given maximum number of mutations which do not satisfy isa, while the remaining mutations, after genotype corrections, are expected to satisfy pp. in order to achieve this, for each mutation q we introduce binary variable k(q) which is set to 1 if and only if mutation q is among eliminated mutations. to exclude eliminated mutations from three-gametes rule, we do not require mutational pairs (p, q), where at least one of p and q is among eliminated mutations, to fulfill this rule. therefore we modify constraint 9 from the integer linear program we described in section 2.1 by replacing it with:"
"the constraints are determined by the following factors: the value of t/r is limited to ensure the accuracy of the selected compliance factors c i . to prevent plastic failure of the material, the constraint equations of the stress analysis must be satisfied, and the safety factor s f is set to 1.5. in addition, the thinnest part of the flexible hinge should not be smaller than 0.3 mm; otherwise, the wire electro-discharge machining (wedm) technique of the micro-motion stage cannot ensure the tolerance of 0.01 mm. under lever bending and hinge stretching, the displacement loss may produce a sub-optimal lever amplification ratio in the optimization of lever dimension."
compounding techniques are quite old and are used during us image acquisition process whereas postprocessing techniques are applied after generation of us images. post-processing methods are broadly classified as:
"super senses based on cluster word features. to build this model, induced word senses are first globally clustered using the chinese whispers graph clustering algorithm [cit] . the edges in this sense graph are established by disambiguation of the related words [cit] . the resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. \"animal\". this set of semantic classes is used as an automatically learned inventory of super senses: there is only one global sense inventory shared among all words in contrast to the two previous traditional \"per word\" models. each semantic class is labeled with hypernyms. this model uses words belonging to the semantic class as features."
"to enable fast access to the sense inventories and effective parallel predictions, the wsd models obtained at the previous step were indexed in a relational database. 5 in particular, each word sense is represented by its hypernyms, related words, and usage examples. besides, for each sense, the database stores an aggregated context word representation in the form of a serialized object containing a sparse vector in the breeze format. 6 during the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. this back-end is implemented as a restful api using the play framework. 7"
perhypers). a correct match occurs when the predicted sense has at least one common hypernym with the gold hypernyms of the target word in a test sentence.
"in this paper, the aim was the generation of a high-quality surface based on the linear contact material removal mechanism. the theoretical model for the micro-motion stage's statics, compliance, and dynamic properties were given and validated by the gwo, followed by prototype fabrication. a closed-loop test and polishing experiment were determined to meet the critical requirements for precision finishing. compared with existing mechanisms, dedicated to polishing, the advantages of the proposed earp system are listed as follows:"
"a closed-loop test and polishing experiment were determined to meet the critical requirements for precision finishing. compared with existing mechanisms, dedicated to polishing, the advantages of the proposed earp system are listed as follows:"
"where e is the elastic modulus and g is the shear modulus of the hinge material. the coordinate system of the right-angle hinge is shown in figure 8 . according to equation (5),"
"the graphical user interface of our system is implemented as a single page web application using the react framework. 8 the application performs disambiguation of a text entered by a user. in particular, the web application features two modes:"
"this is a linear, functional so computing cumulants are zero for orders greater than 1, and the first-order cumulant is equal to the first-order factorial moment, i.e."
"s x, s y and s n are the dst of the log-transformed version of noiseless, noisy and noise component, respectively. all detail subbands are despeckled while approximation band remain untouched. the logtransformed speckle noise can be modelled as zeromean gaussian distribution."
"note that the compliance factors in the matrices of the right-angle hinge and the right-circular hinge can be derived from equations (5) and (10), respectively. for a flexure hinge, the compliance matrix can be transferred from the local coordinate system"
"thus, estimated clean shearlet coefficients can be obtained using eq. (29) . 't ' is a tuning parameter used in the map expression as the noise variance of highfrequency subbands is variable."
"the output stiffness, input stiffness, amplification ratio, and natural frequency of the xy micro-motion stage were validated without damping on the finite element software abaqus/explicit (abaqus inc., palo alto, ca, usa). the material used in the simulations was 6061 al, for which the modulus of elasticity was 69,000 mpa, possion's ratio was 0.3, and density was 2700 kg/m 3 . further, a 3.9 mm tetrahedral grid was chosen in mesh. the boundary conditions were set up after the geometric model meshing step and the input ends of the proposed micro-motion stage were applied to the load. the parameters of the model were described in table 1 . sensitivity and convergence tests were performed at points b and c, respectively, as shown in figure 15 . during the earp process, the hinges suffer from reciprocating tensile deformation, resulting in internal stress. accordingly, internal stress on the compliance structure was tested in this paper. specifically, the input load and output motion of the xy micro-motion stage were obtained as a given displacement on the input end of bridge-type mechanism; then, the input stiffness and amplification ratio of the flexure-based stage were determined based on the input load and output"
"the following section describes the panjer point process [cit], based on the panjer distribution [cit], which extends the negative binomial by considering negative α and β."
"the output stiffness, input stiffness, amplification ratio, and natural frequency of the xy micro-motion stage were validated without damping on the finite element software abaqus/explicit (abaqus inc., palo alto, ca, usa). the material used in the simulations was 6061 al, for which the modulus of elasticity was 69,000 mpa, possion's ratio was 0.3, and density was 2700 kg/m 3 . further, a 3.9 mm tetrahedral grid was chosen in mesh. the boundary conditions were set up after the geometric model meshing step and the input ends of the proposed micro-motion stage were applied to the load. the parameters of the model were described in table 1 . sensitivity and convergence tests were performed at points b and c, respectively, as shown in figure 15 . during the earp process, the hinges suffer from reciprocating tensile deformation, resulting in internal stress. accordingly, internal stress on the compliance structure was tested in this paper. specifically, the input load and output motion of the xy micro-motion stage were obtained as a given displacement on the input end of bridge-type mechanism; then, the input stiffness and amplification ratio of the flexure-based stage were determined based on the input load and output motion (figure 17) . furthermore, the output stiffness can be evaluated by exerting an external force onto the stage. the force-displacement curves of xy micro-motion stage are presented in figure 18 . it can be seen from figure 18a that the driving point displacement in the x direction increased linearly with the corresponding input load, when the stage was driven in x and y directions, respectively. the finite-element analysis shows that the input stiffness was 30.57 n/µm, 6.9% smaller than the theoretical value obtained by compliance matrix method. as shown in figure 18c, the stiffness (31.2 n/µm) obtained by finite-element analysis was 7.4% smaller than that the theoretical value (33.71 n/µm) acquired by compliance matrix method. the output stiffness was tested by applying an external force on the flexure-based stage, and the resulting force-displacement curves are shown in figure 18b,d."
"in this section, we present experimental results for an illustrative example, under parametric evaluations that vary either the clutter model, number of targets, average number of false alarms per frame, or probability of detection. these parametric evaluations intend to show differences of performance of the phd, cphd and linear-complexity cumulant-based (lccumulant) filters for different ranges of settings."
"due to the practical demand and complexities of multiple target tracking, there have been many different approaches to addressing the problem. methods for multiple hypothesis tracking have attracted a great deal of attention since they provide an intuitively appealing basis for designing an engineering system, eg. [cit] . methods for managing joint multi-target probabilities [cit] represent a different way of dealing with data association ambiguities. methods based on nearest neighbour assignments [cit] provide pragmatic solutions to dealing with data association assignment. more recently, closed-form solutions to a class of multi-target tracking problems have been developed [cit], which are gaining attention for scenarios where there is a prioritisation of estimation accuracy over computational complexity."
"in this section, we perform an extra evaluation that assesses how well hypernyms of ambiguous words are assigned in context by our system. namely, the task is to assign a correct hypernym of an ambiguous word, e.g. \"animal\" for the word \"jaguar\" in the context \"jaguar is a large spotted predator of tropical america\". this task does not depend on a fixed sense inventory and evaluates at the same time wsd performance and the quality of the hypernymy labels of the induced senses."
"the polishing experiments were performed on the independently developed vibration-assisted roll-type polishing machine. figure 27 shows photographs of the machine. the polishing roll is mounted on an xz table by linear guides. the micro-motion stage is mounted on a b-axis tilting table and the workpiece is mounted on the flexure stage with bonding wax. the experiment setup consists of the machines, the micro-motion stage, the charge amplifer (type 5018, kistler, winterthur, switzerland), the force sensor (type 9211b, kistler), the peas (40 vs. 12, core tomorrow science co., ltd., harbin, china), signal generator (dg4162, rigol, beijing, china), power amplifier (pi, e-500), and so on. the control signals provided by the signal generator were amplified using power amplifier and were then sent to drive the piezo-electric stacks. to achieve accurate polishing, the force sensor was applied to detect abrasive-workpiece engagement and the charge amplifier was used to measure the polishing forces. the specific experimental parameters were as follows: roll speed: 240 r/min; frequency (f1, f2): 120 hz, 120 hz; amplitude (a1, a2): 5 μm, 6 μm; input voltage (v1, v2): 4 v, 8 v; phase difference: 90 . the workpiece material was ground prior to polishing of sic ceramic, as shown in figure 28 . the white-light interferometer (zygonewview, middlefield, ct, usa) was employed to capture the topography features."
"the compliance of the left section can be written as (17) thanks to the symmetric property, half of the lever mechanism is simulated ( figure 11) . then, the compliance of the left limb m in the coordinate system c-xy can be described as hence, the compliance of limb m in the coordinate system o-xy can be obtained as"
"the bridge-type mechanism often has an extremely small deformation, that is, the   is very small. therefore, the chord length produced by the rigid body rotation is approximately equal to the arc length corresponding to   [cit] . then, y  can be expressed as follows:"
"several offline tests were carried out to verify the performance of the xy micro-motion stage. the experimental setup is schematically shown in figure 22 . two pzt actuators (40 vs. 12, core tomorrow science co., ltd., harbin, china), whose sizes are 12 mm 51.5 mm"
"compared with vector equations, energy methods can easily derive the energy equations of compliance mechanisms [cit] . thus, the natural frequencies of the 2d micro-motion stage are determined by energy equilibrium-based lagrange equation, aiming to give a panorama of the free vibrations in the 2d micro-motion stage."
"the the first four mode shapes of the flexure-based stage ( figure 21 ) show that the two translational vibrations occurred at the first and the second modes with the frequencies 220.43 hz and 233.67 hz, respectively. because of the structure symmetry, the first two modes had almost the same frequencies. the corresponding theoretical values obtained from lagrange's equation were both 239.7 hz, 8.7% and 2.6% higher than the results of the finite-element analysis, respectively. the third and fourth modes had frequencies of 513.20 hz and 628.36 hz, leading to rotation and twisting, respectively. these two modes were neglected in analytical modelling, owing to the simulation difficulties. the first four mode shapes of the flexure-based stage ( figure 21) show that the two translational vibrations occurred at the first and the second modes with the frequencies 220.43 hz and 233.67 hz, respectively. because of the structure symmetry, the first two modes had almost the same frequencies. the corresponding theoretical values obtained from lagrange's equation were both 239.7 hz, 8.7% and 2.6% higher than the results of the finite-element analysis, respectively. the third and fourth modes had frequencies of 513.20 hz and 628.36 hz, leading to rotation and twisting, respectively. these two modes were neglected in analytical modelling, owing to the simulation difficulties."
"as limb w undergoes translational motions and contains limb m, the centre mobile platform, and the decoupler of ideal prismatic joints, the kinetic energies of limbs w in the x direction can be derived as"
"a closed-form solution to the algorithm developed is presented in algorithm 1 in the appendix. it is based on the solutions developed for phd filter [cit], cphd filter [cit], and second-order phd filter [cit], eg. [cit] . the filter is the same complexity as the phd filter with target-number variance [cit], if the variance is computed over the entire state space. compared with the gaussian mixture phd filter [cit], with linear-complexity analysis presented in section iii.c of that work, in the prediction step, there are the following additional calculation to compute the predicted second-order cumulant,"
"our results in table 1 show that, in terms of running time, phiscs-b significantly outperforms phiscs-i in all cases. among the used max-sat solvers, maxino was the top-performing one typically terminating in a few seconds. the only exceptions are cases with 4 subclones and higher false negative error rates. however, even in these computationally most difficult cases, it terminates (within a given time limit) on more instances than most of the other tools which in part explains its higher average running time compared to the other tools (as the running time average was taken only over the terminated instances). while the average running time of z3 and maxhs is higher in comparison to maxino, these tools also show significantly better performance than the gurobi implementation of phiscs-i."
"experimental tests performed show that the vibration strokes in the x-and y-directions can reach 71 µm and 83 µm, respectively, while the resolution at points b and c of the micro-motion stage are 70 nm and 56 nm, respctively. (2) because of the high stiffness of the proposed flexure-based stage, natural frequencies that meet the requirements are also achieved, which were examined to be 205.39 hz and 237.51 hz in the xand y-directions, respectively. (3) compared with the non-vibration roll-type polishing system, 44 nm sa and 856 nm sz are improved by proposed earp system used independently developed polishing machine. accordingly, the micro-motion stage could increase the number of abrasive particles involved in polishing."
where e is the elastic modulus and g is the shear modulus of the hinge material. the coordinate system of the right-angle hinge is shown in figure 8 .
"super senses based on context word features. this model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class."
the flow chart of the proposed method is presented in fig. 4 log-transformed noisy image is used as input image. 2-d dst is applied on the input image to obtain detail bands and approximation band. then modelling of all detail band shearlet coefficients is done assuming the distribution of shearlet coefficients as a nakagami prior. parameters of nakagami distribution are estimated using molc method and then for noiseless coefficients using absolute central moment method. map estimation is then applied on the noisy shearlet coefficients to obtain the noise-free shearlet coefficients. then 2-d inverse dst followed by exponential operation is applied on denoised shearlet coefficients to obtain the final denoised image. a bayesian map estimator is designed to estimate the noise-free shearlet coefficientss x .
"under the axial load, the maximum tensile stress may occur on the thinnest portions of the flexure hinges constructing the bridge-type mechanism, which can be determined by"
"perhaps the best-known variant of csp is the satisfiability problem (sat) which asks to find a boolean assignment to a set of input variables to satisfy (the conjunction of) a number of boolean constraints. 2 among other variants, max-sat asks to find a boolean assignment to variables such that not necessarily all but the maximum number of input constraints are satisfied, while weighted version of max-sat, which can be abbreviated as wmax-sat, asks for the assignment that maximizes the sum of (user defined) weights of the constraints satisfied. the generality of sat and (w)max-sat has prompted the development of many tools to solve them with the goal of obtaining solutions to practical instances of np-complete problems. these tools compete in the annual sat conference on several benchmarking datasets generated by a wide variety of applications (see http:// [cit] .gitlab.io). recently developed wmax-sat solvers such as maxino [cit] and maxhs [cit], are not only very fast but the later is also open source. as a result, a number of studies demonstrated the utility of csp solvers for the haplotype inference problem and its variants -before the advent of high throughput sequencing [cit] . to the best of our knowledge, however, no study has explored the use of csp in the context of intra-tumor heterogeneity or tumor phylogeny modeling."
"the additive noise can be obtained from multiplicative speckle noise by taking log transformation of noisy image. suppose, f, g and n are log values of x, y and n, respectively. it can be represented as given in eq. (2)."
"results of phiscs for the case where both scs and bulk data are used under finite sites model are presented in figure 2 . (note that neither scite, nor sifit exploits vafs obtained from the bulk data.) as figure 2 illustrates, the combined use of single-cell and bulk data results in improved accuracy for both, tp and fn calls."
"considering the huge topological differences, each arm in the bridge-type mechanism is simulated as a link between two flexure hinges, and each flexure hinge is modelled as an elastic beam. as the structure is symmetrical, only one bridge arm of the mechanical model needs to be analyzed. through kinematic analysis, the static mechanics model of the flexure hinge ab can be simplified as a quarter mechanic model, as shown in figure 6 . as shown in figure 7, there are only two symmetric, horizontally balanced forces at point b. meanwhile, the arm ab has two moments, and the force at point a is provided by point b. the equation would appear when the force f is applied at point a ( figure 7 ) [cit] ."
"to sum up, all the test results show that the proposed earp system can follow external commands with fast response speed and high accuracy, and thus can be applied to precision polishing."
where φ x (s) is the first characteristic function of second kind and f x (x) is the density function of nakagami distribution. the first characteristic function of second kind for nakagami distribution is obtained using eq. (9) and is given in eq. (10) below.
"modelling may be the suitable way to improve the use of the earp on several applications [cit] . before the kinematics analysis, it is necessary to simplify the bridge-type mechanism as an ideal multi-rigid body mechanism with ideal pivots, as shown in figure 5 . let la be the length of the arm and α be the angle between the connecting line of arm pivots and the horizontal line. accordingly, the abrasives directly involved in the polishing are insufficient, which impacts the surface roughness. in order to raise the amount of abrasives involved in polishing, an elliptical vibration is introduced in the processing, as shown in figure 4 . l is the roller length, d is the roller diameter, and a is the apparent area of contact between the pad and the substrate."
"according to the simplified model of wu & zhou [cit], the compliance matrix of the right-circular hinge in its local coordinate system can be defined as follows:"
"thanks to the symmetric property, half of the lever mechanism is simulated ( figure 11 ). then, the compliance of the left limb m in the coordinate system c-xy can be described as"
"therefore, the compliance of point d can be calculated as figure 13 shows the free body diagram of the quarter model of bridge-type mechanism. assuming that point d remains fixed [cit], the following equations can be obtained according to the force-deflection relationship at the input end a the two hinges are connected to the limb n of chains 9 and 10; thus, we have"
the second characteristic function of second kind is obtained by taking the natural logarithm of the first characteristic function of second kind given in eq. (11) .
"in this paper, a new transform domain method for speckle noise reduction in us images is presented. discrete shearlet transform (dst) is used for the purpose of signal decomposition. noise-free log-transformed shearlet coefficients are modelled as nakagami distribution and speckle noise coefficients are modelled as gaussian distribution. the major contribution of the presented work lies in the estimation of parameters of nakagami distribution using method of log cumulants (molc) from noisy observations. thereafter, a method based on bayesian framework popularly known as map estimation is employed to obtain speckle-free coefficients. section 2., presents the speckle noise model in medical us images. section 3., presents the brief description of dst which is used to obtain the transformed coefficients. in sec."
"we compared both the ilp and csp implementations of phiscs against two of the methods operating on single-cell data, namely scite and sifit. we were not able to compare against onconem since it terminated with an error for most of the input matrices nor against ddclone which does not infer phylogeny."
"the coordinate system of the right-circular hinge is shown in figure 9 . as the links have much higher stiffness than flexure hinges, the motions of micro-motion stage can be viewed as the elastic deformation of the flexure hinges [cit] . according to figures 8 and 9,"
"dkpro wsd [cit] ) is a modular, extensible java framework for word sense disambiguation. it implements multiple wsd methods and also provides an interface to evaluation datasets. pywsd 3 project also provides implementations of popular wsd methods, but these are implemented in the python language."
"in our prior work, we performed a thorough evaluation of the method implemented in our system on two datasets showing the state-of-the-art performance of the approach as compared to other unsupervised knowledge-free [cit] and two unsupervised knowledge-free wsd systems based on word sense embeddings [cit] . these evaluations were based on the \"lexical sample\" setting, where the system is expected to predict a sense identifier of the ambiguous word."
"the earp system consists of a ball-screw feed drive system, a polishing roller processing system, and an xy micro-motion stage with a large workspace. in particular, the ball-screw feed drive system should be noted, which includes servo motors, ball-screws, couplings, supporting bearings, linear guides, and other machine structures. another part is the polishing roller processing system, which involves an aluminum roller, a hard polyurethane polishing pad (to ensure the flatness of substrate surface), and a dc motor. the polishing roller processing system could achieve its own rotation and translation in the xz plane. the resolution of motion of the ball-screw feed drive system is 1 µm in the z direction. the value of the roller scanning and rotation speed is calculated based on the requirement of the workpiece surface quality and the nc program is generated by the personal computer. the xy micro-motion stage is driven by two pzt actuators (pzt1 and pzt2), owing to its high input stiffness. considering that fixing holes can be easily machined with a tolerance of ±5 µm, precise assembly of the components is not impossible nowadays."
"because of the symmetric property, the kinetic energies of limbs n in the x direction can be considered as then, the input compliance for the micro-motion stage in the x-direction can be derived as follows:"
"the polishing experiments were performed on the independently developed vibration-assisted roll-type polishing machine. figure 27 shows photographs of the machine. the polishing roll is mounted on an xz table by linear guides. the micro-motion stage is mounted on a b-axis tilting table and the workpiece is mounted on the flexure stage with bonding wax. the experiment setup consists of the machines, the micro-motion stage, the charge amplifer (type 5018, kistler, winterthur, switzerland), the force sensor (type 9211b, kistler), the peas (40 vs. 12, core tomorrow science co., ltd., harbin, china), signal generator (dg4162, rigol, beijing, china), power amplifier (pi, e-500), and so on. the control signals provided by the signal generator were amplified using power amplifier and were then sent to drive the piezo-electric stacks. to achieve accurate polishing, the force sensor was applied to detect abrasive-workpiece engagement and the charge amplifier was used to measure the polishing forces. step responses of the apparatus in (a) the x direction and (b) the y direction."
"excluding the bridge-type amplifier in limb m, the compliance model of the micro-motion stage is simulated as the stiffness model in figure 12 . the bridge-type amplifier will tolerate the force applied by the rest of the stage at the interface d. then, integral compliance of chains 7 and 8 with respect to point b can be derived as"
". it is assumed that the kinetic energies come from the rigid links between the flexure hinges [cit] . as shown in figure 15, translational motions are generated by links e, j, and k in limb m; rotational motions are generated by links g and f in that limb; and both translational and rotational motions are generated by links a, b, c, d, h, and i. moreover, the central point o of the motion stage moves along x and y axes without any rotation. the kinetic energy of limb m, consisting of the bridge-type mechanism and the lever model, can be expressed as"
"there are a number of available constraint satisfaction software that could be used for our purposes. [cit] max-sat competition 3 (a well-known competition that has been running for many years) on simulated data for the limited version of the problem (with no isa violations). the competition has both unweighted and weighted tracks. the three top-performing max-sat solvers in the weighted competition were maxhs, qmaxsat and maxino. in addition, two other available tools, cplex/ilog by ibm research and z3 by microsoft research have been benchmarked by the competition organizers. among these tools, cplex/ilog consistently performed the worst in max-sat; this was our experience as well. as cplex/ilog is also commercial and due to its poor performance, here we do not present results obtained by using this tool. comparisons of running times of phiscs-b implementations using each of z3, maxhs and maxino max-sat solvers are shown in table 1 . this table also contains results of the top-performing ilp (gurobi) solver that was used to run phiscs-i. table 1 : comparison of running times (in seconds) of phiscs implementations by the use of top-performing csp solvers from the max-sat competition. results of the top-performing ilp (gurobi) solver that was used to run phiscs-i are also included. runs were performed using the formulation which considers scs data as the input under infinite sites model. all results are obtained on a single core with a time limit of 24 hours. the number of instances solved within this limit is also reported. average running times over terminated instances are shown, rounded to the nearest integer, except for the cases where the average is lower than 10 seconds. sc: number of subclones, fn: false negative rate."
"t, where (p x, p y ) is a pair that specifies a position in cartesian coordinates and (v x, v y ) is the pair specifying velocity in the same coordinates. each target moves at nearly-constant velocity, with transition matrix and state process covariance matrix given respectively by"
"then n-level discrete shearlet transform (dst) is applied on the log-transformed noisy image to obtain the low pass and high pass subbands. for any arbitrary subband, the dst of the log-transformed image can be represented as given in eq. (3)."
"advanced ceramic materials have been found to be used in a new generation of space-to-ground optical information collection systems, so as to make the optical systems constantly have thermal stability and a high stiffness to weight ratio. the growth of optical camera key components that are made of a high-quality optical reflector is also encouraged [cit] . silicon carbide (sic) ceramic is a competitive representative material for establishing the ideal space for reflecting mirrors by the advantages of chemical inertness and corrosion resistance. however, there also remains difficulties with sic linked with the inherent properties of high brittleness and low fracture toughness [cit] . among the various machining methods for sic ceramic materials, polishing stands out as the most suitable way to produce ultra-precision surfaces. in particular, polishing is key in aeronautics for better mechanical performance [cit] ."
the n th order cumulants or log cumulants can be easily derived by taking the n th order derivative of eq. (13) and can be represented as given in eq. (14) .
"where e i c is defined as the compliance of flexure hinge i with respect to the point e in the coordinate system e-xy, i c is the compliance of flexure hinge i in the local coordinate system, and e i t is the transformation matrix from the coordinate system e-xy to the local coordinate system."
"in the current paper, a new linear-complexity multi-target filter is derived and implemented that propagates second-order factorial cumulants. it is shown that the filter is more robust than the phd filter while maintaining the same run-time. this is achieved by making a different approximation on the joint target-measurement process before applying bayes' rule."
"iv. algorithm specification in this section, the new linear-complexity filter and it assumptions are presented. the multi-target model and general assumptions are based on the work [cit] . the key difference is that correlations are maintained into the bivariate probability generating functional through the insight in bates and neyman [cit] that dependencies can be introduced in functionals that take an exponential form through the application of the laplace-stieltjes transform. the proofs are given in the appendix and the algorithm pseudo-code is described in algorithm 1."
"as the conventional roll-type polishing system performs polishing with line-contact material removal mechanism, the polishing contact area is much smaller than that of the rotary cmp system. as shown in figure 2, the vibration amplitudes in the x and y directions formed a square trajectory area under the same conditions. the magnitude of amplitudes affects only the size for the area, without affecting the uniformity of the lissajous figures."
"the average polysemy of words in the gathered dataset was 15.50 senses per word as compared to 2.34 in the induced sense inventory. this huge discrepancy in granularities lead to the fact that some test sentences cannot be correctly predicted by definition: some (mostly rare) babelnet senses simply have no corresponding sense in the induced inventory. to eliminate the influence of this idiosyncrasy, we kept only sentences that contain at least one common hypernym with all hypernyms of all induced senses. the statistics of the resulting dataset are presented in table 1, it is available in the project repository."
"modelling may be the suitable way to improve the use of the earp on several applications [cit] . before the kinematics analysis, it is necessary to simplify the bridge-type mechanism as an ideal multi-rigid body mechanism with ideal pivots, as shown in figure 5 . let la be the length of the arm and α be the angle between the connecting line of arm pivots and the horizontal line. considering the huge topological differences, each arm in the bridge-type mechanism is simulated as a link between two flexure hinges, and each flexure hinge is modelled as an elastic beam. as the structure is symmetrical, only one bridge arm of the mechanical model needs to be analyzed. through kinematic analysis, the static mechanics model of the flexure hinge ab can be simplified as a quarter mechanic model, as shown in figure 6 . considering the huge topological differences, each arm in the bridge-type mechanism is simulated as a link between two flexure hinges, and each flexure hinge is modelled as an elastic beam. as the structure is symmetrical, only one bridge arm of the mechanical model needs to be analyzed. through kinematic analysis, the static mechanics model of the flexure hinge ab can be simplified as a quarter mechanic model, as shown in figure 6 . as shown in figure 7, there are only two symmetric, horizontally balanced forces at point b. meanwhile, the arm ab has two moments, and the force at point a is provided by point b. the equation would appear when the force f is applied at point a ( figure 7 ) [cit] ."
"methods based on point processes [cit] and random finite sets [cit] have also attracted a lot of attention. in particular, the applications of methodology developed in the context of random finite sets have proved to be popular [cit] . specifically, methods based on propagation of first-order moment statistics of a point process, or intensity function, have been developed for addressing tracking problems, eg. [cit] ."
we generated simulated data sets and benchmarked our models against alternative tools using three distinct measures of accuracy as described below. we also implemented phiscs-b using several weighted max-sat non-commercial solvers and compared their running time performance with the best performing solver for phiscs-i showing a significant advantage of using the former. these results suggest that csp approach can be competitive time-efficient alternative strategy for solving some of the existing problems in tumor phylogenetics which are expressed in the form of integer-linear programs and typically solved by some of the available commercial ilp solvers. a detailed description of generating simulated data and running the tools is provided in the appendix.
"as expected, phiscs-b and phiscs-i produce highly similar results with the same value of the objective in all cases and slight differences in the resulting genotypes corrected matrix (denoted as y in section 2.1). these slight differences are very likely consequence of the existence of multiple optimal solutions in some of the cases. all results presented in this section are obtained by taking the average over 10 simulations we generated for each combination of parameters (see appendix a for details of simulations)."
"motivated by the above-mentioned problems, we target the development of an elliptical vibration-assisted roll-type polishing (earp) system considering the flexural mechanism to extend the capabilities of existing mechanisms for precision polishing. this system has the following advantages: it works in the non-resonant mode and can deliver large working stroke; benefiting from the principle of linear contact, ultrathin substrates without brittle fracture could be realized; the flexible system supports frequency adjustment and suits the continuous production line in new electronics production; high polishing accuracy can be realized through the integration of feed and machining modules."
"the workpiece surface morphology with non-vibration and vibration polishing are given in figure 29 . because of the unstable control of polishing slurry, some marks can be observed at the workpiece with both methods. figure 29a shows the surface roughness was improved from 95 nm sa to 80 nm sa and 504 nm sz reduced with form deviation by non-vibration polishing. the polishing scratches can apparently be observed in the area. as shown in figure 29b, the scratches were increased compared with the former as a result of more abrasives involved in polishing (44 nm sa), and 856 nm sz smaller than non-vibration by applying vibration. the performance of the"
"where, f sx (s x ) is the priori distribution of noise-free coefficients, assumed here nakagami distribution and f sn (.) is the probability density function of noise which is assumed to be gaussian. now substituting the value of f sx (s x ) (after taking logarithm) in eq. (35) to yield eq. (28)s"
"the output stiffness, input stiffness, amplification ratio, and natural frequency of the xy micro-motion stage were validated without damping on the finite element software abaqus/explicit (abaqus inc., palo alto, ca, usa). the material used in the simulations was 6061 al, for which the modulus of elasticity was 69,000 mpa, possion's ratio was 0.3, and density was 2700 kg/m 3 . further, a 3.9 mm tetrahedral grid was chosen in mesh. the boundary conditions were set up after the geometric model meshing step and the input ends of the proposed micro-motion stage were applied to the load. the parameters of the model were described in table 1 . sensitivity and convergence tests were performed at points b and c, respectively, as shown in figure 15 . during the earp process, the hinges suffer from reciprocating tensile deformation, resulting in internal stress. accordingly, internal stress on the compliance structure was tested in this paper. specifically, the input load and output motion of the xy micro-motion stage were obtained as a given displacement on the input end of bridge-type mechanism; then, the input stiffness and amplification ratio of the flexure-based stage were determined based on the input load and output motion (figure 17) . furthermore, the output stiffness can be evaluated by exerting an external force onto the stage. figure 16 . convergence process of the grey wolves optimization (gwo)."
"where c e i is defined as the compliance of flexure hinge i with respect to the point e in the coordinate system e-xy, c i is the compliance of flexure hinge i in the local coordinate system, and t e i is the transformation matrix from the coordinate system e-xy to the local coordinate system."
"this paper proposes an alternative to the nnjm, the bnnjm, which learns a binary classifier that takes both the context and target words as input and combines all useful information in the hidden layers. we also present a novel noise distribution based on translation probabilities to train the bn-njm efficiently. with the improved noise sampling method, the bnnjm can achieve comparable performance with the nnjm and even improve the translation results over the nnjm on chineseto-english and french-to-english translations."
"this movement is continued until the parasite is touched by the point where the two tongues 21 contact each other . t1: the mobile continues to the parasite from the two tongue 21 contacts the points of contact with each other . t2: this movement is continued until the parasite by two tongue 21 contact points of contact with each other . task is lower than ce and je tasks, indicating that learning is harder for the fe task than ce and je tasks. the validation perplexities of the nnjm with upd for ce, je and fe tasks are 4.03, 3.49 and 8.37. despite these difficult learning circumstances and lack of large gains for the nnjm, the bnnjm improves translations significantly for the fe task, suggesting that the bnnjm is more robust to difficult translation tasks that are hard for the nnjm. table 3 gives chinese-to-english translation examples to demonstrate how the bnnjm (with tpd) helps to improve translations over the nnjm (with tpd). in this case, the bnnjm helps to translate the phrase \"该 移 动 持 续 到\" better. table 4 gives translation scores for these two translations calculated by the nnjm and the bn-njm. context words are used for predictions but not shown in the table."
"we evaluated the effectiveness of the proposed approach for chinese-to-english (ce), japanese-toenglish (je) and french-to-english (fe) translation tasks. the datasets officially provided for the patent machine translation task at ntcir-9 [cit] were used for the ce and je tasks. the development and test sets were both provided for the ce task while only the test set was provided for the je task. therefore, we used the sentences from the ntcir-8 je test set as the development set. word segmentation was done by baseseg [cit] for chinese and mecab 4 for japanese. [cit] translation task. the training sets for ce, je and fe tasks contain 1m, 3m and 2m sentence pairs, respectively."
") where align (s a i, t i ) is how many times t i is aligned to s a i in the parallel corpus."
"finally, we examine the translation results to explore why the bnnjm with tpd did not outperform the nnjm with tpd for the je translation task, as it did for the other translation tasks. we found that using the bnnjm instead of the nnjm on the je task did improve translation quality significantly for infrequent words, but not for frequent words."
"note that t i could be unaligned, in which case we assume that it is aligned to a special null word. noise for unaligned words is sampled according to the tpd of the null word. if several target/source words are aligned to one source/target word, we choose to combine these target/source words as a new target/source word. 3"
"where occur (w ij ) is how many times w ij occurs in the whole reference set. note p c will not be 1 even in the case of completely accurate translations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word translations contribute less to p c . table 5 shows p g and p c for different translation tasks. it can be seen that the bnnjm improves infrequent word translation quality similarly for all translation tasks, but improves general translation quality less for the je task than the other translation tasks. we conjecture that the reason why the bnnjm is less useful for frequent word translations on the je task is the fact that the je parallel corpus has less accurate function word alignments than other language pairs, as the grammatical features of japanese and english are quite different. 8 wrong function word alignments will make noise sampling less effective and therefore lower the bnnjm performance for function word translations. although wrong word alignments will also make noise sampling less effective for the nnjm, the bnnjm only uses one noise sample for each positive example, so wrong word alignments affect the bnnjm more than the nnjm."
"as can be seen, the bnnjm prefers t 2 while the nnjm prefers t 1 . among these predictions, the nnjm and the bnnjm predict the translation for \"到\" most differently. the nnjm clearly predicts that in this case \"到\" should be translated into \"to\" more than \"until\", likely because this example rarely occurs in the training corpus. however, the bnnjm prefers \"until\" more than \"to\", which demonstrates the bnnjm's robustness to less frequent examples."
"we can see that using tpd instead of upd as a noise distribution for the nnjm trained by nce can speed up the training process significantly, with a small improvement in performance. but for the bnnjm, using different noise distributions affects translation performance significantly. the bnnjm with upd does not improve over the baseline system, likely due to the small number of noise samples used in training the bnnjm, while the bnnjm with tpd achieves good performance, even better than the nnjm with tpd on the chinese-to-english and french-to-english translation tasks."
"the bnnjm learns a simple binary classifier, given the context and target words, therefore it can be trained by mle very efficiently. \"incorrect\" target words for the bnnjm can be generated in the same way as nce generates noise for the nnjm. we present a novel noise distribution based on translation probabilities to train the nnjm and the bnnjm efficiently."
"first, we describe how we estimate translation quality for infrequent words. suppose we have a test set s, a reference set r and a translation set t with i sentences,"
"as a binary classifier, the gradient for a single example in the bnnjm can be calculated efficiently by mle without it being necessary to calculate the softmax over the full vocabulary. on the other hand, we need to create \"positive\" and \"negative\" examples for the classifier. positive examples can be extracted directly from the word-aligned parallel corpus as s"
"nce can be used to train nnlm-style models [cit] to reduce training times. nce creates a noise distribution q (t i ), selects k noise samples t i1, ..., t ik for each t i and introduces a random variable v which is 1 for training examples and 0 for noise samples,"
"for each translation task, a recent version of moses hpb decoder [cit] with the training scripts was used as the baseline (base). we used the default parameters for moses, and a 5-gram language model was trained on the target side of the training corpus using the irstlm toolkit 5 with improved kneser-ney smoothing. feature weights were tuned by mert [cit] ."
"where c stands for source and target context words as in equation 1. the nnjm can be trained on a word-aligned parallel corpus using standard mle, but the cost of normalizing over the entire vocabulary to calculate the denominator in equation 2 is quite large. [cit] 's self-normalization technique can avoid normalization cost during decoding, but not during training."
"in view of the fact that evaluating projects is a problem, yet often it is a difficult task. it is complicated, because there is usually more than one dimension for measuring the impact of each project and especially when there is more than one decision maker. multi-criteria decision-making methods; helps to make better decisions in creating viable options where there are conflicting objectives such as decision makers of uncertainty, complexity."
"in this study; applying the ahp, multi-criteria decision-making methods, it has tried to determine the best project. for this purpose, four projects were evaluated considering four alternative criteria. the criteria are defined as, which have importance in the evaluation of investment projects \"net present value, benefit-cost analysis, rate of return and payback period\". each project is defined according to whether the individual evaluation criteria, and is showed the importance in numerical values. then, the availability of valuation of projects was carried out scores of each project. finally; the best project was project 1 and it was followed by project 4, project 3 and project 2."
"with the nodes that control agent i has in its subnetwork objective terms are associated. the objective function terms associated with each node can depend on the variables associated with that node and its neighboring nodes. as before, the objective terms involving only internal variables require no special attention. the objective terms involving both internal and external variables can be dealt with by fixing the external variables, as is also done for control of touching subnetworks. however, the common variables appearing in control of overlapping subnetworks do require special attention."
"like squall [cit], mgcrab supports the fine-grained range partitions [cit] in the storage as well as transactions with range queries. in addition, it can be extended to support range queries more efficiently. suppose there is a query that reads a large range of data. in the crabbing phase, the executor on the source node will push the entire data in the range to the destination node when processing the enclosing transaction. this foreground push, with a large data volume, may prolong the transaction on the source node and result in performance degradation (especially in the beginning of the crabbing phase). one can minimize this problem by letting the source and destination nodes switch to a master-slave mode for this particular type of transactions. unlike in the normal case (which we called the multi-master mode) where the destination node executes the transaction actively and writes its own results, the destination node does not execute the transaction in the master-slave mode. instead, it waits for the results pushed from the source and then writes the received data it owns into the storage. although this results in additional data being transmitted, the advantage is that it avoids pushing large data in the foreground. the consistency is not sacrificed, and the destination node can write the same data as if it was in the multi-master mode. note that the switching needs to be deterministic on a per transaction basis. to ensure this, we let the source and destination nodes deterministically enter the master-slave mode if the read-set of a transaction is larger than a data chunk pushed in the background. also note that the data locations are still always certain-even if a range is partially migrated, the locations of the corresponding data chunks are tracked in mgcrab. different from squall where the source node needs to abort a transaction if the transaction accesses a range that is partially migrated, the source node can always process the transaction in mgcrab due to the \"crabbing.\""
"the normalized matrix is obtained by dividing the total individual corresponding column to each column value. from-normalized matrix motion; the average value of each row is taken. the obtained values are percentages by weight for each criterion value (palaz & kovancı, 2008) . the other pace is to classify numerical values to the dimensions, such as poor, fair, good, very good and excellent. we can make a change on these values if necessary because it can't be appropriate for all situations. to these qualitative dimensions, we give a numerical value between 0 and 1. it is up to us what value is given to each dimension. as an example: poor-value 0,0; fair-value 0,1; good-value 0,3; very good-value 0,6; excellent-value 1,0 [cit] . we can find the total grade of the project with the following formula;"
"we also conduct the experiments to verify the efficiency of our design and compare mgcrab with squall in more aspects such as the ratio of read-write transactions, the size of read-set, the ratio of distributed transactions, and the degree of load-unbalanced under the ycsb workloads. due to the space limitation, we can not present all the experiments in the paper. please see the appendix of our supplementary materials [cit] for more experiments."
"for multi-agent control of overlapping subnetworks an approach has to be found to deal with the common nodes. since the common nodes are considered by several control agents, the constraints associated with these common nodes appear in the subnetwork models of multiple control agents. even though the control agents have the same objective with respect to these nodes, combined with the objective for their internal nodes, conflicting values for the variables of the common nodes can be the result. below we discuss how to extend the scheme of the previous section for control of overlapping subnetworks. again, for the sake of simplicity of explanation we focus on two control agents: control agent i with neighboring agent j."
"where the values of u b,svc,min,m and u b,svc,max,m are determined by the size of the device [cit] . the constraints of an svc at bus m are assigned to the node m."
"squall [cit] . this approach extends zephyr by allowing an incoming transaction to run on the source node if the required data are 3 squall knows the location of data required by a transaction by splitting the records to migrate into ranges and tracking the migration status of each range. a transaction can run on the source node only if all the relevant ranges are not being migrated or have not been migrated. compared to zephyr, squall improves the system performance in the beginning of the migration process by letting some transactions run on the source node without being delayed by data pulls. however, the improvement is limited by the fact that any record that has started migrating in a range will prevent all transactions accessing data in the range from running on the source node. so, as soon as the data migration begins, the number of transactions that can complete on the source node decreases rapidly, and the system can still suffer from the performance degradation at the beginning of the migration period, as shown in figure 2 (see section 5 for the settings and detailed description). furthermore, on h-store, the router may route a transaction to the source node where the required ranges are migrating. in this case, the transaction needs to abort and restart on the destination node. table 1 summarizes the performance characteristics of the above data migration techniques. as we can see, none of the current approaches is ideal for a deterministic database system. there is a need for a new design."
"a tcsc is a facts device that can control the active power flowing over a line [cit] . it can change the line reactance z x,line,mn . the tcsc is therefore considered as a variable reactance u x,tcsc,mn connected in series with the line. if a tcsc is connected in series with a transmission line between buses m and n, the total reactance z x,line,mn of the line including the tcsc is given by:"
"in this paper, the author has discussed a new optimization algorithm has been presented recently known as \"jaya algorithm\", has shown the victorious nature, does not require any algorithm specific parameters and delivers the optimized result in a time effective manner. previous research conducted has compared the jaya algorithm with several other algorithms shows the computational capabilities, the discussed algorithm holds. it is not the aim of this paper is to prove jaya algorithm is the \"best\" algorithm over the algorithms discussed in the literatures. in fact, no such the \"best\" algorithm exists; it all depends on the problem type and the environment where the problem is solved. rather, the aim is to increase the awareness of the jaya algorithm among the researchers working for optimization."
"mgcrab guarantees the safety and liveness [cit] . theorem 3.1 (safety). mgcrab is safe, meaning that it meets the following conditions: (a) transaction correctness: transactions run with the correct acid guarantees during migration; and (b) migration consistency: a failure during migration does not leave the data and system state inconsistent."
"further extensions and future work. this paper opens up some interesting research directions. first, mgcrab is a live migration technique for a single source, single destination migration plan. one may extend the system controller (e.g., e-store [cit] ) such that it best utilizes mgcrab when generating a multi-source, multidestination migration plan (i.e., a live reconfiguration plan). furthermore, although mgcrab is proposed for deterministic database systems, one may integrate it into a non-deterministic system by requiring the system to enter a \"deterministic mode\" temporally during the migration. this helps avoid the complexity and drawbacks of non-deterministic migration techniques (e.g., [cit] ) discussed in section 1. the cost is that, during migration, the system will not be able to accept ad-hoc queries because all transactions must be totally ordered first. however, recent studies [cit] show that this cost may be acceptable to many oltp applications as the clients (e.g., web/application servers) are usually optimized for performance and issue queries through stored procedures. the above are matters of our future inquiry."
"there is no question about the exploration capabilities, these algorithms holds, has been successfully applied to solve several different types of search and optimization problems. the successes of these algorithms are greatly based on the parameters they used basically guides the searching and are majorly contributing in the exploration and exploitation of the search space. for example, the ga uses crossover operator is mainly an exploration operator, whilst the mutation operator has been considered as exploitation operator. these ga operators work based on the specific probabilities defined in the beginning of the search. it has been observed that if the probabilities of the crossover and mutation operator has been tuned appropriately the ga's search get trapped in a local optimum. this situation is referred as premature convergence, a situation when the diversity of the population is decreased leads to unavoidable local optimum convergence. [cit] has discussed a lot about this and presented a comprehensive review of the approaches have been proposed to address the premature convergence. like ga's crossover and mutation there are other factors also presents are population size, maximum number of generations, elite size, inertia weight (in pso), acceleration rate (in pso), onlooker bees, employed bees, scout bees (in abc), harmony memory (in hs), number of improvisation (in hs), pitch adjusting rate (in hs). these parameters play a significant role in the success of the algorithms: ga, es, ep, gp, de, bfo, aia, pso, hs, aco, abc and others. the controlling and proper tuning of the algorithm specific parameter is a crucial factor, shows a greater impact in the success in terms of finding the global optimum."
"two-phase background pushes. one naive way to migrate the cold data is to break them into fixed-sized chunks and push the chunks one-by-one iteratively in the background. to preserve the determinism (and to ensure the correctness), one should push a data chunk synchronously-the migration controller on the source node creates a bgpush transaction request and sends it to the sequencers in the system so the transaction will be processed by both the source and destination partitions at the same logical time (following the total order). on the source node, the transaction reads and sends out the chunk; conversely, on the destination node the transaction waits for the chunk and then writes it into the storage. before completing the transaction, both the source and destination nodes mark on metadata that the chunk has been migrated. by pushing a chunk synchronously, we can ensure that the locations of migrating data are always certain (after every transaction completes). so, there is no need to deal with the uncertainty of data locations, which may result in transaction aborts [cit] ."
in the following we first discuss an approach that can be used in the case that the subnetworks are touching. then we extend this approach to be able to deal with overlapping subnetworks. for the sake of simplicity we assume below that there are no nodes that do not belong to any subnetwork.
"we employ two oltp workloads, the ycsb and tpc-c, in our evaluation. transaction requests are submitted from 600 and 180 client threads running on 3 nodes in ycsb and tpc-c workloads respectively. each client submits transaction requests to a partition node in a closed loop, i.e., it blocks after submitting a request until the result is returned. in mgcrab, the client can issue the next transaction request once getting the result from the winner node (either the source or destination node) during the migration. in each experiment trial, we warm up the dbms for 150 seconds before collecting the measurements. latencies are measured as the duration from the time a client submits a transaction request to when it receives the result. ycsb [cit] : the yahoo! cloud serving benchmark (ycsb) is a collection of workloads that simulate a large-scale online service. we use a ycsb database consisting of a single table with 1.5 million records (half a million records in each partition). each tuple has 10 columns including the primary key. each column of a tuple contains a randomly generated string of 100 bytes. the workload consists of 85% read-only transactions and 15% read-write transactions. each read-only transaction reads two records in the same partition. and each read-write transaction reads and updates a single record and then inserts another new record to the same partition. the keys of the records are selected from the zipfian distribution specified by ycsb."
"as the focus lies on improving the steady-state network security, the power network is modeled using equations describing the steady-state characteristics of the power network. as we will see, the aspects of the steady-state security that we are interested in can be determined from the voltage magnitude and voltage angle at each of the 57 (physical) buses in the network. we therefore define 57 nodes to model the network, and assign to each node m the voltage magnitude z v,m per unit (p.u.) and the voltage angle z θ,m (degrees) as variables. in order to determine the values for these variables under different disturbance variables and actuator values, models for the components and their influence on the voltage magnitude and angle are defined. we model the transmission lines, the generators, the loads, and the facts devices."
"calvin is designed for workloads that contain transactions only invoked as predefined stored procedures. as shown in figure 1, data are fully replicated across multiple data centers separated geographically for availability. a data center has a cluster of machines, each of which holds only one partition of data to avoid hot spots. when transaction requests arrive, the sequencers in the system communicate with each other to decide a single total order (i.e., an order that all machines agree on) of these requests, and forward the requests to all schedulers following the total order. the scheduler on each machine, after receiving a request, analyzes the request and decides whether the transaction will touch any data stored locally, and if so, forwards the request to the executor residing on the same machine. then, the executor processes the transaction while ensuring that the results will be the same as those of processing the transaction (and all its preceding transactions) following the total order. in other words, the database state on each node at any time is deterministic to the total ordering. this enables lightweight distributed transactions [cit] across partitions (at the serializable isolation level) and low-cost strong consistency between replicas. the latter comes with the fact that two replicas having identical initial data will produce the exactly same results for every transaction (due to the determinism), and therefore need no agreement protocol (e.g, 2pc) to commit a transaction. note that the delay incurred by the total-ordering protocol (e.g., paxos [cit] or zab [cit] ) for ordering a transaction does not count into the contention footprint of the transaction in concurrency control. therefore, such delay has no impact on the system throughput. also, note that this architecture supports any storage engine (either memoryor disk-based) with the crud interface."
"where d p,load,m and d q,load,m are the given active and reactive power consumption, respectively, of the load connected to bus m. for simplicity, only one load can be connected to a bus. multiple loads can easily be aggregated to obtain a single load. the constraints of the loads at bus m are assigned to node m."
"the rest of the paper is organized as follows: section ii has reported the jaya algorithm and its working, whilst the importance of the jaya algorithm over the other algorithms has been discussed in section iii. the concluding remarks and future assessment have been drawn in section iv."
"next, we evaluate the performance of migration approaches during cluster consolidation using the ycsb workload. we make all nodes underloaded so that we could shut down a node in order to save resources. the source, destination, and the other nodes have one partition initially and our goal is to move the partition on the source node to the destination node (and to shutdown the source node later). the other node has 50 client threads submitting transaction request to it, while the source and the destination node only has 10 client threads each. the results are shown in figure 8 . we can see that the performance is unchanged before and after the migration in all figures because the system is not fully loaded. however, the consolidation allows the system to use fewer machines."
"we have discussed how each control agent formulates its prediction model and objective function. the scheme that we propose for multi-agent control for overlapping subnetworks consists of the scheme proposed in section 3 for touching subnetworks, with the following changes:"
"we now discuss some typical data migration techniques. stop-and-copy. this is arguably the simplest and most heavyhanded approach to migrate data. the system stops accepting transaction requests on the source node, moves the data from source to destination, and then re-start serving requests on the destination node. this technique incurs a long downtime lasting the entire migration period. it may also slow down the transactions and throughput on the destination node after service restarts, as the node may require some time to warm up the cache. however, in spite of its inefficiency, this technique is currently the only choice in many commercial database systems (e.g, mysql) currently. next, we review some live migration techniques."
", and c int+com+ext i,com the control agent takes for the external variables values that it has received from neighboring agent j. when control agent i has solved its optimization problem, it sends the values of the internal and the common variables of the constraints of these specialized constraint types to neighboring agents."
"several literatures exist reports the advocacy of controlling the algorithm specific parameters and how these parameters help in balancing the exploration and exploitation. there exist several approaches that have been attempted to balance the exploration and exploitation employing fine tuning are: trial and error (consume more time and is performed in ad-hoc manner), following general guidelines (use recommended parameter setting from the existing literatures), use parameter less algorithms (robust, but less efficient), use experience from the similar applications (not fit in a situation, where experience do not exist), use of mathematical models (sometimes simple but most of the times have been found difficult), etc. additively, some literatures have reported the importance of orthogonal array approach, taguchi approach and full factorial approach in identifying the tuned value of the parameters. the above discussion shows a clear indication and importance of the tuning and controlling of the parameters, which is undoubtedly a tedious and time consuming process. the lack of knowledge in conducting and preparing the correct computational experimental setup makes the whole process even more complex."
"if no generator is connected to bus m, then z p,gen,m and z q,gen,m are zero. if no load is connected to bus m, then z p,load,m and z q,load,m are zero. if no svc is connected to bus m, then z q,svc,m is zero. the constraints resulting from kirchhoff's laws for bus m are assigned to node m."
"the results for the tcsc settings and the difference between the voltage magnitudes and angles for some common buses over the iterations are given in figures 8(a) and 8(b), respectively. the control agent of subnetwork 1 sets the tcsc to its upper limit at the first few iterations. but after some additional iterations, the values that the control agents choose converge to their final values, which are again equal to the values obtained by a centralized control agent."
"the local objective function for control agent i consists of objective function terms that are associated with the nodes in its subnetwork. objective terms associated with internal nodes that are only connected to internal nodes are simply included in the local objective function. however, objective terms associated with internal nodes that are also connected to external nodes cause problems for the same reason as constraints associated with such nodes. coordination on the values of these variables is achieved by obtaining the desired values for the external variables from neighboring agents. table 3 summarizes how the different localized objective term types that control agent i are considered, and how the agent deals with these types, when formulating its optimization problem."
the result is a control scheme that can be used by higher-layer control agents that control subnetworks that are overlapping. the control agents hereby share the responsibility for the common variables. in the next section we apply this scheme on an optimal flow control problem in power networks.
"the parameters of the ieee 57-bus base network can be obtained from the power systems test case archive [cit] . line limits on the apparent power flows have been assigned to all transmission lines in such a way that no lines are overloaded. in order to find an interesting and meaningful situation for facts control, the grid was adapted by placing an additional generator at bus 30 leading to increased power flows in the center of the grid. the parameters of this generator are as follows: [cit] .the limits on the apparent power flows are set as listed in table 7 ."
"modern oltp applications usually have changing workloads or access patterns [cit] . for example, users of a database around the world may become active at different times due to timezone differences and may access their data differently; on the other hand, a tenant in a multi-tenant database may suddenly become very hot as the owner application gains flash crowds originating from viral popularity. elastic load balancing, which moves data around the provisioned machines and increases/reduces the provisioning in response to changes of workload to maintain the system performance, becomes an important design goal of a deterministic database system [cit] . elasticity also helps minimize the operation cost when the system is deployed on a cloud platform that charges per use."
"cr is obtained by comparing a set of the given number below by ci. if the cr is smaller than 0.10, the decision-makers pairwise comparisons are relatively consistent. if the cr is bigger than 0.10, the decision-maker should seriously consider re-evaluating his/her pair-wise comparisons. random index numbers are given in table 3 ."
"our experiments measure how well the above approaches are able to fulfill a single source, single destination migration plan in different scenarios, including scaling out and cluster consolidation. we use a cluster with 3 nodes. in the scaling out scenario, the source node, destination node, and the other node hold two, zero and one data partitions initially and the goal is to migrate one data partition from the source to the empty destination. in the consolidation scenario, each node holds one partition initially and the goal is to migrate the partition held by the source node to the destination."
"a shortcoming of this method is that it requires that the subnetworks are touching, since it assumes that each node in the network model is assigned to only one of the subnetworks. however, in the case of control of overlapping subnetworks, some of the nodes are included in more than one subnetwork and the identification of internal and external nodes of a control agent is not straightforward any more. therefore, the method is not directly applicable to overlapping subnetworks. in the following section we extend the method discussed above to control of overlapping subnetworks."
"stop-and-copy: when a migration starts, the migration controller on the source node issues a distributed transaction that locks both the source and destination nodes and then performs data migration. all conflicting transactions are blocked until this transaction completes."
"mgcrab offers the following advantages as compared to the previous work. first, it allows the transactions to run on the source node to avoid a performance drop at the beginning of the migration period. when the destination node catches up later on, it hands over the winner node seamlessly on a per transaction basis (whenever the transaction on destination completes more quickly) without incurring downtime or aborted transactions. second, we let the destination node \"compute\" the updates of the migrated data (by executing the transactions) instead of waiting for the updates shipped from the source. this saves the shipping cost (e.g., latency) and avoids the termination problem as in the source-based approaches. more importantly, mgcrab employs the two-phase background pushes that migrate chunks without blocking any transaction on the source node. this technique significantly improves the stability of overall system performance during the migration. the following summarizes our contribution:"
"in the second scenario, tcscs are installed on lines 72 and 22. since tcscs are mainly used to influence active power flows and to resolve congestion, the line limits are chosen such that lines 7 and 60 are overloaded if the facts devices are not being used."
"after the destination executor processes t, it needs no more pushes of b and c and can keep their states in sync with the source partition at any logical time by executing the later transactions deterministically following the total order. note that the source executor does not delete b and c until in the termination phase (to be discussed later). this allows the source node to continuously serve later transactions during the migration and hide the migration cost when the destination node is busy catching up with the source. we call the above pushes generated during the transaction processing the foreground pushes. these pushes migrate hot/active data that are likely to be accessed again (due to the locality of access pattern) as early as possible. nevertheless, the cold data may be left behind."
"we consider the control of power networks by multiple control agents that operate in a higher control layer. at this layer, we are interested in controlling the very slow dynamics or the long-term behavior of the network, and therefore we can assume that dynamics of the lower control layers and physical network can be represented or approximated by instantaneous, steady-state characteristics."
"the design goal of existing live migration techniques is to let every transaction that arrives during the migration period run on the most appropriate machine such that the clients perceive minimal service interruption. however, as evidence with squall [cit], deciding a better machine to execute a transaction requires precise information about the location of migrating data, and this information is hard to track in practice. mgcrab takes an opposite approach-instead of relying on data locations to decide the better one, it runs every incoming transaction on both the source and destination machines. mgcrab's live migration proceeds in three phases: the initial, termination, and crabbing phases, as shown in figure 3 . transactions arriving at these three phases are processed on the source node, destination node, and both respectively."
"in figure 9 the line loadings of lines 7 and 60, i.e., the lines which are overloaded without facts devices in operation, are shown. line 7 is immediately brought below its limit whereas for line 60, the loading approaches 100% in the course of the optimization process."
"however, the downside is that a bgpush transaction blocks all conflicting transactions on both the source and destination nodes. even worse, the bgpush transaction is usually longer than normal user transactions due to the relatively large chunk size. this leads to the clogging [cit] where the blocked transactions continue to block incoming conflicting transactions like a chain reaction and finally results in a drastic drop of system performance. the same issue occurs in squall [cit] as well."
"albatross [cit] . this approach allows the incoming transactions to run on the source node after the migration starts. in addition to migrating data, the source node iteratively ships to the destination node the updates made by the transactions. when the data have been migrated and there are relatively few (or stable) updates, the source node performs a short stop-and-copy migration and then hands over the transactions to the destination. although being reduced, the downtime still exists and may not be acceptable to systems with slas. furthermore, to reduce the migration time, one may let the source node perform the stop-and-copy earlier, resulting in a longer downtime as there are more pending data/updates that need to be shipped."
"we first evaluate how well the live migration approaches perform in the scaling out scenario, where the source node is overloaded and the goal is to transfer half of its workload to the destination node via data migration."
"just as in the scaling out scenario, the stop-and-copy blocks all transactions and makes the system temporarily available. in squall, the performance drops again when the reactive and asynchronous migration begins. the reason is similar to the one in the scaling out scenario-the background pushes block many transactions on the source node. the problem is more severe here because the destination node is underloaded and pulls fewer data from the source node, leaving more data cold and be migrated through background pushes, which in turn blocks more transactions and results in a severe drop in throughput on the source node. on the other hand, the two-phase background pushes in mgcrab prevent foreground transactions from being blocked on the source and allows the source node to smoothly hand over the workload to the destination node."
"assume that the management wants to choose the best project amongst all proposed projects. in this study, four criteria are chosen for evaluating the projects. these are net present value (npv), benefit-cost analysis (bc), rate of return (ror) and payback period (pp) and there are four projects with different values depending on the criteria. first, hierarchy structure should be made. as it is seen in table 4, evaluation matrix shows the hierarchy structure. then, normalized decision matrix is constructed. the results are as shown in table 5 . each project must be analyzed individually and must be decided what level of fulfillment each selected criteria is. table 6 shows each project scores and also ranking. finally project 1 got the highest score. obviously, it is the best alternative for the organization. the next step is to calculate the cr. table 7 shows the lambda-max value and table 8 shows the value of cr. as it is seen in table 8 the cr is 0,016955 and the value of the cr is smaller than 0,10 so the result of this study is consistent."
"slow destination. this may happen, for example, in consolidation scenarios where data are migrated to a machine with an existing workload in order to shutdown the source. if its aggregated workload is too high, the destination node may never catch up during the entire crabbing phase, resulting in the waste of computing resources. to help the destination node catch up, mgcrab introduces an optional phase, called the catching-up phase, between the initial and crabbing phases. in this phase, the source and destination nodes execute every transaction using the master-slave mode introduced in section 4.3 so that the destination node can use its full power to catch up with the source before entering the crabbing phase. another way to prevent a slow destination is to let the system controller (see figure 1 ) generate better migration plans based on the statistics and sla [cit] ."
"below we formulate the steady-state models used to describe the network behavior, we assign the constraints to nodes, we set up the objective terms associated with the nodes, we discuss the way in which the subnetworks can be determined using the influence-based approach, and we illustrate the workings of the proposed approach."
"an svc is a facts device that is shunt-connected to a bus m and that injects or absorbs reactive power z q,svc,m to control the voltage z v,m at that bus [cit] . the svc connected to bus m accepts as control input the effective susceptance u b,svc,m . the injected reactive power z q,svc,m of the svc is:"
"recall that, during the crabbing phase, the migrating data are not deleted on the source node after they are pushed to the destination. when processing the terminate transaction, the source needs to schedule a process that deletes the data. however, this process can run in the background after the terminate transaction completes, resulting in little impact on performance. alternatively, the source node can choose to keep the data if it foresees upcoming consolidation tasks. when data are moving back, the source node can locally replay the request logs (see appendix of the supplementary materials [cit] ) to make the data up-to-date faster. we leave this alternative approach to the future work."
"the source and destination nodes need to enter the caught-up phase at the same logical time to preserve the determinism. mgcrab accomplishes this by issuing a special transaction request, which is totally ordered with other user transactions. figure 3 shows the switching from the crabbing phase to the caught-up phase. after the switching, all later user transactions that touch the migrating records will run on the destination node. in practice, this switching can happen as early as 1) the amount of data pushed in the foreground converges, and 2) there is no pending tagged data chunk in the background (see section 4.3)."
"as we can see, squall and mgcrab consume resources very differently on the source and the other node during the migration. in squall, we see a clear drop in resource utilization after the reactive and asynchronous migration starts. there are two major reasons for this. first, the reactive migration lets the destination node pull migrating data from the source, but since only one machine can own the data, this prolongs the distributed transactions that originally lie between the source and the other node. the network latency in these distributed transactions counts into the contention footprint, thereby slowing down the entire system, as evidenced by the drop of resource utilization on the other node. second, the reactive pull and asynchronous push transactions may conflict with the user transactions. this seriously impacts the performance of the source node on serving clients and results in the drop of resource utilization on the source node."
"the destination-based approach prorea [cit] extends zaphyr [cit] by proactively migrating hot records to the destination at the start of migration; it also focus on snapshot isolation. another work, rocksteady [cit], is also a destination-based approach, which transfers the ownership of migrating records at the beginning of the migration and processing incoming client requests at the destination node. this approach reduces the time of the migration by issuing multiple pull requests and taking the advantages of the thread model of ramcloud [cit] . however, unlike mgcrab, the work focuses on key-value stores, and it does not consider how to ensure consistency and isolation while processing transactions."
"due to space limitations, the formal proof is left to the appendix of the supplementary materials [cit] . note that mgcrab does not require the redo logs and a complex aries-like recovery algorithm [cit] to bring a failed machine back to the latest state. therefore, it works nicely with exisitng deterministic systems [cit] that replay the request logs during the recovery. furthermore, mgcrab supports fast checkpointing such as the calc [cit] and zig-zag [cit] algorithms. 4 for readers familiar with calvin: this read-only transaction (t 5) is not the same as the low-isolation snapshot-read transactions [cit], as the former needs to be totally ordered."
"in the ahp method; the preference for solving problems in a hierarchical structure means the separation of the various levels of problems. the hierarchical structure of the creation process is called modeling [cit] . after hierarchy is created, the relative importance of the criteria level against each other is calculated. decider; based on scale of 1-9, decide the degree of importance of the criteria. in table 1, 1-9 scale that is utilized, are described in detail [cit] ."
"the prediction model of control agent i consists of the constraints associated with all its internal nodes. in order to make predictions, control agent i has to know accurate values for all variables involved in the constraints of these nodes. the internal nodes that do not have external neighboring nodes do not require special attention, since the variables involved in the constraints of these internal nodes are of localized constraint type c int i,int and thus only involve variables of the subnetwork of control agent i. however, the internal nodes that are connected to external nodes do require special attention, since the constraints associated with these internal nodes can be of local-"
"tpc-c. the source, destination, and the other nodes contain 11, 0, and 10 warehouses respectively. one warehouse on the source node is hot and will be migrated to the destination node. each cold warehouse has 6 client threads submitting requests to it, and the hot warehouse has 60 client threads acting with it. figures 7(b)(d) show how the performance changes during the migration period. it is not surprising that the stop-and-copy gives similar results as in the last experiment. however, squall renders a significant perfor- mance drop after the migration starts, and the major reason differs from the one in ycsb. generally, tpc-c transactions are much longer than those in ycsb and have about a 10% chance to interact with remote partitions. in squall, a data object is owned by a single machine. the reactive migration, which pulls data from the source node, prolongs distributed transactions by involving the destination node in these transactions. this slows down not only the source and destination nodes but also the other nodes (see figure 7(b) ), thereby resulting in the drop of entire system throughput and increased latency. in contrast, the distributed transactions in mgcrab are not prolonged by the foreground pushes, because it is the winner node that responds to the clients and interacts with the other node in the distributed transactions (see section 4.2). so, the latency and system throughput become much more stable. another reason resulting in the performance drop in squall is the asynchronous pushes. this is because each asynchronous push is handled by a (long) distributed transaction that can easily block other transactions. on the other hand, the two-phase background pushes in mgcrab does not block normal transactions and has little impact on the system throughput. we can see that after the background pushes starts (the first vertical line in mgcrab in figure 7 (b)), the throughput remains stable since both the phase 1 and phase 2 background push transactions can be processed very efficiently on the source node. note that we choose to start the caughtup phase at the time when the data pushed in the foreground converges."
"in mgcrab, the winner nodes of transactions that arrive during the crabbing phase may change over time. in the very beginning of the crabbing phase, the source node is likely to be the winner for each transaction, as the destination node may not have sufficient data and need to wait for the foreground pushes. however, as the destination node accumulates more and more data, the winner will be decided by the running speed of individual machines (taking into account the hardware speed, workload, etc.). so far, we assume that the destination node runs at a similar speed with the source. next we discuss how to extend mgcrab for slow or fast destinations."
"recall that the control agents perform a series of iterations and that in each iteration the control agents solve a local optimization problem followed by an exchange of information. note that internal and external nodes of control agent i correspond to external and internal nodes, respectively, of control agent j. control agent i considers in its local optimization problem the constraints that are associated with its internal nodes and that are of localized constraint type c int+ext i,int as hard constraints, using fixed values for the external variables. the values for these external variables are obtained from the neighboring agent j. control agent i solves its local optimization problem using these values for the external variables. the optimization yields values for the internal variables of control agent i, and for the lagrange multipliers that are associated with the constraints of localized constraint type"
"extra data transferred. since the data locations are certain to every transaction, there is no \"false\" push that transmits additional data not required by the destination. moreover, unlike most existing source-based approaches [cit], the updates to the migrating data are not transmitted to the destination. in mgcrab, the destination node computes these updates by executing the incoming transactions deterministically. in effect, mgcrab trades computing for communication overhead. this is worthwhile for most oltp applications as the transactions are generally short and the communication overhead (in distributed transactions) is a major reason for the slowdown of the system performance [cit] ."
"power networks [cit] are one of the corner stones of our modern society. the dynamics of a power network as a whole are the result of the interactions between the millions of individual components. conventionally, the power in power networks is generated using several large power generators. this power is then transported through the transmission and distribution network to the location where it is consumed, e.g., households and industry. power flows are then relatively predictable, and the number of control agents is relatively low. due to the ongoing deregulation in the power generation and distribution sector in the u.s. and europe, the number of players involved in the generation and distribution of power has increased significantly. the number of source nodes of the power distribution network is increasing even further as also large-scale industrial suppliers and small-scale individual households start to feed electricity into the network [cit] ."
"mgcrab works alongside any data partitioning scheme (e.g., hashbased, fine-grained record-based [cit], or fine-grained range-based [cit] partitioning) used by the storage and any elastic load balancing algorithm [cit] implemented in the system controller for the scheme (cf. figure 1 )."
there is something else noteworthy about mgcrab. we can see slightly higher loads (in both cpu and network) on the source node during the migration period. this is the amount of effort that the source node spends on identifying and pushing data to the destination node. we can also see this in the later migration period in squall (for asynchronous pushes).
"below a distinction is made between constraints that are considered as hard, and constraints that are considered as soft. the hard constraints are constraints that have to be satisfied at all costs. the soft constraints are constraints for which it is desirable that they are satisfied, but for which this should not be done at any price. the hard constraints are included in the formulation of optimization problems as explicit equality constraints; the soft constraints are included in the objective function of optimization problems through a penalty term, weighted by a parameter specifying the costs for violation of the soft constraint."
"this research successfully meets what it aimed for. the author has strongly presented that the algorithms either belongs to eas or si or nature based needs algorithm specific parameters greatly contribute to the success demands extensive tuning before the computational experiments. this process is time consuming and most of the times were found difficult to achieve by the beginner or intermediate level researchers. therefore, they miss the actual melody of these algorithms. in addition, the author has discussed the importance of the tlbo algorithm for the optimization, works in two different phases: teacher phase and learner phase."
"to solve this problem, we split a bgpush transaction into two phases, each accomplished by a distinct transaction, such that the source node can keep hiding the migration cost from users. the first transaction reads the chunk on the source node and sends it to the destination node, and then creates and schedules the second transaction into the system before it completes. the second transaction, after being forwarded by sequencers following the total order, inserts the buffered chunk to the storage of the destination node and then updates the metadata about the chunk location on both nodes. between the first and the second phases, the sequencer may assign other transactions in the total order, as shown in figure 3 . the point of such a split is that both the first (t 5) and second (t 7) transactions can be made very lightweight on the source node to prevent the clogging of system performance. the second transac-tion is clearly lightweight on the source node as it modifies only the metadata, so we focus on the first one. notice that the first transaction (t 5) is read-only on the source node. we let the node read the chunk without acquiring read locks. 4 because of this, the first transaction does not block the following transactions (t 6 and so on) and the source node can maintain the system performance. on the other hand, this may lead to a dirty chunk (to be stored into the storage of the destination node) in the second phase (t 7), as the transactions in between (t 6) may touch data in the chunk. thanks to the crabbing mechanism, the destination node knows which data have been modified since the first phase (t 5) by actively executing transactions, and can simply skip those modified data when storing the chunk in the second transaction (t 7) to maintain consistency. the effectiveness of two-phase background pushes is demonstrated in appendix of the supplementary materials [cit] ."
"in this section, we conduct two larger scale experiments to validate the efficiency of mgcrab. in the first experiment, we run 3 concurrent migration plans on a 12-node cluster, where different migrations involve different machines; while in the second experiment, we run 6 concurrent migration plans on an 18-node cluster, where each pair of destination nodes share the same source node. we use the tpc-c workloads with the scaling-out scenario in the following experiments. figure 10 shows the system throughput during the migration period. as we can see, the impact of migrations to system performance becomes larger when there are more nodes involved. and, in a complex situation (figure 10(b) ), both squall and mgcrab incur a performance drop in the beginning of the migration period. this is mainly due to the overhead of initializing migration states. however, mgcrab still manages to render better performance than the baselines."
"in this study, after the introduction part, it was carried out a literature review of the ahp's application on finance, and focused on the implementation of ahp. then, by using the ahp method, it has developed a case study on investment appraisal. in this context; net present value, benefit cost analysis, rate of return and payback period is determined as a criterion."
"chunk reordering. the master-slave mode introduced in section 4.3 may slow down the migration of active data because only the data written by a transaction and owned by the destination are migrated. to speed up transmitting the active data, one can let the background-push process reorder the data chunks. when the source executor enters the master-slave mode for a transaction, it flags the data chunks that overlap with the transaction's read-set such that the background-push process will push these chunks first (specifically, in a higher priority following a deterministic rule)."
"b) the controlling of the algorithm specific parameters is not as easy as it looks. also, controlling the parameters in each iteration is most of the time difficult and time consuming. this entire process does not belong to the jaya algorithm."
"we present mgcrab, a live migration technique, for deterministic database systems. different from most exiting migration techniques, mgcrab lets both the source and destination machines execute every incoming transaction during the migration period. we show how such a design avoids the complexity of data ownership tracking as well as distributed transactions that slow down the entire system right after the migration starts. we also propose the two-phase background pushes that prevent the migration transactions from blocking normal transactions. in addition, we point out some extensions and optimizations that can further improve the applicability of mgcrab as well as the system performance. extensive experiments are conducted on a real system and the results demonstrate the effectiveness of mgcrab in both the scaling out and consolidation scenarios. mgcrab guarantees the safety and liveness. we proof these guarantees formally in appendixof the supplementary materials [cit] ."
"mgcrab is live as it satisfies the following requirements: (a) termination: if the source and destination nodes are not faulty and can communicate with each other for a sufficiently long period during migration, the migration will terminate; and (b) starvation freedom: every transaction that accesses the migrating data can eventually be processed, despite failures that may occur."
"live migration [3, 11-14, 23, 33 ] is a fundamental step toward elasticity. given a migration plan for moving some data from a source node to a destination node, it moves the data while continuously serving the incoming transactions so to keep the system alive (available). currently, live migration techniques can be divided into those running an incoming transaction on the source machine [cit], destination machine [cit], or on either one of the two machines [cit] . 1 the source-based techniques [cit] have the advantage that the transaction can run smoothly in the beginning of migration because the data are likely to be available locally and the cache is warm. however, these approaches have a termination problem-the data migration may never end as the updates made by transactions running on the source node need to be constantly sent to the destination node (by shipping logs or snapshots). to terminate the migration, these techniques usually introduce a short period of downtime and then copy the final updates from source to destination. after that, the destination node starts serving transactions and the system becomes available again. the destination-based approaches [cit] avoid such system downtime by letting the destination node serve transactions immediately after the migration starts. nevertheless, the transaction execution will be slow in the beginning of the migration period since most data are not available at the destination yet and need to be pulled from the source node. this results in degraded system throughput. squall [cit], the third approach, extends the destination-based methods by carefully tracking the location of data being migrated and allowing a transaction to run on the source node if the data accessed by the transaction locate entirely on the source node. this mitigates the performance drop as some transactions in the system can run faster now."
"ahp in decision-making, it is a mathematical method of assessing qualitative and quantitative variables together, considering the priorities of the group or individual (tekindal & erümit, 2007) . the purpose of the ahp, by being placed to a scale of priorities for the given set of choices, the intuitive judgment and decision-makers taking into consideration the consistency comparison of the options in the decision-making process and to ensure the completion of this process in the most efficient manner (ö zyörük & ö [cit] ) . the most important feature of the ahp, the decision-making process as a visual multi-criteria decision problem can be seen in the form of hierarchy (ç [cit] ) ."
"executing transactions on both the source and destination nodes seems to imply doubled resource utilization (and saturated system). however, this is not true. instead, mgcrab improves resource utilization. we compare the cpu and network utilization of squall and mgcrab, and the results are shown in figure 9 ."
"the local optimization problem of control agent i consists of minimizing the local objective function j i, subject to the prediction model of subnetwork i and additional constraints on inputs and outputs. below we focus on the issues arising due to the presence of subnetworks that touch the subnetwork of control agent i. we discuss the issues arising with respect to the prediction model and the objective function of control agent i. for the sake of simplicity of explanation we consider two control agents, control agent i with neighboring agent j, that together control subnetworks that cover all nodes of the network model. the generalization to more than 2 control agents and not fully-covered networks is straightforward."
"in this section apply the scheme for multi-agent control of overlapping subnetworks, as discussed in section 4, to the problem of optimal power flow control in power networks. a case study is carried out on the ieee 57-bus power network [cit], comprising as components generators, loads, transmission lines, and buses, with in addition facts devices installed at various locations, as illustrated in figure 5 . two configurations are considered: in the first configuration only svcs are included; in the second configuration only tcscs are present. each of the facts devices is controlled by an individual control agent."
"as the second step aside the ahp, it comes pairwise comparisons matrix. after you create a hierarchical structure, it is calculated relative significance of each criterion [cit] . criteria matrixes for pairwise comparisons are shown in table 2 [cit] ."
"elastic load balancing. studies [cit] have shown that a deterministic database system is susceptible to load imbalances, and for this reason, it is important for the system to have components for elastic load balancing. we assume that inside each replica (data center), there is a system controller that collects statistics of system components in the background and decides when and how the data should be re-partitioned to fit the current workload. the controller periodically sends a partition plan to every partition in the replica. a partition plan specifies the data partition for every node so each partition can spot distributed transactions. after receiving the partition plan, the migration controller on each partition compares the old and new partition plans to derive migration plans, each of which specifies partial data being migrated from a source node to a destination node. a partition needs to infer only the migration plans that are relevant to itself (i.e., those where the partition is either the source or destination). then, for each migration plan, the migration controller on the source node triggers a migration task that runs a live migration algorithm. after all migration tasks are completed, the migration controller on each partition sends an acknowledgment to the system controller so the system controller can generate the next partition plan."
"given a migration plan (specifying the data to migrate, a source node, and a destination node), this phase engages all partitions in a replica such that every node enters the crabbing phase synchronously and has enough information to execute transactions correctly. at first, the migration controller on the source node creates an init transaction request that includes the migration plan as a parameter and sends the request to the sequencers in the system so that after being forwarded to the scheduler on every node, the transaction gets into the same place in the total order and can be processed atomically throughout the system. each scheduler forwards the request to the executor on the same machine. the executor then modifies the metadata about the ownership of migrating data (by changing \"source\" to \"both\"). after completing this transaction, all partitions enter the crabbing phase and process later transactions in the total order using the new ownership information."
"the objectives of the control are to improve the system security through minimization of the deviations of the bus voltages from given references to improve the voltage profile, minimization of active power losses, and preventing lines from overloading, by choosing appropriate settings for the facts devices. these objectives are translated into objective terms associated with the buses as follows:"
"usually subnetworks are defined through geographical or institutional borders, such as borders of cities, provinces, countries, the european union, etc. subnetworks can however also be defined differently, e.g., based on a fixed \"radius\" around input nodes. nodes that are reachable within a certain number of arcs from a particular node with an actuator are then included in a particular subnetwork [cit] . or, subnetworks can be defined using an influence-based approach [cit] . the idea of influence-based subnetworks is that the subnetworks are defined based on the nodes that a certain input and, hence, a control agent controlling that input, can influence. sensitivities are then used to determine which variables an input can influence, and hence, which nodes should be considered part of a subnetwork. the fixed-radius and the influence-based approaches have as advantage that the subnetworks are defined taking a more actuator-centered perspective. using the fixed-radius approach, this definition is somewhat ad hoc and heuristic. on the contrary, the influence-based approach is more flexible and allows for a structured determination of the subnetwork that a control agent has to consider. when using the mentioned approaches for defining subnetworks, any pair of two resulting subnetworks can be categorized as non-overlapping, touching, or overlapping, as illustrated in figure 2 . if for two subnetworks, the nodes belonging to one of these do not coincide with the nodes belonging to the other subnetwork, and if there are no arcs going from nodes in the one subnetwork into nodes of the other subnetwork, then the subnetworks are non-overlapping. if for two subnetworks, the nodes belonging to one of these do not coincide with the nodes of the other subnetwork, but if there are arcs between nodes of the one subnetwork and nodes of the other subnetwork, then the subnetworks are touching. if for two subnetworks, the nodes belonging to one of these partially coincide with the nodes belonging to the other subnetwork, then the subnetworks are overlapping. in that case, a common sub-subnetwork is defined consisting of those nodes and arcs that belong to both subnetworks."
"considering the above discussed fact, a new approach was proposed known as teaching learning based optimization (tlbo). the tlbo algorithm does not need any algorithm specific parameters; rather it needs only common controlling parameters (population size and maximum number of generations) to function. the tlbo algorithm works in two phases' are: teacher phase and learner phase. this paper is focused towards the jaya algorithm, a novel approach to the optimization. the jaya algorithm is very similar to the tlbo, as it falls in the category of algorithm specific parameter less algorithm. the jaya algorithm is new, is proposed by r. [cit], has been discussed in this paper in a significant depth, considering the following rational aspects: what is jaya algorithm, how it works, and why to use it."
this work is supported by the most joint research center for ai technology and all vista healthcare (most 107-2634-f-007-004-). we thank ms. tz-yu lin (tylin@datalab.cs.nthu.edu.tw) for helping setup environments for the experiments and plot the results as figures.
"jaya, a novel algorithm has been illustrated in this paper considering the three key rational aspects are: what is the jaya algorithm, how it works and why to use it. there exist several different types of population based heuristic algorithms have been proposed, were classified into two broad categories are: evolutionary algorithms (eas) and swarm intelligence (si) algorithms. the ea and si have been successfully applied to solve many typical real life problems were most of the time difficult to solve employing traditional search algorithms. the genetic algorithm (ga), evolutionary strategy (es), evolutionary programming (ep), genetic programming (gp), differential evolution (de), bacteria foraging optimization (bfo), artificial immune algorithm (aia) are few belongs to the ea, whilst particle swarm optimization (pso), ant colony optimization (aco), artificial bee colony (abc), firefly (ff), shuffled frog leaping (sfl) algorithms fall into the si. there exist some other algorithms are based on the nature, such as: harmony search (hs), lion search (ls), gravitational search (gs), grenade explosion method (gem), and biogeographybased optimization (bbo) and others."
"decision making is the basis of all management functions. therefore, it is necessary to take the right decisions to gain and maintain competitive advantage (kuruüzüm & [cit] ) . multi-criteria decision-making methods, when people deal with complex decisions covering various sizes are designed to help them make better choices [cit] . ahp is a quite widely used multi-criteria decision-making method. ahp; applied in many areas, helps in selecting the best alternative for decision makers in case of multi-criteria decision making."
"for control of overlapping subnetworks, multiple control agents will try to control the values of the common variables. to allow control agents to jointly achieve performance comparable to the performance that an overall centralized control agent include as is can achieve, the responsibility for the objective terms involving only common variables, i.e., of localized objective term type j com i,com, is shared equally by the control agents. hence, each control agent i that considers a particular common node κ, includes in its objective function 1/n κ times the objective function terms of such nodes of localized objective term type j com i,com, where n κ is the number of control agents considering node κ as common node. control agent i in addition includes into its objective function the objective terms of all its internal nodes, and the objective terms of these common nodes that involve only internal and common variables, i.e., the objective terms of localized objective term types"
"each facts device is controlled by a different control agent. the influence-based subnetworks of the control agents controlling the facts devices can be overlapping, and therefore the control problems of the control agents are set up using the approach discussed in section 4. to solve their subproblems at each iteration the control agents use the nonlinear problem solver snopt v5.8 [cit], as implemented in tomlab v5.7 [cit], and accessed from matlab v7.3 [cit] . in the following we illustrate how the approach works for a particular assignment of nodes to subnetworks in two representative scenarios."
"various test scenarios with different facts devices and subnetworks have been examined. here we present two representative scenarios. the subnetworks used in these scenarios are shown in figure 6 . it can be seen that these subnetworks are overlapping, since there are several nodes that are included in both subnetworks."
"as discussed in section 3.3, a background push in mgcrab is very lightweight on the source node thus can minimize the impact on system performance. this offers a significant advantage in a system equipped with the disk-based storage where reading/writing a data chunk from/to the disk-based storage is very slow. mgcrab can be further optimized to prefetch [cit] the data chunks on the source node to reduce the processing time of the bgpush transactions in the first phase. in addition, the destination node can write the data chunk into the storage asynchronously in the second phase in order to speed up and catch up with the source node faster. the asynchronous writes do not have an impact on the correctness in the presence of failures as the chunks can be recreated by replaying the request logs during the recovery process (see appendix of the supplementary materials [cit] )."
"in figure 7 (d), we can see that with squall the latency increases during the migration. this is due to the destination node busying inserting data chunks pushed in the background. on the other hand, the latency of mgcrab remains stable. as stated in section 3.1, mgcrab lets both the source and destination nodes respond to clients, so the latency observed by a client is determined by the winner node, which is the source node in this case. mgcrab lets the source node hide the cost of migrations on the destination node."
"in this section we formalize the way in which we describe the network characteristics, subnetworks, and control objectives in this paper. an example of the application of this formalization is given in section 5."
"to illustrate the topics discussed and the proposed approach, we have defined overlapping subnetworks for flexible alternating current transmission systems (facts) in an adjusted version of the ieee 57-bus power network. using the proposed control approach, we have then solved an optimal power flow control problem. the simulations illustrate that in the considered cases the proposed approach can achieve fast convergence to actuator values that are globally optimal."
"similarly as for control of touching subnetworks, for control of overlapping subnetworks, internal nodes of control agent i that are connected to external nodes require special attention, since the constraints associated to these nodes may involve external variables. in addition to this, common nodes of control agent i that are connected to external nodes also require special attention. the extension of the approach for control of touching subnetworks to the control of overlapping subnetworks involves the following extension of the prediction model. control agent i considers as prediction model the constraints of all internal and common nodes. for the constraints of localized constraint types"
"in this chapter we have focused on an alternative way to define subnetworks for higher-layer multi-agent control. the higher control layer uses steady-state characteristics only. we have discussed how subnetworks can be defined based on the influence of inputs on the variables of nodes. when such an approach is used to define subnetworks, some subnetworks could be overlapping, resulting in constraints and objectives in common sub-subnetworks. we have proposed a method for higherlayer multi-agent control that can be used by control agents that control such overlapping subnetworks."
"in this section, we introduce mgcrab using some simplified assumptions for ease of presentation. we assume no failures, no distributed transaction in the system, and that there is only one migration plan to be fulfilled. the extended design that works with distributed transactions and concurrent migration plans is discussed in section 4, while the failure handling is described in the supplementary materials [cit] ."
the values of the inputs u should be adjusted in such a way that the objectives associated with the nodes are achieved as well as possible. let for a control agent i the nodes that it controls define its subnetwork. the prediction model that control agent i then uses consists of the union of the constraints of each node that is part of its subnetwork. let the subnetwork and the control goals of a control agent be defined using one of the approaches mentioned in section 1.2.
"for the transmission lines the well-known π-model is used [cit] . the active power z p,mn (p.u.) and the reactive power z q,mn (p.u.) flowing from bus m over the transmission line to bus n are then given by:"
this section illustrates the importance of the jaya algorithm over the other algorithms. the detailed discussion presented in the section i and ii has concluded three main reasons are outlined here:
"a) the jaya algorithm does need any algorithm specific parameters, which requires extensive tuning before conducting the actual computational experiments, if not done correctly, leads unavoidable and unwanted convergence."
"the rest of this paper is organized as follows. section 2 reviews the architecture of a deterministic database system and existing migration techniques. section 3 introduces mgcrab while section 4 discusses some practical considerations. section 5 evaluates the performance of mgcrab and section 6 reviews more related work. finally, section 7 concludes the paper."
"in addition to albatross [cit], some studies aim to improve the efficiency of the source-based approaches. slack [cit] minimizes the impact of migration by throttling the rate that the data chunks (specifically, pages) are migrated from the source to destination. the throttling also takes into account the impact of migration on other tenants in a multi-tenant database system. slacker uses recovery mechanisms to stream updates. madeus [cit] employs a middleware that analyzes the operations performed on the source node and propagates only the necessary operations to the destination to synchronize the two nodes. operations are propagated concurrently to improve the communication efficiency. this study is optimized for the snapshot isolation."
"so far, the jaya has shown the computational capability when applied to the constraints and unconstrained problem, but not applied to solve any real life problems. therefore, it would be interesting to see the impact of the jaya algorithm to solve the real world optimization problems. the author strongly believes that this paper will give an edge to the jaya algorithm, which will lead to an increase the use of it in the near future."
(a) cpu usage (tpc-c) (b) network usage (tpc-c) figure 9 : the cpu and network usage during the migration in the scaling out scenario with tpc-c workload. see figure 7 for the explanations of vertical lines.
"neighboring agent j considers in its optimization problem the constraints of the internal and common nodes of control agent i that involve external variables of control agent i as soft constraints by including them in the objective function through a penalty term, weighted by the lagrange multipliers provided by control agent i, and with fixed values for the external and common values in the soft constraints as received from control agent i. note that although control agent j considers fixed values for the common variable in the soft constraints, it will not fix the values for the common variables in the hard constraints (similarly as control agent i). hence, control agents i and j share the responsibility for the common variables. the result of solving the optimization problem of neighboring agent j therefore yields values for the internal, common, and external variables of control agent j. the internal variables of control agent j related to the soft constraints are sent to control agent i. table 5 summarizes how control agent i deals with the different localized constraint types."
"aborted transaction. since the source is able to continue executing transactions and the data are always available during the migration period, there is no transaction that needs to abort due to the live migration."
"in this chapter we propose a coordination scheme for control agents controlling overlapping subnetworks with the aim of obtaining the best overall network performance. this chapter is organized as follows. in section 2, we formalize the modeling of networks, subnetworks, and control objectives used in this chapter. in section 3, we first discuss a recently proposed approach that can be used for the multi-agent control of subnetworks that are not overlapping (i.e., non-overlapping or touching). we then propose an extension of this approach to multi-agent control of subnetworks that are overlapping in section 4. in section 5, we apply the proposed approach to an optimal power flow control problem from the domain of power networks. in particular, we employ the approach to control facts devices in an adjusted ieee 57-bus power network, in which each facts is controlled by a different control agent. section 7 contains conclusions and directions for future research."
"the control structure of power networks can be represented as a multi-agent system [cit], in which the control agents are organized in several layers as illustrated in figure 1 . a control agent hereby is an entity, e.g., a human, a computer, or a hardware device, that on the one hand observes the state or situation of the network and on the other hand chooses actions to be taken in the network by changing settings of actuators, such as the reference for the power output of generators or the reference for settings of facts devices. a control agent has to choose its actions in such a way that the performance of the network in terms of safety, security, and stability, is the best possible, while respecting operational constraints and minimizing costs. high costs hereby indicate a bad performance of the network, whereas low costs indicate a good performance. in the control hierarchy that power networks are controlled by, at the lower layers control agents consider faster dynamics, more local information, smaller subnetworks, and shorter time spans. at the higher layers con-trol agents consider slower dynamics, more global information, larger subnetworks, and longer time spans [cit] . the control problem that an individual control agent in a control hierarchy faces can be cast as an optimization problem, based on a local objective function that encodes the control goals of the agent, subject to a model of the part of the network that the control agent controls, and additional constraints, e.g., on the range of the inputs. the model of the part of the network that the control agent controls is referred to as its prediction model. this prediction model describes how the values of variables of interest (such as voltage magnitudes, power flows, etc.) react to changes in inputs and can therefore be used to predict what the effect of certain input choices is going to be."
"in the following we make a distinction between a bus and a node. a bus refers to an element of the physical power network, whereas a node refers to an element of the model of the physical power network. since for each physical bus a corresponding node is included in the model, references to a bus or its corresponding node can be interchanged, except for when assigning constraints relating to two buses, such as constraints imposed due to transmission lines, to a single node, as we will see next."
"note that all partitions have to process the init transaction because any node other than the source or destination may encounter a distributed transaction accessing the migrating data later (to be discussed in section 4). however, there is no need for the transaction to process/ship complex metadata (e.g, index wireframe [cit] ) in order to help track the locations of migrating data, because the locations can be inferred deterministically on individual machines (to be discussed in the next phase). as a result, the transaction is generally very short and has a negligible impact on performance."
"ycsb. initially, the source, destination, and the other nodes have 1, 0, and 0.5 million records respectively. we migrate 0.5 million records (one partition) from the source to destination nodes. each partition has 200 client threads submitting requests to it from client nodes. the changes of system performance during the migration are shown in figures 7(a)(c) . it is obvious that stop-and-copy has the worst performance (zero throughput and extremely high latency) during the whole migration period. but it gives a lower bound of the migration time because the data transfer can be done much more efficiently by a dedicated process than live migration approaches. squall is alive but gives dropped performance during the migration. squall uses reactive polls to migrate hot data in the foreground and one-phase background/asynchronous pushes to migrate cold data in the background. we observe that the performance drop is mainly due to the one-phase background pushes rather than the reactive polls because ycsb transactions are generally short and the delays of reactive polls do not clog up short transactions easily. however, the one-phase background push transactions are much longer and block many other transactions. on the other hand, the two-phase background pushes in mgcrab are lightweight on the source node and do not conflict with foreground transactions there. this allows the source node to maintain the performance during the migration period. moreover, the fewer blocked foreground transactions speed up the migration of hot data. therefore, the total migration time is reduced. mgcrab also gives a latency comparable to squall's."
"where z, u, and d are the state, input, and disturbance variables of the overall network, and g defines the steady-state characteristics of the network. given the inputs u and the disturbance variables d, the steady state in which the network settles is determined by solving the system of equations (2)."
"foreground pushes. when processing a transaction, the executor on the source node may push data to the destination node in order to 1) migrate the active data as soon as possible, and 2) help the same transaction complete on the destination executor."
"after all data have been migrated, the migration controller on the destination node receives an empty background push and creates a terminate transaction, as shown in figure 3, that changes the ownership of the migrating data from \"both\" to \"destination\" on every partition. like the init transaction, this transaction is very short and has a negligible impact on the performance. after it completes, the source node stops executing transactions accessing the migrating data."
"in a multi-agent system, control is distributed over several control agents. each of the control agents controls only its own part of the network, i.e., its own subnetwork. let for now a network be modeled at an abstract level using a number of nodes with arcs interconnecting the nodes. the nodes represent characteristics of the components of the physical network, whereas the arcs model the direct interaction between the nodes. e.g., one node κ could model the characteristics of a power generator together with a bus and a transmission line, and another node ω could model the characteristics of a load and a bus. if the bus of this load is physically connected to the transmission line, then an arc is defined between the nodes κ and ω. the subnetwork of a control agent then constitutes a number of nodes together with the arcs connected to these nodes 1 ."
"neighboring agent j considers the constraints of the internal nodes of control agent i that involve external variables of control agent i in its decision making by including the associated constraints as soft constraints in its local objective function. in the soft constraints of control agent j, the external variables, which correspond to internal variables of control agent i, are fixed to the values that control agent i has sent to control agent j. also, the soft constraints are weighted by the lagrange multipliers as given by control agent i. neighboring agent j solves its optimization problem, yielding values for its internal variables. it sends the values of the internal variables that appear in the soft constraints to control agent i, such that control agent i can update its information about the corresponding external variables."
"once entering the crabbing phase, the scheduler on the destination node starts forwarding transaction requests to the local executor if the required data overlap with the data to be migrated (regardless of the actual locations of these data). so, both the source and destination nodes process these transactions now. mgcrab employs two techniques, namely the foreground pushes and two-phase background pushes, to migrate data when serving the transactions:"
"the jaya is a simple and powerful global optimization algorithm has been successfully applied to the benchmark function of constraint and unconstrained problems. all though it is parameter less algorithm as the tlbo is, however, it is different since it does not require learner phase, i.e. it uses only one phase is teacher phase, whereas the tlbo performs its action in two phases. it is based on the fact that the solution can be obtained for a given problem moving towards the best solution, avoid the worst solution. it is the beauty of this algorithm that it requires only few control parameters like maximum number of generations and population size, and number of design variables, most of the time common for any algorithms. it does not need any algorithm specific control parameters require extensive tuning before conducting the actual computational experiments. the work procedure of the jaya algorithm is outlined in algorithm-1 is simple to understand. the s4 has clearly represented that the jaya algorithm has the tendency to move to the best, i.e. closer to the success and avoids the worst solution obtained in the iteration. this nature makes this algorithm victorious, hence the name jaya is defined, is derived from a sanskrit word meaning \"victory\"."
"zephyr [cit] . this approach avoids the service downtime by letting the incoming transactions run on the destination node. the source node transmits data chunks to the destination node iteratively. if a transaction on a destination accesses data that are not transmitted yet, it pulls the data from the source node. this increases the transaction latency and the transaction will block more conflicting transactions. note that the blocking can result in a significant drop in throughput in a deterministic database system [cit] because, to ensure the determinism, conflicting transactions cannot be dynamically reordered at run time (conversely, a traditional database system can achieve this easily by using, strict two-phase locking, for example). furthermore, zephyr does not allow a transaction on the source node that was active at the start of migration to change the metadata (specifically, the index used to track the migration progress) during the migration period. if so, the transaction needs to abort. the upside is that there is no additional update that need to be transferred. note that, on the destination node the data migration process that persists the data chunks may conflict with both the read-only and read-write transactions. therefore, setting a larger chunk size in order to reduce the migration time may result in further degradation in performance."
"mgcrab avoids both of the above problems. by allowing the source and destination machines to execute transactions concurrently, the distributed transactions between the source and the other node are not prolonged by the destination. this maintains the performance of the other node, as evidenced by its stable resource utilization between the first solid vertical line and the dash line. furthermore, neither the phase 1 nor phase 2 background push transactions conflict with the normal transactions. thus, after the back-"
"however, in practice, squall still renders a considerable performance drop during the migration period [cit] . the system may fail to meet an sla and incur significant financial losses [cit] . furthermore, with squall and most existing live migration techniques, there is a trade-off between performance and migration time (i.e., the duration of the migration period)-the shorter the migration time, the larger the performance drop can be during the migration period. this is because the migration process usually contends with running transactions on the migrating data. for example, suppose the data being migrated are broken into chunks and transmitted one-by-one. on the destination node, the process that persists a data chunk c will block all transactions accessing c. to meet the sla, one could set the chunk size small such that the migration process will block fewer transactions (as fewer transactions may access data in c) for a smaller amount of time (as it takes less time to persist c). but this can prolong the migration period as there is a higher communication overhead to transmit a larger number of small chunks than a small number of large chunks. the prolonged migration time may prevent the system from reacting to the changing workload in a timely manner. there is a crucial need for a new live-migration technique that is more transparent (in terms of performance degradation) and that avoids such a trade-off."
"we first propose some new definitions, next we consider the issues appearing due to the overlap, and then we propose a way to deal with these issues. again, for simplicity of explanation we consider two control agents, control agent i with neighboring control agent j, that together control the subnetworks, which are assumed to cover the full network model."
"in the first scenario, svcs are placed at buses 14 and 34. as the svcs are mainly used to influence the voltage profile, the line limits are chosen such that no line is at the risk of being overloaded. figure 7 (a) shows the convergence of the svc settings over the iterations. as can be seen, the settings of the svcs converge within only a few iterations to the final values, which in this case are equal to the values obtained from a centralized optimization. figure 7(b) shows the evolution of the deviations between the values determined by both subnetworks for the voltage magnitudes and angles at some common buses. in the figure the error z v,err,m is defined as the absolute difference between the values that control agents 1 and 2 want to give to the voltage magnitude z v,m . similarly, the error z θ,err,m is defined as the absolute difference between the values that control agents 1 and 2 want to give to the voltage angles. as can be seen fast convergence is obtained."
the cost of mgcrab is summarized in table 1 . downtime. there is no downtime because the clients see the results from the winner nodes. the handover of winner nodes is seamless on a per transaction basis.
"tpc-c [cit] : the tpc-c benchmark simulates a warehouse management system. it consists of nine tables and five types of transactions. here we focus on the new-order transactions and payment transactions, which are both read-write transactions and together contribute 88% of the standard tpc-c workload. we create 20 cold warehouses and one hot warehouse, where the hot warehouse receives transaction requests from 10 times more clients than a cold warehouse does. in the scaling out scenario, the migrating partition consists of the hot warehouse, and the remaining partitions contains 10 cold warehouses each. as compared to the ycsb workload, tpc-c has much longer transactions, and there are roughly 10% of the transactions that are distributed across multiple partitions."
"by kirchhoff's laws, at each bus the total incoming power and the total outgoing power has to be equal. this yields the following additional constraints for bus m:"
"caught-up (optional) figure 3 : in mgcrab, a live migration task proceeds in the initial, crabbing, caught-up, and termination phases. user-and system-generated transactions are colored black and white respectively. the caught-up phase is described in section 4.4."
"optimal power flow control is a well known-method to optimize the operation of a power network at higher control layers [cit] . optimal power flow control is typically used to improve steady-state network security by improving the voltage profile, preventing lines from overloading, and minimizing active power losses. the optimal power flow control problem is usually stated as an optimization problem in which variables to be optimized consist of inputs or settings for generators, the objective function encodes the control goals (such as maintaining voltage magnitudes within desired bounds, preventing transmission lines from overloading, minimizing power losses, etc.), and the prediction model consists of the steady-state characteristics of the network."
"in the crabbing phase, mgcrab hides the migration cost from clients by letting each client see the results from the winner node (i.e., the node that completes the transaction issued by the client faster). the source and destination nodes see strongly consistent data, and thus they always process a transaction in the same way. mgcrab leverages the determinism discussed in section 2.1 to avoid the high cost of strong consistency."
"external internal+external control problem as considered by an individual control agent, and then we discuss the scheme used by multiple control agents for coordination and communication."
"squall [cit] : a migration begins with a distributed transaction that updates the metadata on the source and destination nodes. then, each transaction request accessing the migrating data is routed (via the scheduler) to the destination node by default. however, if the data are all available on the source node, the transaction will be routed to the source node. squall has been shown to outperform other live migration techniques, like zephyr [cit], and is designed specifically for deterministic database systems. one key difference between the original squall on h-store [cit] and our implementation is that the transactions on h-store are processed sequentially by a single thread; while transactions on the calvin/our system are processed concurrently by multiple threads. also, squall proposes some techniques for live reconfiguration targeting multiple concurrent migration plans. since our focus is on improving the execution of a single migration plan, we do not implement these live reconfiguration techniques."
"to optimally use and control such devices and to optimally use the existing infrastructure, new control techniques have to be developed and implemented [cit] . a major challenge in this context is that the devices in the network, such as the various facts devices, are usually owned and operated by different authorities. despite this, the operators of the various devices have as objective to determine their actions in such a way that the best overall network performance is obtained. hence, multiagent control, in which communication and cooperation between various control authorities is explicitly taken into account, has to be employed. [cit] ). the control structure consists of several layers of control agents. the control agents make measurements of the state of the network and determine which actions to take."
"throughput drop and increase in latency. there is no expensive agreement protocol (e.g., 2pc) in mgcrab for ensuring data consistency between the source and destination nodes. furthermore, a foreground push can be performed very fast by a transaction on the source node as it does not wait for the same transaction on the destination node to receive and process the data. if the destination node lags behind in physical time, the migration controller on the destination receives and puts the pushed data into the buffer and let the transaction read them locally later. also, the transaction latency perceived by a client is at least as short as that on the source node. therefore, mgcrab has little impact on the transaction latency. the same argument applies to the system throughput."
the relative line loading is penalized in a quadratic way such that an overloaded line is penalized more severely than a line that is not overloaded.
"data migration has a cost that includes the following [cit] : downtime (system or database service outage), number of transactions aborted, increase in transaction latency, drop in throughput, and transfer of extra data (in addition to those specified in the migration plan). also, there is a trade-off between the migration time and the migration cost. normally, the shorter the migration time, the higher the migration cost. a live migration technique is expected to minimize the migration cost while avoiding its trade-off with migration time."
as mentioned before gpus are good at doing parallel computations whereas cpus are much faster at serial calculations. we have therefore divided the computational labor between the gpu and the cpu based on how parallel different parts of the problem are. the scalar field equations (6) are the hardest part computationally but they can be parallelized easily since similar leap frog steps are taken at every point of the lattice with the same equations of motion. the evolution of the scale factor on the other hand is easy to evolve serially once the averages of the gradient and potential terms have been calculated. we will therefore solve the evolution of the scalar fields on the gpu and leave the scale factor evolution to the cpu.
fenchel duality offers a more powerful framework as compared to the classical lagrangian duality approach as it allows us to dualise functions (by means of their convex conjugates) rather than merely constraints.
"the device in nvidia gt200 series is divided into streaming multiprocessors (sm) that do the actual computations. each sm has 8 single precision calculating units, 1 double precision unit and 2 super function units. shared memory and registers are also located on the sms. once a kernel is launched on the device the resource usage of the kernel dictates how many blocks a sm can execute. therefore it is crucial to keep the register and shared memory usage of a thread to its minimum whilst also minimizing the global memory fetches. the global reads should also be coalesced for the performance to be optimal which means that number of threads block should be a multiple of 16. the search for the optimal number of threads per block is however largely an exercise in trial and error."
cosmology has many computationally demanding problems [cit] . the study of interacting fields and reheating after inflation in early universe is one of them. the nonlinear nature of the system compels to study the evolution of the system numerically. the problem then becomes the distretization of the scalar field equations and solving the evolution of the system in a lattice once the initial values have been set. this problem has been previously solved in latticeeasy [cit] and defrost [cit] programs that use very different methods to solve the dynamics of the system.
which we can now use in the leap frog method to evolve the scale factor. note that this is done differently in defrost where the evolution of l ≡ 1/h is solved first and the scale factor a evolution is solved from l. this has the advantage that the computationally complex gradient terms cancel out from the equations of motion. we chose to use the leap frog method mainly for pragmatic reasons: the lattice simulations are usually done with periodic boundary conditions and when calculating the average over the volume of gradient terms we can use the divergence theorem to get
"first, choose a prime number within the range provided by the 32-bit integer known as p .then choose a value for g, withint the range of the 32-bit integer but the number must be lower than value of p. for example, there are two users named along and busu."
another interesting venture would be the porting of cudaeasy into opencl. at the time of writing there are however some obstacles before this can be done though. despite our best efforst we haven't yet found a solid fft implementation on opencl. once this situation improves cudaeasy should be relatively easy to port to opencl language and therefore harness the computational power of amd gpus also into preheating calculations.
"a common method for calculating modular division is extended euclidean algorithm [cit] .this algorithm is used to find out the greatest common divisor of two integers. in this experiment, gcd and extended euclidean algorithm were used to find inverse function to decrypt c 1 ciphertext."
the shear computational power of gpus is however useless if if cannot be harnessed effectively. therefore the use of computationally suitable language is of paramount importance. recent developments in this sector have made programming of modern gpus relatively easy. nvidia's cuda architecture [cit] and the opencl language [cit] of the khronos group are similar to c syntax and therefore lower considerably the learning curve needed to write programs that use gpus as co-processors. how gpus are programmed in these languages is fairly similar and by learning one of these makes the other easy to learn as well.
"the explosive proliferation of interconnected sensing, computing and communication devices has marked the advent of the concept of cyber-physical systems -ensembles of computational and physical components. in drinking water networks this trend has ushered in new control paradigms where the profusion of data, produced by a network of sensors and stored in a database, is used to prescribe informed control actions [cit] . nevertheless, as these data, be they water demand values or electricity prices, cannot be modeled perfectly, the associated uncertainty is shifted to the decision making process."
"given the properties of functions f *, being differentiable with lipschitz gradient, and g *, being prox-friendly, we may use nesterov's accelerated proximal gradient method on the dual problem which produces the sequence"
"we have chosen to write the program in the cuda architecture mainly for pragmatig reasons: when we started to develop this code opencl was still in its infancy whereas cuda was many generations old and had comprehensive documentation and a large group of developers. note however that since opencl code can be run in both amd and nvidia gpus the future de facto gpu programming language is still undecided. in order to future proof the cudaeasy program we are looking into porting it also into the opencl language. this paper is organized as follows: in section ii we will present the cuda architecture, the equations of motion and our implementation of the lattice program. we will present numerical results in section iii and we will conclude in section iv."
and denotes average over the volume of the lattice. note that ∇ is the gradient operator with respect to comoving coordinates. cudaeasy uses a staggered leapfrog method to solve these equations of motion. this means that the field values and their derivatives are stored at different times and they are advanced in turns:
after the different shared memory slices have been updated the laplacian can be calculated with the discretization mentioned before. since the coefficients used in this dicretization are the same at every point of the lattice we have stored these in the constant memory of the gpu which is meant to store read-only data for the threads. this way we can lower the register need of a kernel and make the program faster.
"once the calculations needed in the averages are done the thread block advances in z-direction and loads new values into shared memory, calculates the laplacian, advances the scalar fields and increments the gpe variable and advances again. once the thread blocks reach the top of the lattice the evolution step finishes for the lattice and writes the values of gpe into global memory. this way the global memory bandwidth needed is reduced significantly compared to a method where the increment variables would be written at every point of the lattice since the values are now written only once by every thread."
"at large, not many software and libraries are available for stochastic optimal control; one of the very few one may find on the web is jspd, a generic java stochastic dynamic programming library. quasar is a commercial tool for scenario-based stochastic optimization. one of the most popular tools in the toolbox of the water networks engineer is plio [cit], which implements mpc algorithms. this work covers the yawning gap between engineering practice and the latest developments in control and optimization theory for drinking water networks. these results can also be applied for the control of other infrastructure with similar structure such as power grids [cit] ."
the safety storage cost penalizes the drop of water level in the tanks below a given safety level x s . an elevation above this safety level ensures that there will be enough water in unforeseen cases of unexpectedly high demand and also maintains a minimum pressure for the flow of water in the network. this is given by
elgamal algorithm is a continuation from a diffiehellman algorithm and extended euclidean algorithm. the coding that has been created is a combination of diffiehellman algorithm and extended euclidean algorithm.
"within the cuda framework gpus are called devices whereas the cpu is known as the host. the fundamental concept in gpgpu are the lightweight threads that are executed in parallel on the device with kernel functions. threads within a kernel constitute a two dimensional grid that is divided into different blocks that hold the threads. blocks can be one, two or three dimensional and threads within the same block can share information through a shared memory. threads and blocks have ids (threadidx and blockidx respectively) which can be used to calculate the global thread indexes within the grid."
mixing nodes are intersections where flows of water are merged or separated. the mass balance equations for mixing nodes give rise to algebraic constraints of the form
future developments in rapidnet will involve the implementation of new faster parallelizable algorithms which make use of quasi-newton directions based on our recent theoretical work [cit] and the exploitation of multiple-gpu architectures. we shall introduce a library of available water network entities from the literature as well as reported water demand and energy price models for different water networks and energy markets.
"there is no a specific meanings or standard for trust and privacy. fig. 1 shows the relationship between trust, privacy and security. basically, trust is an acceptance of any vulnerability either it is a positive expectation of behavior or intentions. [cit] definition of security is to maintain the confidentiality, the availability of information and integrity [cit] ."
"the volume in each tank should never exceed a maximum limit v j max and it should always be above a hard lower limit v j min, that is,"
"a forecaster provides to the controller estimates of the upcoming water demands and electricity prices. this is an abstract class which can be subclassed with particular model implementations (e.g., arima, svm, or any other), or the user can provide custom forecasts using any third-party software which exports its forecasts in a json file."
"so long as prox γg is easy to compute, so is prox γg * and it can be obtained from the moreau decomposition formula which is"
"the high uncertainty in the operation of drinking water networks, as a result of the volatility of future demands as well as energy prices (in a deregulated energy market) is likely to lead to a rather expensive operating mode with poor quality of service (the network may not always be able to provide the necessary amount of water to the consumers). in control engineering practice, this uncertainty is often addressed in a worst-case fashion [cit] -if not neglected at all -leading to conservative and suboptimal control policies. it is evident that it is necessary to devise control methods which take into account the probabilistic nature of the underlying uncertainty making use of the wealth of available historical data aiming at a proactive and foresightful control scheme which leads to an improved closed-loop performance. these requirements necessitate the use of stochastic model predictive control : an advanced control methodology where at every time instant we determine a sequence of control laws which minimizes the expected value of a performance index taken with respect to the distribution of the uncertainty [cit] . optimization-based approaches for the operational management of water networks have been studied and are well established in engineering practice [cit] ."
"\"new directions in cryptography\" is a paper by diffie and hellman that presented a secure key agreement protocol called dhke that can be carried out over public communication channel [cit] . diffie-hellman is the earliest practical implementation of cryptographic key exchange [cit] . dhke is a method that allows two entities to share a secret key over insecure communication channels, although they have no prior knowledge of each other [cit] ."
"in this paper we have shown via simulations that when a water network is operated in the context of a volatile energy market, considerable savings can be obtained by using a predictor of the upcoming energy prices and taking into account the associated price-related uncertainty in the formulation of the scenario tree. alongside, we advocate that rapidnet can transform ssmpc from a powerful (but too complex) theoretical development to control engineering practice and enabling the solution of very large ssmpc scale problems and their seamless integration into the control system of the water network."
after finishing the evolution kernel for one scalar field the same calculations need to be done for the other fields as well. all of the fields could be in theory advanced with one kernel call but this would quickly lead to an extremely long kernel function suitable only for a very spesific model. current implementation uses two fields but this can be easily increased for different more complex models. the scale factor evolution on the cpu is done once every scalar field has been evolved.
"where w s is a positive scaling factor. the state constraints (9a) should be satisfied at all times without however jeopardizing the feasibility of the optimal control problem we have to solve at every time instant. for that, we introduce an additional cost which penalizes the violation of the state constraints as follows"
"despite the fact that ssmpc problems typically involve millions of decision variables, the associated optimization problems possess a rich structure which can be exploited to devise parallelizable ad hoc methods to solve the problem more than an order of magnitude faster than commercial solvers running on cpu."
"the architecture of our implementation comprises three independent modules: (i) the network module, (ii) the energy prices and water demands forecaster and (iii) the control module. the network module provides a dynamical system model which describes the flow of water across the network together with the storage limits of the tanks and the constraints on pumping capacities. the network module defines a safety storage level for each tank -a level which ensures the availability of water in case of high demand and the maintenance of a minimum required pressure. the forecasters produce a scenario tree, that is, a tree of likely future water demands and energy prices, upon which a contingency plan is made by minimizing a cost function which quantifies the operating cost and the quality of service. such scenario trees are constructed from historical data of energy prices and water demands. the control module computes flow set-points, which are sent to the pumping stations and valves, by solving a scenario-based stochastic model predictive control problem over a finite prediction horizon."
"where α 0 u k is the water production cost (treatment and acquisition fees), α k u k is the uncertain pumping cost and w α is a positive scaling factor. the smooth operation cost is defined as"
"where τ k is the computation time required by the solver for solving the ssmpc problem at time instant k. the maximum runtime is of higher importance that the average runtime in applications to verify that a decision can be made within the available time period. all three indices are defined so that low values are preferred. in order to justify the need for and underline the importance of taking into account the uncertainty associated with electricity prices, we performed two sets of simulations where in one case we disregarded that volatility (the ssmpc used only the nominal predictions of the electricity prices). we may observe in fig. 9 that the hourly operation of the network becomes more expensive by approximately +5%, therefore, the proposed stochastic control approach can lead to significant economic savings for the network operator. for example, for the case of 631 scenarios, the operation cost when the price uncertainty is disregarded is e 4947/hour, whereas with the proposed approach which takes into account the price uncertainty it drops to e 4748/hour -a saving of e 199/hour, that is, a cost reduction of 4%."
the total computation time per one step is roughly 0.022 seconds which corresponds to a total running time of one hour and 36 minutes for one simulation with 256
"the dwnnetwork class stores data related to the network topology, physical constraints and network dynamics matrices in equations (8a) and (8b). the user can create instances of dwnnetwork simply by passing a json file, network.json, with the network information."
an abstract forecaster which predicts the upcoming electricity prices and water demands using some predictive model (implemented by subclassing forecaster) or reads the forecasts from a json file (so that the user can use forecasts from third-party software).
"in rapidnet, at every time instant k, the method controlaction in smpccontroller returns the control action that is to be applied to the water network (pump and valve set points). all computations involved in this method are either summations or matrix-vector multiplications which can be parallelized across the nodes of a stage. these multiplications are implemented using the function cublassgemmbatched of cublas and vector additions are performed using the cublassaxpy of cublas. apart from the standard cublas methods, we have defined custom kernels for the summation over the set of nodes and to evaluate projections with respect to the box constraints and the proximal operator of the distance function from the set (that is, to compute the proximal operator of g as discussed in section 4.3."
and the surface integral cancels out due to periodic boundary conditions. we can now use this result in the volume averages of the gradient terms in scale factor equation (10) and since the laplacian is already calculated in the scalar field equations (6) the computation of the average gradient term actually has practically no computational effect. this turns out to be useful information in the gpu implementation. note that this is valid only for the average gradient terms. when calculating the energy density and the pressure we still need to calculate the computationally difficult gradient terms.
"note that although g is prox-friendly, g composed with the linear operator h -as it is in (22) -is not. this is the main reason why we resort to the dual problem (31) ."
"it is natural to expect that the upcoming energy prices can be only predicted up to moderate accuracy as they do not follow a regular pattern and are influenced by many market-related parameters. despite our limited predictive capacity, we shall show in section 6.3 that by taking into account this volatility using stochastic model predictive control we do mitigate the effect of the price uncertainty leading to a more economic operation of the water network."
"we may also see that as more scenarios are considered in the ssmpc formulation, the average cost of operation plummets at around 194 scenarios, where, however, the safety index is still high. in order to obtain a safer operation we need to pay the \"cost of safe operation\": indeed, at 631 scenarios, the operation of the network is more expensive compared to the case of 194 scenarios, but kpi s is at a minimum. this reflects a trade-off between the economic and safe operation of the network."
the device has a range of different memories that have different characteristics and roles in computing. global or device memory is the main memory of the gpu but it is located off-chip and therefore has a considerable latency.
"in the context of a deregulated wholesale energy market, prices on the day-ahead market are volatile and are often decided on the basis of an auction (bid-based market) among energy companies instead of bilateral agreements with an energy provider. in such cases, energy prices may change on a daily or hourly basis [cit] . it is then necessary to be able to predict the day-ahead evolution of the prices using past data; several time series analysis methodologies have been developed for that purpose -see [cit] and references therein."
a. cuda programming model nvidia introduced cuda (an acronym for compute unified device architecture) [cit] as a programming language for nvidia gpus [cit] . cuda comes with c for cuda application programming interface that gives programmers familiar with c an easy way to start writing programs that are executed on gpu. this is acchieved by extending c with different cuda commands.
"indeed, scenario-based stochastic model predictive control (ssmpc) has been shown to lead to remarkable decrease in the operating cost and improvement in the quality of service of drinking water networks [cit] . in ssmpc, the uncertain disturbances are treated as random variables on a discrete sample space without assuming any parametric form for their distribution [cit] . the scenario approach was identified in a recent review as a powerful method for mitigating uncertainty in environmental modeling related to water management [cit] . although this approach offers a realistic control solution as it is entirely data-driven, this comes with considerable computational burden as the resulting optimization problems are of particularly large scale [cit] . this has rendered the use of ssmpc prohibitive and has hindered its applicability. indeed, hitherto there have been used only conventional model predictive control approaches [cit], robust worst-case formulations [cit] and stochastic formulations where the underlying uncertainty is assumed to be normally identically independently distributed [cit] . note that it has been observed that demand prediction errors are typically follow heavy-tail distributions which cannot be well approximated by normal ones [cit] ."
after the initial values have been set we need to make the field values and their derivatives desynchronized by advacing the the scalar fields dt pr /2 steps forward.
"our implementation is available as an open-source and free software which can be readily tailored to the needs of different water networks modifying the parameters of its modules. the implementation is done entirely in cuda-c++ and it can be configured either programmatically or using configuration files. the adopted object-oriented programming model is amenable to extensions and users may specify their own predictive models, scenario trees, cost functions, dynamical models and constraints. our results are accompanied by extensive benchmarks and the software is verified with unit tests."
"besides solving preheating evolution cudaeasy can also be used after slight modifications to study the evolution of q-balls [cit], gravity waves, phase transitions, black hole production and many more interesting phenomena in early universe [cit] . with the computational power of gpus many of these problems can now be studied much faster than previously even on a common desktop computer without losing accuracy. we hope that cudaeasy is able to achieve this ambitious goal and be useful to the interested user."
the scenariotree class models the scenario tree structure that represents the uncertainty associated with the volatile energy prices and water demands. this class describes the structure of the scenario tree by assigning a unique index to each node and storing the indexes of the children runs the accelerated dual proximal gradient algorithm can computes a control action to be applied to the water network. computations are carried out on gpu and the output (flow set-points) can be stored in a json file.
reads from the global memory should also be coalesced: threads within a block are divided into warps that are made of 32 threads. threads within a half-warp (i.e. 16 threads) should read from a same segment of memory which size depends on the type of data. for example for floats this should be 128 bytes. if this requirement is not met there will be a performance penalty. the gpu also has a limited amount of fast shared memory that can be used to hide the latency of the global memory and to share data between threads within a block in order to save the memory bandwidth. the device also has registers and read-only texture and constant memories. a more accurate description of these can be found in the cuda programming guide. we will be using a range of these memories in order to optimize the execution of the kernel.
"the work proposed in this paper aims to improve the overall security, trust and privacy for implementation in boot loader or firmware of embedded devices [cit] . this paper focuses on using the diffie hellman and elgamal algorithm since the key exchange used is assymmetric as such is more secure than using symmetric key algorithm. elgamal algorithm in 32-bit computation was studied to deterrnine on how the data have been secured during the encryption and decryption using the elgamal algorithm. other than that, the objective of this paper is to implement data encryption using the elgamal algorithm with the 32-bit computation. as for now, previous research has been focused on finding out the maximum number that can be to computed using the elgamal algorithm in 32-bit computation. in an addition to that, computer capabilities in computing numbers greater than 2 32 was also investigated. based on the experiments that have been done and data that have been collected, the maximum number that can be computed using elgamal in 32-bit computation is limited to the number below 2 32 ."
due to the inherently parallel nature of computer graphics gpus are well suited for problems that are easy to parallelize. in ideal situations a gpu version of a program can be up to three orders of magnitude faster [cit] than the serial cpu version of the same program. speedups of this magnitude can have dramatic effects on how science is made and make some previously computationally difficult problems rather trivial.
"gpus were first developed for video applications and, due to the high demand in high-performance graphics, rapidly evolved to powerful hardware featuring hundreds of computation cores. nowadays, gpus are used for more than video processing and they are becoming popular for computational purposes including, but not limited to, environmental modeling [cit] . by design they are well-suited for data-parallel lockstep applications where the same type of operation is applied to different memory positions. instructions are sent to the gpu (from the cpu) in the form of compute kernels. gpus offer unprecedented parallelization capabilities provided that the program can be parallelized in a lockstep fashion (the same operation is executed on different memory positions). cuda is a parallel computing framework and application programming interface for nvidia gpus used for general-purpose computing. part of the cuda framework is cublas, a parallel counterpart of the popular linear algebra library blas."
"the proposed stochastic model predictive controller leads to measurable benefits for the operation of the water network. it leads to a more economic operation compared to methods which do not take into consideration the stochastic nature of the energy prices and water demands. in this paper, we assess the performance of the controlled network using three key performance indicators: (i) the economic index, (ii) the safety index, which quantifies the extent of violation of the safety storage level requirement and (iii) the computational complexity index with which we assess the computational feasibility of the controller. simulation results are provided using data from the water network of barcelona and the energy market of austria. the advantages of the adopted control methodology are combined with the computational power of gpus, which enables us to solve problems of very large scale."
"there are a few algorithms can be used as a technique to secure data through computer such as diffie-hellman key exchange and elgamal algorithms. elgamal is an improvement from diffie-hellman key exchange protocol. in order to prove the elgamal capabilities to secure data through a computer, an experiment have been performed and the result shows that an integer number equal or less than 32-bit is not capable to secure data through 32-bit computer system. for an improvement, to implement numbers bigger than 32-bit integer needs to use 64-bit computer. however, the usage of standard 32-bit or 64-bit gcc integer is not enough to meet security requirements for current cryptographic key strength which is at least 1024 bits. for that reason, implementation based on modern elgamal needs to use 2 1024 bits computing integer that is using gmp bignum library to replace standard gcc integer number. gmp library allowed for c and c++ codes to computes integer numbers greater than 32-bits for 32-bit or 64-bit computer."
we have presented a gpgpu implementation of a program that solves the evolution of interacting scalar fields in an expanding space. the main implementation has been done in the spirit of latticeeasy which means that users familiar with aforementioned program should find it relatively easy to start using cudaeasy. we have implemented most of the improvements that were first presented in defrost including making the initializations of the program consistent and using more advanced discretizations of differential operators. because of these and other improvements the program achieves roughly same precision as defrost while using single precision in the gpu calculations.
before advancing a thread block in z-direction we will do some additional calculations needed in the scale factor evolution: namely we need to calculate the averages of the squared gradient and the potential term in equation (10) . as explained previously the gradient term can be written in terms of the laplacian and since this has been already calculated the gradient term is trivial to implement in the kernel. we simply make a new variable (called gpe for gradient and potential energy in the program) and subtract the values of φ pr ∆ pr φ pr from it along the column the thread is advancing in z-direction. the potential terms are done similarly but they are written only for the last scalar field being evolved. we will employ additionally the kahan summation [cit] when calculating these sums in order to keep the numerical errors as small as possible.
the entities involved in rapidnet and the relationships among each other are illustrated in fig. 4 which correspond to classes in the cuda-c++ implementation of rapidnet (see also table 1 ).
"first, diffie-hellman key distribution scheme is reviewed [cit] . the secret is made by raising the prime number a to an exponent which can range from 2 until 32. in this case, since these experiments are testing until 32-bits integer so that the maximum numbers that are used are until 32. when the prime number p had a as a primitive root in the eq. (1) shows the range of values generated, (1) in this algorithm, there are two parties that wish to exchange a key indicated as a and b. random number x a for user a will be selected to calculate the public key y a in eq. (2). (2) similarly, user b will select random number x b independently for eq. (3) to calculate the public key y b.: (3) the x values will be kept as private for each side while the y values are available publicly to the other side. the calculation for user a using eq. (4), to calculate the shared secret is as follows: (4) and calculation of user shared secret for b in eq. (5)as follows: (5) this will produce the same result for both calculations, eq.(6), as proven below: (6) both of the users have now exchanged with each other their secret key. the opponents only know the values of p, a, y a and y b, since the value x a and x b are private . to determine the key, the opponents have to solve a discrete logarithms problem. discrete logarithm problem is very difficult to solve compared to exponential modulo a prime that is easier to calculate, that is a fact that security are based on diffiehellman key exchange algorithm.to know the value of x a and x b,the attacker has to solve the discrete logarithm as in eq. (7). (7) secret values can be established to be used in exchanging data in public network by using the diffiehellman key exchange algorithm [cit] ."
this algorithm fits into tseng's alternating minimization framework [cit] . it has been found to be suitable for embedded applications as it is relatively simple to implement and it has good convergence properties (the dual variable y ν converges with rate o( 1 /ν 2 ) and an averaged primal iterate converges at o( 1 /ν 2 ) as well) [cit] . it involves only matrix-vector operations (additions and multiplications) and it is numerically stable. in the following section we discuss how the involved operations can be massively parallelized in a lock-step fashion (performing the exact same operation on different memory positions) and how the algorithm can be implemented on a gpu.
"the engine is, in turn, linked with to a dwnnetwork entity which provides all necessary technical specifications for the network (topology, dynamics, constraints) and a scenariotree entity which encodes the probabilistic information associated with the prediction errors. note that the end-user does not have to create instances of engine or directly interact with it."
we presented the integrated software solution rapidnet for the control of drinking water networks which accounts for uncertainty both in predicted water demand and in predicted electricity prices in the day-ahead energy market. rapidnet is a highly inter-operable software as it can be interfaced using json files which follow a standard api which is detailed in the software documentation. it can be combined with any forecaster as the controller does not need to know the mechanism with which the forecasts are produced. rapidnet features a parser implemented in matlab which allows the conversion of epanet .inp files to json files. it is an open-source and free software which is distributed under the terms of the gnu lgpl v3.0 licence and can be downloaded at https://github.com/ gpuengineering/rapidnet.
once the laplacian is calculated we can proceed to the actual leap frog step i.e. equations (4) . the other terms beside the laplacian depend only on the local values of the scalar fields and can therefore be stored in registers. those terms in equation (6) that either depend on the scale factor or are constants such as any factors in the potential derivatives are stored in the constant memory since these are constant throughout the lattice during a leap-frog step. this way we can save some of the registers but also avoid any divisions on the gpu which are much slower than additions or multiplications. once the leap-frog steps have been taken the new values of the scalar field and its derivative are written into global memory and are used in the next step.
"in this paper, we present a software for the fast and efficient solution of such problems harnessing the immense computational capabilities of graphics processing units (gpu) building up on our previous work [cit] ."
"most modern numerical optimization algorithms such as the (accelerated) proximal gradient algorithm, the alternating directions method of multipliers (admm) [cit], the pock-chambolle method [cit], tseng's forward-backward-forward algorithm [cit] and many another require that the optimization problem be first written in a form"
"the residuals of the model where found to be uncorrelated at the confidence level of 99.9%. indeed, the residuals pass the ljung-box q-test of uncorrelatedness with p-value equal to 1.0000. the model was selected using the akaike information criterion (aic) with value 1.523."
"water demand has been the main source of uncertainty for the operation of drinking water networks and a lot of attention has been paid on the development of models for its prediction. prediction methodologies span from simple linear models [cit] to neural networks [cit] and support vector machines [cit], nonlinear multiple linear regression [cit], holt-winters-type models [cit], as well as more complex neuro-fuzzy models [cit] . increased predictive ability can be obtained using exogenous information such as weather forecasts [cit] and calendar data [cit] ."
"the validation of the software is done through unit testing. a unit represents the smallest functional part of the software and unit testing involves verification of its functionality through predefined inputs and expected outputs. it assists in the debugging and maintenance of the code and facilitates the integration of the various units reliably. in lack of a standardized testing framework for cuda applications, we developed our in-house testing framework. moreover, using cudamemcheck we have thoroughly tested for gpu-side memory leaks."
"in addition, for the sake of service reliability (availability of water when demand rises unexpectedly) and safety, it is required that the level of water remains above a certain level v j safe . this is allowed to be violated occasionally, when the demand happens to be too high."
"smpccontroller requires certain configuration parameters which is provided by the entity smpcconfiguration. there, the user specifies the desired tolerance, maximum number of iterations and can override other solver-specific properties. overall, the flow of information in rapidnet is shown in fig. 5 . the end-user initializes an smpccontroller object by providing the network topology, the controller configuration and a scenario tree. during real-time operation, the controller receives the network state x k (which can be provided in a json file) and, using a demand/price forecaster, computes a control action which is applied to the system."
"where w x is a positive weight factor. the scaling factors w α, w u, w s and w x are the tuning knobs of stochastic mpc as we shall discuss in the following section. the concept of scenario-based stochastic optimal control: at every time instant k, we make an optimal contingency plan by minimizing the expectation of a cost function v which encodes the operation cost along a finite prediction horizon."
"all operations involved in the computation of prox γg are element-wise operations and can be fully parallelized on a gpu; therefore, the computational cost for applying prox γg -or, what is the same -prox γ −1 g * via (33) is negligible."
"confidentiality, integrity and authentication (cia) are core principles of information security. [cit] . basically from donn parker the elements of information are divided into six. the examples of the elements are authentication, confidentiality, and authentication. confidentiality refers to the prevention of information disclosure to unauthorized system or individuals. the information that can be accessed is only limited to certain types of information. integrity refers to a respectable of character. integrity needs people to maintain and measure data consistency and accuracy over its entire life-cycle. in other words, the data cannot be unmodified, undetected and unauthorized. authenticity is necessary to make sure the data, transaction, communication or documents are genuine. in non-repudiation concept, the person that does the transaction need to responsible for the thing that have been done. let says, a is using b's account to a transaction. although b denying that it is not being done by him but b will responsible for the action that have been done by a [cit] ."
"in this section we present rapidnet, a cuda-c++ implementation of the accelerated proximal gradient method for the solution of scenario-based stochastic optimal control problems, particularly tailored to the needs of a drinking water network."
the initial values of the scalar fields are computed as presented in defrost [cit] i.e. the homogeneous scalar field values are given as an input and the random quantum fluctuations are created by the program. the main differences come from the use of cuda fast fourier transform (cufft) instead of fftw (an acronym for fastest fourier transform in the west) and from the fact that we need to transform the scalar fields into program units that the leap frog uses i.e.
recent developments in computer technology have made graphic processing units (gpus) available to scientists as a powerful coprocessor in numerical computations. for example the theoretical peak performance of a state of the art amd 5870 gpu is 2.7 tera floating point operations per second (tflops) in single precision [cit] compared to the peak performance 55.36 giga flops (gflops) of intel's core i7 [cit] . this difference in computational horsepower has made general-purpose computing on graphics processing units (gpgpu) an increasingly popular way to do numerical computations [cit] .
"the cost for the operation of the water network is quantified in terms of three individual costs which have been proposed in the literature [cit] : the economic cost which is related to the treatment cost and electricity required for pumping, the smooth operating cost which penalizes the abrupt operation of pumps and valves and the safety storage cost which penalizes the use of water from the reserves (i.e., allowing the level in the tanks to drop below the safety level)."
"function g is naturally chosen to be the indicator of the set of input constraints plus the total constraint violation penalty function v s . note, however, that the same variable x"
we would like to note that we have not (at least yet) implemented all of the functionalities of latticeeasy and defrost: namely the current version of cudaeasy doesn't yet produce any quantative data about the spectrum but this will be rather trivial to implement with the cuda fft as a post evolution procedure. this is however left to future versions of the program.
elgamal public-key encryption is a continuation of diffie-hellman key exchange protocol. [cit] . there are three main components which are the key generated; the encryption and decryption algorithms. elgamal is characterized as asymmetric cryptography and it is commonly known that the encryption and decryption speed is slower than in a symmetrical algorithm [cit] .
"provides essential functionality to smpccontroller and manages the gpu-side memory. smpcconfiguration contains configuration parameters that are relevant for smpccontroller (tuning parameters, solver tolerance, maximum number of iterations)."
"we present here two application examples of our approach. the scene captured by the camera (webcam) is augmented by virtual objects in real time. in the first example, the \"logitech quickcam pro 9000\" webcam is used with the resolution 320*240. in the second one; a \"sony vgp-vcc3 0.265a\" webcam is used with a resolution 640*480. images presented in figure9 and figure10 are selected arbitrarily from the webcams. according to these examples, we note that using 12 corners is sufficient to model the camera using a pinhole model; the virtual object is positioned in the desired place with a good accuracy and it size depends of the camera depth. we also note that the used of tracking technique (tracking of chessboard corners) gives good and acceptable results and the virtual object is well tracked across camera frame in real time. the last point is the efficacy of the developed technique of virtual object insertion; the virtual object follows the motion of the camera and it is positioned and drawn correctly without any information about intrinsic or extrinsic parameters of the camera."
"to ensure tracking of the virtual object, the camera position must be estimated for each view. several techniques can be used. they are classified in two groups: vision-based tracking techniques and sensor-based tracking techniques [cit] . most of the available vision-based tracking techniques are divided into two classes: feature-based and model-based. the principle of the feature-based methods is to find a correspondence between 2d image features and their 3d world frame coordinates. the camera pose can then be found from projecting the 3d coordinates of the feature into the observed 2d image coordinates and minimizing the distance to their corresponding 2d features [cit] . the features extracted are often used to construct models and use them in modelbased methods; edges are often the most used features as they are computationally efficient to find and robust to changes in lighting [cit] . the sensor-based techniques are based on measurements provided by different types of sensors: magnetic, acoustic, inertial or gps ... in augmented reality, they are mainly combined with techniques of the first group (hybrid tracking). these methods have the advantage of being robust to abrupt movements of the camera, but they have the disadvantage of being sensitive to environmental disturbances or a limited range in a small volume. recently, there is another classification more used to distinguish between vision-based tracking techniques: fiducial marker tracking and markerless tracking [cit] . in the first one, the fiducial marker is surrounded by a black rectangle or circle shape boundary for easy detection. markerless techniques are the other vision-based tracking techniques which don't use fiducial marker. the common thing between augmented reality applications is the necessity to calibrate the camera at first; if the user wants to change the camera, the resolution or the focal length (zoom lens), he must calibrate his camera again before starting working with it. this paper introduces a technique to model a camera using a robust method to find the correspondences without any need of calibration. we use the least squares method to estimate the perspective transformation matrix. for tracking, we use chessboard corners as features to track; these corners are extracted from each video image and used to estimate the perspective transformation matrix. this system does not need any camera parameters information (intrinsic and extrinsic parameters which are included in the perspective transformation matrix). the only requirement is the ability to track the chessboard corners across video images."
"the term augmented reality (ar) is used to describe systems that blend computer generated virtual objects with real environments. in real-time systems, ar is a result of mixing live video from a camera with computer-generated graphical objects that are recorded in a user's threedimensional environment [cit] . this augmentation may include labels (text), 3d rendered models, or shading and illumination variations. in order for ar to be effective, the real and computergenerated objects must be accurately positioned relative to each other. this implies that certain measurements or calibrations need to be made initially. camera calibration is the first step toward computational computer vision. the process of camera calibration determines the intrinsic parameters (internal characteristics of camera) and/or extrinsic parameters of the camera (camera position related to a fixed 3d frame) from correspondences between points in the real world (3d) and their projection points (2d) in one or more images. there are different methods used to estimate the parameters of the camera model. they are classified in three groups of techniques:"
"in this paper, a barrier lyapunov function based dynamic surface control is proposed in the position control loop of ehs to constrain the full system state errors in the corresponding desirable bounds. different from the quadratic positive definite function, the blf employed a logarithm function to describe the energy dissipation of ehs. to avoid violent control and chatter response, the dynamic surface is adopted to design a stabilizing filter function instead of the virtual control derivative in backstepping iteration. the effectiveness of tracking performance improvement has been verified in the comparison results with pi controller."
"foreground and background focused images of the bottle dataset are displayed in figs. 1(e) and 1(f). foreground focused image displays the focused bottle region and text on it. background focused image displays the small bottle and a wheel. mff can combine this diverse information into a single image. fused images of various mff methods with some regions of interest are highlighted with red rectangles in figs. 8 (a)-(i) . foreground region of bgs fused image (fig. 8 (c) ) is well focused. but, it fails to incorporate well focused background content. for example, small bottle and wheel regions are completely distorted. msvd fused image (fig. 8(d) ) is visually not good. sidwt (fig. 8(e) ) introduces artifacts into the fused image. as shown in figs. 8(b) and 8(f), grad and fsd fused images are looking good but quality of these fig. 8(i) the proposed method integrates bottle in foreground and small bottle, wheel in the background into the fused image with more visual clarity compared to that of the existing methods (figs. 8(a)-(h) ). left and right focused images of a parachute dataset are illustrated in figs. 1(g) and 1(h) respectively. left focused image pay attention to a big parachute on the left hand side. right focused image shows the remaining parachutes on the right hand side of the image. fused images of various methods are displayed in fig. 9 and some regions such as a portion of big parachute, text on another two small parachutes and a part in the right bottom are highlighted in red rectangles in all of these fused images for better analysis. bgs fused image (fig. 9(c) ) contains only right side focused regions. it failed to integrate left side focused big parachute into the fused image. grad and fsd fused images (figs. 9(b) and 9(f)) contain both left and right focused regions. but they are not able to give sufficient information. as shown in figs. 9(a), 9(d-e), 9(g-h) ), fused images of pca, msvd, sidwt, dchwt and ddct are looking good. however, as displayed in fig. 9(i) proposed fused image gives more focused regions. for example, as shown in red rectangles on the right hand side of the image, text on the parachutes and right bottom regions are more visible and clear than remaining fused images."
"comparison of fusion metrics api, mi, h, fs and nc for different methods along with the proposed method is presented in tables. [cit] . average values of api, mi, h, fs and nc over 4 image datasets are presented in table. 5. fusion metrics with highest value are highlighted in bold letters."
saliency information from out-of-focus images is extracted using msss detection algorithm [cit] . this algorithm is reviewed in section 3. the process of saliency extraction from source images n i is represented as:
"here, detail layers are scaled with help of weight maps n w calculated from msss detection algorithm and these scaled detail layers are combined to get the final detail layer d as shown below:"
"2) salient regions are calculated based on symmetric surrounds. hence it can effectively highlight the salient regions in images with complex background. in multi-focus images, focused regions provide more visual information than defocused regions because focused regions are more salient than defocused regions. so, these salient regions (visually significant regions) should be identified from the source images using sd algorithms. from the above discussion, we preferred sd using msss [cit] for multi-focus image fusion. the theory behind this sd algorithm is as follows:"
"experiments are carried on various multi-focus image datasets however due to space constraint results are presented for 4 image datasets namely flower, leopard, bottle and parachute. these datasets are displayed in fig. 1 for reference. these datasets are available at http://home.ustc.edu.cn/~liuyu1/."
"this condition denotes the relation between the blf and qlf, which can be used in theorem 1 and derive the ultimate stable condition of ehs (27)"
"fusion process can be done at three levels [cit] namely pixel, feature and decision. at pixel level, fusion is performed on each source image pixel by pixel. at feature level, features are derived from each image and fusion is performed on those extracted features. decision level is a high level fusion. at this level, local decision makers are derived from each feature of an image and fusion is executed on probabilistic decision information derived from local decision makers. pixel level fusion process is simple and very effective compared to other levels of fusion schemes. in this paper we concentrate only on pixel level fusion scheme."
"0.9885 (5) 0.9900 (4) 0.9835 (7) 0.9917 (1) a. qualitative analysis (fig. 6(o) ) is completely blurred. switch portions of pca (fig. 6(j) ), msvd ( fig. 6(m) ), sidwt ( fig. 6(n) ) and ddct fig. 6(q) ) are partially blurred. zoomed switch regions of grad (fig. 6(k) ), fsd ( fig. 6(l) ) are looking good. however, they are not providing enough details of the switch region. dchwt switch region (fig. 6(p) ) is also looking better. but, this method introduces extra information (artifacts) into the switch region. edges of the zoomed portion are distorted. it can be observed from the switch region (fig. 6(r) ) that the proposed method is able to integrate more focused regions with few artifacts compared to the state-of-the-art fusion methods."
"it can be noted that in tables. 1-4, no individual fusion algorithm is giving better performance in all fusion metrics for all image datasets. in addition, we can observe that no individual fusion algorithm has given the best fusion metric value for all the datasets. performance (fusion metric values) of a fusion algorithm is changing from one dataset to other. so it is difficult to judge from the individual dataset evaluation."
"in digital photography, each multi-focus image provides information about a particular focused region. we need to integrate all the focused regions into a single fused image. this can be done by properly choosing weight map of each source image. these weight maps should highlight the focused and defocused regions of the source images. fig.4 shows the weight maps of various image datasets flower, leopard, bottle and parachute. these weight maps represent the complementary information i.e, focused and defocused regions. for example, as shown in fig.4 weigh maps of focused and defocused regions are highlighted in red and green rectangles respectively represent the complementary information. these weight maps are calculated by normalizing the saliency maps as follows:"
the last important factor of this tracking technique is its robustness against the problem of lighting change; actually corners have the propriety to be detected in different lighting condition. figure 2 shows an example of corners detection in a very low lighting environment and figure 3 shows another example in a very high lighting environment.
to solve above mentioned problems we propose a new image fusion method based on two-scale image decomposition and saliency map detection using maximum symmetric surround. the advantages of the algorithm are as follows:
"but this method fails when source image contains complex background. it highlights the background along with the salient object because this method treats entire image as the common surround for all pixels in the image. this is not desirable because to detect a pixel at the center of the salient object it should contain small lower cut-off frequency or it should contain high lower cut-off frequency to detect a pixel near boundary. so as we approach image boundaries we should use local surround regions instead of common surround regions to detect a pixel. this can be done by defining surround symmetry around the center pixel of its own sub image near the boundary. this process can increase the lower cut-off frequency. msss saliency map detection [cit] of an image i of width w and height h, from eq(1) is defined as:"
"next category of interest is transform domain based fusion methods [cit] . gradient (grad), laplacian, filter-substrate-decimate (fsd), contrast, morphological difference pyramids are successfully used for the purpose of fusion [cit] . however, these methods may produce artifacts around boundaries in the fused image."
a. decompose source images into base and detail layers using an average filter. b. calculate saliency maps of each source image using msss detection algorithm. c. compute weight maps from extracted saliency map of each source image. d. scale detail layers with these weight maps and combine all the scaled detail layers to obtain final detail layer. e. compute final base layer by taking the average of all the base layers. f. take linear combination of final base and detail layer to get the fused image. this algorithm is explained in-detail in sub-sequent sub sections.
"the remainder of this paper is organized as follows. the ehs model is constructed in section ii. the controller with full-state constraints based on blf and dynamic surface is given in section iii. the comparative results verification of two controllers are given in section iv. finally, the conclusion is drawn in section v."
"motivated by the analysis in recent papers [cit] we have also considered the average value of the fusion metrics (calculated over 4 image datasets) in table. 5. it obvious from this table, that our algorithm is giving superior performance in all fusion metrics compared to that of state-of-the-art fusion methods."
"v. conclusion this paper introduces a technique of virtual object tracking in real time. this technique is developed as part of an augmented reality system and does not require any information or computation of the camera parameters (intrinsic and extrinsic parameters). it is based on both detection of chessboard corners and a least squares method to estimate the perspective transformation matrix for the current view of the camera. the use of this technique is simple and does not require initialization steps or manual intervention, the only requirement is the tracking of the marker (chessboard) across images provided by the camera. the results show the efficiency of the tracking method based on detection of chessboard corners; the major advantages of tracking corners are their detection robustness at a large range of distances, their reliability under severe orientations, and their tolerance of lighting changes or shadows. the technique used for the insertion of virtual objects gives its proof when the camera parameters are not calculated."
"in msif applications, source images are captured by using different modalities. for example in medical imaging, modalities like computer tomography (ct), magnetic resonance imaging (mri) are used to capture complementary information of the targeted scene of a human body. ct provides hard tissue information (bone structure) whereas mri gives the soft tissue information. the useful information from these complementary images should be integrated into a single image for better diagnosis and treatment."
"occultation problem between real and virtual objects are not treated here, but only the occultation between the virtual object components, i.e. the different faces which form the augmented object. the virtual cube shown in figure5 is inserted in a special order (face5, face3, face4, face2, face1 and face6). if this order changes, the inserted object does not appear as a cube. figure6 depicts a wrong presentation of a virtual object (house) when its faces are inserted in another order; the front faces are occulted by others since they are inserted in advance. note that this technique is not necessary when using the wire frame technique for virtual object insertion this problem can be avoided by estimating the camera viewpoint. this viewpoint consists of the orientation of camera estimation according to the direction of the 3d world reference axes. we note that this technique is not required if the virtual object is drawn by the wire frame technique. the technique is based on the use of a virtual cube put on the horizontal plan (oxz) where its vertices are s 1 (0,0,0),"
"a generic classification [cit] of image fusion is: single sensor image fusion (ssif) and multi sensor image fusion (msif). in ssif, single sensor is used to acquire multiple captures of the targeted scene whereas msif uses multiple sensors for the same purpose. these multiple images provide diverse information of the scene. it is very difficult to understand the scene from these multiple captures. so a single image should be generated out of these several images to provide more visual information than any one of the individual source images. digital photography applications (multi-focus fusion [cit] and multi-exposure fusion [cit] ) come under ssif category whereas applications such as concealed weapon detection, navigation, medical imaging, military, remote sensing fall under msif class."
"in this section, we discuss about the image database, various image fusion algorithms used for comparison and the analysis of free parameter (average filter window size w ) for optimal performance of the proposed method."
nc 0.9704 (5) 0.9627 (7) 0.9620 (8) 0.9735 (4) 0.9769 (2) 0.9595 (9) 0.9691 (6) 0.9737 (3) 0.9787 (1) (5) 113.5008 (4) 113.4811 (6) 113.4781 (7) 113.5030 (3)
"in this paper, a dynamic surface control method based on barrier lyapunov function is proposed for electro-hydraulic system to realize the full-state error constraints under the existed lumped uncertainties. firstly, the ehs model is constructed as a nonlinear strict-feedback structure. secondly, to achieve the full-state error constraints including the desirable position tracking accuracy, the rate-limit of the cylinder response and the load pressure boundary, the constraint holding technique of blf is used to restrict the system state errors in required constraints and all the states of ehs are guaranteed to be ultimate boundary. furthermore, the dynamic surface is adopted to avoid the explosion of complexity in backstepping design. the comparative results with the pi controller verify the effectiveness of the proposed controller."
"the model is a mathematical formulation which approximates the behavior of any physical device, e.g. a camera. there are several camera models depending on the desired accuracy. the simplest is the pinhole model. in an ar system, it is necessary to know the relationship between the 3d object coordinates and the image coordinates. the pinhole model defines the basic projective imaging geometry with which 3d objects are projected onto a 2d image plane. this is an ideal model commonly used in computer graphics and computer vision to capture the imaging geometry. the transformation that maps the 3d world points into the 2d image coordinates is characterized by the next expression: equation (1) can be simplified to:"
"to evaluate our method, several measures are performed using three cameras. all tests are performed on the rate frame of 20 hz with the resolution of 640*480. the quality of each measurement is estimated by computing the re-projection error, which is the euclidean distance between the identified corner and the projection of its corresponding 3d coordinates onto the image. about 100 calculations are performed for each case. the obtained results, which represent the average of each case are represented on the table1 and compared with results obtained by fiala and shu [cit] . according to table1, we note that our obtained results are very close from those obtained by fiala and shu [cit] ."
"figs. 1(c) and 1(d) are foreground and background focused images of the leopard dataset respectively. foreground focused image conveys the focused regions such as legs, claw of the leopard and a portion of the tree. background focused image conveys the information of head, body regions of the leopard and the background. to know the entire information in the scene, all the focused regions have to be combined in the fused image. figs. 7(a)-(i) give the visual display of various methods with the claw of the leopard highlighted in red rectangle. bgs fused image (for example claw region) (fig. 7(c) ) is visually distorted. grad, fsd fused image results (figs. 7(b) and 7(f)) are not satisfactory. here ddct fused image (fig. 7(h) ) is good. as shown in figs. 7(a), 7(d), 7(e) and 7(g), pca, msvd, sidwt and dchwt fused images are partially blurred. as shown in fig. 7 (i) our proposed method is able to generate visually a good focused image for this dataset as well."
if both the sources images are equally symmetrical to the fused image then the value of fs is closer to 2 then the quality of the fused image will be good.
"we have ranked each fusion algorithm based on its performance. fusion algorithms with highest fusion metric value is given rank 1. there are total nine fusion algorithms including the proposed method. so, nine ranks are given as shown in tables. 1-4. it is easy to observe that the rank of our algorithm is good and stable with not much fluctuation, unlike other algorithms."
"proposed method is compared with spatial domain fusion algorithms such as pca [cit], bgs [cit] and transform domain fusion algorithms like grad [cit], fsd [cit], sidwt [cit], dchwt [cit], ddct [cit] and msvd [cit] . default settings are adopted for all of these methods."
"with the proposal technique, the extrinsic parameters are not required to be calculated. this characteristic prevents the use of some famous 3d graphics libraries like opengl or openvrml. nevertheless, other standard libraries provided with any development environment like c++builder or delphi can be used to create a simple 2d or 3d objects."
in this section a comparative analysis of the proposed method with different image fusion methods is done in terms of visual quality (qualitative analysis) and fusion metrics (quantitative analysis). (2) 1.8981 (7) 1.8995 (6) 1.9569 (4) 1.9481 (5) 1.7860 (9) 1.8762 (8) 1.9618 (3)
"the sub images obtained using eq(3) and eq(4) are the maximum symmetric surround regions for a given central pixel. in multi-focus images, focused regions provide visually more information than defocused regions. in other way, focused regions are more salient than defocused regions. so we need to detect salient regions from these out-of-focus images using sd algorithms. we observe that the msss saliency detection algorithm is able to extract salient regions of the multi-focus images. the multi-focus datasets used for simulations are shown in fig. 1 and their corresponding saliency maps are displayed in fig. 2 . the process of saliency extraction using msss algorithm is denoted as:"
"since the cylinder position x 1 is a physical state, andẏ d, y d are bound from assumption 1, thenβ 1 is also bound by β 1 max . furthermore, from assumption 2 and (7), β 2 is also bound. according to assumption 1 and the physical states x 2, x 3,β 2 is also bound by β 2 max . for convenient description, v is converted into the cascade elements as follows"
"an image fusion algorithm can be evaluated from the visual quality and by using a set of fusion metrics. we consider traditional image fusion metrics such as average pixel intensity (api) [cit], mutual information (mi) [cit], entropy (h) [cit], fusion symmetry (fs) [cit] and normalized correlation (nc) [cit] . their mathematical expressions are given below."
"the common nonlinear backstepping method exists the explosion of complexity [cit] since the virtual control variable has to been repeatedly calculated its differentiation in backstepping iteration. these high-order derivatives of virtual control variables will enlarge some noises in the control input which leads to violent control and output chatters [cit] . to solute this difficulty, the dynamic surface is designed to transform repeatedly calculated derivative of virtual control [cit] into stabilizing filter functions. the adopted controller with dynamic surface can eliminate the severe proliferation and system singularity to guarantee fast convergence and satisfactory dynamic behavior [cit] ."
"according to lemma 1, the system state error z is a fullstate constrained vector, which is restricted in respective prescribed error boundary as shown in (5) by the afterward mentioned blf (the logarithm elements in (10))."
"remark 2 [cit] : the external load f l is unknown bound variable, which is the driving force of someone mechanical plant. furthermore, f l depends on the motion variables of the hydraulic cylinder y,ẏ,ÿ. certainly, f l is bound by an unknown bound constant"
the organization of the remaining paper is as follows: section 3 reviews the saliency extraction algorithm. section 4 explains the proposed methodology. section 5 briefs various fusion metrics. section 6 presents the experimental setup. section 7 discusses the results. section 8 concludes the paper.
"where c d, w are the discharge coefficient and the area gradient of the servo valve respectively, ρ is the hydraulic oil density, c tl is the leakage coefficient of the hydraulic cylinder, β e is the effective bulk modulus of the hydraulic oil, a p is the annulus area of the cylinder chamber, v t is the half-volume of the whole cylinder, m and k are the load mass and spring constant, b is the viscous damping of the hydraulic oil, u is the control voltage of the servo valve, sgn(·) is the sign function. remark 1 [cit] : some hydraulic parameters c d, ρ, k, b, β e, c tl are often uncertain positive constants, but the others are known."
"the ehs consists of one motor, one fixed displacement pump, one servo valve, one symmetrical cylinder, and one relief valve as shown in fig. 1 . the external load f l of ehs is a force or torque driving one mechatronic plant. the pump is driven by the motor and outputs the supply pressure p s, which is considered as the pressure threshold of the relief valve. the load pressure on the hydraulic cylinder p l should compensate the external load f l and control the cylinder motion when the servo valve throttles the flow q l in both chambers of the cylinder. volume 6, 2018 since the cutoff frequency of servo valve is very greater than the motion frequency of the cylinder, the dynamic model of servo valve can be neglected in ehs model construction [cit] . thus, if the state vector is defined as ["
"a single image fusion algorithm may not always give better performance in all fusion metrics for all the image datasets. an algorithm which gives better performance for one dataset may not give the same performance for another dataset. some fusion algorithms may give better results for few fusion metrics alone. in other fusion metrics these methods may fail. so, fusion algorithm should be assessed by taking the performance of all the fusion metrics into the consideration."
"single sensor is not able to focus more than one object present in the scene at the same time because of inherent system limitations. so, several images with different focuses have to be captured and an all-in-one focus image has to be generated from these multiple images by the fusion process. similarly, over and under exposed images provide visually less information so a well-exposed image can be generated by the process of fusion."
"it indicates the amount by which the resultant image is relevant to the given input images [cit] and is measured as with these fusion metrics in hand, we evaluate the performance of the proposed algorithm. for better quality of the fused image all these fusion metrics have to be high."
"the rest of the paper is organized as follows. section ii describes related work. section iii discusses and evaluates the proposed tracking approach using a chessboard pattern. in section iv, we present how virtual objects are inserted into real scenes and we give some results. finally, in the last section, conclusions and discuss of results are given."
theorem 1: consider the dynamic surfaces (7) together with the stabilizing filter functions (8) for the ehs model (2) under assumptions 1-2. if the initial compact set of the system state errors z is z (0) ⊂
fused image cannot be judged exclusively by seeing the fused image.so far we have discussed qualitative analysis. now we will discuss the quantitative analysis of different fusion methods.
"numerous people visit bird sanctuaries to glance at the various bird species or to praise their elegant and beautiful feathers while barely recognizing the differences between bird species and their features. understanding such differences between species can enhance our knowledge of exotic birds as well as their ecosystems and biodiversity. however, because of observer constraints such as location, distance, and equipment, identifying birds with the naked eye is based on basic characteristic features, and appropriate classification based on distinct features is often seen as tedious. in the past, computer vision [cit] and its subcategory of recognition, which use techniques such as machine learning, have been extensively researched to delineate the specific features of"
"in the future, we intend to develop a method for predicting different generations of specific bird species within the intraclass and interclass variations of birds and to add more bird species to our database."
"the iob is a crowdsourced metasearch-engine database specifically for birds, where any individual can store bird images and instantly retrieve information about the birds therein. uploaded bird images are identified from extracted features. this platform encourages individuals to become involved in birdwatching and to enrich their knowledge of various bird species. the iob is available online for free (with keyword: who cares? keep walking). fig. 1 shows the app interface. because a fall detection module is embedded in the system, the app also serves as a wellness platform to assist individuals in staying safe while birdwatching. in addition, the system can track the distance individuals cover from their daily physical strides using a pedometer to promote fitness and motivate users to walk while birdwatching [cit] ."
"using the aforementioned client-server computing setup, we provided a mechanism to encapsulate the cloud and mobile session. bird recognition can be executed through cloud-and device-based inference. in this approach to deep learning inference on a mobile device, the trained model parameters are loaded into the mobile app, and the computations are completed locally on the device to predict the image output. the mobile phone is constrained by memory size and inflexibility when updating the trained model. however, in the cloud-based deep learning model, the trained model is stored on a remote server, and the server connects to the mobile device via the internet using a web api to predict the uploaded images. therefore, deploying the learned architecture with the cloud-based model can be easily ported to various platforms or mobile phones, and can upscale the model with new features without much difficulty. because of the aforementioned benefits, cloud-based inference was used to execute bird image recognition. fig. 6 shows the proposed system for bird information retrieval from the trained model stored in the workstation. the server with the tf platform takes prediction requests for bird images from client mobile phones and feeds and processes in the deep learning trained model the images sent from the api. after an image has been predicted, the tf platform classifies and generates the probability distribution of the image and transmits the query image result back to the user's mobile phone with the classified label."
"the research on the model-free tuning problem is still at the developing stage and there are many issues that have not fully explored. we would like to provide two interesting ones that are valuable to be further studied. one is to incorporate delay estimation in the recursive tuning procedures. in process industries, most processes contain delays and some of them contain very large time delay. although the proposed algorithm works well for systems with small time delay, it becomes inaccurate for systems with large latency. therefore, combining delay estimation procedure is of great importance. second, more attention should be paid to the disturbance attenuation. as well known, the robustness of the control systems is of top priority for end-users. when disturbance exists, it is necessary to take some additional treatment to extract the disturbance information and hence improve the control performance."
"to acquire the output of images with or without birds, the multiscale sliding window strategy was applied so that the extracted subwindow could define the target object categories. the base learning rate was 0.01 and subsequently shifted to 0.0001. the network was trained until the cross-entropy stabilized. skip connections were implemented when the input and output layers had equal weights. for instance, when the dimensions of the output were increased, the weights were concatenated in a deeper layer to capture and reconstruct features more effectively in the next layer. we compared different α parameters to check their influences on the final model. table 5 compares the performances of different α values in identifying whether a bird appears in an image. in these experiments, 3563 images were split into sets of 80% for training and 20% for testing. the comparisons reveal that a high α increases the redundancy in the model; therefore, we set the α value to 0.5, which resulted in average accuracies of 100% and 99.7% for the training and test datasets, respectively."
"for given f 0, f 1 and f 2, the controller parameters, i.e., k, t i and t d, are computed by,"
"the emergence of deep learning [cit] algorithms has resulted in highly complex cognitive tasks for computer vision and image recognition. recently, deep learning models have become the most popular tool for big data analysis and artificial intelligence [cit], outperforming traditional image classification algorithms, and they are currently being downscaled for feasible mobile implementation. the proposed deep learning model for bird image classification using the cnn framework is described as follows."
"where h is the amount of misalignment, and τ is the additional z torque that we shall discuss below. we hypothesize that there are two mechanisms that prevent large angle part rotation. when the probes start to squeeze the part, the part will start to rotate. the indentation of the probe into the part will cause a reaction force in the y direction which will generate a z torque countering the moment due to the misaligned squeeze forces. there is also the adhesion force along the y direction that could also generate a z torque. determining the precise mechanism would require further experimentation. as an order of magnitude calculation, the torque due to misalignment is f s h where f s is the squeeze force 2kδ and the counter torque due to adhesion force is 2f a where f a is the contact adhesion force. to prevent undesired rotation, we would require"
"the assembly process described above uses vision feedback to locate the probes, die, and parts, and to position and move them to desired locations. joint position feedback alone is not sufficient to achieve the required accuracy."
"the assembly process involves two tasks: 1. leg pick up, rotation, and insertion 2. insertion of platform into top of the legs the first task is performed autonomously with minimal user supervision. the second task is performed by teleoperation at the present. this section presents the autonomous leg manipulation strategy using multiple-camera vision feedback."
"the high zoom allows for resolutions of around 1.25 µm per pixel, but a limited depth of field in both cameras. for this reason it is challenging to calibrate a multi-camera vision model for this configuration. instead, the two cameras are used independently with relative measurement operations rather than attempting absolute position localization in the world frame."
"in this subsection, we explain using a high-resolution smartphone camera to identify and classify bird information [cit] based on deep learning. to complete the semantic bird search task, we established a client-server architecture to bridge the communication gap between the cloud and mobile device over a network. the entire setup was executed in the following manner:"
"in the future, we intend to develop a method for predicting different generations of specific bird species within the intraclass and interclass variations of birds and to expand bird species to our database so that more people can admire the beauty from watching birds."
"because of the relatively early stage of this research, large tolerances were used for the slots to ease assembly. while this makes insertion simpler, it also can cause the posts to tilt to the side before assembly (see figure 11 (f) ). it was necessary to manually \"poke\" the posts vertical before placing the top piece. while this step is necessary for the experiment described, it may not be necessary in the future as tolerances are improved or the parts are glued in place immediately after insertion."
"after the fc layers were added, the n-softmax layer activation function [cit] was added; here, n is the number of bird categories. the softmax layer yields a probabilistic interpretation of multiple classes. each label corresponds to the likelihood that the input images are correctly classified using vector-to-vector transformation, thereby minimizing crossentropy loss."
"(11) θ contains all the controller parameters and η contains all the noise filter parameters. the estimated parameter vector at time k will be noted as(·) k, e.g.,θ k is the estimate of θ at time k."
"the purpose of this filtering is to decompose the estimation of θ and η into two parts, which will be seen soon after. the input and output signals are both pre-filtered by the noise filters and so the noise effect is filtered out at each recursive step."
"in this paper, we aims at solving these issues by proposing a model-free tuning approach with recursive implementation procedures. following the idea of the vrft methods, we first formulate the tuning problem as an optimization problem; then we discuss and show the characteristics of the colored measurement noise. to reduce the negative effect of the noise, we employ the refined instrumental variable (riv) method to solve the parameter optimization problem. for pi and pid control, different notations and implementation steps are also provided."
"the aim is to re-tune the pid parameters such that the closed loop system settles down within 80 seconds. to achieve this goal, we pick the target system g * cl as"
"to implement the skip connection in the network, downsampling is performed by conv3 and conv4 with a stride of 2. we directly use skip connection when the input and output have the same dimensions. when the dimensions of the output are increased, the shortcut performs identity mapping with an extra zero-padding entry for increasing dimensions. two fc layers were implemented with the same 4096-dimension configuration to learn the gradient descent, compute the target class scores in the training set for each image, and localize objects positioned anywhere in the input image. a schematic of the convnet architecture is presented in fig. 4, and the parameter configuration for convnet is provided in table 2 ."
"where the target deformation, δ, needs to be carefully chosen through repeated experimentation. if it is chosen too low, the part cannot be lifted; if it is chosen too high, the part flies off. our threshold of 10 µm corresponds to between 1 mn to 3 mn of grasping force based on the modulus of the silicon part. note that if the two probes are synchronized, then from (1), there will be no net motion. if there is a time delay between the two probe controllers, the feedback gain k p needs to be chosen small enough to avoid oscillation. with the properly chosen deformation value (i.e., grasping force), the part/probe contact is essentially rigid, i.e., the contact can support full force and torque, provided that the values are not too large. the light weight parts in this application are less than 1 µn. in section iv, the amount of contact force due to contact adhesion (van der waals and capillary forces) is estimated to be about 50 µn. therefore, the rigid contact assumption, in the absence of other external forces is a reasonable one. in figure 5, the part is supported by a single probe (this is achieved by first grasping the part using both probes, and then removing one of them), further demonstrating the rigid contact at the probe/part contact. in this case, only the contact adhesion force is present to counteract the small inertial force of the part. due to the positioning accuracy (around 1.2 µm) and image pixel resolution (between 1.5 µm and 3 µm, depending on the zoom level), there may be a slight misalignment of the probe in the y direction as shown in figure 6 . in this case, a moment about the vertical axis will be generated. indeed, in our experiments, we have observed that the parts will sometimes rotate by a small angle. what prevents the part from continuing to rotate beyond this angle? assuming negligible inertia, the equation of motion is given by:"
"the structure was assembled using a combination of autonomous and telerobotic assembly. the three post grasp, rotation, and insertion sequences are based on the automated sequence. the post alignment and top placement was accomplished through teleoperation. automatic implementation of this step is completely feasible. we are developing a general automated microassembly planner with this structure as the first test case."
"in this section, we show two examples to validate the effectiveness of the proposed approach. the first example is a dynamic system written in the form of the first-orderplus-dead-time (fopdt) model,"
the other use for high zoom is visual force sensing during the grasp. it is possible to detect the actual position of the probe and compare it to the commanded position of the actuators [cit] . this provides a sufficiently accurate estimate of the force to generate a repeatable robust grasp. without this force information generating a grasp is nearly impossible.
this study developed a mobile app platform that uses cloudbased deep learning for image processing to identify bird species from digital images uploaded by an end-user on a smartphone. this study dealt predominantly with bird recognition of 27 taiwan endemic bird species. the proposed system could detect and differentiate uploaded images as birds with an overall accuracy of 98.70% for the training dataset. this study ultimately aimed to design an automatic system for differentiating fine-grained objects among bird images with shared fundamental characteristics but minor variations in appearance.
"object recognition with a high-level feature extraction architecture comprises the following steps: (1) data content analysis, in which all generic raw data are preprocessed to extract nonlinear transformations and to fit the parameters into a machine learning model for feature extraction; (2) optimal probabilities of relevant structural information from each tuned parameter are clubbed into a new array of classifiers; and (3) a prediction is made based on trained and learned parameters. to extract multiple feature levels from raw data and evaluate the performance of the cnn for the dataset [cit], the dataset was split into the three modules discussed as follows. (1) the training dataset comprised raw data samples that were incorporated into the training model to determine specific feature parameters, perform correlational tasks, and create a related classification model. (2) the validation dataset was used to tune hyperparameters of the trained model to minimize overfitting and validate performance. the model regularizes early stopping to prevent overfitting and to enhance learning when the precision of the training dataset increases while the error of the validation dataset remains the same or decreases. (3) the test dataset was used to test the classifier parameters and assess the performance of the actual prediction of the network model. once the features had been extracted from the raw data, the trained prediction model was deployed to classify new input images. fig. 2 shows the module for extracting unique features of birds with the cnn and predicting the most classified labels for the input images. table 1 provides a list of terms and related abbreviations commonly used in this study."
"when images are learned, deep neural network models train a base network from scratch to identify associations of features and patterns in the target dataset. features are transformed from general to specific by the last layer of the network to predict the outputs of newly imposed inputs. if first-layer features are general and last-layer features are specific, then a transition from general to specific must have occurred somewhere in the network [cit] . to address and quantify the degree to which a particular layer is general or specific, we proposed adding skip connections [cit] among corresponding convolutional layers, as shown in fig. 3 . the skip layer connections should improve feature extraction through weighted summation of corresponding layers as follows:"
"the top camera only provides the x-y information of the probe tip and the part to be manipulated. the z-axis information requires the side camera. as a result, our strategy is to decouple the vision guided motion into x-y plane motion and z direction motion. note that due to the light weight of the part (less than 1 µn) and the low speed operation, only the kinematics need be considered. the post insertion is based on our previous work [cit] ."
"to filter nonbird images uploaded to the system automatically and to validate the effectiveness of the proposed system, 100 bird images were uploaded from a mobile phone for preliminary testing. the model achieved 100% accuracy in classifying the images as true bird pictures. table 4 shows the bird detection results."
"for the case of pi control, the recursive riv algorithm can be obtained by replacing θ, ψ k and φ k with their counterparts as above defined. the algorithm details are similar as algorithm 1 and hence omitted here for brevity."
"in this study, we developed an automatic model to classify the 27 endemic birds of taiwan by skipped cnn model. we performed an empirical study by the skip architecture. the intuition behind using the skip connections is to provide uninterrupted gradient flow from the early layer to later layer, so that it can resolve the vanishing gradient problem. we compared the performance of various models such as cnn with skip connections, cnn without skip connections, and svm. cnn with skip connections outperformed the other two algorithms. however, in this study, we are more focused on predicting the 27 species of bird endemic to taiwan more efficient and effective. the proposed model can predict the uploaded image of a bird as bird with 100% accuracy. but due to the subtle visual similarities between and among the bird species, the model sometime lacks the interspecific comparisons among the bird species and eventually leads to misclassification. in average, the test dataset yields 93.79% of sensitivity, 96.11% of specificity and this model can be used for prediction and classification of the endemic bird images. volume 7, 2019 the proposed architecture encountered some limitations and has room for improvement in the future. sometime the model confused the prediction of endemic birds when the uploaded bird images shared similar colors and size. if most bird species within a district need to be retrieved from the system, the database must be updated and need to be retrained with new features of the birds. for extending the proposed system to some specific districts for birdwatching may encounter imbalanced distribution of the dataset among the bird species if only a small size of dataset is available."
"the different features in the workspace require different lighting conditions in order to be extracted by computer vision algorithms. the overhead microscope is equipped with a coaxial illumination and a ring light. the coaxial illuminator is used to detect the gold reference marks on the die, the probes, and the slots for insertion. due to the parts being made out of the same material as the die, they are not directly visible with this illumination. the ring light illumination highlights the edge of the parts and is used for part detection. figure 10 shows the same view with the two different illumination sources. the front camera is equipped with a separate ring light. figure 11 shows snapshots of different points in the assembly of the structure. the assembly starts at (a) with the parts lying in the pickup area. next, a part is centered and the zoom is increased. a part grasp is initiated by pressing the probe against either side of the part as shown in (b), until the desired deformation (squeeze force) is achieved. the part is slightly lifted off the surface (c) by coordinating the motion of both probes. the part is next pressed against the third passive probe to generate the out-of-plane rotation. panel (d) shows the part before the rotation; and (e) after. panel (f) shows the three posts after insertion into the reception slots. because of the loose tolerances of the slots the parts do not sit straight. it is necessary to \"poke\" them vertical before the top piece can be installed. panel (g) shows the posts in their vertical configurations and the top piece being held over the posts. (h) shows the final assembled structure from the front camera, and (i) shows the structure from the overhead camera."
"the remainder of this paper is organized as follows. section ii briefly reviews related approaches for fine-grained visual categorization. section iii describes the various types of dataset used for feature extraction. section iv focuses on the deep learning model and its features used in object part models, and describes the correlation between part localization and fine-grained feature extraction. section iv also describes various correlation requirements, such as data augmentation, for excellent performance, localization, segmentation, identification of subcategories, as well as the requirement of a classifier for effective object prediction."
"since the middle of last century, the proportional-integralderivative (pid) controllers have become the most popular controllers in different process industries and nowadays pid controllers still dominate the industrial control area. over the decades, people have made great efforts on the research of pid controllers; numerous books and papers can be referenced on how to design a pid controller. moreover, many commercial software, such as, emerson deltav insight, honeywell profit pid, supcon pid-suite, etc., were published to assist engineers in pid controller design. despite the progress made in the previous work, pid tuning is still often an non-trivial task in industry, especially for the users without control background. one important reason is that the traditional pid tuning approaches are either model based or rely on the testing data with sufficient input excitation. however, in many situations, people are not allowed or impossible to carry out the required tests, which brings with the development of model-free tuning approaches for the pid controller design."
"3: compute u f * k and y f * k with (4), (12) and (13). 4: compute the iv variablex * k . 5: construct the data vectors ψ k, φ k using (14)- (15). 6: update the pid controller parameters,ê"
"this paper has briefly reviewed the model-free tuning of feedback controllers. the practical issues were addressed and we proposed a new model-free tuning approach to compute pid parameters with recursive implementation procedures. the side effect of measurement noise was analyzed. a recursive riv algorithm was provided. the simulation examples have shown that the proposed method can be applied to nonlinear systems and the system with time delay. especially, when initial pid parameters are badly tuned, the proposed algorithm still works fine."
"the procedure presented is highly experimental and is run at a very slow rate. the post insertion procedure for centering, grasping, flipping, and inserting takes between 8 to 10 minutes per post. the top placement for this particular experiment took 17 minutes. the overall experiment lasted 3 hours including setup. this time is far longer than a final commercial device would require to assemble the parts. the grasp dynamics are very fast and could potentially move the part at a higher velocity. with faster cameras and stages the assembly time in a commercial application could potentially be in the ten minute range, but this is very difficult to predict until faster systems are tested."
"assembling the top part was accomplished through telerobotic operation after all three of the posts had been assembled. the assembly process is simply to grasp the part, move it into position, align, and place on top of the posts. the part is normally moved with synchronized probe motions with slight differential movements to align at the end. this process was done manually to demonstrate the feasibility of the process. repeatability of this process is low at the moment due to the human operator attempting to directly estimate relative position and orientation from the two views."
multiple-camera vision is implemented through the use of two cameras. the overhead camera provides a top-down view of the workspace. its image plane is parallel to the die surface. the second camera is a perspective camera providing a front view. this camera is rotated about the x-axis 70
where the e(s) is a white noise signal with zero mean and its variance equal to 1. the signal to noise ratio (snr) is approximately 30db.
the everyday pace of life tends to be fast and frantic and involves extramural activities. birdwatching is a recreational activity that can provide relaxation in daily life and promote resilience to face daily challenges. it can also offer health benefits and happiness derived from enjoying nature [cit] .
"time ( works very well although the initial pi parameters give unstable step response. moreover, it is worth pointing out that the target system is nonlinear, but the proposed algorithm is still able to catch the system characteristics. compared with the model-based tuning methods, the proposed tuning method can avoid the risk of model mismatch that usually happens at the identification step."
"where tp, fp, tn, and fn represent true positive, false positive, true negative, and false negative rates, respectively. accuracy denotes the ratio of correctly detected bird images in the entire dataset, sensitivity indicates the ratio of correctly detected birds based on the bird images, and specificity is the true negative rate of the images that are bird images. the performance evaluation of the bird images is shown in table 6 . the average sensitivity, specificity, and accuracy were 93.79%, 96.11%, and 95.37%, respectively."
"for the part rotation, the external force due to the pressing against the passive probe should be large enough to overcome the adhesion force so the contacts now become point contacts with friction (i.e., spherical joints). the motion of the part should be coordinated to ensure it is perpendicular to the part and towards the passive probe. we have implemented this by pre-scripting the motion, but it may be performed using vision feedback as well, to afford an additional level of robustness."
"note that, in the presence of the colored noise, the standard least squares methods cannot maintain the estimation consistency in solving the optimization problem in (6) and the estimated controller parameters may give unsatisfactory control performance. therefore, in this case, we'll consider an alternative method, i.e., the refined iv method, to estimate tuning parameters."
"assembly of microscale structures has been the focus of significant studies in recent years [cit] . numerous concepts have been proposed to move beyond planar microelectro-mechanical system (mems) devices to spatial mechanisms through the assembly of parts manufactured using bulk micromachining processes. these efforts have produced functional spatial structures that would be difficult if not impossible to fabricate using the traditional photoresistetch procedures. however, they are passive structures that are either fixed to the substrate, or mounted on a moving component that has been bulk micromachined."
"based on the proposed tuning approach, we will now provide the recursive procedure to solve the optimization problem in (6) with consideration of measurement noise added in the output channel, see fig. 2 .1. we shall first look at the characteristics of noise and then introduce the refined recursive instrumental variables (riv) algorithm to calculate the tuning parameters."
"in this study we also compared the performance of three methods such as cnn with skip connections, cnn without skip connections, and svm with endemic bird dataset. the performance comparison of the model is set with learning rate of 0.00001 and 100 epochs. for the svm, we used the models were implemented in python 3.6.6 using the scikit-learn version 0.18.0 package. a comparison among the three models for the training dataset is shown in fig. 10 . the proposed cnn with skip connections achieved higher accuracy of 99.00% than that of 93.98% from the cnn without skip connections, and 89.00% from the svm. this validated the effectiveness of the presented model with skip connections."
"before measuring the efficiency for the test dataset, we randomly selected numbers of images, increasing from 10 to 100 images (with an increment of 10), from the test dataset to predict their highest and five highest accuracies. if the model's top guess matched the target image, then the image was listed as top-1. similarly, if the model predicted the target bird at least among its top five guesses, then the image was listed as top-5. results from 10-fold cross-validation showed that accuracies of top-1 were 91.20%-95.20% and of top-5 reached 93.00%-98.50% for test bird images."
"the associate editor coordinating the review of this manuscript and approving it for publication was biju issac. objects, including vegetables and fruits [cit], landmarks [cit], clothing [cit], cars [cit], plants [cit], and birds [cit], within a particular cluster of scenes. however, considerable room for improvement remains in the accuracy and feasibility of bird feature extraction techniques. detection of object parts is challenging because of complex variations or similar subordinate categories and fringes of objects. intraclass and interclass variation in the silhouettes and appearances of birds is difficult to identify correctly because certain features are shared among species."
"recently, some fine-grained visual categorizations methods have been proposed for species identification, and they have become a promising approach within computer vision research, with applications in numerous domains [cit] . numerous fine-grained recognition datasets, such as imagenet, ilsvrc, caltech-256, and cub 200, have trained models with a wide variety of data to extract global features such as colors, textures, and shapes from multilabel objects [cit] . many approaches have been applied for generic object recognition [cit] . some methods apply local part learning that uses deformable part models and region-cnn for object detection [cit], generation of a bounding box, and selection of distinctive parts for image recognition. some studies have focused on discriminative features based on the local traits of birds [cit] . simultaneous detection and segmentation are used to localize score detections effectively [cit] . pose-normalization and model ensembles [cit] are also used to improve the performance of fine-grained detection by generating millions of key point pairs through fully convolutional search. discriminative image patches and randomization techniques are integrated to distinguish classes of images and prevent overfitting [cit] . the present work also approached the learning of discriminative image features using a cnn architecture for fine-grained recognition. however, a complementary approach using domain knowledge of general bird features was integrated to provide detailed information about the predicted bird."
"in addition, we introduce another two pre-filtered input and output signals: u f k * and y f k *, which are expressed as,"
"the input and output signals are collected and displayed in fig. 2. fig. 3 fig. 4 shows the step response of the closed loop system with the initial and updated controller parameters. by using the new control parameters, the closed loop system performs as desired, although the collected data is relatively noisy and the system has a time delay."
"the second example shows the level control of a water tank, see fig. 5 . water is pumped into the tank at the top at rate of flow of ku(t) m 3 /sec and the water flows out of the tank through a hole at the bottom at the rate of flow of a √ 2gy m 3 /sec, where a, y and g are respectively the area of the bottom hole, the water level and the gravitational acceleration. a is the cross-sectional area of the tank."
"ultimately, information obtained from a bird image uploaded by an end-user, captured using a mobile camera, can be navigated through the client-server architecture to retrieve information and predict bird species from the trained model stored on the server. this process facilitates efficient correlation of fine-grained object parts and autonomous bird identification from captured images and can contribute considerable, valuable information regarding bird species."
"motivated by the planar actuated platform developed at nist [cit] as shown in figure 1 mechanism is that the platform has full 6 degree-of-freedom (6-dof) motion while the actuation is entirely planar and the joints are all flexure based, which are easy to manufacture in small scale. our approach is to apply our 3 dimensional (3d) assembly technology [cit] for the construction of this structure."
"1) probe positioning in x-y: move the probes in the x-y plane until they are next to the specified grasp point (in the x-y plane, about 5 µm away from the part edge). 2) probe positioning in z: move the probes in the z direction using the reflection of the probe (side camera alone does not provide high enough resolution), until they are just above the die stage (about 5 µm, the part itself is 25 µm). 3) grasping: move the probes together toward the edge until a firm grasp is established. the threshold of a secure grasp has been experimentally determined to be combined x direction error of 10 µm. 4) part lifting: coordinate both probes to maintain x direction separation and move vertically up towards the passive probe. 5) part rotation: coordinate both probes to move the part until it presses against the passive probe while maintaining the grasp. the probes move in a direction perpendicular to the part surface and towards the passive probe, causing the part to rotate about the axis between the probes. 6) part insertion: the die stage positions the hole underneath the part, and the probes move the part into the hole while continuing to maintain grasp. 7) part release: the probes move quickly away from the part to release contact. in the first step, only the top camera image feedback is needed for the positioning. in the second step, only the side camera is used. therefore, the 3d multiple-camera vision calibration and image feedback is needed. repeatability of the post insertion procedure is generally good, with most failures caused by the vision system being distracted by imperfections on the die and lighting issues. currently the least repeatable steps are grasping and insertion. grasping is around 70% successful and can be easily repeated if it fails initially. insertion has not been attempted a large number of times and most failures are due to the vision system being distracted by noise. with some limited assistance the insertion success rate is around 80%. the third probe rotation process is nearly always successful."
"a pool of images is required for deep learning of subcategorization. bird images containing 27 bird species endemic to taiwan on various backgrounds were compiled from the iob and several other online resources. the use of public-domain images has benefits and drawbacks. although internet volume 7, 2019 image sources add diversity to the dataset, the images may be contaminated with noise, harshness, spurious pixels, and blurred parts, all of which degrade image quality. therefore, to limit the intensity of deformity in an assortment of images, high-pixel images with clear boundaries were used. finally, to obtain standardized balance in the dataset, the bird species images were transformed and augmented as follows [cit] :"
"the advancement of consumer products, such as smartphones, digital cameras, and wearable gadgets [cit], has transformed multidisciplinary approaches toward technology by connecting the physical and digital worlds. high-resolution digital cameras in smartphones are the most pervasive tools used for recognizing the salient features of physical objects, enabling users to detect, identify objects and share related knowledge. birds present in a flock are often deeply colorful; therefore, identification at a glance is challenging for both birdwatchers and onlookers because of birds' ambiguous semantic features [cit] . to address this problem, an information retrieval model for birdwatching has been proposed that uses deep neural networks to localize and clearly describe bird features with the aid of an android smartphone [cit] ."
the experimental results and analysis of the datasets are presented in section v. section vi summarize the discussion and limitation part of the study. conclusions and directions for future study are provided in section vii.
"to classify the aesthetics of birds in their natural habitats, this study developed a method using a convolutional neural network (cnn) to extract information from bird images captured previously or in real time by identifying local features. first, raw input data of myriad semantic parts of a bird were gathered and localized. second, the feature vectors of each generic part were detected and filtered based on shape, size, and color. third, a cnn model was trained with the bird pictures in a graphics processing unit (gpu) for feature vector extraction with consideration of the aforementioned characteristics, and subsequently the classified, trained data were stored on a server to identify a target object."
"where k, t i, t d are the controller parameters.the tuning objective is to design the three parameters so that g cl approaches g * cl as close as possible. note that, we here focus on the development of a tuning approach without a priori knowledge of p or estimating a plant model beforehand. due to the lack of the information of g cl, we aim at minimizing"
"where k is the spring constant, x is the center of the part, is the width of the part, and (x 1, x 2 ) are the contact locations for the two probes. due to the quasi-static nature of the problem, we use a simple proportional kinematic control law to achieve the desired squeeze force:"
"the actual and desired closed loop control systems. p is short for plant, which is unknown and could be difficult or impossible to be modeled accurately with limited number of operating data. c here represents the standard pid controller, which can be written in the following discretetime form,"
"bearing the tuning objective in mind, we next show the main idea of designing the pid controller in (1), that is, selecting appropriate parameters k, t i and t d, in the least squares sense. we start by writing the reference signal r k in the following form,"
feature extraction is vital to the classification of relevant information and the differentiation of bird species. we combined bird data from the internet of birds (iob) and an internet bird dataset to learn the bird species.
"this paper describes a spatial microstructure that has been assembled using sharp-tip probes and computer vision through automatic and telerobotic operation without the need for fixtures, specialized grasp points, or snap-fasteners. the flexibility of the two-probe grasp is demonstrated by manipulating two different types of parts with no change to the grasp mechanics. the structure demonstrated is intended to represent the general configuration of a compliant micromechanism. future designs will use the same assembly procedure with parts that contain compliant elements. the process will be completely automated with the use of an automated planner and a generalized grasp controller. the development of the planner will require a simulation that includes the unique microscale effects presented. finally, different materials other than silicon will be investigated to provide more functional micromechanisms."
the remaining of this paper is organized as follows: section 2 addresses the tuning objective and presents the idea of the proposed model-free tuning approach; section 3 details the recursive implementation of the tuning approach; section 4 shows some simulation results to demonstrate the effectiveness of the algorithms. section 5 concludes this research work.
"the cameras are configured with motorized zoom and focus capabilities to allow for high resolution during grasp and part insertion operations. the height of the probes must be accurate enough to hit near the center of the thin edge of the part being grasped, which measures only 25 µm. to achieve this level of accuracy in height it is necessary to use the reflection of the probe on the surface. figure 9 shows a high zoom image of the probe next to the edge of the part. this approach is a far more accurate measurement of the height than the pure stereo vision."
"lichens and corals, representatives of symbiotic associations with algal symbionts, experience a period without symbionts in their life cycle and must acquire fresh algae as symbionts to complete their life cycle. there is no such symbiont-less period for p. bursaria; the algae are retained through cell division as well as sexual reproduction [cit] . consequently, the symbiotic relationship with p. bursaria appears to be permanent. however, diversity in the photobionts, as mentioned above, does exist. thus, it is not understood how p. bursaria gain such algal diversity."
"the green ciliate paramecium bursaria is one of the most studied protists due to its observable endosymbiosis. their symbiotic relationship is able to start over, i.e., artificially algae-removed p. bursaria can absorb again and fix the algae as new photobionts [cit] . despite the re-symbiosis ability of p. bursaria, there are unusual characteristics in terms of the small diversity of their photobionts. although almost 50 strains of photobionts (partly directly gained sequences from p. bursaria extracts) have been genetically identified, most belong to either chlorella variabilis or micractinium reisseri (chlorellaceae, trebouxiophyceae) [cit] . neither species has ever been collected as a free-living species from natural water sources. this is possibly due to following reasons. both species are essentially nutritionally fastidious [e.g., 12, 13] . additionally, they are very sensitive to the paramecium bursaria chlorella virus (pbcv), which is abundant in natural water sources [cit] . escaped photobionts from p. bursaria cell would be attacked by pbcv immediately. both species appear to be highly dependent on their host refuge. chlorella variabilis and m. reisseri are therefore thought to have adapted to exclusively dwell in p. bursaria. in a few cases, p. bursaria is associated with other species of chlorella or scenedesmus (chlorophyceae) [cit] . namely, p. bursaria has replaced its photobionts on several occasions."
"individuals of p. bursaria were carefully picked from the surface of the culture medium (to avoid picking up the coccoids on the bottom of the flask, though there were not many), the cells were disrupted and suspended in pure water then spread onto an oligotrophic agar plate (1/5-concentration gamborg's b-5 basal medium with minimal organics, sigma aldrich, st louis). observed colonies were picked and transferred to 1/5 gamborg liquid medium. these were maintained under led illumination (12 h l:12 h d) at 15˚c."
we are grateful to prof. hiroshi hosoya (hiroshima university) for kindly providing paramecium strains. this study was supported by the sumitomo foundation and showa seitoku memorial foundation.
"ssu rdnas for p. bursaria (host) were determined. of these, three (ag-35, bp-11, and dc3: ab699097-099) were identical and matched some previous sequences we refer to as genotype d [cit], whereas olg-3 (ab699100) differed from the others and matched what we refer to as genotype b."
"the p. bursaria strains here were obviously different from the ordinary ones. under the microscope, green coccoids of different sizes (ca. 5 µm or 1.5 µm) were clearly seen in p. bursaria (strains ag-35, bp-11, and olg-3) (figure 2) . note that ordinary p. bursaria retains a cloned single-species of alga (ca. 5 µm). the number of small balls was much larger than the large ones except dc-3 (small balls were remarkably few). the small coccoids were inserted in between the trichocysts (figure 2(c)) ."
"paramecium bursaria usually gains energy by feeding as well as by the photosynthates of photobionts. in general, p. bursaria collected from nature may contain more than one green alga. photobiont and feed are difficult to determine. these algae will be unifiedin the cell of p. bursaria during several days of culture conditions. in most cases, the remaining algae (namely, natural photobionts) are either c. variabilis or m. reisseri. however, the algae of p. bursaria strains cited here have not been unified during 20 years of culture conditions. the mother stock of olg-3 also has shown the same feature [cit] . microscopic observations showed c. minor (small coccoids) were inserted in between the trichocysts. this phenomenon can be thought of as the advanced symbiosis stage rather than feed in the food vacuole [cit] . therefore, it is regarded that these p. bursaria strains deal both c. variabilis and c. minor as their steady photobionts. this situation, namely, stable symbiotic relationships between p. bursaria and multiple photobionts, will encourage the hypothetical theory for the group i intron transmitions between c. variabilis and m. reisseri (figure 1) ."
"cells of p. bursaria and their symbiotic algae were observed under light microscopy cx31 (olympus, tokyo) and photos were taken with an hdce-31 digital camera (as one, osaka)."
"next, we conducted algae-targeting pcr (18) . three p. bursaria strains (bp-11, dc-3, and olg-3) produced two length-polymorphic bands, whereas strain ag-35 produced three bands (figure 3) . the length polymorphisms were due to the variations in intron insertions. blast search indicated the sequences were identical with c. variabilis (large bands: ab699101-104), choricystis minor (medium bands: ab699105-110), and some chlorella and micractinium species (small band: ab699111) ( table 1 ). the amplified region (exon) was fairly conservative among chlorella-related species and we were unable to identify the small band to the species level."
"on the oligotrophic agar plates with the p. bursaria extract, only small coccoids were obtained from bp-11, dc-3, and olg-3 extracts. because c. variabilis requires organic nitrogen sources to grow [cit], this oligotrophic condition might have prevented its growth. both sizes of green coccoids were present on the plate from the ag-35 extract. we picked up several green colonies from this plate and transferred each to liquid medium. one of the algal strains, an approximately 5 µm coccoid, was named ag-35_zf1 and we sequenced its ssu-its1-5.8s-its2 rdna. the sequence (ab-699112) of the isolated alga, ag-35_zf1, was very close to those of c. vulgaris. an ncbi blast search indicated the closest taxon, c. vulgaris ccap 211/80 [cit] 53, covering its1-5.8s-its2 rdna), where only two transitions and one indel were found within the its1 region. it is likely that this photobiont is c. vulgaris."
"in a single host individual, multiple symbiont species performing similar-functions often have negative effects on the host's growth [cit] . however, if it is regarded as a phase for determining a more optimal partner, it can be an advantage for survival in the long term."
particular kind of paramecium bursaria strains were maintained in lettuce juice medium [cit] under led illumination (12 h l:12 h d) at 15˚c ( table 1) . these strains were once collected by dr. t. kosaka (hiroshima university) [cit] in the united states and have been maintained in a laboratory at the university. the stock cultures were kindly donated by prof. h. hosoya (hiroshima university) [cit] and have been cultured for more than four years; therefore symbiotic conditions should be regarded as stable.
"a theoretical nonlinear model of the pals quarter car is first built for nonlinear simulations to evaluate the potential of the novel suspension in terms of performance enhancement and energy consumption. this theoretical model, referred to as \"simqc,\" is built in a multibody software platform as follows. 1) multibody model of the quarter car retrofitted with the pals: the kinematics of each body (e.g., sprung mass, wheel) is defined with specified motion constraints and mechanical joints. the dynamics of each body is specified with the mass properties (e.g., the inertias) given and the external force/torque exerted (including the spring/damper force, the vertical tire force, the rotary actuator torque, etc.). the nonlinearities, for example, the damping force against the spring-damper compression/extension velocity, are built by lookup tables. 2) the rocker actuator: a permanent-magnet synchronous motor (pmsm, also known as a brushless ac motor) is selected as the rocker actuator due to its high power density, high efficiency and fast response. the classical \"d-q\" vector modeling and control algorithm (also referred to as field-oriented control) is commonly adopted in ac motors [cit] . the basic idea is to transform a three-phase stator current into two independent variables-a magnetic field-generating part (i d ) and a torque-generating part (i q ). the electrical dynamics of this equivalent d-q model are written as"
"a pmsm (supplied as kollmorgen akm33h [cit] ), together with its servo drive (kollmorgen s300) are selected to drive the rocker, with the main parameters listed in table v in the appendix. a 40:1 gearbox (ut075-40) is added between the actuator and the rocker to shape the torque-speed envelope. the pmsm control aims to minimize the tracking error of the reference rocker torque (t rc * ), which is given by a transformation by β −1 of the output force f rc * of synthesized h ∞ control (see fig. 9 )."
another use of titan is to test and evaluate cad tool quality. both post-technology mapping tools and logic resynthesis tools can be plugged into the flow.
"with the given performance objectives and the tuned weighting functions, a conservative and an aggressive h ∞ controllers are synthesized separately, with the corresponding tuning constants listed in table i ."
"quartus ii spends a large portion of its run time during placement, and this is reflected when looking at the relative placement run time of the two tools. here, vpr's placement engine is faster than quartus ii's, taking 49% less time. there could be several reasons behind this. vpr typically uses fewer labs than quartus ii (see section 7.5), which decreases the size of vpr's placement problem. quartus ii also enforces stricter placement legality constraints and uses more intelligent directed moves [cit] ."
"a gt quarter car equipped with a double wishbone suspension assembly and a road excitation mechanism has been developed to experimentally study the savgs prototype [cit] with the main parameters listed in table iv in the appendix. the novel pals mechanism is designed and implemented to integrate with this quarter-car test rig, following the concept design described in section ii-a."
"the h ∞ control technique is widely applied to multi-input multioutput systems, and it aims to minimize the disturbance propagation to the system performance objectives [cit] . fig. 10 shows the interconnection diagram of the pals quarter-car linear model and the synthesized h ∞ controller. the performance objectives [e 1 e 2 e 3 ] to be minimized are: 1) the sprung mass vertical acceleration, which reflects the ride comfort; 2) the tire deflection, which evaluates the road holding; and 3) the control effort of the actuator reference force, as the active suspension is expected to contribute only at the frequency range of interest. the control effort penalization benefits the system with energy saving as well as high-frequency noise attenuation. the system disturbances are: 1) d 1, the vertical road velocity; 2) d 2, the noise of the suspension deflection measurement (y 1 ) by a potentiometer; and 3) d 3, the noise of the sprung mass acceleration measurement (y 2 ) by an accelerometer. the disturbance of the load transfer on the quarter car, which is often considered in active suspension studies [cit], is not included here, as it cannot be emulated by the test rig. the system measurements, y 1 and y 2, are selected on the basis of availability of sensors."
"the characteristics outlined above make the titan23 benchmark suite quite different from the popular mcnc20 benchmarks [cit], which consist of primarily combinational circuits and make no use of heterogeneous blocks. furthermore, the mcnc designs are extremely small. the largest (clma) uses less than 4% of a stratix iv ep4sgx180 device, making it one to two orders of magnitude smaller than modern fpgas."
"half of the labs in a stratix iv device can also be configured as small rams, referred to as memory labs (mlabs), which were also modelled. the f cin and f cout values were set to 0.055 and 0.100 respectively, to match the global routing connectivity in stratix iv."
"the demonstrated feasibility of the novel pals in a quartercar setting establishes its promising prospect for full-car applications and on-road tests. the pals, featuring low-energy cost and notable performance improvement, is a promising alternative solution in the active suspension area. appendix table iv main parameters in pals quarter-car test rig table v parameters of rocker actuator and servo drive control"
"the backlash effect is undesirable as transient changing loads result in impact of the gears and thereby accelerate the gearbox failure. the parameter of the half-backlash gap α is estimated by comparing the nonlinear simulations (with \"simtr\") and test results, where in both cases a conservative and an aggressive control schemes (detailed in section iv) are applied separately to the pals. according to the variation of the high-speed shaft (hss) torque and speed (see fig. 5 ), α is approximated to be 0.01 rads (a whole backlash gap of approximately 1.15°)."
"where y min ak, z min ak, and z min c k are specified in fig. 2 to establish the boundary of point \"k\"."
"the titan23 benchmark suite consists of 23 designs ranging in size from 90k-1.8m blocks, with the smallest utilizing 40% of a stratix iv ep4sgx180 device, and the largest designs unable to fit on the largest stratix iv device. the designs represent a wide range of real world applications and are listed in table 2 . all benchmarks make use of some or all of the different heterogeneous blocks available on modern fpgas, such as dsp and ram blocks."
"the pals is proposed and studied in this paper. overall, it aims to enhance the suspension performance (e.g., ride comfort, road holding, and chassis leveling) with less power consumption and less weight increment than other parallel active suspensions. moreover, the pals inherits the savgs advantages of failsafe behavior, negligible unsprung mass increment, and small sprung mass increment, while it avoids the need to support the equilibrium sprung mass as in the savgs case."
"this paper thus performs an experimental study with a quarter-car test rig to evaluate the feasibility and the performance of the novel pals. the following are the main contributions: 1) pals geometric optimization and mechanical implementation, 2) control scheme synthesis for the pals quarter car that aims to improve ride comfort and road holding, and 3) experimental validation of: a) the pals practical feasibility, b) the accuracy of the pals theoretical mathematical models and the identified practical features, and c) the robustness of the synthesized controllers."
"in all experiments, version 12.0 (no service packs) of quartus ii was used, while an early revision of vpr 7.0 (r1499) was used. the newer version of vpr was selected over vpr 6.0 since it provides substantial improvements to packing performance. during all experiments a hard limit of 48 hours run time was imposed; any designs exceeding this time were considered to have failed to fit. most benchmarks were run on systems using xeon e5540 (45nm, 2.56ghz) processors with either 16gb or 32gb of memory. for some benchmarks, systems using xeon e7330 (65nm, 2.40ghz) and 128gb of memory were used; performance data collected on these machines is not directly comparable to the 16/32gb systems. for each benchmark, both tools were run on the same class of machines."
"the overall control implementation to the nonlinear pals quarter-car test rig is shown in fig. 9, where the function β −1 (see fig. 8 ) converts the equivalent linear force (f rc ) to the rocker torque (t rc ) and also lumps the geometry nonlinearity of the pals."
"the simulation results with the simqc model over the 2-hz harmonic road are shown in fig. 14 . all performance objectives are significantly attenuated by the pals with the conservative controller, and further (but marginally) improved with the aggressive controller."
"initial attempts to use the detailed architecture capture (section 5) resulted in most circuits taking over 48 hours to pack in vpr. this prompted the creation of a simplified architecture (section 6). to quantify the impact of these modifications, vpr's relative performance and qor on these two versions of the stratix iv architecture was investigated."
"in stratix iv, each lab consists of 10 alms with 52 inputs from the global routing, and 20 feedback connections from the alm outputs. stratix iv uses a half-populated crossbar at the alm inputs to select from the 72 possible input signals [cit] . the lab has 40 outputs to global routing driven directly by the alms. since no detailed information is available on the exact switch patterns used for the half-populated alm input crossbars, they were modelled as shown in fig. 3, otherwise the capture is accurate."
"this section introduces the pals concept in terms of operating principle, functionalities, and features over the existing suspension solutions. fig. 1 illustrates the pals application to a quarter car with double wishbone suspension. in addition to all conventional elements of a passive suspension assembly, a rocker-pushrod assembly (\"k-j-f\") is introduced between the chassis and the lower wishbone. the rocker (\"k-j\") is driven by a rotary actuator, which provides a torque (t rc ) on the rocker-pushrod assembly to actively change the wheel-travel position and the wheel load, thereby making it possible to adjust the chassis attitude and reduce heave, roll, and pitch acceleration."
"we investigated the effect of telling quartus ii to always pack densely, and the effect of disabling placement finalization. in default mode quartus ii varies packing density based on the expected utilization of the targeted fpga, spreading out the design if there is sufficient space. also by default, quartus ii performs placement finalization, where it breaks apart clusters by moving individual luts and flip-flops. this indicates that a significant portion of vpr's higher wl is likely due to packing effects, and principally due to a focus on achieving high packing density. we suspect that vpr's packer is sometimes packing largely unrelated logic together to minimize the number of clusters. this appears to be counter productive from a wl perspective."
"the pals is developed and integrated to improve the vehicle performance by actively controlling the rocker torque at each corner of the vehicle. in low-frequency applications, it is capable of i) chassis leveling by reducing the pitch angle in braking/acceleration events and by reducing the roll angle in cornering events and ii) improving the aerodynamic behavior by adjusting the chassis-heave motion and roll and pitch angles. in high-frequency applications, the pals further enhances iii) the ride comfort by minimizing the chassis heave, roll, and pitch acceleration at human-sensitive frequencies and iv) the road holding by reducing the variation of the vertical tire force."
"2) physical space constraints, which aim to avoid any installation interference by the introduced pals actuator and its bearing house shown in fig. 3 (e.g., actuator to wishbones and to chassis)"
"where k e is defined as the torque constant. t em is simply proportional to i q, equivalent with a dc motor, and therefore, it can be directly measured by the current sensors. however, due to the iron (ignored here) and copper losses, and the rotor friction and damping (the mechanical loss, denoted as t m ), t em does not exactly equal the output mechanical torque of the actuator hss (i.e. high-speed shaft):"
4) the torque transmission efficiency of the gearbox (η gbx ) is decreased from the nominal 90% (provided in the manual) in simqc to an equivalent 70% in simtr. this reduction is appropriate to give a good match of the gearbox fatigue as well as the friction loss in rocker-pushrod joints.
"overall, the titan flow enables a wide range of fpga architecture experiments, and can be used to evaluate new cad algorithms on realistic architectures with realistic benchmark circuits, and allows for more extensive scalability testing with larger benchmarks."
"in comparison with active suspensions that are equipped with hydraulic actuators or linear electromagnetic actuators, the pals introduces a negligible unsprung mass increment (the rocker actuation is attached onto the chassis, and the transmission mechanism is mostly referred to the chassis side) and a small sprung mass increment, since the rocker and pushrod in their optimized form demand a low actuation mass. the pals also has fail-safe characteristics as it reverts to the passive suspension when it suffers an actuator failure or power loss. moreover, the geometric optimization of the rocker-pushrod arrangement leads to an efficient propagation from the rocker torque to the vertical tire force increment, and thereby consumes less power. finally, a rotary electromechanical actuation is utilized in the pals that allows geared transmission designs, and it is, therefore, more compact than other forms of actuation, such as linear actuators, and also, it is provided by conventional electric motors, which is a readily available, mature, and reliable technology."
"the global or inter-block routing in stratix iv uses wires 4 and 20 labs long in the horizontal routing channels, and wires 4 and 12 labs long in the vertical routing channels. there are approximately 70% more horizontal wires than vertical wires. in stratix iv the long wires are only accessible from the short wires and not from block pins. additionally, stratix iv allows labs in adjacent columns to directly drive each other's inputs. while vpr can model a mixture of long and short wires, it assumes the same configuration in both the horizontal and vertical routing channels. additionally, vpr cannot model stratix iv's short to long wire connectivity, or the direct-link interconnect between labs in adjacent columns. as a result, the inter-block routing was modelled as length 4 and 16 wires (the average lengths), with both long and short wires accessible from logic block output pins. unidirectional routing was used and the channel width was set to 300 wires."
"some benchmarks make use of bidirectional pins, which cannot be modelled in blif. therefore vqm2blif splits any bidirectional pins into separate input and output pins, and makes the appropriate changes to netlist connectivity."
"according to (2), the objective to be maximized can be obtained, and it can be calculated as the fraction of two partial derivative items. the term a minimum value is to be found throughout all the possible wheel travel displacement (from the maximum bump to the maximum rebound). additionally, the following constraints should be considered for the feasibility of the pals. 1) geometry constraints based on the triangle inequality"
"in this section, with the derived linear equivalent model of the pals quarter car, an outer loop h ∞ control is synthesized to enhance the ride comfort and the road holding. an inner loop proportional-integral control is also developed for the reference torque tracking of the rocker actuator."
"several non-fpga specific benchmark suites also exist. the various ispd benchmarks [cit] are commonly used to evaluate asic tools, but are only available in gate-level netlist formats. this makes them unsuitable for use as fpga benchmarks, since they are not mapped to the appropriate fpga primitives. [cit] benchmarks [cit] are available in hdl format, and the titan flow enables them to be used with fpga cad tools. however, the largest design consists of only 36k blocks after running through the titan flow -too small to be included in the titan23."
"it is also important to note that the sizes of benchmarks created with the titan flow are not limited by the capacity of the targeted fpga family. quartus ii's synthesis engine does not check whether the design will fit onto the target device, allowing vqm files to be generated for designs larger than any current commercial fpga. the vqm2blif tool also runs quickly, taking less than 4 minutes to convert our largest benchmark."
"in this section, the theoretical model of the nonlinear pals quarter car (simqc, built in section iii-a) is utilized. simulations with different road profiles, including a harmonic road, a smoothed bump and hole, and an iso random road (classes a-c), are performed to evaluate the pals potential in enhancing the suspension performance and the power demand in the rocker actuator."
where z i is the vertical coordinate of the unsprung mass center. this equation can be written as a function of the lower wishbone angle (θ lw ) as
"weighting functions [w acc, w td, w eff ], respectively, indicate the importance of the performance objectives in the frequency domain, and they are given as follows:"
"another benchmark suite of interest is the collection of 19 benchmarks included with the vtr design flow. these benchmarks are larger than the mcnc benchmarks, with the largest (mcml) reported to use 99.7k 6-luts [cit] . interestingly, when this circuit was run through the titan flow, it uses only 11.7k stratix iv aluts (6-luts) after synthesis, indicating the differences between odinii+abc and quartus ii's integrated synthesis. additionally, only 10 of the vtr circuits make use of heterogeneous resources, and none use dedicated carry arithmetic. the titan23 benchmark suite provides substantially larger benchmark circuits that make more extensive use of heterogeneous resources."
"a linear equivalent model of the pals quarter car is built to enable the linear control synthesis, as shown in fig. 6 . this linear equivalent model is independent of the rotation δθ rc, with the geometric nonlinearities lumped in function β."
"as shown in fig. 3, a rotary \"crankshaft\" (originally for the savgs experimental study [cit] ), which is connected with the upper end-eye of the spring-damper, is fixed onto the chassis by a key-keyway joint to restore the passive suspension operation. in addition to the conventional suspension assembly and the road wheel, a rotary actuator as well as its gearbox (40:1) are attached onto the sprung mass. the low-speed shaft (lss, i.e., the output shaft) of the gearbox is connected to a rocker through a key-keyway joint. a bearing house that fixes the actuation assembly also supports the rocker to ensure its rigid rotational motion. a pushrod is in series with the rocker and the lower wishbone, with its lower pivot joint aligned with the lower end eye of the spring damper. the pals mechanical design has benefits of structural compactness and torsional stiffness."
"to convert a benchmark from hdl to blif, the design was first synthesized in quartus ii. for most designs this required no hdl modification, but some required replacing vendor/technology specific ip (e.g. plls, explicitly instantiated ram blocks) with an equivalent altera implementation, or working around obscure language features. once the design was synthesized successfully, the resulting vqm file could be passed to vqm2blif."
"to further investigate this correlation between packing density and wl, we re-ran the benchmarks through quartus ii using several different combinations of packing and placement settings. the impact of these settings on the relative qor between vpr and quartus ii are shown in table 7 ."
"the associated bode magnitude diagrams of the two controllers are shown in fig. 11 . the controllers work as bandwidth filters for both measurement inputs, as they take effect essentially at around the human-sensitive frequency range (0.5-5 hz). bode magnitude diagrams of the pals linear equivalent model for the passive and active cases are shown in fig. 12, where the suspension performance objectives are significantly attenuated at around the human-sensitive range, which includes the sprung mass resonant frequency of 2 hz."
"in terms of comparison with the savgs [cit] in which the single link is in series with the spring damper and needs to support the whole sprung mass during the operation, while the single link is rotated and not closely aligned with the spring damper, the pals demands less actuation torque for heavy vehicle applications (e.g., the suv). this is due to the passive suspension strut supporting the equilibrium chassis weight in the pals case, while the active components provide only an incremental parallel force. however, the actuation torque requirements as explained earlier may be reversed in lighter vehicle applications that may involve a passive spring that is significantly stiffer, because in that case a larger parallel incremental force would be required to cause any displacement of a stiffer spring. finally, while an implementation of the savgs to a macpherson strut is not possible because an savgs single-link rotation at the top of the spring-damper unit would also cause a wheel camber angle change, which is highly undesirable. the parallel arrangement of the rocker-pushrod assembly with the spring-damper unit in the pals does not suffer from this limitation. therefore, the pals has a more general application than the savgs, including the macpherson strut and the double wishbone suspension structures."
"a wide range of benchmark designs were run through the titan flow, with the goal of creating a set of large benchmarks representative of modern fpga usage. of the 46 benchmarks converted, the 23 largest from a diverse set of application domains were chosen to create the titan23 benchmark suite described in section 4.2. while the rest of this paper reports results primarily on the titan23 benchmark suite, we are releasing the full set of 46 converted benchmarks. we believe that a range of benchmark sizes can be useful during the development stages of new fpga cad tools and architectures."
"the second category of metrics focus on the quality of results (qor). we measure the number of physical blocks generated by vpr's packer, and the total number of physical blocks used by quartus ii. another key qor metric is wire length (wl). unlike vpr, quartus ii reports only the routed wl and does not provide an estimate of wl after placement. if a circuit fails to route in vpr, we estimate its required routed wl by scaling vpr's placement wl estimate by the average gap between placement estimated and final routed wl (~40%)."
"semiactive suspensions cannot deal with the steady-state suspension position keeping as they actively adjust the damping force, which is solely related to the suspension deflection rate. therefore, the pals offers an additional low-frequency benefits of i) and ii) in section ii-b as compared to semiactive suspensions."
"as will be shown in section 7.3, the detailed architecture capture described above performs very poorly in vpr's packer. as a result, a simplified architecture was created which makes some additional approximations. in the lab, the half-populated crossbar used for the alm inputs was replaced with a full crossbar. to avoid limitations related to placing labs at mlab locations, it is assumed that all labs can be used as mlabs. for ram blocks operating in mixed-width mode, the exact depth and width constraints were relaxed. while these relaxed constraints can potentially allow more ram slices to pack into a ram block than is architecturally possible, the ram block will typically run out of pins before this occurs."
"a bump and a hole are common cases for on-road driving, and the quarter-car behavior over these cases can also indicate the step response characteristics. fig. 15 shows the performance difference between the active and passive cases. the actively controlled rocker torque significantly contributes to the suspension dynamics attenuation, in terms of both peak and rms value reduction of the main performance indicators."
"test results with the quarter-car test rig prove the practical feasibility of the pals, as it notably improves the ride comfort despite the drawbacks of the sprung mass-railway friction and the rocker transmission backlash. the comparison between the test and nonlinear model simtr results demonstrates that the identified practical features in the rig are accurate and modeled appropriately."
"the sprung mass-railway friction in the test rig attenuates the suspension dynamic response in the passive case, which makes the pals contribution less significantly presented [cit] . on the other hand, the backlash in the rocker torque transmission results in spikes in the variation of the actuator velocity and the sprung mass vertical acceleration. despite these two shortcomings, the pals still improves the suspension performance notably, as shown in figs. 18-21. the ride comfort enhancement over the passive suspension is quantified by the reduction in the sprung mass vertical acceleration rms value, as summarized in table iii, while the maximum positive power is also specified. similarly, with simulation results with simqc, the test results show that the pals with the aggressive control, further, but marginally improves the suspension performance, with higher actuator power/torque/velocity values. therefore, the selection of the rocker actuator for real applications should take into account both the performance expectation and the cost (e.g., the actuator power demand and the weight increment)."
"since our stratix iv architecture capture does not yet include timing information, both tools were run in a nontiming-driven mode. in vpr, this meant providing the -timing analysis off command line option, while in quartus ii the optimize timing fitter option was set to off."
"nonlinear simulations with a theoretical pals quarter-car model (simqc) indicate the novel suspension's promising potential in terms of ride comfort and road-holding enhancement. moreover, the pals demands a highly limited rocker actuator power, which can be provided by a small actuator, as the optimized rocker-pushrod assembly influences efficiently the variation of the vertical tire force. thus, the pals introduces only a small weight increment and low power consumption to its vehicle applications."
"the most obvious limitation of the current comparison between vpr and quartus ii is that both tools were run without timing optimization. in the future, we plan to add timing information to the stratix iv architecture capture and evaluate the quality of these tools under real timing constraints."
"finally, given the substantial gap between vpr and commercial fpga cad tools, it is clear that there remains significant room for improvement in the run time, memory usage, and result quality of this academic cad tool."
"a linear equivalent model of the pals quarter car is also derived, with the geometric nonlinearity in the pals mechanism compensated. this linear model is used for the linear control synthesis in section v."
"based on the derived linear equivalent model of the pals quarter car, a torque control scheme is synthesized with an outer loop h ∞ control and an inner loop rocker torque tracking control."
"stratix iv is an island style fpga architecture, where the core of the chip is divided into rows and columns of blocks, and each column is built from a single type of block (lab, dsp, etc.). the device aspect ratio and average spacing between blocks was determined by viewing an ep4se820 device, the largest in the stratix iv family, in the quartus ii floorplanner. an example floorplan is shown in fig. 2 ."
"the titan23 benchmark suite represents a first step forward, but will need to be continually updated to keep pace with increasing fpga design size and complexity. therefore we would welcome additional benchmark contributions to cover larger design sizes and a wider range of applications."
"for example, consider a lab that is mostly filled with related logic a, but which can accommodate an extra unrelated register b. during placement, the cost of moving this lab will be dominated by the connectivity to the related logic a. this could result in a final position that is good for a but may be very poor for the extra register b (i.e. far from its related logic), as shown in fig. 4a . if this is a common occurrence it could lead to increased wl. a better solution to the above scenario would have been to utilize additional clusters (pack less densely) to avoid packing unrelated logic together, as shown in fig. 4b . alternately, if the placement engine was able to recognize the competing connectivity requirements inside a cluster, it could break it apart, much like quartus ii's placement finalization."
"the alm was modelled as two lcell comb primitives, each representing a 6-lut and full adder, along with two dffeas primitives representing flip-flops. the modelled alm connectivity is shown in fig. 3 . the stratix iv alm contains 64-bits of lut mask, less than what is required by two dedicated 6-luts. vpr cannot model this restriction and assumes two 64-bit lut masks; however this extra flexibility is expected to have minimal impact on results, since few pairs of 6-luts can pack together in one alm due to the limited number of inputs (8) ."
"2 without access to quartus ii's internal packing and placement statistics, it is difficult to identify which step(s) of the design flow are responsible for this difference. however, in table 4 it appears that circuits where vpr uses substantially fewer labs than quartus ii often have the largest difference in wl."
"nonlinear models of the theoretical quarter car with the pals are presented in this section. practical features existing in the test rig (see fig. 4 ), including the sprung mass-railway friction, wheel-road plate friction, and backlash gap between the lss and the rocker, are further identified and included in order to compensate the discrepancy between the theoretical and experimental behavior."
"a quarter-car experimental study of the newly proposed pals is performed in this paper. the associated rocker-pushrod assembly is geometrically optimized, mechanically designed, and physically implemented in a gt quarter-car test rig."
"benchmark circuits with open-source tool flows. first, obtaining large benchmarks can be difficult, as many are proprietary. second, purely open-source flows have limited hdl language coverage. the vtr flow, for example, uses the odin-ii verilog parser which can process only a subset of the verilog hdl -any design containing system verilog, vhdl or a range of unsupported verilog constructs cannot be used without a substantial re-write. as well, if part of a design was created with a higher-level synthesis tool, the output hdl is not only likely to contain constructs unsupported by odin-ii, but is also likely to be very hard to read and rewrite with only supported constructs. third, modern designs make extensive use of ip cores, ranging from low-level functions such as floating-point multiply and accumulate units to higher-level functions like fft cores and off-chip memory controllers. since current open-source flows lack ip, all these functions must be removed or rewritten; this is not only a large effort, it also raises the question of whether the modified benchmark still accurately represents the original design, as ip cores are often a large portion of the design. in order to avoid many of these pitfalls, we have created titan, a hybrid flow that utilizes a commercial tool, altera's quartus ii design software, for hdl elaboration and synthesis, followed by a format conversion tool to translate the results into a form open-source tools can process. the titan flow has excellent language coverage, and can use any unencrypted ip that works in altera's commercial cad flow, making it much easier to handle large and complex benchmarks. we output the design early in the quartus ii flow, which means we can change the target fpga architecture and use open-source synthesis, placement and routing engines to complete the design implementation. consequently we believe we have achieved a good balance between enabling realistic designs, while still permitting a high degree of cad and architecture experimentation. our contributions include:"
"the performance enhancement by the pals quarter car with different road profiles and with two different aggressive controllers are more particularly quantified by rms values' reduction, as listed in table ii . both the ride comfort and the road holding are significantly improved by the pals, especially with the 2-hz harmonic road, the smoothed bump and hole, and the class c random road. the pals performance marginally deteriorates with class a random road, which can be tolerated as both the passive and active suspensions respond with small vibrations. fig. 17 shows the torque-speed operating points of the rocker actuator, with the conservative and the aggressive controllers applied, respectively. the operation over the class c random road demands the highest electromagnetic torque t em (1.8 n·m with the conservative controller and 2.8 n·m with the aggressive one) and the highest velocity ω hss (2500 r/min). the maximum power of the rocker actuator corresponds again to the case of random road class c, and it is 530 w in the generating mode and 210 w in the motoring mode. 18 . test results over a harmonic road profile, with 2 hz frequency and 2.75 cm peak-to-peak amplitude. both \"test active\" and \"simtr active\" are with the conservative control scheme. the variables depicted are the sprung mass vertical acceleration, the suspension deflection increment, and the actuator electromagnetic torque."
"the stratix iv lab also includes three chain like structures (carry chain, share chain, register chain), however vpr does not currently support dedicated routing chain structures within logic blocks. extra flexibility was added to allow each intermediate segment of a chain to drive a lab output to ensure these structures were routable in vpr's packer."
"vqm2blif can output different blif netlists to match a variety of use cases. circuit primitives such as arithmetic, multipliers, ram, flip-flops, and luts are usually modelled using blif's .subckt structure, which represents these primitives as black boxes. while this is usually sufficient for physical design tools like vpr, some primitives like luts and flip-flops can also be converted to the standard blif .names and .latch primitives respectively. this allows the circuit functionality to be understood by logic synthesis tools such as abc. vqm2blif also supports more detailed conversions of vqm primitives, depending on their operation mode. this allows downstream tools, for instance, to differentiate between ram blocks operating in single or dual port modes."
"the purpose of the geometry optimization is to maximize the rocker torque effect on the vertical tire load, and thereby the pals can efficiently affect the suspension dynamics. when the virtual work principle is applied to the quarter-car system (with the sprung mass assumed to be fixed), the relationship between the rocker torque (t rc ) and the vertical tire-force increment (δf tz ) can be derived as (the lateral tire force and the lateral acceleration are ignored as a first approximation)"
"the comparison between test results and nonlinear simulations (with simtr) shows that the theoretical nonlinear model of the pals quarter car together with that, which includes the identified practical features in the rig, are essentially accurate, despite complex nonlinearities in the tire lateral deformation, lateral friction, etc. if these practical features existing in the test rig are eliminated or minimized in the practical vehicle applications, the actual pals performance can be improved and can reach the theoretical one predicted with the simqc (see table ii )."
"in this section we use the titan23 benchmark suite described in section 4, in conjunction with the stratix iv architecture capture described in section 5. this allows us to compare the popular academic vpr tool with altera's commercial quartus ii software. using the stratix iv architecture capture, vpr was able to target an architecture similar to the one targeted by quartus ii, allowing a coarse comparison of cad tool quality."
"the basic steps of the titan flow are shown in fig. 1 . quartus ii performs elaboration and synthesis (quartus map) which generates a verilog quartus map (vqm) file. the vqm file is a technology mapped netlist, consisting of the basic primitives in the target architecture; see table 3 for primitives in the stratix iv architecture. the vqm file is then converted to the standard berkeley logic interchange format (blif), which can be passed on to conventional open-source tools such as abc and vpr [cit] . the conversion from vqm to blif is performed using our vqm2blif tool. at a high level, this tool performs a one-to-one mapping between vqm primitives and blif .subckt, .names, and .latch structures. to convert a vqm primitive to blif, the vqm2blif tool requires a description of the primitive's input and output pins. vpr also requires this information to parse the resulting blif; we store it in the architecture file for use by both tools."
"the these factors significantly ease the process of creating large benchmark circuits for open-source cad tools. for example, converting an lu factorization benchmark [cit] for use in the vtr flow [cit] involved roughly one month of work removing vendor ip and re-coding the floating point units to account for limited verilog language support. using the titan flow, this task was completed within a day, as it only required the removal of one encrypted ip block from the original hdl which accounted for less than 1% of the design. in addition, since over 68% of the design logic was used by the floating point units, the titan flow better preserves the original design characteristics. a concern in using a commercial tool to perform elaboration and synthesis is that the results may be too device or vendor specific to allow architecture experimentation. however this is not necessarily the case. the titan flow still allows a wide range of experiments to be conducted as shown in table 1 . the ability to use tools like abc to re-synthesize the netlist ensures experiments with different lut sizes, and even totally different logic structures such as aics [cit], can still occur. ram is represented as device independent \"ram slices\" which are typically one bit wide, and up to 14 address bits deep. these ram slices are packed into larger physical ram blocks by vpr, and hence arbitrary ram architectures can be investigated. similarly, multiplier primitives (up to 36x36 bits) are packed into dsp blocks by vpr, allowing a variety of experiments. a simple remapping tool could also resize the multiplier primitives if desired. the structure of a logic element (connectivity, number of flip-flops, etc.) can also be modified without having to re-synthesize the design, and inter-block routing architecture and electrical design can both be arbitrarily modified."
"this work is licensed under a creative commons attribution 3.0 license. for more information, see http://creativecommons.org/licenses/by/3.0/ fig. 1 . pals application to a quarter car with double wishbone suspension. t rc is the rocker torque and θ rc is the rocker angle; m s is the quarter-car sprung mass, while m u is the unsprung mass; f tz and f ty are the vertical and the lateral tire forces, respectively; θ lw is the lower wishbone angle and l s is the suspension deflection."
conversion coefficients between the nonlinear multibody model and the linear equivalent one are listed in (21) and can be derived from (19) and (20). all the coefficients are functions of the suspension deflection increment (δl s )
"the schematic of the pals quarter-car test rig is illustrated in fig. 4 . the following practical features that exist in this rig are further recognized and included in the nonlinear model (referred to as \"simtr\") to compensate the discrepancy between the simulation and the testing behavior."
"the aggressive tuning constants are first chosen to ensure the best suspension performance by the pals, while the actuator torque does not clip at the continuous torque saturation limits, in the most demanding road case to be evaluated later in section vi (iso class c random road, see fig. 17 ). the conservative tuning constants are chosen so that the pals still offers a decent suspension performance enhancement, while the torque and power requirements by the actuator are highly limited, as compared to the aggressive controller. the tuning of two different sets of control parameters is to highlight the design compromise between performance enhancement and actuator capability."
"road holding, and also due to their compatibility with the long pursued full electric vehicle [cit] . research topics mainly focus on mechatronic system development, advanced sensor integration, intelligent control synthesis, and so on [cit] . historically, hydraulic or linear electromagnetic actuators are commonly utilized in parallel with the conventional spring dampers to form active suspensions, where the vertical forces between the sprung (chassis) and unsprung (wheel) masses can be actively adjusted [cit] . more recently, a new series active variable geometry suspension (savgs) is proposed, and theoretically and practically proven to be an alternative solution to existing systems [cit] . the savgs is based on rotary electromagnetic actuation and has promising prospects due to zero unsprung mass increment, limited power demand, and the use of the rotary actuation that is a mature and widely available technology. the savgs is expected to be especially suitable for light-to medium-weight vehicles with stiff suspension springs (e.g., the grand tourer (gt)). however, as the active components, the single link together with its actuation, are in series with the spring-damper unit and support the large sprung mass above, a \"parallel\" suspension is expected to be more suitable for heavier vehicles. this motivates the introduction of the novel parallel active link suspension (pals) to complement the range of vehicle categories that can benefit from a rotary-electromechanical-actuation-based active suspension."
"the frequency lower than 8 hz is deemed to be human sensitive, as the corresponding chassis vibration significantly affects passengers' comfort [cit] . with respect to the quarter-car test rig, the largest performance gains mainly concentrate in the 1-5 hz frequency range due to the 2 hz car body resonant frequency, as shown by the linear theoretical analysis in fig. 12 . therefore, a 2-hz harmonic road is adopted, while the peak-to-peak amplitude of 2.75 cm is selected according to the physical limitation of the test rig excitation mechanism [cit] ."
"the key component of vpr's poor packing performance on the detailed architecture is the partially depleted crossbar feeding the alm inputs, which leads to many failures of the packer's routability check. table 5 shows both the absolute run time and peak memory of vpr, and the relative values compared to quartus ii on the titan23 benchmark suite, using the simplified architecture of section 6. quartus ii's absolute run time and peak memory across the same benchmarks, while targeting stratix iv, is shown in table 6 . vpr's run time is dominated by the packing step, which takes on average~78% of the total run time on benchmarks that completed. in contrast, quartus ii has a more even run time distribution with placement taking the largest amount of time (49%), and with a significant amount of time (22%) spent on miscellaneous actions. for both tools, run time can be quite substantial on the larger benchmarks, taking up to 36.5 hours with quartus ii, and in excess of 48 hours with vpr."
"quartus ii targets actual fpga devices that are available only in discrete sizes. in contrast vpr allows the size of the fpga to vary based on the design size. while it is possible to fix vpr's die size, we allowed it to vary, so that differences in block usage after packing would not prevent a circuit from fitting."
"the first category focuses on tool computational needs, which we quantify by looking at wall clock execution time for each major stage of the design flow (packing, placement, routing), and also the peak memory consumption."
"with the conservative and aggressive controllers applied, respectively, i.e., test results with a harmonic road (see fig. 18 ), a smoothed bump and hole (see fig. 19 ), and a swept frequency road (increases linearly from 0 hz to the maximum frequency possible by the test rig excitation mechanism of 3 hz in a time"
"recall that to use the titan flow (without re-synthesis), the architecture file must use the vqm primitives as its fundamental building blocks. the architecture file can describe an architecture built out of these primitives, which can be combined into arbitrary complex blocks with arbitrary routing. we chose to align the architecture closely with stratix iv. this allowed us to compare computational requirements and result quality between vpr and quartus ii, and identify possible areas for improvement."
"some ip blocks, such as the sld mux in some of altera's jtag controllers and older ddr memory controllers are encrypted. these ip blocks were removed from the original hdl to avoid generating an encrypted vqm file. if possible, an equivalent unencrypted ip block was substituted. this was the case for some ddr controllers, since new altera ddr controllers are not encrypted. once encrypted ip was removed in the hdl, the design was re-synthesized and the new vqm file passed to vqm2blif. in general, only a small portion of the design logic had to be modified or removed."
"additionally, the synthesized h ∞ controllers are discretized with a sampling period of 0.005 s by standard zero-order hold discretization through the matlab command c2d, as the realtime microcontroller (ni crio-9022) utilized in the rig is configured to have 200 hz sampling frequency in all i/o ports."
"there are still some limitations to this study. the first is that only a few datasets were used for testing; namely, only the shhs2 dataset. thus, future work will test our model on many more datasets. the second limitation is that we only used the time-frequency spectra as a classification feature. thus, future work will use or combine other features to perform sleep stage classification."
"sleep is an indispensable part of our life and occupies about one third of our total lifetime [cit] . sleep not only reduces tiredness from daily life, but during sleep, our brain and other organs are also repaired [cit] . however, with the pace of life becoming faster and faster, the stresses that people experience are gradually increasing, and sleep time and quality are gradually decreasing [cit] . these factors cause a series of sleep-related diseases, including hypertension [cit], angiocardiopathy [cit], and depression [cit] . thus, better sleep is necessary for a normal life."
"in this study, the time-frequency spectra for each 30 s data epoch was used as the input for sleep stage classification [cit] . time-frequency spectra for each 30 s data epoch were calculated from four of the channels (eeg, eog l, eog r, and emg)."
"for the cnns, the normal cnn had the best classification accuracy and ck coefficient. however, when classification of each of the five sleep stages was compared using the sensitivity index, the performance of the normal cnn was not ideal. for the waking stage, the vgg16 network had the best sensitivity with a value of 90.1%. for the n1 stage, vgg16 had the best sensitivity with a value of 28.4%. for the n2 stage, the normal cnn had the best sensitivity with a value of 89.6%. for the n3 stage, vgg16 had the best sensitivity with a value of 80.9%. for the rem stage, alexnet had the best sensitivity with a value of 85.7%. additionally, the ck coefficient of vgg16 was only 0.0059 less than that of the normal cnn. thus, after this comprehensive consideration, these results suggest that the vgg16 network shows better performance than the normal cnn. the normal cnn had relatively high sensitivity only for the n2 stage, which might be due to the n2 stage class having the highest sample number. in the training phase, in order to minimize the cost function, this simple normal cnn would tend to classify each sample into the class with the maximum number of samples (i.e., it suffered from an overfitting problem). relying on its complex structure, the vgg16 network overcame the overfitting problem to some extent, and thus had a better classification performance for every sleep stage (quantified using the sensitivity index). although googlenet had the most complex structure, its sleep stage classification performance was worse. this might be due to the gradient attenuation problem, and also because of the performance saturation problem. in summary, when network structure became more complex, sleep stage classification performance of the cnns increased and tended toward saturation."
"to investigate the effect of mini-batch size on the classification results, for each network (cnn and lstm), a series of minibatch sizes were used (4,096, 2,048, 1,024, 512, 256, and 128). for each model, the mini-batch size was set to the specific value that yielded the maximum accuracy index. as the mini-batch size of each batch created using the method described above was 4,096, the other mini-batch sizes were created by splitting the 4,096 mini-batch into several component mini-batches. for example, a 4,096 mini-batch was split into two 2,048 mini-batches, or four 1,024 mini-batches, and so on. detailed hyper-parameter settings for each network can be found in table 1 ."
"for the lstm networks, which considered the temporal information in each time-frequency spectral stack, lstm 1 performed better than any of the cnns. when more temporal information was considered, the performance of lstm 2 and lstm 3 increased, with the performance of lstm 3 being better than all the networks mentioned above. thus, similar to the cnns, with more temporal information being taken into consideration, the sleep stage classification performance of the lstm networks also increases and tended toward saturation. compared with the cnns, the lstm networks not only had a simpler network structure, but also had the best sleep stage classification performance. additionally, we also constructed a normal cnn network with input mode 3 (i.e., the same as lstm 3). because the number of features that could be extracted increased, the ck coefficient of the normal cnn network also increased. however, the classification performance of this network was only approximately the same as lstm 1. this result further highlights the importance of time information between epochs for the final sleep stage classification performance."
"where, a i represents the real number of epochs in stage i; b i represents the number of epochs predicted in stage i by the networks; and num represents the total number of epochs. figure 4 shows the sleep stage classification performance for each cnn. from the figure, it can be seen that the normal cnn had the simplest structure, but the best classification performance, with an accuracy of 84.4% and a ck coefficient of 0.7786. figure 5 shows the sleep stage classification performance for the normal cnn with input mode 3. from the figure, it can be seen that the accuracy and ck coefficient of normal cnn with input mode 3 were increased to 85.8% and 0.7980, respectively, when more features were input into the network."
"for the eeg channel, the averaged data of the c3 and c4 channels were used to compute the spectra. if one of these two channels had a problem with electrode dropping, the data of the non-dropped channel was used. for the two eog channels, if one channel was dropped, then both eog l and eog r channel used the data from the non-dropped channel to compute the spectra. these two protocols were used to maximize the number of training and testing samples and to make the classification model better adapted to the situation that occurs during real sleep."
"a relu function was used as the activation function for the cnns. considering the unbalanced numbers of samples in each of the five sleep classes, a weighted softmax cross entropy logit function was used as the cost function for each network [cit] ). an adam optimization algorithm [cit] with 0.9 and 0.999 exponential decay rates for the 1st and the 2nd moments, respectively, was used for training each network, and the maximum number of iterations was set to 200 epochs. the initial learning rate was set to 0.001 and was divided by 10 every 50 epochs. training was stopped under the following conditions: (1) if the number of epochs reached the maximum; (2) if an absolute cost difference of less than 1e-5 occurred between two successive testing epochs on five successive occasions; or (3) if the testing cost increased across five successive epochs."
"all the eeg data in these two datasets were first checked for problems with electrode dropping ( figure 1b) . briefly, the average absolute amplitude of the c3, c4, eog l, eog r, and emg channels across the entire night were calculated. if the average amplitude of a channel was larger than a predetermined threshold, the electrode of this channel was identified as \"dropped\" and was excluded. in this study, we set the threshold of each channel to half of the maximum physical acquisition amplitude of each channel, which was obtained from the header of each eeg data file."
"secondly, band-pass filtering was performed on each nondropped channel. for the c3 and c4, eog l and eog r, and emg channels, the filtering frequency ranges were set to 0.3-45 hz, 0.3-12 hz, and 0.3-20 hz, respectively [cit] . we used a 50th order hamming window-based finite impulse response (fir) band-pass filter to zero-phase filter the data. briefly, the eeg data were firstly filtered by the fir filter. next, the output was reversed and was filtered again by the fir filter. finally, the output was reversed again to yield the zero-phase filtering results. the zero-phase filtering preserves the phase information of eeg data while performing filtering on the data."
"we used the sleep heart health study (shhs) dataset from the national sleep research resource database 1 . this dataset contains two sub-datasets, named shhs1 and shhs2. shhs1 contains one night of sleep eeg data for 5,793 subjects, collected between november 1st, 1995 and january 31st, 1998 (visit 1). shhs2 contains one night of sleep eeg data for 2,651 [cit] (visit 2); these are the second acquisition time points for a subset of the shhs1 subjects. all the eeg datasets contain c3, c4, electrooculography (eog) l, eog r, and electromyography (emg) channels, and nine other heart rate related channels. considering the large number of subjects in each of these two datasets, we used the shhs1 dataset for training and the shhs2 dataset for testing."
"finally, the filtered data was checked for problems with temporary electrode dropping and for continued eeg data acquisition after the electrodes were removed from the participant in the morning (figures 1c,d) . similar to the protocol used to check for the electrode dropping problem, in each 30 s data epoch, the average absolute amplitudes of the c3, c4, eog l, eog r, and emg channels were calculated. if the average amplitudes of both the c3 and c4 channels, or both the eog l and eog r channels, or the emg channel were larger than the maximum physical acquisition amplitude of each channel, the corresponding 30 s data epoch was excluded. figure 1a displayed the normal eeg data which were used for further analysis."
"generally, besides asking people about their sleep, doctors also judge sleep quality by acquiring and analyzing electroencephalography (eeg) data from an entire night. as this method requires calculating the ratio between the time of each sleep stage and the total sleep duration, a crucial aspect of this method is the precision of sleep stage classification. however, for eeg data from a typical night, the average total duration is about 6-8 h, meaning a relatively large workload for a technician to manually classify the data. generally, an experienced technician can only complete the eeg datasets for 3-4 entire nights in 1 day. considering the tiredness of the technician and other subjective factors, manual classification is not an efficient approach and easily results in mistakes being made. thus, an automatic sleep stage classification method is needed."
"the problem of sleep stage classification was approached as a fiveclass classification problem, with the following classes: 0 for the waking state; 1 for the n1 sleep stage; 2 for the n2 sleep stage; 3 for the n3 and n4 sleep stages; and 4 for the rem sleep. for each input spectral stack from one 30 s eeg data epoch, if the classification model identified this input as being of class 0-4, then the sleep stage of this 30 s eeg data epoch would be set to 0-4 (i.e., the sleep stage was detected every 30 s)."
"using a nvidia geforce gtx titan x pascal gpu on an intel core i7-6900k pc, it took about 30-40 hours to train a cnn or lstm network due to the complexity of each model. in the testing stage, a 4,096 mini-batch took 2 s to test; thus, sleep stage classification for an entire night of data can be performed for 2 subjects every second."
"in this study, we proposed an lstm model for sleep stage classification with different durations of time-frequency spectral data as the input. for comparison, we also assessed the sleep stage classification performance of classical cnns. the corresponding results are discussed in detail below."
"for each input mode, a mini-batch was first constructed. the size of the mini-batch was set to 4,096, according to the gpu memory size used. considering the memory size of our workstation, we used the random splitting method to create each 4,096 mini-batch. briefly, each subject index in the training and testing dataset was randomly permuted, independently of each other. after that, the permuted training and testing datasets were split into n train and n test groups, with each group containing num train and num test numbers of subjects. next, the entire timefrequency spectral stacks from each subject in each splitting group were randomly permuted, independently in each splitting group, and used to create several 4,096 mini-batches. if the number of time-frequency spectral stacks in one group could not be exactly divided by 4,096, the size of the last mini-batch in this group was set to the remainder. finally, all of these smaller mini-batches were combined to create several 4,096 minibatches, but this time, the remainder mini-batch was excluded. in total, 1,324 training mini-batches and 684 testing mini-batches were constructed, containing about 5,324,000 training samples and 2,457,600 testing samples."
"in this study, an lstm network with a time-frequency spectra as a feature was used to perform sleep stage classification. the results showed that an lstm network with three consecutive spectral stacks as an input achieved the best performance. compared with cnns, lstm networks can take temporal information into account, and thus, may be more suitable for sleep stage classification."
"additionally, we also constructed lstm networks with an input mode that considered the time information from the previous, current and future epochs (the initial input mode we used only considered the time information from the previous and current epochs). however, the performance of this input mode was approximately the same as the input mode 3 we used. differences between these two input modes only occurred during epochs in which the sleep stage changed. however, for an entire night of sleep, this kind of epoch occurs very infrequently. this led to the training samples from these two input modes being almost the same, and thus, their performances were almost the same. assuming that an n3 stage feature occurred at time point t 2, then the technician would label the eeg data epoch between t 1 and t 2 as n2. thus, some eeg data epochs which did not have any particular features would still be classified as being from a specific sleep stage. the second reason accounting for the results is that the cnns only received the time-frequency spectral stack as an image and did not consider the temporal information within it. when temporal information was considered across the time-frequency spectral stack, the lstm networks overcame this problem to some extent, and thus, had better performance than the cnns. considering these two reasons, the first might be said to be the more important. in addition, the rules for scoring sleep stages that were designed for a human eye to make sense of objectively measured analog signals may also account for these results. these rules were thoughtfully but arbitrarily defined, and are not always very specific, evidenced by the fact that two humans rarely agree 100% on any one record. thus, the imperfect performance may also partially reflect this aspect of the scoring system itself."
"lstm network performance the lstm 1 network was better than the normal cnn network on all stages, except for the n2 and n3 stages. furthermore, the lstm 1 network had a better classification accuracy and a better ck coefficient than any of the cnns, with values of 85.3% and 0.7911, respectively. compared with figure 5, the classification performance of lstm 1 also approximated to the performance of normal cnn with input mode 3 (ck coefficient was only less 0.0069 than that of normal cnn with input mode 3), which further implied the potential advantage of time information. moreover, when two consecutive 30 s epochs (1 min) of temporal information were considered at a time from the time-frequency spectral stack, the sensitivity index of the lstm 2 network was almost better than normal cnn with input mode 3, except for the n2 and rem stage, and the classification accuracy and the ck coefficient also increased to 86.4% and 0.8074, respectively. when three consecutive 30 s epochs (1 min 30 s) of temporal information were considered at a time from the time-frequency spectral stack, the performance of the lstm 3 network again increased a little bit. however, when four consecutive 30 s epochs (2 min) of temporal information were considered at a time from the time-frequency spectral stack, the performance of the lstm 4 network was lower. for the waking, n1 and rem stages, lstm 3 had the best sensitivity with values of 94.2, 43.7, and 89.3%, respectively. for the n2 stage, lstm 3 had a sensitivity of 89.6%, which was only 0.9% less than the best-performing network among lstm networks. although for the n3 stage, lstm 3 had the worst sensitivity with a value of 72.6%, its classification accuracy and ck coefficient were the highest at 87.4% and 0.8216, respectively. thus, after this comprehensive consideration, these results suggest that the lstm 3 network has the best performance among all the networks mentioned above. figure 7 shows real classification results from an entire night of eeg data from one subject using the normal cnn with input mode 3 and lstm 3 networks. combining the information from figures 4-6, it can be seen that lstm 3 had the best classification accuracy and ck coefficient of any of the networks mentioned above, but it also had the worst sensitivity for the n3 stage. from all the networks, lstm 2 had the best sensitivity for the n3 stage. [cit], we combined the predicted probability of each sleep stage from the lstm 2 and lstm 3 networks to make the final decision. the detailed decision criteria were as follows: (1) samples which were identified as n2 or n3 by the lstm 3 network were input into the lstm 2 network and (2) if the identified maximum probability of lstm 3 was larger than that of lstm 2, the final decision would be the one identified by lstm 3, and vice versa. the results showed that, compared with lstm 3, although the sensitivity of the combined network on the n2 stage decreased a little bit, the sensitivity on the other stages increased, especially for the n3 stage, and the classification accuracy and ck coefficient also increased by 0.2% and 0.0036, respectively (figure 8) . thus, this method of combining networks would be one direction to consider in a future study."
ethical review and approval was not required for the study on human participants in accordance with the local legislation and institutional requirements. written informed consent for participation was not required for this study in accordance with the national legislation and the institutional requirements.
"note that φ k, γ k, φ * k and γ * k are simple summations that can be updated at each packet arrival in a similar way as done for (2)-(3). we can then rewrite the lomb periodogram as follows:"
"we have developed a prototype of this tool. with this tool, we investigated whether we can find any useful heuristics. through many trials of interaction, we found that users tend to give constraints to certain data pairs, which are selected from a large cluster that may be unseparated from the others, and are must-link pairs spread apart as far as possible within the cluster. we recognize it as a kind of heuristics and investigated its performance by comparing it with a random sampling and an uncertainty sampling. the compared methods are summarized as follows."
"the methods proposed so far for estimating the rtt have focused on the evaluation of the characteristics of the tcp connections for monitoring purposes: the interior monitoring point collects packets from a router interface (e.g. from both directions of a network card) and the traces are analyzed offline. in this paper, we have a completely different point of view. we consider the estimation of the rtt as a building block for actively controlling the traffic inside the network. we start considering a flow-aware networking approach, where routers are able to manage flows rather than simply packets. this in turn opens the possibility of exploring novel active queue management (aqm) techniques in order to efficiently manage the router resources."
"in this section, we show the results of experiments using three machines connected through internet (fig. 8) . the source is at inria sophia antipolis (france), the intermediate machine (hop1 ) is at university of trento (italy) while the destination is at eurecom (france). we created two ssh tunnels, one between source and hop1, and one between hop1 and the destination dest. the traffic generated from the source passes through the tunnel at hop1, where packets interarrivals are measured, and reaches the destination. table 2 shows the mean and the variance of the rtts (real and estimated) for two different experiments: the first, during a peak hour (monday, may 4th, 2009, 10 am, cest) and the second, during an off-peak hour (sunday, may 3rd, 2009, 12 pm, cest). the high variability of the peak hour results in a higher error, that is not smoothed by considering the overall mean. the reason can be understood looking at fig. 4 : the estimation algorithm often does not detect sudden decreases of the rtt. when the drops in the rtt occur, we obtain large positive error values, while there are not large negative error values to compensate for the large positive ones. figure 7 shows the whole empirical cdfs of the error. as for the testbed experiments, the estimation lies within 10% of the estimated value with probability equal to 99%."
"customers' increasing demand for better quality of service (qos) and quality of experience (qoe) [cit] have increased the interest in techniques for actively managing and controlling the traffic inside the network. with the introduction of the new generation high speed routers, it becomes possible to treat individually a significant number of flows. an example of \"flow-aware\" traffic management is alcatel-lucent's framework of \"semantic networking\" [cit] ."
"furthermore, if the rtt is known, it is possible to better estimate the flow rates: by averaging the number of packets received within an interval equal to rtt it is possible to correctly estimate the tcp throughput. a smaller value of the averaging interval results in a fluctuation of the estimate, while a bigger value does not allow for a prompt detection of rate changes (e.g. due to dropped packets)."
"the self-clocking mechanism of tcp introduces periodic components into the arrival times of packets. the basic idea of the rtt estimation is to use spectral analysis to extract such periodic components. with spectral analysis, it is possible to build an estimation of the spectrum of a signal starting from a finite sequence of samples. there is a large set of useful methods suitable for different scenarios [cit] . in order to choose the best method, the first step in spectral analysis is to identify the characteristics of the signal whose spectrum is to be estimated."
the spectral analysis represents the basic building block of our rtt estimation method: the whole estimation process is composed of different steps that we describe in detail in the following section.
"although the use of constraints is an effective approach, we have some problems in preparing constraints. one problem is the efficiency of the process. since a human user generally needs to label many constraints with \"must-link\" or \"cannot-link\", his/her cognitive cost seems very high. thus, we need an interactive system that can help users cut down such an operation cost. the other problem is the effectiveness of the prepared constraints. many experimental results in recent studies have shown that the clustering performance does not monotonically improve (sometimes deteriorates) as the number of applied constraints increases. the degree of performance improvement relies on the quality of the constraints very much. these results imply that not all constraints are useful, some are effective but some are ineffective or even harmful to the clustering. we also need an interactive system to help users select suche effective constraints that improve the clustering performance. the second problem is much more related to an active learning that has not been thus far researched in the field of constrained clustering."
"we tested our estimation algorithm both on a controlled testbed and on the internet. the results show that our solution is able to accurately estimate the rtt, the estimation error being within ±10% (resp. ±20%) of the true value with probability equal to 75% (resp. 99%). the results given by our methodology show a higher accuracy with respect to the results obtained in previous studies (see sect. 2 for a review of the literature), using less information, i.e., packets of only one direction instead of both directions."
"in table 1 we report the mean and the variance obtained from the empirical cdfs of the rtts (real and estimated). while the real rtt has more variability, the estimated rtt is more stable, due to the averaging effect of the spectral analysis."
"the tool described in this paper is still in the development phase. we are planning to provide more sophisticated functions. for example, displaying data information is a very important function because users determine the labels of the constraints based on the information. however, a methods for displaying them depend on their data type. we need to implement different methods when displaying images and document data. we are also considering implementing an active learning function that is important but rarely explored in constrained clustering, especially interactive constrained clustering. we think this active learning function may help users, or users may notice the drawbacks of the active learning algorithm. we are currently planning to conduct larger scale user studies in web clustering to evaluate the advantages of our proposed gui."
"in our case, the signal h k, the inter-arrival times of the packets, either contains a single strong sinusoidal component, with large dynamic range, due to the rtt, or it does not contain any periodicity (consider the case of a flow that has reached the bandwidth-delay product, where packets form a continuous stream). in the latter case, no method based on spectral analysis is able to extract the rtt: only methods based on the analysis of the traffic in both directions would work-a case that we cannot consider since a flow-aware router has access in real-time only to one direction. thanks to the characteristics of the signal h k, the periodogram does represent a good estimator."
"the open directory project (odp) 1 is a large humanedited directory of the web and it is constructed and 1 http://www.dmoz.org/ constantly maintained by a global community of volunteer editors. odp provides a large directory that covers various fields like the arts, business, computers, games, health, home, kids, recreation, and so on. we can easily access and utilize on a large number of web pages through the well-structured directory."
"the main flow we consider is between machines source and dest. we add additional cross traffic directed to dest, starting from source and from cross. we have a measuring point of the packets' arrivals at the machine btlnk. table 3 summarizes the experiments done. figure 4 shows an example of the output of one experiment (exp2). for each packet, we compute the rtt at the source and its estimation at the measuring point. we have found that the estimated rtt is of the same order of magnitude as the real rtt, even though it seems to be less variable than the rtt at the source. as explained in sect. ??, this is due to the averaging effect of the spectral analysis that uses the last observed 256 packets (which include 5-8 flights) for the spectrum estimation."
"another contribution lies in the fundamental frequency extraction algorithm: once the estimation of the spectrum is done, we need to extract the fundamental frequency that corresponds to the inverse of the rtt. this is done using a pattern matching technique that looks for the greatest common divisor of a subset of frequencies."
"in summary, while the rtt estimation is a well covered area of research, no techniques are available for the continuous, real-time and passive estimation of the rtt, using the traffic of one direction."
"1) human sampling: this is a sampling method using human heuristics. it selects a large cluster and the probable must-link data pairs that are greatly separated in the cluster. 2) random sampling: this method selects must-link data pairs at random. 3) uncertainty sampling: this is a sampling method that is based on a well-known active learning approach in the machine learning. original idea is introduced in the classification problem [cit], in which the most preferable data to be labeled is the nearest one to the classification boundary. we arrange this idea for selecting data pairs to be constrained. it selects the nearest probable mustlink data pair where each member belong to a cluster different from other's one. we used only must-link pairs in these experiments because cannot-link constraints often bring about a large performance deterioration and thus creates highly unstable results. since we found that none of the above three methods took advantage of cannot-link in the experiments, we omit the results with cannot-link constraints. we used three datasets in the experiments, two datasets from the uci repository [cit] and one from the open directory project. from the uci repository, we used the \"soybean-small\" and \"iris\" datasets, which are wellknown benchmarks for the machine learning algorithm. the dataset from the odp was more practical and consists of the web index pages that are listed in the directories in the odp. we select four top or sub-top directories from the odp. they were \"agriculture\", \"astronomy\", \"math\" and \"linguistics\". each directory has a certain number of registered urls. we retrieve the top pages from these urls and perform some preprocessing, such as removing the tags and stopwords and stemming. we summarize the characteristics of the three datasets in table i, where the number of data, class and dimensions of each dataset are listed."
"are immediately updated, and the periodogram is recomputed accordingly. the number of operations done at each packet arrival is o(n ), since updatingh k,"
the number of data used in the experiments is relatively small as compared with the normal benchmark datasets. computational cost of this tool depends on the number of data and the dimensions of original feature vectors. computational time is almost spent on the calculation of distance matrix and its eigen decomposition in mds. we need any specific technique or special hardwere if we apply our tool to large volume datasets.
"the summations in (2) and (3) can be efficiently updated at each packet arrival. the periodogram (1) is evaluated for a number of frequencies equal to 2n . in particular, we compute the minimum frequency at arrival of packet k, f min k, considering the interval of time of the n current samples, i.e., f"
"figures 4(a) and 4(b) shows the results from the soybean-small and iris datasets, respectively. in both graphs, human sampling (the label is \"human\" in the following graphs) thoroughly outperforms other methods (labels are \"random\" for random sampling and \"uncert\" for uncertainty sampling, respectively), the effect was especially prominent in the early stage of the interaction. although both human and uncert reaches the best score, human achieved a few steps earlier. on the other hand, figures 5(a) and (b) shows the results for the odp dataset. in this dataset, human and random are comparable in the early stages of interaction, and the human sampling gradually outperforms at around 10 steps passed. we changed the learning rate η to promote the performance of the first few steps. however, there was no effective improvement. the performance of uncert remains comparable or low to random."
"the power spectrum is to be updated at each packet arrival. since the values of τ k andh k change at each packet arrival, the summations in (1) must be always recomputed. this can be avoided by decomposing (1) using basic trigonometric properties. we define"
"we first considered a testbed set up at inria using dell precision 380 workstations running fedora linux version 10 (kernel 2.6.27). the topology is shown in fig. 2 : all machines are directly connected as shown (using ethernet cables) and there is no outside traffic. given that the propagation and processing delay are very small, we introduce random delays between the machines. the link between the source of the traffic and the machine cross has a delay uniformly distributed on [ all the links have a capacity of 100 mbps (fast ethernet) except the link between the machines btlnk and dest which is capped to 10 mbps in order to act as the bottleneck link. the random delays and the capacity limitations are implemented using the netem kernel module. throughout the experiments we explicitly configured the sender to use reno tcp and not cubic (the default option for kernel 2.6.27)."
"in this section, we explain the process of interactive constrained clustering when using our proposed tool. figure 1 shows gui (graphical user interface) of the tool, which consists of some buttons and a 2-d display area to visualize data distribution. each data is represented by a circle, triangle, rectangle or cross in the 2-d display area. users can interactively select additional constraints and reflect them to update the clustering result through the gui. we briefly describe the interaction process between a user and our tool in the following. 1) a user loads a dataset to be clustered to our tool. each data must be represented by a feature vector with a pre-defined format. the tool calculates the initial distance matrix from the feature vectors. 2) the tool runs modules of clustering (k-means) and multi-dimensional scaling (mds) [cit] modules to get the temporal clustering result and coordinates of the dataset to display on the gui. we explain the details behind mds in the next section. then, the tool displays data on the gui according to the 2- we describe the details of this update procedure in section 4. 4) repeat steps 2 and 3 until the user is satisfied with the clustering results. users can select a pair of data to assign a constraint by clicking the data figures. after selecting the data, users can assign a constraint to it by clicking the \"must\" or \"cannot\" buttons. figure 1 shows examples of mustlink (two triangles) and cannot-link (triangle and cross). then, they update the distance matrix and re-clustering by clicking the \"update\" button. we use a k-means algorithm for a clustering process."
"the file size used for the testbed and for the internet experiments is 120 mbytes. from the samples of the rtts we create the empirical cumulative distribution function (cdf). from this empirical distribution we can derive any performance index of interest, such as the mean rtt and the variance of the rtt."
"another performance index we are interested in is the error in the estimation: as fig. 6 shows, in experiments exp1-exp3, the estimation lies within 10% of the estimated value with probability 95%. in the worst case, our solution is able to accurately estimate the rtt with an error within [−10%, 10%] with probability equal to 75%, and with an error within [−20%, 20%] with probability equal to 99%."
"there have been various studies on web page clustering and web usages clustering [cit] . we took this work into consideration for interactive clustering as is one of the promising approaches for application to such largescaled clustering. thus, we think this work will provide fundamental technologies for web clustering."
"the paper [cit] makes use of spectral analysis-the basic mechanism used by our solution-as one of the possible steps for off-line estimation of the rtt. in particular, the authors apply the spectral analysis to a set of samples, but they do not consider the continuous, real-time update of the spectrum as new samples arrive. moreover, the post processing of the spectrum for the extraction of the rtt is based on a simple evaluation of the frequency with the maximum power, which not always corresponds to the fundamental frequency."
"flows for large data transfers use tcp to compete for bandwidth. more specifically, they use window based congestion control, where each flow increases its sending rate until a packet loss is detected (or an explicit congestion notification is received). conventional routers that are not flow-aware cannot insure fairness under high traffic loads, especially when the competing flows have different rtts [cit] ."
"most researches in constrained clustering use labeled datasets in their experiments and prepare constraints at random. however, it is clear that random selection is quite wasteful when humans must determine the labels of the constraints. our tool helps to reduce the labeling cost. in addition, we use selection bias for the constraints because we can recognize the proximity relation between data. this functionality may help users to find better selection strategies."
"our tool can incrementally change the clustering result based on the distance matrix updated by the above formula. figure 3 shows a series of cluster changes achieved by using this incremental algorithm. the dataset used in figure 3 is also the \"soybean-small\" one. two clusters (circle and cross) are slightly overlapping in figure 3(a), but are clearly separated in figure 3"
"if the information about the rate and its growth is made available at the router, it is then possible to design novel aqm policies that take into account this information. for instance, consider the scenario with two competing flows that are saturating the available bandwidth, i.e., the buffer occupation starts increasing. assume that both flows have, at a given point in time, the same instantaneous rate, but different rtts (which means the instantaneous flow rates have different growth rates). the router may decide to mark/drop in advance packets belonging to one or both flows, before the buffer becomes full. by knowing the rtt, it is possible to choose, for instance, the more aggressive flow, i.e., the flow with the smaller rtt."
"as we will discuss in detail in the ensuing section on related work, none of the existing methods for rtt estimation satisfies simultaneously the above mentioned criteria."
"η is a regularization parameter that determines the degree of constraint's influence. in order to analytically derive an updated formula, d a (u t, v t ) is approximated by d at (u t, v t ) . although we omit the details of the introduction process, the update distance matrix can be analytically solved."
"in this paper we have presented a methodology, based on spectrum analysis, for the passive online estimation of the rtt of a long-lived tcp connection, using one-way traffic. the estimation of the rtt represents a basic building block in the framework of \"semantic networking,\" where flow-aware routers can implement novel aqm techniques in order to control the traffic."
"meanwhile, another approach uses constraints to modify the similarities between data as the similarities of must-link pairs must be large and the similarities of cannot-link pairs must be small. there are several methods to realize this approach [cit] ."
we approach these problems by developing an interactive tool that helps users efficiently select effective constraints during the clustering process. the main objectives to build the interactive tool can be sum up as follows.
"since the method is based only on the arrival pattern, it is clear that if the arrival pattern is strongly modified inside the network, the method may not be accurate. for instance, if we observe a flow after it has passed through a bottleneck, the arrival pattern may become a continuous stream of packets. should this happen, the method may not be applicable, however one would still be able to estimate the rate of a connection through a moving average estimator for instance. in future evaluations, we plan to investigate in detail the scenarios where the methodology may not be accurate, in order to provide a comprehensive view of the benefits of our solution."
"where c is a set of clusters returned by the k-means algorithm, and t is a set of true clusters. i(c, t ) is the mutual information between c and t, and h(c) and h(t ) are the entropies. we simulated the interaction process described in section 2. we repeated 20 steps in each interaction, thus the constraints were increased up to 20. we tested 10 interactions for a dataset and ran k-means algorithm 10 times with randomized seeds in each interaction steps. thus, the value of nmi was averaged over 10 x 10 trials for a certain number of constraints."
"the algorithm is based on the problem of learning a mahalanobis distance function. given n-dimensional vectors u and v, the squared mahalanobis distance between them is defined as"
"multiple frequency search. starting from the smallest frequency in the list, we evaluate if the other frequencies in the list can be a multiple of the considered frequency. if we find at least two multiples, this step ends and returns the estimated fundamental frequency f 0 . otherwise it goes on with the following value in the list. if no fundamental frequency is found, this step returns a null value."
"in order to minimize the effects of the lag time between the rtt samples collected at the source and at the measuring point, we consider the empirical cumulative distribution function (cdf) of the rtts built from the samples. figure 5 compares the two empirical cdfs (exp2): both cdfs share approximately the same support, i.e., the estimation lies in the same interval of the real rtt."
"interactive clustering is a promising approach for implementing intelligent web interactions because it is very powerful for the interactive visualization, data mining, and data analysis of the web [cit], instead of using the conventional clustering methods [cit] . interactive clustering consists of two main techniques: active learning and constrained clustering. the active learning selects effective data that are judged by the user and used as constraints [cit] . the constrained clustering categorizes the data under such constraints judged by the user. both pursue the common requirement of dealing with the constraints given by the user [cit] ."
"it is extremely important to have a spectrum estimation methodology that works in real time, i.e., that is able to update the estimations at each packet arrival. the solution should also be computationally efficient. the simplicity of the periodogram helps in designing an online version of the method. all the other methods (even the more advanced ones recently proposed for irregular sampled data, see [cit] ) are based on complex optimized algorithms, and cannot be translated into corresponding online versions. in summary, among the different methods for spectral analysis, the periodogram represents the only choice for two reasons: fig. 1 . the rtt estimation process: after the update of the spectrum estimation, the fundamental frequency is extracted. the estimation of the rtt is the inverse of the fundamental frequency f0."
"-the structure of the signal h k (with a single strong sinusoidal component) makes the periodogram a sufficiently good estimator, overcoming its limitations; -the periodogram allows for the creation of an efficient online version of the algorithm, something not possible in the case of irregularly sampled data with other proposed spectral analysis approaches."
"the periodogram is simply the discrete fourier transform (dft) of the finite sequence. in case of irregularly sampled data, the periodogram is computed using the lomb-scargle method [cit] and it is generally referred to as the lomb periodogram. the periodogram has well-known limitations [cit] : (i) it is not possible to distinguish two sinusoidal components which are too close (low resolution), and (ii) the variance of the estimation error at a given frequency does not decrease as the number of samples used in the computation increases (non-consistent estimator). despite these limitations, the periodogram remains an efficient solution in case of well-separated sinusoids in white noise and medium to large dynamic range [cit] . the dynamic range is defined as the ratio between the maximum and the minimum power values, usually measured in decibel. a dynamic range between 20 db and 30 db (i.e., between two to three orders of magnitude between the maximum and the minimum power) is said to be \"medium\"; if it is greater than 30 db, then it is said to be \"large.\""
"with flow-aware routers, where each elephant uses a different queue, there is the possibility of new flow-aware queue management algorithms capable of proactively controlling each flow. being able to predict the behavior of a flow can be very useful in order to achieve this goal. in the case of a tcp connection, its future behavior (in the absence of a packet drop or congestion signal) is dictated by the increase of the congestion window. the rate of the increase depends on the rtt, as the congestion window is incremented by one or more packets each rtt. therefore, tracking in real time this parameter is a fundamental step in predicting the evolution of a flow. while the rtt is available at the sender, current tcp versions do not propagate this information to the routers; only experimental protocols like xcp convey this information explicitly."
"we validate our solution through measurements in a controlled testbed and over the internet showing a good accuracy. since we considered only a limited set of scenarios, we plan to extend the evaluation on different scenarios: for instance, it is interesting to understand the variation in estimation accuracy as the rtt changes due to changes in the number of connections sharing a link or in the routing."
"our work makes the following important contributions. we design a method for rtt estimation based on the traffic observed only in one direction. the method relies on spectral analysis of the signal built considering the inter-packet times. the self-clocking mechanism of tcp introduces periodic components into the arrival times of packets. we use spectral analysis to extract such periodic components. there are different tools available for spectrum estimation, mainly for regularly sampled data. the samples collected at the router are irregularly spaced, thus the choice is essentially limited, for either technical or computational reasons, to the lomb periodogram. the traditional implementation of any spectral analysis tool considers an offline approach. our main contribution is the development of an online version of the lomb periodogram. at each packet arrival, the estimation of the spectrum is updated with o(n ) operations, where n is the length of the initial sequence collected (the number of frequencies in the spectrum is 2n )."
"constrained clustering is also an important function for our interactive clustering. semi-supervised learning creating classifiers or clusters from limited supervised information and a lot of unlabeled data has been vigorously researched [cit] . in this framework, constrained clustering is used as a learning problem. in the typical setting for constrained clustering, several pairwise data are given as either must-link or cannot-link constraints."
"in this section, we analyze how our heuristic sampling changes the data arrangement through the interaction. figures 6(a)-(d) shows the data arrangements of the odp datasets. this dataset has 4 classes. for better understanding, data are represented by four types of figures (circle, triangle, rectangle and cross) based on the true class. figure 6(a) shows the initial data arrangement with a certain axis combination. the difficulty of this dataset lies in the part of intersection of all classes. thus, one of good strategies is to separate data of this part. our heuristic first selects one data from the part of the intersection, then searches the other data that is the farthest from and belongs to the same class as the first one, finally make them a must-link pair. based on this strategy, the part of intersection is gradually separated from each other as shown in figure 6 (b). the performance is improved along with this changes. this change is more clearly recognized as learning proceeded. as for the reference, we put figure 6 (c) that shows the data arrangement after 100 steps. classes are completely separated from each other. although two classes (circle and rectangle data) are overlapping, they are separated with another axis combinations. figure 6(d) shows one of such combinations."
various studies on the web have utilized odp. one of odp's typical usages is to use it as a well-classified dataset of web pages [cit] . one of our main purposes in this work is to apply the proposed interactive clustering to web page clustering. thus we built a dataset of web pages by using the structural information in odp.
"these results indicate that our tool can help users to select effective constraints. moreover, the interactive tool may help embody the user's unconscious heuristic approach by objectively watching interaction. this type of research is related to \"human active learning\" [cit] . different from their experiments, our interest exists in the \"human sampling\", where humans only selects the training examples and the learning itself is done by a machine. we consider this is more important for put machine learning to more practical use."
"the outcome of the computation of the lomb periodogram is usually noisy, with many local maxima and a global maximum that not always corresponds to the fundamental frequency f 0 : sometimes the global maximum corresponds to a frequency multiple of the fundamental one. the operations performed at each packet arrival by the fundamental frequency extraction module on the updated periodogram are summarized in the following."
"periodogram smoothing. a basic low-pass fir filter is applied to the sequence that composes the lomb periodogram: in particular, the filter is a moving average filter of order three. we have tested different orders for the filter obtaining similar results, as long as the order is not too high (e.g., greater than 8), since the smoothing effect decreases the dynamic range."
"where v is the square matrix whose columns are the eigen vectors of s. since we calculate s from a symmetric distance matrix d, s is also symmetric. thus, s can be factorized as"
"a problem common to all the methods for spectral analysis (for either regularly or irregularly sampled data, including the periodogram) is that they are based on offline algorithms: they consider a sequence of n samples and they build an estimation of the spectrum. if the signal is composed of more than n samples, it is divided into subsequences of n samples and the methodology is applied to each subsequence separately. while this approach can be suitable for traditional analysis of signals, it cannot be applied in case of rtt estimation by a flow-aware router. it may take several rtts before being able to collect n samples (typical values of n are 256, 512 or 1024). if the rtt varies, the router is not able to correctly follow these variations, leading to a wrong estimation of the flow rates, with negative consequences on the efficiency of the aqm policies."
"to provide an interactive environment in which users can visually recognize the proximity of data, and give constraints easily by mouse manipulation. 2) to provide hints for the better selection strategies through the interaction process between the interactive system and users."
at each packet arrival a new sample is added to the sequence h k . the estimation process takes as input the new packet arrival time and updates the current estimation of the rtt. figure 1 shows the functional blocks of the estimation process.
"we validated our methodology with experiments both in a controlled environment and over the internet. we focus on a single long-lived scp transfer between a source and a destination and we collect traces with tcpdumb at two points: (i) at the source, where we capture the traffic in both directions, in order to have the packets and their acknowledgments; (ii) at a measuring point between the source and the destination, where we record the packets in one direction. in addition to the observed long-lived scp transfer, there are different flows on the path between the source and the destination: in the controlled testbed, the cross-traffic flows are generated by scp transfers."
"the traces collected at the source are post-processed computing the instantaneous rtt and the smoothed rtt (srtt) using a moving average, without considering retransmitted packets. the parameter α for the exponential weighted moving average (ewma) algorithm (see rfc 2988) is set to 1/8. note that the rtt is updated for every packet."
"although other various clustering for web systems have been developed [cit], a majority of them focused on the clustering results of the search engine. we agree that such clustering is important for web applications, especially web searches. however, clustering web pages is still significant for data mining with a huge amount of web data. we proposed a promising interactive framework to deal with such noisy web pages and showed its feasibility."
"one of the most important notions for a tcp flow is its aggressiveness or how fast a tcp connection increases its sending rate. most deployed tcp versions are based on congestion window which limits a number of packets that can be sent during one round trip time (rtt). this implies that the rtt is one of the principal parameters which determine the aggressiveness of a tcp flow, and it needs to be taken into account for the design of new \"flow-aware\" traffic management schemes."
"the basic architecture can be summarized as follows. there is a single physical buffer that is divided into a number of virtual queues, one of which has high priority and the others have low priority. flows are sorted into two classes, whether they are short-lived or long-lived flows. the basic idea is to put all the short-lived flows in the high priority queue, and to assign a low priority queue to each of the long-lived flows. when a new flow arrives, it is initially treated as short-lived. if the cumulative number of packets of a given flow reaches a threshold, this flow is then considered as an elephant, i.e., its packets are enqueued in a low priority queue associated to this flow. 4 low priority queues are served in a round robin fashion or a similar scheduling policy."
"in this paper, we present an algorithm for the online estimation of the rtt by passively monitoring, in real-time, only one direction of a tcp flow. specifically, our algorithm satisfies different constraints. the estimation is passive, as the measuring point (e.g., the router) does not inject packets into an existing flow, nor does it alter their flow. the estimation is done in real-time, since the growth rate of a flow should be instantaneously available at the measuring point. this constraint implies that the used algorithms must be both computationally efficient and working incrementally as new packets arrive. in other words, we need to find efficient online algorithms that provide sufficiently accurate results. the last constraint imposes the use of information on only one direction of a tcp flow. one cannot assume that the monitoring point sees packets in both directions as forward and reverse paths may be different. furthermore, even when forward and reverse traffic do flow through the monitoring point, collecting and real-time processing of two-way traffic may impose excessive load and complexity on the network cards."
"this paper presented an interactive tool for constrained clustering that provides some basic functions such as the display of a 2-d visual arrangement of a dataset, constraint assignment through mouse manipulation, incremental learning of a distance matrix, and clustering by kmeans. these functions help users intervene in the process of constrained clustering, and finally create a satisfactory clustering result with less user cognitive load than that for a clustering process under randomly selected constraints. in addition, the selection bias of the constraints may help users find better selection strategies. we consider our proposed gui is a promising approach for large-scaled applications like web clustering."
"in addition to the 2-d visual arrangement of a dataset and the constraint assignment function, our prototype tool has distance metric learning and k-means clustering that can be quickly executed as the background process. using these functions, the users can compare the results of the clustering before and after the constraints addition easily. we consider such interactions helpful for providing hints for better selection strategies. although there are many datamining tools that have clustering function, we have not found any other tool in the following sections, we first explain an overview of our tool in section 2. then, we describe the two main functions of the tool -display arrangement by multi-dimensional scaling and incremental distance metric learning, in sections 3 and 4. section 5 shows the experimental results where we compare the performance of a heuristic sampling method found by our tool with random sampling and a well-known active learning technique. section 6 analyzes the data arrangement through the interactions. finally, we conclude our work in section 7."
"the beamformers we have seen thus far are fixed beamformers (fbfs), which only rely on the doa or the rtfs of the target source. fbf designs are suitable when the target direction is known a priori, e.g., in cellphones, cars or hearing aids. in these cases, the beamformer is designed to focus on the target source while minimizing noise and reverberation arriving from other directions. these designs require low computational complexity, but they may be prone to performance degradation when the microphone positions are not accurately known (see section iv-a). a semi-fixed beamforming approach, suitable for cases when the position of the target source cannot be determined in advance, is to estimate its doa and to design a fbf steered towards it. alternatively, the airs or the rtfs between the target source position and the microphones can be estimated during a calibration process and used to construct a matched-filter fbf [cit] ."
"the goal of the test case generator is to produce traces which are valid sequences of calls to lingeling's api. for our prototype we encoded the models necessary to describe valid traces, input data, and options directly in c, which allows direct communication with the solver. for test case generation, no intermediate layer is necessary. by sacrificing generality, the prototypical implementation is tailored towards testing lingeling and allows to gain a first understanding of the power of the suggested approach for testing a state-of-the-art sat solver. a sample trace is shown in fig. 4 . api model. the api model (see fig. 3 ) documents some contracts which have to be fulfilled when using the api. the omitted features deal with additional optimization techniques which have to be called at certain positions within the model. after initialization (state init), options (state opts) may be set. the path to be taken is decided by random. by empirical evaluation it turned out that setting options with a probability of 0.5 is a good choice. if the path to opts is taken, then options are set according to the option model. in the next step, the formula to be solved has to be generated. here, knowledge of the data model is necessary."
"we consider the number of lines executed during one test case as an important metric for the quality of the test case. to obtain the number of executed resp. covered lines, the binary was recompiled with debugging support (-g). the compiler was also instructed to include code for producing coverage information. after running the test case the number of executed lines was determined with the help of gcov. for each tester resp. delta debugger the average numbers are calculated over all successful runs, while the relative numbers give the same information, but are normalized w. r. t. the tester."
"). this can be intuitively understood from the illustration in fig. 9 . as any other nonlinear optimization strategy, the em algorithm does not guarantee convergence to a global maximum. providing an appropriate initialization of the parameters θ is therefore very important."
"in principle, only sub-traces are extracted, but it has to be ensured that the api model discussed in the previous subsection is not violated, i. e., certain parts like the initialization and the release commands may not be removed. also the values of the solver options are changed during the trace reduction process, with the hope that another configuration of the solver triggers the failure earlier."
"in this section, we provide some theoretical results on the convergence and accuracy of recursive fmp. due to the page limit, the proofs are omitted here and provided in the journal version of the paper in preparation. v. experimental results in this section, we demonstrate the performance of recursive fmp using both simulated models on grids and a large-scale gmrf with about a million variables."
"in spite of its limited scope, this overview still covers a wide field of research. in order to classify existing techniques irrespectively of their origin in microphone array processing or bss, we adopt four transverse axes: 1) the acoustic impulse response model, 2) the spatial filter design criterion, 3) the parameter estimation algorithm, and 4) optional postfiltering. these four modeling and processing steps are common to all techniques, as illustrated in fig. 1 . the structure of the article is as follows. we recall useful elements of acoustics and introduce general notations in section ii. after describing various acoustic impulse response models in section iii, we define the fundamental concepts of spatial filtering in section iv and review existing design criteria, estimation algorithms, and postfiltering techniques in sections v, vi, and vii, respectively. we provide a list of resources in section viii and conclude in section ix by summarizing the similarities and the differences between approaches originating from microphone array processing and bss and discussing perspectives in the field."
"this section concludes this survey article. first, we discuss the two major algorithmic families introduced in this survey, namely microphone array processing and bss and their differences and similarities. then, we provide some guidelines on the selection of the proper algorithm, based on the acoustic scenario and available resources. we conclude this section and the entire article by reviewing some current and future trends in the field."
"let us now consider the binary activation model in (64) . as explained in section v-e, given the index j (n, f ) of the active source and the model parameters θ, the optimal value of the predominant source s j (n, f ) is obtained by the mvdr beamformer. the log-likelihood then simplifies to"
"sound is a variation of air pressure on the order of 10 −2 pa for a speech source at a distance of 1 m, on top of the average atmospheric pressure of 10 5 pa. for such pressure values, the wave equation that governs the propagation of sound in air is linear [cit] . this has two implications: 1) the pressure field at any time is the sum of the pressure fields resulting from each source at that time; 2) the pressure field emitted at a given source propagates over space and time according to a linear operation. unless clipping occurs, microphones operate linearly to record the pressure value at given point in space. if one considers the pressure field emitted by each source as the target, 1 the overall phenomenon is therefore linear."
"after having created the formula, optimizations are performed with a certain probability. the formula is then handed to the solver. after completing the solving process, the incremental feature of the solver may be tested by changing to the state inc (this is only possible if the formula is sat), to extend the formula with additional constraints, and to start the solving again. alternatively, the solving process could be stopped. if this is done according to the api contract, some functions to free memory have to be called."
which is exactly the lcmv beamformer. it is easily verified that the lcmv beamformer is equivalent to the linearly constrained minimum power (lcmp) beamformer [cit] :
with b j k (f ) the basis spectra and h j k (n) the time activations [cit] . the set of parameters to be estimated
"in general, sat solvers implement conceptually simple algorithms to decide the (un)satisfiability of a propositional formula. a propositional formula is a conjunction of clauses. a clause is a disjunction of literals, with a literal being a variable or a negated variable. the task of a sat solver is to find an assignment to each variable such that the overall formula evaluates to true in case of satisfiability or to show that there is no such assignment in case of unsatisfiability. a variable may be assigned the value true or false. a negated variable ¬x is true (resp. false) if it is assigned false (resp. true). a clause is true if at least one of its literals is true. a formula is true if all of its clauses are true. propositional formulas of the described structure are said to be in conjunctive normal form (cnf), which is the default representation for state-of-the-art sat solvers."
"where θ (l) denotes the estimated model parameters at the l-th iteration of the algorithm. the auxiliary function q(θ, θ ) is proven [cit] to satisfy"
"note that spatial filtering is performed in (109) as part of the e-step. this can be thought of as a feedback loop from spatial filtering to parameter estimation, as illustrated in fig. 1 ."
"option model. the description of the options to be generated uses an introspective api function of lingeling which allows to query the solver for its available options and how they shall be initialized. a list of options to be excluded from testing is also provided, including options related to logging. an option is set to a new value with a probability of 50 %. the choice of the new value depends on the range of valid option values."
"for replaying the traces, a simple interpreter is provided. this interpreter executes not only the traces produced by the fuzzer and delta debugger, but also traces produced during all runs of the solver. the solver is equipped with a logging functionality implemented by the means of a macro calling a certain api function of lingeling, which outputs every api call in the required format. logging can be enabled through an api call or by setting an environment variable (lglapitrace) to point to the trace file."
"data model. lingeling processes propositional formulas in cnf. in our framework, this formula is randomly generated. unlike in previous work, the formula is not written to a file, but it is fed programmatically to the solver. api calls are used to add literals, represented as positive and negative integers. the generated formulas should not be trivial, but they should also not be too hard, to avoid that the solver does not terminate. it also has to be ensured that no tautological clauses are generated, i. e., clauses which contain a literal in both polarities."
"a schematic illustration of the shape of an air is provided in fig. 2 . it consists of three successive parts. the first peak is the direct path from the source to the microphone, as modeled in (1). it is followed by early echoes corresponding to the first few reflections on the room boundaries and the furniture. subsequent reflections cannot be distinguished from each other anymore and they form an exponentially decreasing tail called reverberation. this overall shape is often described by two quantities: the reverberation time (rt), that is the time it takes for the reverberant tail to decay by 60 decibels (db), and the direct-to-reverberant ratio (drr), that is ratio of the power of direct sound (i.e., direct path) to that of the rest of the air. the rt depends solely on the room, while the drr also depends on the source-to-microphone distance. the rt is virtually equal to 0 in outdoor conditions due to the absence of reflection and it is on the order of 50 ms in a car [cit], 0.2 to 0.8 s in office or domestic conditions, 0.4 s to 1 s in a classroom, and 1 s or more in an auditorium [cit] . fig. 3 depicts a real air measured in a meeting room. it has both positive and negative values and it exhibits a strong first reflection on a table just after the direct path, but its magnitude follows the overall shape in fig. 2 ."
"we can therefore conclude that the array processing and bss paradigms share many underling concepts, and will eventually converge to a point in which they will become indistinguishable."
"the main contribution of this paper is therefore to introduce model-based api testing and model-based option testing, their combination with delta debugging, and an empirical evaluation showing the effectiveness of our framework."
"in future work, we plan to compare our dedicated mutation tool to more general approaches like milu [cit] and extend the presented testing framework to multi-threaded and reentrant engines. testing our smt solver boolector through its api is another target. furthermore, we plan to investigate how the design of the input models is correlated with the quality of the generated test cases. today many developers of smt and sat solvers rely on fuzzing and delta debugging from our previous work. our new approach described in this paper is much more effective and efficient, and is hoped to have a similar impact."
"in the next subsections, we will elaborate on specific structures and implementation of widely-used beamformers. in section v-b two important distortionless beamformers, namely the mvdr and lcmv beamformers, are discussed. we extend the discussion on mmse beamformers in section v-c and elaborate on methods to control the level of distortion. beamforming structures that extend the narrowband approximation are discussed in section v-d. spatial filtering criteria that go beyond second-order statistics of the signals are presented in section v-e."
"a straightforward implementation of the bm is given by selecting the first i − j p columns of the projection matrix to the null subspace ofǎ, given by:"
"the effectiveness of random resp. fuzz testing depends not only on the quality of the generated tests, but also on the number of test cases executed per second, which we define as throughput. for our mbt approach we achieved a throughput in the order of 251 test cases per second: 919,058 test cases were executed during one hour of running the model-based tester on an intel xeon e5645 2.40 ghz cpu. note, that roughly 10 % of these test cases are actually terminated early due to contract violations. this occurs because the model is not precise enough to entirely exclude invalid api call sequences. those are executed at the point where the contract violation is detected (by api contract assertions in the library) and thus can still be considered valid test cases. this feature of our approach allows a trade-off between the effort needed to capture api contracts precisely in the model and the effectiveness of testing."
"two main paradigms for speech enhancement and source separation were explored in this survey, namely microphone array processing and bss. we claim here that recent trends are showing that these two paradigms are converging by borrowing ideas from each other. gray cells mean that either the dataset or the evaluation metric was not considered during the corresponding evaluation campaign, \"-\" sign means that the corresponding dataset was not processed entirely, and the highest scores are in bold."
"using these definitions and the assumption that speech and noise stft coefficients are gaussian distributed, it can be shown (see [cit] ) that λ is given by"
"spherical microphone arrays [cit] have also attracted attention, due to their ability to symmetrically analyze tridimensional sound-fields [cit] (see also dual-radius spherical arrays [cit] ). this analysis is conveniently carried out in the spherical harmonic domain by using the spherical fourier transform (sft). the interested reader is referred to a recently published book entirely dedicated to this topic [cit] ."
"the spatial covariance can be either time-varying or timeinvariant. in the former case, it can be represented via a dynamical model [cit] . in the latter case, it simplifies to"
"besides the above deterministic characterization of airs, it is useful to adopt a statistical point of view [cit] . to do so, we decompose airs as"
"the representation of the spatial filtering capabilities of beamformers as a function of the doa is not very informative for unstructured arrays, whose geometry does not comply with a particular structure, e.g., linear, circular or spherical. moreover, sound propagation in a reverberant environment is much more intricate than in free field."
"we include runs with and without fuzzing options resp. with and without delta debugging of options. in the experiments for model-based testing with option fuzzing, we distinguish two variants of delta debugging. the first variant \"∆ dbg 1\" only reduces options explicitly set in the failing test, while default options are not changed. the second variant \"∆ dbg 2\" considers all options for delta debugging. note, running regressions only allows to delta-debug options but does not really allow to fuzz them."
"iii. recursive fmp in many practical networks, each node has limited local memory and communication bandwidth. in addition, individual nodes often do not know the diameter of the whole networks. in recursive fmp, each node i has a local list of feedback nodes, denoted by l i . the set l i is initially empty and has a maximum size of k i . as will be explained later, the number of messages node i sends out is proportional to the current size of l i . another parameter d, called the effective diameter, indicates the default estimate of the network diameter. there are three stages in recursive fmp: in the first stage, feedback nodes are elected using a distributed algorithm similar to the \"leader election\" algorithm [cit] . the current feedback nodes are also called inactive nodes since they do not pass any messages; the non-feedback nodes are referred to as active nodes. the inactive nodes later \"wake up\" to become active nodes again. we denote the set of feedback nodes by f and the set of active nodes by a. the subgraph induced by all active nodes is called the active subgraph (the active subgraph before any feedback node wakes up is called the initial active subgraph). in the second stage, lbp is run on the initial active subgraph while the feedback nodes remain inactive. in the third stage, each of the feedback nodes wakes up to become an active node when some local conditions are satisfied and broadcasts correction messages. in practice, the three stages are integrated together and have no clear separation; however, for clarity, we present the protocols in three separate stages."
"as mentioned above, the narrowband approximation holds only when the frame length is sufficiently long. time-domain filtering can be exactly implemented in the frequency domain using overlap and save techniques [cit], provided that the analysis frame-length is larger than the filter length. however, this framework necessitates rectangular windows of different length in the analysis and synthesis stages. this might limit the performance of the separation algorithms, especially in dynamic scenarios."
"the operator diag(·) denotes a diagonal matrix with the argument on its diagonal and q α,1j denotes the first element in the j-th column of the matrix q α . constructing the lcmv beamformer with the constraints set in (59) can be shown to be equivalent to the construction with (58) . moreover, using (58) relaxes the requirement for estimating the rtf vectors for each of the sources, and substitutes it with estimating two basis matrices, one for each group of sources (desired and interfering, respectively). a practical method for estimating the basis matrices q α and q β is discussed in section vi-b."
"in order to be able to compare the effectiveness of trace based and cnf based techniques, we show in the remaining columns the size of failing test cases as well as the number of lines executed. the size of a failing test is measured in terms of the size of the api trace produced either directly by the model-based tester or obtained implicitly after tracing the api calls when reading and solving the cnf file. commands to set options are not counted. this size metric allows to compare sizes of test cases across different testers (with and without fuzzing options)."
"the length of individual clauses is decided as follows. clauses of length one, two or three are special and are handled differently than other clauses. for example, in unary clauses (clauses of length one), the truth value of its literal can be decided immediately and therefore be propagated to all other occurrences of the respective variable. the generation of these three kinds of clauses is fostered by giving them a higher probability to be generated than other clauses. the length of a clause is naturally constrained by the number of variables occurring in a formula. a variable in a clause is negated with the probability of 50 %."
"in this section, we show how the presented testing framework is used for testing the sat solver lingeling. therefore, the three different models have to be specified as well as the different components presented in the previous section. the framework is available at http://fmv.jku.at/lglmbt. before we discuss the details of the testing framework, we shortly review the features of lingeling."
"one limitation of the ica criterion is that it is invariant with respect to permutation of the sources. yet, the order of the sources must be aligned across the frequency bins. linear constraints [cit] such as the one used for mvdr and penalty terms constraining a j (f ) to vary smoothly over frequency [cit] or to be close to the anechoic steering vector d j (f ) [cit] have been used to constrain the optimization (68) . post-processing permutation alignment techniques which exploit the additional fact that the source short-term spectra are correlated across frequency bands have also been proposed [cit] ."
"this expression only holds for sources such as human speakers which emit sound in a tight region of space. the airs result from the summation of the multiple propagation paths and they vary over time due to movements of the source, of the microphones, or of other objects in the environment. when such movements are small, they can be approximated as time-invariant and denoted as a j (τ )."
"let us denote by θ the set of model parameters. when no prior information about the model parameters is given, θ is often estimated in the ml sense as"
"the second family consists of bss algorithms [cit], which are based on the fundamental assumption of mutual statistical independence of the different source signals."
"given an input which observably triggers a failure of the system under test, the delta debugger has the goal to simplify the input while preserving the failure. on this simplified input the analysis of reasons for the failure, i. e., the debugging, becomes easier. in the context of sat, input formulas often consist of tens of thousands of clauses. for a human developer it is hardly feasible to manually step through the code of the sat solver when such a huge input is processed. in order to reduce the input to a new syntactically correct test case, and also for the delta debugging process itself, knowledge on the structure of the input data is useful. for sat formulas in cnf, delta debuggers remove either clauses or some literals of a clause. delta debuggers for non-cnf formulas like qprodd 5 or for smt like deltasmt 6 and ddsmt 7 need more sophisticated reduction techniques, since the underlying data structures are trees instead of lists of lists (see also [cit] )."
"i.e., it is a lower bound of l(θ) that is tight at the current estimate θ . these properties are enough to prove that the cost function l(θ) is non-decreasing under the update (107), i.e.,"
"the option model describes valid options and valid combinations of options of the sut. for example, if an option requires an integer of a certain range, this constraint is documented in the option model. further, the model might contain probabilities stating the chance that a certain option is selected to be set. this feature is necessary to avoid that options which are not so relevant are tested too extensively at the expense of other, more important options. the selection of the probabilities is based on the experience of the modeler."
"using a trace interpreter to replay a trace has another practical advantage. if a solver is used as a verification back-end in a larger verification system, it might happen that the solver triggers a failure when a certain sequence of api calls is performed. it might be difficult to reproduce the failure by simply dumping the formula and passing it as command line argument, because internally the solver follows another sequence of api calls. in order to report the defect to the solver developer without giving away the whole verification system, the trace of the failing run of the solver might be produced (under the assumption that the solver is equipped with a logging functionality). the solver developer can replay the trace, analyze the undesired behavior, and fix the bug."
"where r(n, f ) denotes position estimate of source active in timefrequency bin (n, f ). a plethora of methods exist for estimating source positions, however, this topic is beyond the scope of this overview paper. by adopting a gaussian model for the error of the estimated position, and by applying bayes rule, the spp in (95) can be reformulated as"
"the narrowband approximation significantly simplifies estimation algorithms, since the decoupling between frequencies reduces the dimension of the problem. however, it may raise other problems, most notably gain ambiguity and permutation ambiguity. these ambiguities can be mitigated by smoothing between nearby frequencies [cit] or by introducing geometrical (soft or hard) constraints [cit] . interestingly, the latter references demonstrate the common foundations of microphone array and bss methods for separating speech sources in reverberant environments."
"in the so-called underdetermined case, when the number of sources j is larger than the number of microphones i, ica cannot recover all sources anymore and joint ml estimation of a(f ) and s j (n, f ) is difficult. an approximate solution is to obtain a(f ) first using, e.g. the techniques in section vi-b3, and to subsequently estimate s j (n, f ) in the ml sense:"
"we combined the presented model-based testing approach with delta debugging, to reduce failure triggering traces. this combination of model-based testing and delta debugging is a powerful tool for testing verification back-ends. as proof of concept we implemented the proposed testing framework for the sat solver lingeling and performed an extensive empirical evaluation. different kinds of experiments confirmed the effectiveness of model-based testing in combination with delta debugging. based on these experiments and on our long-time experiences in solver development, we believe that the techniques described in this paper are effective in general, and are particularly useful when applied to other formal reasoning engines like smt solvers, theorem provers, or model checkers."
"the gsc structure is utilized to combine spatial robustness considerations (in the bm block) and numerical robustness considerations (in the adaptive noise canceller (anc) block) [cit] . a gsc-type beamformer utilizing advanced bss techniques, namely trinicon, is also proposed for increasing robustness [cit] ."
"for the api testing approach, the delta debugger not only has to reduce the input data, i. e., the formula, but the trace itself such that the failure still occurs. as a trace is a linear sequence of api calls, no complicated rewriting is needed when a call is removed as it would be necessary if the internal node of a tree is removed. however, the delta debugger has to obey the description of the api model in order to maintain a valid trace. for example, there might be calls which may not be removed, like the initialization and release routines or the function which starts the actual solving process."
"the gsc implementation hence consists of two branches, as depicted in fig. 8 . the upper branch is responsible for satisfying the constraint set, and is usually denoted fbf. it should, however, be stressed that in some scenarios the constraint matrix is time-varying, e.g. when the sources are free to move. even in such scenarios, the term fbf, although inaccurate, will still be used. a widely-used fbf is the perpendicular to the constraint set:"
"both the api model and the option model contain information specific to the sut. since verification back-ends like sat solvers often have similar functionality and apis, reuse is achievable by specifying a generic model and appending to each state the api calls to be performed. if another system with similar functionality has to be tested, only the names of api calls have to be changed."
"considering the indexes j (n, f ) of the active sources as latent data, the following em algorithm can be derived: 1) e-step: compute the posterior probability of j (n, f )"
"we measured the code coverage with the tool gcov of the gnu compiler collection. the evolution of the code coverage for 10,000 runs is shown in fig. 5 . cnf fuzzing achieves code coverage of about 75 % after 10,000 runs which could be improved by 5 % by mbt without option fuzzing, and by additional 5 % by mbt with option fuzzing. the difference between cnf fuzzing and the mbt approaches might be explained by the fact that cnf fuzzing does not test the incremental feature of the solver. coverage of 100 % is not possible due to the fact that only correct formulas and traces are generated, so the error handling code is never called. we observed that even for more runs, the values do not change anymore. creating corrupted input for testing error handling is not within the scope of this work, but might be interesting in the future."
"in this stage, each feedback node k becomes active again when some local conditions are satisfied and broadcasts correction messages (which are called correction messages about feedback node k). the third stage ends when all nodes are active and all correction messages have been broadcast. the message protocol is as follows (see figures 2b-2d for illustration)."
"alternatively, the sut may be wrapped into an execution environment, where a trace interpreter interacts with the sut. the trace interpreter has to be developed for each sut individually and is able to call the functions of the sut directly. the output of the sut may then be translated into a format which can be processed by the testing framework. the trace interpreter allows to replay the trace reduced by the delta debugger which can then undergo manual debugging in order to find and eliminate defects."
"actually, the work presented in this paper can be seen as a crucial technique for enabling the integration of an incremental sat solving api into lingeling, which in turn made it possible to use a state-of-the-art sat solver back-end in our smt solver boolector. [cit] competition is attributed to this fact."
"this can be achieved by the following gem algorithm [cit] (one iteration is given below): 1) e-step: compute expected sufficient statistics c j (n, f ) and σ c j (n, f ) as in (109) and (110). 2) m-step: update r j (f ) as in (111), and update b j k (f ) and h j k (n) via multiplicative update (mu) rules [cit] as:"
"over the years, the above speech enhancement and source separation techniques have led to a number of software tools, which are referenced on repositories such as lva central 7 and 8 in the following, we provide a non-exhaustive list of popular resources, databases and results, which will be useful for readers to get an idea of the typical performance that may be achieved and to start their own work in the field."
"the beamformers we have seen thus far rely on the narrowband approximation. the underlying mmse criterion can also be used when this approximation does not hold, e.g., with interframe, inter-frequency, or full-rank covariance models."
"with three different kinds of input models, better control on the test case generation is achieved, assuming the models reflect the behavior of the sut in an accurate manner. if a model is too restrictive, code coverage is decreased for the test case generator and also less reduction can be achieved by the delta debugger. if the model is too lax, i. e., not precise enough, (external) contracts of api functions might be violated and invalid traces are generated."
"we propose to apply model-based testing for verification back-ends, like sat solvers. in this approach, not only the input data is randomly generated, but also sequences of valid api calls. this makes it possible to test, for example, the incremental features of sat solvers. these incremental features play an important role in verification applications. besides that, we additionally included option fuzzing in our testing framework, which randomly selects different configurations of the sut."
"where ∠ denotes the phase in radians of a complex number. the itd is unambiguously defined only below the frequency c/ i1, known as the spatial aliasing frequency, with i1 the distance between microphones i and 1. with a sampling rate of 16 khz, this corresponds to a sensor spacing of less than 4.3 cm. above that frequency, the phase difference becomes larger than 2π and the itd can be measured only up to an integer multiple of 1/ν f . for that reason, the interchannel phase difference (ipd)"
"we have surveyed a plethora of algorithms and methods for speech enhancement and separation. in this section we will not attempt to pick up the \"best\" algorithm, but rather give guidelines for selecting the most appropriate class of algorithms for a given scenario."
"such a filter is called a beamformer. the term beamformer originally referred to directions of arrival (doa) based filters and was later generalized to all linear spatial filters. we will see in sections v-e and vii that there also exist nonlinear spatial filters, which we will simply call \"spatial filters.\""
"a common fbf is the ds beamformer [cit], which consists of averaging the delay-compensated microphone signals. although simple, it can be shown to attain the optimal directivity for a spatially-white noise field. the beamwidth and sidelobe levels of the beampattern can be further controlled by spatial windowing of the microphone signals before averaging them. this is simply implemented as a weighted-sum beamformer [cit] ."
"experiences showed that formulas with between 10 and 200 variables give the best results. if n is the number of variables, the number of clauses is given by (n * x)/100 where x is a number between 390 and 450. again, the values are based on many years of solver development experience, but related to the phase transition threshold of sat solving."
"in our approach, both the test case generator and the delta debugger communicate with the sut to achieve better results. for the test case generator this means to get a high coverage rate for uncovering defects. for the delta debugger this means to reduce the failure-triggering traces as much as possible. communication can be achieved in two ways: either the test case generator and delta debugger directly call api functions. although this is the more direct implementation, it reduces the reusability of the framework. alternatively, calls could be attached to the transitions of the api model. if a transition is taken, its attached function is invoked as in modbat [cit] . a potential issue of that approach is that such a testing framework may only support certain programming languages. furthermore, the testing framework has to interpret the output and return values of the sut; this again makes the testing framework tailored towards a given system."
"to measure the throughput of file-based fuzzing we piped the output of the fuzzer to lingeling to avoid disk i/o. still the throughput did not reach more than 25 runs per second, measured for batches of 100 runs, on the same machine. this is an order of magnitude slower than for mbt. in both cases we used a binary compiled with -o3 but assertion checking enabled (so no -dndebug). by this huge difference in the throughput, the benefits of accessing the solver via the api become directly visible. no time is wasted with i/o and parsing."
"2) array geometry: if the specific array geometry is known, e.g., linear, differential or spherical, preference should be given to array processing or source separation methods which exploit this information. in entirely blind scenarios, bss methods are commonly used, however some modern array processing methods based on atfs or rtfs can also be used."
"if there is only one source in noise, the natural choice would be the mwf/mvdr family of beamformers. if multiple sources of interest exist (either desired or interference) but their number remains smaller than the number of microphones, then lcmv beamformers or bss methods can be considered. if the number of microphones is smaller than the number of sources of interest (under-determined problem), then the speech sparsity should be utilized, usually in conjunction with bss methods, but also with modern beamformers."
"as for the test case generator, the delta debugger communicates with the system under test in order to incorporate the result of a selected action into the reduction process. since the mere removal of an api call may not be enough to obtain the expected reduction, it might be necessary also to vary the arguments of a call."
"frames) is shorter than the stationarity time of the noise. alternatively, a hard binary weighting, obtained by applying a threshold to the spp, can be used instead of the soft weighting."
this has inspired researchers to design spatial filters that take this distribution into account. this is typically achieved by optimizing a maximum likelihood (ml) criterion under the narrowband model (6) . three approaches have been proposed.
"although many methods for increasing the robustness of beamformers in speech applications can be found in the literature, designing a robust beamformer that takes room acoustics and speech properties into account, remains an open research question."
"the test case generator takes three different kinds of models as input: (1) the data model, which is basically the same as the data model used in the approach described in the previous section. additionally, (2) the option model and (3) the api model have to be provided."
"from now on, we assume that two or more sound sources are simultaneously recorded by two or more microphones. the microphones are assumed to be omnidirectional, unless explicitly stated otherwise. the set of microphones is called a microphone array. each recorded signal is called a channel and the set of recorded signals is the array input signal or the mixture signal."
"where c is the spatial image of the speech source and u is the sum of all noise components. the microphone and sources indices are omitted for brevity and the time and frequency indices are (n, f ) unless otherwise stated. denote the speech activity and absence hypotheses in time-frequency bin (n, f ) as h s and h u, respectively. the problem at hand can be viewed as a classical hypothesis testing problem. denote the a posteriori snr as:"
"the api model describes valid traces of api calls. it documents how the api has to be used. to generate a trace, the results of fuzzing options and input data have to be included. hence the test case generator has to combine the three models. for example, the input data may be either read from a file or it may be programmatically handed over by dedicated api calls. as the call of certain functions might be optional, also the api model may be equipped with probabilities for the selection of functions which are not mandatory to be called."
"(40) the mwf cost function comprises two terms. the first term is the power of the speech distortion induced by spatial filtering, while the second is the noise power at the output of the beamformer. these two terms are also known as artifacts and interference, respectively, in the source separation literature."
"an alternative approach to handle the gain ambiguity is to consider the relative transfer function (rtf) between channels for a given source. taking the first microphone as a reference, the vector of"
"2) deep learning-based parameter estimation: deep neural networks (dnns) have emerged as a promising alternative to spp estimation, em, vb, or mm in the situations when large amounts (typically, hours) of source signals similar to those in the mixture to be separated, are available for training. dnns model complex, high-dimensional functions by making efficient use of this wealth of data. they typically operate in the magnitude stft domain, take several frames of the mixture as inputs, and output the spp or the spectra of all sources in each time frame. most work in this area has focused and still focuses on single-channel separation using spectral cues or channel-wise filtering using ild and itd cues [cit] or pitch and azimuth [cit] (using multi-layer perceptrons)."
"the above properties of airs can be modeled and exploited to design enhancement techniques. five categories of models have been proposed in the literature. a model is defined by a parameterization of the airs and possible prior knowledge about the parameter values. this prior knowledge can take the form of deterministic constraints, penalty terms which we shall denote by p(.), or probabilistic priors which we shall denote by p(.)."
"these constraints are based on the fact that, in the absence of echoes and reverberation, the vector of atfs simplifies to the steering vector, that is the dft of (1):"
"). this algorithm still guarantees that l(θ) is non-decreasing over the iterations and is referred to as generalized expectation-maximization (gem) algorithm [cit] . these updates may result from gradient ascent, the newton method, or explicit optimization over a discretized set, for instance [cit] ."
"in this paper, we propose recursive fmp, a recursive and purely distributed extension of fmp, where all nodes use the same message-passing protocol. in recursive fmp, an inference problem on the entire graph is recursively reduced to those on smaller and smaller subgraphs until the final inference problem can be solved efficiently by an exact or approximate message-passing algorithm. a purely distributed algorithm is of great importance because in many scenarios, such as wireless sensor networks, it is easy to implement the same protocol on all nodes while centralized computations are often expensive or impractical. in this recursive approach, there is only one active feedback node at a time, and thus centralized communication among feedback nodes in fmp is reduced to message broadcasting 1 from the single feedback node."
"1) estimating the speech presence probability: many speech enhancement algorithms, implemented in the stft domain, require information regarding the temporal-spectral activity of the speech signals. contrary to the voice activity detection (vad) problem where low resolution is sufficient, high-resolution activity estimation in both time and frequency is required here for proper enhancement."
"due to these small values, many successive wave reflections typically occur before the power becomes negligible. this induces multiple propagation paths between each source and each microphone, each with a different delay and attenuation factor. the waves corresponding to different paths are coherent and may result in constructive or destructive interference."
a feedback vertex set (fvs) is defined as a set of vertices whose removal (with the removal of the incident edges) results in an cycle-free graph [cit] . a pseudo-fvs is a subset of an fvs that breaks not all but most crucial cycles.
"peech enhancement and separation are core problems in audio signal processing. real-world speech signals often involve one or more of the following distortions: reverberation, interfering speakers, and/or noise. in this context, source separation refers to the problem of extracting one or more target speakers and cancelling interfering speakers and/or noise. speech enhancement is more general, in that it refers to the problem of extracting one or more target speakers and cancelling one or more of these three types of distortion. if one focuses on removing interfering speakers and noise, as opposed to reverberation, the terms of \"signal enhancement\" and \"source separation\""
"where the matrices q α and q β are arbitrary bases spanning the column-subspace of the matrices a α and a β, respectively, and q α andq β are their normalized counterparts defined as:"
"the experiments showed that our mbt approach is substantially more effective in finding defects than previously used techniques. taking the time-outs into account, it is also faster and even without option fuzzing produces much smaller test cases. fuzzing and in particular delta debugging of options is particularly effective in reducing the size of traces. we see a reduction of almost two orders of magnitude by delta debugging options, while delta debugging without touching options gives a reduction of slightly less than one order of magnitude."
"with few exceptions such as speech codecs and old sound archives, the input signals are multichannel. the number of microphones per device has steadily increased in the last few years. most smartphones, tablets and in-car hands-free systems are now equipped with two or three microphones. hearing aids typically feature two microphones per ear and a wireless link [cit] to enable communication between the left and right hearing aids, and conference call systems with eight microphones are commercially available. research prototypes with forty to hundreds of microphones have been demonstrated in lecture halls, office and domestic environments [cit] . the enhancement capabilities offered by these multichannel interfaces are usually greater than those of single-channel interfaces. they make it possible to design multichannel spatial filters that selectively enhance or suppress sounds in certain directions (or volumes) by exploiting the spatial diversity, e.g. phase and level differences, or more generally, the different acoustic properties between channels. single-channel spectral filters, in contrast, require much more detailed knowledge about the target and the noise and they usually result in smaller quality improvement. as a matter of fact, it can be shown that the maximum quality improvement theoretically achievable with only two microphones is already much greater than with a single microphone and that it keeps increasing with more microphones [cit] ."
"3) prior information: if additional prior information is available, preference should be given to methods which exploit this information. for instance, information about the source doa can be exploited both by array processing and source separation methods, while information about the nature of the sources and training data is more easily exploited by the latter."
"in this paper, we present a model-based testing framework for verification back-ends like sat solvers. this framework allows testing different system configurations and sequences of calls to the application programming interface (api) of the verification back-end. whereas in previous approaches only the input data has been randomly generated, we suggest to randomly produce valid sequences of api calls. possible sequences are described by the means of a state machine."
"2) beampattern: in the rest of this section, we omit indexes j and f for legibility. define a spherical coordinate system, with θ the elevation angle measured from the positive z-axis, and φ is the azimuth angle:"
"from now on, we focus on data-dependent spatial filters, which depend on the input signal statistics. compared with fbfs, data-dependent designs attain higher performance due to their ability to adapt to the actual atfs and the statistics of target and interfering sources. in many cases these spatial filters are also adaptive, i.e. time-varying. however, they usually require substantially higher computational complexity. in this section we explore many popular data-dependent spatial filter design criteria. we start in section v-a with a general framework for the narrowband model and recognize several well-known beamforming criteria as special cases of this framework. in section v-b we elaborate on the minimum variance distortionless response (mvdr) and linearly constrained minimum variance (lcmv) beamformers, and in section v-c on the multichannel wiener filter (mwf) beamformer and its variant known as the speech distortion weighted multichannel wiener filter (sdw-mwf). we then proceed with beamforming criteria for inter-frame, inter-frequency, or full-rank covariance models in section v-d and spatial filter design criteria for sparse speech models in section v-e. all these criteria rely on a set of parameters such as the rtfs and the second order statistics of the sources, whose estimation will be handled in section vi."
"model-based api testing is for example realized in the tool modbat [cit], which is a scala-based tool providing an embedded domain-specific language (dsl) for specifying the model. modbat supports only the testing of java bytecode, but provides a more sophisticated event handling than necessary for our purposes. in the .net framework, the abstract state machine language (asml) can be used for the automatic generation of method parameters and the automatic derivation of test sequences [cit] . in this context, also work has to be mentioned which uses contracts as provided by the api for the generation of test data [cit] ."
"the beamformers we have reviewed thus far are obtained by minimizing power criteria which can be expressed in terms of the second-order statistics of the signals. mathematically speaking, these statistics are sufficient to characterize gaussian signals. however, audio signals are often nongaussian. fig. 7 shows that, in the time-frequency domain, the distribution of speech signals is sparse: at each frequency, a few coefficients are large and most are close to zero compared to a gaussian."
"we realize the proposed testing framework for the sat solver lingeling [cit], which is an advanced industrial-strength sat solver, with top rankings in recent sat competitions. 4 it is used in many verification applications both in industry and academia. to evaluate the presented approach, we set up three experiments where we randomly seed some faults in lingeling and compare the new approach to other well-established testing techniques."
"in principle, the test case generator could be used without communicating to the trace execution environment which is described in section 3.3. if the test case generator has a direct exchange with the sut while generating the test data, the results of api calls can be directly considered during the search for a trace which triggers a failure."
"in this section we will explore some widely-used structures and estimation procedures for implementing the beamformers and the spatial filters discussed in section v. we discuss the generalized sidelobe canceller (gsc) structure, often used for implementing mvdr and lcmv beamformers in section vi-a. the estimation of the speech presence probability (spp), the (spatial) second-order statistics of the various signals, and the rtfs of the signals of interest are discussed in section vi-b. although, traditionally, the extraction of geometry information and signals' activity patterns were only used by microphone array processing methods, in recent years they were also adopted by the bss community. we will elaborate on the differences and similarities of these paradigms in section ix-a. numerous statistical estimation criteria for estimating the various components of the spatial filters, such as maximum likelihood (ml), maximum a posteriori (map), and variational bayes (vb), are discussed in section vi-c."
"the a priori snr and a priori snr can be signaldependent [cit] or fixed to typical values designed to meet certain false-alarm and miss-detection rates [cit] . this approach can be extended to multichannel spp estimation under the narrowband approximation [cit], where multivariate gaussian distributions are assumed for the speech and noise components. the resulting spp is calculated using (85) with ζ and ξ redefined as"
"the reflections of the sound wave are captured by the air. from this perspective, each source can be represented as a vector in a high-dimensional space whose dimension is the number of reflections times the number of microphones. a beamformer can be interpreted as a linear operator in this (abstract) space. the various operations can be interpreted in terms of linear algebra, without resorting to beampatterns as a function of the doa. one advantage of this perspective lies in the ability to separate desired and interfering sources sharing the same doa [cit], due to the fact that two sources with the same doa, but with different distances from the microphone array, generally exhibit different reflection patterns. as previously discussed, working in a very high-dimensional space is usually impractical."
"concerning the application of the methods, the array processing paradigm traditionally addressed speech detection, parameter estimation and signal separation successively, while the bss paradigm addressed them in parallel. this line is becoming blurred too, as certain bss techniques relies on successive estimation [cit] while joint parameter estimation has been used [cit] and is becoming more popular in the array processing community [cit] ."
"the testing approach discussed in the previous section is agnostic of the sut. communication with the sut is done via a file, which is generated by the test case generator and reduced by the delta debugger in the case a failure has been found. for solving propositional formulas, all sat solvers which are able to process the standard dimacs format, can be plugged into the testing workflow shown in fig. 1 . in that workflow, the test case generator and delta debugger produce and reduce propositional formulas in cnf. however, little control over the execution behavior of the sut is possible with the consequence that not all features of the system are covered by the generated test cases. in particular, incremental sat solving as implemented in most modern sat solvers, cannot be tested. incremental sat solving is used for many applications, e. g., for enumerating all solutions of a formula. after solving a satisfiable formula, the solver neither terminates nor is reset; instead, additional constraints are provided. the sat solver checks if the formula is still satisfiable with the new constraints. furthermore, the sut is run with certain options set, but there might be defects which only show up under a certain combination of options."
"our experience in using the presented framework when developing lingeling is extremely positive. this section describes experiments to corroborate this, measuring efficiency in terms of throughput, code coverage and detect defection capability."
"in the case of a point source, the second linearity assumption makes it possible to express c j (t) by linear convolution of a single-channel source signal s j (t) and the vector a j (t,"
"recorded at the microphones. this formulation is very general: it applies both to targets and noise, and multiple noise sounds can be modeled either as multiple sources or as a single source [cit] . in particular, it is valid for spatially diffuse sources such as wind, trucks, or large musical instruments, which emit sound in a large region of space."
"3) estimating the relative transfer function: two common approaches for rtf estimation are the covariance subtraction (cs) [cit] and the covariance whitening (cw) [cit] methods. here, for brevity we assume a single speaker scenario. both of these approaches rely on estimated noisy speech and noise-only covariance matrices, i.e. σ x and σ u . given the estimated covariance matrices, cs estimates the speaker rtf byã"
"the first family is based on the concept of casa [cit], which aims at imitating the behavior of the human auditory system [cit] ."
"in the particular case when the complete data distribution belongs to the so-called exponential family of distributions [cit], the em or gem algorithm can be reformulated as computing the conditional expectation of the sufficient statistics representing the distribution (e-step) and maximizing the complete data posterior as a function of these statistics (m-step). with gaussian or discrete models, the sufficient statistics are typically zeroth-, first-, and second-order moments. for more details, see [cit] . although this reformulation does not change the final algorithm, it can simplify its derivation. most em algorithms considered in the literature and all the em algorithms considered here fall into this particular case."
"the matrix whose elements are the absolute values of the corresponding elements in r. 2) for a walk-summable gaussian graphical model, lbp converges and gives the correct means. 3) in walk-summable models, the variance computed by lbp for each node is the sum of all backtracking walks 2, which is a subset of all self-return walks needed for computing the correct variance."
"both model-based approaches (with and without option fuzzing) have the highest success rate. actually, for 31 mutations only these two were successful in producing a failure within a time limit of 100 seconds. the file-based fuzzers did not produce any failure that was not found by model-based testing as well. the regression suite was slightly more successful and detected three failures that no other method could detect. the 5th column contains the average time needed to produce a failure (not including time-outs)."
where ω is the covariance matrix of a diffuse sound field whose entries ω ii are given in (5) . maximizing the directivity with respect to the array weights results in
"the ula is just one possible array geometry among many others. in most algorithms discussed in this survey, no particular array geometry is assumed. nowadays, microphones can be arbitrarily mounted on a device (e.g., cellphone, tablet, personal computer, hearing aid, smart watch) or several cooperative devices. in many cases, the microphone placement is determined by the product design constraints rather than by acoustic considerations. ad hoc arrays can also be formed by concatenating several devices, each of which equipped with a small microphone array and limited processing power and communication capabilities. ad hoc arrays will be briefly discussed in ix-c3."
"in contrast to ml/map, the vb criterion [cit] does not rely on finding a point estimate of the model parameters θ, but consists in computing directly the posterior distribution of the source stft coefficients while marginalizing over all possible model parameters:"
"when a sat solver serves as back-end in a verification tool, its correctness and stability is of particular importance, as the trust put in the system to be verified strongly depends on the trust in the verification system, and hence in the sat solver. efforts have been made to verify sat solvers, but since the implementation of modern sat solvers relies on sophisticated low-level optimizations, complete verification is hardly possible. to ensure robustness of a sat solver, one has to rely on traditional testing techniques. in particular, grammar-based black-box testing and delta debugging have shown to be of great value [cit] ."
"in contrast to the test case generator, the delta debugger has to call the system under test. in order to simplify given input data, the delta debugger goes through the following process: first, the sut is run on the given input data. then the delta debugger tries to reduce the size of the input based on some heuristics. the sut is run again, now with the reduced test case. if the failure is still observed, the delta debugger tries to perform more simplifications. otherwise, it undoes the changes and applies different reductions. the latter steps are repeated until either a time limit is reached or the obtained test case fulfills some predefined quality criteria."
"the third family is based on a binaural versions of the mmse [cit], mvdr and lcmv criteria. the binaural mwf inherently preserves the binaural cues of the desired source but distorts the binaural cues of the noise (i.e. the beamformer imposes the noise to be coherent and perceived as arriving from the same direction as the desired source). several extensions of the binaural mwf have been introduced aiming to also preserve the binaural cues of the noise [cit] . by design, these methods suffer from some distortion imposed on the desired source component at the output. alternatively, distortionless criteria, such as the mvdr and lcmv, can be used instead of mwf [cit] ."
"additionally, we randomly vary the different configurations of the verification back-end. often, a verification tool implements a huge number of options which enable/disable/configure different pruning techniques and heuristics. the optimal settings for the options is strongly dependent on the problem to be solved, so there is no general optimal setting. we use a model to describe the different configurations of a verification back-end. guided by this model, we instantiate the verification back-end randomly. if a defect in the verification back-end triggers a failure, we show how to reduce a failure producing trace by delta debugging."
"time-domain modeling of airs exhibits several limitations. firstly, prior knowledge about the spatial position of the sources does not easily translate into constraints on the air coefficients [cit] . secondly, the source signals are typically modeled in the time-frequency domain instead, which forces estimation algorithms to alternate between one domain and the other [cit] . finally, the large number of parameters involved translates into large computational cost [cit] ."
"some attempts were carried out to compare different source separation methods, e.g., [cit], beamforming methods, e.g., [cit], and source separation methods vs. beamforming methods [cit] . most of the efforts were carried out in the source separation research community, where six international signal separation evaluation campaigns (sisec) [cit] . this allows an objective comparison of different source separation approaches on the same data. moreover, several source separation methods were used as pre-processing for speech recognition within a series of speech separation and recognition challenges [cit] . it was also proposed to objectively evaluate the performance measures of the corresponding source separation methods through the sisec campaigns [cit] ."
"the beamformers and the other spatial filters defined in the previous sections assume that certain parameters are available for their computation, namely the rtfs of the speakers, the covariance matrices of the background noise and the speakers, and/or the cross-covariance between the mixture signals and the desired signal. numerous methods exist for estimating these parameters. many of them rely on estimating the spp for determining noise and speech time-frequency bins combined with speaker classification (for the multiple speakers case) in a first stage and independently estimating the various model parameters in a second stage. in the following, we review spp estimation, and proceed with the estimation of covariances matrices and rtfs."
"we developed a delta debugger which reduces a given trace as follows. first, the file containing the trace is parsed and a list of all commands is built. at the moment, about 30 different commands are supported, having either one or no argument. then the original trace is replayed in order to obtain the golden exit status of the execution, which should also be returned by the execution of the reduced trace. then the rewriting of the trace is initiated."
"exhibits, however, higher sensitivity to misalignment errors than the mvdr beamformer [cit] . finally, it can be shown [cit] that the maximum snr (msnr) beamformer that maximizes the output snr:"
"] to all neighbors with status u. iii) if maxscore(i) is less than the maximum received score, then replace maxscore(i) with the maximal received score and maxindex(i) with the corresponding node index."
"for testing incremental sat solving, additional clauses have to be generated which are added to the current formulas between calls to the solving routine. these clauses are generated in the same way as just described, over already existing and a certain small number of new variables."
"let us now move from the physical domain to discrete time signal processing. we assume that the recorded sound scene consists of j sources and that the number of microphones is equal to i. we adopt the following general notations: scalars are represented by plain letters, vectors by bold lowercase letters, and matrices by bold uppercase letters. the source index, the microphone index, and the time index are denoted by i, j, and t, respectively. the operator t refers to matrix transposition, and h to hermitian transposition."
"in recursive fmp, it is entirely possible (and very likely) that different nodes have different lists of feedback nodes. at the beginning of the second stage, the list stored at an active node includes only neighboring feedback nodes. the lists are then exchanged only within distance d, and thus a node may not know feedback nodes located far away. in addition, if the total number of feedback nodes exceeds k i for some i, then the lists may be different even if d is as large as the network diameter. moreover, if the initial active subgraph is disconnected, then the lists may be different even if d and all of the k i 's are sufficiently large, because some message pathways are broken. however, in this case, as will be stated in proposition 2, the inference results are exact."
"we first consider the estimation of the spp in a single-speaker scenario using a single microphone and then discuss the multimicrophone scenario. in this scenario, the stft of one of the microphone signals is given by:"
"in the free field, the solution to the wave equation is given by the spherical wave model. the waveform x i (t) recorded at point i when emitting a waveform s j (t) at point j is equal to"
"it should be noted however that such penalties do not match the actual distribution of ild and ipd in the presence of echoes and reverberation [18, fig. 2 ]. the preservation of interaural quantities is especially important in hearing aids, in order to increase speech intelligibility [cit] and preserve the spatial awareness of the wearer. for penalties specifically designed for this application area, see [cit] ."
"3) source spatial image em: let us consider the lgm in (18)- (19) . a first approach [cit] which is applicable when r j (f ) is full-rank is to consider the source spatial images c j (n, f ) as latent data. one iteration of the resulting gem algorithm 3 can be written as follows: 1) e-step: compute the expected sufficient statistics"
it can be shown that a sufficient statistics (in the bayesian sense) for estimating s 1 in mmse sense is the output of the mvdr beamformer:
"witht denoting continuous time, q ij the distance between points i and j, and c the speed of sound, that is 343 m/s at 20"
"in order to obtain good code coverage and increase flexibility for delta debugging, but also for reducing modeling effort, the testing framework is able to deal with under-approximative models by relying on a callback feature to give certain feedback from the sut back to the testing framework. then the testing framework can react immediately when contract violating traces occur. to use this feature, the sut also has to be equipped with api contract assertions similar to assertions used in specification-based testing (cf. for example [cit] )."
"note that fuzz testing is \"embarrassingly parallel\" and we have successfully used the combination of fuzz testing and delta debugging on a cluster with 128 cores, with the goal to produce smaller failure traces than an existing but large and impossible to delta debug trace obtained from an external user or from failing runs on huge benchmarks."
"note that in practice p γ (θ) is usually considered in the above equation rather than p(θ) where the parameter γ controls the strength of the prior [cit] . note also that the map criterion generalizes the ml criterion, since (104) reduces to (100) when a non-informative uniform prior p(θ) ∝ 1 is considered. in the following, we therefore formulate the em algorithm in its most general form for the map criterion."
"3) distributed algorithms for ad hoc microphone arrays: in classical microphone array processing, as explored in this survey, both the sensing and the processing of the acquired speech are concentrated in a single device, usually called a fusion center. in many scenarios, this approach cannot provide the required performance, since the acoustic scene may be spatially distributed, and a powerful fusion center may not be available. it is therefore reasonable to alleviate the performance drop by a large spatial deployment of inter-connected microphone sub-arrays (nodes), arranged in a wireless network, preferably equipped with local processors. recent technological advances in the design of miniature and low-power electronic devices make such distributed microphone networks, often referred to as wireless acoustic sensor network (wasn), feasible. as a matter of fact, cellular phone, laptops and tablets are perfect candidates as nodes of such networks, as they are selfpowered and equipped with multiple microphones (typically two to three), as well as powerful processors and various wireless communication modules. the large spatial distribution of wasns increases the probability that a subset of the microphones is close to a relevant sound source and has the potential to yield improved performance as compared with classical, condensed, microphone arrays. however, the distributed and ad hoc nature of wasns arises new challenges, e.g. transmission and processing constraints and intricate network topology, that should be addressed to fully exploit their potential."
"for each compilable mutation, testing resp. fuzzing continued until the first failure or the time limit of 100 seconds was reached. each failing test case was then subjected to delta debugging with a time limit of one hour. the algorithm for delta debugging depends on the type of testing: trace shrinking for mbt, and cnf reduction for file-based fuzzing and regression testing. even with a time limit of one hour per test case, some delta debugging runs timed out and thus the success rate dropped slightly (except for model-based testing without option fuzzing, the first row below the header in table 1 )."
"the performance of certain beamformers is limited when the undesired signals are not point sources or when there are too many interfering sources. moreover, some beamformers suffer from the existence of nonstationary interference, due to the larger observation time required to estimate signal statistics. single-channel enhancement methods can achieve nonlinear spatial and/or spectral filtering and usually adapt much faster to changes in the interference characteristics. in this section, we explore the use of such algorithms as postfilters applied at the output of the beamformers [cit] . we then proceed by presenting single microphone separation algorithms utilizing spatial information and conclude in presenting joint spatial-spectral estimators. beamformers with a subsequent postfiltering stage, utilizing both spatial and spectral information, adopt some of the single-channel speech separation methodologies, and therefore usually lead to improved performance as compared with both multichannel and single-channel algorithms. note, that some of the modern multichannel techniques, reviewed in this paper, are utilizing the entire reflection pattern of the speech propagation rather than resorting to doa-based steering vector, and are therefore capable of separation sources with identical doa."
"the objective of a binaural noise reduction algorithm is not only to selectively extract the desired speaker and to suppress interfering sources and ambient background noise, but also to preserve the auditory impression, as perceived by the hearing aid user. existing methods can be roughly categorized into three main families."
"concerning the signal models, array processing methods traditionally utilized the spatial resolution of the array as a function of the doa while bss methods were originally designed for instantaneous mixtures (no delay, echoes or reverberation). it was then proposed, in the field of array processing, to substitute the simple doa-based propagation model by atfs and rtfs [cit] . in parallel, bss methods developed from instantaneous mixtures to convolutive mixtures also modeled by atfs and rtfs [cit] . under this perspective, there is no distinction between the two paradigms anymore. as a matter of fact, their equivalence was already observed in an early stage [cit] ."
"the desired signal defined in the previous general beamformer formulation consists of a linear combination of the \"dry\" sources (prior to the filtering process of the airs). hence, the designed beamformer not only aims to reduce the noise, but also aims to de-reverberate the speech signals. assuming that reverberation alone does not compromise intelligibility, which is the case in many scenarios, the de-reverberation requirement can be relaxed. a modified beamformer can be obtained by redefining the desired signal as a linear combination of the sources as received by some reference microphones. generally, the reference microphone for each of the sources can be selected differently. here, for brevity, we assume that the reference microphones are the same for all sources, and arbitrarily select it to be the first microphone. redefine the modified vector of desired responses as:"
"individual spps for each of the speakers can be approximated from their estimated positions [cit] . given that any of the speakers is active (i.e. hypothesis h s is true and hence p(n, f ) is high), and assuming that each time-frequency bin is dominated by at most one speaker signal (i.e. the time-frequency sparsity assumption), the a posteriori probabilities are obtained by"
"hearing aids impose severe design constraints on the developed algorithm: short latency, fast adaption, small number of microphones, limited connectivity between the hearing devices and low-complexity, to name a few. designing algorithms, satisfying these constraints, and still exhibiting high noise and interference reduction together spatial cues preservation, is still an ongoing research topic."
"with the full-rank covariance model in section iii-e, for instance, the target signal to be estimated is the vector c j (n, f ) of stft coefficients of each spatial source image. beamforming can then be achieved using a matrix of weights"
"despite the fact that arbitrary array constellations are widespread, specific array geometries are still very important and have therefore attracted the attention of both academia and industry. we will now briefly describe some of the common microphone array geometries, namely differential and spherical microphone arrays."
"rigorous formal techniques provide the tools for verifying crucial stability and correctness properties of hardware and software systems in order to increase their reliability as well as the trust of their users. examples of successful verification techniques include model checking and automated theorem proving (cf. [cit] for a survey). for applying these techniques, dedicated software is required which provides (semi-)automatic support during the verification process. solving verification problems is not a trivial task and therefore, many sophisticated approaches have been developed. many of these approaches break down the original problem to the problem of deciding the satisfiability of propositional logic (sat) and extensions (smt) [cit] . for sat, the prototypical np-complete problem, not only a myriad of results are available giving a profound understanding of its theoretical properties, but also very efficient tools called sat solvers [cit] have been made available over the last ten years."
"to circumvent these problems, we propose to use a model-based testing approach for verification back-ends like sat solvers. in particular, we suggest to fuzz not only the input data, but to generate sequences of api calls, to cover more features of a solver. the sequences of valid api calls are described by a state machine. to test different combination of options, setting options is also fuzzed. the range of possible options is also defined by a model. the adopted workflow is shown in fig. 2 . the goal is still to produce syntactically valid input data, which uncovers defects of the sut. the individual components of the proposed approach are described in the following."
"is often considered instead. this model is popular for channel-wise filtering in the context of casa, where the ild and itd are called interaural level and intensity differences, respectively, and are influenced by the shape of the pinna, the head and the torso [cit] . it has however been used for multichannel filtering too [cit] . the use of level and phase differences retains the information about the source positions while discarding absolute levels and phases which are considered as irrelevant. indeed, in the absence of echoes and reverberation, the itd at all frequencies becomes equal to the tdoa (q ij − q 1j )/c, the vector of rtfs becomes equal to the relative steering vector (14) and the normalized vector of atfsā j (f ) becomes equal to the steering vectord j (f ) normalized as in (10) . this has been exploited to constrainã j (f ) in anechoic conditions [cit] and to derive penalties overã j (f ) [cit] orā j (f ) [cit] in reverberant conditions, such as"
"differential arrays [cit] are small-sized arrays with microphone spacing significantly smaller than the speech wavelength. they implement the spatial derivative of the sound pressure field and achieve a higher directivity than regular arrays, close to that of the sd beamformer. however, the sensitivity to array imperfections is excessively high. the most commonly used differential arrays implement the first-order derivative, but higher-order geometries exist. a device that can directly measure the sound velocity, i.e., the first-order vector derivative of the sound pressure, is also available [cit] ."
"geometry-based information (e.g., microphone distances and doa) is usually exploited by array processing. it can however also be incorporated into bss criteria [cit] . the use of priors [cit] breaks the \"blindness\" of bss methods even further. conversely, some array processing methods, e.g., [cit], are not using any spatial priors but rather rely on a specific activity pattern."
assume the narrowband approximation in the stft domain (6) holds. further assume that the received microphone signals comprise j p point sources of interest and j − j p noise sources with arbitrary spatial characteristics. using (2) and (6) and the above assumptions the microphone signals are given by:
"besides explicitly running lingeling from the command line, the solver can also be included as a library. the api includes more than 80 functions. additionally to the sequential version of lingeling, there is also a multi-threaded variant which builds on top of the sequential version. the testing framework discussed in the following has only been used for testing the core library and the sequential solver. the api functions used by the multi-threaded front-end are hard to test with, since they mainly define call-backs. we leave it to future work to extend the framework to the multi-threaded case."
"formula delta debugger reduced formula for solving a propositional formula, most state-of-theart sat solvers implement a variant of the algorithm by davis, logeman, and loveland (dll) [cit] which traverses the search space in a depth-first manner until either all clauses are satisfied or until at least one clause is falsified. in the latter case the sat solver backtracks if not all assignments have been considered. for the application of sat solvers on reasoning problems of practical relevance, a naive implementation of this algorithm is insufficient. very sophisticated pruning techniques like learning and effective heuristics and data structures have to be realized within a sat solver, such that the source code of a sat solver has thousands lines of code usually written in the programming language c; e. g., the sat solver lingeling [cit] consists of more than 20,000 lines of code. for making a sat solver efficient on a certain set of formulas, mostly the right configuration, i. e., a specific combination of the options and parameter settings of the solver, has to be found. due to very sophisticated pruning techniques and well-thoughtout implementation tricks, a sat solver can be tuned in such a manner that it solves most problems occurring in applications in a reasonable amount of time, although the worst case runtime of course remains exponential."
"for dealing with inadequate api models and for realizing the previously described call back functionality, lingeling internally executes a state machine. if an invalid state transition would be caused by an api call not possible in the current state, a special assertion fails. this gives the feedback to the caller of the api function that the invocation was incorrect. with this information the caller could adopt its behavior accordingly, i. e., in the case of the delta debugger a different kind of reduction is performed."
"an important family of markov random fields (mrfs) is the family of gaussian markov random fields (gmrfs) or gaussian graphical models. such models are widely used in medical diagnostics, oceanography, robotic mapping, and gene regulatory networks. for gmrfs of moderate size, exact inference can be solved by algorithms such as direct matrix inversion, cholesky factorization, and nested dissection, but these algorithms cannot be used for large-scale problems due to the computational complexity [cit] ."
"digital object identifier 10.1109/taslp.2016.2647702 become essentially interchangeable. these problems arise in various real scenarios. for instance, spoken communication over mobile phones or hands-free systems requires the enhancement or separation of the near-end speaker's voice with respect to interfering speakers and environmental noises before it is transmitted to the far-end listener. conference call systems or hearing aids face the same problem, except that several speakers may be considered as targets. speech enhancement and separation are also crucial pre-processing steps for robust automatic speech recognition and understanding, as available in today's personal assistants, gps, televisions, video game consoles, and medical dictation devices. more generally, they are believed to be necessary to provide humanoid robots, assistive devices, and surveillance systems with machine audition capabilities. while the above applications require real-time processing, off-line separation of singing voice, drums, and other musical instruments has been successfully used for music information retrieval, upmixing of mono or stereo movie soundtracks to 3d sound formats, and remixing of music recordings. other applications, e.g. meeting transcription, can be also processed off-line."
"hundreds of multichannel audio signal enhancement techniques have been proposed in the literature over the last forty 2329-9290 © 2017 ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"the 3rd and 4th columns show the success rate of fuzzing resp. delta debugging with respect to all 404 mutations, for which at least one method was able to produce a failure: an assertion violation, segmentation fault, etc. as in exp. 2 the executable is optimized (-o3) but does include assertion checking code (no -dndebug). mutation and compilation time are not taken into account."
"this paper is structured as follows. first, we introduce basic notions of sat solving and testing techniques in section 2. then we introduce a general architecture for model-based testing verification back-ends in section 3, which is instantiated for the sat solver lingeling as described in section 4. experiments that underpin the effectiveness of our framework are shown in section 5. section 6 discusses related work, and section 7 concludes with a discussion of related approaches and an outlook to future work."
"the rest of this paper is organized as follows. the problem formulation is presented in section 2 followed by some needed concepts explained in section 3. the simulation results presented in section 4 verify the capability of the proposed method, and finally the conclusions are drawn in section 5."
"fog detection and characterization is thus an important question to deal with in these problems. several algorithms were proposed for defogging, in particular in the recent years. a first class is based on color and contrast enhancement, for instance [cit] . a second class is based on the koschmieder's law to model the effect of the fog, for instance [cit], tarel and hautière, 2009, [cit] . with a linear response camera, the koschmieder's laws gives the apparent intensity i of an object as a function of its intrinsic intensity i0 and of the fog extinction coefficient β:"
"this previous method can be applied on static or moving cameras. when the depth-map of the observed scene is known with enough accuracy, in particular when the scene is static, a little better can be done. in such a case, the value of is can be estimated by taking the maximum intensity at the farthest distance detected by the pixels selected by the gradient thresholding. this refined method gives better results as shown in the experiments."
"actual scenes are composed of many objects, and thus its image is composed of several regions, and each region have its own random variable. let sc the set of selected regions in the previously described step. the total entropy is thus computed on this set of selected region as:"
"in fig. 3, the total entropy on the three selected regions is shown versus the value of β. the ground-truth β values are shown as vertical lines. this figure shows how the ground-truth is close of the minimum of the total entropy, in various weather conditions."
"where v i is the voltage phasor at bus i, and f is a submatrix of the matrix h. that, h is obtained from the admittance matrix and relates the vector of voltages at load buses and currents at generator buses to the vector of currents at load buses and voltages at generator buses, as:"
"when i(x) and d(x) are known, in the equation (1), the extinction coefficient β, the restored image i0 and the intensity of the sky is remains unknown. the first step of the algorithm is thus to estimate is in an original way. then, given a β, the restored image i0 can be computed by solving (1). the second step consists in searching the value of β which minimizes the total entropy of the gray level distributions over selected regions of the restored image."
"also, corresponding right and left eigenvectors can be used to identify weak buses and branch and generator participation factors [cit] . the major drawback of the minimum singular value and the minimum magnitude of the eigenvalues is that they display nonlinear behavior and cannot predict the voltage collapse point. in addition to the minimum singular value and the minimum magnitude of the eigenvalues, some other indices based on the jacobian matrix have been presented that show better behavior. one of them is the minimum singular value of the reduced load-flow jacobian matrix. but this index displays also nonlinear behavior as the system approaches the voltage collapse point [cit] ."
"to evaluate the proposed algorithm, we need foggy images with a visibility distance ground-truth. a visibility-meter can be used for the ground-truth. however and until now, we are not able to have moving stereo images of foggy scenes with a groundtruth. indeed, the measures obtained by a visibility-meter are very perturbed by air turbulence around the sensor when it is on a moving vehicle. this is probably due to the fact that a visibilitymeter uses a relatively small volume of air for its measure. as a consequence, we rely on synthetic images to experiment with moving scenes. for static scene, we rely on camera images."
"to study the effect, on the estimated β, of bad matches in the stereo reconstruction, we compared the case where the groundtruth depth-map is used with the case where the depth-map comes from a fast stereo reconstruction algorithm. in practice, we use the libelas algorithm which is very fast and is dedicated to road scene images [cit] . tab. 2 shows the obtained results in terms of mean relative error on the estimated visibility distance in the two cases. results obtained using the ground-truth depth-map are very accurate, when the stereo reconstruction is used, accuracy is lower but still of good quality."
"initially, based on the proposed method in section 3.2, the set of binding contingencies is determined using the l-index. these contingencies and the corresponding voltage stability margins are tabulated in table 1 . after each contingency, the maximum value of l-index and the related bus are shown in the third column of this table; each bus denotes the affected part of the power system by a subset of contingencies that the binding one is shown in the first column. so, these 9 contingencies, as the first set of binding contingencies ( α bv in the first step), constitute the set α b that must be respected in the rpp optimization problem to attain the first solution."
"first, in section 2., the problem of fog detection and extinction coefficient estimation is explained and our approach is presented. then, the algorithm is detailed in section 3.. in section 4., the proposed algorithm is evaluated on a synthetic database for a moving camera and evaluated with a ground truth database for static camera images."
"tab. 1 shows the evaluation results in term mean relative error on the visibility distance. we can see that the algorithm produces results with a similar accuracy than the visibility-meter, up to 1000 meters. notice that results are even better when the distance range is limited to 50 meters. this can be explained by the fact that fog is not always homogeneous, and a short range allows to better fit the ground-truth which is measured close to the camera. in these experiments, the second variant on the is estimation is used to achieve better results. fig. 4 shows the results during two foggy in the morning days, with an image every 10 minutes, night images are skipped. these results show that the proposed algorithm is able to clearly discriminate between visibility distance lower or higher than 500m. the distance were fog is no ground truth visibility figure 4 : comparison between estimated visibility distance and ground-truth during 1600 minutes (one image every 10 minutes, images during the night are skipped) on the matilda database. the proposed method is tested with two depth ranges: maximum distance where is is obtained (green crosses) and a fixed range [cit] m (red crosses), using a single image segmentation and is also shown the case with a different segmentation at every image (blue crosses). the ground-truth value is shown as a black line. the horizontal blue line shows the 500m visibility distance which is considered as the distance were fog is no more considered in driving situations."
"the entropy minimization is meaningful only on a set of regions with constant albedo and with varying enough depths. the input image is thus segmented in regions where the intensity varies slowly. the regions with constant intensity or fronto-parallel to the image are rejected as well as the tiny regions which are useless. the used criterion for this selection is to select regions with a depth range higher than 10% of the whole image depth range and with an area higher than 2% of the number of image pixels. this segmentation can be performed with advantages on a clear day image when a static sensor is used. the segmentation can be also performed on both the image and the depth map, when the depth map is enough accurate."
where α c is the set of contingencies included in the optimization problem. the innovation of this paper is to select a small number of contingencies so that the allocated var sources cover all of the contingencies.
"in defogging methods, the intensity of the sky is usually obtained using simple methods such as the selection of the maximum intensity in the image, or as the value corresponding to a given quantile on the intensity histogram, see for instance [cit], tarel and hautière, 2009) . if this kind of approach gives approximate results in practice, we observed that an accurate is is important to estimate correctly the value of β. in particular, the fog being not necessarily homogeneous on long range distances, it is necessary to estimate a correct value of the is in the close surrounding of the camera."
"we thus introduce a first novel and simple method, based on contrast detection using the sobel operator to estimate is. first, we compute the sobel operator on the input image. second, pixels with a gradient magnitude larger than an arbitrary threshold are selected. third, we chose is as the highest intensity value in the vicinity of the pixels selected by the gradient thresholding."
"-count the number of black pixels in the restored image i0, and if this number is higher than a threshold, the search on higher β values is stopped."
"let α c be the set of contingencies considered and α b be the set of binding contingencies included in the optimization problem. α v is the set of contingencies which violate the required voltage stability margin and α o is the set of contingencies which violate other constraint types. also, α bv is the set of binding contingencies from the set α v, and α bo is the set of binding contingencies from the set α o . the steps of solving the optimization our conjecture is that by solving the optimization problem in iteration 1, all constraints are probably satisfied and the computation terminates. because the optimization problem is formulated for a reactive power planning wherein, in addition to rescheduling control variables, some new reactive power sources are added. if only rescheduling control variables are considered, some post-contingency constraints satisfied at an iteration may be violated during the subsequent iterations. this necessitates some iterations before reaching the optimal solution. in spite of the above descriptions, step 3 has been presented to assure a credible and optimal solution."
"the total entropy is computed for a set of possible β values, and the estimate of β is obtained as the value minimizing the total entropy (4). this computation is quite fast, since only pixels in the selected region with valid depths need to be considered and restored."
"in the above mentioned papers, the contingency filtering methods are proposed for operation problems. in an operation problem, the objective function is minimized by rescheduling existing control means such as active generator powers, transformer ratios, shunt element reactances, etc. this paper uses l-index to filter contingencies for a voltage stability constrained reactive power planning problem. in a reactive power planning problem, some new reactive power sources (ex. switchable capacitors and/or reactors) are added to the power system. the main objective is to improve the voltage stability margin for contingency states by using the minimum value of new sources. in the proposed method, the predetermined set of all contingencies is divided into some subsets, wherein each subset of contingencies affects an especial part of the power system. then, only the worst contingency from each subset is selected and addressed in the rpp problem. this method reduces the problem dimension and improves the convergence ability of its solution for large scale applications, whilst the obtained results are similar to ones taken from the enumeration method of n-1 contingencies."
"as mentioned several times in this paper, voltage stability constrained rpp problem must be formulated as an optimization problem. here, we focus on the preventive mode. the objective function is to minimize the installation cost of new var-plants (switchable capacitors and/or reactors) as follows:"
"computing the entropy of each intensity is linear according to the number of pixel. if β has k possible values and the image n pixels, the complexity of the proposed algorithm is o(kn ). the proposed algorithm can be easily parallelized. indeed, the restored image for each beta can be computed independently and the entropy can be computed with a map-reduce approach."
"even if fog is rare, fog impacts the quality of the results of many images processing algorithms when applied on outdoor scenes, see fig. 1 . for example, fog reduces vehicle and obstacle visibility when driving, which leads to death injuries. solutions have been investigated to tackle the problem of reduced visibility when fog is around the vehicle. one possibility is to detect fog, turn on the fog lights and depending of the importance of the fog which is measured by the extinction coefficient β, adapt the strength of the fog lights, as demonstrated in icadac project"
"in this paper, the affected area by each contingency is identified using the l-index. then, the contingencies are grouped according to the corresponding affected areas. after occurrence of a contingency, the affected area is the weakest bus i.e. a load bus with the maximum value of l-index. contingencies with the same weakest bus are included in the same group. in each group, the worst contingency is selected as a binding contingency."
"thanks to the fui aware project for partial funding. 1 icadac project (6866c0210) is within the deufrako program and was funded by the anr (french national research agency). where d is the object depth, and is is the intensity of the sky. the \"meteorological visibility distance\" is defined conventionally, as a function of β, by:"
"with ground truth distance mean relative error 4.67% with stereo reconstruction depth map mean relative error 16.03% table 2 : mean relative error between the estimated visibility distance and the ground-truth value on 66 synthetic stereo images from frida3 database. in the top part, the ground-truth depthmap is used and in the bottom part, the depth-map comes from the stereo reconstruction using libelas algorithm."
"in this article, we propose an original method for estimating the extinction coefficient of the fog or equivalently the meteorological visibility distance. the proposed method is based on the entropy minimization of the restored image on pixels where the depth is known. the depth-map can come from a 3d model of the scene, or from a stereo reconstruction as well. the results obtained on a static scene are as accurate that what is usually obtained with a visibility-meter, when the depth-map is of good quality. the proposed algorithm can be also used with a moving sensor and is tested with success on synthetic stereo images. the proposed algorithm have several advantages. first, a small depth range can be used to characterize a visibility distance 10 times larger than the maximum depth in the depth-map. this is important for vehicular applications where obstacles or other vehicles can hide a large range of distances. another important consequence is that the used stereo reconstruction algorithm needs to provide good matches only up to a critical distance which is much lower than the visibility distance. second, the proposed method is close to real time and it is fast enough to investigate real-time applications for vehicular applications. in particular, it can be easily speed-up using parallel computations."
"1 . an alternative is to display to the driver an image of the scene ahead of the vehicle after visibility restoration by image processing, see for instance [cit] ."
"when a region with a uniform albedo is observed in presence of fog, its image is not of constant intensity. indeed, its image becomes whiter with an increasing depth. this is due to the last additive term in (1), the so-called atmospheric veil which is an increasing function of the depth d. the shanon entropy is a measure of the redundancy of a probability distribution. as a consequence, the entropy of the intensity distribution of a region of constant albedo, i.e without fog, is very small. on the contrary, the entropy of the same region in presence of fog is larger if the region span a large range of depths. our main idea is thus to estimate the extinction coefficient β as the value which gives the restored image with the smallest entropy on regions of constant albedo."
"the used sensor is stereo cameras. for every stereo pair, the stereo reconstruction is performed by one or another method. the choice of the stereo reconstruction algorithm is studied in the next section experimentally, where it is shown that the stereo reconstruction only needs to be correct in a relatively short range of distances close to the cameras. the input of the proposed fog detection and characterization algorithm are thus an input camera image i(x) observed with or without atmospheric scattering and the corresponding depth map d(x) obtained from the stereo reconstruction."
"many number of contingencies must be addressed in the problem of the voltage stability constrained rpp. this makes the problem very complex and computationally large. since, separate variables and parameters must be considered for each contingency. this paper focuses on selection of a small number of contingencies instead of all of them. to do this, an l-index based contingency filtering is proposed to divide the predetermined set of all contingencies to some subsets. then, only one contingency from each subset, known as the binding contingency, is selected and addressed in the rpp problem. the case studies of ieee 39 and 118-bus systems are used to try the capability of the proposed method and compare it with other methods. the attained results show, by the proposed method, the number of the addressed contingencies in the rpp problem is significantly reduced. in addition to that, total capacity and investment cost of the new var-plants are reduced."
"it is well known that in a power system, the voltage collapse point (the voltage stability boundary) coincides with a saddle-node bifurcation point [cit] . at this point, the minimum singular value and the minimum magnitude of the eigenvalues of the load-flow jacobian matrix become zero, i.e. the jacobian matrix becomes singular [cit] . therefore, they have been proposed as indices to detect proximity to the voltage collapse point."
"where e is the expectation function and p(y) is the probability density function of variable y. the entropy is defined for a single random variable, but for several independent variables, the entropy of the set of these variables is the sum of the individual entropy."
"in this section, some simulation on the ieee 39 and 118-bus test systems are performed to verify the capability of the proposed method. the predetermined set of contingencies contains the base case in addition to any single line, transformer or generator outage with the exception of the contingency that cause bus isolation. the number of these contingencies in ieee 39 and 118-bus systems is 44 and 231, respectively. all load buses are considered as candidates for installation of new var-plants, and maximum allowable var capacity at each bus is 200 mvar with the step size of 5 mvar. let us assume, the fixed c 0 and unit c 1 costs for a var-plant are 100 $ and 300 $ /mvar, respectively, and the desired voltage stability margin is 30 % . the genetic algorithm (ga) is performed to search the optimal solution of the rpp optimization problem. ga is a capable tool to solve nonlinear and non-convex optimization problems and is frequently used in rpp literatures [cit] ."
"from equation (5), an elevation angle in the range of [, /2] can be transformed to [0, /2]. note that in the adew model, the measured elevation angle, which is lower than in the adem model, will instead be determined by the virtual elevation angle, and for the results, the rejection range is changeable according to variations of ℎ as well as . higher ℎ or longer values will lead to a narrower range of rejection with careful weighting. this improvement not only increases the positioning accuracy through the assignment of phase observations with careful weights, but also promotes the usage of available observations."
"in our scheme, an original image is first decomposed into multiresolution subimages through the wavelet transform; then, coefficients in the low-pass octave bands are encoded by the huffman coding method while coefficients in each of the high-pass octave bands are encoded by the proposed novel vector quantization with variable block. the diagram of the proposed algorithm is shown in figure 1 . the huffman coding method is a common method in the field of image compression, so the introduction of this part is ignored. the optimized vector quantization with variable block size includes two steps. the first step is dividing the subimage into series of sub-blocks. the smooth areas of the coefficient matrix are divided into sub-blocks with a relatively large size, while those containing many details are divided into small-size sub-blocks. the metric of complexity is based on the value of local fractal dimension (lfd). in order to classify the local fractal dimension into predetermined classes, a method of discriminant analysis is applied [cit] . after that, an optimized quadtree (qt) approach is exploited to split the subimage into different sizes of blocks. the second step is quantization step. in the procedure, every codebook is made up of several sub-books."
"many countries around the world suffer from recurring geological disasters, especially landslides, and the losses of lives and properties caused by the latter have increased each year. presently, there is an urgent need for the development of high precision, real-time landslide monitoring technology. global navigation satellite systems (gnss), which capture global, continuous, and high precision geospatial data, have been widely applied in many fields [cit], especially for the monitoring of geological hazards [cit] . given the excellent efficiency and reliability, real-time kinematic (rtk) techniques based on short-baselines have been implemented in geologic deformation monitoring over the past few years [cit] . however, local obstacles surrounding antennas can lead to both diffraction and multipath errors. these have been recognized as the major sources of errors that impact the accuracy of positioning [cit] ."
"where d(i, j, m, n) is the distance between the pixel (i, j) and pixel (m, n). in this formula, the points (m,n) are taken to be the four immediate neighbors of the point (i, j). the volume of the blanket is calculated from"
"both the svm and ls-svm are capable of fitting any data perfectly well, even when there is no relation between the target and the input variables. a good validation setting is critical to obtain optimal parameters to generalize a train model to make meaningful predictions. although various methods have been proposed for parameter optimization, long computing time could become an obstacle for all the methods with a large dataset. a universally good initial guess for the parameters can accelerate the search for optimal parameters. but it is impossible to obtain universally good initial guesses without data scaling. many articles have pointed out that data scaling is very import to using the svm and ls-svm effectively. input variables rarely have the same units and changing the units of a variable may change its variation range significantly. the variable having a significantly larger variation range would likely dominate the result of the kernel function. with units removed and data scaled to similar ranges, the optimal parameters become more predictable."
"in summary, the proposed algorithm has been demonstrated to not only improve the positional accuracy, but also the usage of available observations, and it can achieve accuracies ranging from the centimeter-scale to the millimeter-scale in complex environments. additionally, the coefficients of adem used in the adew model have been demonstrated to be available for a long session without the need to consider the sidereal cycle."
"multi-gnss methods provide a convenient way to model a physical elevation mask for landslide monitoring in complex environments, and such an approach improves upon the traditional method that relies on theodolite measurements."
"many studies on parameter optimization used raw data. in extreme cases, the outcome of the kernel function could be dominated by certain variables that vary in a large range; therefore, scaling data of independent variables to remove units is important to generalize the results of parameter optimization."
"where l k is the number of pixels in the kth sub-block and x i is the gray level of the pixel i. then the energy of all of the subblocks is ordered, and the predetermined numbers of code vectors are taken out in equal interval. in the novel method, the image information is taken into account at the process of selecting the initial codebook."
"the dataset was used mainly to demonstrate the behavior of the svm and ls-svm in two extreme settings. the first setting simulates a perfect nonlinear dependence of y on five independent variables, i.e., the target includes no noise; the second setting uses one of the five x variables as the target and other four as inputs to simulate the extreme case that the target are all noises. the data were split into two equal parts, one for training and the other for validation."
"in this paper, an optimized quadtree method is performed to decompose the subimage into several sizes of blocks. in order to understand the optimized quadtree method, the quadtree algorithm is first explained as follows."
"we investigated both the svm and ls-svm with an artificial dataset and a dataset used for the global surface ocean co 2 mapping, which included more than 200,000 data points. the results would give users general hints for picking up initial parameter guesses and tuning parameters effectively. the side-by-side comparison would also help users to understand the differences between the two models."
"protein functions are associated with their three-dimensional structures and in-solution dynamics. after the technical breakthroughs in cryogenic electron microscopy (cryo-em) single-particle analysis (spa), numerous structures have been solved at atomic ~ nearatomic resolutions, including extremely large macromolecules that were not solved by conventional techniques. their dynamics analysis based on the solved structures further deepens the understandings of the functional mechanisms. however, the elucidations are often hampered by the large molecular sizes and the complicated structural assemblies making both the experimental and computational approaches challenging. here, we report a deep learning-based approach, defmap, to extract the dynamics information \"hidden\" in a given cryo-em density map, using a three-dimensional convolutional neural network (3d-cnn). defmap successfully provided dynamics information equivalent to molecular dynamics (md) simulation and experimental approaches only from cryo-em maps. indeed, defmap has detected dynamics changes associated with molecular recognitions and the accompanying allosteric conformational stabilizations. we expect that this new approach allows cryo-em spa to quantitatively grasp the in-solution protein behavior at atomic and residue levels beyond the static information, gaining biological insights into the functional mechanisms."
"the experiments presented above were assessed with bds observations. next, three groups of gnss observations collected on random days for nq02 were employed to further assess the performance of the proposed adew model. the navigation systems used in these experiments consisted of gps, glonass, and bds. figure 13 illustrates the deviation of the coordinate bias in the time series for the horizontal direction derived from a fixed solution on doy 021 (a); 045 (b); and 072 (c) [cit] . the results shown in figure 13 indicate that the adew model (green) exhibited a lower rms than the tfc model (red); details of the rms values are shown in table 2 . the results shown in figure 13 indicate that the adew model (green) exhibited a lower rms than the tfc model (red); details of the rms values are shown in table 2 . it is clear that the rms of the coordinate bias with the adew model was found to have improved, relative to that of the adem model, by 30.11%, 37.27% and 37.71% in the horizontal direction on doy 021, 045, and 072, respectively. the mean improvement of the rms in the vertical direction was greater than 31.58%. furthermore, the results suggest that the polynomial coefficients of the adem model are applicable for a long period of time, which can help to mitigate the effects of diffraction and multipath errors, even within a full month."
"a defining characteristic of hsm, vis-à-vis hs, is it reliance on inter-disciplinary research in the methodology space. how could such research be effectively realised? we use the term hybrid teams to emphasise the need for interdisciplinary m&s groups that bring together problem stakeholders, researchers and practitioners. they are essentially composed of individuals specialising in specific fields of study or, as in the case of problem stakeholders, having tacit knowledge of the underlying system of enquiry. when considered as a whole, such hybrid teams will have recourse to knowledge constructs (theories, methodologies, techniques, applications, etc.) that have not traditionally been applied to m&s studies. such teams are arguably better poised to address challenges pertinent with hybrid systems as the very constitution of the team allows for opportunities to leverage from the diverse body of knowledge and individual expertise and skillsets and make it possible to work towards common end goals."
"it can be noted from figure 2 that the physical structures surrounding the antennas can be described precisely by the e min of the multi-gnss, although a slight discrepancy with the photograph may be observed. based on the e min of 24 h, the curve fitting package in matlab was adopted to estimate the fitting coefficients for e a . then, the e a can be expressed as follows:"
"in order to realize micro-deformation monitoring in real-time, we developed a rtk platform, which can deal with heavy data streams and estimate coordinate biases in real-time. the main processing procedure is shown in figure 5 : the procedure and adew model can be divided into three steps:"
"on neural level, large changes in the viewpoint of an object, such as inversion of faces [cit] and alphanumeric characters [cit], result in delays of the n170 component. the n170 is thought to reflect object classification, and inversion-related delays of n170 possibly reflect increases in time required to accumulate sufficient neural activity to reach a threshold at which recognition can occur [cit] . if changes in viewpoint delay visual object encoding, this could explain why accurate recognition of rotated objects requires longer viewing times than recognition of canonically oriented objects [cit] ."
"as shown in figure 6, when the compression ratio is up to 25, liver image compressed by all the above algorithms exist distortion phenomenon. however, the texture feature of the de-compressed image with our proposed algorithms is better than those in images with other methods."
"1. over a session time of 24 h, gnss observations were collected to compute all azimuth-elevations (azi-el) for single positions given the approximate coordinates of the stations so that the adem coefficients could be initialized by for the first use. 2. single-difference observations (sd) derived from raw observation data were applied to establish the dd observations. the float ambiguity of the dd ambiguity was estimated by the leastsquares (ls) criterion, and the virtual elevation was used for careful weighting. to achieve a higher precision of the dd ambiguity, lambda was employed to estimate the group of optimum integer ambiguities according to a threshold of the least-squares ratio (3.0). 3. fixed coordinate biases were calculated as well as output by the optimum integer ambiguities if the solution was successfully fixed; otherwise, the float solution was used instead."
"initial coordinates solved were derived from the pdb (https://www.rcsb.org) and processed by using a structure preparation module in the molecular operating environment (moe). briefly, loops were modeled for disordered regions whose number of residues was less than seven and the other non-natural n-termini and c-termini were capped with acetyl groups and formyl groups, respectively. addition of the hydrogen atoms and generation of the topology files were carried out using a pdb2gmx module in the gromacs 31 . all md simulations were carried out in periodic boundary conditions (pbc) by using the gromacs on nvidia geforce gtx 1080 gpu. amber ff99sb-ildn force field was used for proteins, nucleotides and ions 32, and tip3p was used for water molecules 33 . water molecules were placed around the complex model with an encompassing distance of 10 å. counter-ions were added to neutralize the system. electrostatic interactions were calculated using the particle mesh ewald (pme) method 34 with a cutoff radius of 10 å, and a nonbonded cutoff of 10 å was used for van der waals interactions. the p-lincs algorithm was used to constrain all bond lengths 35 ."
"the discussion above indicates the multi-disciplinary nature of an m&s study. within the overarching framework of a hybrid m&s study, hsm recognises and deploys the use of inter-disciplinary methods at various other stages of a simulation study. figure 3 shows our conceptualisation of a hybrid m&s study, identifying some inter-disciplinary methods that have been used (or can potentially be used) in specific stages of an m&s study. our conceptual representation is not exhaustive (indeed not all stages of a simulation study are depicted; stages pertaining to input data and output data analysis have been combined; model formalism has been introduced as a stage). figure 3 includes the model development stage in the centre and depicts four simulation techniques which can be used either in isolation (as in the conventional studies) or can be combined to implement an hs. the techniques are represented in grey boxes to distinguish them from non-simulation-centric methods and techniques ("
"next, we compared the adem model results with those from the sf approach to evaluate the performance in terms of positioning accuracy in the coordinate domain for deformation monitoring within the study environments. in this test, we adopted the first day (doy 020) to build the sf model. the db8 wavelet was used to filter the high frequency noise [cit], and filter value subtraction was performed at the sidereal (23 h 56 min 04 s) [cit] . the data at doy 021 were set as the multipath reduction target. experimental results are shown in figure 11 . figure 11 shows the coordinates bias in each enu direction before and after the multipath correction at doy 021 for the sf model, both with ambiguity fixed successfully (green) and failed (orange) solutions. the rms of the correction coordinate bias reached up to 0.1784 m, 0.4530 m, and 0.2666 m in the e, n, and u direction, respectively. it is clear that the performance of the sf model as shown was not very good according to the data from our experiment, since the ambiguities in the repeat period were not fixed every time. in fact, the integer ambiguities with the data collected from high-multipath environments were difficult to fix. therefore, the sf model may not be suitable for use in such complex environments, and the following work only shows the typical model results for comparative purposes."
"the specific coefficients of the adem model presented in this study were estimated by a 24 h session of gnss observations collected from nq02 in the complex environment of site (a). results of these experiments indicated that the specific coefficients were applicable for doy 021, 045, and 072 [cit] . subsequently, the performance of the adem model was investigated by using bds observations, and we compared the results to those obtained with the sf model. results of these experiments revealed that the rms values of coordinate bias when using the adem model had improved by 18.91% and 34.93% in the horizontal and vertical direction, respectively, in comparison with those derived from the use of the tfc model; significant improvements over the sf model were also observed. however, a break point remained, and this could be accounted for by the rejection of observations caused by the fixed cut-off elevation."
"ten co 2 datasets were derived from the primary dataset for monte carlo cross-validation [cit] . each derived dataset comprises of 10% of randomly sampled data from the primary data for training and the remaining 90% for validation. a different random seed was used for each random sampling. there are two reasons for choosing the sample ratio of training and validation. first, the matrix size for solving equation (9) is the square of the sample size. if a large proportion of the primary data was used for training, the matrix alone would exhaust the memory of a pc or training would take too long to be practical. second, with all co 2 measurements in all years combined in a 1x1 degree grid mesh, less than 10% of the global oceans was sampled in any single month [cit], which indicates that an ideal model should perform well with a smaller training dataset and a larger validation dataset."
"in equation (2), is decided by, when is set as a constant value. the main challenge of the adem model is to derive in a convenient way. note that when the satellite rises and sets behind a mountain ridge, the line-of-sight between the satellites and receiver can be interrupted. as a result, the elevations of the initial and terminal points ( ) in the trajectory approximately intersect with the mountain ridge. therefore, the scatter of for gnss can be employed to describe the physical horizon surrounding the antenna. here, data were collected from gnss receivers mounted in the study environment during a 24 h session. all the trajectories of gnss satellites in the elevation and azimuth domain are shown in figure 2 ."
"the introduction section presents functional definitions for hs and hsm. as the objective of both hs and hsm is to best represent the system of interest, we believe that a unifying conceptual framework will further clarify the terminologies and put them in perspective and, more importantly, enable exploration of synergies between hs and hsm. as hsm methods are inter-disciplinary and span the various stages of a hybrid m&s study (figure 2 ), and which is unlike hs which mainly concerns with model development / implementation stage (figure 1 ), a unifying conceptual representation could take several forms (e.g., figure 3 ). however, with increasing interest in hs and debates around its definition, scope and purpose, it is important to learn from the current academic discourse and present a hs-centric view of hsm. we therefore find it pertinent to lay the foundations of a unifying hs-hsm representation using the vocabulary of operations research (or) that most academics and practitioners in our field are familiar with. in other words, our unifying conceptual representation will, for now, be restricted to a discussion on hs and the hsm methods and techniques that are used in wider or literature. towards this, we begin by first developing a definition for hs, which will need to align with the historic, albeit infrequent, use of the term (case 1 below), it should be representative of its present use (case 2) and, ideally, put in place an intellectual scaffolding to support future research (case 3 and 4)."
"here, sum i is summation of the structure similarity and number of the smallest-size sub-blocks. n i is the number of the highest-priority sub-blocks in the ith initial split position."
"we take peak signal-to-noise ratio (psnr), mean squared error (mse), the running time, and normalized crosscorrelation (ncc) as four evaluation indexes to compare our proposed algorithm with the contrast methods. when the compression ratio is fixed to 25, the value of the above evaluation index is shown in table 1 . as shown in table 1, the psnr and ncc values of our proposed algorithm are higher than the other four contrast methods, while the running time and mse values of it's lower than the other contrast methods."
"our approach aims to quantitatively extract the hidden dynamics information at atomic or residue levels only from distributions of density data in cryo-em maps, named as defmap (dynamics extraction from cryo-em map). defmap determines the dynamics corresponding to root-mean-square fluctuation (rmsf) values representing atomic fluctuations around the average structure in md simulations. we utilized one of the deep-learning methods, 3d-cnn [cit], which is widely used to detect or classify threedimensional objects constructed from several resources, such as a video data [cit] and magnetic resonance imaging [cit] . moreover, 3d-cnn has been shown to exhibit remarkable performance on the three-dimensional cryo-em maps to recognize several structural patterns, such as secondary structures, amino acids, and the local map resolutions [cit] ."
"the experiments presented above were assessed with bds observations. next, three groups of gnss observations collected on random days for nq02 were employed to further assess the performance of the proposed adew model. the navigation systems used in these experiments consisted of gps, glonass, and bds. figure 13 illustrates the deviation of the coordinate bias in the time series for the horizontal direction derived from a fixed solution on doy 021 (a); 045 (b); and 072 (c) [cit] ."
"hence, the adem model is proposed, whereby instead of the geoid approach, the model uses a specific azimuth-dependent elevation mask. the adem model can be expressed as follows:"
"where ⃗ and ⃗ respectively denote the azimuth and elevation angle vector of in 24 h, and ⃗ represents the coefficients for the polynomial of degree . the functions and are in the curve fitting package. the ⃗ coefficients can be derived by, while the on azimuth of any line-of-sight can be recalculated from ⃗ by ."
"although the adem model (blue) performed better than the tfc (red), as shown in figure 10, there was still a break point detected at around 15:55 utc. the break may be accounted for by the discontinuous variance of c07 (figure 9d ) used in the stochastic model. to overcome the limitations discussed in section 3.3, the adew model is proposed, and it uses projection transformation for a continuous variance without considering the heights of mountains and the distances between the obstacles and the antennas. the variance is shown in figure 9c, and the improvement in terms of positioning is presented in figure 12 . figure 11 shows the coordinates bias in each enu direction before and after the multipath correction at doy 021 for the sf model, both with ambiguity fixed successfully (green) and failed (orange) solutions. the rms of the correction coordinate bias reached up to 0.1784 m, 0.4530 m, and 0.2666 m in the e, n, and u direction, respectively. it is clear that the performance of the sf model as shown was not very good according to the data from our experiment, since the ambiguities in the repeat period were not fixed every time. in fact, the integer ambiguities with the data collected from high-multipath environments were difficult to fix. therefore, the sf model may not be suitable for use in such complex environments, and the following work only shows the typical model results for comparative purposes."
"from equation (5), an elevation angle in the range of [e a, π/2] can be transformed to [0, π/2]. note that in the adew model, the measured elevation angle e, which is lower than e m in the adem model, will instead be determined by the virtual elevation angle e, and for the results, the rejection range is changeable according to variations of h m as well as dis. higher h m or longer dis values will lead to a narrower range of rejection with careful weighting. this improvement not only increases the positioning accuracy through the assignment of phase observations with careful weights, but also promotes the usage of available observations."
"as we move through our environment, we encounter familiar objects from various viewpoints. despite the ensuing variability of the images projected onto the retina, we have seemingly little difficulty when it comes to recognizing objects we encounter. we can, however, see how the objects are oriented, suggesting that object recognition is to a certain degree dissociable from perception of other object \"features\" such as orientation. changes in orientation of objects, particularly inversion, can also affect how we perceive the objects. a particularly illustrative example (shown in figure 1 ) is that of the thatcher illusion [cit], where the grotesque appearance of a face with its inverted eyes and mouth is \"hidden\" when the whole face is also inverted. the percept itself, therefore, is affected by the change in orientation. in addition, there are also subtle effects of viewpoint changes on object recognition itself. for example, identifying rotated objects is more difficult when they are briefly presented than when viewing time is unlimited [cit], and identifying a face is considerably more difficult the face has been inverted [cit], as is discrimination between characters \"b\" and \"d,\" or \"p\" and \"q\" which requires (physical or mental) rotation of the characters to upright, before we can be certain which letter we are looking at [cit] ."
"it has been previously demonstrated that a significant improvement with the adem model can be achieved by using based on, wherein a specific elevation range below is rejected. however, the rejection range should not be a constant value since there are different heights of vegetation and changeable distances between antennas and objects. to estimate the relationships among the range, the height of a mountain peak, and the horizontal distance between antennas and the peak, we assumed that the antenna was mounted beside the peak of a mountain, as shown in figure 3 . the geometric relationships among the specified range ( ), height of a mountain peak (ℎ ), and horizontal distance ( ) between an antenna and the peak."
"liver ct images and brain ct images are used to test the proposed method. figure 3 shows the reconstructed images of the different codebooks. figures 3(a) the calculation of compression ratio should take both of the lossless part and lossy part into account. the compression ratio of lossless part is 1.5097, and the lossy part is 15.4451, so the real compression ratio is 9.7943. as for the head image, the compression ratio of lossless part is 1.3458, and that of the lossy part is 15.3250, the real compression ratio is 9.5363."
"in order to overcome the limitations of the tfc and adem models, the range of impacts caused by the cut-off elevation related to h m and dis were investigated. detailed analyses revealed that the adaptive range was greatly affected by h m but not by dis. therefore, the adew model, in which the projection transformation is employed to project the satellites elevation range from [e aziele mask, π/2] to a virtual range [0, π/2], was developed by careful weighting in a stochastic model. the model experiments with bds observations revealed that the rms values of the adew model had further improved by 21.9% and 29.8% in the horizontal and vertical direction, respectively. model experiments with gnss observations demonstrated mean rms improvements of 34.69% and 31.58% in the horizontal and vertical direction, respectively. it was also demonstrated that the physical coefficients of the adew model are applicable for a long period of time, even more than a month. finally, another rover station bm02 at site (b) was used to assess the validity of the adew model. the results of this analysis agreed in that the adew model can significantly reduce the effects of diffraction and multipath errors by 18.10% and 9.21% in the horizontal and vertical direction, respectively."
"as already mentioned, face recognition is worse when faces are inverted [cit], both in terms of reduced recognition accuracy and increased reaction times. this seems to be the case both for familiar and unfamiliar faces, and may be a consequence of disrupted neural processing underlying object classification although a causal relationship has not been firmly established. it should be noted here that faces are nevertheless recog- descriptor such as \"a semi-circle attached at an end of a long stem\" could lead to selection of possible four candidates, and the remaining possibilities would need to be resolved with mental rotation. mental rotation has been associated with linear increases in centro-parietal negativity between ∼400 and 800 ms after stimulus onset (e.g., [cit] b) which last somewhat longer for larger angular departures from upright [cit] b; [cit] ) . the erp correlates of mental rotation are probably generated by a distributed network of sources localized [cit] b) within a network of prefrontal and posterior parietal areas which has been identified using fmri (e.g., [cit] ) . whether these areas also subserve recognition of rotated parity-defined objects is still unclear as this particular question has not been investigated using neuroimaging."
"as shown in figure 3, we can see that the qualities of two images decoded with different codebooks are almost the same when the compression ratio is fixed. however, the contour of the abdominal of the energy codebook is clearer than the random codebook. besides, the ssim value of the decoded image of energy codebook and the original image is higher than that of random codebook by 0.02. as to the head image, the fidelity of the two decoded images is lower than that of liver images, but it will not cause diagnosis failure. the contrast of the reconstructed image of the energy codebook is superior to that of the random codebook, and the ssim between the energy decoded image and the original image is higher than that of random decoded image. the average ssim values of 30 liver images and 30 head images decoded by the two kinds of codebook at different compression ratio are illustrated in figures 4 and 5 ."
"to validate the ability of the adew model in another complex environment, the rover point (bm02) at site (b) was analyzed. the observations of gnss were collected on doy 040 [cit], and the polynomial coefficients were estimated by using the polynomial fitted model. details of the receivers and antennas used have been given in table 1 . figure 14 shows the time series of the deviation of coordinate bias in the adew model (green), and the results for the tfc model (red) are also shown in figure 14 for comparison. as shown in figure 14, the rms values of the coordinate bias time series obtained by using the adew model were 0.0095 m and 0.0069 m in the horizontal and vertical direction, respectively. use of the tfc model resulted in rms values of coordinate bias of 0.0116 m and 0.0076 m in the horizontal and vertical direction, respectively. thus, significant improvements of 18.10% and 9.21% were achieved by applying the adew model over the adem model."
"although the adem model (blue) performed better than the tfc (red), as shown in figure 10, there was still a break point detected at around 15:55 utc. the break may be accounted for by the discontinuous variance of c07 (figure 9d ) used in the stochastic model. to overcome the limitations discussed in section 3.3, the adew model is proposed, and it uses projection transformation for a continuous variance without considering the heights of mountains and the distances between the obstacles and the antennas. the variance is shown in figure 9c, and the improvement in terms of positioning is presented in figure 12 ."
"to validate the ability of the adew model in another complex environment, the rover point (bm02) at site (b) was analyzed. the observations of gnss were collected on doy 040 [cit], and the polynomial coefficients were estimated by using the polynomial fitted model. details of the receivers and antennas used have been given in table 1 . figure 14 shows the time series of the deviation of coordinate bias in the adew model (green), and the results for the tfc model (red) are also shown in figure 14 for comparison. it is clear that the rms of the coordinate bias with the adew model was found to have improved, relative to that of the adem model, by 30.11%, 37.27% and 37.71% in the horizontal direction on doy 021, 045, and 072, respectively. the mean improvement of the rms in the vertical direction was greater than 31.58%. furthermore, the results suggest that the polynomial coefficients of the adem model are applicable for a long period of time, which can help to mitigate the effects of diffraction and multipath errors, even within a full month."
"it has been previously demonstrated that the signals of gnss can be potentially obstructed by concrete obstacles (i.e., topographic ridges) as well as by disperse obstacles (i.e., trees), as is shown in figure 1 . note that the maximum bias on the carrier-phase was due to the diffraction of ~7 cm [cit] and that the bias due to the multipath on frequency l1 was ~4.8 cm [cit] . traditionally, a typical fixed cut-off elevation (tfc) (15°-30°) dependent on the geoid is employed to reduce the multipath errors, as it is assumed that the antenna is mounted in an unsheltered environment. however, an antenna may be mounted in a complex environment as shown in figure 1 . where denotes the changeable cut-off angle dependent on the azimuth, denotes the constant cut-off elevation, and denotes the specific elevation mask. hence, the adem model is proposed, whereby instead of the geoid approach, the model uses a specific azimuth-dependent elevation mask. the adem model can be expressed as follows:"
"the difference in viewpoint-sensitivity of identification and categorization has also been established for other classes of objects. for example, identifying letters of the alphabet is affected by character orientation while the same is not the case for between-category decisions such as letter-digit categorization [cit] . in a sense, categorization may relate to recognition at a basic or entry level described by roch [cit], while identification may be more closely related subordinate-level recognition. object recognition at basic level (e.g., deciding a shape is a dog) are not affected by changes in viewpoint, while subordinatelevel decisions (e.g., identifying a dog as a poodle) are affected by viewpoint changes in terms of reaction times and accuracy [cit] ."
"to assess the proposed method in a complex environment, a series of experiments were performed in two regions with frequent landslide occurrences. the data were processed by a gnss software package developed by our research group, which can achieve millimeter-scale precision for deformation monitoring of a short-baseline within 10 km under an unsheltered environment with gnss data. for comparison purposes, the sf model was also employed to evaluate our proposed method. here, we present our results."
"it can be noted from figure 2 that the physical structures surrounding the antennas can be described precisely by the of the multi-gnss, although a slight discrepancy with the photograph may be observed. based on the of 24 h, the curve fitting package in matlab was adopted to estimate the fitting coefficients for . then, the can be expressed as follows: where e m denotes the changeable cut-off angle dependent on the azimuth, e c denotes the constant cut-off elevation, and e a denotes the specific elevation mask."
"in conventional quadtree method, the image is first divided into many initial sizes of sub-blocks from the first row to the last, from the first column to the last column, then the initial sub-blocks will continue to split. in the proposed technique, the initial split position will change regularly and then the best initial split position is chosen to process the initial division of the image. the novel method consists of two steps: statistics step and selection step. the schematic diagram of the optimized quadtree method is shown in figure 2 ."
"geostationary earth orbit (geo) and inclined geosynchronous orbit (igso) were used, which could be tracked by the antenna at all times. therefore, only the observations of bds are considered in this section, as well as in section 3. it is clear from figure 8 that the signal of nq02 was lost around 16:50, but the signal was still traced by nq01. according to the panoramic view of nq02, the signals of c07 were affected by the vegetation at about 16:10, and the influence lasted until the signal was lost in the set behind the from figure 7, it can be observed that the fitted line coincides with the boundary of the satellite tracking on the selected days, which indicates that the specific fitted coefficients are suitable for a long period of time without considering the sidereal cycle. in this study, the coefficients were renewed at 00:00 utc on the first day of each month."
"change in orientation must affect processing of visual information. for example, as our viewpoint changes, so does the shape of the image that falls on the retina. in the case of picture-plane rotations, the orientation of the edges of that shape will also change and thus stimulate different populations of orientation-tuned visually responsive neurons in primary visual cortex. however, these initial effects of orientation-changes on neural processing probably do not give rise to altered perceptual experience such as those associated with inversion of a thatcherized face."
"original cryo-em density maps and the corresponding atomic models with the overall map resolutions less than 4.5 å, [cit] (5.6 å), were selected and downloaded from pdb (thirty-four datasets). the training and evaluation datasets were composed of twenty-five and nine macromolecules, respectively (supplementary table 1 ). the maps were rescaled as 1.5 å/pixel and low-pass filtered to 5, 6, 7, 8, 9 and 10 å by using eman2.3 30 . subsequently, the intensities were normalized within each map data and those less than zero were discarded. the grid in the maps were assigned with md-derived dynamics (logarithmic rmsf) of the nearest atoms in the voxelized coordinate. the resulting maps were sub-voxelized to generate the input density data with the sizes of 10 3 voxels (15 3 å 3 ). for training data, we implemented data augmentations by horizontal/vertical rotations with 90, 180 and 270 degrees. voxelization of atomic models were carried out by utilizing high throughput molecular dynamics (htmd) 38 ."
"however, while this is a critically important phase, an m&s study comprises several other well-defined stages [cit], for example, problem formulation stage/conceptual modelling [cit], input data analysis, v&v, experimentation and output data analysis. it is, then, appropriate to explore the use of multiple techniques in the wider perspective of a study. we, therefore, distinguish between hybrid simulation and a hybrid m&s study, the latter referring to studies that apply multiple methods and techniques to one or more stages of a simulation study. while a hybrid m&s study provides the conceptual framework to consider the constituent stages of a conventional m&s study and explore complementary techniques, we refer to the actual application of these techniques together with simulation as hsm. thus, hsm is an enabler to hybrid m&s study. figure 2 clarifies the differing scope of hybrid simulation vis-à-vis hybrid m&s. it shows that a hybrid m&s simulation study will apply well-defined methods from disciplines outside m&s in one or more stages of the study (quadrant 2; figure 2) . a hybrid m&s study will also be a hybrid simulation when multiple simulation techniques have been used in the model implementation stage (quadrant 1; figure 2 ). however, implementation of hybrid simulation without the application of inter-disciplinary methods in the wider study will disqualify it from being a hybrid m&s study (quadrant 4; figure 2 ). quadrant 3 represents the traditional studies which have used only one modelling technique and which in methods from other disciplines have not been used."
"in equation (5), the projection transformation is adopted to project the measured elevation ( ) used in the stochastic model to a virtual elevation ( ), that equaling to change the coefficients in the weighting model."
"studies which have directly compared identification and categorization of objects using neuroimaging methods are scarce. nevertheless, studies investigating neural correlates of rotated-object categorization show little evidence of orientation-dependence at visual processing stages beyond the initial encoding of the objects (see above). in contrast, studies investigating rotated-object recognition either as identity-matching or in terms of explicit identification show that there is [cit], but not size transformation [cit], suggesting that changes in orientation degrade certain types of perceptual information which may be required for taskspecific decision making, and may be, thus, associated with some form of perceptual decision making [cit], 2010), such as whether sufficient information is available for the perceptual goal to be achieved. this decision would then trigger other visuospatial cognitive operations, such as mental rotation or more detailed inspection of individual features of an object. those cognitive operations would lead to acquisition of additional information about the object which would, in turn, enable a more accurate completion of the perceptual task at hand. for the purpose of illustration, two types of \"perceptual goals\" that depend on object orientation will be described: object identification and parity-based recognition."
"these subtle, yet persistent, effects of viewpoint changes on perception and recognition arise as a consequence of how visual object processing is handled by the brain. here, i discuss how neural mechanisms underlying visual processing give rise to perception and recognition which can be both viewpoint dependent and viewpoint invariant depending on the timing of those processes, as well as specific task demands or current \"perceptual goals\" of an individual. to do so, i will firstly explain how temporal dynamics of low-level visual processing may give rise to impaired recognition at short viewing latencies and suggest that this may also relate to effects of viewpoint changes on perceptual experience. i will then discuss how the perceptual goals of an individual determines whether recognition is accomplished in viewpoint invariant or dependent manner with a particular focus on cognitive operations thought to be subserved by ventral and dorsal visual streams, namely object recognition and mental rotation, respectively."
"various studies have explored techniques for mitigating the effects of diffraction and multipath errors, and these techniques can be classified into three groups. the first group usually mitigates"
"here, we report a deep-learning-based approach to investigate dynamics information using only a cryo-em map. three-dimensional cryo-em maps solved by spa are reconstructed from a vast number of two-dimensional molecular particle images in the micrographs [cit] . in the cryo-em spa, the specimens are prepared by rapidly freezing the macromolecule solutions in which proteins adopt variable conformations. therefore, the dynamics properties are \"hidden\" in the reconstructed cryo-em maps. in this regard, an approach, manifold embedding, was reported to investigate the conformational heterogeneity using the two-dimensional particle images 9 . nevertheless, avenues for the direct extraction of the dynamics information only from the cryo-em maps have not been challenged, while researchers have known that the local map intensities are, to some extent, correlated with the dynamic properties, i.e., the intensities at flexible regions are weakened."
"two sites in shaanxi, china, were used as study areas to assess the proposed technique's ability to improve the accuracy of micro-deformation monitoring results. one site was located in the city of ningqiang, while the other site was located in the city of hanzhong. both surrounding environments are complex with mountainous terrain and various obstacles, which act as major sources of diffraction and multipath errors. at site (a), the gnss observations were collected randomly on the days of year (doy) 021, 045, and 072 (21 january, 14 february, and 12 march, respectively) [cit] . at site (b), the gnss observations were collected on doy 040 [cit] . the distributions of stations at sites (a) and (b) are shown in figure 6 . the procedure and adew model can be divided into three steps:"
"it has been previously demonstrated that a significant improvement with the adem model can be achieved by using e c based on e a, wherein a specific elevation range below e m is rejected. however, the rejection range e c should not be a constant value since there are different heights of vegetation and changeable distances between antennas and objects. to estimate the relationships among the range, the height of a mountain peak, and the horizontal distance between antennas and the peak, we assumed that the antenna was mounted beside the peak of a mountain, as shown in figure 3 ."
"although changes in viewpoint rarely interfere with common perceptual goals, such as categorizing objects into basic categories, this type of viewpoint invariant recognition can only be achieved after initial viewpoint-dependent neural processing has been accomplished. depending on current perceptual goals, changes in viewpoint may impose certain recognition costs, observable in terms of increased response latencies or reduced accuracy. these costs are likely to reflect increased cognitive demands associated with recognition of misoriented shapes such as detailed analysis of object features or mental rotation of the shape to its canonical upright. in this sense, recognition of objects will always be affected by changes in viewpoint early on in the visual processing stream, but these effects will taper off with time. at later visual processing stages, some types of perceptual goals such as object identification or parity discrimination, will require additional processing operations which will give rise to viewpoint dependent behavioral performance."
"conventional simulation techniques such as monte carlo simulation (mcs), discrete-event simulation (des), system dynamics (sd) and agent-based simulation (abs) [cit] ) . these techniques have, however, mostly been used in isolation. the application of hybrid methods can overcome the unavoidable limitations of any single approach. the search for the best possible representation and analysis of the system under scrutiny has, then, led to an increasing number of studies that combine simulation techniques (e.g., [cit] . this is commonly referred to as hybrid simulation. these combined methods allow application of multiple techniques in the model development / implementation stage of a simulation study (figure 1), thereby enabling synergies across techniques to engender improved insights."
"the surface ocean co 2 atlas (socat) is an international effort, endorsed by the international ocean carbon coordination project (ioccp), the surface ocean lower atmosphere study (solas), and the integrated marine biogeochemistry and ecosystem research program (imber), to deliver a uniformly quality-controlled surface ocean co 2 database. the many researchers and funding agencies responsible for the collection of data and quality control are thanked for their contributions to socat."
"we used the standalone freeware libsvm [cit] for the svm model. although there are other freeware like svmlight [cit] and svmtorch [cit], they had problems with large datasets in our test. for the ls-svm, there was no standalone freeware to our knowledge. available matlab toolboxes, such as ls-svmlab [cit] and statlssvm [cit], had problems with our large co 2 dataset. we have written a standalone freeware available at http://united-csfe.com/fcew/ann.zip. it implements a conjugate-gradient method to solve a large linear equation efficiently and includes the option to normalize data internally to release users from the normalization procedure."
"two sites in shaanxi, china, were used as study areas to assess the proposed technique's ability to improve the accuracy of micro-deformation monitoring results. one site was located in the city of ningqiang, while the other site was located in the city of hanzhong. both surrounding environments are complex with mountainous terrain and various obstacles, which act as major sources of diffraction and multipath errors. at site (a), the gnss observations were collected randomly on the days of year (doy) 021, 045, and 072 (21 january, 14 february, and 12 march, respectively) [cit] . at site (b), the gnss observations were collected on doy 040 [cit] . the distributions of stations at sites (a) and (b) are shown in figure 6 . as shown in figure 6, the lengths of the baselines were 204.41 m and 144.99 m for sites (a) and (b), respectively. the rover station (nq02) at site (a) was installed beside a hillside that was severely impacted by ridges and trees, especially from 0 to 180° in the azimuth direction (shown in figure 1 ). the rover station (bm02) at site (b) was installed in an area with mountainous ridges nearby. the reference stations at both sites were mounted in better places, and the equipment used was the same as that used at the rover sites. the operational details for the receivers and antennas used in the test, which are capable of tracking bds (b1, b2, b3), gps (l1, l2, l5), and glonass (p1, p2) satellite signals, are summarized in table 1 . the following three different model approaches were employed: (1) a conservative cut-off elevation angle was used with the tfc model; (2) a fixed cut-off elevation based on the adem was used; and (3) the proposed adew model was used. in addition, the sf model, which realizes the results in the coordinate domain in a simple manner, was employed to allow for comparisons with the adem model results. in our experiments, dual-frequency observations from gnss were processed by our program in a simulated real-time mode, which means that the data stream was input to the platform epoch by epoch; the results and output were also estimated epoch by epoch."
"after energy-minimization of the fully solvated systems, the resulting systems were equilibrated for 100 psec using a constant number of molecules, volume, and temperature condition (nvt) and run for 100 psec employing a constant number of molecules, pressure, and temperature condition (npt), with the heavy atoms of the macromolecules held in fixed positions. the temperature was maintained at 298 k with velocity re-scaling with a stochastic term 36, and the pressure was maintained at 1 bar with the help of parrinello-rahman pressure coupling 37, with temperature and pressure time constants set to 0.1 psec and 2 psec, respectively. subsequently, twenty-nanosecond production runs were carried out under the npt condition without positional restraints. the generated trajectories after pbc corrections were aligned using overall cα atoms, followed by calculations of the rmsf values (å) for heavy atoms with an rmsf module in the gromacs. the logarithms of rmsf are used as the dynamics."
"the last few years have seen an increasing number of papers being published in hybrid simulation (hs). this is a testament to the wide applicability of the combined use of multiple simulation techniques for system representation. [cit] . hs studies are extending m&s methodology by presenting a robust comparison of different techniques, through the development of hs frameworks (both conceptual and as a guide to practice), model integration artefacts (e.g., software), case studies and implementation of multi-methodology, multi-technique, and multi-methodology-multitechnique hs models (refer to figure 4) . however, as the m&s community embraces hs, we would like to emphasise on the opportunities that are made possible by the use of interdisciplinary approaches in traditional simulation studies; we use the term hybrid m&s study to refer to such studies. hybrid systems modelling (hsm), which we see as an enabler to hybrid m&s studies, is the combined application of simulation with approaches from other disciplines. use of multiple techniques does not necessarily have to be in the model development / implementation stage (as is the case with hs) but could be applied to other stages like conceptual modelling, input/output data analysis, v&v and model experimentation. irrespective, the objective of both hs and hsm is to better represent the underlying system of interest. as such, having a unifying conceptual framework for hs and hsm will help provide clarity of definitions and will enable the exploration of the combined use of hs with hsm approaches. in this paper we have taken an hs-centric view of our unifying framework; we have discussed this using the vocabulary of or and have restricted the scope to hsm methods and techniques that are used in wider or literature. our unifying conceptual representation is the classification of hs, with models of type d and type d.1 specifically referring to hsm applied to both qualitative and quantitative or."
"the double differencing (dd) technique allows for the elimination of most orbital, tropospheric delay, and ionospheric delay errors in short-baseline relative positioning. consequently, this technique has been widely used for short-baseline positioning."
"as shown in figure 3, the term can be used to denote the realistic range of observations that should be rejected because of multipath errors, and denotes the horizontal distance between the antenna and peak; the terms ℎ and ℎ denote the height of dispersed obstacles (i.e., trees) and the height of the mountain peak, respectively. according to their geometric relationships, the term can be expressed as follows:"
"during the process of variable block division, what sizes of block a pixel will be contained is not only depending on the priority of itself, but also on the priority of its surrounding pixels. so we can draw a conclusion that adjacency relationship is very important to each pixel. as for the whole sub-band, if we can find a good adjacency relationship for all pixels, the proportion of different sizes of image blocks will be changed and the quality of decoded subimage will also be changed. the purpose of the optimized quadtree is to select an excellent initial position to split the subimage, which will improve the compression ratio or boost the image quality at the same bit ratio."
"where ⃗ and ⃗ respectively denote the azimuth and elevation angle vector of in 24 h, and ⃗ represents the coefficients for the polynomial of degree . the functions and are in the curve fitting package. the ⃗ coefficients can be derived by, while the on azimuth of any line-of-sight can be recalculated from ⃗ by ."
"gnss techniques are recognized as an effective way to monitor geologic deformation in real-time. however, the signals of satellites are severely affected by both concrete and dispersed obstacles in complex environments; thus, the results of many existing models such as the sf model can lead to failures in interpretations. this study presents an adew model based on the specific adem surrounding antennas. the experiments on bds observations demonstrated that the rms of coordinate bias using the adem model improved relative to that of the tfc model by 18.91% and 34.93% in the horizontal and vertical direction, respectively. the extended adew model conferred a further improvement of 21.9% and 29.8% in the horizontal and vertical direction, respectively, when compared to the results from the adem model. the following conclusions can be drawn from the experimental results and validation work:"
"as shown in figure 3, the term e w can be used to denote the realistic range of observations that should be rejected because of multipath errors, and dis denotes the horizontal distance between the antenna and peak; the terms h t and h m denote the height of dispersed obstacles (i.e., trees) and the height of the mountain peak, respectively. according to their geometric relationships, the term e w can be expressed as follows: in the adem model, the rejected range e w is instead by the constant value e c . in fact, however, e w should be a variable because of the growth of vegetation throughout the year. for quantitative research, as shown in figure 3, we approximately set the heights of trees h t as a constant value of 5 m, set dis as 20 m, and set h m as 20 m. then, the geometric relations among e w, h m, and dis were visualized in figure 4 ."
"the adew model depends upon specific coefficients for each adem. therefore, developing a universal algorithm to eliminate the effects in real-time will be the goal of our future work. we will also consider the dilution of precision (dop) of gnss satellites in the future."
"the experiments presented above were assessed with bds observations. next, three groups of gnss observations collected on random days for nq02 were employed to further assess the performance of the proposed adew model. the navigation systems used in these experiments consisted of gps, glonass, and bds. figure 13 illustrates the deviation of the coordinate bias in the time series for the horizontal direction derived from a fixed solution on doy 021 (a); 045 (b); and 072 (c) [cit] . the results shown in figure 13 indicate that the adew model (green) exhibited a lower rms than the tfc model (red); details of the rms values are shown in table 2 ."
"in this paper, we introduce a new method to set initial code vectors of the codebook, which is based on the energy of each sub-block. in the proposed method, we first calculate the energy of each sub-block. the energy function is defined as"
the adem model constructed by gnss data instead of geoid data can greatly reduce the impact of physical obstacles near the antenna in complex environments.
"with the rapid development of modern medical industry, medical images play an important role in accurate diagnosis by physicians. however, the large amount of images put forward a high demand on the capacity of the storage devices. besides, telemedicine is a development trend of medical industry, while narrow transmission bandwidth limits the development of this project. to solve the problems mentioned above, a large number of researches have been carried out into medical image compression."
"a modelling & simulation (m&s) study commences with a system that needs further investigation -this is usually referred to as the system under scrutiny. this could be, for example, an investigation of a realworld problem or a consideration for a future system. a conceptual model is then developed and validated, followed by implementation of a computer model. in the verification stage, the computer model is checked to ensure that it is a good representation of the conceptual model and its implementation is free from errors. experiment scenarios are developed and verified, followed by experimentation. after the process of ensuring operational validation, the results of the simulation may be implemented. figure 1 [cit] . a discussion around m&s study is important as it enables us to explore the complementary techniques for problem understanding, problem conceptualisation, experimentation, data analysis, etc., and that could be applied to specific stages. our functional definitions (below) for terms used in the paper also refers to certain stages of the m&s study."
"the second dataset is an update of the one used for the reconstruction of the global surface ocean co 2 concentration [cit], which was assumed to be the function of latitude (lat), sea surface temperature (sst), sea surface salinity (sss), surface chlorophyll concentration (chl), mixed layer depth (mld), and month (mon), i.e.,"
"the atomic dynamics values calculated by defmap were post-processed for the subsequent validation and analyses as follows; the output values were normalized, followed by averaging for each residue. the residue-specific dynamics values (termed as dynamics defmap in the main text) were assigned to atomic models as temperature factors with htmd 38 and visualized using pymol 41 and ucsf chimera 42 . all of the postprocessing was performed using python."
"protein functions under physiological conditions are associated with the threedimensional structures and in-solution dynamics. therefore, their investigations at high resolutions are essential for elucidations of the molecular mechanisms underlying biological events, such as regulations in cellular signaling mediated by protein-protein interaction and metabolisms catalyzed by enzymes 1, 2 . to determine the protein structures, various structural biology techniques, such as nuclear magnetic resonance (nmr), x-ray crystallography, and cryo-em spa [cit], have been developed over the decades 6 . along with their techniques, the dynamics information has been quantitatively measured through several experimental and computational approaches, such as nmr, hydrogendeuterium exchange (hdx) mass spectroscopy (ms) 7, and md simulations 8 ."
"a common used method to initialize codebook is the forgy and random partition [cit] . it randomly chooses k observations from the data set and uses these data as the initial means while the random partition method first randomly assigns a cluster to each observation and then carries out an update step, thus computing the initial means to be the centroid of the cluster's randomly assigned points. the forgy method tends to spread the initial means out, while random partition places all of them close to the center of the data set. according to hamerly and elkan [cit], the random partition method is generally preferable."
"cryo-em spa opened a new era in molecular biology through solving threedimensional structures of macromolecules and supra-molecules. the complementary use of defmap with the conventional cryo-em spa would accelerate the elucidations of the molecular mechanisms underlying the biological events. it is noteworthy that defmap requires only cryo-em maps as inputs, being applicable to macromolecules with extremely large molecular sizes and the complicated structural assemblies, which make the dynamics investigations through conventional md simulations and experimental techniques difficult in principle. moreover, any researchers can access the dynamic properties without additional experiments requiring costs, time-consumptions, and indeep expertise. the present study showed the advanced usage of the structural data derived from cryo-em spa and the potential of the deep learning-based techniques in structural biology studies. we believe that defmap will be one of the promising choices in the data-driven structural investigations to clarify protein functions."
"single-difference observations (sd) derived from raw observation data were applied to establish the dd observations. the float ambiguity of the dd ambiguity was estimated by the least-squares (ls) criterion, and the virtual elevation e was used for careful weighting. to achieve a higher precision of the dd ambiguity, lambda was employed to estimate the group of optimum integer ambiguities according to a threshold of the least-squares ratio (3.0)."
"inversion affects how we perceive the spatial relations between objects' features and may, as james (1890) suggested, depend on perceptual experience with an object at a given orientation. this could explain why recognition of faces is particularly impaired by inversion: faces are most frequently seen the right way up, and are thought to be recognized using information about the configuration of the constituent features. as mirror reversal is also a special case of a configural change where the relative configuration of object's features remains the same but reverses in its left-right orientation, this could also explain why mirror-images are difficult to tell apart when they are rotated away from a canonical viewpoint, and which is why we must rotate objects into alignment with our egocentric reference frames before we can distinguish between parity-defined characters such as \"b\" and \"d\" [cit] . interestingly, neural responses to unaltered and thatcherized images also follow the perceptual illusion and disappear as the face is rotated away from upright [cit] ) ."
"defmap has a neural network architecture consisting of 3d convolutional blocks and dense blocks ( fig. 1a ). defmap reads 15 3 å 3 sub-voxels generated from low-pass filtered cryo-em maps and assigns the logarithmic rmsf of the heavy atoms to each grid based on the density data. we defined residue-specific values as residue-averages of normalized outputs from defmap and used them for the following evaluations and analyses (see post-processing and visualization of output from a neural network in methods). hereafter, the residue-specific values of defmap-and md-derived dynamics are termed as dynamics defmap and dynamics md, respectively."
"over a session time of 24 h, gnss observations were collected to compute all azimuth-elevations (azi-el) for single positions given the approximate coordinates of the stations so that the adem coefficients could be initialized by e min for the first use."
"it has been previously demonstrated that a significant improvement with the adem model can be achieved by using based on, wherein a specific elevation range below is rejected. however, the rejection range should not be a constant value since there are different heights of vegetation and changeable distances between antennas and objects. to estimate the relationships among the range, the height of a mountain peak, and the horizontal distance between antennas and the peak, we assumed that the antenna was mounted beside the peak of a mountain, as shown in figure 3 . the geometric relationships among the specified range ( ), height of a mountain peak (ℎ ), and horizontal distance ( ) between an antenna and the peak."
"in equation (2), e m is decided by e a, when e c is set as a constant value. the main challenge of the adem model is to derive e a in a convenient way. note that when the satellite rises and sets behind a mountain ridge, the line-of-sight between the satellites and receiver can be interrupted. as a result, the elevations of the initial and terminal points (e min ) in the trajectory approximately intersect with the mountain ridge. therefore, the scatter of e min for gnss can be employed to describe the physical horizon e a surrounding the antenna. here, data were collected from gnss receivers mounted in the study environment during a 24 h session. all the trajectories of gnss satellites in the elevation and azimuth domain are shown in figure 2 ."
"recent breakthroughs in cryo-em spa have uncovered numerous structures of biological molecules at atomic or near-atomic resolutions, including extremely large macromolecules that have not been solved by conventional techniques. however, the dynamics investigations of the molecules targeted in cryo-em studies are technically challenging in both experimental and computational approaches due to the large molecular sizes and the complicated structural assemblies."
"the specific coefficients of the adem model presented in this study were estimated by a 24 h session of gnss observations collected from nq02 in the complex environment of site (a). results of these experiments indicated that the specific coefficients were applicable for doy 021, 045, and 072 as shown in figure 14, the rms values of the coordinate bias time series obtained by using the adew model were 0.0095 m and 0.0069 m in the horizontal and vertical direction, respectively. use of the tfc model resulted in rms values of coordinate bias of 0.0116 m and 0.0076 m in the horizontal and vertical direction, respectively. thus, significant improvements of 18.10% and 9.21% were achieved by applying the adew model over the adem model."
"task-dependent effect of viewpoint changes on neural processing are only observed around 250 ms after stimulus onset and coincide with the p2 component of the erp. for example, if the observers need to determine whether a rotated alphanumeric character is normal or mirror-reversed, they will mentally rotate it to upright before making the decision. although the beginning of mental rotation is later than the p2, parity decisions are associated with linear increases of p2 amplitudes while this is not the case for p2 preceding categorization of alphanumeric characters which does not require mental rotation [cit] . interestingly, similar increases in p2 amplitudes can be observed as a consequence of stimulus degradation, either by addition of noise [cit] or by occlusion an increase in activity in areas involved in object recognition within the inferior temporal cortex for various object classes such as faces [cit], bodies [cit], landscapes [cit] . some authors have suggested that this increase in activity may reflect a shift in recognition strategy from one that is based on the whole shape to one that is based on the analysis of individual object features (i.e., [cit] ) ."
"fixed coordinate biases were calculated as well as output by the optimum integer ambiguities if the solution was successfully fixed; otherwise, the float solution was used instead."
"to assess the proposed method in a complex environment, a series of experiments were performed in two regions with frequent landslide occurrences. the data were processed by a gnss software package developed by our research group, which can achieve millimeter-scale precision for deformation monitoring of a short-baseline within 10 km under an unsheltered environment with gnss data. for comparison purposes, the sf model was also employed to evaluate our proposed method. here, we present our results."
"hsm extends m&s methodology by combining approaches from across disciplines (including the wider or), thereby adding further value to both the conventional and the hs studies and its application to practice. based on the discipline-specific methods and what it has to offer, this added value gained could be mapped to various stages of a simulation study, for example: conceptual framework for the application of descriptive and predictive analytics methods/techniques with computer simulation (prescriptive analytics) for the analysis of urgent care/a&e wait time data; they discuss the implementation architecture for an a&e model (des) that will use, as inputs, near real-time/business intelligence data from the nhsquicker platform (h& [cit] (h&cin, 2017b and predictions based on historic data."
"the data collected from site (a) are discussed in sections 3.2-3.5, and the data collected from site (b) are discussed in section 3.6 of this manuscript. as was mentioned earlier, there was no significant motion during the selected session time, and thus, the coordinate bias time series were caused solely by noise and propagation effects, such as those due to diffraction and multipath errors. figure 7 shows the curve of the line fitted with the e min points of all satellite traces received by rover station nq02 on doy 021. the other data from doy 045 and 072 were used to assess the reliability of the curve line. as figure 7a clearly shows, the scatter points were evenly distributed on both sides of the fitted-line curve. from figure 7, it can be observed that the fitted line coincides with the boundary of the satellite tracking on the selected days, which indicates that the specific fitted coefficients are suitable for a long period of time without considering the sidereal cycle. in this study, the coefficients were renewed at 00:00 utc on the first day of each month."
"the adew model can be used to further improve the performance of micro-deformation monitoring in complex environments relative to the adem model as it has been demonstrated here to exhibit not only lower rms values of coordinate bias in the time series, but also increases the usage of available observation data."
"author contributions: j.z., overall concept and original draft writing; z.-h.t., idea of side by side comparison, review & editing; t.m., satellite data usage and project administration; t.s., validation method."
"as shown in figure 3, the term can be used to denote the realistic range of observations that should be rejected because of multipath errors, and denotes the horizontal distance between the antenna and peak; the terms ℎ and ℎ denote the height of dispersed obstacles (i.e., trees) and the height of the mountain peak, respectively. according to their geometric relationships, the term can be expressed as follows: figure 3 . the geometric relationships among the specified range (e w ), height of a mountain peak (h m ), and horizontal distance (dis) between an antenna and the peak."
"as the area surrounding a gnss antenna remains unchanged for a relatively long period, this study constructed a physical elevation mask model around antennas by using multi-navigation satellites systems. model experiments were then used to demonstrate the performance of this approach for high-precision micro-deformation monitoring in complex environments, such as those where landslides occur."
"decisions regarding the direction of the left-right axis of an object, or its handedness, require alignment between the object and our own egocentric frame of reference. for example, deciding whether a shoe is the left or the right one requires either physical or mental rotation of the shoe into alignment with our feet, or the feet with the shoe. the same holds for any object class that has a well-defined left-right orientation, such as alphanumeric characters, which can be readily recognized as \"backward\" if they have been mirror-reversed [cit] ) -but only if they are presented at upright. rotated characters require rotation to their canonical upright before we can notice if they are normal or backward, particularly if they are rotated by a large degree [cit] . when the identity of an object depends on its left-right parity, as is the case with lower-case letters \"b\" and \"d\" or \"p\" and \"q,\" then the discrimination of such characters also requires rotation to upright before it can be successfully recognized [cit] ."
"as shown in figures 4 and 5, at low compression ratio, the performance of the energy codebook is similar to that of the random codebook. however, the performance of the energy codebook is obviously superior to the random codebook at high compression ratio."
"geostationary earth orbit (geo) and inclined geosynchronous orbit (igso) were used, which could be tracked by the antenna at all times. therefore, only the observations of bds are considered in this section, as well as in section 3. from figure 7, it can be observed that the fitted line coincides with the boundary of the satellite tracking on the selected days, which indicates that the specific fitted coefficients are suitable for a long period of time without considering the sidereal cycle. in this study, the coefficients were renewed at 00:00 utc on the first day of each month."
"in this paper, we propose an optimized medical image compression algorithm based on wavelet transform and vector quantization with variable block size. we, respectively, manipulate the low-frequency components and highfrequency components with the huffman coding and improved vq by the aids of wavelet transformation. although the implement of the proposed algorithm is more complex than some traditional medical image compression algorithms, it can compress an image with good visual quality with high compression ratio. besides, in contrast with some traditional and current medical image compression algorithm, the proposed algorithm owns better performance through the evaluation index of psnr, running time, mse, and ncc."
"the data collected from site (a) are discussed in sections 3.2-3.5, and the data collected from site (b) are discussed in section 3.6 of this manuscript. as was mentioned earlier, there was no significant motion during the selected session time, and thus, the coordinate bias time series were caused solely by noise and propagation effects, such as those due to diffraction and multipath errors."
"this suggests that information regarding the identity of the object must be extracted before information about the handedness of an object can be determined. although generally we need to recognize an object before mental rotation begins [cit], this cannot be the case for objects whose identity depends on their handedness, such as \"b\" and \"d\" or \"p\" and \"q.\" with the exception of alphanumeric characters, there are not many commonly encountered objects whose identity is defined by parity (i.e., a hand is a hand irrespective of whether it is a left one or a right one) and those objects can be seen as special case whose identity cannot be determined at all orientations. for these objects, identification from a feature-based nized as faces, what seems to be disrupted is the identification of the face as belonging to a particular person or identification of an emotional expression, while differentiation between categories of \"face\" and \"non-face\" objects is largely unimpaired by inversion."
"in equation (5), the projection transformation is adopted to project the measured elevation (e) used in the stochastic model to a virtual elevation (e ), that equaling to change the coefficients in the weighting model."
"as the area surrounding a gnss antenna remains unchanged for a relatively long period, this study constructed a physical elevation mask model around antennas by using multi-navigation satellites systems. model experiments were then used to demonstrate the performance of this approach for high-precision micro-deformation monitoring in complex environments, such as those where landslides occur."
"in addition to the balancing challenges, the large amount of data required, and the labeling for biomedical data, deep learning also requires technological improvements. unlike other images, subtle changes in medical images may indicate disease. therefore, analyzing these images requires highresolution inputs, high training speed, and a large memory. additionally, it is difficult to find a uniform assessment metric for biomedical data classification or prediction. unlike other projects, we can tolerate false positives to some extent, and reject few or no false negatives in disease diagnosis. with different data, it is necessary to assess the model carefully and to tune the model according to characteristics of the data. fortunately, the deeper networks with inception modules are accelerated [cit] and provide higher accuracy in biomedical image analysis [cit] . on the other hand, crowdsourcing approaches have begun to pave the way in collecting annotations [cit], which may be an important tool in the next few years. these bidirectional drivers would promote the applications of deep learning in biomedical informatics."
"despite the notable advantages of deep learning, challenges in applying deep learning to the biomedical domain still remain. take biomedical image analysis for instance: we use fundus images to exemplify how deep learning works to define the level of diabetic retinopathy, and to detect lesion areas in different ways. besides high accuracy and speed, the intelligent use of receptive fields also endows deep learning with overwhelming superiority in terms of image recognition. furthermore, the development of end-to-end classification methods based on deep learning sheds new light on classifying pixels as lesioned or not. however, the usage of deep learning in medical images is still challenging. for model training, we need large amounts of data with labels, sometimes with labels in terms of pixel classification. manually labeling these medical images is laborious and requires professional experts. on the other hand, medical images are highly associated with privacy, so collecting and protecting the data is demanding. furthermore, biomedical data are usually imbalanced because the quantity of data from normal classes is much larger than that from other classes."
"an ann with more hidden layers offers much higher capacity for feature extraction [cit] . however, an ann often converges to the local optimum, or encounters gradient diffusion when it contains deep and complex structures [cit] . a gradient propagated backwards rapidly diminishes in magnitude along the layers, resulting in slight modification to the weights in the layers near the input (http://deeplearning.stanford.edu/wiki/ index.php/ufldl_tutorial) [cit] . subsequently, a layer-wise pre-training deep auto-encoder (ae) network was proposed, bringing anns to a new stage of development [3, 4, [cit] ( figure 1 ). in this network, each layer is trained by minimizing the discrepancy between the original and the reconstructed data [cit] . the layer-wise pre-training breaks the barrier of gradient diffusion [cit], and also results in a better choice of weights for deep neural networks (dnns), thereby preventing the reconstructed data from reaching a local optimum where the local optimum is usually caused by the random selection of initial weights. in addition, the employment of graphic processing units (gpus) also renews the interest of researchers in deep learning [cit] ."
"new health problems, medications, and regimens emerge on a daily basis. to understand their clinical impact in a timely manner, large amounts of accurate genotypic and phenotypic data must be readily available for research in a cost-effective manner. the advent of highthroughput gene sequencing technologies has reduced the cost of obtaining genomic data exponentially, from 2.7 billion usd for the first human genome (the human genome project) 1 down to 1000 [cit] . currently, million-people-scale sequencing projects are under way to generate genomic data for research. 2 however, amassing phenotypic data remains a challenge, 3 as it traditionally takes human effort to record the phenotypes of patients."
"where w and b are the parameters of the model, g is the activation function (same definition applied in the following context), and h w;b represents the hidden units. when the number of hidden units, which represents the dimension of features, is smaller than the input dimension, the ae performs a reduction of data dimensionality similar to principal component analysis [cit] . besides pattern recognition, an ae with a classifier in the final layer can perform classification tasks as well."
"detection of pathologies on stained histopathology images [cit] exemplify the high precision of deep learning-based approaches. for breast cancer detection in histopathology images, [cit] established a deep learning model to precisely delineate the invasive ductal carcinoma (idc) regions to distinguish the invasive tumor tissue and noninvasive or healthy tissue. their 3-layer cnn architecture, composed of two cascading convolutional and pooling layers, a full-connected layer, and a logistic regression classifier for prediction, attained a better f-measure (71.8%) and higher balanced accuracy (bac; 84.23%) in comparison with an approach using handcrafted image features and a machine learning classifier."
"the normalized main icd-9-cm and nlp count features do not leverage information from other features, such as counts of competing diagnoses and medication prescriptions, which provide additional characterization of the presence of the target phenotype. in fact, some of these remaining features have been shown to possess predictive values beyond the main features in supervised algorithms trained with gold-standard labels. [cit] we thus wish to utilize the additional information contained in the entire candidate feature set to further refine phenotype definition, but in the setting of not using any gold-standard labels."
"the 2 steps of the phenorm procedure are outlined in figure 1 . the first step transforms a highly predictive feature of the target phenotype, such as the number of international classification of diseases, ninth revision, clinical modification (icd-9-cm) codes of the target phenotype in the patient records, to resemble a 2-component normal mixture distribution with high accuracy for prediction. the second step involves self-regression with dropout to denoise the transformed feature based on additional candidate features, similarly transformed, to further improve the prediction. the output is a linear combination of all the transformed features."
"besides the activation function, there are two particular types of layers in cnns: the convolutional layer and the pooling layer (figure 2 ). in the convolutional layer, the image is convolved by different convolutional filters via shifting the receptive fields step by step [cit] (figure 2a ). the convolutional filters share the same parameters in every small portion of the image, largely reducing the number of hyperparameters in the model. a pooling layer, taking advantage of the ''stationarity\" property of images, takes the mean, the max, or other statistics of the features at various locations in the feature maps, thus reducing the variance and capturing essential features (http://deeplearning.net/tutorial/lenet.html) ( figure 2b )."
"recurrent neural networks (rnns) outperform other deep learning approaches in dealing with the sequential data. based on the property of sequential data, parameters across different time steps of the rnn model are shared. taking speech as an example: some vowels may last longer than other sounds; the difference makes absolute time steps meaningless and demands that the model parameters be the same among the time steps [cit] ."
"the phenorm score can be converted to a predicted probability of having the disease phenotype using the em algorithm. if the goal of the phenotyping is to link the phenotype to genomic data, one can directly use the predicted probability as a continuous trait and perform association analysis by fitting a quasi-binomial model. in fact, one could gain power by leveraging the predicted probability, as compared to converting the probability to a binary trait. 54 when a small number of labels are available for validation, one can use these labels to estimate the receiver operating characteristic (roc) curve and then select a threshold value optimizing the tradeoff between positive predictive value and sensitivity."
"breakthroughs in technologies, particularly next-generation sequencing, are producing a large quantity of genomic data. efficient interpretation of these data has been attracting much attention in recent years. in this scenario, uncovering the relationship between genomic variants and diseases, and illustrating the regulatory process of genes in cells have been important research areas. in this review, we introduced the way deep learning gets involved in these areas using examples. with deep architecture, these models can simulate more complex transformations and discover hierarchical data representations. on the other hand, almost all of these models can be trained in parallel on gpus for fast processing. furthermore, deep learning can extract data-driven features and deal with highdimensional data, while machine learning usually depends on hand-crafted features and is suitable only to low-dimensional data. thus, deep learning is becoming more and more popular in genomic sequence analysis. deep learning is represented by a group of technologies (introduced in brief description of deep learning), and has been widely used in biomedical data (introduced in applications in biomedicine). saes and rbms can extract patterns from unlabeled data [cit] as well as labeled data when stacked with a classifier [cit] . they can also deal with dynamic data [cit] . cnns are most commonly used in the biomedical image analysis domain due to their outstanding capacity in analyzing spatial information. although relatively few cnns are used in sequencing data, cnns have great potential in omics analysis [cit] and biomedical signals [cit] . on the other hand, rnnbased architectures are tailored for sequential data, and are most often used for sequencing data [cit] and in dynamic biomedical signals [cit], but less frequently in static biomedical images. currently, more and more attention is being paid to the usage of deep learning in biomedical information, and new applications of each schema may be discovered in the near future."
"in this paper, we introduce phenorm for training accurate phenotyping algorithms without using gold-standard labels. in our road map to high-throughput phenotyping, we have decomposed the task into automated feature curation and algorithm training without gold-standard labels. the former goal has been achieved by afep and safe, and the latter goal has now been achieved by phenorm. phenorm is easy to implement, and its accuracy is similar to algorithms trained with gold-standard labels. the bandwidth provided by safe þ phenorm can potentially reduce the algorithm development process from months to a few days, providing the valuable phenotypic big data necessary for the study of precision medicine. 55"
"detection of lesion and abnormality is the major issue in medical image analysis. deep learning methods learn the representations directly instead of using hand-crafted features from training data. a classifier is then used to assign the [cit] representations to a probability that indicates whether or not the image contains lesions. in other words, the deep learning schemas classify each pixel to be a lesion point or not, which can be done in two ways: (1) classifying the mini patch around the pixel with a deep network, and (2) using a fully convolutional network to classify each pixel. [cit] applied a dnn to histologically characterize healthy skin and healing wounds to reduce clinical reporting variability. two unsupervised pre-trained layers of denoising aes (daes) were used to learn features in their hybrid architecture, and subsequently the whole network was learned using labelled tissues for characterization. detection of cerebral microbleeds [cit] and coronary artery calcification [cit] also produced better results when using deep learningbased approaches. in addition, brain tumor progression prediction implemented with a deep learning architecture [cit] has also shown a more robust tumor progression model in comparison with a high-precision manifold learning approach [cit] ."
"the phenorm algorithms using icd þ nlp count achieved an auc comparable to that of the corresponding supervised algorithms when 100 labels were used for cad and 300 labels were used for ra and cd. the auc of the phenorm score of uc appeared to be lower than those of the supervised algorithms, but an auc of 0.935 is acceptably high when comparing across phenotypes. none of the supervised algorithms attained an auc significantly higher than the unsupervised phenorm icdnlp . the aucs from the xpress and anchor methods were significantly lower than that of phenorm. in addition, as reported in the supplementary material, the performance of phenorm was not sensitive to the choice of the corruption rate or feature set."
"although applications of deep learning have been primarily focused on image recognition, video and sound analyses, as well as natural language processing, it also opens doors in life sciences, which will be discussed in detail in the next sections."
"the out-of-sample auc estimates for various algorithms are shown in table 1 . recall that the phenorm algorithm involves 2 main steps: (1) normalization and (2) denoising via dropout regression. comparing the auc of the icd-9-cm codes before and after normalization, we found that the normalization step substantially improved the accuracy of the codes, with average improvement in auc of around 0.04 across phenotypes. the denoising step further improved the auc with varying degrees of magnitude depending on the phenotype and the feature; it substantially improved the auc of the normalized icd-9-cm count, but was not as critical for the nlp or icd þ nlp count as it was for the icd-9-cm. comparing the phenorm algorithm applied to the different features, it appears that using the icd þ nlp count gave the most robust results across phenotypes, and the score based on majority voting achieved similar accuracy."
"although the underlying assumptions and theories are different, the basic idea and processes for feature extraction in most deep nn (dnn) architectures are similar. in the forward pass, the network is activated by an input to the first layer, which then spreads the activation to the final layer along the weighted connections, and generates the prediction or reconstruction results. in the backward pass, the weights of connections are tuned by minimizing the difference between the predicted and the real data."
"deep learning is moving toward its original goal: artificial intelligence. the state-of-the-art feature extraction capacity of deep learning enables its application in a wide range of fields. many deep learning frameworks are open source, including commonly-used frameworks like torch, caffe, theano, mxnet, dmtk, and tensorflow. some of them are designed as high-level wrappers for easy use, such as keras, lasagne, and blocks. the applications of deep learning algorithms is further facilitated by the freely available sources. figure 4 summarizes commonly-used frameworks in github (https://github.com/) where the number of stars reflects the popularity of the frameworks."
"deep learning is a recent and fast-growing field of machine learning. it attempts to model abstraction from large-scale data by employing multi-layered deep neural networks (dnns), thus making sense of data such as images, sounds, and texts [cit] . deep learning in general has two properties: (1) multiple layers of nonlinear processing units, and (2) supervised or unsupervised learning of feature presentations on each layer [cit] . the early framework for deep learning was built on artificial neural networks (anns) [cit] s [cit] . since then, deep learning has been applied to a wide range of fields, including automatic speech recognition, image recognition, natural language processing, drug discovery, and bioinformatics [cit] ."
"the past decades have witnessed a massive growth in biomedical data, such as genomic sequences, protein structures, and medical images, due to the advances of highthroughput technologies. this deluge of biomedical big data necessitates effective and efficient computational tools to store, analyze, and interpret such data [cit] . deep learning-based algorithmic frameworks shed light on these challenging problems. the aim of this paper is to provide the bioinformatics and biomedical informatics community an overview of deep learning techniques and some of the state-of-the-art applications of deep learning in the biomedical field. we hope this paper will provide readers an overview of deep learning, and how it can be used for analyzing biomedical data."
"in addition to static images, time-series medical records such as signal maps from electro-encephalography and magnetoencephalography can also be analyzed using deep learning methods [cit] . these deep learning schemas take coded features of signals [cit] or raw signals [cit] as input, and extract features from the data for anomaly classification or understanding emotions."
"genetic variation can influence the transcription of dna and the translation of mrna [cit] . understanding the effects of sequence variants on pre-mrna splicing facilitates not only whole genome annotation but also an understanding of genome function. to predict splice junction at the dna level, yoon and his collaborators developed a novel dbn-based method that was trained on the rbms by boosting contrastive divergence with categorical gradients [cit] . their method not only achieved better accuracy and robustness but also discovered subtle non-canonical splicing patterns [cit] . furthermore, by exploiting rnns to model and detect splice junctions from dna sequences, the same authors also achieved a better performance than the previous dbn-based method [cit] ."
"the results from our numerical studies indicate that phenorm achieves the same accuracy as supervised algorithms based on training set sizes between 100 and 300, depending on the phenotype. current large-scale phenotyping efforts (eg, 10 phenotypes at a time) rarely have the bandwidth to offer more than 200 gold-standard labels for training for each disease, thus illustrating the potential of our method to streamline phenotyping without compromising the accuracy of a supervised approach. additionally, our results demonstrate that the normalization step (x ! z) always significantly improves prediction performance, while the subsequent denoising step contributes in varying degrees. denoising appears to be critically important for icd-9-based algorithms, but contributes minimally to icd þ nlp-based algorithms. this suggests that the effectiveness of the denoising step is inversely related to the predictiveness of the normalized feature. in practice, one may wonder if the denoising step is still necessary, since z icdnlp is typically highly accurate. we would argue that such a step is still potentially beneficial, particularly in settings where the icd-9-cm code provides a poor characterization of the desired phenotype or the nlp software fails to accurately capture the description of the phenotype. in this case, z icdnlp would benefit from the additional information in the related features offered by denoising."
"where a is a balance of these two components, and in practice, the loss function is usually calculated across randomlysampled training samples rather than the data-generating distribution, since the latter is unknown."
"as a long-term goal, precision medicine research demands active learning from all biological, biomedical, as well as health data. together with medical devices and instruments, wearable sensors and smart phones are providing unprecedented amounts of health data. deep learning is a promising interpreter of these data, serving in disease prediction, prevention, diagnosis, prognosis, and therapy. we expect that more deep learning applications will be available in epidemic prediction, disease prevention, and clinical decision-making."
"the mammogram is one of the most effective imaging modalities in early diagnosis and risk prediction of breast cancer. a deep learning model [cit] trained on a large dataset of 45,000 images attained performance similar to that of certified screening radiologists in mammographic lesion detection. [cit] investigated the scoring of percentage mammographic density (pmd) and mammographic texture (mt) related to prediction of breast cancer risk. they employed a sparse ae to learn deep hierarchical features from unlabeled mammograms. multinomial logistic regression or softmax regression was then used as a classifier in the supervised training. as a result, the performance of their approach was comparable with that of the subjective and expensive manual pmd and mt scorings."
"where t is the label for time, w and v represent the weights connecting hidden and input units, and hidden and output units, respectively, b and c are the offsets of the visible and hidden layers, respectively, g is the activation function, and u represents the weights connecting hidden units at time t à 1 to hidden units at time t (figure 3 ). similar to other deep learning architectures, rnns can also be trained using the bp method. a variant of the bp method called back propagation through time (bptt) is the standard optimization method for rnns [cit], and some alternative methods have also been proposed to speed up the optimization or to extend its capacity [63, [cit] ."
"frobenius parameter regularization is induced by the inner product and is block decomposable, therefore it is easier to compute [cit] . frobenius parameter regularization can be defined as"
"regularization term l 2 parameter regularization is the most common form of regularization term and contributes to the convexity of the optimization objective, leading to an easy solution for the minimum using the hessian matrix [cit] . l 2 parameter regularization can be defined as"
"this work was supported by us national institutes of health grants u54-hg007963, u54-lm008748, r01-hl089778, r01-hl127118, f31gm119263-01a1, and k23-dk097142, the harold and duval bowen fund, and internal funds from tsinghua university and partners healthcare."
where r i is the i-th largest singular value. frobenius parameter regularization has a function similar to nuclear norm in terms of regularization. nuclear norm has been widely used as regularization in recent years [cit] . nuclear norm regularization measures the sum of the singular values of x and can be defined as
"in addition to the secondary structure prediction, deep learning was also employed in protein region prediction [cit] . for instance, sequenced-based predictor of protein disorder using boosted ensembles of deep networks (dndisorder), a deep neural network with multi-layers of rbms [cit], achieved an average balanced accuracy of 0.82 and an auc of 0.90. incorporated with predicted secondary structure and predicted asa, a weighted deep convolutional neural fields (deepcnf) was proposed to predict protein order/disorder regions, obtains an auc of 0.898 on the critical assessment of techniques for protein structure prediction (casp10) dataset [cit] . all of these methods surpassed other state-of-the-art predictors in accuracy while still maintaining an extremely high computing speed. recently, raptorx-property, a web server employing deepcnf, was also presented to predict protein structure properties, including secondary structure, solvent accessibility, and disorder regions [cit] . raptorx-property can be easily used and offer good performance (an auc of 0.89 on its test data)."
"wv is the energy function [cit] . the conditional probability distribution can also be computed by integral, and the parameters can then be optimized by minimizing the kullback-leibler divergence. overall, given the network architectures and optimized parameters, the distribution of the visible units could be computed as:"
"owing to advances in high-throughput technologies, a deluge of biological and medical data has been obtained in recent decades, including data related to medical images, biological sequences, and protein structures. some successful applications of deep learning in biomedical fields are reviewed in this section and a summary of applications is shown in table 1 ."
"all the aforementioned applications illustrate that as a frontier of machine learning, deep learning has made substantial progress in medical image segmentation and classification. we expect that more clinical trials and systematic medical image analytic applications will emerge to help achieve better performance when applying deep learning in medicine."
"with the focus of more attention and efforts, deep learning has burgeoned in recent years and has been applied broadly in industry. for instance, deep belief networks (dbns) and stacks of restricted boltzmann machines (rbms) [cit] have been applied in speech and image recognition [cit] and natural language processing [cit] . proposed to better mimick animals' perceptions of objects [cit], convolutional neural networks (cnn) have been widely applied in image recognition [cit], image segmentation [cit], video recognition [cit], and natural language processing [cit] . recurrent neural networks (rnns) are another class of anns that exhibit dynamic behavior, with artificial neurons that are associated with time steps [cit] . rnns have become the primary tool for handling sequential data [cit], and have been applied in natural language processing [cit] and handwriting recognition [cit] . later on, variants of aes, including sparse aes, stacked aes (saes), and de-noising aes, have also gained popularity in pre-training deep networks [49, [cit] ."
"note: nn, neural networks; cnn, convolutional nn; sae, stacked auto-encoder; dbn, deep belief network; rnn, recurrent nn. tosa, because these diseases share similar characteristics. through deep learning methods, [cit] successfully built a system to discriminate retina-based diseases only using fundus images. first, a dbn composed of a stack of rbms was designed for feature extraction. then a generalized regression neural network (grnn) was employed to reduce dimensionality. finally, a multi-class svm was used for classification. interestingly, kaggle organized a competition on the staging of diabetic retinopathy from 35,126 training and 53,576 [cit] . using convolutional neural networks, the top model outperformed other machine learning methods with a kappa score of 0.8496 (https://www.kaggle.com/c/diabetic-retinopathydetection/leaderboard)."
"to overcome the scarcity of phenotypic data, genomic and other medical studies have begun to extract phenotypic information from electronic health records (ehrs) to augment existing biorepositories or quickly create new ones. 4, 5 notable efforts include the i2b2 effort led by harvard university and partners healthcare, [cit] the biovu effort led by vanderbilt university, 17 and the multicenter emerge"
"where mean z áj à á is the mean of the jth column of z, and w ij è é are independent and identically distributed bernoulli random variables,"
"thanks to the improvement in computer capabilities and methodologies [cit], anns with efficient backpropagation (bp) facilitated studies on pattern recognition [cit] . in a neural network with bp, classifications were first processed by the ann model, and weights were then modified by evaluating the difference between the predicted and the true class labels. although bp helped to minimize errors through gradient descent, it seemed to work only for certain types of anns [cit] . through improving the steeper gradients with bp, several learning methods were proposed, such as momentum [cit], adaptive learning rate [cit], least-squares methods [cit], quasi-newton methods [cit], and conjugate gradient (cg) [cit] . however, due to the complexity of anns, other simple machine learning algorithms, such as support vector machines (svms) [cit], random forest [cit], and k-nearest neighbors algorithms (k-nn) [cit], gradually overtook anns in popularity (figure 1 )."
"which is equivalently described as the squared error. [cit] s [cit] . however, it often tends to penalize outliers excessively, leading to slower convergence rates [cit] ."
"beside the parameter sharing, rnns are different from other multilayer networks by virtue of having a circuit, which represents hidden-to-hidden recurrence. a simple recurrent network corresponds to the following equation:"
"the input for the phenorm algorithm consists of unlabeled data on a set of potentially informative features, either automatically curated or designed by experts. for the purpose of illustration in a highthroughput phenotyping scenario, we use safe 37 to automatically curate features (listed in the supplementary material). briefly, online articles about the target phenotype from publicly available knowledge sources, such as wikipedia and medscape, are scanned with nlp software to extract medical concepts recorded in the unified medical language system. 40 these concepts are potentially related to the target phenotype. then, narrative notes in the ehr database are processed with nlp software, which identifies mentions of the above medical concepts. we include only positive mentions, ie, mentions confirming the presence of a condition, the performance of a procedure, the prescription of a medication, etc., in all analyses. the patient-level counts of these concept mentions are assembled as candidate feature data. the safe procedure selects a subset of the candidate features via frequency control and repeated fitting of sparse logistic regressions to predict silver-standard labels created from combinations of icd-9-cm diagnosis codes and nlp counts of the target phenotype. features predictive of the silver-standard labels are deemed as informative features for further algorithm training. the nlp analyses used for processing the notes are provided in many out-of-the-box software tools, [cit] and some hospitals and research institutions have their own nlp implementations."
"the proper architecture and objective function should be selected according to data considered. as a type of machine learning, deep learning can also encounter ''overfitting,\" that is, low error on training data but high error on test data. in addition to the regularization terms, other methods for regularization are also important for reducing test error. adding noise to the input or to the weights are efficient regularization strategies [cit], as in the case of a denoising ae [cit] . stopping the optimization early by setting an iteration number is another commonly used strategy to prevent the network from overfitting [cit] . parameter sharing, just like in cnn, can also contribute to regularization [cit] . dropout can force units to independently evolve, and randomly remove portions of units in ann on each iteration, and can therefore achieve better results with inexpensive computation [cit] ."
"the 3d structure of proteins is determined by their comprising amino acid sequence [cit] . however, the computational prediction of 3d protein structure from the 1d sequences remains challenging [cit] . the correct 3d structure of a protein is crucial to its function, and improper structures could lead to a wide range of diseases [cit] . deep learning technologies have shown great capabilities in the area of protein structure prediction, which aims to predict the secondary structure or contact map of a protein."
"where x represents weights of connecting units in the network (the same as in the following context). compared to l 2 parameter regularization, l 1 parameter regularization results in a sparser solution of x and tends to learn small groups of features. l 1 parameter regularization can be defined as"
"in addition to feature extraction, rbms can also learn distributions of unlabeled data as generative models, and classify labeled data as discriminative models (regard the hidden units as labels). similar to aes, rbms can also pre-train parameters for a complex network."
"14 though our results demonstrate the ability of phenorm to provide accurate phenotyping for 4 different diseases in the absence of gold-standard labels, further work is needed to understand the performance of our method across a diverse range of phenotypes, particularly for phenotypes that have more subtle definitions, in which case a combination of phenorm and handcrafted rules or regular expressions might be effective. additionally, while phenorm eliminates the annotation typically required for algorithm estimation, labeled examples are still needed to evaluate the algorithm's accuracy. future research is thus warranted in unsupervised approaches to estimating the roc parameters where the statistical inference is particularly challenging."
"different from other deep learning structures, artificial neurons in convolutional neural networks (cnns) extract features of small portions of input images, which are called receptive fields. this type of feature extraction was inspired by the visual mechanisms in living organisms, where cells in the visual cortex are sensitive to small regions of the visual field [cit] ."
"metamap, a widely used system for identifying medical concepts in the unified language medical system (umls), is used to extract potential concepts from our tweet data set [cit] . given a sentence as input, metamap identifies phrases that could be medical concepts, and maps concepts to a preferred name using umls. however, since metamap is designed to parse clinical documents rather than free text on social media, we consider only those marked phrases that are the same as the preferred name as valid medical concepts. after processing, 1, 340 concepts were extracted by metamap from ade tweets and 3, 921 concepts were extracted from non-ade tweets. concepts are later split into single words."
"(1) what method should be used to pick a neighbor? to answer this question, we fixed the neighborhood size as 15 words, and selected one of the following three methods to choose neighbors:"
"after the above examination of our model, we argue that our model suffers from three main limitations. first, although metamap has been found useful at parsing medical notes, due to the different linguistic use on social media, running metamap on tweets may not identify relevant concepts. second, the use of collocation graph and aggregated medical concept representation reduced precision of models, although the overall recall and f1 improved. additional studies are need to further improve the precision. third, the collocation graph is built solely on the training data set. this may not favor the model when the data set is not representative enough to provide neighborhood of high quality. to address the first two issues, we believe a pre-trained state-of-the-art medication detection system could be helpful to identify high-quality medical concepts from tweets. for the third issue, we plan to use domain based knowledge base such as umls to expand the coverage of the limited data."
"all hyperparameters are jointly trained with a learning rate of 0.001 for ten epochs. in the experiments, we used fasttext pretrained embedding, and the hidden size for lstm is set to be 300. number of multi-head attention layer is set to be 3. for each experiment, the score is taken from the average of five runs."
"in addition to the word-based medical concept embedding described in sec. 3.1, we propose another aggregated medical representation strategy using the collocation information that aggregates the medical concept information in a sentence into a fixed feature space."
"when run against the test set for the shared task, the clapa model achieves a f1 score of 0.5676 (see table 3 ). as a comparison, the average f1 score of systems participating in this task is 0.5019. this shows our clapa model performs significantly better than average on this task."
"in this section, we describe the architecture of our model in detail. the model contains the following three key components -medical collocation embedding, sentence encoder, and max pooling. the overall architecture of our model is shown in figure 1 . for each word, the embedding is composed of two parts, namely, a pre-trained word embedding and an attentive neighborhood embedding. attentive neighborhood embedding is de- rived from the concept-neighbor (c-n ) tensor. in a c-n cube, each n i represents the neighborhood for the i-th concept. based on an attention vector (medattn i ), a concept embedding matrix c is formed in which c i is the embedding for the concept. the collocation embedding for a word w t will be c i if w t is the i-th concept, otherwise, the collocation embedding will be initialized to the zero vector. the concatenated embedding is then fed into an lstm layer, and multi-head attention and maxpooling are applied to extract informative neurons, which are then concatenated with (1) the final state of the lstm (sentence encoding) and (2) the sum of the concept embedding matrix. the final output is then computed via a fully connected neural network with a softmax function. table 1 summarizes the notations used in this paper."
"the challenge for the emoas in the framework is to identify non-dominated (optimised) solutions in each of the regions in the search space. fig. 2 shows the results obtained by combining the outputs of four emoa algorithms (the four algorithms used were nsga2 (non-dominated sorting genetic algorithm 2), paes (pareto archived evolutionary strategy), pesa2 (pareto envelope-based selection algorithm 2) and spea2 (strength pareto evolutionary algorithm 2)). all the algorithms identify process designs near the pareto front of each of the five regions of the search space. this is a strong indicator of the performance of the algorithms and the confidence in the generated designs being near optimal. fig. 2 demonstrates two optimised business process designs (a) and (b), one with 4 tasks and one with 6 tasks (taken from the pareto fronts indicated in fig. 2 ). each of these designs belongs to a different island based on its size. the arrows in fig. 2 indicate the island from where each design in fig. 3 originates. fig.3 . (a) shows a business process design with one of the generic steps missing. the forecasting results are not plotted into a graph but they are just faxed back to the requestor. the framework reduces cost in this instance. therefore, in a semi-automated process the framework can take 'initiative' and alter the generic design provided that the process input and output requirements are still satisfied. fig. 3(b) is composed of 6 services and involves two tasks for obtaining the company's financial data either from selecting one or both (or is not exclusive choice). this provides better confidence in terms of accuracy of the data obtained and improved reliability of the process execution itself."
"as presented in table 3, the model performance is significantly improved with the addition of collocation medical embedding and aggregated embedding, over the attention-based bi-direction lstm models. further, adding aggregated medical information helps improve recall, but reduces the model precision and only slightly increases the f1 score, compared to the collocation based model. hence, while highlighting medical information can reduce false negative decisions, it also causes more instances to be labeled as ade tweets, thereby increasing a false positive rate as well. the clapa model, that integrates both collocation and aggregated representation along with attentive pooling strategy performs the best."
"where r is the combination of the final state of lstm, multiple pooled states using max pooling, and aggregated medical concept representation. each pooled state vector signal l comes from one attention layer (l attention layers in total) that is applied in sentence encoding (eq. 3). u 1, u 2, b 1, and b 2 are parameters to be trained. cross-entropy is used as the loss function for training:"
"next, we analyzed the effect of medical concepts observed in the ade tweets to understand if there is any difference in terms of the use of medical concepts in ade tweets vs. non-ade tweets. we calculated a propensity ratio of each medical term, based on number of times it appears in ade tweets compared to non-ade tweets. we found that causing, gain, drowsiness, and sweats are likely to appear in ade tweets about 15 times more often than in non-ade tweets. similarly, crippled is likely to appear in an ade tweet about 26 times more often than in a non-ade tweet. considering the highly skewed appearance ratio for certain concepts, we analyzed the effect on using concepts from the ade tweets alone. we compared two models -one trained over medical concepts identified from the ade tweets and another trained over concepts from the entire training set, i.e. both ade and non-ade tweets."
"tysabri contains other medical words as neighbors such as infusion, treatment, and gilenya. the collocation graph on the right is for a word, walgreens. it contains few medical words such as cipro and miralax."
"for our experiments, we used the data set provided as part of task 1 of the smm4h 2019 shared tasks . as summarized in table 2, the total number of annotated tweets is 25,678. the data set was randomly split into a training set (80%) and a validation set (20%), while maintaining the target class proportions according to the original distribution. as a result, our training set contains 1,892 tweets that have an ade mention (positive cases), and 18,650 tweets that do not have any mention of ades (negative cases). the validation set contains 485 positive and 4,651 negative tweets. we cleaned the tweets by separating punctuation marks, removing special characters, and replacing mentions, urls, and number representations with normalized tokens. finally, we used fasttext [cit] as the pre-trained word embedding model."
"first, we use an attentive embedding, c i, described in eq. 1, to construct a medical concept representation using the neighborhood information. then, the aggregated representation is constructed, as follows:"
"sdp sft appropriate process patterns so that (i) the process input and output requirements are satisfied and (ii) the attribute values are optimised. results from the real-life scenario, featured in this paper, demonstrate that the optimisation framework can identify business process designs with optimised attribute values."
"in this work, we argue that a collocation graph can be utilized to enrich the representation of a medical concept. we further propose a novel neural network architecture that uses attentive information from a collocation graph to re-embed medical words. our experiments show that, with a good selection of neighborhood, more useful local information can be accessed, which in turn improves the medical concept representation and the overall model performance in detecting mentions of adverse drug events in tweets."
"the first step towards visualising the results is to generate the scenario's search space by producing 1000 random feasible process designs. the initial business process design in fig. 1 involves 5 main steps. a design with less than 5 tasks shows that there is a web service that consolidates two or more tasks. a design with more tasks shows that one step requires two or more web services to be implemented. the search space for this scenario is shown in fig. 2 . the search space consists of five different regions, each corresponding to a group of designs with same number of tasks (4, 5, 6, 7 or 8)."
"in this paper, we propose collocated lstm with attentive pooling and aggregated representation (clapa), a novel approach that integrates bidirectional lstm model with attention and pooling strategy and utilizes the collocation information in the training data set to help enhance the pre-trained word embedding of medical concepts. we show that our model leads to a significant improvement on an ade detection task. to the best of our knowledge, this is the first attempt that utilizes local collocation information to improve the representation of domain concepts in social media."
"to evaluate our model, we set two baselines: an attention-based lstm model (eq. 3), and an attention-based lstm model with max pooling (eq. 4). the results are presented in table 3 as rows (1) and (4), respectively. (5) table 3 : comparison of models on precision, recall, and f1 measures for the ade detection task on the validation set. [cit] smm4h 2019 shared task 1."
"in order to better utilize the medical information embedded in text, we propose two word embedding methods -a pre-trained word embedding, and a second embedding method that enhances the pre-trained representation of medical terms by extracting information around those terms from the collocation graph."
"in the final output layer, the classification decision is made on whether or not a sentence contains an ade mention. a fully connected network module is implemented as:"
the scenario featured in this paper demonstrated how an optimised business process can be automatically created by the optimisation framework using web services. the framework identified the optimal designs for all the available process sizes. the generated designs select and incorporate different web services arranged with the (a)
"having sketched the initial business process design, we can compile the library of alternative web services based on the main steps of the generic process design. the proposed optimisation framework is tested for two objectives; service delivery price (sdp) (specifies the amount of money the service customer has to pay for the consumption of distinct service volumes, i.e. the cost to use the service) and service fulfilment target (sft) (specifies the service provider's promise of effective and seamless delivery of the defined benefits to any authorised service consumer). sft is expressed as the promised maximum number of successful individual service deliveries with respect to the total counts of individual service deliveries."
"to show that our model consistently works better even with smaller training data, we independently and randomly sampled 10%, 30%, 50%, 70%, and 90% data from training set and retrained the models. figure 3 shows that our model consistently performed well on the validation set, even with reduced training size, compared to the baseline model of bidirectional lstm model with attentive pooling (the \"lap\" model). the results are similar to those on the full validation data set in table 3, in that even when only a fraction of training data is available, the model achieves higher f1 score because of significantly better recall and at a relatively small reduction in precision."
"having a robust representation of words is important to train high-performance information extraction approaches. in domain-specific tasks, being able to properly represent domain words or concepts could significantly improve the models. while many studies have undertaken classifications of ade mentions in posts with various state-of-the-art techniques [cit], there is still room to improve for the task. for example, in many trained word embedding models [cit], the embedding of each word is treated as a vector summarizing multiple semantic meanings for each word as independent dimensions. indeed, pre-trained embeddings that are trained on a large data corpus usually provide robust representation for common words, compared to traditional feature-based techniques such as bag of words. yet, for domainspecific tasks, a drawback of pre-trained embeddings is that representations of domain words may not be sufficiently tuned to be able to represent the expected meaning."
"we used fasttext as the pre-trained word embedding for our model. while fasttext is trained on sub-word representations, models trained over medical or larger text corpora might provide additional contextual representation. additional studies are needed to test our model on different pretrained word embeddings such as word2vec over twitter [cit] . we also note that there is a difference in the use of medical related concepts in different classes by testing two scenarios -a model using medical concepts identified from both ade and non-ade cases and one using those from the ade cases. in future, we plan to test this approach by exploring the use of unique nodes in different classes. meanwhile, the application of our approach on other domain-specific tasks should be verified to examine the generalization of the approach."
"motivated by previous studies [cit], the application of max pooling behavior can highlight the important signals from features and hence improve classification tasks. following these previous approaches, we apply a max pooling layer to extract important signals from the attentive hidden state in each attention head (eq. 4)."
"multiple studies have analyzed health forums and other social media for drug uses, pharmacovigilance, and effectiveness of medications [cit] . however, research related to drugs and adverse drug effects (ade) in social media continues to grow rapidly. automatically detecting ade mentions in social media posts has been challenging due to the large variability of free text. one of the main challenges in studying natural language processing (nlp) approaches for medical information extraction is the lack of access to health-related information on social media ."
"figure 2: examples of a collocation graph: tysabri is considered as a medical concept while walgreens is not considered as a medical concept. figure 2 shows the examples of a collocation graph. the graph has two colors: red and grey. the red nodes are words that are identified as medical concepts while the grey nodes are words that are not identified as medical concepts. the collocation graph on the left is for a medical word, tysabri. the neighborhood of the word is comprised of both medical and non-medical words."
"to encode a sentence for the classification task, we used an attention-based lstm to encode the entire sentence into a fixed vector space. l attention heads are applied to re-represent hidden states. the new hidden states from the l-th attention head can be described as follows (eq. 3):"
