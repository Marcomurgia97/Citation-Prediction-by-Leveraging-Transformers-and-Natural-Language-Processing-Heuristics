text
"clinical evidence supports the implications of these enzymes in cancer and many other human diseases, including inflammation, brain disorders, metabolic and cardiovascular diseases, what has attracted considerable interest in the development of selective small molecule inhibitors targeting pkmts [cit] . a parallelism between the current status with pkmt inhibitor design and that for kinases 20 years ago was established [cit], although here the main current challenge is that here some pkmt subfamilies remain unexplored. at sight of reported selectivity profiles for currently available chemical probes and advanced compounds in clinical trials, achieving selectivity within the pkmt family seems a trivial task compared to the situation in the field of kinases, although the high structural conservation of substrate and cofactor binding sites challenges the design of selective inhibitors. nevertheless, selectivity profiling of advanced compounds is expensive and unaffordable for academic groups. here, rationale approaches that incorporate information on ligand recognition and that go beyond traditional sequence-based relationships between targets might be helpful to guide the identification of surrogate ligands for unexplored pkmts and/or to prioritize targets for selectivity screening. in this sense, previous efforts with well-established therapeutic targets include ligand-based organization of gpcrs [cit], protein-ligand interaction fingerprint-based clustering of kinase complexes [cit] and cavity analysis of serine proteases [cit] and epigenetic inhibitors [cit], to mention a few. relationships between different epigenetic families beyond pkmts were recently explored on the basis of the chemical structures of their reported inhibitors [cit] . an inconvenient of these approaches is that they either rely on a vast number of available ligands or are restricted to proteins with crystallographic structures (at least in the apo form for cavity analysis). [cit] that transfers patterns of ligand-residue interactions to sequence-based comparisons of proteins to deorphanize class a gpcrs [cit] ."
"this section is structured as follows. for each binding site (cofactor and substrate), an updated analysis of detected interactions of bound ligands is firstly discussed, with emphasis on novel (un)conserved interaction patterns undisclosed in previous analysis because of the higher number of available crystal structures. second, the results of the pkmt-coinpocket approach are presented, to end up with the validation cases."
"for each set of complexes, we carried out an all-againstall comparison by calculating the pairwise tanimoto coefficient between any two plif values. the r software [cit] was used to cluster and draw the hierarchical tree using the average linkage clustering method (function hclust) and the plif similarity (previous conversion to distance)."
"a detailed update of experimentally detected ligand interactions with set domain containing pkmts at the sam and substrate binding sites is discussed. for the sam binding site, interactions of the n1 and n3 of the adenine ring and hydroxyl groups of the ribose of sam/sah evidence a higher similarity with prmts that initially expected [cit] . this analysis also reveals interaction patterns that are conserved across different subfamilies and that could be exploited to develop selective sam competitive inhibitors. while the analysis of substrate-bound inhibitors is restricted to only 7 pkmts with ligands occupying different sites of the cavity, some important hot spots arise that are shared by different series of inhibitors. to our knowledge, this is the first study that applies protein ligand interaction fingerprints to the study of set domain containing pkmts. interestingly plif approach captures changes in the interaction network of sam/sah molecules with the different pkmts, despite strong binding mode conservation, as evidenced by the low rmsd of superposition of the ligands. as the predictive power of this approach is restricted to proteins for which a crystal structure is available, the novel gpcr-coinpocket methodology was adapted for the entire family of set domain containing proteins. for both sites, the novel organization retains sequence-based relationships, although some interesting unexpected similarities appeared that were confirmed experimentally for a set of dual ehmt2 / dnmt1 inhibitors at the substrate binding site. it must be noted that during our initial selectivity profiling of cm-272 against 8 closely related pkmts, only ehmt1 was identified [cit] . here, kmt5c and the orphan nsd1 were identified as alternative targets for this chemical series. this is especially relevant for nsd1, as it requires deorphanization and these compounds could be used as starting points to develop chemical probes with enhanced potency and selectivity. for the retrospective study cases, it should be noted that because of the lack of massive ligand information, a comprehensive benchmarking study is still unaffordable. three are the main disadvantages of this approach: it is very sensible to the alignment, water contacts were not considered and the presence of many inserts (gaps) in the alignment avoids translation of contact information to all the proteins. moreover, in the current analysis the flexibility of the proteins was not contemplated. in this sense, the incorporation of contact occupancies (and alternative explored contacts) resulting from the analysis of molecular dynamics trajectories could improve the approach. finally, we think that this study provides a useful compilation of available selectivity data for known inhibitors of pkmts at both sites."
"we summarize the results of our laser frequency dependent imaging measurements in fig. 9, where shg images of dermal collagen in an in vitro murine sample are shown that were recorded at different pulse repetition rates. for recording the shg images, the pulse picker performance was optimized at each frequency without adjusting the grating pair compressor or the polarization controller settings. in theory, the second-order nonlinear signal (2pef or shg) amplitude would scale inversely with the repetition rate of the laser system if the average power and the pulse duration are kept constant. in practice, however, we could not measure such an exact dependence due to the imperfect settings of the pulse picker and/or some nonlinear effects in the two-stage yb-amplifier -even when the pulse spectrum of the oscillator was kept constant. in spite of these uncertainties, we dare say that the tendency is clear: the lower the repetition rate of the yb-laser amplifier the higher the measured shg signal. for our laser system, we found an optimum performance at around 1.89 mhz (when the pulse separation is ~0.53 μs), which is well below the thermal time constant of living tissues (i.e., water, see ref. 11 and references therein), which is at around 1 μs, hence we do not have accumulated thermal effects in our biological samples. note that at laser repetition rates below the thermal constant mentioned, i.e. below 1 mhz, not the average laser power but the energy of a single laser pulse determines the local temperature increase in the focal point of the laser beam at fixed pixel dwell time values."
"a comprehensive study of the cofactor binding cavity (without ligands) of 10 set pkmts, 3 classical prmts and the non-set pkmt dot1l [cit] using grid maps [cit] . here, a network of six hydrogen bonds for the cofactor was established as a motif present in all studied set pkmts. our analysis of explicit interactions for the 23 setdomain containing pkmts reveals that most of the 1320 residue-based contacts detected with plif correspond to hydrogen-bond interactions (86%), with a preference for the ligand being hbd (50.6%) versus hba (35.4%). hydrophobic interactions, quantified as the number of surface contacts and arene attraction, account for only 11% of the interactions. these contacts are widely distributed across a set of 30 residues in the alignment (hereafter referred to as plif interacting residues, fig. 2a and details in additional file 1: table s6 )."
"compared to the sam binding site, the number of plif detected interactions between the 33 substrate-competitive inhibitors and the 7 pkmts is much reduced at the substrate binding site, with a total of 265 residue-based contacts, distributed across a total of 24 plif interacting residues (fig. 7a . and additional file 1: table s7 )."
"protein ligand interaction fingerprints (plif), [cit] .0801 [cit] were generated for each complex, using default settings of minimal and maximal contact energies. two different analyses were run for each binding pocket (101 cofactor bound complexes for 23 pkmts and 33 substrate bound complexes covering 7 pkmts). for analysis of the interactions, we counted the number of plif-detected interactions that occur according to its type of interaction: hydrogen bond donor (hbd), hydrogen bond acceptor (hba), ionic, surface contacts and arene attraction (these two last corresponding to hydrophobic interactions). for hbd, hba and ionic interactions, if the ligand establishes multiple interactions with a given residue, the interaction was counted only once and no matter if the contact is established with the side chain or the backbone of the protein."
"there are a few commercial 3d microscope systems presently used for diagnostic purpouses in dermatology. lucid's vivascope is a confocal, handheld 3d mircoscope system that utilizes a cw near infrared laser for imaging through a pinhole for optical sectioning [cit] . due to the low photon energy and the low intensitiy of cw laser radiation applied, it is not suitable for single photon excitation fluorescence (1pef) or nonlinear imaging (2pef, shg). therefore, it does not offer chemical selectivity for the different tissue components. jenlab's dermainspect system [cit] has the advantage of utilizing nonlinear imaging techniques (2pef, shg, cars) for microscopic 3d tissue imaging, however, the extra price that a customer (i.e. a dermatologist) has to pay for chemical selectivity is very high: it is basically the price of a femtosecond pulse tunable ti:sapphire laser applied."
"as summarized for sam in fig. 2b, not all the mentioned six hydrogen bonds are 100% conserved in all 23 set pkmts, especially with respect to the interaction of the amine group of the methionine, and more particularly with residue x at position 545 (gxg), which was detected in only 21% of the complexes and for 10 out the 23 pkmts. ktm5a complexed with sam exemplifies well the conservation of this hb interaction network as well as other hydrophobic interactions (fig. 3a) . in contrast, the prdm9-sah complex exhibits fewer explicit interactions, with the residue at position 559 (gly257) replacing hb contacts at 557, so basically only the hb interactions of the adenine ring with residues at 1362 and 1361 are conserved (fig. 3c) . examples for six additional pkmt complexes, representative of different patterns of interactions, are given in additional file 1: figure s5 . from the viewpoint of non-nucleoside competitive inhibitors design, the two structures of ezh2 complexed with pyridone inhibitors (additional file 1: table s2) suggest that targeting the residue at position 557 of the gxg motif (trp624 in ezh2) is more relevant than blocking the hydrogen-bond network of the adenine ring of sam/ sah with residues nh (1361-1362) of the pseudoknotmotif of the set domain (asn688 and his689 in ezh2) (see individual 2d maps of interactions for the bound ligands, additional file 1: figure s6 ). more interestingly, our analysis of plif interacting residues reveals other highly conserved interactions between sam/ sah cofactors and analogues that might be relevant for the design of sam competitive inhibitors. specifically, we mean the role of the n1 and n3 atoms of the adenine ring and the hydroxyl groups of the ribose ring of sam/sah and analogues. in 14 pkmts, a bidentate hydrogen bond interaction is established between (1) the n1 of the adenine ring and the backbone of non-sequence conserved residue at alignment position 1619 of the c-terminal region and (2) the -nh2 group of the adenine nucleotide and the conserved cys at 1612 (side chain contact) (fig. 3d for kmt5b ). these 14 pkmts (kmt5b, kmt5c, ehmt1, ehmt2, setmar, suv39h2, ash1l, setd2, nsd1, nsd2, nsd3, kmt2a, kmt2c, kmt2d) are representative of different subfamilies in the phylogenetic tree (fig. 1 )."
"the physical length of the whole microscope imaging system is at around ~180 mm, which includes both the scanner unit and the microscope objective. the real challange in designing such a scanning microscope system was the design of the 1:3 imaging telescope between the scanner mirrors and the microscope objective by the use of commercially available lenses. for this work, we applied an optical design software (zemax release 13 sp 2 standard (64bit); zemax llc, redmond wa, usa). the final design of the telescope system is illustrated in fig. 7, where a screenshot from the zemax program is displayed. on the left, the rays start from the scanner mirrors (more precisely, from the half way position betwen the x and y scanner mirrors). the initial beam diameter is 3 mm. on the right, the beams reach the entrance aperture of the microscope objective having a diameter of 8.9 mm. the physical distance between the scanner plane and the entrance aperture is 135 mm, while the microscope objective itself is 42 mm long. (4) is placed in front of the microscope objective. fig. 8 . photo of the laser scanning 2pef/shg imaging system. the optical elements shown in the optical design (see fig. 7 ) are fixed within the metallic tube between the two scanner mirrors (left) and the microscope objective (right)."
"epidermal langerhans cells (lcs) function as professional antigen-presenting cells of the skin. in a recent experiment [cit] we investigated the lc-targeting properties of a special mannose-moiety-coated pathogen-like synthetic nanomedicine dermavir (dv), which is capable to express antigens to induce immune responses and kill hiv-infected cells. in order to visualize the uptake of alexa-labelled dv (af546-dv) by lcs in vivo, we used a commercial lsm 7mp scanning 2pef microscope system of carl zeiss. knock-in mice expressing enhanced green fluorescent protein (egfp) under the control of the langerin gene (cd207) were used to visualize lcs. the (ti:sapphire) operation wavelength was set to 890 nm for efficient excitation of both the egfp and the af546 dye. during our studies we found that after 1 hour af546-dv penetrated the epidermis and entered into the egfp-lcs."
"approach, distances are fully dependent on the set (proteins or binding site) under study, and comparisons cannot be directly translated among different sets. for comparison purposes, the classical gonnet similarity matrix was separately calculated for all residues within 4.5 å of each set of ligands (co-factor and substrate sites, separately)."
"in the followings, we describe our similar all-fiber, all-normal-dispersion yb ring oscillator, in which the polarization controller element is replaced by an electronically controllable device. this simple upgrade in the laser setup, together with some improvement in the laser control electronics and data analysis hardware and software, allowed us to perform a detailed analysis on the mode-locked laser performance as the function of different polarization states of the optical pulses reaching the fiber integrated polarization element. based on such an analysis, one can precisely control the mode-locked operation of an allfiber, all-normal dispersion yb-fiber ring laser."
"during the last couples of years, there have been several attempts aiming for the development of a fiber laser based or fiber delivered nonlinear microendoscope systems [17, [cit] . as the operation laser wavelength reaches that of the yb-fiber lasers at around 1 μm, the risk of photochemical damage due to cpd formation in the dna is negligible [cit] . that is why the main possible source of sample damage due to laser irradiation is of thermal origin in ex vivo or in vivo biological measurements. in order to minimize the thermal load on the sample, there are a few requirements that we list below. first of all, the operation wavelength of the pulsed laser source should be close to the 2p excitation fluorescence maximum of the fluorescent labelling applied or that of the autofluorescence of the biological sample. in order to meet this requirement, we have developed an optical fiber based wavelength conversion unit similar to the one published in ref. 12 (see also the dashed box in fig. 1), however, the power we have obtained so far was too low for nonlinear imaging. therefore, a detailed discussion of this optical fiber based wavelength conversion unit is not provided in this paper. secondly, the laser should deliver the highest peak powers at the lowest average power for the highest signal to noise ratio. to this end, one can use shorter and shorter laser pulses at the expense of higher sensitivity for temporal and spatial dispersive effects. this latter one is an enormous problem in microendoscope systems utilizing grin lenses for focusing the laser beam, since a grin lens itself is not compendsated for chromatic aberrations. furthermore, a grin lens has a limited numerical aperture, which ultimately determines not only the focal spot size and hence the resolution of the microscope, but also the maximum focused intensity of the laser pulses applied and the signal level that can be reached at a certain average power level and pulse duration. from this respect, the best choice is a high numerical aperture, ir corrected microscope objective that focuses a well shaped laser beam onto the sample. the third factor that determines the thermal load of the biological sample is the scanning method applied. for nonlinear microendoscope systems, there are a few possible fiber scanning techniques that have been demonstrated: piezo scanning of the fiber ends in a spiral or a lissajous curve. in the first case, the thermal load on the sample is the highest at the middle of the imaged area, which limits the maximum average power level that can be applied without thermal damage of the sample. in the latter case, the thermal load on the biological sample is nearly uniform over the investigated area. however, data collection and imaging is rather difficult and requires a difficult calibration process at different imaging speeds and areas. one can also apply mems [cit] for beam steering, but the laser beam size and the field of view is typically limited."
"in order to test both imaging channels (\"green\": 500-550 nm, \"red\": 565-610 nm) of our handheld device, we repeated this measurement, the result of which is shown in fig. 11 . we can observe that the langerhans cells, which are a part of the immune system, accumulate the af546-labelled nanoparticles after 1 hour of the topical treatment by nanomedicine of the skin. due to the very low risk of thermal or photochemical damage of the skin samples, we consider our device as a safe, cost efficient, potential candidate for similar clinical applications in dermatology, cosmetology or nanomedicine research. fig. 11 . in vivo penetration measurement of alexa-546 labelled nanoparticles (af546-dv) in murine skin. excitation wavelength: ~1030 nm, z-stack image (128x128 pixels/frame). green: shg signal of collagen. red: fluorescence signal of af546-labelled nanoparticles 1 hour after of the topical treatment by nanomedicine of the skin. one can observe that the langerhans cells, which are a part of the immune system, accumulate the af546-labelled nanoparticles [cit] . scale bar: 50 μm."
"a novel, yb-fiber laser based, handheld 2pef/shg microscope imaging system has been introduced. we demonstrated that the system is suitable for in vivo imaging of murine skin at an average power level as low as 5 mw at 200 khz sampling rate. it has the main advantages of the low price of the fs laser applied, fiber optics flexibility, a relatively small, light-weight scanning and detection head, and a very low risk of thermal or photochemical damage of the skin. owing to the advantageous features described, we regard it as a new useful diagnostic or imaging tool for clinical applications in dermatology or cosmetology."
"the c-terminal segment of set [cit] . this pseudoknotmotif contains the highly conserved nhs/cxxpn motif, where x is any amino acid, and is in close proximity with the loop having the second highly conserved elxf/ydy motif (the last y being the catalytic residue). a third highly conserved motif is the gxg triplet at the n-terminal region. the core set domain forms part of the catalytic domain and is flanked by non-conserved set of regions like the i-set and post-set (cset) domain that form the binding groove for the substrate peptide. the cofactor binds at a different pocket, also partially contributed by the post-set domain and connected by a narrow hydrophobic binding channel to the substrate binding site. depending on the pkmt subfamily, set domains can also be flanked by pre-set, n-set, mynd and ctd domains [cit] ."
"after optimizing our laser and imaging setup described above, we have performed a high number of in vivo shg imaging measurements on the collagen content of living murine skin samples. having in vivo human applications in mind, using this visual information a dermatologist can investigate tumor borders (e.g., in case of basal cell carcinoma [cit] ) or to follow biochemical processes in the human skin, such as the effects of obesity on dermal collagen alterations for instance [cit] . shg image of the collagen distribution in a murine skin shown in fig. 10 was recorded at ~5 mw average power. the pixel dwell time was set to 5 μs, which corresponds to our 200 khz sampling rate. [cit] image processing software of carl zeiss. fig. 10 . in vivo shg image of collagen in a murine skin sample being measured by the use of an yb-fiber laser operating at a 1.89 mhz repetition rate. the laser average power measured directly above the skin is 5 mw. scale bar: 20 μm."
"here, we propose a novel pharmacological organization of pkmts according to the experimental interactions detected in both, the sam and the substrate binding sites, by using an adaptation of the gpcr-coinpocket methodology for the analysis of pkmts (hereafter referred to as pkmt-coinpocket). unexpected similarities between pkmts emerged from the resulting family arrangements of the separate analysis of both sites that were retrospective and prospectively validated, leading to the identification of three hits targeting the orphan nsd1."
"as for the sam-binding cavity, a preference for polar contacts over hydrophobic interactions is observed, highlighting the electronegative character of the substratebinding groove [cit] . of note, there is a preference for the ligand acting as hbd instead of hba: 53.6 versus 10.6%. hydrophobic interactions, quantified as the number of surface contacts and arene attraction sum up a total of fig. 7 a percentage of substrate-competitive inhibitors that interact with a given plif interacting residue according to interaction type. b-h mapping of plif interacting residues for each of the seven pkmts. red balls correspond to residues for which at least one interaction was detected for all ligands (in orange) of this protein and grey balls to any of the remaining 24 different plif interacting residues (if conserved for the given pkmt at the corresponding positioning of the alignment) and for which no interaction was detected for any of the ligands of this pkmt. note that because of structural protein differences in the alignment not all plif interacting residues could be mapped onto each protein. for reference, sah is shown in blue 33.2% of the interactions (27.5 and 5.7%, respectively) and explicit ionic interactions contribute to only 2.6% of the contacts. next, we inspected the degree of conservation of plif interacting residues across the 7 pkmts and which amino acids contribute to specific contacts."
"chembl [cit] database was queried [cit] to retrieve ligands with inhibitory activity against any of the different pkmts in additional file 1: table s1 . only data for human proteins and activities given as ic 50, inhibition, k d or k i were retained. a total of 1712 data points were retrieved, associated to 908 different compounds. however, only for 43 out of the 908 compounds, there is available data on more than one single target. allosteric compounds, compounds without a well-defined binding site (e.g. chaetocin and derivatives potentially binding to cysteine-rich regions) [cit], inactive compounds against all pkmts and compounds already in additional file 1: table s2 or their very close analogues with similar selectivity profiles were discarded. after this refining, only the substrate-competitive inhibitor cyproheptadine and the sam-competitive ezh2 inhibitor gsk343 remained. lastly, in order to enlarge the data set, 5 additional inhibitors with selectivity profiles were rescued from literature [cit] . the chemical structures and biochemical profiles of these 7 structures are given in additional file 1: table s5 ."
"the matrix of normalized pkmt-coinpocket scores for all pairwise comparisons of sequences in the alignment was converted into distances (1-corresponding similarity score) and used to cluster pkmts using the unweighted pair group method with arithmetic mean (upgma) algorithm [cit], as implement in r [cit] . dendrograms obtained from clustering were saved as newick files and trees were obtained with itol [cit] ."
"there are two additional issues that should be mentioned before using the method introduced for computer or microcontroller control of an all-fiber, all-normal dispersion ybfiber ring oscillator for everyday use. firstly, the yb-fiber laser tested (and the corresponding stability map) had some thermal drift according to the temperature change in the laboratory, even if all of the optical components and fibers were fixed in the housing. this is due to the temperature dependent, mechanical stress induced birefringence in the sm fiber components. when using such components, this fact requires a continuous, fine re-adjustment of the polarization control voltages during the operation of the laser in order to keep the highest signal to noise ratios. this unfavourable effect, however, can be minimized by applying pm fiber optic components where applicable. secondly, the all-fiber, all-normal dispersion ybfiber ring oscillator reported here had some inherent nonlinearity, which resulted in an optically bistable behaviour [cit], as demonstrated in fig. 6 . for recording the \"stability maps\" shown in fig. 6, the quarter-wave plate control voltage values (horizontal axis) were increased and reduced for the even and odd rows, respectively. it is clearly seen that the cw mode-locked status of the laser had a feedback to the stability maps (the stable/unstable borders were slightly shifted), which fact arouses additional challenges when writing a computer code for laser control. regarding the tuning range of the fiber laser system we can say that the yb-oscillator can be tuned from 1025 nm to 1042 nm by the use of the polarization controller, which might limit the application of the current laser setup compared to that of utilizing ti:sapphire lasers. however, fiber laser technology can offer a solution to this problem: properly designed, highly nonlinear photonic crystal fibers can convert light from one wavelength to another one by nonlinear processes such as fiber optical parametric frequency conversion [cit] . to this end we have made some numerical simulations and preliminary experiments using our current laser parameters. we found that our 2 mhz, ~1030 nm laser output can be converted to shorter wavelengths depending on dispersion parameters of the photonic crystal fiber. the efficiency of this nonlinear frequency conversion process is at around 5%, higher conversion rates at around 10% could be obtained only at the expense of lower pulse quality. at the moment, the average power of the frequency converted laser output is too low for nonlinear imaging. however, it can be improved by increasing the average power of our yb-amplifier, i.e. by building an additional amplifier stage. work is in progress (see fig. 1, frequency conversion in pcf) to build an all fiber laser system suitable for cars microscopy [cit] and exciting different fluorophores such as nadh or egfp at around ~780 nm and ~900 nm. an alternative solution to extend the operation range of fiber lasers into the red is frequencydoubling of a femtosecond er-doped fiber laser [cit] for instance, but the low temporal quality of the laser pulses reported in ref. 17 arouses laser safety concerns in this latter case. in both cases, dispersive properties of optical fibers [cit] play an important role and can improve laser performance."
"the synthesis and biological activity of the three proprietary compounds cm-272, cm-679 and cm-986 has been reported [cit] . chemical structures and ic 50 values against ehmt2 (g9a) and dnmt1 are given in fig. 12a . inhibitory assays against setd2, kmt2a, kmt5c, nsd1 and nsd2 were performed by eurofins (https :// www.eurofi ns.com/) with radioligand binding assays ([3h] sam and different substrates: nucleosome (setd2, nsd1, kmt5c), core histone (nsd2) and histone h3 full length (kmt2a) at 10 µm and 100 µm and tested in duplicates."
"as for the sam binding site, pkmt-coinpocket similarity heat map (fig. 10 ) and its derived tree (fig. 11) arrange some set-domain containing pkmts outside of their subfamily, while most relationships are conserved (e.g. for the smyd subfamily). for example, prdm2 and prdm6 shift away from the other prdms and toward members of the suvar3-9 subfamily (suv39h2, suv39h1, ehmt2 and ehmt1). interestingly, all of them (except for prdm6), share h3k9 as histone target. setmar groups with members of the setd2 subfamily (setd2, nsds and ash1l), all of them having in common h3k36 as substrate. analogously, for the case of setdb2 regrouped with h3k9-targeting prdms mecom and prdm16. this does not mean that pkmtcoinpocket yields pkmt arrangement according to the histone substrate (fig. 11), but simply suggests that the new organization has sense from the view point of common substrates. compared to the sam binding site, the greater z-score of the substrate binding site (3.54 against 2.97, respectively in figs. 5 and 10) emphasizes the greater diversity of this last pocket (as also observed when comparing the similarity matrix of both sites, additional file 1: figures s15 and s16 for the co-factor and substrate sites, respectively)."
"here, the calculation of pkmt-substrate-competitive ligand basilico contact strengths resulted in the identification of a 'cloud' of 75 residue positions that outline the i-set, nhs/cxxpn pseudoknotmotif and the post-set domain (fig. 9) . again, this cloud of 75 interacting residues detected by basilico is much higher than the 24 plif interacting residues because of differences in contact definitions, although, again, main hot spots identified by both approaches and discussed above are mostly in common (e.g. 1124, 1130, 1148, 1151 and 1473)."
"we tested the handheld nonlinear microscope imaging system for different in vitro and vivo murine skin samples. for our 2pef and shg in vivo measurements, first we optimized the repetition rate of our laser system for the highest nonlinear signal before reaching the low frequency limit of any thermal or photochemical damage of the skin samples. then, we used the optimized setup for (one channel) collagen measurement of in vivo murine skin samples. finally, we used a two-channel setup for in vivo monitoring of penetration of fluorescent labelled nanomedicine into the skin for a few hours."
"where i and j are amino acids at a given position of the alignment between two sequences and m is the gonnet [cit] residue comparison matrix. this calculation was obtained with moe using a customized svl script that writes a csv file that stores, per each residue in the reference sequence, its non-normalized gonnet coefficient against every other sequence in the alignment. these csv files were processed with pipeline pilot to carry out pkmt-coinpocket calculation. here, the per-residue gonnet similarities (s ij ) between a couple of sequences seq1 and seq2 in the alignment were multiplied, elementwise by the projected binding site positional fingerprint vector obtained for each of the proteins for which contact strengths were calculated (i.e. proteins for which there is crystallographic information available of protein-ligand complexes). then, gonnet pfp_l, the similarity between seq1 and seq2 for a given projected binding site positional fingerprint vector of protein l was calculated as the sum of this vector over all residues. note that due to the different lengths of the pkmts (and alignment gaps), not all contact residues could be propagated. then, gonnet pfp_l values were standardized into z-scores for each bindingsite fingerprint and the final profiled pkmt-coinpocket score for a pair of proteins seq1 and seq2 in the alignment was calculated as the average of the z-scores across the different binding profiles (23 and 7 for cofactor-and substrate-binding sites, respectively). finally, to convert pkmt-coinpocket scores to distances, the respective value was first normalized to be in the range of 0 to 1. a pkmt-coinpocket score was calculated for each binding site. due to the nature of the pkmt-coinpocket"
"nonlinear microscopy, such as two-photon excitation fluorescence microscopy (2pef) and second-harmonic generation (shg) microscopy is increasingly used to perform in vivo studies in life sciences, for example in dermatology [cit] . these techniques enable us to investigate the morphology and monitor the physiological processes in the skin by the use of femtosecond pulse lasers (such as a broadly tunable ti-sapphire laser [cit] ) operating in the near-infrared spectral range (680-1060 nm). recent years brought revolutionary progress in the development of femtosecond pulse, all-fiber laser oscillators [cit] and amplifiers being suitable for nonlinear microscopy. fiber lasers are of great interest not only because of their considerably lower prices but also because they can easily be combined with endoscopy [cit] . this latter feature greatly increases the utility of nonlinear microscopy for pre-clinical applications and tissue imaging."
"according to the design issues listed above and discussed in ref. 11 in details for minimum thermal load on the biological samples, our handheld scanning 2pef/shg imaging head comprises an x-y galvano scanner unit designed for laser beams of ~2.3 mm in diameter (cambridge technologies), and a 1:3 telescope system custom designed for an ec planneofluar 40x/0.75 objective (carl zeiss, germany). this objective is optimized for a cover glass having a physical thickness of 0.17 mm. the use of (a replaceable) cover glass in our imaging system assures in longer term that the skin of each patient can be investigated by a medical device that does not carry any infection from previous patients. additionally, skin position can be fixed to the cover glass by a properly designed vacuum system that should be necessary for high resolution, 3d pathological measurements, which takes a few minutes even at a 200 khz sampling rate."
"in fig. 8, a photo of the laser scanning 2pef/shg imaging system is shown. the 2pef/shg signal is detected by two small size pmt detectors (hamamatsu, japan) supplied with properly chosen dichroic and bandpass filters. the analogous electronic signals of the pmt detectors are amplified and digitized by some home-built electronics."
"in the followings, we introduce our novel, handheld nonlinear microscope system comprising a ~2 mhz repetition rate yb-fiber laser as a pulsed light source for nonlinear imaging. the system has the main advantages of the lower price of the fs laser applied, fiber optics flexibility, a relatively small, light-weight scanning and detection head, and a very low risk of thermal or photochemical damage of the skin. in order to mention some possible applications in dermatology, we show that our novel microscope system is capable of high quality, in vivo shg imaging of the collagen content of murine skin at average power levels as low as ~5 mw. among others, using shg imaging of the collagen, one can investigate basal cell carcinoma [cit] or follow the effects of obesity on dermal collagen alterations [cit] for instance. for demonstration purposes in cosmetology and nanomedicine, we also used our two-channel 2pef/shg imaging system for in vivo visualization and for monitoring the uptake of alexa fluor 546 labelled nanomedicine by langerhans cells [cit] for a few hours."
"the fiber integrated, programmable pulse picker unit with controller electronics requires ttl level synchronization input signals, which can be easily generated from the electronic signal of the fast photodetector (pd) by using some home built comparator electronics. the twostage yb-amplifier is pumped by wavelength stabilized 976 nm laser diodes similar to those used to build the yb-oscillator with cw output powers up to 500 mw. for seeding the first amplifier stage, we used the polarized pbs output of our oscillator. pump powers of the laser diodes and lengths of the yb-doped fibers were optimized for the ~1.89 mhz repetition rate of the seed pulses, since the ~10 mw average output power of the yb-oscillator operating at 36.4 mhz was reduced by a factor ~20 by the pulse picker. after this optimization process, the two-stage yb-amplifier delivered ~15 ps long optical pulses at around 1030 nm at ~1.89 mhz repetition rate with an average power of ~200 mw without observable ase content. the positively chirped output pulses were compressed by a transmission grating compressor with grating separation of ~80 mm resulting in sub-500 fs long, ~50 nj pulses for nonlinear imaging."
the tanimoto pairwise similarity for the 30 substratecompetitive inhibitors in additional file 1: table s2 was calculated using fcfp-4 [cit] as implemented in pipeline pilot [cit] .
"in probabilistic anti-collision schemes, to estimate the number of present tags, binomial distribution has been utilised [cit] . for a given initial q in a frame with f slots and n tags, the expected value of the number of slots with occupancy number x is as follows:"
"there are linkedin job offer pages crawled in this experiment and important information related to a particular job offer is extracted. job offer pages need to be crawled periodically since they are updated and outdated. user interface for job offer search is available at http://try.ui.sav.sk:7070/2012-01-07/browse. more detailed information about one of our test crawls for linkedin is available in table 3 . there are 70 116 job offers of overall 113 268 linkedin documents fetched, which is about 62%. doc. ratio represents the percentage of all fetched documents, where a particular entity occurs at least one time. the percentage marked with an asterisk is computed from the total number of job offer pages since the corresponding entity is extracted only from job offer pages. the system extracts non-spatial information such as jobtitle, jobtype, jobfunction, company, industry, skill, experience, posteddate and spatial information like joblocation, city and country. city and country entities are extracted by the gazetteer from the textual content only (just like it is in the bbc news task), while the joblocation is extracted by traversing the html dom tree of a job offer page and looking for its div element. city and country entities are geocoded directly, while the joblocation entities need to be treated differently, because of their content. joblocations are in a free form text, always beginning with a company name followed by a location of the job. there is no strict format for the location. it can be anything that user writes down, for example \"anywhere\" as it has been seen many times in offers. below are some concrete examples of such joblocations:"
"there is also a graph search user interface available, which allows users to search within a graph built from extracted entities. a spread activation algorithm is applied during the graph search."
"various information is extracted from the textual content of crawled web documents as well as from the html dom objects. a built-in nutch html-parser plug-in is modified to produce formatted output of the textual content found in a web document. this parser tries to preserve visual formatting of the source html page in the output text, so this feature can be exploited in the text segmentation and better information extraction. simple gazetteers, regular expression patterns and a combination of both approaches are used for information extraction. there are several types of nes (named entities) are extracted. the entities listed below were extracted from the linkedin job offers:"
"an important aspect of the web-scale search service is its scalability. it can be fulfilled by distributed architecture [cit], which can process a large amount of data in parallel. our search service is built on top of apache hadoop [cit] ."
"generic nes are used also for creating jobskill gazetteers. this is done by taking nes extracted from the \"desired skills & experience\" part of job offer and then filtering out those with the lowest frequency. finally a gazetteer list is built and applied in the next crawl cycle. since there is no strict job offer structure required by linkedin, some job offers have their own structure, but most of the job offers contain recommended parts like the mentioned one."
"in this study, we have chosen ptes approach to be used as backlog estimation tool in our newly proposed probabilistic cluster-based technique. ptes method is easy to implement with low overhead computation and provides accurate tag estimation. this approach has the best performance in terms of system efficiency, and the number of frames and slots queried by the algorithm. the optimal efficiency of ptes can be obtained when the suitable parameters have been chosen for the specified initial frame-size."
"schoute [cit] developed a backlog estimation technique for dynamic framed-slotted aloha using poisson distribution. the backlog, after the current frame bt, is given by equation:"
"rfid system may only consist of a tag and a reader but a complete rfid structure involves many other components and software such as computer, network, internet, middleware, and user applications. rfid reader retrieves information from tags and sends that information back to host computer via middleware. rfid data streams that are captured by readers can be accumulated very fast, resulting in data collisions. while there are approaches previously proposed in literature, based on identifying and rectifying the missing observational data after it has been stored within the database [cit], it is crucial that the rfid system must employ anticollision protocols in readers in order to enhance the integrity of captured data."
"several methodologies have been proposed to minimise collision issues in rfid system. the two types of tag anti-collision methods widely used are the deterministic and the probabilistic approaches. deterministic methods perform the tag identification by transmitting a query with specific prefixes, and the tags respond with their id. this procedure leads to lengthy queries issued by the reader and causes identification delay. on the other hand, probabilistic methods decrease the probability of collision by scheduling the responses of tags at random time. this process leads to tag starvation problems as not all tags can be identified after the long waiting period of time."
"in a probabilistic approach, tags respond at randomly generated times. if a collision occurs, colliding tags will have to identify themselves again after waiting a random period of time. this technique is faster than deterministic based approach but suffers from tag starvation problem where not all tags can be identified due to the random nature of chosen time. when we mentioned the probabilistic anti-collision approach in rfid, we usually refer to the alohabased approach, which is the most widely used type of anti-collision. slotted aloha [cit], which initiates discrete time-slots for tags to be identified by reader at the specific time, was first employed as an anti-collision method in an early days of rfid technology. [cit] s [cit], where each tag is identified randomly. to improve the performance and throughput rate, different anti-collision schemes were suggested in the past literature. the three most accepted techniques, basic framed-slotted aloha, dynamic framed-slotted aloha, and enhanced dynamic framed-slotted aloha, are described in the next section."
"where * represents temporal convolution and h rr (t) and h vr (t) are time-varying linear filters. the proposed methodology utilizes measurements of x qt (t) and x rr (t) to estimate the time-frequency (tf) distribution of x qt:vr (t). the following formulation is an extension to the tf domain of classical signal processing theory describing multiple inputs/single output relationships [cit] . assuming that inputs x rr (t) and x rv (t) are locally uncorrelated, the transfer function h rr (t, f ), which describes h rr (t) in the tf domain, is [cit] :"
"but there is google places autocomplete service [cit], which can complete the address of some establishment, which we decided to use in the geocoding process. it takes establishment name, latitude, longitude and radius as input parameters. as the establishment, we put the company name and for latitude/longitude we put a geocoded sub-location string by the google geocoding service. google places autocomplete service returns up to 5 results -complete addresses matched for the company near the specified latitude/longitude. each result is then geocoded and its distance from the reference point is computed in order to filter distant results, which might be irrelevant. in addition a success probability for each geocoding service result is computed. the computation is based on the service return values \"location type\" and \"partial match\", which indicate the geocoding success. then, latitude/longitude coordinates of the result with the highest probability are picked as a job location and stored in the index."
"where s x (t, f ) represents the tf spectrum of a given process x, and γ qt,rr (t, f ) is the coherence function between x qt (t) and x rr (t). these expressions are used to estimate the spectrum of x qt:vr (t):"
"as it is stated earlier in the text, there is only linkedin website [cit] crawled for job offers currently. in the future we want to crawl and parse also monster website [cit] to retrieve job offers and user cvs. linkedin claims it offers roughly 80 000 job offers (79 871 at the time of making our tests -october 10, 2011). there are 70 116 job offers crawled in our database with an overall size of 2.1 gb crawled/parsed data and around 740 mb metadata with the index (includes also textual content)."
"after the cleaning and trimming process a set of sub-location strings and company names is almost ready for geocoding. but most of the sub-location strings cannot be yet precisely geocoded because they contain only city and country information (rarely, there is also a zip code). the geocoding service would return coordinates in the middle of the cities or countries, which is not sufficient. to get more precise geocoding results, one needs to make the location more specific, but this cannot be done simply by specifying the company name in the query because geocoding services do not recognize business names."
"1. successful slot: where there is only one tag reply, the reader sends ack(rn16) to a tag. the tag then backscatter its epc to the reader and the reader issues queryrep for the next slot. 2. empty slot: where there is no tag reply, the reader then issues queryrep for the next slot. 3. collision slot: where there is more than one tag reply, the reader then issues queryrep for the next slot."
"with initial frame-size of 256 has a better performance than the pct128 with initial frame-size of 128. table 11 shows that there is no improvement of our proposed methods compared to existing methods when the number of tags are low (up to around 300 tags). this is because pct methods start dividing tags into groups only when the number of tags reach the specific threshold. as a result, for certain tag sizes, the number of slots and performance efficiency remained unchanged due to the same identification procedure, compared with dfsa and edfsa methods. moreover, table 11 demonstrates the pct128 as the only method that has different results when the number of tags are 100 and 200 tags. this is due to the fact that pct128 is the only method that uses initial frame-size of 128 to predict backlog. therefore, even the number of tags still low, the pct128 start splitting tags into group resulting in different tag outcomes. furthermore, when the number of tags is 100 tags, pct128 shows the minimal number of slots issues. the reason for the outcome is because pct128 uses frame-size of 7 instead of 8 like other methods. thus, when the number of tags are as low as 100 tags, the pct128 perform the best. however, we can see from figure 4 that the performance efficiency of pct128 stabilised and does not improve any further when the number of tags increased. table 11 and figure 4 show that both pct256 and pct-e maintained its system efficiency above other methods and has the most stable performance. nevertheless, the pct-e required additional number of group sets from the pct256 method throughout the identification process (see table 6 and table 10 ). as a result, the pct-e required extra time to initiate a new group compared with the pct256 method. on the other hand, the dfsa's efficiency dropped dramatically when the number of tags increase, while the edfsa's efficiency become unstable during the time when number of groups doubled up from 1 to 2 and from 2 to 4. the pct128 has steady performance but does not perform as good as pct256. table 12 demonstrates the percentage of improvement of the proposed pct method versus edfsa and dfsa methods. it can be seen that when the number of tags are low and the pct methods have not divided these tags into groups, there is no difference for the outcome of our methods and existing methods, and the percentage of improvement remain unchanged. however, during the time when number of groups doubledup from 1 to 2 (400 tags) and from 2 to 4 (800 tags), pct256 and pct-e show the highest percentage of improvement compared with the edfsa method. the percentage of improvement increased more stably compared with dfsa method, since the dfsa method does not imply group splitting rules. the pct256 has a better performance than the edfsa by about 4 percent on average, while it is approximately 11 percent better than the dfsa approach as demonstrated in figure 5 . the optimal percentage of improvement of pct256 method can achieve up to 14 percent and 21 percent compared with the edfsa and dfsa respectively, depending on the number of tags within the interrogation zone. nevertheless, the pct-e method required additional number of groups from the pct method and acquired slightly lower percentage of improvement compared with the pct method. on the other hand, the pct128 has a better performance than the dfsa method by around 6 percent on average, but does not show any improvement from the edfsa technique, as displayed in table 12 . however, the pct128 still shows some improvement in some cases and able to achieve up to 16 percent compared with the edfsa and dfsa methods. therefore, we conclude that our proposed pct256 method is the most effective method in terms of system efficiency and number of slots minimisation."
"it has been proven that the highest efficiency can be obtained if the frame-size f is equal to the number of tags n, provided that all slots have the same fixed length [cit] :"
"the benefits of using the mapreduce architecture are discussed in our prior work [cit], where we compare its performance to a single machine solution. there are two different applications compared with the result of 1.9 and 12 times performance gain after executing them on a hadoop cluster."
reader sends queryadjust; end algorithm 2: probabilistic anti-collision algorithm with ptes frame-size prediction according to the optimal system efficiency obtained for specific number of tags. we first conducted an experiment to acquire optimal frame-size for specific number of tags as shown in figure 2 . it can be seen that the optimal system efficiency achieved by the probabilistic method is approximately 38% and the optimal number of tags are close to the maximum frame-size. efficiency is calculated as shown in equation 1:
"from the results and analysis, we have found that both our proposed pct256 and pct-e methods have produced a minimal number of slots when compared with existing state-of-the-art approaches. specifically, the pct256 displays higher performance efficiency than the edfsa by about 4 percent on average, while it is approximately 11 percent better than the dfsa approach. the pct-e method required an additional number of groups and acquired a slightly lower percentage of improvement compared with the pct256 configuration. through our empirical evaluation, we have concluded that our pct256 method is the most reliable method. it has maintained its system efficiency above other existing state-of-the-art approaches and has the highest achieving performance."
"the remainder of this paper is organised as follows: in section 2, we provide general background information related to tag collisions and different anti-collision schemes. section 3 contains discussion of the different probabilistic anti-collision methods along with backlog estimation techniques and their limitations. we present our new methodology, the probabilistic cluster-based technique in section 4. in section 5, we present our experimental evaluation, results and analysis; and finally, we conclude the paper in section 6."
"the basic framed-slotted aloha (bfsa) is the most basic of all the algorithms that use a fixed frame-size throughout the identification round. the reader offers information to the tags including the frame-size specification and the random number selected by each slot within the frame. each tag selects a slot using the random number and then sends its id back to the reader [cit] . since the frame-size of the bfsa is fixed, its implementation is simplistic. however, the system's efficiency drops significantly in the event of there being too large or too small tag counts. for instance, no tag may be identified in a read cycle if there are too many tags within the interrogation zone. on the other hand, under small tag counts where large frame-size is used, lots of empty slots are produced, resulting in decreased system efficiency."
"a new method to track changes in qtv unrelated to rrv during non-stationary conditions is proposed. the results of the simulation study demonstrates that it provides accurate estimates of qtv unrelated to rrv, with correlation with theoretical variability higher than 0.88. the analysis of data from healthy volunteers shows that qtv unrelated to rrv increased during tilt table test. this methodology extends classical multiple inputs/single output theory [cit] to the tf domain to study the interactions between qtv and rrv. cohen's class tf distributions and tf coherence function [cit] are utilized to separate qtv into two partial tf spectra [cit], representing qtv related and unrelated to rrv. to the best of our knowledge, this is the first work presenting a non-stationary model-free methodology to disentangle changes in rrv related and rrv unrelated qtv. in this framework, qtv unrelated to rrv is assumed to be due to ventricular repolarization (vr) dynamics, and the two inputs, rr and vr variability, are assumed to be uncorrelated. this assumption is based on the fact that rr and vr variability have different anatomical origins, i.e. the sinus node and the ventricular myocardium. however, common drivers cannot be excluded since respiratory and low-frequency oscillations have been observed in human vr dynamics [cit] and both rrv and qtv are known to be affected by sympathetic modulation [cit] . of note, this methodology allows x qt:vr (t) and x qt:rr (t) to have components in the same tf regions, since their power is assigned from qtv proportionally to the squared magnitude of the tf coherence function. the framework used in the simulation study tested the proposed methodology in a challenging situation: signals were am-fm random processes with quick changes in instantaneous frequency and amplitude, and vr dynamics was a white gaussian noise with fast amplitude modulation. nevertheless, correlation between theoretical and estimated instantaneous qt:rv power was very high. however, since tf coherence is a biased estimate [cit] the estimates were also slightly biased (see fig. 2d ). these promising results may be the basis for further study to assess the role of this methodology as a marker of ventricular sympathetic activity and cardiac instability."
"bayesian method [cit] first computes the frame size l based on the current probability distribution of the random variable n that represents the number of tags transmitting. then it starts frame with l slots and waits for tag replies and update probability distribution of n, based on evidence from the reader at the end of the frame. the evidence comprises the number of empty slots, successful slots, and collision slots in the last frame. the method then adjusts probability distribution n by considering newly arriving tags and departing tags including the ones which successfully replied and do not transmit in subsequent slots. bayesian method requires the most complex computation and implementation of algorithm [cit] . this resulted in high overhead and delays the identification process. it is therefore not widely applied in rfid systems."
"fetched web documents are indexed by all extracted entities described in chapter 3. these entities are used inside the lucene index as fields (following the key/value sense). if there are multiple entities of the same type but with different values extracted, they are all put into the index, because multi-valued fields are supported in nutch (since version 1.2)."
"rfid technology is an automated wireless technology that uses radio frequency waves to track items. it has the potential to improve the efficiency of business processes by providing automatic identification and data capture. in modern day, there are various applications where rfid technology is integrated such as warehouse and supply chain monitoring. in those applications where numerous rfid tags are presented in the interrogation zone simultaneously, the rfid reader is required to have an ability to read data from individual tags. if more than one tag tries to communicate to the reader at the same time, a collision occurs and the tag will need to be re-transmitted. a technical approach that handles tag collision without any interference is called an anti-collision scheme."
rfid technology is an automatic identification technology that identifies electronic tags attached to items without the need for a direct line of sight between the reader and the tag. there are several methods of identification but the most common is to store data containing the tag's identifier known as epc (electronic product code).
"vogt method presents the most accurate tag estimation, however, the complexity of the algorithm resulted in high overhead and therefore cannot be applied to gen2 protocol [cit] ."
"the concept of indexing by spatial data is very important. if talking about web documents, there are two general approaches to indexing by geographic coordinates. the first is to index each document by one geographic location and the second is to index each document by multiple geographic locations. the advantage of the first method is in straightforward indexing and searching implementation, where each document in the index has been assigned a pair of latitude/longitude coordinates. therefore, searching is a simple question whether a tested document's latitude/longitude coordinates are within a specified range. the disadvantage is that only one geographic location can be assigned to each document in the index. the second approach is more suitable for indexing web documents, because it is natural that one document could refer to more geographic locations and it is expected then to index such document by all of them. for instance a news article, which informs about explosions in two different cities or a job offer, where several positions on different places can be announced."
"the deterministic approach starts by asking for the first binary bit of the tag until it matches the tags; then it continues to ask for additional characters until all tags within the region are found. this approach is slow and introduces an identification delay but leads to fewer collisions, and has 100 percent successful identification rate [cit] ."
"we must mention that we did not start from scratch in this work, but we exploited several tools developed in the scope of the nazou project [cit] (e.g. ridar and erid) [cit] ."
"the cardiovascular response to orthostatic challenge was studied in 16 healthy volunteers (aged 29 ± 3 years) using a tilt table test. the protocol included early supine (es) position (4 min), head-up tilt to an angle of 70"
"in our previous work we have showed a suitable indexing (as well as searching) approach which uses an htm (hierarchical triangular mesh) [cit] method for indexing geo-locations on the earth's surface. we integrated it and tested for nutch 0.9 [cit] . in this work, we use the same spatial indexing approach and integrated it in nutch 1.3 and solr 3.1. there is an htm id computed for each geocoded geo-entity (represented by latitude and longitude) and stored in the \"geohash\" index field of particular document. the geohash field is then used in spatial searches."
"in this study, we propose a new method called a probabilistic cluster-based technique (pct), to minimise the total number of slots and frames queried during the tag identification process and to maximise the system efficiency. in order to show the significance of our proposed methods, we conducted an experimental evaluation and compared our methods to the existing techniques. the results from our experimentation studies indicate that the proposed pct256 method maintained its system efficiency above other existing methods and has the highest achieving performance."
"due to the fact that web content is very diverse, building an intelligent web-scale search service is still a challenge and faces plenty of problems. there are many heterogeneous sources of information in different languages and there are many different formats of information representation too. according to latest surveys [cit], there are about 65% of top 1 million websites using xhtml markup language, while the rest use html. the trend of xhtml usage is slightly growing, but on the other hand, semantic standards like rdf, grddl, rdfa, sparql, owl, rif or skos occur sporadically in xhtml. in general, semantic web solutions based on mentioned standards cannot be yet applied. therefore, there is still a need to deal with the information extraction and semantic analysis of crawled web documents to support intelligent search. we propose an architecture for distributed large-scale information processing for the intelligent web search task in the following chapter."
"the first indexing approach is currently implemented in apache lucene [cit] (lucene is an indexing base for solr and nutch), but there are also other methods investigated in lucene, which follows the second approach [cit] . to be more precise, there is cartesiantier concept, which has been abandoned and locallucene, which is still under development. the methods of the second approach exploit hash functions to encode latitude/longitude coordinates into a single string, which gives an ability to store coordinates in a multi-valued index field and to attach multiple geo-locations to one document."
"in aloha-based anti-collision, there are three kinds of slot as shown in figure 1 : 1) empty slot where there is no tag reply; 2) successful slot where there is only one tag reply (tag 4); and 3) collision slot where there is more than one tag reply (tag 2 and tag 3)."
"the aim of this study is to propose a novel framework for estimating qtv unrelated to rrv during non-stationary conditions, that enables to specifically track changes in vr dynamics."
"pct approach derived new rules using particular equations expressed by β beta, κ kappa, and µ mu. all rules split the number of backlog into groups then used one of q8 (frame-size 256), q7 (frame-size 128), or q6 (framesize 64), to identify a current set of tags. we make the assumption that the performance efficiency can be improved by dividing tags into accurate number of groups, then perform the tag identification separately for each group. in this research, we have chosen the frame-size of 256, 128 and 64 for our pct rules because the initial q of 8, 7 and 6 provides the most appropriate range for current rfid reader and passive tags specification. generally, the ultra-high frequency (uhf) reader is capable of capturing various number of passive rfid tags that depends on the reader type and tag class (e.g. class 0: read-only tag). thus, selected initial qs are the most suitable for our proposed rules. each pct rule, with the minimum and maximum bounds, is explained as follows:"
"in order to predict accurate number of unread tags and determine the new frame-size, aloha-based anticollision algorithms gather and use information such as number of successful slots, empty slots, and collision slots from the previous round to predict the appropriate frame-size for the next identification round. in past literature, there have been several methodologies related to backlog estimation. these approaches include schoute method [cit], vogt method [cit], bayesian method [cit], chen1 and chen2 methods [cit], and ptes method [cit] . unfortunately, some of these methodologies either have low performance efficiencies or are too complicated to be implemented for some rfid applications. these methods are explained as follows:"
"beat-to-beat qtv is modeled as a continuous-time signal composed of two separate contributions, x qt:rr (t) and x qt:vr (t) (see fig. 1 ). signal x qt:rr (t) represents qtv due to rrv. this component includes oscillations at respiratory frequency (respiratory sinus arrhythmia) and at lower frequencies (mayer waves). signal x qt:vr (t) represents oscillations unrelated to rr variability which are assumed to be due to ventricular repolarization (vr) dynamics. mathematically, this is described as:"
"the enhanced dynamic framed-slotted aloha (edfsa) first estimates the number of unread tags. if the number of tags within the interrogation zone is larger than the maximum frame-size, the edfsa algorithm splits the number of backlog into number of groups and allows only one group of tags to respond. when the reader limits the number of responding tags, it transmits the number of tag sets and a random number to the tags when it issues the query. only the tag that picks zero as its slot counter responds to the request. if the number of estimated backlog is below the threshold, the reader adjusts the frame-size without grouping the unread tags. after each read cycle, the reader estimates the number of unread tags and adjusts its frame-size. this procedure repeats until all the tags are read [cit] . the problem with edfsa method is, it assumes that 256 is the optimal frame-size and splits tags into group set by using the power of two (2,4,8...) . this results in a decrease in system efficiency when the number of tags is a fraction above the threshold and the number of group sets will be doubled."
"after \"queryrep\" command is received, each tag decreases its slot counter by 1. at the end of each frame, the reader checks if all tags have been identified, estimates the number of backlog using ptes algorithm, and adjusts its frame-size."
"the interval between the onset of the q-wave and the end of the t-wave in the ecg, so called qt, is a marker of cardiac repolarization. the beat-to-beat qt variability (qtv) conveys relevant information regarding cardiac pathophysiology. several studies have suggested that qtv may be a marker of sympathetic ventricular outflow and cardiac instability [cit] . however, relevant aspects of qtv analysis are still only partially understood [cit] . among them, there is the interaction between qtv and rr interval variability (rrv). qtv can be modeled as the sum of a component due to ventricular repolarization (vr) dynamics [cit] and another due to rrv. the former is thought to be a marker of ventricular sympathetic activity [cit] and it may have a better predictive value than qtv. therefore, it would be desirable to disentangle these two components. previous techniques to remove the influence of rrv from qtv have mainly utilized heart rate corrections [cit], time-invariant multivariate autoregressive models [cit] or other modelbased approaches [cit] . however, there is still a need for a methodology that provides accurate and robust estimates of the dynamic profile of qtv changes unrelated to rrv. the proposed framework uses multivariate time-frequency analysis to provide an estimates of changes in x qt:vr (t) based on measurements of x rr (t) and x qt (t)."
"in order to show the significance of our proposed methods, we conducted an experimental evaluation and compared our methods to the existing techniques. in this section, we describe the data sets used in the experiment, present the results of the experiment, and provide the analysis."
"our future plans are to extend the semi-automated mapping between job offers and cvs (related to the linkedin task), to include job offers and cvs from the monster website and to support other formats like pdf or doc for the cv upload. intelligent matching of job offers and users cvs to find the most suitable job for the applicant and the most suitable applicants for the job is our other goal. last but not least, we want to invite users to use and evaluate the whole system from their point of view. regarding the spatial index and search capabilities, we are working on their integration into lucene since there is not yet multi-location indexing per document supported."
"pct approach first estimates the number of unread tags, then it decided if the number of tags needs to be spliced or not. the probabilistic anti-collision algorithm, using ptes as accurate frame-size prediction, is then applied to each selected group of tag. algorithm 2 demonstrates our probabilistic based anti-collision algorithm applied to each selected group of tags, where only one group of tags respond to the reader. there are three kinds of slot:"
"simultaneous transmissions in rfid systems lead to collisions as the readers and tags typically operate on the same channel. three types of collisions are possible: reader-tag collision, tag-tag collision, and readerreader collision [cit] ."
"location related entities (geo-entities) extracted from web documents are in the form of a free-form text and need to be converted into latitude/longitude coordinates before they can be used for indexing the documents and in spatial search. the conversion process is called geocoding. there are several free geocoding services available. the most known are google geocoding [cit] and yahoo! placefinder [cit] services. we use both services as a basis for our geocoding approach, which is explained in more detail in chapter 6.2 on linkedin task. after the geo-entities are extracted and geocoded, they are ready for indexing. the spatial indexing is described in chapter 4.1."
"in this paper we have presented a work-in-progress framework for distributed crawling, extracting, indexing and lightweight semantic search over the extracted data with spatial support. the use of this framework has been shown on two example tasks, the linkedin job offer search task and the bbc news search task. spatial indexing and searching has been implemented as a plugin for nutch and solr. this plugin has been used for indexing documents by more than one geographic location and for performing searches within a specified bounding-box (other options such as circle area can be easily implemented too)."
"to show the derivation of all five cluster-based equations in detailed, table 2 demonstrated given information found from figure 2, and all missing fields. from table 2, it is visible that at optimal system efficiency of fig. 2 performance efficiency of different frame-size on different number of tags 38%, the number of tags is equal to the available framesize calculated by 2 q . we have set the minimum and maximum boundary efficiencies at 33%. the information on maximum number of tags at 33% is also available from figure 2 . for example, when q is equal to 8, the optimal percentage efficiency can be obtained at 256 tags and the number of tags at maximum boundary is equal to 352 tags. table 3 demonstrated the derived answers for missing fields from table 2 . the minimum boundary with 33% efficiency is calculated by the maximum boundary of the previous frame plus 1. thus, for q8, the minimum boundary is equal to 177 (176 + 1). after finding all information needed, the reverse engineered equations for maximum and minimum boundaries are derived for each q. after we found all outcomes for each q, it is now possible to find the reverse engineered equation for two or more type of qs. for example, if q8 and q7 is applicable, then the reversed engineered equation for maximum boundary is equal to ["
"the dfsa algorithms change the frame-size to increase the performance efficiency of the tag identification. however, as the number of tags becomes larger than the frame-size, the probability of collision increases rapidly. if number of unread tags can be estimated accurately, frame-size can be determined to maximise the system efficiency or minimise the tag collision probability. for instance, when the number of tags is large, the probability of tag collision can be reduced by increasing the frame-size. however, the frame-size cannot be increased indefinitely. when the number of unread tags is too large to achieve high system efficiency, the number of responding tags somehow must be restricted so that the optimal number of tags responds to the given frame-size [cit] ."
"we analyzed a huge amount of joblocations parsed from crawled job offer pages (totally 70 116) and observed that in many cases there are multiple locations defined in one joblocation. due to uncertain location format and multiplicity of locations in one joblocation string, it is not very smart to send the whole joblocation string to the geocoding service and expect a successful result. there should be another approach used because multiple locations in one geocoding request do the job in confusing the geocoder to return erroneous results. there was a gazetteer approach considered for finding the sub-locations, but it was desisted from it because it would require a very precise gazetteer to cover as many as possible location names. instead of it the location is split into several parts, where each part contains the possibly one sub-location. during the joblocation analysis, one can observe that sub-locations are often separated by conjunctions (e.g. \"and\", \"or\", \"und\", \"oder\", \"y\", \"o\" in english, german, spanish and other languages), which occur in geographic names very rarely. in addition, most of the joblocation strings contain a \"bracket part\" describing wider geographic areas of the job. both facts can be used to split one joblocation string into several sub-location strings. sub-location strings as the result of the split need to be further processed. words, which are not typical for the geographic names and which occur quite a lot in the sub-location strings, like \"anywhere\", \"business\", \"next\", \"next to\", \"office\", \"work\", etc. are cleaned off. afterwards, non-alphanumeric characters except the \".\" and \"&\" are cleaned off as well. finally, there are leading and trailing white-spaces trimmed."
tag collision problem is more complex than those within reader collision categories. the various types of tag anti-collision approaches for tag collision can be reduced to two basic types: deterministic approaches and probabilistic approaches.
"throughout this research, we have identified the significance of rfid tag anti-collision and developed a concept to minimise the tag starvation problem. we have proposed the probabilistic cluster-base technique (pct) to maximise the performance efficiency and reduce the total number of slots and frames queried during the tag identification process."
"the dynamic framed-slotted aloha (dfsa) overcomes the problems associated with bfsa by dynamically changing the frame-size according to estimated number of backlog, which is a number of tags that have not been read. in dfsa, each tag in an interrogation zone selects one of the given n slots to transmit its identifier, and all tags will be recognised after a few frames. each frame is formed of specific number of slots that is used for the communication between the readers and the tags. to determine the frame-size, it gathers and uses information such as number of successful slots, empty slots, and collision slots from the previous round to predict the appropriate frame-size for the next identification round [cit] ."
"after applying specific equations for each group division, table 6 shows the final pct rule for pct256. for instance, if the number of backlog equals to 900 tags, the pct256 algorithm will split the unread tags into 3 groups of q8 (256)."
"customized algorithms were used to detect the temporal occurrence of r-waves and t-end [cit] . the latter was defined utilizing a tangent method. the qt interval was approximated by the interval from the r-wave to the end of the t-wave in lead v4. ectopic beats and artifacts were rare. when present, they were removed and the time series were interpolated. rr and qt time series were interpolated at a sampling frequency of 4 hz, and the rrv and qtv signals, x rr (t) and x qt (t), were obtained by high-pass filtering these interpolated series with a cut-off frequency of 0.03 hz. instantaneous powers and coherence in a given spectral band ω were estimated as:"
"more details on bbc index are available in table 2 . documents represent bbc news. doc. ratio stands for percentage of overall fetched documents, where particular there is a user interface available at http://try.ui.sav.sk:7070/ apache-solr-3.1.0/browse (fig. 3), but it is still experimental and not very user friendly. we use it only for testing purposes."
"the aim of the experiment is to compare the performance of our proposed pct to the existing probabilistic dfsa and edfsa anti-collision approaches. the best approach is the one with the highest system efficiency and the lowest number of slots required for tag identification. in this experiment, we considered different number of tags, from 100 to 1400, within the interrogation zone. the number of simulated tags are assumed to be no more than 1400 tags, due to maximum range of uhf reader and passive tags. before the tag anti-collision algorithm is utilised, the number of tags are unknown."
"bbc news contains a lot of interesting information about what is happening in the world. there are 18 705 web pages crawled, including rss feeds and there are extracted entities like person names, addresses, cities, countries and nes. spatial entities are geocoded and indexed so that the documents in which they appear can be found when searching within a bounding-box."
"precise tag estimation scheme (ptes) [cit] approach assumes that for the current identification round, each collision slot has at least 2 tags colliding. however, it is unknown how many tags actually caused the collision. there is exactly 1 tag per successful slot, thus successful slots are not taken into consideration. on the other hand, empty slots will continuously occur during the next rounds of identification despite the frame-size. therefore, backlog after the current frame is defined by equation:"
the system was deployed and tested on a small cluster with 1 server and 8 nodes with configuration described in table 1 . overall storage capacity is over 5 tb and every node in the cluster can process up to 6 tasks in parallel (48 for whole cluster).
"lucene/solr dispose with a rich query language, which interprets query strings into a lucene query. one of such query types are range queries, which are used inside our application to filter jobs by their submission date. range queries can be applied on custom numerical fields like \"salary\" field, for instance."
"pct approach first estimates the number of backlog. if the number of backlog within the interrogation zone is larger than the specific frame-size, it splits the number of backlog into a number of groups and allows only one group of tags to respond. the reader then issues \"query\", which contains a 'q' parameter to specify the frame-size. each selected tag in the group will pick a random number between 0 to 2 q -1 and put it into its slot counter. only the tag that picks zero as its slot counter responds to the request. when the number of estimated backlog is below the threshold, the reader adjusts the frame-size without grouping the unread tags. after each identification cycle, the reader estimates the number of backlog using ptes algorithm and adjust its frame-size."
"person names are extracted in two-step approach, where in the first step a gazetteer is used to match a given name and finally extraction patterns with gazetteer results awareness are applied. the same approach is used for other ne types (e.g. companyname, telephonenumber). there are also patterns used, which combine results extracted by other patterns. this way an address entity is extracted for example. the generic named entities are sequences of capital letter starting words (with sentence beginning awareness) and have assigned a \"ne\" key (e.g. ne ⇒ \"gare sncf\"). users can interact with the extracted data using the graph search tool and change the type of extracted entity, delete its value or merge entities representing the same object (e.g. \"international monetary fund\" with \"imf\"). this way the user can help in creating negative and positive gazetteers for the next re-parsing and extraction."
"a full-text search with facets is accessible by customized native apache solr user interface, where users can input their queries and receive displayed results (see the fig. 2 and fig. 3 ). on the left side of the pane we can see several lists of the 10 most frequent values for each indexed entity type (jobtitle, jobcompany, joblocation, etc.) -\"field facets\". in the top pane there is a \"find\" search box (for full text search) with the list of already selected facets under it. as an example of a full-text faceted search, we can search for \"php\" to filter job offers with the word \"php\" in the content. we receive 2 944 results. if we are interested only in full-time jobs and a requirement for mysql and javascript, we select corresponding facets and receive 203 filtered job offers. the search can be also restricted to london. then 10 results matching the criteria for the london area are returned. a faceted search is also connected with entity relation search tool gsemsearch [cit], which benefits of entity relation graph traversing and spreading activation."
"from the results acquired for performance efficiency evaluation, we have developed five cluster-based equations to find a minimum and maximum number of tags suitable for particular frame-size. these minimum and maximum number of tags are derived to acquire the optimal performance efficiencies as in figure 2 . each equation is then used to exploit rules for pct."
"in order to simplify the derived equations, we employ the use of β (beta), κ (kappa), and µ (mu), and assigned these three icons to express each rule. in this research, we proposed three rules for pct: pct256, pct128, and pct-e (pct-extended). all rules split the number of backlog into groups then used one of initial q8 (frame-size 256), q7 (frame-size 128), or q6 (frame-size 64), to identify a current set of tags. equation 2 shows the conversion of all three key sets, for the five cluster-based equations, into β, κ, and µ. for equations 3, 4, 5, 6 and 7, we applied three key sets within these equations. these key sets employed β, κ, and µ and applied them into each pct rule, as shown in table 4 . table 4 displays equations 3 to 7, with the minimum and maximum bounds for each rule. for instance, equation 3 is applied to all three rules: pct256, pct128, and pct-e. equation 4, however, only apply to pct-e. table 4 the conversion of pct rules to β beta, κ kappa, and µ mu"
"dfsa can identify the tag efficiently because the reader adjusts the frame-size according to the estimated number of tags. however, the frame-size change alone cannot reduce sufficiently the tag collision when there are large numbers of tags because it cannot increase the frame-size indefinitely. dfsa has various versions depending on different tag estimation methods used. there have been several researches to improve the accuracy of frame-size by implementing a frame estimation tool [cit] ."
"the system presented in this paper operates over hdfs (hadoop distributed file system) [cit] . some of the produced data is stored in hbase [cit] (open-source implementation of the google bigtable [cit] ). in general, there can be any distributed file system used in hadoop under the condition that it can be mounted by the underlying operating system. internet content is crawled and processed by apache nutch [cit], which is executed in the hadoop distributed environment in parallel. nutch is a very powerful crawler with a lot of configurable features, plug-ins and filters such as:"
"the attention was paid more by the citizens in the place where a disaster happened than in other provinces. the citizens there concerned the damages and their needs while the citizens in other places wanted to know more on future situation, such as the possible track, intensity, tendency, and meteorology."
"in the two-input representation, each cppn has two instead of four inputs. to create a robot, the position of the midpoint between each of the 20 vertically-neighboring and 20 horizontally-neighboring grid points was input to the cppn in turn. the output of the cppn was interpreted in the same way as the four-input cppn representation. the robot controller was created by inputting the coordinate of the midpoint between the center of the cube to which a given body segment was attached and the midpoint of that segment into the cppn to determine whether the corresponding motor command had a phase offset of zero radians (an output of zero) or a phase offset of π radians (an output of one)."
"each bit string for every design was stored on a centralized server. so, if a participant drew a robot body that had already been attempted by themselves or another member of her group, her computer would perform another iteration of search for that design, starting from the best controller found up to that point. this enabled members of the cmt to consciously collaborate on a common design. however, participants in both groups could also unknowingly contribute computational effort to an existing design if they were not aware that that design had already been created by another member of their group: only 13 historical designs were shown to a participant in the interactive robot design tool but many more designs were stored on the central server. if a user invoked a design that had already been drawn and run by another user in her own group (either the imt or cmt), she would be continuing the hill climber for that design."
"every time that a participant clicked the 'go' or 'reset' button, or refreshed the page, the 13 designs were erased. each of the 13 slots was refilled as follows. for members of the imt, one of their past designs was chosen at random. for the members of the cmt, any past design produced by members of that team was chosen at random. then, one of the n controllers generated for that design by its hill climber was chosen at random from the central database. the robot design was drawn in the empty slot, and the distance traveled by that robot when controlled by that controller was written to that slot. the 13 summaries were then arranged in order, from the designs with the least displacement on the left to the most displacement on the right. this process was instituted to provide the participant with a sense for which morphologies may be amenable to controller optimization, and which morphologies may present the hill climber with a more multimodal search space."
"participants who arrived at the experiment website were double-blindly placed at random into either the crowd/machine team (cmt) or individual/machine team (imt) with equal probability. by comparing the performances of the cmt and imt, we were able to address the question of whether participants spontaneously collaborate in this domain. do they improve upon promising designs created by their peers, or do they become mired in group pathologies such as groupthink [cit] ? although recent work has begun to quantify conditions under which teams work well [cit], social collaboration within human/machine teams requires further study. for both teams, robot designs were simulated using a web-embedded physics simulation engine (github.com/kripken/ammo.js/). the physics engine is a javascript-based open source version of the popular c++ physics simulation engine, bullet (http://bulletphysics.org/). the fig 1. a hypothetical set of interactions in the crowd/machine team. participant 1 designs a robot (a) and then allows an optimization method to program his robot once on his computer (b). (the red and blue body segments oscillate in anti-phase with each other, resulting in a small amount of forward travel indicated by the gray arrow.) participant 1 allows the optimizer to re-program the robot (c), hoping its behavior will improve. participant 387 sees and likes this design, so she allows the optimizer to try again on her computer (d). meanwhile, participant 2 designs a different robot (e) and performs one round of optimization on it (f). later, several more participants also contribute computational effort to this design (g,h). after observing the quality of participant 2's design, participant 1 abandons his original design and attempts to improve on participant 2's robot by creating a larger variant of it (i). after performing an initial round of optimization on it (j), several more participants confirm the quality of this design (k,l)."
"we chose robotics as the domain in which to study human-computer collaboration as it has traditionally been viewed as an extremely challenging enterprise: only small academic or industrial teams composed of individuals with advanced degrees have so far produced capable robots. however, work on crowdsourcing robotics [cit] has shown that it is possible for casual users to accelerate the programming of autonomous robots and provide them with some semantic understanding of the world [cit] . these studies make clear that humans can play an important part in human-computer interaction and human-robot interaction, but an understanding of why and in what ways this collaboration can be successful is still underexplored."
"so far it has been shown that casual users can help in scientific domains such as protein folding [cit], galaxy classification [cit] and brain analysis [cit] . additionally, human participants in crowdsourced experiments have contributed pattern recognition capabilities to algorithms for aiding algorithms in generating realistic images [cit] and defining effective robot control schemes [cit] . however, the focus of these studies has been on the demonstration that it is possible to solve a complex problem by casual participants working collaboratively on the web rather than understanding why such a group of volunteers is successful at these tasks."
"we found that robots designed by both human teams on average outperformed the designs created by the mt (fig 3) . moreover, despite the fact that more computational effort was expended by the mt than by the cmt, no design generated by the mt was able to outperform the best design created by the cmt. using the complementary cumulative probability distribution of mt performance at the final evaluation, we found that there was near zero probability that the mt would produce a better design than the cmt (probability less than 0.0001). the greatest distance achieved by a robot out of all runs and all treatments of the mt was 28.0 units as compared to the maximum distance of 31.7 achieved by the cmt."
"positive sample is the set of messages posted by social users, which can be used to mine the spatial information of an urban emergency event. for example, the message \"a fire is happened at jiangsu road.\" can be seen as a positive sample. based on common sense and our observations on real data, we have three heuristics to select positive samples from weibo search results."
"the robots were virtual and behaved within a 3d simulated environment. participants designed and observed their robots in a web browser (fig 1a and were free to spend as much time at the task as they liked. they could also return and continue at any time. each participant could also execute a search algorithm on their computer, which gradually improved controllers for their current robot (fig 1b and 1c) . the participant was free to dedicate as much or as little computational effort to a given robot as they liked, and could design as many robots as they liked. if the participant copied another participant's robot design, the new participant's computer continued searching from where the originating participant's computer left off (fig 1d. we will refer to this team as the crowd/machine team (cmt)."
"the top of the interface housed the history panel, which displayed 13 pictorial and numerical summaries of past robot designs. members of the imt could only see their own past designs. members of the cmt saw their own past designs, as well as those produced by other members of their team."
"the early related work in this emerging research field has addressed the use of social media to generate map mashups to support collaborative real-time mapping [cit] and give the overview of harvesting geospatial content [cit] attempted assessing how fast tweeters reacted to the smaller (4.3 magnitude) [cit] . recently, the work of harvesting spatial information from the web has seen some activity in recent years. for example, it has been demonstrated that general purpose points of interest (poi) can be automatically derived from the users' map annotations [cit] and vague geographic regions (e.g., midlands or middle west) delineated. besides the texture content, georeferenced pictures from the photo-sharing website such as flickr have been processed in terms of their density to show where the most famous landmarks are for a given location [cit] . the geospatial exploratory data mining web agent that retrieves geographic information from web pages (related to outdoor activities) has also recently been discussed [cit] ."
"this intuition favored designs with symmetry, a morphological attribute that results in fast forward locomotion. that the crowd/machine team outperformed the individual/machine team demonstrates that social processes have a beneficial effect in this domain. while we cannot confirm which social processes resulted in the improved outcome, we hypothesize that this beneficial social collaboration derives from one or a combination of the following factors."
"the spatial information of an urban emergency event usually consists of two aspects including the happening spatial information and the spread spatial information. the happening spatial information means the happening place of the urban emergency event. for example, in fig. 2, the happening place of the fire is 358 huaihai road. the spread spatial information means the influential scope of the urban emergency event. for example, in fig. 2, the influential place of the fire is ruijing road, which is shown in the check-in information. the influential place is usually equal to the check-in information of the social users. the check-in information means that the social users can see the happening emergency events in his place."
"in order to ensure the robustness of our results, we investigated the performance of the cppn-neat algorithm using five sets of experimental parameters as reported in table 1 . in addition to varying the experimental parameter settings for the cppn-neat algorithm, we also investigated two different representations for the generation of robot morphology. we considered both a four-input representation described above as well as a two-input representation."
"participants could connect a 5-by-5 grid of points with lines. they could draw as many lines as the grid allowed. if the participant connected two points neighboring one another horizontally, or two points neighboring one another vertically, they were connected by a single line. if the participant attempted to connect two points that were not vertical or horizontal neighbors with positions (x 1, y 1 ) and (x 2, y 2 ), those points were connected with a series of horizontal or vertical lines connecting pairs of horizontally or vertically-neighboring points. these lines were chosen such that they approximate as closely as possible the straight line connecting (x 1, y 1 ) and (x 2, y 2 ). this made diagonal and/or overlapping lines impossible, simplifying the process of converting these points and lines into a robot that can be easily simulated in the rigid-body physics engine. they could click the 'reset' button at any time to re-start the design process."
"we selected the compositional pattern producing networks-neuroevolution of augmenting topologies (cppn-neat) algorithm [cit] for this, as it has been shown to be superior to other search algorithms for finding efficient gaits for robots [cit] and has also been employed for optimizing both robot morphology and control [26-28, 30, 32] . another reason we chose this algorithm is because it was originally designed to mimic biological development's bias towards the production of regular patterns. this bias toward regularity has been implicated in cppn-neat's ability to outperform other algorithms in automatically designing high-performance complex artifacts [cit] ."
"in this section, a natural gas leakage monitoring system is developed, which composes sensor network and data collection system based on cloud storage platform. the sensor network consists both mobile and stationary sensors, which help to provide large-scale city areas monitoring with less sensors and provide more rapid and accurate leakage locating with gis mapping."
"typhoon chan-hom hit zhejiang province at 16:40 on july 11, 2015 as a category 2 storm. it was one of the fig. 1 the hierarchical structure of the proposed method strongest ones to hit the region this year and caused tremendous damages. more than one million people were evacuated in zhejiang, and total economic losses in the province amounted to ¥8.86 billion (us$1.43 billion)."
"(1)social user layer. in this layer, the proposed method wants to collect the related data of urban emergency events. for example, if a user makes a message in weibo about a fire occurrence, then the proposed method should collect this message. (2)crowdsourcing layer. in this layer, address and gis information is mined from messages from social users. of course, location and gis information related to the same urban emergency events should be clustered. (3)spatial information layer. in this layer, the spatial information of the urban emergency event is mined."
"each cppn generates a robot design as follows. each cppn takes four real-valued inputs and produces two binary outputs. from the five by five grid of points in the experimental website, each pair of horizontally neighboring points is selected in turn. for each pair, the horizontal and vertical coordinates of that point pair are input to the cppn. the first binary output value is then read out. if the value is zero, the next point pair is supplied to the cppn's inputs. if the value is one, a line is constructed between those two points, just like a human participant might draw a line connecting that point pair. after each pair of 20 horizontally-neighboring points is supplied to the cppn, the positions of each pair of 20 vertically-neighboring points is supplied to it. this resulted in the creation of zero or more horizontal and vertical lines connecting some of these point pairs together."
"the explosion is also a major damage to the urban especially the big city. with the development of terrorism, the number and influence of the explosion event has number of microblog posts 1154 2176 fig. 3 risk distribution map. in order to make the figure readable, the maximum of the range was 300 though the number of microblog posts in zhejiang was 1320 become higher than before. for example, the \"boston marathon\" explosion event causes dozens of people dead. thus, it is important to build the knowledge base of explosion emergency event. for example, the image or video of an explosion can be used by policeman to detect the suspect. we select an explosion event that happened in 18:50 at \"taoyuan apartment\" on january 17, 2015 in dalian. the proposed method is used to build the knowledge base of that event. the word \"爆炸 (explosion)\" is used to search in weibo from 18:00 to 22:00. the search location is set as dalian. totally, 478 messages are returned. according to principles 1, 2, 3, and 4, 45 messages providing location information, check-in information, and image are selected as candidate messages. figure 5 shows the timeline description of this case study. only six timestamps are selected to build the timeline since they provide new keywords or locations. the red point in the map means the real location of that fire event. the red point with number means the gis information of the different timestamps."
"despite only slight differences between the aggregate behavior of the different teams (table 2), samples independent and normally distributed). the only difference between the two human/machine teams was that members of the cmt could see pictorial representations of their own and others' designs, and the distances achieved so far by those robot designs, whereas members of the imt could only see their own designs, and the distances those designs had so far achieved. this indicates that the superior performance of the social over the imt must be due to collaborative innovation: members of the cmt avoided poor designs, dedicated more computation to promising designs, and/or created variants of good designs more than members of the imt. in fact, the collaboration on designs can be seen in the average and maximum number of unique user contributions to designs in table 5 . although groupthink and other performance of the physically constructed robot developed through the collaborative effort of 209 participants. a-i: gait of the physical realization of the best robot found by the cmt, and panels j-l report the average distance travelled by the physical robot when equipped with the controller generated by the cmt (blue), three different randomly-generated asymmetric controllers (purple), and a randomly-generated symmetric controller (gray). each bar in these panels reports average distance achieved over 10 independent trials. panels m-o compare the average distance travelled by a randomlygenerated, asymmetric, physical robot with the same topology as the symmetric robot (purple) to a randomly-generated symmetric controller on the symmetric robot body generated by the cmt (gray) and the symmetric controller discovered by the cmt (blue)."
"by clicking on the leakage location, the relative position information will appear on the gis map, including the district name and street names, as well as the locations of significant subjects, such as maintenance center, firefighter offices, hospitals, schools, and chemical plants. a message will be sent automatically to relevant offices, such as management office of gas companies, maintenance center, and emergency response office. figure 6 shows examples of the system."
"participants in either team were free to design as many or as few robots as they wished. they were also free to copy the designs produced by others (if they belonged to the cmt), create variants of other participants' designs, or create completely new designs."
"the second treatment was the same as the first, with one exception: participants could not see designs created by other participants. we will refer to this team as the individual/machine team (imt)."
"once a participant completed a robot design, she could click the 'go' button, which would dedicate some of her own computer's computational effort to finding a good controller for that design. when this button was pushed and the design did not yet exist in the central server's database, a hill climbing search algorithm was assigned to that design. the hill climber improved behavior for its assigned robot design as follows. the hill climber searched over the space of possible controllers for that design, attempting to find those that enabled the robot to move as far from its starting position as possible. every time a participant pressed the 'go' button, one iteration of the hill climber would be performed."
"machine intelligence is arguably out-competing humans in a growing number of domains: autonomous robots are taking over warehouse [cit], construction [cit], and agricultural [cit] tasks; machine learning methods are becoming adept at finding patterns of interest in massive data sets [cit] as well as heretofore unknown biological relationships [cit] and even physical laws [cit] in raw data; and new search methods are increasingly challenging professional players in complex games [cit] ."
"in addition to a preference for symmetric designs, we investigated whether collaboration influenced the performance of the cmt. to do so, we compared the top designs produced by the cmt with those produced by the imt."
"each participant was instructed to design a robot that could move as far as possible in the simulation. participants accomplished this by designing a robot in the design panel (fig 2c), which was initially blank. they could then command a search algorithm to find good controllers for that robot. the quality of a controller is defined by how far it enables the robot to move from its starting position in fifteen seconds of simulation time. they could watch the progress of this optimization process in the simulation panel (fig 2b) . members of the imt could see their own past designs in the history panel (fig 2a), while members of the cmt could see designs produced by themselves and other participants in the same panel. it was through this history panel that users 'communicated' designs to other participants."
"here we have shown that a team composed of human designers who can dedicate more or less computational effort to their own or other participants' robot designs outperformed an machine team lacking human members as well as a treatment composed of human members who could not collaborate. although we are not the first to demonstrate that human and algorithm teams can outcompete machine-only groups [cit], we are the first to present reasons for why this occurs for design tasks. we showed that individuals bring pre-existing intuitions to bear on one part of the problem, while machines determine, through search, whether these intuitions are born out."
"heuristic 3: the original weibo message is prone to be a positive sample. if a message is a forward message other than an original message, the social user is not a witness of the urban emergency event."
"this investigation was complicated however by the fact that varying numbers of robot body/controller combinations were evaluated by each of the cmt and imt. in some cases, only a single controller was evaluated for a robot design, whereas other robot designs attracted many controller evaluations by members of a team. thus, to better estimate the quality of robot designs produced by the imt and cmt, we investigated in more depth how amenable to behavior optimization their best designs were. to do so, we first drew the top 50 ranking designs from both imt and cmt according to the best controller found for those designs within their originating teams. we then performed 100 replicates of a genetic algorithm [cit] against each design. each genetic algorithm replicate was given a population size of 100 with a 10% mutation rate and 10% crossover rate. genomes were bit-strings that indicated either a zero-phase (0) or π-phase (1) controller and the fitness objective was identical to that used in the web-based tool-a simulated robot was to move as far as possible within the allotted fifteen seconds of simulation time. the maximum displacement achieved by the design for each replicate was extracted and averaged over the 100 replicates. the resulting relative performance of the teams' top k designs for various values of k is reported in table 4 ."
"the semantic analysis on the geo-tagged microblog data helped obtain public opinion from the spatial perspective. the assistance could be offered to where it was required. keyword-frequency analysis and comparative analysis were conducted in three provinces where risk was relatively high. these provinces were zhejiang, shanghai, and beijing. they were the representatives of the landfall place, the adjacent province, and the capital, respectively."
"the cppn created a controller for its robot as follows. for each cppn, each pair of horizontally-neighboring and vertically-neighboring points connected by a line is iterated over. if each of these point pairs have positions (x i, y i ) and (x j, y j ), the cppn is supplied with the position where one of the two resulting one-degree of freedom joints would be (x i, y i ) and the position of the midpoint of the line connecting the point pair á about the cube centered at (x i, y i ) with a phase phase offset of zero radians, while a value of one indicated that that motor would rotate with a phase offset of π radians. for the same point pair, the cppn's inputs were now supplied with positions (x j, y j ) and"
"in this paper, a participatory sensing-based model for mining spatial information of urban emergency events is introduced. firstly, basic definitions of the proposed method are given. secondly, positive samples are selected to mine the spatial information of urban emergency events. thirdly, location and gis information are extracted from positive samples. at last, the real spatial information is determined based on address and gis information. moreover, this study explores data mining, statistical analysis, and semantic analysis methods to obtain valuable information on public opinion and requirements based on chinese microblog data. typhoon chan-hom is used as an example. semantic analysis on microblog data is conducted and high-frequency keywords in different provinces are extracted for different stages of the event. with the geo-tagged and time-tagged data, the collected microblog data can be classified into different categories. correspondingly, public opinion and requirements can be obtained from the spatial and temporal perspectives to enhance situation awareness and help government offer more effective assistance."
"the functions of concentration monitoring are as follows. (1) concentration monitoring includes the provision of a methane concentration heat map. the original data collected by the mobile detectors and stationary detectors are merely a cluster of discrete locations that describe the relationship between the gps information and methane concentration. therefore, these data locations must be processed using a classic gaussian plume diffusion model in order to obtain the concentration distribution of the entire monitored area. then, these data are shown by means of distribution on a gis map. (2) concentration monitoring also includes mobile detectors. the real-time locations (latitude, longitude, and height) and methane all the data for the citywide natural gas leakage monitoring system come from every detector, so the working state of the detectors determines whether the system is working normally. therefore, it is necessary to monitor the working state of all the detectors. each detector should periodically transfer a diagnostic report message to the control center. if the detector sends a faulty report or no report, then the detector is faulty and the maintenance staff must address such problems. (6) concentration monitoring includes the level of warning. the real-time warning presents the security status of the monitored area. (7) concentration monitoring includes a query detector. users are able to query each detector according to the detector number and highlight it on the gis map."
"in this paper, a participatory sensing-based model for mining spatial information of urban emergency events is introduced. firstly, basic definitions of the proposed method are given. secondly, positive samples are selected to mine the spatial information of urban emergency events. thirdly, location and gis information are extracted from positive samples. at last, the real spatial information is determined based on address and gis information. moreover, this study explores data mining, statistical analysis, and semantic analysis methods to obtain valuable information on public opinion and requirements based on chinese microblog data. typhoon chan-hom is used as an example. semantic analysis on microblog data is conducted, and high-frequency keywords in different provinces are extracted for different stages of the event. with the geo-tagged and timetagged data, the collected microblog data can be classified into different categories. correspondingly, public opinion and requirements can be obtained from the spatial and temporal perspectives to enhance situation awareness and help government offer more effective assistance."
"the keyword-based search method was applied to collect microblog data through the weibo api (an open interface of sina weibo). the keywords included \"can hong\" (the chinese of \"chan-hom\"), \"typhoon,\" and \"typhoon chan-hom,\" and the period of data collection was from july 8, 2015 to july 15, 2015. in the search, only microblog messages in chinese language were collected, and there were no geolocation constraints. after filtering the irrelevant data, the number of event-related microblog posts was 3321 with the geographic location of the posters and posting time attached. the number of microblog posts was counted in each province. the top four provinces were zhejiang, jiangsu, shanghai, and beijing, and their details were shown in table 1 . based on the posting time, the collected data were classified into two categories: (1) before typhoon landfall (0:00 on july 8 to 16:40 on july 11) and (2) after typhoon landfall (16:40 on july 11 to 23:59 on july 15). the number of microblog posts at these two stages was 1154 and 2167, respectively, as shown in table 2 ."
"the rest of the paper is organized as follows. in the next section, the related work is given. section 3 presents the proposed model. case studies on real data sets are conducted in section 4. the application on the proposed method is given in section 5. the last section gives the conclusion of our work."
"heuristic 1: the weibo message with the location or gis information is prone to be a positive sample. since the location information is an important aspect for mining spatial information, the message with the location information can be thought as a positive sample. different from the location information in weibo message, the gis information means the location of social users. heuristic 2: the weibo message with the image or video information is prone to be a positive sample. the image or video information is an important evidence to show the presence of social users. if a social user posts a message with the image or video of an urban emergency event, he is prone to be a witness of this event. thus, he is also prone to post the accuracy spatial information of the urban emergency event."
"with the advances of information communication technologies, such as cloud computing [cit], internet of things [cit], and big data [cit], it is critical to improve the efficiency and accuracy of emergency management systems through modern data processing techniques. the past decade has witnessed the tremendous technical advances in sensor networks, internet/web of things, cloud computing, mobile/embedded computing, spatial/ temporal data processing, and big data, and these technologies have provided new opportunities and solutions to emergency management. data processing/analysis in emergency management is a typical big data scenario. numerous sensors and monitoring devices continuously sample the states of the physical world, while the web data processing techniques make the internet a big data repository which can reflect the states of the cyber world and the human world. the efficient processing of these data imposes a challenge to the data management community. it is important to develop advanced data management and data processing mechanisms to support disaster detection, disaster response and control, rescue resource planning and scheduling, and emergency commanding. the objective conditions, such as the lack of information, timely changeable situation, short time for decision-making, and serious consequences, bring a great challenge for the government emergency response. the extraction of accurate information is very important in emergency response. social media provide a platform for data mining and information extraction."
"the first time the participant pressed 'go' for a unique design she had created, the number of one-degree-of-freedom rotational joints in the robot's design was counted. for a design with j such joints, a random binary string with j bits would be created. this string is assigned to the hill climber for this design. if a given design had received one or more iterations of search previously, then the current bit string associated with that design was copied, and each bit was flipped with 10% probability."
"y i þy j 2 à á relative to the cube at (x j, y j ). the processes described in this and the previous sections thus enable a cppn to generate both the morphology and control of a single robot."
"words set consists of keywords of an urban emergency event. usually, these words can be used to search messages of an urban emergency event. it is noted that elements in the words set are synonyms since social users may use different words to describe the same urban emergency event."
"in addition to the thirteen randomly-sampled designs shown at the top of the interface, members of the imt were shown their own best design at the bottom right of the page. the best design is defined as the one that had been most displaced by its hill climber, regardless of how many iterations of search each design has accumulated. members of the cmt were shown the best design discovered by that team as a whole so far."
"two databases on the central server were established: one stored designs and controllers from the imt, and the other stored designs and controllers from the cmt. this ensured that members of one team could not continue behavior optimization for robots designed by members of the other group."
"given more time and computational resources, we would be interested in extending the analyses to account for other parameterizations of the evolutionary algorithms used in the study. due to resource limits, we needed to define a somewhat arbitrary cutoff on the duration of the experiment and work with a fixed set of parameters in evolutionary algorithms. had we investigated other parameters, we might have found results that were different from the findings of the present study. additionally, the optimization method used may have converged upon a locally optimal result, thus missing the global optimum. it could be that the set of globally optimal robot designs are those that are not symmetric and the human participants exposed the algorithm to a locally, but not globally, optimal bias. future studies will focus on the social dynamics of the crowd interactions and the means by which human designers were influenced by both their interaction with other members of the crowd and their own experimentation with the design tool. in this study, we observed that it was beneficial for the human-algorithm variant to be exposed to other human designers. however, the modes that this social dynamic are beneficial may or may not be similar to the social dynamics present in other studies. users may have, as the result of their own experimentation or by observing the design of other participants, discovered morphological variations that work well in tandem with a particular control structure; or they may have been able to improve substantially on a design by another participant that they might not have discovered on their own."
"moreover, we here report the first investigation into whether casual users can design robots, not just help them learn: in some of the teams we created, participants were tasked with designing robots that their computer could then program to move rapidly. we hypothesized that people may be able to bring their intuitions about animal movement to bear on this problem: casual users may, consciously or otherwise, know what kinds of body plans facilitate (or obstruct) the discovery of fast forward locomotion [cit] ."
"in this section, the proposed method is illustrated. firstly, basic definitions of the proposed method are given. secondly, positive samples are selected to mine the spatial information of urban emergency events. thirdly, the semantic analysis of the social media is given. fourthly, location and gis information are extracted from positive samples. at last, the real spatial information is determined based on address and gis information."
"in this section, basic definitions of the proposed method are given. an event is something that happens at some specific time, and often some specific places [cit] . in the proposed method, the spatial information of an urban emergency event can be mined since some messages sensing by social users have exact location information. an urban emergency event is defined as follows."
"typhoon chan-hom formed in the northwest pacific ocean and was classified as a tropical depression on june 30, 2015. it was assigned the name chan-hom when it upgraded to a tropical storm on july 1. with heavy storm and extreme rain, chan-hom made landfall on zhoushan, zhejiang, on july 11, 2015 and the maximum sustained winds reached to 45 m/s. thereafter, the storm accelerated north-northeast and moved over the yellow sea with degradation of its structure. chan-hom was tracked each day based on the physical data mainly including gis data, wind speed, rain rate, storm surge, and typhoon intensity."
this process enabled each cppn to design one robot. it was possible that some cppns create 'null' robots comprised of zero lines. such robots were automatically assigned an objective function value of zero.
"our finding suggests that increasingly large, diverse and complex collaborations that combine people and machines together in the right way (and further empowered by collaborationenabling web tools, cost-effective additive manufacturing [cit] and cultural trends such as the maker movement [cit] ) may accelerate innovation in a wide range of fields. finally, such work may help ensure that accelerating technological advancement develops into an empowering rather than a disenfranchising phenomenon."
"then, the participant's design was rendered as a simulated robot in the physics engine as described in the previous section. the robot was allowed to move in the simulator for 15 seconds and *22 cycles of motor oscillations. at each time step, each of the j motors associated with each joint is controlled with position control. the desired position sent to each motor is determined by a sinusoidal signal. this signal has a frequency of 1.5hz and oscillates within [−45 o,+45 o ]. all of the joints that have a zero associated with them in the current bit string oscillate in phase with one another, but are offset by a phase of π radians from those joints that have a one associated with them. the hill climber could in this way tune which joints move in phase or in antiphase with one another. the participant could observe the resulting movement of the robot in the simulation panel (fig 2) . if the participant clicked the 'go' button again, her computer would perform another iteration of the hill climber. the more times a participant clicked the 'go' button, the more search would be conducted for that robot design."
the third team we studied was composed only of computers: a state-of-the-art search algorithm [cit] was employed to design robot body plans and to find good controllers for them. it has been demonstrated [cit] that this algorithm outcompetes other algorithms at designing and programming robots. we will refer to this team as the machine team (mt).
"participation numbers for both the cmt and the imt are summarized in tables 2 and 3 . the top performing robots for each group are shown in fig 4. the individual/machine team portion of the experiment was stopped when we saw an appreciable decrease in participation (number of participants on day 1: 2805, day 2: 2108, day"
"additionally, vetting users according to their expertise would be an interesting addition the analyses. while we suspect that the majority of users in the present study were non-experts, the participants were anonymous and thus we cannot verify the extent to which the users were experts or non-experts."
"whenever a participant simulated a robot, its current displacement was reported in the simulation panel. if its current displacement matched or exceeded the distance achieved by a robot in the history panel, that number in the history panel would change from white to pink. this 'thermometer' metaphor gave the participant a real-time indication of how well their current design was able to move relative to the random sampling of past designs being displayed. it was hoped that this implicit competition might further incentivize participants to design robots with good potential for behavior optimization."
"emergency management aims to develop strategies and establish operations to decrease the potential impact of unexpected events (i.e., human or natural disasters). by quick response and rescue, it saves human lives from the secondary disasters and enhances the stability of communities after disasters. emergency management involves four stages: planning and mitigation, preparedness, response, and recovery. geospatial applications (including geographic information system (gis)) have been extensively used in each stage of emergency management. decision-makers can utilize the geospatial information to develop planning and mitigation strategies. gis models and simulation capabilities are used to exercise response and recovery plans during nondisaster times. they help the decision-makers understand near real-time possibilities during an event."
"semantic analysis aims at finding useful data from mass data to obtain valuable information or knowledge. sina weibo, one of the most popular microblogs in china, provides a platform for data mining. people express and record what they saw, heard, or experienced in the selforganized platform, and they also receive information from it. sina weibo is a huge information interaction center and the source of microblog data collection in this paper. the keyword-based search method was applied to collect microblog data through the weibo api (an open interface of sina weibo). in the search, only microblog messages in chinese language were collected and there were no geolocation constraints. text classification is to divide all the messages into several categories, and then semantic analysis is conducted in a certain category. the geo-tagged and time-tagged information of microblog messages was the standard for text classification. the collected microblog data were classified based on the geographic location of the posters. the number of microblog posts was counted in each province. the location information can be extracted from the posted messages from positive samples. for example, in fig. 2, a user post a message \"i see a fire at huaihai road\", the posted location information \"huaihai road\" can be extracted. in the proposed method, the location information detection is based on the baidu map, which can detect whether a word is a location name or not."
"as this process spreads and accelerates, it remains to be seen what role humans will play in an increasingly automated society. we present evidence that one key role that humans may continue to play is one of cooperation with machine intelligence: complementing the speed of machines with the human capabilities of creativity, pattern recognition, and an ability to apply intuitions about the physical world to abstract problems."
"first, our interface allowed participants to view the work of others. we hypothesize that allowing participants to strengthen their pre-existing intuitions by viewing the work of others may have benefitted the crowd/machine team. based on these growing intuitions, the participants could dedicate more or less computational effort to vet designs using search algorithms. the search algorithms in turn expose incorrect assumptions or validate intuitions. second, human-produced designs and their machine-generated quality estimates were advertised to the group using a representation interpretable by casual users. if our interface did not represent the designs in this way, the improved performance of the crowd/machine team over the individual/machine team would have been possible only by chance, which we demonstrated to be unlikely (see discussion section). whether the participants benefitted from seeing the humangenerated designs, the machine-generated quality estimates, or both is unknown, but at least one of the two must have been responsible for the superior performance of the crowd/machine team over the individual/machine team. thus, by capturing intuitions from casual users and exploiting synergistic social processes arising among them, we have shown that casual users may contribute useful creative work to a collaboration between humans and machines."
"crowdsourcing or participatory sensing may be a potential solution mining the spatial information of urban emergency events. the proposed method aims at collecting and analyzing the spatial information from social users. the social network can be seen as a sensor receiver. usually, the social network users can be seen as social sensors. the proposed method is set as a hierarchical data model including three different layers. the different layers of the proposed method are illustrated in fig. 1 ."
"what caused the superior performance of the human/machine teams over the mt? the mt finds a robot that is able to achieve a distance of 28.0 units, which is near the best found by the cmt of 31.7 units, but this is the best result of 100 independent runs each of 5 different treatments explored with the mt: a much larger number of attempts than the cmt was allowed. although the cmt evaluated more designs than the mt, we do not expect the superior performance to be due to these differences. this is because, in comparison, the imt designed more robots than the cmt, yet performed worse than the cmt ( table 2 ). the mt did use cppn-neat to define both the morphology as well as the control algorithm for the robots in contrast the the hillclimber that was used to define the control of the robots in the cmt and in the imt. however, we hypothesize that using cppn-neat for control versus defining the control scheme with a hill climber is in fact an advantage that the mt has over the human/ machine teams based on past work [cit] ."
"to understand how people and computers might work best together in this domain, we created three separate teams, each of which was tasked with designing and programming robots (note that we are here using the term \"team\" to distinguish between experimental treatments without necessarily referring to the standard social sciences definition of the term)."
"our hypothesis is that humans can effectively work as part of a hybrid human-algorithm team by contributing their experience as embodied and social organisms to their algorithmic counterpart. to test this hypothesis, we created treatments that combined human participants and search algorithms together in different ways. each team was responsible for designing and programming autonomous robots such that the robots performed a desired task, which in this study was rapid locomotion."
"risk perception can be estimated by the number of social media messages to a certain degree. risk perception was a driver to motivate a person to use social media during crisis. the scale that people use social media reflects people's risk perception during emergency. the number of microblog posts in different provinces was estimated based on the geolocation information. the risk distribution map was drawn based on the number in each province, as shown in fig. 3 . it was clear that the citizens' risk perception in some areas was relatively high, such as beijing and some coastal provinces, i.e., zhejiang, jiangsu, and shanghai. the closer away from the landfall place was, the higher the citizens' risk perception was. beijing, as the capital of china, citizens there tends to pay more attention on crisis than people in other provinces."
"each summary presented a top-down view of the robot's morphology, and the distance that that robot had managed to travel using one of the controllers that had been supplied to it by its hill climber."
"the bit arriving at the cppn's second output now dictated the phase with which the motor at (x j, y j ) would rotate the body segment at"
"in this study, public opinion was extracted from the keywords whose frequencies were higher than 10. the number after a keyword represented its frequency. the comparative analysis of keywords distribution of these three provinces was shown in fig. 4 . the extracted keywords were classified into five categories based on their meanings. they were current situation, damage information, response, future situation, and citizens' requirements. the number of keywords describing current situation was the biggest of the five categories in all provinces. the keywords of zhejiang mainly clustered in four categories: current situation, damage information, response, and citizens' requirements while shanghai and beijing were current situation, response, and future situation. it was clear that the number of keywords was bigger and their frequencies were higher in zhejiang than in other provinces."
"the first team was composed of human participants and algorithms: participants created, shared, and improved upon each other's robot designs (fig 1) . [this work was exempted by the committees on human subjects serving the university of vermont and fletcher allen health care, approval number 14-228.] participants were recruited through the online bulletin board system, reddit. we requested participation by querying users of several subreddits, pages devoted to subtopics of interest (such as artificial intelligence, robotics, programming and visualization). therefore, participants were interested in technical subjects, but likely came from from varying backgrounds. however, users were anonymous so we cannot confirm the experience level of participants in the study with respect to each of these subjects. reddit demographics are predominantly white, suburban males aged 18 to 29 years old [cit] . all participants were unpaid volunteers."
"remove an edge in g. 3) high degree failure (hdf). select and remove the node with the largest degree in g. 4) degree product failure (dpf). select and remove the node with the largest degree product in g. we calculate the degree product of an edge by multiplying the degrees of the two nodes at the ends of the edge with each other. failed nodes or edges cannot continue transferring data. the less time the remaining nodes take to transfer data to the targeted node, the more robust is the network. this paper assumes that the shortest time to transfer data between two entities is proportional to the length of the shortest path between them. in this case, global efficiency e is an excellent metric for the robustness of network g:"
"despite the advantages of our research, there exists one primary limitation. our network model specifically views the links among entities as bidirectional while real-world information systems contain many unidirectional links. therefore, it is significant to develop more practical network models that take unidirectional links into account. this will be the priority for our future research."
"attacking an entity causes it to lose operational capacity, which is modeled as the removal of the corresponding node from the network structure of an smis. attacking the relationship between two entities allows the entities to operate but causes their collaboration to fail, which is modeled as the removal of the corresponding edge. in accordance with the above four types of attacks, the entities in an smis can have four failure modes, which are modeled as follows: 1) random node failure (rnf). randomly select and remove a node in g (which denotes the network structure of an smis)."
"resources in the physical layer of an smis contain physical-layer devices (e.g., robots, sensors, and cameras), software, data, and people. for the same reason as the cyber layer, we consider physical-layer devices and people as physical entities, which collaboratively produce and collect the data mirroring the states of the devices and products (e.g., work conditions and environmental parameters), and become new elements of an information system by connecting with the cyber layer. similarly, we view physical entities as nodes (supposing that the number of the nodes in a cyber layer equals that in the physical layer) and the relationship between two physical entities as an edge. the network structure of the physical layer of an smis is denoted as:"
"traditional information systems are composed of terminals, servers, software, network communication devices, data, processes, and people [cit] . most of these elements exist only in cyberspace. in contrast, an smis not only contains the above elements from multiple enterprises but also incorporates a vast number of additional devices (e.g., digital cameras and machines) from physical space, as well as the plentiful software and data residing on these devices [cit], as shown in fig.1 . for various reasons, such as hostile environments and prolonged operations, resource failures can occur gradually or abruptly. with the increasing use of devices in the physical space of an smis, such failures become more serious. once a resource fails, it may induce abnormal operations in neighboring resources, followed by cascading failures, which could finally cause a widespread outage. in fig. 1, if the server fails, the terminals cannot access data from the server, and the robot cannot offload its storage and computing processes to the server, leading to the ineffective operations of both the terminals and robot. in time, the neighbors of these devices are affected, eventually causing the failure of system functions and the exponential growth of risk. thus, robustness is vital for any smis, especially when resources frequently fail."
"from a complex network perspective, this paper describes a novel methodology to quantitatively examine the robustness of smis experiencing resource failures. the analysis of system elements and modeling of resource failures underlie the formation of a network model for an smis, by which we analyze how different failure modes and link patterns affect robustness. the results of the simulations indicate that, for an smis, especially its cyber layer, with the properties of being robust-yet-fragile and heterogeneous, al is the best link pattern against targeted failures. in addition, node failures have greater impact on robustness than do edge failures. these findings are conducive to the front-end design and the back-end control of an smis."
"driven by various cutting-edge technologies, such as the internet of things and cyber-physical systems, physical entities can link to cyber entities. this paper considers three common link patterns which are depicted as follows:"
"the remainder of the paper is organized as follows: section ii reviews the relevant literature. section iii describes a network model for an smis and simulations of different resource failures. section iv presents a numerical analysis of how resource failures influence the robustness of an smis. finally, section v gives our conclusions."
"to analyze network robustness in a quantitative manner, many metrics have emerged, but there is no unified measurement system. robustness metrics proposed in the literature include geometric connectivity [cit], assortativity [cit], endurance [cit], natural connectivity [cit], information entropy [cit], giant components [cit], the proportion of failed nodes [cit], the proportion of failed edges [cit], global efficiency [cit], reliability and average degree [cit], algebraic connectivity [cit], degree diversity [cit], clustering coefficient [cit], and betweenness centrality [cit] . of these metrics, global efficiency assumes that two nodes transfer data through the shortest path to minimize data-transfer time and an smis should be designed to quickly provide information to users. this high correspondence motivates us to adopt global efficiency to quantify the robustness of an smis."
"in sm, information systems break through traditional cyberspace and gradually extend to physical space. thus, the structure of an smis can be divided into a cyber structure and a physical structure."
"in recent years, manufacturing intelligence has gradually integrated with traditional manufacturing, bringing about an advanced manufacturing paradigm known as smart manufacturing (sm) [cit] . many national manufacturing strategies, such as ''industry 4.0 [cit] '' and ''industrial internet'', give priority to sm. according to research by smart manufacturing leadership coalition (smlc) [cit], sm is and will continue to be the mainstream manufacturing mode in the 21st century, being driven by disruptive technologies, such as big data [cit], cyber-physical systems [cit], cloud computing [cit] and network technologies [cit] . with the purpose of integrating decentralized resources to fulfill dynamic and diverse customer demands, sm emphasizes vertical integration within enterprises, horizontal integration among enterprises, and endto-end integration in the same value chain surrounding the product lifecycle [cit] . to realize the above integrations, enterprise information systems belonging to different enterprises need to be interconnected to support the provision of products and services. when all enterprise information systems in the same value chain are interconnected, a smart manufacturing information system (smis) is created."
"the neoteric aspects in this research are: (1) in terms of system modeling, this paper adopts complex network thinking to establish a network model for an smis, which fills gaps in the current research marked by the structural modeling in the design phase of an smis; (2) in terms of robustness, this paper quantitatively analyzes the robustness of an smis affected by resource failures, which presents a novel attempt to address robustness at the root of an smis. our methodology synthesizes previous research on network robustness to provide a pragmatic framework for analyzing robustness under conditions of resource failure."
"faced with node failures, e c and e p (i.e., the robustness of g c and g p ) do not significantly vary by link patterns, and the robust-yet-fragile property is significant in g c, as shown in fig. 6(a) and fig. 6(b) . due to the differences between g c and g p, hdf first makes e c decline sharply but markedly impacts e p only when f increases to a certain value. however, e c and e p under dpf significantly vary by link patterns, which is very different from under hdf. to be specific, compared to rl and al, dl under dpf can preferentially protect the edges within g c and g p from failures. thus, dl is best for the resistance of g c and g p against dpf, as shown in fig. 6(c) and fig. 6(d) . moreover, under dpf, more edges in g c than in g p fail when h is small. as a result, the declining rate of e c is higher than that of e p . it should be noted that dl is best for g c and g p against dpf, but causes the biggest damage to the smis."
"since the development of smiss is still at an early stage, existing research mainly focuses on the front-end lifecycles (e.g., system implementation and ict adoption) of smiss and hardly involves their robustness. this paper devises a methodology to analyze the robustness of an smis from a complex network perspective. the methodology decomposes an smis into two interconnected layers and introduces three link patterns between these layers, which underlies the formation of a network model of an smis. the methodology also considers and models four kinds of resource failures, then describes the robustness of an smis under each failure mode."
"an smis is a networked system that covers many cyber and physical entities. through mutual collaboration, these entities convert data into information, information into knowledge, and knowledge into actions through decision-making. in a dynamic and uncertain sm environment, the entities may fail in a gradual or sudden way. the failures either hinder the entities from realizing their functions or destroy the collaborative relationships among the entities, finally impairing the robustness of the smis. the factors inducing resource failures can be roughly divided into two categories, which are random and intended attacks. random attacks refer to random adverse causes, such as extreme environments and human errors, and are of two types: attacks against an entity or the relationship between two entities. intended attacks are goaldirected interference, such as purposive physical attacks, and are also of two types: attacks against the entity that has the maximum number of links with other entities or against the relationship between two entities that has the largest link product."
"resources in the cyber layer of an smis cover cyberlayer devices (e.g., servers, routers, and laptops), software, data, and people. the relationships among these four kinds of resources are that software and data reside on cyber-layer devices, some of which collaborate with people. as such, we view cyber-layer devices and people as cyber entities that synergistically handle business data within the scope of traditional enterprise informatization, the data from the external internet (e.g., social media data), and the data collected from devices in the physical layer. after processing, these entities send feedback to the physical and decisionmaking layers. here, we treat each cyber entity as a node and the cooperative relationship between two cyber entities as an edge. the network structure of the cyber layer of an smis can be represented by:"
"we compare the results obtained on 6 epochs with 10 epoch for phoneme pair ( ) and observed that accuracy increases by 12% almost. ) with 20 epochs. we apply the transfer learning algorithm on phoneme pair ( ) and set epochs to 20. we obtain an accuracy of 67% for phoneme pair ( ). we compare the results obtained on a different number of epochs (5, 10, 15, and 20) for phoneme pair ( ) and find that we obtain optimal results on 10 epochs. we execute the transfer learning algorithm on 6, 10, 15 and 20 epochs for all six confusing pairs and distinctive group. the results of the transfer learning algorithm on six confusing pairs and distinctive group with a different number of epochs are shown in table 11 . ( ) phoneme pair, alexnet achieves the accuracy of 94% using transfer learning process. the similar process applies to all phoneme pairs. the phoneme pair's ( ) accuracy vary from 70% to 98%, ( ) phoneme pair's accuracy vary from 74% to 99%, ( ) phoneme pair's accuracy vary from 68% to 98%, ( ) phoneme pair's accuracy vary from 45% to 82%, ( ) phoneme pair's accuracy vary from 45% to 71% and all other phonemes' accuracy vary from 58% to 95% from 1st to 10th epoch."
"in the context of high reliability of wqm application data delivery to water monitoring stations, the nb-iot solution may be advantageous compared to the proprietary-based solutions. the main reason for this is because of the massive presence of cellular infrastructure in urban locations."
"because of the power requirement of the ieee 802.11ah network, which is slightly higher than the lpwan variants, future directions should focus on designing new strategies to improve on the communication network to support low-power dissipation. this can be realized by exploring the design of novel and adaptive hardware that supports different modulation schemes at the phy layer (or multi-phy layers). based on this, different power dissipation levels at different data transmission rates will be achieved, while adaptive communication schemes are employed for the selection of a phy layer, depending on the pattern of traffic."
the dropout strategy can be viewed as a sort of regularization by stochastically setting some of the information neurons or hidden neurons to be zero to diminish the co-adjustments of the neurons. this is typically used in the completely associated layers in the alexnet design. the detailed architecture of alexnet explained in the following sections.
"typically, the communication architecture of wsn solutions in wqm applications can be classified into two, namely local communication and remote communication. in both cases, the water quality sensors employed for the monitoring of water can communicate over either a radio link or a satellite link, depending on the adopted communication technology. meanwhile, radio communication technologies are mostly employed in the perspective of wsn solutions for wqm applications. however, a satellite-based communication technology may represent a viable data networking solution in application environments where radio technologies are not viable due to the lack for the necessary supporting infrastructures such as terrestrial systems. examples of such application environments include large open seas. nevertheless, it is worth clarifying that both technologies may be combined in wsn architectures for wqm applications, subject to the environment."
data preprocessing is a basic part of applying machine learning procedures to present culmination. it aims to perform a significant analysis of the information and ideal outcomes are accomplished. in our framework data preprocessing involves the following steps.
"where v as, v bs, and v bs are the generator-phase currents, v bemf is the peak amplitude of the generator back emf voltage, ω r is the generator electrical speed, and n is the harmonic number. the transfer function given by (2) still holds for the inductorless converter by replacing the inductance and resistance values by 2l s and 2r s, respectively and neglecting the diode forward voltage. therefore, the pi controller gain tuning follows the same procedure shown by (3) with the new inductance and resistance values."
"experimental results show that the neural network achieves the highest accuracy of 90.01% as shown in fig 10. svm achieve an accuracy of 83.5% and knn achieve 74.37% accuracy. if we compare the result of layer4 features with layer 5 feature set, we found that there is a minor change in accuracy from layer 5 to layer 4."
"it is noteworthy that there are unique features associated with different applications, and each specific application requires distinctive properties from a wsn solution. understanding of the differences is important to address the peculiar requirements of a specific application. for example, in applications that include healthcare monitoring (such as the monitoring of cardiac conditions) and urban traffic monitoring, network data interchange is typically characterized by continuous monitoring, at the expense of energy resources. also, in video-based applications high data rate and lowlatency are crucial requirements, and high energy usage for a fixed time becomes acceptable. the peculiar features of wqm applications to be considered are in terms of the types of water quality sensors to be deployed, the sampling rates of water quality parameters, the patterns of water quality data traffic, deployment locations, energy sources, and types of communication networks to be adopted."
"the idea of eradicating disease with an appropriate molecule came out of advances in synthetic chemistry and stereochemistry at the turn of the twentieth century [cit] . paul ehrlich, in particular, paved the way to targeted medicine. as a 'microbe hunter', he was one of the first scientists to use staining techniques to detect particular cells. while studying the selective action of aniline dyes on biological tissues, he hypothesized that cells have specific receptors enabling them to take up specific molecules. just as emil fischer had ventured the analogy between lock and key to explain how enzymes bind with specific substrates, ehrlich used the hypothesis of chemical affinities-called today 'mechanisms of molecular recognition'-to identify toxins with a selective therapeutic action on specific cells. he thus tested hundreds of dyes on mice in search for the unique dye that could target the large trypanosome responsible for sleeping sickness. [cit], he obtained an effective drug against syphilis, salvarsan. [cit] biographical movie, dr ehrlich's magic bullet (fig.1 ). [cit], original motion picture poster, © 1940 warner bros. 8 ehrlich wrote that \"if we picture an organism as infected by a certain species of bacterium, it will obviously be easy to effect a cure if substances have been discovered which have an exclusive affinity for these bacteria and act deleteriously or lethally on these alone, while at the same time they possess no affinity for the normal constituents of the body and can therefore have the least harmful, or other effect on that body. such substances would then be able to exert their full action exclusively on the parasite harboured within the organism and would represent, so to speak, magic bullets, which seek their target of their own accord\" (ehrlich [1906 (ehrlich [ ] 1960 ."
"where k represents the kernel function, ci is the support vectors, w gi is the weight, c is input data and b is biased. we test the performance of svm on our dataset with different kernels (linear, cubic, quadratic). we found that the cubic polynomial kernel categorizes our confusing phonemes more accurately as compared to the linear and quadratic kernel."
"convolutional neural network alexnet only takes an input of size 227 * 227 * 3, so we resized all the spectrograms according to an input layer of alexnet to extract features. after data preprocessing, we fed the spectrogram data to convolutional neural network i-e alexnet for feature extraction."
"a transceiver is employed to provide a suitable communication service for exchanging information among water quality sensors, including a bs, in a network. it is the most energy consuming device in a sensor node [cit] . the choice of a transceiver is essential in terms of energy efficiency. a transceiver may be implemented using different types of communications that include radio frequency (rf), satellite, microwave, infrared (or optical), magnetic-induction, ultra-wideband, and acoustic [cit] . typically, the transceiver in wsn solutions for wqm is implemented as rf communication based on radio technology. the transceiver for underwater communication may be implemented as acoustic communication because of the underwater channel characteristics, which is not really suitable for rf communication in terms of diffusion and absorption [cit] . rf communication covers the electromagnetic frequency in the range of 3 khz and 300 ghz. the transceiver in wsn solutions for wqm may also be implemented as the more expensive satellite communication depending on the application environment. consequently, in wsns for wqm applications perspective, the medium of communication may be a radio-or a satellitefrequency. however, a radio-based frequency is advantageous compared to a satellite-based frequency because line-of-sight (los) is not a requirement between a transmitter and a receiver, reasonable error rates at acceptably low energy rate are possible, and a long communication range is provided. for radio-based communication, the recommended radio frequencies for wsn applications fall in the category of the industrial, scientific, and medical (ism) spectrum as a result of the cost of operation [cit] . for the transmission of a bit or a byte, the transceiver module provides a suitable platform for the medium access control (mac) layer for initiating the transfer of data from a sensor node's memory storage and handing it over to the transceiver of the sensor node."
"we use the area under the roc curve (auc) to measure the quality of the handcrafted features-based model, cnn features based model and transfers leaning based models. roc curve is a more reliable and meaningful measure to assess the quality of models. the results of each model presented using the roc curve as shown in fig 18. roc curve of transfer learning-based model is close to the leftmost corner and give the best result for classification as compared to handcrafted features-based model and cnn_features based model."
"wi-fi is a wlan-based technology, managed by the wi-fi alliance, and defined in ieee 802.11 standard specification [cit] . wi-fi can be described as a high-rate wireless technology that is used for medium range communications in wsns. it employ schemes that include offset qpsk (oqpsk), bpsk, and a multiple code dsss (mc-dsss) at the phy layer. wi-fi solution dissipates more energy compared to other communication technologies such as bluetooth and zigbee. wi-fi devices can utilize either 5 ghz or 2.4 ghz of the free-license ism bands for data transmissions at rates of 11 -54 mb/s, over a distance of about 200 m. wi-fi is an internet-based technology that could be employed as an internet solution by wsn systems for internet connectivity. it is important to emphasize that a wi-fi solution does not provide energy efficiency for the transmission of data among the sensors in a network, as well as the base station, because of its high energy dissipation characteristics. however, it may be employed as an internet access point for remote data transmissions at the base station, and it could also potentially be utilized at remote monitoring stations for internet connectivity. it is suitable for monitoring and control applications at homes and water stations to gain access to the internet."
"to develop an alternative oïkological metaphoric framework we have to clarify the mode of existence of nanomedical devices for targeted therapy. for this purpose, the conceptual distinction between dispositions and affordances may be helpful. it is often claimed that nanotechnology enables to control and monitor the delivery process [cit] . the objects designed for drug targeting are usually named 'nano-devices'-a very well chosen term, because 'devices' (in french 'dispositifs') precisely can activate dispositions. to put it in more philosophical terms, nanoplatforms rely on dispositional properties when they are designed as missiles. 23 in the case of nanoplatforms, the dispositions are the unique physicochemical properties of the nano-scale that can be actualized in a specific 'dispositif'. targeted drug delivery systems are 'devices' precisely because they activate dispositions. by contrast the term 'affordance' refers to the intrinsic capability of an object as well as to the gift or the service that the said object is able to deliver to its user (for instance thick ice affords skating)."
"paracelsus were really what their heirs have made of them. the master narrative built on the two heroes are still framing pharmacological culture. role of the physician was to restore what we call today the body's homeostasis by prescribing the opposite humour. while galenic medicine dealt with the organism as a whole and a nexus of relations, paracelsian therapeutics rests on the affinity of chemicals with specific tissues. the holistic concept of disease characteristic of the galenic tradition has been overthrown by an ontological concept of disease as an ens morbi, a material and localized entity."
"the consideration of communication technologies for providing wireless connectivity requires two key factors. one is the energy efficiency of the solution technology in connection to the lifetime of the batteries in the sensors, and the second factor is the communication range of the wireless technology. unfortunately, the existing solutions for wireless communication technologies in wqm applications are currently confronted by two key issues, namely the short range communication capability, as well as the high power consumption issue."
"in summary, based on the review presented by this work, the choice of a communication technology depends on factors that include cost, installation simplicity, reliability, and bandwidth requirements (or data rate) for a required qos guarantee. for efficient wq monitoring, it is important to note that a reliable communication technology is required. communication technologies such as ieee 802.15.4, wi-fi, and zigbee are mostly employed to transfer the measured wq data to a local monitoring station, while 3g, 4g, and gprs technologies are good examples of wireless communication connectivity for sending wq measurements to remote wq monitoring centers from a local monitoring station. the collected wq data by the local monitoring station can be used for the analysis of the local data, while the wq data obtained by the remote monitoring wq center is used for remote data analysis. the provision for the remote monitoring of wq is crucial as it facilitates a proactive and a quick response to any changes in wq due to contaminations. among the communication technologies that were reviewed in this work,"
"the main reason for the growing popularity of wsns in monitoring water quality is their advantageous and dynamic capabilities without any form of human intervention. consequently, they are regarded as essential tools that promise to bring interesting innovations to the field of wqm. conventionally, wqm is performed based on laboratory analysis, such that water samples are manually collected from different water sources and transported to water testing laboratories for the necessary chemical and microbiological analyses. unfortunately, laboratory-based strategies are often ineffective due to the operational cost involved, the human intervention required in the context of skilled personnel, and the failure to provide measurement results in a timely manner [cit] . to guarantee the supply of clean water needed for good health, there is a need to communicate reliable and real-time measurements of water quality parameters to water management personnel. a high level of pro-activity in combating any possible water pollution can be achieved. indeed, wsns have emerged as a good alternative to achieving the requirements involved in realizing modern wqm systems [cit] ."
"the modification of the boost converter does not necessarily introduce changes in the control strategy followed when a conventional converter is used, and virtually any control alternative for conventional converters found in the literature could be potentially applied. only the boost-current controller will require further attention as will be in-depth explained in section iii."
"the integration of wsn technology to wqm applications involves the deployment of water quality sensors at water fields of interest to monitor the desired water parameters and forward measured data to the appropriate quarters such as local water stations and remote water stations, in a realtime manner. unlike the traditional approach to wqm, a devoted wsn solution to wqm applications involve stages that include water quality data sampling, processing of data, transmission of data, storage of data, and data analysis for intelligent decision making. it is obvious that the stages involve in wsn solutions for wqm applications make them a more robust approach, as the need for human intervention is eliminated, including high cost and unnecessary waste of time."
"we use the same three classification algorithm as we used for the handcrafted features-based model but in cnn_features based model we do not use the handcrafted feature. we used the features extracted from deep convolution neural network alexnet. table 6 represents the results obtained by the classification algorithm using the feature set obtained from fully connected layer 7 of alexnet fc7. the dimension of features is 4096. result analysis shows that the neural network performed best for all arabic phoneme pairs as compared to knn and svm. svm performed better than knn and knn show the least accuracy as compared to svm and nn. experimental results show that the neural network achieves the highest accuracy of 83.58% on the fc7 feature set as shown in fig 7. svm achieve an accuracy of 78.48% and knn obtain 71.26% accuracy on 28 phonemes dataset. if we compare the result of handcrafted features with fc7 layer features, we discovered feature set obtained from fc7 is good as compared to handcrafted feature."
cnn feature-based model obtained an accuracy of 91.7% on 28 phonemes using the neural network as a classification algorithm. svm achieve the accuracy of 85.7% while knn achieves 75.2% accuracy
"after applying a narrow band-pass filter to the measured eeg oscillations, the resulting x t, θ t can be described by the additive model of"
"this requirement demands attention to make the dream of modern wsn solutions for wqm applications come true. the simple reason for the necessity of large communication coverage requirements can be attributed to the distances of water control centers and water monitoring centers to various water stations. in many cases, the remote water centers are situated in the range of kilometers away. so, for timely and reliable delivery of water quality application data to various remote water centers, large communication coverage is an important design requirement in the deployment of wsn solutions for wqm applications."
"narrowband (nb) iot is a lpwan cellular network based technology that was proposed by the third generation partnership project (3gpp) release-13, as a new promising solution for long range and low-power communication in future wsn and iot applications. nb-iot was designed to overcome the shortcomings of the legacy cellular network solutions which were not originally designed to suit wsn applications [cit] . one of the key reasons of the nb-iot technology is to extend the coverage range of lowpower end-devices (such as sensor nodes) communication over larger areas. the nb-iot solution can also be referred to as cat-nb1, which is a variant of the long term evolution in m1 category (lte m1). the nb-iot was designed and optimized to support the cellular network bands and co-exist with the legacy conventional cellular network technologies, at 180 khz minimum system bandwidth requirement. as a consequence, it is achievable to deploy a carrier of an nb-iot system into possibly a gprs or gsm network by a legacy network operator (such as gprs or gsm operator), by just dedicating one of their (gprs or gsm network) physical resource blocks (prbs) to nb-iot system bandwidth of at least 180 khz. this minimum system bandwidth represents one prb in the transmission of lte."
"so appealing was this popular image that it has been taken up by drug designers and gradually became a kind of ideal-type driving research efforts [cit] . the 'magic bullet' model underlies many research strategies that came to prevail in twentiethcentury pharmaceutical industry. [cit] s reinforced the paracelsian concept of disease as a local target to shoot. when systems of controlled drugdelivery came into use for a number of diseases [cit], the 'magic bullet' became a fashionable motto. [cit] s as pharmaceutical companies began producing monoclonal antibodies by cloning a unique parent cell having some specific action on tumours. it became even more prevalent with the proofs of concept of 'microspheres' [cit], 'nanokapsul' [cit], and 'nanopartikel' [cit] ) that penetrate into specific cells for drug, gene or protein delivery, due to peter speiser and his team at eth zürich [cit] . the 'magic bullet' is now integrated in more complex systems aimed at materializing the ideal-type of targeted therapeutics. the expectation is to shift from the painstaking drug screening-testing lots of molecules on all possible targetsto the direct design of the unique weapon adjusted to its target."
"matlab/simulink software package has been used for the simulations. discrete-time controllers have been obtained using tustin approximation, and they have been implemented in c language. therefore, the same control code runs in the simulation as in the actual digital signal controller (dsc). a pwm and sampling frequency of 20 khz is used for the boost-converter control. ad and pwm quantization and 3.5-khz antialiasing filters are introduced in the simulation model to match the experimental setup conditions. a pmsg model with the same parameters as the actual generator used in the experimental study has been used. the generator parameters can be found in table iii . the machine simulation model does not include cogging torque. fig. 4 demonstrates the current controller capability. a boostcurrent command is increased from 0 to 6 a in 1-a steps every half second when the generator rotor speed is fixed to 400 r/min. the boost-current controller bandwidth is tuned to 400 hz. fig. 4(a) shows the actual current i b while fig. 4(b) shows the current measured by the dsc (i.e., filtered, quantized, and sampled), i b dsc . the line thickness is due to the ac ripple present in the signals. both signals are similar and show a moderate ripple content at the harmonic frequencies present in v dc bemf . this is due to the voltage-command approximation (6). this is more clearly seen in the magnified view shown in fig. 5(a) . the switching harmonics are negligible due to the large equivalent boost inductance (twice the stator inductance). moreover, there is not a significant increase of the current ripple with increasing load, as can be seen in fig. 4(a)-(b) . fig. 4(c) shows the corresponding generator torque. it also shows ac harmonic content corresponding to the back emf harmonic frequencies. a detailed view can be seen in fig. 5(b) . however, in this case the torque ripple magnitude is load dependent, as can be seen in fig. 4(c) . however, this torque ripple is not expected to be an issue for the turbine speed control due to the large turbine inertia. the reason for this torque ripple increase is found in the q− and d−axes components of the stator current that show an increasing ripple magnitude with the load level [see fig. 4(d) ]. the measured and filtered rectifier voltage v r dsc can be seen in fig. 4(e) . when the boost current is zero and the power switch is open, the rectifier voltage equals the back emf voltage. the signal used for the turbine control is v r dsc whose detailed view can be found in fig. 5(c) ."
"metaphors are also vehicles for the transmission of meaning and values. like viruses, they contaminate and colonise our minds regardless of well-established boundaries such as science versus society. george lakoff and mark johnson emphasize that war metaphors are ubiquitous in the practices of arguing (we 'defend' or 'attack', 'gain' or 'lose', 'demolish') . they argue that not just our words, but our common world would be quite different had 'dance' been, for instance, the source domain of metaphors (1980, pp. 4-5) . metaphors do literally change the world. they belong to the realm of what john l. austin called 'performative utterances' [cit] . they are 'performative' and not 'constative'. more than describing reality, they inform and transform it."
"for mispronunciation detection using handcrafted features, we need to extract the handcrafted features from audio files. the list of features we used is listed in table 1 . we extracted volume 7, 2019 figure 1. methodology of our proposed framework."
where p i is the probability of getting the i th value while randomly selecting from the data set s. the highest information gain is used for creating the decision tree and is calculated as by (9) .
"the present paper analyzes the low-cost boost converter for small wind generators where the boost coil and the input filter capacitor are removed and the boost function is made by the generator-phase inductance. this can be safely made in systems using generators with a rated voltage similar to that of the dc link. this solution brings an immediate reduction of the size and cost of the system, as it has been shown. this paper demonstrates that removal of the boost inductance does not require any modification in the turbine-control algorithms or the sensed variables, but a subtle change in the boost current controller, making possible system retrofitting. moreover, the system efficiency and the generator-torque dynamic response are improved. as drawbacks, a moderately higher torque ripple and slightly higher generator losses at medium to high loads have been identified. the large turbine inertia makes the torque ripple increase negligible. the higher generator losses are largely compensated by the elimination of the boost-coil losses. the modification allows a better integration of the generator and the power electronics for small wind turbines. simulations and experimental results have proven the viability of the proposed controller solution."
"furthermore, these methods are not typically tailored to the detection of small objects in high-resolution images. proximity (usually, when less than one body length). as a trade-off to its computational efficiency, we did not 245 incorporate issues arising from a shaking camera in the mothe application. nonetheless, it can be used in 246 combination with image stabilizing algorithms to solve camera shake issues or could be resolved by smoothing 247 the trajectories after processing."
"3. we propose transfer learning-based method that automates feature selection and classification task. the rest of paper is organized as follow. in section ii, we provide a brief introduction of related approaches for mispronunciation detection than we provide the detail of the proposed methodology in section iii and after that, we present the results, obtained from experiments and discussion. in the end, we provide a conclusion along with future work based on our research findings."
"features gathered from different layers of the convolutional neural network are mapped on a training model that use features to predict correct or incorrect phoneme. we use three different classification algorithms to train the model namely k-nearest neighbors, svm and nn for phonemes mispronunciation detection. the classification algorithms are already discussed in the paper."
"the boost inductance and the input filter removal in the boost converter will introduce some changes in the boost-current controller, seen in fig. 1(b) . for the sake of clarity, a recall on the design of the current controller using the conventional topology will be first made."
"in the ul communication phase, ingenu employs a dsss based rpma modulation scheme at the mac layer to carry out various demodulations of the communication links from the sensor nodes to the base stations in a concurrent manner, using a random delay strategy [cit] . the dsss can be described as a variant of the cdma scheme that permits the sharing of a single communication channel (or time-slot) among several sensor nodes in the ul phase. to achieve the realization of sharing a single channel among various sensor nodes, the dsss apply a delay strategy to the sensor nodes that occupies the same channel, in a random off-set manner, to reduce the occurrence of any possible over-lapping among the sensor nodes [cit] . this technically means that the transmission of signals by the sensor nodes is randomly delayed. based on this strategy, the dsss increases the ratio of the signal-to-noise interference in the communication link of each of the sensor nodes."
where the sign function is 1 for positive contentions and 0 for negative contentions and y(k) is the time-space signal for r frame. the zero-crossing rate measures the signal noise. zcr is a time domain feature and is an exceptionally powerful and discriminative element to separate sound signs.
"consequently, it is important to take into account the aforementioned peculiar requirements of wqm applications during the process of network deployment. by taking such unique requirements into account, sustainable wsn systems will be implemented for wqm applications."
"the most widely recognized approach to compute the energy of a speech signal is the rmse, which is the square root of the average of the sum of the squares of the amplitude of the signal. the rmse e k is given"
mothe was able to detect all 156 individuals. we surmise that it should be able to detect even larger 250 numbers of individuals as long as the distance between individuals is at least one body length. in and demonstrations via github. we believe that this end-to-end package will encourage more researchers to manned aerial vehicles (uavs) and artificial intelligence revolutionizing wildlife monitoring and conservation.
"low power cellular-based technology: this section briefly examines an example of a new relevant wireless networking solution that can be employed in wsn solutions for wqm application data delivery. the wireless technology in this category is based on cellular-based infrastructures (such as 4g/3g/2g). consequently, unlike other technologies that employ proprietary infrastructures (i.e. gateways) to connect water quality sensors' data to the internet, the cellular-based technology solution eliminates the usage of gateways and connects water quality sensors' data directly to the internet. as a result of the direct connection to the internet in this category, the nb-iot solution is envisaged to be cheaper compared to its contemporary technologies such as lora, sigfox, and ingenu. an example of a communication network in the category of cellular-based technology that is promising for wsn systems in wqm applications is nbiot. other available solutions in the category of cellular based technology suffer from high power dissipation due to their high data rates, for example is the new lte-m (or cat-m1) network."
"when contaminated water is ingested, it may lead to the manifestation of chronic illnesses, accompanied by untimely death. as an example, in the case of micro-organism contamination, possible health issues include infections and gastrointestinal illness, as well as hazardous impacts on animals such as livestock. in the case of heavy metal contamination, possible health issues include failure of the renal system, damage of organs such as the intestine and liver, and cancer [cit], while the impact of water contaminated with heavy metals on the eco-system is devastating [cit] . water contamination due to nitrite and nitrate may cause illnesses that include the hindering of blood from carrying oxygen (methemoglobinemia) [cit] . apparently, a methemoglobinemia patient will be prevented from breathing normally, and this may result in premature death. for the sake of clarity, a taxonomy of water contaminants is presented in fig. 2 ."
"in the context of energy efficiency, the ble solution provides energy efficient data communication through the exploitation of low energy chipsets, including the implementation of optimized maximum data transmission and reception energy strategy [cit] . the new chipsets employed by the ble solution and the optimized usable maximum power helps to achieve ultra-low energy consumption at various operation modes (such as receive, transmit and idle) compared to the classic bluetooth solution. the ble solution implements a simple protocol stack that provides low-complexity communication links which supports easy and rapid connections. this interesting development makes ble radios to be energy efficient. the ble technology also incorporates energy efficient energy saving schemes that include low and adaptive duty cycling techniques that sleeps the ble sensors often and wakes them up at a scheduled time for necessary data communication. this strategy adopted by the ble technology helps to save a significant amount of energy. note that ble sensors could be woken up within a few ms, while the classical bluetooth solution requires a longer period in the ms range. to further minimize energy consumption, the duty cycle operation of the ble transceiver, as well as the length of data that can be transmitted may be adapted [cit] ."
"2 metaphors defined as acts of 'understanding and experiencing one kind of thing in terms of another', are ordinary cognitive phenomena rather than mere stylistic devices [cit] . as the source domain shapes the 'target domain', metaphors allow grasping and shaping hitherto-unstructured or not-easily-accessible realities. metaphors matter. they are powerful 'catalysts' in the dynamics of knowledge; they provide insights and generate new meanings and knowledge [cit] ."
the ranker method is used in combination with the feature evaluation method and ranks features by their separate evaluations. we set the threshold value to 0.02 that means all the values below that threshold is discarded from the ranked list of attributes and values above that threshold are retained.
"localisation and classification. the localisation step is performed using an efficient thresholding approach 122 that restricts the number of individual classifications that need to be performed on the image. the first stage 123 grayscale thresholding will output pixels that contain animals along with false positives (i.e. the locations in 124 the background that have a similar color profile to the animals). the classification at each location is then 125 performed using the trained cnn generated in the previous module. the outputs, detected animals, are in the 126 form of .csv files that contains locations of identified animals in each frame."
"the schematic representation of the inductorless converter can be seen in fig. 3(b) . in a three-phase passive rectifier, the two ac phases having the highest voltage and the lowest voltage are connected to the dc output. therefore, a dc equivalent model of the ac machine can be considered as seen in fig. 3(b), where a dc-equivalent back emf voltage v dc bemf can be given by (4) [cit], and the two conducting phases can be seen as a single coil with an inductance and resistance being the sum of both phases."
"fc-s and ds were both involved in implementation of the study, measurements, and discussions. zm is involved in proposing the methodology part of the study and analysis of data. kt has participated in the discussion of the results, future directions of the study, and contributing in the revision of the paper in its accepted form. all authors participated in the discussion of the results."
"the deployment of wsn solutions for wqm applications should consider a low-cost design requirement. this will enhance the global acceptance, mass-market and the popularity of wsn solutions that are dedicated to wqm applications. as a consequence, the design phase of wsn solutions should consider the total production cost for deploying a solution, while the aspect of energy efficiency is not neglected. once the energy efficiency aspect is compromised, the solution becomes useless no matter how cheap."
in equation (2) x lf represents the log-frequency spectrum. z represents the number of octaves; b represents integer pitch class (chroma) and β is the number of bins per octave.
"from (5), it can be established that the energy consumed to receive s bits of data from a transmitter is a function of the circuitry of the receiver r e, and the number of bits defined by s. consequently, the total energy spent by a sensor node to transmit and receive data can be modeled as (6) :"
"contaminated water is a carrier of dangerous components that may include micro-organisms, heavy metals, and chemical compounds. micro-organisms are characterized by bacteria, parasites, and viruses. an important indicator organism is fecal coliform [cit], with e.coli often used as the indicator organism of choice for microbiological contamination of water [cit] . water sources can be contaminated by micro-organisms through underground storage waste leakage (such as septic tanks), animals waste, agricultural run-off, and rainfall run-off. these processes allow micro-organisms to be washed into, or seep into, water sources. water sources can also be contaminated by heavy metals through leaching from natural minerals, cement plants, waste disposal (household and industrial), petroleum refineries, and mining processes. examples of harmful heavy metals are mercury, lead, copper, and arsenic [cit], which are poisonous. nitrite (no 2 ) and nitrate (no 3 ) are other types of contaminants, which are found in fertilizers, animals waste, and human sewage. their entrance into water sources through disposal or run-off introduces nitrite and nitrate into the water systems, which eventually results in water contamination. contamination by heavy metals, nitrite and nitrate affects the chemical composition of water. also, water can be contaminated through soil erosion, suspended solids, and oil spills. this type of contamination affects the physical characteristics of water. the summary of key parameters that can exploited in assessing the quality of water is presented in fig. 1 ."
"the applicability of the model to real measurements 1 . finally, we discuss the setting of the ks parameters and its potential use for future studies."
"the same simulation conditions explained for figs. 4 and 5 have been reproduced using the conventional converter in figs. 6 and 7. fig. 6(a)-(b) shows the actual current i b and the sampled current i b dsc . in this case, the actual current has a significant amount of ripple, while the sampled current is a clean signal. the reason is now that the ripple is due to high-frequency switching harmonics, as seen in fig. 7, which are not seen by the sampled signal. since the voltage command provided by the pi controller is correctly calculated by (2), there are no back emf dependent harmonics in the controlled signal i b dsc . the corresponding generator torque can be seen in fig. 6(c) . the average value of the torque is similar for the same boost-current level. despite the average current, i b dsc does not contain back emf dependent harmonics, the generator torque does, as can be better seen in fig. 6(b) . this is explained by the fact the q− and d−axes stator current components contain that harmonic content. however, when compared with the inductorless converter the torque ripple is not load dependent, showing a smaller magnitude at medium to large load. it can also be seen that switching harmonics are present neither in the generator currents nor the torque. this is due to the effect of the input filter capacitor. the rectifier voltage is shown in figs. 6(e) and 7(c) for reference."
"zigbee is a variant of a wpan technology. it was developed and managed by a group known as the zigbee alliance [cit] . the zigbee can be described as a low-power wireless technology that is used for short range communications in wsns [cit] . its standard supports the ieee 802.15.4 communication standard, and is defined by the specification of ieee 802.15. zigbee utilizes the free-license ism rf bands such as 2.4 ghz, 915 mhz, and 868 mhz for communication [cit] . because of zigbee's low-power characteristic, its transmission coverage spans less than100 m range, depending on the environment. for example, it may cover a distance of up to 100 m in areas such as rural and open space locations where there are less or no obstructions, while it may cover a distance that is typically less than 40 m in environments where there are obstructions, such as in urban settings. zigbee devices require line-of-sight, and they may achieve a maximum data transmission rate of 250 kb/s depending on the used frequency band. for example, when operating at the frequency band of 868 mhz, a data transmission rate of 20 kb/s is achievable, while at the 915 mhz and 2.4 ghz bands, data transmission rates of 40 kb/s and 250 kb/s can be achieved, respectively."
"24 affordances combine generic material dispositions and specific intentions and purposes. whereas a disposition is a latent property inherent to a substance or based on the laws of nature, affordance is relational and definitely not substantial. unlike dispositions, affordances are not latent potentials waiting for being activated; they do not pre-exist their actualization. affordances are created within and by the coupling of an agent and a material system; they are rendered possible by this encounter. affordances thus blur the potential/actual dichotomy, as well as the subject/object divide. affordances are objective instances because they are offered by the environment and subjective because the offer is generated with regard to a perceptive agent. affordances are hybrid entities-part constructs, part nature-whose causal powers cannot be disentangled."
"the detailed sequence of steps for mispronunciation detection using convolutional neural network features is shown in fig 2. firstly, we have gathered the audio dataset consisting of arabic phonemes from native and non-native speakers."
"in (3), t n denotes the number of periods a transmitter is turned on, e t defines the energy consumed by a transmitter's voltage-based oscillator and a synthesizer, t i represents the turn-on time of a transmitter, t k defines the energy consumed during the start-up time of a transmitter, e o means the output energy of a transmitter, r n accounts for the number of periods a receiver is turned on, e r is the energy dissipated by a receiver during data reception, r i connotes the turn-on time of a receiver, and r k is the energy expended during the startup time of a receiver. t i can be further described as the ratio of transmitted bits of data s, and data rate q. therefore, t i can be written as s q . it is worth mentioning that the number of periods a transmitter is turned on (i.e. t n ) or a receiver is turned on (i.e. r n ) is determined by the algorithms deployed at the operation layers of the wsn protocol stack (such as mac), and application requirements."
this claim is based on a number of research papers dealing with targeted drug delivery both in standard scientific journals and pharmaceutical research journals. our review of literature has been guided and completed by oral interviews and extensive discussions with three scientists working specifically research field.
"in equation (16), k represents the convolution feature map, mk is input maps selection, q ik is the filter, bk is the bias of feature map. the l represents the layers of convolutional neural network and f represents the activation function. the relu activation function is used to add nonlinearity."
"an important advantage of the inductorless converter can be seen by comparison of figs. 5 and 7. while the current control dynamics are similar in both cases [see figs. 5(a) and 7(a)], the generator-torque response is slower for the conventional converter, as seen in figs. 5(b) and 7(b). this can be explained by the fact that the boost current in the conventional case is decoupled from the generator current to some extent by the input filter capacitor."
"it is less a matter of questionning the performances of the therapeutic applications of nanotechnology than addressing the relevance of metaphors used in the design of nanovectors. however, it is not a mere clarification of words, disregarding concepts, material practices and ethical choices. richard rorty claims: 'it is pictures rather than propositions, metaphors rather than statements, which determine most of our philosophical convictions' [cit] ."
"in this paper, we proposed a novel framework for mispronunciation detection of arabic phonemes that aid in the language learning process. we proposed two methods with different techniques, cnn features based technique, transfer learning-based technique, and a baseline method i-e handcrafted features-based technique. in the handcrafted features-based technique, we used handcrafted features and used knn, svm, and nn for classification."
"importantly, the study presented in this paper serves as a complement to the existing studies in literature, with the aim of extending works that focus on enabling networking solutions for long range communication and low power capabilities in wsns for wqm. the existing reviews have not explored the possibilities of the newly emerging wireless technologies in the category of low-power and wide area network solutions in the context of their utilization and suitability, as the interface of communication for delivering water quality application data in wsn systems devoted to the monitoring of water quality. additionally, the few surveys in literature on new wireless networks in wsns for wqm applications have not considered the strategies for the deployment of the new solutions."
"proprietary technology: the wireless technologies in this category are designed for lpwan and are commercial solutions that are based on proprietary infrastructures such as gateways. as a consequence, the technologies in this category employ proprietary gateways to connect water quality sensors' data to the internet (or network server). the solutions include lora, sigfox, and ingenu networks. the proprietary-based wireless technologies are discussed as follows."
"the detailed sequence of steps for mispronunciation detection using transfer learning model is shown in fig 5. a transfer learning model for mispronunciation detection automates both feature extraction and classification steps. we do not use classification algorithms for this model while the convolutional neural network automatically learns the features and classify them to predict whether phonemes is correctly pronounced or not. first, we need to preprocess the data, and data preprocessing step for cnn feature model and transfer learning model are the same. the brief description of each step is provided in the following subsection."
"in the transfer learning model, we automate the process of feature extraction and classification. in this model, we only give spectrogram data as input and obtained mispronunciation detection results. transfer learning model learns from the input and provides us with the output. we fine tune the transfer learning process by varying the parameters (learning rate, learn rate factor and bias learn factor).we check the result by varying the learn rate from 1e-1 -1e-10, weight learn factor and bias learn factor from 10-100 and found that optimal results obtained on 1e-5 learning rate, minibatchsize size of 10 and at 90 weight/bias learn factor."
"one of the main incentives of the current study is to remove the spurious variations in ip for a more reliable assessment of phase information. we present a model based on a kalman smoother that models the variations of phase in a narrowbandpassed signal. we evaluated the model for synthetic signals with spurious and actual phase jitters. we added different level of noises to signals and evaluated the number of true and false positives as an indicator for correct detection of actual phase jumps. results show a significant improvement in reducing the number of false positives. the method is also applied on synthetic eeg signals generated as the superposition of sinusoidal waves with noise to assess the removal of spurious phase variations. inspecting on various settings the method is able to remove the rapid transitions in phase that correspond to a low envelope. in both cases of known and unknown underlying phase shifts, an estimation to the variance of signal and noise measurements has been presented. we use the same approach on ongoing eeg recordings for testing the applicability of the approach. the proposed approach shows success in removing the spurious phase variations corresponding to a low envelope."
"we compute the ip using the analytic form of the signal with the hilbert transformation. to have a meaningful interpretation of the ip, the signal is narrow-bandpassed around a certain center frequency of interest. a zero-phase forward backward iir elliptic filter is used to narrow-bandpass the signal. [cit], in many studies a fir filter is used to avoid phase distortion, however as the signal is narrowbandpassed, the order of the filter can significantly increase which leads to long transient response episode. the transient response are usually discarded for analysis. we therefore applied an iir filter which requires a much lower order than a fir filter and a zero-phase forward backward filtering ensures a zero-phase distortion."
"in practice, there are four key modes that define the possible operation of a transceiver, including sleep, idle, receive, and transmit. these modes fall under two states, namely non-active and active. when a transceiver is in a non-active state, then such a transceiver is in a sleep (or off) mode. when a transceiver is in an active state, it can switch between the idle, receive and transmit modes, and although idle consumes less power, comparable energy is expended in all these modes. importantly, the different levels of energy that are consumed in the various states could be advantageously exploited to conserve a sensor node's battery power by optimizing the operation of the transceiver through duty cycling, as no or little energy is consumed during the sleep mode."
"targeted drug delivery systems are flagship products of nanotechnology. being less controversial than military applications of nanotechnology, such \"nano-weapons\" encourage public acceptance of nanotechnology. reminiscent of the popular image of the 'magic bullet', current research projects in nanotechnology for drug delivery are pervaded by a host of warfare metaphors such as 'therapeutic missiles', 'nano bullets', 'nano-weapons', 'smart bombs', 'stealth kill', and 'targeted strike' without 'collateral damages' [cit] . such metaphors used in popular journals and scientific publications have shaped the conceptual structure of the research field."
"equation (4) models the energy spent by a transmitter to transfer data to a receiver. the first term in (4) defines the energy expended by the transmitter circuitry in association with processing the data, such that s is the bits of data and t e is the electronics of the transmitter. the second part in (4) describes the energy dissipated by a transmitter's amplifier t a in association with communication of data over a distance d, with an exponent of ϕ which range from 2 to 4, and is related to transmission path loss. the energy consumed by a receiver to collect data is modeled in (5):"
"this can be clearly seen both by their low magnitude (note the logarithmic scale) and by the relative magnitude difference observed between figs. 17(c) and 18(c), the second having an increased signal-to-noise ratio. these results confirm the analysis made by means of simulations."
"it is thus clear that the missile metaphor is over-simplistic and not sufficient to achieve therapeutic efficacy. more importantly, this metaphor conveys the misleading image of a kind of navigation system. unlike a homing missile, the nanovector is not heading straight from the injection site to the target. 18 it has a random and hazardous journey through the blood vessels until it meets its target and binds to it. 19 finally, the image of a weapon hitting a target is not even appropriate to describe the release of the drug. one major challenge is that along its journey through the blood vessel, the decorated carrier is modified by the milieu and even redecorated by it, for instance by dragging a variety of proteins forming what toxicologists name the 'protein corona'. the proteins in the serum form a protective shield, which alters the targeting capabilities of the device [cit] . what the tumour cells 'sees' is not the expected keys that fit in their locks. they 'see' a different and unreadable message [cit] . whereas an unambiguous lock-and-key response is expected from the target, the target actually affords a unique viewpoint on the nanoparticles, opening unexpected perspectives over unforeseen interactions."
"the network contains 5 convolutional layers transferred from alexnet and new adaptation layer fully connected layer, softmax layer, and classification layer. only new adaptation layers are trained on the spectrogram dataset to correctly classify the phoneme. to train the network for spectrogram dataset we pass training images that are 77% of the whole data to the network and provide some training options. the parameters used for training the network include several epochs (maxepochs), batch size (minibatchsize), learning rate (initiallearnrate) and validation frequency etc. we train the network using the batch size of 10 and learning rate of 1e-4. we use a maximum 6 epochs for training and validation frequency equals to 3. we use stochastic gradient descent with momentum (sgdm) as the optimization algorithm. this algorithm updates the weights and bias parameters by minimizing the loss function. the replaced last layers learn the features of spectrogram/new classes using these training data along with training parameters."
"unlike the existing surveys that focus mainly on wsn systems, there is a shift in paradigm in this survey to wsns for wqm. the focus of this survey is on the exploration of the newly emerging wireless network technologies and deployment strategies specifically for wqm application communication. this survey also considers the quality-of-service (qos) support of wqm applications, alongside the network deployment for wsn in wqm applications. the key contributions of this paper are described as follows:"
"the current controller shows a good dynamic response, as seen in fig. 5(a), but even more important, a similar dynamic response can be seen for the generator torque in fig. 5(b) ."
"a magnified view of the current step is shown in fig. 16 . the ripple observed both in the boost current and in the measured voltage is very similar to that predicted by the simulation results. by comparison with fig. 5(a)-(c) the degree of accuracy of the performed simulations can be noticed. it must be remarked that no offline processing has been done to the experimental results, the measured signals being directly stored in the dsc memory for later representation."
"the wireless technology solutions in the category of wireless lan (wlan) are typically employed to achieve medium range data communication, with coverage of up to a few kilometers. this section reviews both legacy and new solutions for implementing a lan setup."
"the new proprietary-based lpwan communication networks may encounter high data latency because of their co-existence with the existing legacy wireless technologies on the unlicensed bands. considering the critical nature of wqm application data to proactive actions by water management personnel, it will be good to improve the data latency of the proprietary-based lwpan solutions for timely delivery of wq data to various water control and monitoring centers. this will make future wsn and iot solutions for wqm applications that adopt lora, sigfox, and ingenu technologies more robust."
"20 of the tumour capillaries allows liposomes or nanoparticles to slowly permeate the tumour sites (fig. 6 ). this 'non-specific targeting' (fig.3) enables pegylated liposomes to reach tumours sites to deliver drugs while making chemotherapeutic drugs less toxic for healthy tissues. the doxorubicin-loaded liposomes doxil® with their size of about 100nm allow administrating a higher dose of doxorubicin while providing the patients with a better quality of life (no more nausea, vomiting or hair loss), although new side effects are also experienced 21 . doxil® totals almost $600 million in annual sales around the world and will soon lose its patent protection. new strategies for drug delivery are being envisioned that take into account the complex and dynamic nature of the biological milieu. in particular the modifications induced by the pathology and the treatment in the target at different space and time scales have to be systematically investigated. the tumor microenvironment is characterized by a high structural heterogeneity, multicellular composition, dense extracellular matrix, non-uniform leaky vasculature, continuously evolving interstitial pressure and solid stress, hypoxia and involvement of immune cells [cit] . multistep strategies are being envisioned including a first step to prepare the milieu (normalization of tumor vasculature or of the tumor matrix) and a second step to facilitate drug delivery to the cellular or intracellular target [cit] . designing nanoparticles that respond to properties of the tumor microenvironment (for subjected to mechanical pressure and friction, which causes instantaneous dilatation of the endothelial tissue of blood vessels which, similarly to the epr effect. yet this allows the nanocarriers to locally cross the endothelial wall of healthy tissues. the areas affected become red, dry, peel, numb or painful, with possible necrosis. this unwanted leakage of the nanocarrier can be attenuated by modifying some everyday activities (avoiding wearing tight clothes, using tools, jogging, taking hot showers or being exposed to strong sunlight). but for very sensitive patients such an adverse effect unfortunately limits the maximal safe doxil® dose that can be administrated as compared with doxorubicin in the same treatment regime. example low ph and partial oxygen pressure) is a major trend in the field of cardiovascular diseases. targeted drug delivery could also be 'pathology-inspired', that is, achieved through endogenous disease-specific mechanical stimuli. 22 here again the interactions of nanocarriers with the milieu, and more specifically with the diseased area, triggers opportunities of therapeutic activation. such stratagems engaging with the milieu as a partner promise to be more robust and widely applicable than the simple molecular recognition initiated with the model of magic bullet. 'intelligent design' [cit] of drug delivery could be based on a better understanding of interaction mechanisms between the biological milieu and foreign bodies so as to benefit from them."
"after data preprocessing step we extract the feature from deep convolutional neural network alexnet. alexnet, a pretrained convolutional neural network is trained on the imagenet dataset to deal with complex classification tasks on imagenet dataset, but it is also used to extract features from other datasets. alexnet consists of 5 convolutional layers, 3 fully connected layers and 3 max pooling layers as shown in fig 4. alexnet architecture is successful due to the rectified linear unit (relu) nonlinearity layer and the dropout regularization system. the relu, as appeared in (15), is a half-wave rectifier work, which can essentially speed up the training process and prevent overfitting."
"cmos technology is usually considered in the design of memory devices and micro-controllers because it is cheap due to economy of scale and offers a small form factor. examples of commercial micro-controller devices that may be deployed in water quality sensors are texas instruments (ti) series (msp430 [cit], msp 430f 16 1 1 [cit] ), arm series (arm9 [cit], arm cortex m3 [cit] ), and the series of atmel atmega (atmega 128l [cit] ], [cit], atmega 256rfr2 [cit] ). it is worth mentioning that the atmega series and msp430 micro-controllers from atmel and ti are often employed in commercial sensors [cit] . however, the msp430 micro-controller is preferred to the atmel atmega micro-controllers in terms of processing speed, cost, low-power, and ram memory: for example, the size of the ram memory of the msp430 micro-controller is 10 kb, while the size of the ram memory for the atmel atmega micro-controllers is typically around 4 kb [cit] . the memory of a micro-controller can be classified into three categories, namely a flash memory, a random-access memory (ram), and a read only memory (rom) or data memory. the sizes of these memories are typically within the range of kilobyte (kb) to a few megabyte (mb or m)."
"rom memory is used for permanent data storage, which implies that it is non-volatile in nature. this type of memory is only suitable for small data storage space, typically less than 4m [cit] ."
"as stated earlier w t+1 and w t are samples from a gaussian distribution with a variance of α :(n (0, αi)). due to symmetry of the gaussian distributions, the sum of two normal distribution is a gaussian with the summation of the means and the variances. therefore, in our case, the resulting distribution will be n (0, 2αi). by taking the variance of both sides in equation (10), we have"
"an example of a possible network topology that can be created with lora devices, which include sensor nodes, is star-to-star such that each sensor node in the network is connected directly to the gateway of the lora network in a single-hop fashion. in wqm perspective, the structure of a lora network will include a lora gateway, lora sensor nodes, lora server, and a lora remote device (such as a computer terminal). for the purpose of clarity, it is worth mentioning that lora-based water quality sensors are realized by buying an off-the-self lora module and embed it into the communication unit of a water quality sensor node to provide lora communications. the lora-based water quality sensors are used for monitoring the quality of water and disseminate their water quality data to the lora gateway using the pure aloha protocol. the lora gateway helps to gather the individual signals of the lora-based water quality sensors in a lora network and provides an internet backhaul role. the lora server performs the role of network management to the lora network server. the lora remote device is responsible for processing the acquired water quality data by the lora-based water quality sensors, and also carries out data analysis."
"are we to assume that targeted drug delivery may be seen as a reconciliation of the two alternative conceptions of disease and therapeutics? to address this question, it is necessary to characterize the material practices of design."
1. we develop an efficient algorithm for mispronunciation detection for arabic phonemes. 2. we propose a method that can select the most discriminative acoustic-phonetic features for mispronunciation using the deep convolutional neural network.
". minor changes due to noise or background eeg variations to the real and imaginary parts of the analytic signal (which are narrow-bandpassed) can lead to significant changes to the computation of phase as the numerator and denominator values are small [cit] and the references therein). hence, the aim of this study is to estimate the true phase using the ie and ip and have a more reliable measure of the ip such that the spurious phase variations due to a low envelope will be removed."
"where p mech is the mechanical power calculated using the average torque and the rotor speed, and p t are the total losses seen in fig. 11(d) ."
"(i) generation of training dataset -dataset generation is a crucial step in object detection and tracking. 104 generating enough data for training takes a lot of time if done manually. in this step, we automate the 105 data-generation. users run the command line code to extract images for the two categories i.e. animal and 106 background. it allows users to crop regions of interest by simple clicks over a graphical user interface and 107 saves the images in appropriate folders. on each run, users can input the category for which the data will be 108 generated and specify the video from which images will be cropped. outputs from this module are saved in 109 two separate folders one containing images of animals (yes) and the other containing background (no)."
"abdomen which corresponds to around 150 pixels in our videos. in figure 3, the first column shows the results of running object detection on these video clips and the 203 second column displays the results after implementing track linking on the detections. we observe that the 204 package is able to detect and track nearly all individuals in all types of habitat. however, as expected, there 205 are some errors in animal detection using mothe."
"sigfox is a lpwan based technology and is managed as a proprietary network. sigfox can be described as a low-power wireless technology that can be employed to realize end-toend long range communications in wsn applications [cit] . its standard supports the ieee 802.15.4 communication standard. sigfox employs an unb scheme at the phy layer. because of the shortcomings of the commonly used mac schemes, sigfox resorted at using the aloha scheme, which provides a random communication channel access service at the mac layer [cit] . similarly, a random fdma (rfdma) scheme may also be employed at a sigfox network mac layer [cit] . sigfox technology is suitable for transmission of small amounts of information in a line-of-sight fashion over the sub 1 ghz free-license ism rf such 868 and 915 mhz. sigfox devices can transmit at a low data rate of 100 b/s over the bands and can cover a typical range of 10 km and 50 km in urban and rural settings respectively [cit] . the possible coverage range is an indication that sigfox technology prospers well in rural areas where there are fewer obstacles compared to the case of urban settings. sigfox supports the third generation partnership project (3gpp) specification to enable ipv6 connectivity on sigfox lpwa networks. in the context of a sigfox structure, the provision of a gateway is required and determined by a network service provider. in the perspective of wqm application, the structure of a sigfox network will take elements that include sigfox sensor nodes, privately-owned base stations that incorporates cognitive software defined radios, and ip-based backend servers, into consideration. the base stations are connected to backend servers, which are based on ip networks. for clarity sake, sigfox water quality sensors can be realized incorporating a commercial solution of the sigfox into the communication unit of a water quality sensor node to provide sigfox communications."
"the current proprietary-based lpwan solutions support the pure aloha scheme at the mac layer. the pure aloha scheme does not include a clear channel assignment (cca) scheme, which is useful in minimizing packet collisions. as a consequence, the proprietary-based lpwan communication networks may encounter an excessive number of packet collisions. therefore, to improve the energy efficiency volume 7, 2019 of the proprietary-based lpwan solutions, a cca scheme may be combined with the pure aloha, like the case of the csma-ca scheme. based on this development, the network computational overhead for handling packet collisions is envisaged to improve."
"the journey of wireless sensor network (wsn) [cit] s, and has evolved rapidly, with remarkable impact [cit] . recent developments in silicon technology and wireless networks are key players in the evolution of wsns, including the increase in their popularity. wsns are indispensable tools in this present age. wsn technology is a core part of internetof-things (iot) paradigm that has transformed the world to a better place for human-kind via machine-to-machine communications which makes it possible for devices -such as sensor-enabled machines -to communicate without any form of human intervention. wsns have gained wide popularity in the field of environmental monitoring, specifically water quality monitoring (wqm) [cit] . the application area of wsns is not limited to wqm only [cit], they have also been employed to carry out critical monitoring in other application fields that include battlefield surveillance [cit], agriculture"
"computer-aided language learning (call) frameworks have gained much popularity because of the adaptability they give to enable students to refine their language abilities at their own pace. with the advancement of computer engineering and artificial intelligence, much research [cit] has been done to help call systems. a more particular call sub-region called computer-aided pronunciation training (capt) emphasis on areas such as recognizing an error in non-native speech. call perform many tasks like speech recognition, pronunciation scoring, and"
"in equation (6), l represents the length of the audio signal divided into k segments. x k represent the predicted speech sample of the k th frame whilex k representing the actual speech sample of the k th frame."
"this section discusses the differences between the inductorless converter and the conventional converter in the boostcurrent control, resulting generator torque, harmonic current content in the electrical machine, and efficiency. a boost converter with the parameters seen in table iv is used for comparison."
"the model in (2) is a function of the behavior of cmos transistor technologies employed in the design of microcontrollers. the first term of (2) relates to the dynamic power dissipation, where j stands for a switching capacitance, q d represents the voltage supplied to the processor, f is used to denote the clock frequency. the second part of the expression in (2) accounts for the loss in energy as a result of current leakage [cit], where i c denotes the leakage current, m is a constant defined based on the hardware of a processor, and q t represents the defined voltage threshold."
"summary of low power wan variants: without any iota of doubt, it is clear that the new wireless network solutions are capable of playing crucial roles in wqm applications, specifically in the areas of energy efficiency, long communication range, and a reliable delivery of wqm application data to remote water control and monitoring centers in a timely manner. it is worth mentioning that the battery life of the sensor nodes of lpwan solutions is around years. for example, the battery of sigfox and ingenu sensor nodes is approximately around ten years, while lora and nbiot sensor nodes' battery life is more than ten years [cit] . these are good energy consumption figures that indicate how promising the lpwan solutions are. the comparison of the reviewed lpwan solutions is provided in table 5 . other important considerations are cost and data latency."
"we can write the above expressions as a two dimensional real linear gaussian state-space model by separating the real and imaginary parts. in the following we takeŝ t,x t to be two dimensional vectors of real numbers consisting of the real and imaginary parts of the underlying complex number. with this convention we have:"
"the hardware architecture of water quality sensor nodes is composed of four key sections that include sensors, microcontrollers, transceivers, and power sources [cit] . these sections work collectively in water quality sensor nodes to achieve the objective of monitoring water quality parameters, as well as reporting measurement data. for instance, the sensor section is composed of the water quality application sensors and analog-to-digital converters (adcs) that work hand-in-hand to generate data about water quality [cit] . specifically, the application sensors (such as ph, e.coli, temperature) are responsible for collecting data about the quality of water in an analog fashion. the analog data is delivered through an analog front end to the adc, which is responsible for transforming the received analog data to a suitable digital form for the micro-controller. the micro-controller performs the function of a processor, coordinates the entire sections of a water quality sensor, and integrates a memory for data storage. as an example, it is responsible for collecting water quality data from the adc of the sensor section [cit] . the collected data can be stored in the micro-controller's memory and transferred to the neighboring sensor nodes through a transceiver device [cit] ."
"the lpwan solutions offer a novel paradigm in data networking. the solutions are suitable for reliable data delivery at low-power rates [cit], and promises to offer what the traditional networks lack (such as low-power and long communicate range). with lpwan networks, water quality sensors can be linked to the internet by using a proprietary-based technology solution [cit], as in lora [cit] and sigfox [cit], for example, or a cellular-based technology solution (such as nb-iot). the aforementioned two categories of wireless network solutions are discussed in the following subsection."
"the resulting system is like a 'mille-feuilles' of disciplines comprising of various layers: synthetic chemistry, physics, biochemistry or molecular biology (figure 5). like most nanoengineering projects the design of nanocapsules requires the convergence of various disciplines at the nanoscale. to a certain extent the nanocarriers are crafted like 'materials by design', they are then functionalized for performing specific tasks. it should be clear by now that the resulting approach to therapeutics belongs neither to the galenic culture of pharmacology nor to its paracelsian rival. in opposition to the paracelsian approach, the chemical agent is by no means the unique active principle of the medicine. it is part of a larger technological system combining various functions that all cooperate to the therapeutic action. conversely, the galenic formulation becomes so functionalized at the nanoscale that it is no longer possible to draw a clear boundary between the excipients and the active principle [cit] . the formulation is an integral part of the therapeutic system. 15 targeted drug delivery systems do not fit in the traditional categories of pharmacological culture and knowledge. they instantiate a technoscientific approach dominated by an abstract engineer's view 16 . even the performances that are expected from them are formulated in the language of technology assessment (cost/benefit analysis). each of the functions added to the nanoplatform should increase the ratio of benefits versus costs and be assessed under this model [cit] . 15 the nanocarriers obtained by 'squalenization' by couvreur and his team instantiates the identification of the drug with its galenic formulation. squalene, a biological organic compound, can chemically bind with the anticancer drug gemcitabin, thus forming a new molecular entity, gemcitabine-squalene, which in turn selfassembles into nanoparticles in water [cit] . 16 on the difference between abstract and concrete engineering views in nanotechnology see bensaude-vincent and guchet (2007, pp. 79-82) ."
"the missile metaphor is attractive because of the translation of a therapeutic project into an engineering problem with clear end (cure cancer) and means (nanocarriers for the transport and stimuli for the release of the medicine). however, is the ballistic metaphor sufficient, is it even necessary? what kind of model is required to combine technological efficiency with therapeutic efficacy?"
"probability-based strategies can recognize the pronunciation quality, yet these scoring calculations are not enough to distinguish the nature of mistake and correct location of that error. therefore, classification-based mispronunciation detection methods are used for this purpose."
"the design of the experiment was planned and in accordance with ethics guidelines and the declaration of helsinki and the study was also approved as a scientific study by the local ethics committee [arztekammer des saarlandes (medical council of the saarland), germany]. every person had the free choice to abandon the procedure and withdraw their participation at any time."
"the flash memory is usually an external memory, while the rom and ram memories are internal memories [cit] . a flash memory is a special type of memory employed for providing additional and general-purpose storage in the sensor node architecture, and belongs to the family of the electrically erasable programmable read only memory (eeprom). it is advantageous in speeding up the rate of operation of a micro-controller and is not volatile. no energy is needed to hold the data stored in a flash memory, unlike in a ram memory, and as a result it can be used by a micro-controller device to provide either a permanent or a temporary data storage service due to the limited storage capacity of the internal memories. depending on the type employed, flash memory may provide a storage space of about 8 kb to 8 mb [cit] . an example of a flash memory device that can be incorporated in a sensor node architecture is at45db from atmel [cit] ."
"changing obstacles into positive principles of work is exactly what the french philosopher gilbert simondon recommended for the design of 'concrete' machines as opposed to 'abstract' ones [cit] . for simondon, a 'concrete engineer' is one who pays attention to, and takes advantage of, the environment affordances. unlike the abstract engineer, a concrete engineer does not aim to improve the machine's efficiency first, to introduce it ultimately into its work environment. but instead, by imagining the machine in its environment and 'playing the milieu', the concrete engineer anticipates the effects of its operational effects on the environment, and then tries to integrate them into the machine's working principles. a concrete nanomachine would work precisely because of-and not despite-its association with its environment, which becomes the machine's 'associated milieu' and not an external parameter that engineers have to take into account after designing artefacts. simondon's 'associated milieu' is not something to which a standard, ready-made machine will have to be adjusted. it is an intrinsic aspect of the design of the machine. it means that each ingredient of the complex system is envisaged as a relational entity defined by its interactions with the environment rather than a physicochemical entity with a stable identity (table 1)."
"the associate editor coordinating the review of this manuscript and approving it for publication was prakasam periasamy. monitoring and precision agriculture [cit], intelligent transportation [cit], industrial monitoring [cit], and smart homes [cit] ."
"brigitte nerlich has suggested that gibson's notion of affordance could help conceptualize metaphors themselves in ecological terms. 'an ecological theory of metaphor would study the \"structural coupling\" between a metaphor and the environment (…). over and above its intrinsic semantics [the metaphor] therefore has a \"pragmatic\", dynamic, action-oriented face' [cit] ). if we assume, with nerlich, that metaphors do have affordances, depending on their environment or 'niches', then we have to have to acknowledge the 'magic affordances' of the 'magic bullet' and the 'therapeutic missile'. first, in the current context of crisis in pharmaceutical industry the missile metaphor affords public confidence. the underlying values of control and precision are more socially acceptable than the view of a nanovectors seizing opportunities in the course of a random journey through the body. missiles and nanorobots are more reassuring for patients and more convincing for investors and industrial companies."
"in this section, we apply the proposed model on few examples of an eeg recording (see figures 12, 13) . the recording measurements are obtained from the right and left mastoid electrodes that have been obtained during an experimental listening paradigm (for more information about the data measurement and the details see corona- . the eeg signals were bandpassed between 1-70 hz. in order to test the applicability of the model, we need to narrow-bandpass the signal and then estimate the ip of the filtered data."
"also, the ieee 802.11ah architecture presented in fig. 11 can be employed to realize long range and lowpower communication in modern wsn solutions for wqm applications in two manners, namely through a direct internet connectivity strategy or through an indirect strategy. the direct strategy can be realized through the adoption of an ethernet solution, while the indirect strategy goes through a cellular gateway. volume 7, 2019 figure 11. proposed architecture of a modern wsn-based solution for wqm applications using lpwan solutions and ieee 802.11ah wireless network."
"the boost current needed to brake the wind turbine will be injected into the dc link seen in fig. 1(a) increasing the dclink voltage. the h-bridge inverter controller seen in fig. 1(c) regulates the dc-link voltage using a cascaded control structure. the inner loop controls the current injected into the grid using a proportional-resonant (pr) controller, while the outer loop controls the dc-link voltage using a proportional-integral (pi) controller improved with a notch filter. the dc output of this controller, i * g rms, is converted into an ac reference for the pr current controller i * g using a grid synchronization block (i.e., phase-locked loop). the h-bridge controller does not require any modification by using the inductorless converter. it must be remarked that the converter modifications only affect the boost converter. therefore, this solution can also be applied to step up battery systems."
"in this section we generate additional synthetic signals for testing the effect of spurious phase variations. the synthetic signals are generated as a superposition of sinusoidal signals and noise with different amplitudes 4 . the signals has been narrowbandpassed using a fir filter to a center frequency of 7.6 hz. the goal is to analyze the effect of the ks on removing the spurious phase variation for regions where the envelope is low. in this case, we considered the ip of signal corresponding to envelopes below 0.2 to be noise, and therefore need to be removed."
"wsn systems for wqm are characterized by stringent quality of service (qos) requirements that include reliable and timely delivery of wqm application data over long distances to various remotely located water control centers, while maintaining energy efficiency. wsn system criticality in the context of wqm monitoring ensues from the impact of water on human health, as any water ingested has a direct influence on health. incorporating wsns in such applications requires robust and reliable data transmission. however, the existing wsn solutions for monitoring water quality are plagued by limited energy, low computational performance, insufficient data storage, and communication capability issues [cit] . these issues have attracted the attention of industry and academic research communities in recent years in attempts to find solutions to the problems associated with wsn systems. typically, water quality monitoring wsn systems employ sensor nodes that incorporate batteries. unfortunately, the batteries used for powering water quality sensor nodes cannot have large energy capacity because of a portability size constraint, as well as cost factors. the tiny batteries that are accommodated severely limit the power budget, and it is expedient to ensure efficient use."
"bluetooth technology was developed by the ieee 802.15.1 [cit] . however, the bluetooth special interest group (sig) alliance [cit] has presently taken over the maintenance of the technology. the group is equally responsible for defining and managing the technical specifications of bluetooth technology. bluetooth is a wpan-based technology that was intentionally proposed to replace the usage of wires in mobile applications. it employs a gaussian frequency shift keying (gfsk) scheme or a frequencyhopping spread-spectrum (fhss) scheme at the phy layer. interestingly, the conventional bluetooth technology has recently enjoyed widespread acceptance as a means for wireless connectivity in systems such as wsn solutions for wqm applications. this can be attributed to its cheap operational cost and high-rate features. this feature makes it a suitable candidate for wsn systems that specifically require a high data transmission rate. bluetooth is a low-power wireless technology that is used for short range communications in wsns and can cover a distance that is typically up to 100 m. it is an ip compliant wireless connectivity solution and supports the ieee 802.15.1 communication standard [cit] . examples of network topologies that can be created with bluetooth end-devices are star, and peer to peer (p2p). unfortunately, this technology is limited by the number of devices that can be connected at a time, which are typically two. one of the devices is configured as a master, while the other is a slave. bluetooth devices operate at the 2.4 ghz free-license ism band, and can achieve a typical data rate transmission of 1 mb/s on the band. because of the high transmission rate of the conventional bluetooth solution, it suffers from higher energy consumption as a drawback. due to the high energy consumption of bluetooth devices, which is typically more than the energy demand of its counterpart solutions such as zigbee, a newer variant of the bluetooth device known as the bluetooth low energy (ble) technology as defined by bluetooth version 4.0 [cit], has recently been proposed to circumvent the energy consumption constraint of the conventional bluetooth, and at a lower cost. the new ble device supports both the gfsk and fhss modulation schemes of the conventional bluetooth solution, and also uses the same ism frequency band. also, the 802.15 group has proposed a new communication model named as bluetooth smart for defining new structures for the ble devices, to ensure its efficient deployment, and to improve its communication coverage for wsns. the new technology offers massive connections of ble devices. for example, the technology allows about 5,917 ble slave devices to be connected to one ble master device [cit] . as an example, the new ble technology can provide a data rate of 1 mb/s, covering a typical distance of 200 m. the energy consumption of ble devices can be controlled by setting the communication cadence, which could be in the range of 7 -32 ms, between the ble devices configured as slave devices and a master device [cit] ."
"when the boost-current level increases, the low-frequency harmonic content relatively decreases compared with the fundamental waveform, as can be seen in fig. 9 . it shows the same signals as fig. 8 with a boost current of 3.7 a (rated). however, the inductorless converter exhibits a smaller reduction. this is in agreement with the larger torque ripple seen as the boost-current level increases. nevertheless, there is not a dramatic difference between both converters. the high-frequency harmonics remain at a similar relative level, as can be seen in fig. 9(c) ."
"there is need to develop novel analytical models to investigate and improve the energy consumption performance of the proprietary-based solutions (sigfox, lora, ingenu) for the next-generation of wsn solutions for wqm applications, as there are few analytical models on the aforementioned solutions at present. the analytical models will further facilitate the growth and popularity of sigfox, lora, and ingenu, as they are crucial for designing optimization problems to improve on their performances. the few analytical models in literature focus on communication range, while power consumption consideration is also essential for analytical studies."
"third, the over-emphasis on the dispositions of nanoparticles is also important for patenting purposes. despite significant extensions over the past decade, the patent system still relies on claims of invention, and thus on artefacts. since drug delivery research is mainly patent-oriented, it must avoid putting the emphasis on processes already performed by nature. hence the idea that drug delivery research in comparison with other fields of nanotechnology only contains a small proportion of biomimetic stances-mostly limited to the 'trojan horse' stratagems exemplified by viruses and phage."
"to give a better idea on the size reduction and cost reduction that this modification can bring to the system, a design study has been conducted. the boost-coil design conditions shown in table i have been used as base figures for our system for the design of an e-core type coil, as seen in fig. 2 . the final size will depend on the goal inductance that will be selected according to the allowable ripple in the boost current. table ii summarizes the dimensions, weight, and coil for inductance values ranging from 1 to 10 mh. the costs of the core and copper have been calculated for reference using retail prices. manufacturing costs are not included."
"for instance, in today research programs on drug delivery, the process of opsonisation of the carrier is treated as an obstacle to be overcome, whereas in an 'oïkologic' framework, it would be treated as an affordance of the milieu. all the responses of the biological milieu to the introduction of a nanocapsule could serve as models of design. as pointed out by richard jones, nanotechnology could learn a lot from nature by investigating in vivo molecular 23 to attribute a dispositional property to a thing amounts to saying that if certain conditions are obtained, then that thing will behave in a specific manner or bring about a specific effect. for instance 'a negatively charged particle is one of which it is true that, if brought into proximity to another negatively charged particle, it will experience a force of repulsion' (harré 2001, p. 97) . 24 in gibson's ecological theory of perception, affordances are the possibilities of action that are offered to an agent by an environment [cit] . the concept has also been taken over in design theory, to express how objects invite and constrain their users by offering 'cues for action' [cit] . it has recently been used by rom harré to develop a pragmatic account of scientific experimentation which consists of 'apparatuses/world complexes' affordances (harré, 2003) . apparatuses/world complexes afford things, processes and activities that cannot be constituted nor sustained independently from technical projects."
"the interconnection of the sensors and a bs may be realized through the employment of short range wireless technologies, while the interconnection of a bs and remote centers require long range communication technologies. an architecture of a typical wsn solution configured with both local communication and remote communication for a typical wqm application is illustrated in fig. 5 . an insight into the understanding of communication media for data networking between the sensors and the bs, as well as the bs and the remote wqm application is discussed in the subsequent section"
"the mac layer takes care of the uplink communication and involves the sensor nodes transferring their signals to a base station they are connected to in a network. the ieee 802.15.4 standard employs a carrier sense multiple access based collision avoidance (csma-ca) modulation mechanism as the mac protocol to manage channel access, while a direct sequence spread spectrum (dsss) modulation scheme is employed at the phy layer, as well as at the data link (dll) layer."
"for the cellular-based solution described in fig. 11, it is based on the utilization of the nb-iot solution, and the nb-iot water quality sensors are connected to the base stations of cellular networks. consequently, they are connected to the internet cloud via a cellular system's network core. it is clear from the architecture in fig. 11 that the nb-iot wireless network circumvents the need for gateways, compared to the proprietary-based solutions."
"wimax is a high-speed (or data rate) communication technology that is based on wireless broadband technology. wimax communication technology is defined according to ieee standard 802.16, and is compliant with the 3gpp specification. based on ieee 802.16, wimax is suitable for fixed devices. to make wimax suitable for mobile devices, a new ieee standard is defined as 802.16e, to allow them to be connected to the internet. in practice, the technology is employed as a substitute to the dsl and wired technology to provide wireless internet access to wireless applications [cit], and is compatible with the networks of various radio access models to ensure its compliance with ip networks. wimax transmission rate can go as high as 80 mb/s, and it can cover a distance of 50 km [cit] . two types of license plans and propagation requirements are possible in wimax technology. the two types of possible license plans are free-license plan and paid license plan. the free-license plan is suitable for non-line-of-sight propagation (nlosp), while the paid license is suitable for line-of-sight propagation (losp). with the free-license plans, wimax operate within the frequency bands of 2 -11 ghz of the ism to provide internet connection to sensor nodes. information transmission in this category can reach 50 km. with the licensed plan, wimax technology operates in the range of 10 -60 ghz. it is worth clarifying that wimax services are mostly provided by network service providers."
"the role of phase synchronization of ongoing oscillatory activities has been deeply discussed in the area of erp generation as well [cit] . the phase modulation view of erp genesis states that the generation of evoked-related potential (erp) is not independent from the background (ongoing) eeg activities [cit] and the reference therein for more details). it is assumed that erps are generated by the re-organization of stimulus induced phase resets of ongoing eeg rhythms [cit] . thereby, the erp generation is not solely based on superposition of evoked, fixed-latency and fixedpolarity responses that are independent from the ongoing eeg activity [cit] . based on this definition, the background eeg activity comprise an important part of the erp generation process. this view is also referred to as phase modulation (pm), in contrast to the classical view, namely the amplitude modulation(am). figure 1 illustrates the classical view of erp generation against the phase-resetting model."
"(7) is a simplified energy cost model that can be employed to find the energy dissipated by a radio to transmit data of s bits to a receiver at a distance of d, as well as data reception. for the purpose of clarity, the comparison of the energy dissipated by the different systems of a sensor node is depicted in fig. 4, and shows the disparity in the various levels of energy consumed by them. from fig. 4, it is apparent that the energy cost of data communication is expensive compared to sensing and processing activities. also, the transceiver wastes a valuable amount of the battery power through idle listening, over-hearing, packet control, and collision issues during the data communication process of a network's sensor nodes. the aforementioned issues have contributed to the energy resource problems in wsns. as a consequence, energy efficient radios (or communication networks) and mac protocols are crucial to minimize the energy cost for data communication in order to optimize the battery power."
by using the first order markov property that the current data at time t is independent from the past given the state at timeŝ t .
"in figure 8, the average number of false positives (fp) and false negatives (fn) for different number of change points have been plotted. [cit] batches of data with different levels of noise and reported the average number of fn and fp. after applying ks, the average number of fps is significantly reduced. however, the case with no filtering yields very unstable results as the noise level increases. the average false negatives is however lower for the case that we apply no smoothing compared to results after ks. this is mainly due to the fact that more random changes are detected in the pre-smoothing condition. therefore, as many indices will be assigned correctly as change points, satisfying the minimum distance criteria. in the case of smoothing, the overlap of a a coefficient of +1 indicates a perfect prediction (i.e., in our case a perfect detection of change points at the correct indices), zero indicates no better than random assignment of change points and a coefficient of -1 means a complete disagreement between the predicted change points and the actual ones. figure 9 shows the average mcc for different snrs for varying change points n. the mcc significantly decreases as the snr increases (as noise level decreases), indicating a random assignment of change points. this is also consistent with the average results of falsely detected change points. as the number of falsely detected change points increases, we have a more random assignment of change points. this is however not the case for the post-ks condition. the higher average of mcc indicates a significant improvement in the accuracy of detected change points."
"where l b and r b are the inductance and the resistance of the boost coil, respectively, and s is the laplace variable. a pi controller with gains tuned using zero-pole cancellation provides the desired closed-loop bandwidth (3). the controller output being the voltage across the inductance command, v * l, as seen in fig. 1(b), which is converted into a duty cycle value by finding d in (1), since v r and v dc are known (i.e., measured) magnitudes"
"to evaluate the performance of our proposed methods, we use accuracy as an evaluation parameter. the mispronunciation detected by our models is compared with actual labels assigned by the language expert to evaluate the accuracy of a model. accuracy can be defined as in (21) . where m represents correctly detected pronunciation mistakes and t represent the total detected pronunciation mistakes. the performance of the handcrafted feature model and the cnn feature model is evaluated for each classifier and compare the accuracy of each classifier to find the best classification algorithm. the performance of the cnn feature model is evaluated for each classifier with different feature set gathered from different layers and compare the accuracy of classifiers obtained for different layers to filter the best feature set for the classification algorithm. the performance of transfer learning model is evaluated with accuracy and draw a confusion matrix to compare the mispronunciation detected by the algorithm and actual mispronunciation detected by experts. to evaluate the performance of the three models we also draw receiver operating characteristics (roc) curve to compare the results. roc curve is sensitivity versus specificity and filters the most promising mispronunciation detection model."
"what is exactly the descriptive power of the therapeutic missile metaphor (its adequacy) in nanomedicine? what about its prescriptive potential: does it shape research practices in this field? this metaphor suggests that new therapeutics can be framed along the model of ballistics. the objective is to carry and deliver the drug onto a target in order to optimize its 'launching window'. precision guidance, efficiency without 'collateral damages', … the advantages of nanovectors are listed in terms similar to those used to describe the performances of missiles or drones in modern warfare. remarkably, the source domain in turn makes an extensive use of medical analogies such as 'surgical strikes', a metaphor used in warfare strategy and communication during the first gulf war. no matter whether you want to heal people or kill them, no matter whether your action is good or bad, the ultimate values are control and precision."
"features data obtained from audio files contain missing values. to deal with data sparsity issue we fill the missing values, first, by marking the missing values using numerical cleaner filter. the numerical cleaner filer cleans the numeric data and replaces the values that are too big or too small with the default value. after detecting and distinguishing the missing values, these are accredited by a filter that replaces them with the mean estimation of the data distribution."
"both long range and low-power are essential features in wsn for wqm applications. to meet the aforementioned requirements, the ieee 802.11ah is another promising communication network since the ieee 802.11ah network optimizes its battery life, data transmission rate, and communication coverage. however, the ieee 802.11ah communication coverage is not as large as the lpwan communication networks, and also consumes more power compared to the lpwan solutions with their more moderate data transmission rates. for ieee 802.11ah to fulfill its potential in this application calls for more improvement in the energy efficiency."
"it is now clear that the exclusive focus on the nanoparticle (the vehicle) conceals many complex phenomena associated with transport through a living body [cit] . it reduces the spectrum of possibilities by excluding other potential perspectives. while the image of a missile provides insights, it also generates new forms of ignorance precisely because of its evident simplicity. in particular the offensive rhetoric undermines a subtle finetuning between a variety of mechanisms of protection or even of care. first, the biological tissues are protected from the drug's toxicity thanks to the container, which is in turn protected from the body's defences by the coating. as a matter of care, targeted drug delivery requires a sense of tact and diplomacy. the so-called 'missile' has to be also a 'secret agent' in order to infiltrate into a series of biological milieus and manipulate the interactions between various protagonists. like the trojan horse, the nanocapsules conceal their operation by adopting the codes of the infiltrated milieu. the so-called 'missile' has to negotiate access like a good diplomat. instead of subjecting all obstacles along its path, it has to establish temporary alliances with various smugglers. nanovectors require the tricks of a wizard and the special talent of kaïros. this persona of ancient greek mythology, the son of athena and grand-son of zeus and metis, is good at seizing opportunities. he epitomizes the art of making decision in the right place and at the right moment. this art consists in seizing what the situation affords for performing the next step in an action."
"various tuning parameters of the network, for e.g; number of nodes, size of nodes, convolutional layers etc., are 116 fixed to render the process easy for the user."
"(iv) track linking -this module assigns unique ids to the detected individuals and generates their 128 trajectories. we have separated detection and tracking modules so that the package can also be used by nests of ropalidia marginata are sites for social interactions between mobile adults as well as between adults and immobile brood [cit] . these nests are made of paper, which offers a low contrast to the dark-bodied social insects on the nest surface. nest comprises of cells in which various stages of brood are housed and thus add 172 to the heterogeneity of the background. additionally, different nest colonies differ in the age composition of 173 individuals, contributing to the variation in the appearance of wasps across videos. therefore, this system 174 too presents challenges to classical computer vision methods used to detect animals from the background."
"this section presents the architectural design and network deployment of wsn solutions for wqm applications using lpwan variants (proprietary-based and cellular-based) and ieee 802.11ah solutions for the realization of a reliable delivery of water quality data to remote water stations over long distances at a low-power rate as shown in fig. 11 . the reason for considering the solutions is because they serve as the representative network solution for long range and lowpower communication. fig. 11 shows how an lpwan network can be designed to achieve long range and low-power communication in modern wsn solutions for wqm applications. as depicted in fig. 11, the proprietary-based lpwan solution is connected to an internet cloud through a gateway since the solutions in the category does not define schemes for the higher levels, they only cater for only the mac and phy layers. as a result, the proprietary-based solutions cannot be directly connected to the internet, except through a gateway."
"the use of nanoparticles for delivering drugs on a specific site is one of the most attractive promises of nanotechnology today. targeted drug delivery systems are nanostructures tailored to deliver pharmaco-active molecules just where it is needed [cit] . the major advantage of drug vectorization is that it reduces the systemic toxicity of the medicine, thus minimizing side effects while improving its efficacy [cit] . in addition, this technique allows treating body parts that were once out of reach from most medicines (e.g.: the brain). finally, the innovation is also sought for 'rescuing' or 'repurposing' drugs that were shelved for being too toxic."
"svm yields an ideal hyperplane which orders the information by labeled classes. svm is most widely used for binary classification. we used one versus one approach in this research work. we also standardize the data before applying the svm algorithm. our dataset consists of arabic 28 phonemes and we deal with 6 confusing pairs, so data related to confusing phonemes is extremely similar and difficult to categorize. to classify the input data, svm first recognizes the support vectors ci, weights w gi, and bias b . it utilizes (11) to classify the data"
"to find the optimal number of epochs we take a phoneme pair ( ) and execute the transfer learning algorithm on 6, 10, 15 and 20 epochs. fig 12 represents the training process of the phoneme pair ( ) on 6 epochs. blue line represents the accuracy of training data while black dots represent the accuracy of testing data. we apply the transfer learning algorithm on phoneme pair ( ). we obtain an accuracy of 59.51% for phoneme pair ( ) on 6 epochs. ) on 10 epochs. blue line represents the accuracy of training data while black dots represent the accuracy of testing data. we apply the transfer learning algorithm on phoneme pair ( ). we obtain an accuracy of 71.81% for phoneme pair ( ) on 10 epochs."
"in the indirect communication architecture, the sensor nodes are connected to a satellite through gateways, consequently, the sensor nodes in an indirect satellite-based architecture gain access to the satellite via the employed gateways. basically, there are two categories of gateways in an indirect satellite-based architecture, namely fixed gateways and mobile gateways. the fixed gateways are installed on ground stations, while the mobile gateways can be realized through the adoption of ships and unmanned aerial vehicles. in both cases, the water quality sensors, which may employ zigbee radio, are mostly connected in a multi-hop pattern to a sink node (or a gateway) that is responsible for gathering the measurements of the sensors in each cluster. in practice, no less than two-hops is required. the gateway is responsible for forwarding the aggregated data through some of the existing satellite communication technologies such as orbcomm [cit], iridium [cit], o3b satellite [cit], the digital video broadcasting s2 [cit], to the satellite in the space. the architecture of an indirect satellite-based wsn solution for wqm applications using a fixed type of gateway and a mobile type of gateway are presented in figs. 9 (a) and (b) ."
"in the context of energy efficiency, nb-iot attains energy efficiency through energy saving mechanisms that include a prolonged discontinuous data reception, an internal interrupt enabled duty cycling strategy, and modulation schemes [cit] . as an example, nb-iot uses a single carrier fdma (sc-fdma) modulation scheme at the mac layer for ul communication, while an ofdma scheme is employed at the phy layer for dl communication, as in the lte [cit] . the schemes employed are capable of handling efficient allocation of radio resources [cit] . in the perspective of wqm application, it is worth mentioning that the nb-iot structure will include four key elements, namely nb-iot water quality sensor nodes, base stations of cellular networks, an internet cloud platform, and wqm application servers. the nb-iot water quality sensor nodes are built by adding the commercial solution of an nb-iot module to the communication unit of a water quality sensor. the module is further connected to core elements necessary to provide an nb-iot communication in an nb-iot network. such elements are an antenna and a sim card."
"a: data cleaning data cleaning is a process to 'clean' the data by smoothing noisy information, distinguishing or evacuating anomalies, and settling irregularities. during the data cleaning process, repeated records, spelling mistakes, and implausible information are distinguished and removed. we also removed the noise from the audio signal."
"in this section, we briefly explain the effect of spurious phase variation followed by describing the proposed approach for removing the phase slips. in the second part of this section, the experimental setting and the measuring procedures are explained."
"the power section is the most important of all the sections in water quality sensors, and plays the role of energy supply to the sensor node components such as the sensor, adc, microcontroller, and the rf transceiver. as a result, the power source section is responsible for making a water quality sensor node operational. traditionally, the crucial components in the power section of a sensor node for wqm are a battery, an implementation of energy management schemes, and a dc-dc converter. the power section may be equipped with an energy harvesting technology to improve the availability of power within a sensor node, mitigating for the limited lifespan of batteries. fig. 3 gives a diagrammatic representation of the hardware architecture of a sensor node for water quality monitoring, indicating the key components discussed in this section."
"a transceiver is a dual system, containing a data transmitter (tx) and a data reception (rx). the primary function of a tx volume 7, 2019 is to transfer the data of a sensor node to a desired receiver (or sensor node). this function is achieved by the tx of such specific sensor node by first converting the obtained data (which could be in the form of a bit, a byte or frame) from the micro-controller module of the sensor node to radio waves. the transmitted radio waves are picked up by the destination sensor node receiver (rx). to achieve the tx and rx functions, a transceiver employs key electronic circuits such as mixers, filters, amplifiers, modulators, and demodulators [cit] . for instance, binary phase shift keying (bpsk) and quadrature phase shift keying (qpsk) are typical examples of modulation schemes that may be employed in the communication section of a sensor node for physical (phy) layer communications. the data rate settings of a bpsk scheme operating in the 915 mhz ism spectrum include a bit rate of 20 kb per second (kb/s), or 20 ksymbol per second for a binary symbol type, while the data rate settings of an qpsk scheme over the 2.4 ghz ism spectrum may be configured using a bit rate of 250 kb/s,or 62.5 ksymbol per second for an orthogonal 16-ary symbol type [cit] ."
"the cellular networks discussed in this work may be used for long range communication purposes in wsn applications. similarly, since they operate as a mobile internet provider, they can be employed to provide internet connection to wsn systems for wqm application data delivery. the summary of the reviewed cellular network solutions are presented in table 3 . however, they are not suitable for wsn solutions dedicated to the monitoring of water quality because of their low energy efficiency. wsn solutions in wqm applications are energy constrained systems that require low-power and long-range communication solutions. from a technical perspective, cellular network solutions are not optimized for utilization in wsn systems. for example, the battery life of cellular network devices is days to a few months, whereas, wsn solutions for wqm applications require little to no human intervention, since they may be deployed in critical places. other important considerations are cost and data latency. it is worth noting that cellular networks have high data latency."
spectral contrast is a decibel difference between peak and valley of a spectrum. each phoneme has a unique spectrum represented as a vector. zero degrees angle shows that two spectra are the same and angle difference of 90 degrees show the maximum difference between spectrums. to compute spectral contract sc k we first calculate the peak p k and valley v k of each subband. here k represents the k th subband. spectral contrast can be computed as in (4) .
"the variables often sensed to operate the wind turbine in this type of systems, seen in fig. 1(a), are the rectifier voltage v r, the boost current i b, the dc-link voltage v dc, the grid current i g, and the voltage v g, respectively. the proposed converter controller uses the same set of variables. however, the rectifier voltage will now be a switching signal since the rectifier output is directly connected to the boost-converter switch. if an antialiasing filter was being used to interface the voltage sensor output to the analog-to-digital (ad) converter, no further modification is needed. an antialiasing filter with a cutoff frequency of 3.5 khz is used in this study. if analog filters were not used in the original system, a simple rc filter network can be placed between the sensor output and the ad converter [cit] ."
"water is used for many purposes that include agriculture, industrial consumption, drinking, and recreation, among others. for instance, in agriculture, plants depend on water to obtain necessary nutrients, and such water is expected to be clean and meet the precise water quality requirements (such as dissolved oxygen and ph levels) for the optimal growth and productivity of plants. this also applies to humans, animals, and the fish of the water, as precise levels of water quality are crucial for their survival. as advantageous as water is to mankind and the eco-system on one hand, it could also be disadvantageous on the other hand if it is not well maintained. poor water maintenance practices would result in unclean water, which may not be fit for any of the aforementioned areas of consumption. the presence of contaminants in water systems poses a threat to the health of animals and humans [cit] . contamination could be attributed to man-made activities and naturally occurring events (such as volcano eruptions, soil erosion, natural minerals, flora, fauna, and global warming). examples of man-made activities are extensive industrialization accompanied by urbanization, livestock waste disposal, mining operations, septic tank leakage due to poor construction or ageing, industrial effluent, household waste disposal, excessive use of fertilizers and pesticides, and uncontrolled deforestation. these activities go a long way in contaminating the environment, especially water sources such as rivers, groundwater, reservoirs, streams, and lakes. for example, many important rivers in europe (such as the danube river) and across the globe, suffer from contamination [cit] ."
"it is noticed that the new wi-fi optimizes key parameters that makes it a reasonable wireless technology solution in wsns for wqm applications. such parameters are communication range, energy, data transmission rate, and efficiency (battery life). these are important design considerations to be critically examined since water quality sensors are traditionally operated by batteries, and for the realization of energy-efficient wsn solutions for wqm applications. other important considerations are cost and data latency. category are 2g, 3g, and 4g. the aforementioned communication technologies represent the generations of cellular networks. these generations can be attributed to the developments in mobile telecommunications in the perspective of data transmission speed. for instance, with a new generation, a better data transmission speed is provided. thus, it is important to note that the key difference that lies between the cellular network evolutions is data transmission speed. a quick overview of the above-mentioned cellular networks is given as follows."
"it is worth mentioning that the lpwan architectural design in fig. 11 is suitable to realize higher energy efficiency, as well as satisfying the individual qos demands of the water quality sensors in a network. these developments can be attributed to the performance of the channel access modulation schemes employed by the lpwan technologies at the mac layer, as the schemes used by the solutions at the mac layer are energy aware and minimizes energy consumption."
"the boost-converter input filter capacitor commonly placed at the rectifier output, also seen with a dotted line in fig. 1(a), must also be removed to avoid short circuit when the boost-converter switch is on. this will bring about additional benefits in terms of size, cost, and reliability."
the proposed work has achieved better accuracy for a smaller number of pronunciation mistakes as well as for a large number of mistakes. the comparison shows that the proposed work outperformed the existing work for the same dataset.
"since network sensor nodes traditionally run on battery power, it is important to analyze how energy is being expended in wsn solutions. this is essential because the finite energy budget of a battery directly causes limited operational lifetime of a sensor node. consequently, the understanding of energy expenditure will assist to efficiently optimize modules that dissipate significant energy in a sensor node, namely sensing, processing, and communication."
"because of the critical nature of wqm applications, this requirement describes the response time of the water quality sensors in wsn-based solutions for wqm applications. specifically, wsn solutions for wqm applications require a strict periodic real-time transmission of water quality application data to various water monitoring centers. it is important to emphasize too that quite a number of the microbiological and chemical parameters involved in assessing the quality of water are best measured in-situ. consequently, the insitu measurements demand a strict real-time transmission to water quality analysts for reliable and accurate decision making. to support timely delivery of data, the latency of the employed communication network(s) in wqm applications should be low."
"the ieee 802.15.4 communication technology was developed by an ieee working group, and it has been defined as a de facto standard for low-rate, low-power, and low-cost wireless communications standard employed by wireless end-devices such as sensor nodes. consequently, the ieee 802.15.4 technology has become an acceptable standard that provides a suitable platform for mounting other communication technologies, to extend wireless communications coverage and connectivity between the sensors and the sink node. as a result, the zigbee and other communication technologies in the class of ieee 802.15.4 can be deployed on top of the ieee 802.15.4 standard. the ieee 802.15.4 technology is suitable for wireless communication over a short range within a coverage range that is typically less than 100 m. the essence of the short coverage range is to ensure low power consumption in order to extend the sensor node lifetime. the ieee 802.15.4 technology operates at the first two layers of the protocol stack, including the phy and the mac."
"the commonly engaged legacy wireless technologies in water quality applications for delivering the data are zigbee, ieee 802.15.4, wi-fi, 4g, 3g, and gprs solutions. the identified issues are barriers to the realization of modern wsn solutions for wqm applications in the context of energy efficiency and long range communication. to address the aforementioned issues, which have been lingering for years, this survey explores new promising lpwan communication technologies that may be employed in wqm applications. also, this survey is compared with the state-ofthe-art surveys on wsn solutions for wqm applications with respect to the new long range wireless technologies, as shown in table 6 ."
the zero-crossing rate includes the quantity of time a given signal amplitude changes sign from positive to negative. the zero-crossing rate can be computed as in (7) .
"we discovered that cnn features outperform handcrafted features, but we extract features from other layers of cnn to compare which layer perform optimal results. experimental results show that the neural network achieves the highest accuracy of 87.29% on 28 phonemes as compared to svm and knn shown in fig 8. svm achieve an accuracy of 81.21% and knn achieve 71.91% accuracy. neural network achieves an accuracy of 83.58% on fc7 feature set while obtaining 87.29% accuracy on fc6 features set. if we compare the result obtained on fc7 feature set, we found that accuracy on the feature set of fc6 increased by 4%. experimental results show that the neural network achieves the highest accuracy of 90% as compared to svm and knn as shown in fig 9 . svm achieve an accuracy of 82.80% and knn achieve an accuracy of 74.07%. if we compare the result of layer 5 and layer 6, a neural network obtained an accuracy of 87% on the fc6 feature set and 89.90% on conv5 layer features set. we found that accuracy on the feature set of conv5 layer increased by 3% as compared to fc6 layer features. table 9 represents the performance of a classification algorithm on feature set obtain from convolutional layer 4 (conv4) of alexnet. features extracted from this layer are 64894 and pass to classification algorithms. result analysis shows that the neural network performed best for all arabic phoneme pairs as compared to knn and svm. svm performed better than knn and knn show the least accuracy as compared to svm and nn."
"the solutions in the category of short range technologies typically span distances of ten to hundreds of meters. the coverage range of, for example, zigbee networks can be extended by employing a multi-hop strategy for data networking, but unfortunately this strategy is not energy efficient. the summary of the short range solutions presented in table 1 will help wsn designers to determine a suitable technology for wqm application deployment purpose, while taking important parameters that include energy dissipation, data rate, and communication coverage, into consideration. since wireless devices and water quality nodes are often powered by batteries, examination of the aforementioned parameters is necessary for the realization of energy-efficient wsn systems for wqm applications. other important considerations are cost and data latency."
"the efficiency and efficacy of the military strategy of targeting and shooting are thus questionable. even when it is extended beyond the realm of ballistics to encompass smart and fine-tuned devices performing various functions, the missile metaphor is not fully adequate. twelve years of intensive research did not bring about the promised results. the expected economic benefits are so disappointing that some experts recommend reconsidering the entire technology in close interactions with users and regulatory agencies [cit] ."
we have an audio dataset of arabic phonemes that contains some monotype and some stereotype files. monotype audio files contain only one channel in the signal while stereotype files contain more than one channel in the signal. we take the average of stereotype signal to convert them to monotype signal. we converted all stereo signals to mono signal as in (14) .
"where y j represents the j th component of the contribution to softmax, which compares to class j, and n represents the number of classes. the outcome is a vector containing the probabilities that sample x belong with each class. the output is the class with the highest likelihood."
the audio files present in the dataset are converted to spectrograms after converting all the stereo signals to monotype signals. the spectrograms are used by the convolutional neural network for feature extraction.
"the first operation-loading (fig. 2) -requires the design of a vehicle (capsule, sphere, micelle, dendrite, shell…) 'to carry the drug in a controlled manner from the site of administration to its therapeutic target' [cit] : 1417 . [cit] s with polyacrylamide, polyalkylcyanoacrylate, albumin, or micelles, numerous candidate materials-polymers, liposomes, cyclodextrins and other nanoparticles-have been tested as drug vehicles. there are some specific requirements for the design of a nanocarrier: size (it must not be too big, not too small in order to selectively cross barriers); encapsulation rate (it must take up a reasonable amount of therapeutic molecules while polymerizing 10 ); stability (it must resist bio-erosion); biocompatibility (solubility and avoidance of unwanted accumulation) and biodegradability once the capsule's mission is completed."
"we apply a ks to remove the noisy variations in the signal. to get an estimate of how reliably the change points have been removed, we apply a change point algorithm to detect the time steps that the mean of the signal has been significantly altered 3 . the detected time instances are recorded as the estimated significant change points before and after applying ks (see figure 7) . if the difference of the estimated change points and the actual random change points are less than 10 samples, the estimated change point is assumed to be correct (true positive, tp). however, if the difference is larger than the determined threshold, the data point has been falsely recognized as a change point (and is referred to false positive, fp). if the change point algorithm fails to detect the actual change point, then the point is referred to as false negative, fn."
"in addition, the capsule does not only carry a 'payload', it also protects the biological milieus from the virulence of the active principle. targeted drug delivery really took off when doxorubicin has been encapsulated in biodegradable polyalkylcyanoacrylate nanoparticles [cit], lessening the drug's notorious and often prohibitive cardiac toxicity. the container secures the containment of a dangerous material. its protection is also crucial for expanding the spectrum of potential drugs to biological substances. so far peptides, proteins, nucleic acids have been considered as undeliverable because they are rapidly cleared and degraded in the biological milieu. the still uncertain future of gene therapy will thus depend on the ability to protect the fragile drug through its journey. whether the therapeutic principle is a highly toxic substance that needs to be isolated from the vulnerable milieu or a fragile degradable biological material that needs a protective container, in both cases, the carrier affords care."
the mel-scale is a non-linear scale that is adapted to the non-linear pitch perception of the human auditory system. the mel-scale can be defined as in (3) .
"this communication technology solution is based on a new communication protocol known as random phase multiple access (rpma), which was developed by ingenu [cit] . in ingenu, a two-way communication, namely uplink (ul) and downlink (dl) are possible in a half-duplex manner, based on a dsss and a cdma schemes, respectively, on the 2.4 ghz ism license-free band."
"4g technology was designed based on the international mobile telecommunications (imt) advanced standards defined by the itu. different from 2g and 3g technologies, 4g technology is designed to provide a much better data transmission speed. 4g technology has been defined in two key standards, namely long term evolution (lte) and worldwide interoperability for microwave access (wimax). lte provides a data rate of about 100 mb/s, while wimax has a data rate of 50 mb/s. this has given birth to a 4g lte network and a 4g wimax network, even though the networks are mostly branded as 4g by the cellular network service providers. in recent times, new variants of the lte and the wimax standards have been proposed as lte advanced and wimax 2, and incorporated to the 4g networks accordingly."
"in the face of this crisis, genomics, genetic engineering, nanotechnology and synthetic biology are expected to reinvigorate a decaying business model [cit] . both personalized medicine and targeted drug delivery attract big investments from the private and the public sectors concerned with the sustainability of the public health system [cit] ). they search for more 'rational' (based on molecular features) and more 'reasonable' (more profit and increased efficacy) ways of designing pharmaceutical products through more individualized administration of drugs. while in personalized medicine better administration means better prescription, 3 in drug targeting it translates as better delivery. personalized medicine and target drug delivery are seen as complementary rather than competing strategies [cit] . 4 by adjusting prescription and delivery to 3 because all individuals are not responding to the same drug, personalized medicine is looking for molecular signatures-so called 'biomarkers'-that would allow to direct different categories of patients to more adapted therapies, possibly on the basis of early diagnoses (molecular biomarkers are anything that can be detected and used for measuring the probability of incidence of a disease, its progress, or its treatment's effects: dna or rna single nucleotide polymorphism, protein, complex of proteins, or changes in protein expression). 4 personalized medicine seeks to specify therapy by means of profiling and stratification: it sets up distinct categories of patients with regard to their probability of better responding to this or that therapy on the basis of tests determining the presence of a biomarker. it is to form categories of patients fitting with prescriptions and conversely to adjust prescriptions to categories of patients. targeted drug delivery, by contrast, starts from a individual profiles, well-established medicines may be used to better effect [cit] . it is a key argument for stimulating public adhesion because it seems to overcome the disadvantages and defects of conventional therapeutics. both sectors have raised intense mobilization over the past decade as a result of a remarkable convergence of interests between patients, science, politics and business. patients and physicians want to avoid the damages caused by conventional chemotherapy and radiotherapy in the treatment of cancers. pharmaceutical companies want to avoid the rising research costs for new therapeutic molecules, and public health services want to avoid the tremendous financial loss due to the inefficacy of a large proportion of medical prescriptions. although big pharmaceutical companies are still hesitant to invest in this field 5, they see targeted drug delivery as an opportunity to renew their patent portfolios 6 . on the business side, drug targeting is expected to cover 75% of the nanomedicine market [cit] . therefore hundreds of research laboratories across the globe are currently bringing together the resources from nanotechnology, advanced polymer and lipid chemistry, physics and molecular biology in order to find new ways of vectorizing drugs. such is the pressure for introducing this new technique in cancer therapeutics that hundreds of candidate products are now in clinical trials. it is reported that 27 [cit] . targeted drug delivery has become a target in itself justifying a warlike mobilization, reinforced by a rhetorical arsenal of 'therapeutic missile', 'smart bombs', and the like. as the military metaphors inspire a convergence of efforts on a common target designated as the enemy, they are self-vindicating. in using them, nanotechnologists, physicians, and patients struggling against cancer may feel that they participate in a general mobilisation for an intrinsically good cause ."
"for mispronunciation detection, we developed three models as shown in fig 1. in handcrafted_features model, we take an audio dataset and extracted handcrafted features from audio files and pass them to classifiers like knn, svm, and nn to detect mispronunciation. in cnn_features model, we convert the audio data set to spectrograms dataset and passed them to the convolutional neural network for the feature extraction process. we extracted features from convolutional layers (conv4 and conv5) and fully connected layers (fcl6, fcl7, fcl8) of the pre-trained model. then we applied knn, svm, and nn on extracted features for mispronunciation detection. in the transfer learning model, we passed the spectrogram data to pertained convolutional neural network i-e alexnet, feature extraction, and classification both done by convolutional neural network automatically and detected mispronunciation. the complete details for each method are given in the following subsections."
"the architecture of the nn we used in this research work is shown in fig 3. we divided the data into training, validation, and testing. we use 70% data as training, 15% data for validation and 15% data for testing. to train an nn we used the scaled conjugate gradient backpropagation function. we used multiple hidden layers to detect mispronunciation and use cross-entropy as a performance function. to verify the accuracy, we iterate the nn 10 times on the same data and used the mean accuracy obtain from 10 iterations."
"we extract the different feature set from the deep convolutional network and compare the results of the feature set from different layers with three classification algorithm knn, svm, and nn. the comparison of classification algorithm shows that neural network performed best as compared to knn and svm while comparison of feature set shows that least results obtained on fully connected layer 7 and highest accuracy achieved at convolutional layer 4 as shown in fig 11. we obtained the best results on layer4 features and the feature dimension of layer 4 is 64894. to process large features, require a lot of time and efficiency of the classification algorithm decreases so we reduce the feature dimension using pca. to check the effectiveness of feature reduction technique we rum the algorithm twice, one without feature reduction and one with feature reduction. experimental analysis shows that feature reduction technique enhances the accuracy by 2% as shown in table 10 ."
"the early layers of alexnet contain low-level features while last layers i-e, fully connected layers learn target/classspecific features. so, our purpose is to train the network on our own dataset, so we replaced the layers that learn classspecific features with new layers, so they can learn classspecific features of our spectrogram dataset."
"wqm practice is vital to the wellbeing of mankind and a healthy eco-system, and is therefore strongly required [cit] . wqm is concerned with the monitoring of the key parameters of water. such parameters include physical, chemical, and microbiological characteristics. these parameters are essential measuring yardsticks for investigating the quality of water. globally, microbiological and chemical contaminations are essential water quality issues [cit] ."
"the pharmaceutical arsenal enriched with such magic bullets is clearly an outcome of the paracelsian tradition. to what extent does it mark the death of the galenic approach to diseases? could targeted drug delivery be alternatively viewed as a revenge of galenics, as it has been suggested by patrick couvreur, a belgian scientist who pioneered the field of targeted drug delivery? [cit] ) surely, drug delivery just like today's galenic pharmacology is concerned with the formulation and administration of drugs. 9 the delivered substance matters less than the mode of delivery. all efforts are thus aimed at designing new medicinal forms (galenic forms) rather than new drugs. the goal is less to increase the efficiency of the therapeutic agent than to enhance the therapeutic index (the toxic vs. therapeutic dose ratio) by increasing the efficacy of delivery."
"we therefore aim to additionally investigate the effect of a varying α in time and its impact on the smoothing procedure. we will also investigate the impact of the proposed method in different experimental settings where ip is heavily used for classification between different neural processes. in many past studies [cit], the phase locking of ip has been used as an indicator for separating the presence and absence of attentional-binding due to different auditory stimulations. we therefore require proper methods to measure the level of phase locking of the neural activities in response to different stimulations and denoising the spurious phase variations by optimizing over ks parameters such that the phase resets bounded to neurological activities are preserved. in this regard, the overlap of the ip resets due to neural activities with the ones due to low envelope has to be investigated."
"(iii) object detection -to perform the detection task, we first need to identify the areas in an image 118 where the object can be found, this is called localization or region proposal. then we classify these regions 119 into different categories (eg whether an animal or background?), this step is called classification. the object 120 detection module uses the trained cnn model and performs above two key tasks on any given input image:"
"the wireless network solutions in the category of low power and wide area networks (lpwan) are envisaged to address the need for long range communication and low power requirements of modern wsn systems because of the traditional powering nature of the water quality sensors in such networks, which is battery power. the lpwan solutions attain energy efficiency by adopting strategies that include novel modulation schemes (such as ultra-narrowband and spread spectrum) and duty cycling techniques. one of the key advantages of the ultra-narrowband (unb) scheme is the provision of a low-power transmission and a low receive signal demodulation power of about −142 dbm [cit] . the variants volume 7, 2019 of the spread spectrum modulation scheme include the chirp spread spectrum (css) and the dsss [cit] . the unb, css, and the dsss are the available choices of the phy layer modulation schemes which are employed by different lpwan solutions at their phy layers. the duty cycling technique is employed for switching the energy hungry communication radio (or rf transceiver) of water quality sensors, in an opportunistic manner [cit] . this allows the rf transceiver radio of a sensor node to be duty cycled such that a sensor node can switch off its transceiver radio when it is not in use to save the battery power."
"following a brief presentation of metaphors as conceptual tools, we contextualize the use of the missile metaphor against the background of the current pharmaceutical innovation crisis in relation to the history of pharmaceutical doctrines. taking a closer look at the devices that have been designed over the past decades and at current research trends in this field we then argue that the missile or bullet metaphor provides a simplistic view of the complex technological systems required to achieve therapeutic efficacy. the military metaphor may suffice for chemists, physicists and engineers who design multi-functional devices operating in vitro, as long as they do not operate in the complex and messy environment of a diseased body. but actions and reactions of biological milieu must be integrated into the operational scheme of therapeutic devices. to go beyond efficiency and achieve therapeutic efficacy in vivo, nanodevices have to be designed not only as destructive weapons but as protective shells as well as secret agents conducting negotiations; the biological environment should be conceived as a dense milieu populated with multiple and heterogeneous actors rather than as an abstract space traversed by a moving body. finally, we argue that the use of alternative 'oïkological' metaphors encapsulating not just the dispositions of the nano-object but also the affordances of the milieu may be useful to improve the technique. this requires dealing with nanoparticles as entities defined by their relations rather than as stable physical or chemical substances thereby calling for a redefinition of research priorities."
"faria nazir received the b.s. and m.s. [cit], respectively, where she is currently pursuing the ph.d. degree. her research interests include machine learning, data mining, speech processing, and pattern recognition."
"the shortcomings of the laboratory-based systems make them unsuitable for efficient wqm. an efficient wqm system is expected to possess characteristics that include fast response time, low cost, ease of deployment, real-time results, and reliable measurements [cit] . these requirements are essential, as highlighted by various international bodies that include who and epa [cit] . to meet the challenges of wqm systems today, wsns have been proposed as a promising technology [cit] ."
"the ieee 802.11ah network serves as a representative of the ieee 802.11 standards developed to target the need of low-power devices and wsn applications with an enhanced communication range capability. it also caters for the massive connection of sensor nodes [cit] . specifically, the ieee 802.11ah is a low-power wi-fi solution, and is advantageous for creating ip-network capable built-in sensor nodes [cit] conveniently, since the deployment is compatible with already available infrastructure of conventional wi-fi solutions. the energy consumption of low-power wi-fi is typically around hundreds of milliwatt, which is considered acceptable for water quality sensors. with the new low-power wi-fi solution, a data transmission rate of about 7.8 mb/s can be realized by the ieee 802.11ah devices over the sub-1 ghz free-license ism spectrum [cit] . the ieee 802.11ah solution technically supports the sub-1 ghz spectrum to achieve energy efficiency since there is typically a low propagation loss when transmitting on a low frequency such as 915 mhz [cit] . as a consequence, at a low transmission power, it is possible to achieve the required communication coverage since the 915 mhz spectrum is characterized by a minimal signal pathloss and this in turn results in longer communication coverage, thus, saving the energy cost of data communications. the ieee 802.11ah solution further achieves energy efficiency through the incorporation of a new scheduling scheme for data transmission recently proposed by a research group known as the task group ah [cit] . the scheme may also be referred to as a restriction-based window access mechanism."
"in the context of lora architecture, a lora gateway is needed [cit] . lora supports the third generation partnership project (3gpp) specification to enable ipv6 connectivity on lora lpwa networks."
we characterize the pitch of the sound as a recurrence of vibrations when constrained air from the lungs goes through the choral folds. the basic recurrence of the sound is when vocal tracts vibrate. pitch is the fundamental frequency of a human speech waveform and is a vital parameter in the examination and combination of speech signals [cit] .
"to evaluate the performance of the training process we pass the validation data that is 33% of the whole data to the trained network and evaluate the performance of trained data using accuracy measure. on validation data, we can discover how well a network trained to differentiate between correct and incorrect phoneme."
"3g stands for the third generation of the mobile network.3g technology was designed based on the telecommunication standards defined by the international telecommunication union (itu) [cit] . the essence of this technology is to provide services that one could also achieve by the internet technology under the initiative of a personal wireless internet access (pwia). this became necessary as the internet technology has been overcrowded by huge numbers of mobile and wireless devices across the globe. since the internet has been bombarded by many devices, communications over the internet has been negatively impacted. one of the key objectives of 3g networks was to provide the network users with global wireless access to telecommunication network infrastructure by using both satellite and terrestrial systems. this objective helps to harmonize a global interoperability among several network operators to ensure reduced cost. as a result, the itu recommended different requirements in the context of data transmission speed for various users under the universal mobile telecommunications system (umts) [cit] . for example, under the 3g umts network, a data rate of 144 kb/s, 384 kb/s and 2 mb/s is proposed for a user in motion, a pedestrian user, and a fixed user respectively. 3g networks employ a wideband code division multiple access (wcdma) communication protocol to carry data over the licensed cellular network bands in the range of 850 [cit] mhz, and to also ensure that the maximum data transmission speed is achieved. from wsns perspective, the proposed requirement for a fixed user indicates that wsn applications can achieve a data rate of about 2 mb/s for remote data transmissions. consequently, 3g umts networks has been a central tool for ubiquitous computing which specifically means computing anywhere and anytime. with the advent of a high speed access (hspa) standard in 3g, the data transmission speed of the network was increased. this makes it possible for high data rate applications on wireless devices in the 3g hspa network. the hspa was later upgraded to hspa+ [cit], to further increase the data transfer speed of the 3g hspa network. the 3g hspa+ network is often referred to as a 3.5g network. the third generation partnership project (3gpp) is an important standard in 3g."
"because of the limited battery life of water quality sensors, suitable energy harvesting technique(s) may be combined with the power section of the water quality sensors to complement the battery power. it is worth mentioning that energy harvesting technology is presently an open research problem."
"preprocessed and selected features are used to train model which classify the features to predict correct or incorrect phoneme. we train the model using 10 cross-fold validation process instead of dividing the data into training and testing. the classification algorithms (k-nearest neighbors, svm, and nn) are described as follow."
"it is important to mention that the devices in the ieee 802.11ah can span a coverage range of about two hundred meters to a few kilometers. in an attempt to further improve on the energy consumption rate of the ieee 802.11ah solution, the wi-fi working group has recently introduced wi-fi halow, which is low-power solution, into ieee 802.11ah. this is the wi-fi solution that is envisaged as the low-power solution that promises to advance the field of monitoring water quality in the context of long communication coverage and low-power capabilities."
"we transferred the convolutional layers of alexnet; c1-c5 that were trained on imagenet dataset and replaced the last 3 layers of alexnet to fully connected layer, softmax layer, and classification layer. the parameters used to create the fully connected layer include the output size, weight learn factor and bias learn factor. we set the output size of the fully connected layer to the number of output classes. weight learn factor determine the learning rate for the weights in the layer. similarly, bias learns factor parameter determines the learning rate for the bias in the layer. we used weight learn factor and bias learn factor value equal to 50 that means learning rate for the weights and bias in the layer is 50 times the current global learning rate. softmax layer applies the softmax function to the input. the parameters for the classification layer include output size, the name of loss function used for training the multiclass classification. we used the cross entropy function for k mutually exclusive classes (cross entropy) as loss function and output size is equal to the number of class labels."
this section has demonstrated the inductorless converter capability with the proposed control structure for controlling the boost current and the generator torque. the next section analyzes the advantages and limitations when compared with the conventional topology.
"the fully connected layer neurons are associated with neurons of adjacent layers. this is a similar way that neurons are organized in the artificial neural network (ann). in the alexnet, there are three fully connected layers. these layers mix the features of two channels to acquire a 4096-dimensional feature vector. we utilize dropout method in first two fully connected layers, we set the input of first two fully connected layers to zero with the likelihood 1/2, which don't add to the forward pass and don't take part in reverse. the features can be automatically extracted from these layers and use for classification and prediction processes. we extract features from convolutional layers conv4, conv5 and fully connected layers fc6 andfc7. the dimension of features obtained from convolutional layers; layer4 and layer5 are 64896 and 43264 respectively. the dimensions for fully connected layers-layer 6, 7 are 4096. these features are collected from each layer and passed them to classification algorithms."
"the concept of qos focuses strictly on the key stringent requirements of a specific wsn solution. qos is an indispensable parameter in wsn applications, and as a result, qos parameters are critical design goals in wsn solutions for wqm applications. qos parameters provide a platform for satisfying the service requirements of a wsn-based system for monitoring the quality of water [cit] . specifically, this section highlights some key qos requirements to be volume 7, 2019 satisfied when deploying wsn solutions for wqm applications. examples of such requirements are energy efficiency, deployment cost, large communication coverage and reliable delivery, efficient data transmission rate, and strict real-time operation. the aforementioned requirements are briefly discussed in the subsequent subsections."
"a crucial pivot upon which the usefulness of wsn in wqm revolves is the context of guaranteed and timely detection of any possible water contaminations to protect the public health, which places strict demands on the energy source. similarly, it is possible for water quality sensors to be installed in places that are difficult to access after deployment, such as inside underground water pipes. in such an application scenario, replacing the in-built batteries may not be feasible. importantly, it is expected of such sensors to be operational for a meaningful number of years."
"typically, the communication coverage of zigbee and ieee 802.15.4 radio solutions span a range within tens of meters [cit] . the coverage of these solutions typically limits wsn systems for wqm applications to contexts with low spatial resolution [cit] . another possible solution for extending the coverage range for data communication is the introduction of relay nodes to a network. these nodes typically use the unlicensed industrial, scientific and medical (ism) spectrum bands for data communication. however, this strategy is not secure due to the overcrowding of the ism communication platform and may likely encounter security attacks and interference. also, the strategy does not guarantee low cost communication routing."
"lora is a lpwan based wireless technology and is managed by the lora alliance. lora can be described as a low-power wireless technology that is used for long range communications in wsn systems. its standard supports the ieee 802.15.4 communication standard. lora technology does not require line-of-sight and this property makes it a suitable candidate for both rural and urban locations. lora technology is established on a technology known as lorawan, which is based on the pure aloha scheme [cit] . lora uses a proprietary lorawan modulation scheme which is a css solution or a gfsk scheme for dl communication at the phy layer [cit], to perform signal modulation on the sub 1 ghz free-license ism rf bands such as 915 mhz and 868 mhz. pure aloha scheme is used at the media access control (mac) layer for ul communication [cit] . the aloha mac scheme offers asynchronous random access to the lora sensor nodes in an orthogonal manner to schedule the data transfer process of each of the sensor nodes. the aloha mac scheme is considered as an optimal scheme to circumvent the hardware complexity and design cost issues associated with most of the schemes employed in short range and cellular network solutions, to make the radio transceiver of sensor nodes energy efficient, cheap and simple [cit] . lora offers different modes of operations such as classes a, b, and c, to further attain energy efficiency. the various modes provide different levels of data rate, energy consumption and latency. as a consequence, an adaptive mechanism may be employed to enhance energy efficiency, for example the data rate can be adapted to optimize the utilization of energy resources. it is worth mentioning that a typical lora network spans a transmission range of about 5 km and 15 km in urban and rural locations respectively, and it can provide a typical transmission rate of 50 kb/s [cit] ."
"the generator phase-a current has been acquired with a keysight dsox3014a digital scope for two values (2 and 3.7 a) of the boost current, as can be seen in figs. 17 and 18 . they show a close agreement with those analyzed in simulation, in figs. 8 and 9 . however, small differences exist:"
"summary of high power wan: satellite and wimax are good examples of high transmission rate and long range technologies that may be employed in wsn solutions for wqm applications. however, both solutions are characterized by extremely high energy use because of their high transmission rate property as they consume more power during water quality data transmission to remote water monitoring centers. even though they can be utilized to meet the long range communication requirement of wsn solutions for wqm applications, their power consumption rate makes them unfit to realize the goals of modern wsn systems dedicated to wqm applications as the system's water quality sensors operate on batteries and are expected to operate at least for a reasonable numbers of years. secondly, the water quality sensors in the wsn solutions for wqm applications are not designed to be interfaced with high transmission rate communication technologies, which draw more power. since energy efficiency requirement is of top priority in wsn solutions for wqm applications, then it will be reasonable to explore other efficient solutions. other notable considerations are data latency and cost. both the satellite and wimax have high data latency. in table 4, the summary of the reviewed high power wan technologies are presented."
we compare the results obtained from each model for mispronunciation detection. we discovered that the proposed model transfer learning-based method outperforms the other two models and achieve an accuracy of 92.2% for 28 arabic phonemes. mispronunciation detection using transfer learning-based model achieves the highest accuracy because of the already learned knowledge are used for classification. cnn_features based model achieve the second highest accuracy of 91.7% due to enriched features extracted by cnn for classification. handcrafted feature achieves the least accuracy as compared to the other two models. fig 17 shows the comparison of models in term of accuracy for mispronunciation detection.
"the phy layer is responsible for the downlink communication, which involves a base station (a sink node or an access point) sending signals to the sensor nodes connected to it in a network. the phy layer of the ieee 802.15 4 radio can operate at the free-license ism rf bands such as 2.4 ghz, 915 mhz, and 868 mhz [cit] . when operating in the 2.4 ghz band, the achievable data transmission rate is 250 kb/s and a qpsk data modulation scheme may be employed at the phy layer. this band supports sixteen channels (occupies channels eleven to twenty-six) as depicted in fig. 7(a) . when operating in the 915 mhz band, it is possible to achieve a data transmission rate of 40 kb/s and a bpsk data modulation scheme may be deployed at the phy layer. this band supports ten channels (occupies channels one to ten) as shown in fig. 7(b) . when operating in the 868 mhz band, a data rate of 20 kb/s may be achieved, and a bpsk data modulation scheme may be employed [cit] . this band has one channel (occupies channel zero) as illustrated in fig. 7(c) ."
"the coherent activity in neural population can be observed among different neural assemblies for different cognitive and motor tasks. it has been particularly used to study the effect of cognitive binding with regards to different stimulations [cit] . [cit] the level of phase alignment has been used to differentiate between different processes of habituation and non-habituation. [cit] phase synchronization has been used to objectively determine the level of selective attention to auditory stimulations. abnormal activities in phase synchronization of neural oscillators have been investigated across different spectrums of neuropsychiatric disorders such as schizophrenia and attention deficit disorders [cit] . epilepsy is associated with a hypersynchronous state of neuronal activities across the brain. using the phase synchrony techniques, different approaches for prediction of epileptic episodes have been proposed [cit] ."
"data transmission rate describes the number of water quality application data that may be transferred at a specific time. data transmission rate is vital to a fast response rate in the delivery of wqm application data to water monitoring centers. however, there exists a challenging trade-off within the data transmission rate and power consumption, such that the power consumption level increases as the data transmission rate increases. therefore, efficient strategies are required to strike a balance between the two parameters: to achieve an efficient data transmission rate for an appreciably fast response in wsn solutions for wqm applications with adequately low power communications."
"despite its advantages, the missile metaphor does not do full justice to the complexity of the issue. addressing drugs to a specific site and releasing the right dose in the right place at the right moment in the complex environment of a living body requires more than a magic bullet. in addition to the various disciplines that converge in all engineering project, nanomedicine requires the engagement of the biological milieu as a partner. therefore, the paracelsian model underlying the ballistic metaphor which may be adequate for in vitro demonstrations of efficiency must be completed by a genuine galenic model of a complex and responsive milieu. the metaphor of the magic bullet has a limited heuristic power and brings about disappointing results. ballistic is not the relevant technological model because it provides a distorted and biased view of the operations to be performed by nanomedical devices. since they have to negotiate with the biological milieu and take advantage of what it affords by turning obstacles into facilitators, an alternative metaphoric framework is needed. it thus comes as no surprise that a number of recent research trends in targeted drug delivery focus increasingly on the milieu thus exploring the potentials of the 'oïkological' approach. [cit] although the broad question of the definition of nanoparticles remains far beyond the scope of this paper, it is not extravagant to suggest that the oïkological metaphor could be extended to other fields of nanotechnology, where it would afford new perspectives. if the relations between nanoparticles and their environment were no longer treated as side effects and rather as integral parts of their definition and characterization, research priorities could be dramatically changed. in today nano-initiatives nanotoxicological studies of interactions between nanoparticles and biological tissues or interactions with the environment are often viewed as a necessary detour for the safe and successful mass diffusion of nanotechnological products whereas they could become the hard core of the research field. this is just a tentative scenario of what might happen if nanoresearch were to focus on the interactions of nanoobjects with their environment."
"statistical features are general features that are not related to speech. these are the numerical descriptor of the statistical properties of the audio signal. statistical features include mean, variance, standard deviation, slope and centroid skewness."
"svm classifier obtained an accuracy of 82% and perform best as compared to other classification algorithms (knn and nn) having an accuracy of 75% and 79% respectively. in cnn features based technique, we extracted features from different layers of cnn and used classification algorithms (knn, svm, and nn) on features of each layer to find out that which layer features are best for mispronunciation detection. the results show that nn obtained accuracy of 83.58% on layer7, 87.29% on layer6, 91% on layer 4 and 5. we discovered that the nn performs best on features extracted from layer 4 and layer 5. we also discovered that on handcrafted features, support vector machine outperforms nn while on cnn features nn outperforms svm. the method using transfer learning of alexnet perform best in terms of accuracy and obtained an accuracy of 92.2%. transfer learning method gives the highest accuracy as compared to cnn features based techniques and handcrafted based technique because of the knowledge transfer. we also observed that the method with deep convolutional neural network cnn_features outperform the handcrafted featurebased method due to enriched features extracted by cnn and obtained an accuracy of 91.7%. our proposed transfer learning-based method also outperforms the state of art methods in term of accuracy."
"communication technologies are powerful tools in wsns that provides a platform for connecting devices for the purpose of information transmission. specifically, they provide a communication platform for devices such as sensor nodes in wsn applications to communicate with the neighboring sensor nodes, as well as with the base station (or sink node for example, a sensor node that is equipped with a zigbee radio can communicate in a wireless fashion with another zigbee compliant sensor node in a zigbee network at a distance that is typically greater than 100 m, depending on the application environment. on the other hand, key available communication technology solutions based on wired connectivity are plc, ethernet, pon, and dsl. unlike the aforementioned wired technologies which are employed to connect a base station to remote locations, uart is an example of a local, short distance wired technology for directly connecting sensors to a base station. plc and ethernet are used for neighborhood area network (nan) communications in the range from 10 m to 10 km, while pon and dsl are used for wide area network (wan) setup that covers 10 m to 100 km. the shortcomings of the legacy wireless communication technologies in terms of limited communication range and high power consumption in wsns have been a subject of concern in the research community, and have motivated the need for developing advanced wireless network solutions. recently, advances in wireless communications have produced novel wireless network solutions. such solutions are classified as low power and wide area networks (lpwans). it is hopeful that the new lpwan solutions will advance wsn solutions for wqm applications because of their longer range and low-power characteristics. the need for these features cannot be over-emphasized in wsn solutions for wqm applica- in the different classes of wireless technologies are discussed in the subsequent sections."
the current controller has been first tested using detailed simulations to analyze not only the boost-current control but also the corresponding torque produced. it must be remarked that the main goal of controlling the boost current is to indirectly control the generator torque.
"the proposed controller for the inductorless converter for small wind turbines has been experimentally tested to validate its performance. an alxion 190stk3m alternator, whose parameters can be found in table iii, is used as generator. a wind turbine with the parameters seen in table v is emulated using a 4-pole 11 kw vector controlled induction motor drive. the turbine power curves as well as the turbine inertia are programmed in the load drive using a sm-applications lite module from control techniques. a custom converter following the design seen in fig. 1(a) is used for the generator operation. the control blocks seen in fig. 1(b) -(c) are implemented in a texas instrument tms320f28335 dsc. the pwm and sampling frequencies for the boost converter are set to 20 khz. the h-bridge inverter switching frequency is 10 khz. the experimental setup can be seen in fig. 14 ."
the wireless technology solutions in the category of wireless pan (wpan) are typically employed to achieve a short range data communication within meters. this section reviews some of the legacy solutions commonly employed for short range networking. the newer amendments are also discussed.
"traditionally acoustic-phonetic features set along with svm classification algorithm are used for mispronunciation detection. a lot of work has been done on mispronunciation detection for different languages (english, dutch, chinese, mandarin, and japanese) but little work is done on arabic phonemes mispronunciation detection. literature study show that different techniques/methods used for mispronunciation detection, posterior probability-based methods [cit] classifier-based [cit] and deep learning based methods [cit] . existing approaches work on some confusing phonemes of language instead of all phonemes. arabic language mispronunciation is more important because of its large number of speakers. to detect the more suitable features for mispronunciation detection is still an open research area."
"2006; [cit] ) . a fir (finite impulse response) filter is usually applied to avoid the distortion of phase information (see chapter 5 [cit] for more references). the ip is obtained by computing the analytic signal, commonly by either applying a wavelet transformation (wt) or hilbert transformation (ht). the ht of a signal x(t) is represented aŝ"
"ram memory provides fast data reading and writing access services to a micro-controller. this type of memory is volatile in nature. that is, the data written or stored in such memory requires energy to be maintained. once the energy supply to such memory is off, then the stored data is wiped away. in the perspective of sensor nodes and subject to the type of micro-controller employed, ram memory may provide temporary storage space of about 1 kb to 1 mb [cit] ."
"considering the fact that energy is a crucial resource in wsns, energy efficient communication mechanisms are necessary at the mac layer of the protocol stack of wsns. without energy efficient mechanisms at the mac layer, more power will be dissipated during data transmission by the radio transceiver of a sensor node. this can be attributed to different issues. examples of issues that waste energy resources at the mac layer include idle listening, the overhearing problem, and packet re-transmission due to packet collisions. therefore, for energy resources to be efficiently utilized during data communication, energy efficient communication networks are crucial. to achieve this, lpwan solutions are promising communication networks because of their novel channel access modulation schemes. this will help to minimize the energy dissipated by water quality sensors that traditionally run on battery power."
"mfcc is perceptually boosted features that are based on short-term fourier transform (stft). subsequent to taking the log-amplitude of the spectrum magnitude, the fast fourier transform (fft) containers are gathered and smoothed by the perceptually encouraged mel-recurrence scaling. at last, with a specific end goal to de-correlate the feature vectors a discrete cosine transform is performed. mfccs are spectral features element that can be computed utilizing a band-pass filter [cit] ."
"it estimates tonal centroids as coordinates in a sixdimensional interval space. the tonnetz is a harmonic network representation of pitch intervals where, by imposing an equal temperament tuning system (octave equivalence), the surface of the model wraps onto itself as a six-dimensional hyper torus [cit] ."
"some arabic phones are similar in term of sounds with other phones we call them confusing phones. non-native speakers have difficulty in learning such phonemes that produce similar sounds, we grouped the phonemes on the basis of their sound similarity [cit] like and is a confusing pair and have t-like sound and speakers of english and urdu have difficulty in pronouncing such sounds. in this research work we deal with 6 confusing pairs ( ), ( ), ( ), ( ), ( ) and ( ). all remaining phonemes are placed in a separate group due to distinctive sounds. the grouping of phonemes along with sound similarity phonemes is shown in table 3 ."
"to quantify these error rates in mothe, we prepared ground-truth data by visually counting the number of we demonstrate that mothe is relatively easy to use software pipeline that allows users to generate datasets, 225 train a simple neural network and use that to detect multiple objects of interest in the heterogeneous background. localisation and classification, are expected to reduce error rates compared to our approach and do not require 236 colour thresholding. however, these types of neural network require access to high specification gpus. using 237 these kinds of specialised object detectors for animal tracking requires sufficient user proficiency to configure."
"to achieve the dream of realizing low-power and long range communication capabilities in modern wsn systems for monitoring water quality applications, new energy efficient communication technologies that target devices with low energy requirements are emerging, which promise to advance the field of wqm. examples of such communication technology solutions include sigfox [cit], lora [cit], ingenu [cit] and nb-iot [cit] . these solutions are cheaper in terms of operational cost for providing a low-power interface to sensors for water quality data communication, compared to the conventional cellular network solutions [cit] . since the water quality sensors traditionally run on battery, with the new communication technologies the battery life of the water quality sensors may operate for years compared to the conventional cellular network solutions that typically last for only a few days [cit] . also, the new technologies are promising solutions to deal with the short range limitation of zigbee and ieee 802.15.4 networks."
"the technique is divided up in three basic unit operations: loading, addressing, and releasing the active therapeutic molecule, each operation having specific design requirements. this sequence of operations could apply to the preparation of a missile, and the therapeutic action is conceived in terms similar to any ballistic problem."
"the climate of crisis in pharmaceutical innovation may have intensified the use of warfare metaphors but the metaphors have been around in the twentieth century. their success testifies to the triumph of the chemical approach to medicine, initiated by paracelsus in the sixteenth century against the older galenic tradition 7 . paracelsus grounded his therapeutic approach on a theory of secrete correspondences or sympathies between each of the seven metals known in his time and the specific parts of the human body [cit] . he assumed that disease was caused by external foreign agents acting as poisons on a specific area of the body. diseases were viewed as localized physical things that the physician sought to eradicate from the body with the help of an appropriate chemical substance. by contrast, the galenic tradition emphasized the role of fluids and viewed disease as the result of disturbance in the balance of fluids due to an excess of one of the four basic humours. the given molecule and a given target (organ, tissue, cell, organelle or molecular receptor) and seeks the most fitting nanoscale formulation to carry the molecule to the target. 5 currently, the innovation landscape of nanovectorization is mostly populated with small start-ups selling their patents to big pharmaceutical companies. [cit] . they rely on venture capitals and business angels to bear the costs of preclinical development, scaling-up studies, upgrading to legal standards, and phase-i to mid-phase-ii trials. big pharmas cover only end-phase-ii and phase iii. they adopt a 'wait-and-see' strategy [cit] : 4) and refuse to rush head down towards a disruptive technology unless it has pugnaciously proven its safety and efficiency [cit] b) . 6 it is expected that targeted drug delivery will provide pharmaceutical industry with new patents thanks to the nanoformulation of established molecules before the end period of their ip rights. it also allows patenting older medicines that so far could not enter the market [cit] : 3) . 7 here we take the two rival founding fathers as mythical figures. it matters little to us whether galen and"
"the description of the nanoplatform as an assemblage of independent functionalities that can be activated by internal or external stimuli is somewhat misleading. it does not take into account the synergistic or antagonistic effects between the artifact and the milieu, which may alter the devices' functions and effects. the image of a carrier moving straight to hit a specific target is only an abstract model that may be appropriate to conduct laboratory experiments. while in vitro tests are indispensable for obvious epistemic and ethical reasons, they tend to generate illusions of control and efficiency. they may become counter-productive if they conceal the problems posed by the complex and stochastic behaviour of the biological milieu. it is thus clear that a strict paracelsian chemical approach to drug delivery is doomed to fail in nanomedicine."
"in this paper, we developed an (i) cnn features based method an (ii) transfer learning the based method. in the cnn_features method, the deep convolutional nn automatically extracts the distinguishing features from the spectrogram of audio files. we extract the features from different layers (convolutional layer 4 and 5, fully connected layer 6, 7 and 8) of cnn to find the most suitable features for mispronunciation. in the handcrafted features-based method, we extract the features (mel-frequency cepstral coefficients (mfcc), chroma, mel spectrogram, spectral contrast, tonnetz) from audio files. three different classifiers; knn, svm, and nn are used to evaluate the handcrafted_features and cnn_features methods. in the transfer learning method, the process of feature extraction and classification are both done automatically using deep convolutional neural network alexnet. in the transfer learning method, the cnn learn the features of the audio spectrogram to classify the phonemes correctly. we analyze that transfer learning method performs better than handcrafted features and cnn_features methods. the proposed technique transfer learning method enhances the performance of mispronunciation detection process in terms of accuracy. the contributions of this paper are;"
"the random access operation of the pure aloha scheme employed in the proprietary-based lpwan solutions provides scope for improving the allocation of the communication channel resources. as a consequence, the performance of the aloha scheme, which allocates communication channel resources in a random manner, should be improved by investigating the hybridization of aloha and a scheduled access technique (such as tdma). this development will improve channel allocation and data transmission performance."
"as discussed in this work, the new wireless technologies represented by the lpwan variants and the ieee 802.11ah are promising solutions for achieving the requirements of modern wsn solutions for wqm applications in terms of energy efficiency, data rate, and long range communication."
1) the general harmonic content is slightly smaller than in simulation; this is easily explained by any small difference in the parameter values used in simulation from the actual values.
"the phase-reset model of oscillatory eeg activity has received a lot of attention in the last decades for decoding different cognitive processes. based on this model, the erps are assumed to be generated as a result of phase reorganization in ongoing eeg. alignment of the phase of neuronal activities can be observed within or between different assemblies of neurons across the brain. phase synchronization has been used to explore and understand perception, attentional binding and considering it in the domain of neuronal correlates of consciousness. the importance of the topic and its vast exploration in different domains of the neuroscience presses the need for appropriate tools and methods for measuring the level of phase synchronization of neuronal activities. measuring the level of instantaneous phase (ip) synchronization has been used extensively in numerous studies of erps as well as oscillatory activity for a better understanding of the underlying cognitive binding with regard to different set of stimulations such as auditory and visual. however, the reliability of results can be challenged as a result of noise artifact in ip. phase distortion due to environmental noise artifacts as well as different pre-processing steps on signals can lead to generation of artificial phase jumps. one of such effects presented recently is the effect of low envelope on the ip of signal. it has been shown that as the instantaneous envelope of the analytic signal approaches zero, the variations in the phase increase, effectively leading to abrupt transitions in the phase. these abrupt transitions can distort the phase synchronization results as they are not related to any neurophysiological effect. these transitions are called spurious phase variation. in this study, we present a model to remove generated artificial phase variations due to the effect of low envelope. the proposed method is based on a simplified form of a kalman smoother, that is able to model the ip behavior in narrow-bandpassed oscillatory signals. in this work we first explain the details of the proposed kalman smoother for modeling the dynamics of the phase variations in narrow-bandpassed signals and then evaluate it on a set of synthetic signals. finally, we apply the model on ongoing-eeg signals to assess the removal of spurious phase variations."
"first of all, if the warfare metaphor is to be maintained, it requires at least more than performing the sequence of unit operations: loading, addressing, releasing. to successfully deliver the right dose in the right place at the right time in the complex environment of a living organism requires at least 'smart weapons' with multifunctional parts. consider the first unit, the capsule. it has to be designed both as a vehicle and as a container for transporting the active principle. while the nano-size facilitates the circulation through the blood vessels and allows intravenous injections (usually the most dangerous drug administration route for the risk of embolism), for containing the optimal dose of active principle, the nanocapsule presents the inconvenience of a low volume for drug loading. designing the capsule implies a trade-off between size and dosing. for a successful therapeutics, the designer has to find out the optimal trade-offs afforded by the nanoscale, with regard to the kind of effect s/he chooses to prioritize."
"removal of artificial phase variations in the signal is one of the important steps for obtaining a reliable measure of phase synchronization. spurious phase variation in this context refers to phase resets which are not related to any underlying neurophysiological activities. in the recent study by seraj (2016, 2017), one of the effects that lead to artificial phase variations in eeg signals has been thoroughly explained. it has been demonstrated that the low envelope of an analytical signal after narrow band-passing can cause abrupt changes in the instantaneous phase (ip) or instantaneous frequency (if) of the signal. as a consequence, these spurious phase variations can be falsely correlated and interpreted as a response to stimulations or cognitive activities and distort the results. these artificial phase alignments are called spurious phase-resets. [cit], a robust measure based on a monto-carlo estimation scheme has been proposed for computing a more reliable estimate of the phase."
"due to the energy problem in wsns and iot applications, low-power and low-rate communication technologies are important for efficient data communications in a way to optimize the rate of energy consumption. as a consequence, this section presents a review of communication technologies with the view of identifying their strengths and weaknesses in the context of power consumption, coverage range, data rate, latency, and cost."
"processes and try to capitalize on its findings-a major feature of what jones dubs 'soft engineering': 'the advantage of soft engineering is that it does not treat the special features of the nanoworld as problems to be overcome, instead it exploits them and indeed relies on them to work at all' [cit] ."
"the most common power-converter topology for grid-tied low-power wind turbines is based on a passive rectifier, a boost converter, and an h-bridge inverter, as shown in fig. 1(a) . the wind turbine is coupled to a pmsg in direct-drive configuration. the machine terminals are connected to a diode rectifier. a boost converter increases the rectified back emf voltage to a level higher than the grid-voltage peak magnitude. this allows to inject current into the grid using a single-phase full-bridge inverter and a line filter."
"one concern with the inductorless inverter is the distribution of the power losses once the boost coil is removed, as well as the total system efficiency. it has been shown in the previous section that the same average torque is produced for the same boostcurrent level. therefore, comparisons between both alternatives will be made in terms of the boost-current level. fig. 8(a) shows two periods of the generator phase-a current when the boost-current command is 2 a and the rotor speed is fixed to 400 r/min (i.e., 40 electrical hz), both for the inductorless and the conventional converter. they show a similar magnitude and shape distortion. the fast fourier transform (fft) is applied to analyze the frequency spectrum of both signals. fig. 8(b) show the per unit (p.u.) fft magnitude relative to the fundamental frequency component magnitude at 40 hz. the frequency range shown in fig. 8(b) has been limited to those frequency components having a magnitude larger or equal to 0.01 p.u. the total harmonic distortion (thd) is calculated for harmonics up to 1 khz. both converters create a similar low-frequency harmonic distribution in the generator currents for this boost-current level. this agrees with the similar torque ripple level seen in figs. 5(c) and 6(c) for this current level. to evaluate the effect of switching harmonics the signal power spectrum is shown in 8(c), where the thd is calculated for frequencies up to 45 khz. above this frequency there is no meaningful change in thd. a low impact in the inductorless converter torque production can be expected from these harmonics and nonexistent in the conventional converter case. nevertheless, they can contribute to increased iron losses."
"in a local communication setup, the water quality sensors in the network are controlled by a base station (bs), which also means a controller or a sink node. consequently, the water quality sensors communicate with the bs through a radio-or a satellite-link. in a remote communication setup, the bs acts as an intermediary (or proxy) between the network water quality sensors and a remote monitoring system (or an application station) by providing communication connectivity to the remote monitoring centers, including the remote control systems."
"moreover, another trend in neuroscience is to characterize variability of neural signals in single trials or attempt to relate between neural variability and sensory/motor variations being observed [cit] . therefore, we would like to extend our current method to characterize single trial neural oscillation data as well."
2) the high-frequency spectrum contains harmonics at multiples of 10 khz not seen in simulation; this is radiated noise induced in the scope probes from the h-bridge commutation.
"as discussed in this section, the negative side of water quality can be addressed through efficient monitoring systems. importantly, the issues raised have necessitated the need for monitoring water quality to safeguard human lives, as well as protect the eco-system. traditionally, laboratory-based systems are employed to carry out wqm. the traditional approach to wqm encompasses four stages, namely water sampling, transportation of water samples to laboratory, water sample testing, and analysis. these offline processes waste much time, and does not guarantee reliable results as parameters that include temperature and ph are best measured in-situ [cit] . the traditional approach is based on laboratory-based systems like optical spectroscopy, optical-infrared spectroscopy, [cit], membrane filtration [cit], and fermentation tubes [cit] . these systems are limited by several drawbacks that include requirement for operational expertise (which may include manual handling), human errors, high operational cost, few data sets, and inaccurate detection of contaminants [cit] ."
"this paper has presented a survey on the legacy and new communication technologies that are suitable for wsn solutions in wqm applications. this is necessary to address the shortcomings of the existing wsn solutions for wqm applications due to the inefficiency -such as short communication range and high power consumption -of the legacy communication networks often combined with the wsn systems in wqm applications. to achieve this, long range and low-power communication networks are desired in wqm applications. these requirements can be achieved by incorporating the newly emerging low-power wide area network (lpwan) solutions into the communication architecture of the wsn systems for wqm applications. as a result, three categories of potential solutions for wqm applications have been proposed in this paper. these categories include the proprietary-based lpwan, the cellular-based lpwan, and the low-power wifi-based ieee 802.11ah solutions. the variants of the lpwan technology employ different modulation schemes to offer low power consumption at a low data rate. these features make the reviewed solutions of the lpwan variants suitable and promising data networking candidates for wqm applications. also, the ieee 802.11ah solution, which is an improved version of the legacy wi-fi network, is another promising candidate for water quality data networking in terms of energy efficiency and long range communication. however, it offers a lower communication range and higher power dissipation compared to the lpwan solutions, because its data rate is higher than that of the lpwans. based on the identified new communication networks, new architectural design and network deployment have been proposed for wsn solutions in wqm applications. these developments are expected to advance the field of wsn for wqm in terms of energy efficiency, long range communication, reliable delivery of water quality data to different water monitoring centers, addressing the long-standing energy issues that have hindered the wide popularity of wsn solutions. also, critical recommendations are provided to revolutionize wqm applications. moreover, key future directions are provided to improve the performance of the recommended lpwan variants and the ieee 802.11ah network for the next-generation of wsn systems for wqm applications."
thus the target is by no means a passive site of impact. rather it is a new milieu reconfiguring the particle's identity. one major challenge for the success of drug delivery devices is therefore to continuously pay attention to the interactions between the technological device and the milieu.
"in order to investigate phase of neural oscillatory activities in general, the need of better analytic machinery to exactly characterize the phase is high. the general limitations can come from the preprocessing of wide-band neural signals being recorded, and phase computation methods themselves. by taking a parametric approach, we focused on removing the spurious phase variations that can produce misleading or incomprehensive results."
"typically, water quality sensor nodes run on battery power, which determines the lifetime of a specific water quality sensor node in a network devoted to the monitoring of water. it is important to emphasize that the system availability requirement, which is a function of the battery life, should be satisfied at any possible cost as a result of the critical nature of wqm applications. to maintain a balance between the battery life and system availability, it is obvious that an energy efficiency requirement is of the highest priority. basically, wsn solutions for wqm applications are anticipated to operate indefinitely without any form of human intervention such as battery replacement, which may be not be feasible or costly to realize in many cases. the software and hardware design of wsn solutions should consider energy efficiency in order to build a sustainable network. this can be realized through strategies that include energy harvesting, energy efficient communication technologies, energy optimization techniques, and duty cycling. energy harvesting is a promising strategy that allows water quality sensors to receive energy from their environment. communication technologies are key to realizing energy efficient data communication in wsn solutions for wqm application. as a consequence, it becomes crucial to consider energy efficient communication technologies for wqm application data networking since a huge amount of the battery power is often dissipated during the data communication process. energy optimization techniques are highly important to optimize energy resource allocation to achieve low-power operation. duty cycling is another interesting strategy that can be employed to lower the power consumption at the mac layer. this can be achieved by adopting mac modulation schemes that implement duty cycling techniques. such modulation mechanisms that implement duty cycling allows the communication radio to alternate between the active and sleep states, such that they are only active during data communication when they have data to either receive or transmit, and they switch back to a sleep state once they have accomplished their periodical data communication tasks and are not expected to participate in data communication for a period of time."
"traditionally acoustic-phonetic features are used for mispronunciation detection. we also developed a handcrafted feature-based model as a baseline to compare our results with other models. we extracted mfcc, spectral contrast, mel-scale spectrogram, chroma and tonnetz features from an audio dataset and pass them to classification algorithms. table 4 shows the performance of the three classification algorithms on handcrafted features in term of accuracy. the experimental results show that the svm algorithm due to its generalizability works best on handcrafted features for mispronunciation detection and achieve the highest average accuracy of 82.8% on 28 phonemes. svm algorithm is famous for mispronunciation detection tasks and optimal results are obtained by varying the kernel parameters and these results are obtained by using a polynomial kernel with data standardization. the neural network gives the second highest average accuracy of 79.22% and knn gives the least average accuracy of 75.6% on 28 phonemes because data consist of confusing phonemes and not easily separable, so accuracy is less as compared to svm and nn."
"we obtained the best results by using svm and achieve an accuracy of 82.8% on handcrafted features. to enhance the efficiency of the algorithm we reduce the feature dimension using information gain. to check the effectiveness of feature reduction technique we rum the algorithm twice, one without feature reduction and one with feature reduction. experimental analysis shows that feature reduction technique enhances the accuracy by 1% as shown in table 5 . we compared the result of svm, knn, and nn and experimental results show that on handcrafted features svm performed best and achieve an accuracy of 83.87% as compared to nn and knn as shown in fig 6 . nn achieve the second highest accuracy of 80.6% and knn achieve the least accuracy of 76.5%."
wherev l is the estimated voltage across the equivalent boost coil. this approximation will introduce some distortion in the voltage command due to the actual ripple content of the back emf voltage. the next section analyzes its effect in the boost current obtained.
"since the phy and the mac layers are defined by the ieee 802.15.4 standard alone, the specification of the remaining layers (i.e. network, transport and application) of the protocol stack is defined through company alliances. the ieee 802.15.4 communication technology is suitable for monitoring applications, and may as well be linked with an internet protocol (ip) network for internet capability."
"in (1), s r denotes the frequency of sampling (or sampling rate) and bn represents number of bits of a sensor's data. commonly adopted sensing rates are continuous and periodic, based on the requirement of an application. continuous sensing may dispense more energy compared to periodic sensing since it usually has a higher sampling frequency rate which obviously involves energy dissipation. an example is in a case event detection application that requires constant monitoring with about 1 ms sampling rate. in the case wqm applications, periodic sensing is acceptable and consumes less energy. for instance, in wqm applications, it takes minutes to hours for temperature to change in state [cit] . this makes it reasonable to use periodic sampling at a rate that ranges from once in minutes to hours. as a consequence, the exploitation of sampling rate is an effective approach to minimizing energy dissipation of the sensing system."
"wsn solutions for wqm applications should take flexibility into account. such consideration will enhance the productivity of wsn solutions in wqm application, including satisfying the needs of wqm stations in terms of easily adapting to new changes based on water station requirements. as an example, to enhance the points (or positions) interest that need to be closely monitored, there may be need to add a few sensor nodes to a network by the water monitoring personnel or engineers, or in a situation whereby there is a need for a wqm station to re-locate to another location which could probably be out of the signal coverage of the data transmitted by the sensor nodes on the water field. these issues should be adaptively managed by wsn solutions for wqm applications in a flexible manner."
"data preprocessing is a data mining method to convert the data into an understandable format. we have an audio dataset of arabic phonemes. to extract the features form cnn, we preprocessed the audio files which are explained in the following steps."
"a zigbee network is composed of three fundamental elements, which include zigbee sensor nodes, a zigbee router, and a zigbee coordinator. the aforementioned zigbee entities and zigbee compliant devices may be connected based on three possible network topologies, which include tree, star, and mesh or peer-to-peer (p2p) [cit] . with tree topology, a zigbee network is managed by a zigbee coordinator in terms of deciding on critical network parameters. a zigbee router is employed to extend a zigbee network. these situations are also applicable to a mesh-based zigbee network. the routers in a tree-based zigbee network employ a hierarchical routing approach to control signals and route data, while the routers in a mesh-based zigbee network provides complete p2p communication. with star topology, the zigbee sensors are connected directly to only one device known as a zigbee coordinator. consequently, the zigbee sensor nodes communicate their individual signals to the zigbee coordinator in a direct pattern. in a star-based zigbee network, the zigbee sensor nodes are controlled and managed by the zigbee coordinator. it is essential to note that with zigbee, a cluster tree network cannot be implemented."
"most existing solutions on wsn for wqm have not harnessed the utilization of lpwan communication technologies for water quality wireless connectivity over long distances, instead 4g, 3g, and 2g networks are mostly employed. lpwan communication technologies can be incorporated into wqm applications, which are believed to advance the development of energy efficient and reliable communications in wsn for wqm applications. also, based on the interesting features of lpwan-based solutions, including long range communication and low-power consumption, it is therefore envisaged to be a promising wan technology that could be employed as a communication media to deliver wqm application data to different water centers."
"as pakistani, arabic language learning is very important because pakistanis want to learn this language. we collect the data using a microphone and record in an open office environment. due to non-availability of soundproof room, noise added to the recording and we use fifth order high pass butterworth filter to evacuate noise from data. the arabic language consists of 28 phonemes. table 2 shows the number of arabic phonemes along with the number of samples used in this research work. the dataset was made by considering an equivalent number of male and female speakers as their ages extend from 10-50 and having distinctive first languages like punjabi, pushto, and urdu. a few speakers are profoundly capable, and some are toward the starting phase of learning the arabic language. five arabic language expert labeled the data. to remove any ambiguity, each language expert labeled a phone separately, if three experts have the same opinion than that label assigned to the phone."
using the linearity of the ht. here h(w t ) is also a gaussian distributed random variable (see chapter 8 [cit] with mean 0 and variance α. the analytic form of the signal x t using the ht is then
"in figures 10, 11 we show different examples of synthetic eeg signals where at some instances the corresponding envelope of the band-passed signal approaches zero. the regions corresponding to the low envelope has been illustrated in a red box. we show how applying the method with proper set of parameters can remove the spurious changes in the ip of the signal. the main incentive is to remove the spurious variations in r(t). as we are considering the phase information in a narrow-bandpass, we are required to filter the data accordingly. in section 3.4 we describe in more detail on the choices of parameter setting."
"considering the features of the new ble technology, it holds great potential for wireless connectivity in the next generations of wsns and iot applications for wqm systems, since the development of the ble solution captures their needs in the context of energy demand for short range communications. the new ble solution is also envisaged as a suitable technology for healthcare monitoring, wearables, and industrial application control systems."
"2g represents the second generation of the mobile cellular networks and is defined in the global system for mobile communications (gsm) [cit] . the network operates globally in the licensed cellular network bands of 900 mhz and 1800 [cit] mhz in american alone data [cit] . 2g gsm networks employ the frequency division multiple access (fdma) as a communication protocol to carry data such as voice call over the licensed cellular network bands. in an attempt to improve on the capacity of the 2g gsm network, two key protocols were introduced. the protocols are the time-division-multiple-access (tdma) and the code-division-multiple-access (cdma). the tdma protocol is used in 2g networks to divide the frequency bandwidth available in the 900 mhz band, which is 25 mhz, into eight possible time-slots. the essence of the frequency division into time-slots is to allow multiple users to communicate on the frequency concurrently. for example, with the created time-slots on the same frequency, eight voice calls is possible at the same time. on the other hand, the cdma protocol is used to identify caller, improve messaging and voice call qos requirements, and enhance the wireless connection of the user's access to the network airwaves [cit] . it is important to mention that the 2g gsm network was specifically designed for voice call services, and it works as a circuit switched system [cit] . therefore, it is not efficient for applications that demands fast data transmission such as multimedia. as a result, it is commonly used for fax, voice and messaging applications at a data rate of 2.4 kb/s to 9.6 kb/s. to circumvent the problem of slow transmission in 2g gsm networks when used for messaging purposes, a new communication standard known as general packet radio system (gprs) was incorporated to 2g to achieve an improved data transmission speed capability. with the integration of the gprs standard to 2g, a new cellular network is formed and the network is known as 2.5g or 2g gprs. consequently, for 2g technology to support gprs standard, there is a need for transitioning the 2g gsm network to a packet switching system, since the switching circuit system does not have the capability for packet data [cit] . the gprs is efficient for data transmission and can provide a data rate of 114 kb/s. with the 2g gprs network, the number of short message service (sms) messages that can be transmitted within a minute is increased. the gprs network is an ip compliant system [cit] . with the incorporation of gprs to 2g network, the network provides a suitable platform for services that include multimedia messaging service (mms), wireless application protocol (wap), and point-to-point (p2p) protocol. the data transmission speed of the 2g gprs network was further improved by extending the gprs to the enhanced data rates for global evolution (edge). with the 2g edge network, the data rate of the network is increased to about 384 kb/s. the edge technology, which is defined in the 3gpp standard, is also recognized as a member of the 3g network having met the specifications highlighted by the itu."
"since the missile metaphor is neither sufficient nor necessary to account for all the spectrum of astute stratagems required to secure therapeutic efficacy, it seems relevant to open the register of linguistic practices to alternative metaphors. [cit], technology plays a leading role in framing medical knowledge. it contributes to shape our concept of disease, our vision of medical intervention on the human body and, last but not least, our vision of the body. the ballistic metaphors rely on an abstract view of the body as a blank geometrical space through which a vector is moving towards a target. by contrast, the emphasis on the interaction with the biological milieu conveys the view of the body as a complex and heterogeneous environment. it consequently brings to mind a quite different metaphoric framework."
"(ii) network training -the network training module is used to create the network and train it using the 111 dataset generated in the previous step. users run a command-line script to perform the training. once training 112 is complete, the training accuracy is displayed and the trained model (classifier) is saved in the repository. the 113 accuracy of the classifier is dependent on how well the network is trained, which in turn depends on the quality 114 and quantity of training data (see section \"how much training data do i need?\" in supplementary materials)."
"examples of network topologies supported by ieee 802.15.4 technology are tree, star, mesh, and cluster tree. these topologies are shown in fig. 8 (a) through (d) . in a star network topology, the sensor nodes are connected directly to a coordinator node (such as a base station). this type of network model is employed to implement single-hop routing in wsns practices. in a tree network model, the sensor nodes communicate through a root node towards the base station. the sensor nodes that are connected to a root node are referred to as the children nodes. an important variant of the tree network topology is a cluster tree. in a cluster tree network topology, sensor nodes are organized into clusters using a parent-children relationship such that each of the cluster(s) contains a parent and the children. the parent is otherwise referred to as the cluster head, while the children are the cluster nodes. it is worthy of mentioning that each of the clusters in a network is configured with a unique cluster id, and the main essence of the cluster id is to recognize each of the clusters. a mesh topology also connotes a peerto-peer (p2p) topology. in a mesh topology, a sensor node may communicate via any of its neighbors to the coordinator. it is important to mention that there can only be one coordinator in a mesh topology. in practice, a mesh topology is utilized to implement a multi-hop routing in wsns. some important advantages of multi-hop routing are battery power optimization and self-healing. the battery power of a sensor node is optimized since it may communicate through any of its nearest neighboring sensor nodes to the base station. as a result of the multi-hop communication technique, the energy consumed by each of the sensor nodes is minimized. on the self-healing characteristics, a sensor node may use any of the available connecting links to its neighboring sensor nodes in case any of them is not available or fail."
the spectral roll-off is characterized as the frequency beneath which 85% of the extent of dissemination is concentrated. spectral roll-off measure the shape of spectra.
"secondly, to automate the process of feature extraction, we extracted the features using pre-trained convolution neural network alexnet. for this purpose, we preprocessed the data and fed to cnn for feature extraction. we extracted the features from different layers of convolution neural network i-e convolution layers (layer 4, 5) and fully connected layers (layer 6, 7, 8) . the features extracted from cnn layers have high dimensions and classification algorithms require a lot of time for processing. therefore, we reduced the dimension of data using principal component analysis and fed to the classification algorithm to evaluate the performance of each feature on these layers. the detail of each step is given in subsection."
"second, achieving a successful therapy is not simply a matter of guiding a projectile toward a target with the highest precision and optimal trajectory. the designer of the 'smart bomb' has to ensure the stability of the vehicle in the messy biological milieu for increasing its residence time in blood vessels. this challenge requires astute tricks to deal with obstacles occurring at each step. the first obstacle is the opsonisation of the carrier (the marking of a foreign body alerting the defences of the host organism that this exogenous element has to be cleared out from the blood vessels). how to negotiate with this self-defence of the host organism? to face this obstacle the 'magic bullet' had to be disguised or coated. the socalled 'second-generation nanovectors' [cit] are grafted with a sort of hair on the surface of their capsule to make them invisible to the macrophages along their trajectory. made of a polymer-most often polyethylene glycol (peg), because it is fda approved, cheap and biodegradable-the coating creates a cloud of hydrophilic chains, which repels for a time the plasma proteins that mark the non-self. this process named pegylation protects the carrier from disintegration by the immune system, thus enabling its circulation in 17 efficacy is generally defined as the ability to bring about a desired effect, whereas efficiency measures the ratio of beneficial output (e.g.: useful fork, economic profit) versus (the amount of means/resources) involved (time, energy, effort, costs…). therapeutic efficacy is the main concern in the development process (where it becomes the most important criterion). the blood for a much longer period of time. the pegylated particles have a circulating half-life of 45 hours versus a few hours for conventional liposomes."
"as a consequence, recommendations are given to advance the field of wsn for wqm, as well as future directions on the recommended communication network solutions."
"the spectrum is filtered with different band-pass filters and the power of each frequency band is computed. the filter bank consists of a set of filters with triangular frequency responses and their center frequencies are equally spaced on a mel-scale. for automatic speech recognition (asr) and speaker verification, such type of filter bank structure is used [cit] ."
nb-iot networks are mostly operated by cellular network operators. nb-iot is suitable for the transmission of small amounts of information over the free-license ism bands (noncellular network spectrum) and the licensed cellular network bands. nb-iot devices can transmit at a low data rate of 250 kb/s within a coverage range of about 25 km depending on the environment. nb-iot is a low-power wireless technology that can be employed for long range communications in iot and wsn applications.
"the new lpwan nb-iot communication network may experience high data latency because of the scarcity of random access channel resources. considering the critical nature of wqm application data to proactive actions by water management personnel, it will be good to improve the data latency of nb-iot for timely delivery of water quality data to various water control and monitoring centers. this will make future wsn solutions for wqm applications that adopt nb-iot technology more robust. to address the high latency issue that is associated with the nb-iot technology, the mechanisms employed in the design of the technology, which are aimed at minimizing power dissipation during communication, should be improved through optimization to enhance the latency performance of nb-iot. also, efficient decoding techniques for detecting multiple sensor nodes should be designed and deployed at the nb-iot base stations. the decoding techniques should have low-complexity."
"a satellite technology solution supports different types of mac modulation mechanisms for various satellite communication architectures. examples of such mechanisms are the conventional-based slotted aloha [cit], the enhanced spread spectrum aloha [cit], the slotted aloha based on conventional resolution diversity [cit], and the aloha based on multiple coded slots [cit] . it is worth mentioning that the frequencies in the ka-band (26 -40 ghz) and s-band (2 -4 ghz) are employed for data communication, unlike other wireless technology solutions that utilize either the ism license free spectrum or the licensed cellular network spectrum. satellite technology may only find more suitability in wsn solutions that employ high-technology-based sensor nodes for real-time data transmission of image and video recording in applications that include disaster monitoring."
"in direct communication architecture, the water quality sensor nodes are configured with satellite radios and are directly connected to a satellite over some of the aforementioned satellite-based communication technologies. the sensor nodes in the direct communication architecture can be deployed in two ways in practice, namely mobile deployment and specified deployment. in the former, the sensors may be connected to mobile objects that include aircrafts and ships, while the latter requires the placement of sensor nodes at specified locations. the direct connection of a wsn solution for wqm application to the satellite is described in fig. 10 ."
"the boost converter is required when the generator-voltage rating is lower than the grid voltage, or more precisely, the dc-link voltage. the boost inductance withstands the voltage difference between the dc-link voltage and the voltage at the generator terminals. in systems using generators with a voltage rating similar to or higher than the grid voltage, the boost converter is also required to enable the injection of current into the grid at low rotor speed. however, in the latter the boost inductance can be removed since there is no need of protecting the motor against a high voltage at its terminals. in this case the voltage boost function can be achieved by the generator stator phase inductances. the stator inductance in low-power pmsg is generally large enough to provide the boost capacity required for the generator voltage. therefore, this paper further investigates the elimination of the boost coil found in these systems, as seen with a dotted line in fig. 1(a) . this will automatically bring size reduction and cost reduction. this modification can be easily introduced even in existing low-power turbine converter designs."
"at the sensor node level, the battery is responsible for providing suitable energy to the node load, including at minimum the application sensor(s), the microcontroller, and the communication module. compared to the sensing and processing modules, the communication unit usually dominates regarding energy consumption [cit] . because of the high energy consumption during data communication, which often leads to the quick depletion of the available energy in a battery, it may be difficult to meet the specific requirements of wqm applications. this challenge may be mitigated by energy efficient strategies on the level of network design. it becomes important to ensure that wqm applications employ low transmission power communication technology solutions for disseminating data. yet, long communication range remains a key design goal in wsns that are dedicated to wqm applications because the water quality sensors may need to transfer their measurements to monitoring centers a great distance away from the application environment. consequently, both low transmission power and long communication range are crucial requirements to realizing energy efficient communications in modern wsn-based systems for monitoring water quality applications."
"in the dl communication phase, the base stations in an ingenu network employ a cdma scheme at the phy layer to synchronize the frequency and time among the base stations (sink nodes or access points) and the nodes (such as sensors) that are connected to the base station in order to transfer a continuous signal to the connected sensor nodes [cit] ."
"the average voltage across the generator-equivalent boost inductance, seen in fig. 3(b), in a pwm period is given by (5), assuming continuous conduction mode obtained from v r dsc and the measured boost current i b dsc, after low-pass filtering. the first option gives poor results since the signal ripple lags the ripple content of v r dsc . the second and third options give identical results, since the low bandwidth estimation of the third option is easily achieved by the current controller if the low-pass filtered rectifier voltage is used. therefore, for simplicity, the voltage across the generator-equivalent boost inductor will be calculated as follows:"
"the associate editor coordinating the review of this manuscript and approving it for publication was xudong zhao. pronunciation error detection. pronunciation means the way we pronounced/speak some word. speakers of various languages tend to form distinctive muscles of the mouth for a specific language. when we speak a non-native language, our muscles may not be able to take the same formation for that language which may results in pronunciation error [cit] ."
"in handcrafted feature model, firstly we extract the features that are important to distinguish the phonemes. an impressive number of features are used in literature, each one representing the characteristics of audio content [cit] . we used spectral features [cit] (mfcc, chroma, mel spectrogram, spectral contrast, tonnetz, pitch, root mean square energy and zero-crossing rate) as well as statistical features [cit] (mean, standard deviation, and slope) to detect mispronunciation. secondly, the data gathered is sparse and present in raw form and so we preprocessed the data to handle sparsity problem. thirdly we reduced the feature space to obtain the best features for classification. in the end, we pass the features to classification algorithms for mispronunciation detection. the detail of each step is given in the following subsections."
the hype surrounding nanotechnology and the claims that it brings about a revolution in medicine and pharmacy are based on the awareness of the limitations of current trends in pharmaceutical research. [cit] ended in failure. it is now widely accepted that the 'blockbuster model' is inadequate. cost soars and the rise of generic drugs are putting an increasing pressure on big pharmas as their amount of r&d time and expenditures rise while the output of new therapeutic molecules decreases [cit] . many products that are tested in clinical trials fail despite tremendous time investments -up to 15 years -and resource spendings. so hard is the 'valley of death' for candidate molecules before reaching the market that selective reporting of trials is a temptation for pharmaceutical companies [cit] .
"another promising communication technology solution for monitoring purposes that could push the advancement of wsn and iot applications forward is z-wave. currently, z-wave is utilized as a low-power and low-rate proprietary communication technology, and is managed by the alliances of z-wave. presently, the z-wave solution is not common in public research like other technologies because of the alliance requirements. z-wave is employed by a control unit of a wireless network to control the network devices [cit] . this can be achieved by configuring devices in a network as slave devices, and connected to a controller device, which has a controlling capability. z-wave solutions support sub- [cit] . specifically, the ietf ipv6 working group was responsible for standardizing the 6lowpan as an ipv6 on lowpan. as a consequence, the lowpan sensor nodes have an in-built internet ability, hence, they can directly communicate with an ip-enabled system, and can be accessed from the internet directly without utilizing any gateway. this is unlike a zigbee node that indirectly connects to ip-enabled systems through the help of a special gateway known as the 802.15.4/ip [cit] . this is the key difference between the standard zigbee solution and the 6lowpan solution. the 6lowpan solution provides a standard compression service by introducing a new adaption layer that converts sensor data in the ipv6 packet format to a lowpan-based packet format which carries a compression format header to make them fit for transmission on the low-rate and low-power links of the ieee 802.15.4 networks (phy and mac layers) [cit] . the 6lowpan technology mostly employs user datagram protocol (udp) as the traffic agent. it is worth mentioning that the battery life of 6lowpan sensor nodes is around a year (i.e. 365 days) [cit] . the 6lowpan technology supports the 868 mhz, 915 mhz, and 2.4 ghz license-free ism bands, and a maximum of 250 kb/s data transmission rate is supported by these bands over a typical distance of less than 100 m. 6lowpan technology may be employed in control (such as automatic light control), industrial and monitoring applications. 6lowpan is envisioned to have great potential for wsns and iot applications because of its low-power and low-cost capabilities. the 6lowpan technology achieves the transmission of its packets through the help of an application programming interface (api) standard socket."
"after data preprocessing, we load the pre-trained model alexnet that was trained on the imagenet dataset. we provided spectrogram dataset to alexnet and divide the data into training and testing images. we used 77% data for training and 33% data for testing."
"current research on targeted drug delivery differs in substantial ways from both traditions in that it turns therapeutics into engineering. indeed, like all therapeutic traditions, targeted drug delivery is mainly concerned with maximizing the efficacy of treatment and reducing harm. as such, however, therapeutics is conceptualized as an engineering problem that can be anatomized in unit operations."
"this investigation aimed to analyse and evaluate a range of sensors present in modern off-the-shelf mobile devices, and to determine which sensors, if any, would be suitable as a antirelay mechanism for nfc-based smartphone transactions. we shortlisted 17 sensors accessible through the google android platform, before limiting the investigation to those which are widely-available and displayed promise in our initial trails. in existing literature, only 12 sensors have been suggested as an effective proximity detection mechanisms, as listed in table i . some sensors are only available in specialised ambient sensor hardware and, in almost all instances, no relay attack data was collected to determine their effectiveness against such attacks. in this study, we implemented and evaluated 7 sensors and collected data representing a genuine transaction and a malicious transaction (from actual relay attacks). the objective of collecting these two separate sets of data at the same time was to empirical evaluate the implemented sensors with a high degree of objectivity. as part of our ongoing research, we are experimenting with simultaneously measuring multiple sensors within the transaction time duration. this is to evaluate whether this would reduce the risk associated with these sensors individually."
"questions from examples 4 and 5 in russian language have the same meaning but they differs lexically. simhash can handle this case because we remove stopwords and particles before calculating hash values. as an example this measure gives a strong connection between \"pc other\", \"computers, internet -software\" (\"software\"), \"computers, internet -internet\" (\"internet\") and \"hadrware\" subcategories. these 4 subcategories share common top-level \"computer, internet\" category in original otvety@mail.ru hierarchy. qsim also connects \"pc other\" subcategory with the subcategories \"science, technology, languages -technology\" (\"technics\") and \"goods and services -mobile devices\" (\"mobiles\") from different top-level categories. indeed in \"technics\" subcategory users ask many questions about computers and hardware like in the example 6. according to qsim the subcategories \"technics\" and \"mobiles\" has weak connection but they are connected too. example 7 shows the question that is more suitable to the \"mobiles\" category but was asked in the \"technics\" subcategory."
"category named \"golden\" is useless because it is not topical. according to formal definition the \"golden\" category is a special one and it includes selected questions about some facts which may be of interest to a wide range of users. we also do not use this category in our experiments."
recall that users who ask more than one question in otvety@mail.ru more often assign them only two different subcategories. we motivated by the assumption that users who are confused between two semantically similar categories ask question in two similar categories. we use this assumption to compute categories similarity. we call this measure user similarity and denote it by u sim. user similarity is calculated as follows:
"finally our contributions are threefold: 1) we describe a yearly non-english data set of questions that has not been previously used in research, 2) we perform a classification on a large data set that significantly exceeds in size data sets reported in the literature, 3) we investigate several approaches to category similarity, including users activity that can be helpful for category alignment in case of unbalanced and noisy label information."
an open-ended question is how to choose similarity measures' thresholds. recall that we selected it empirically and we do not provide clear guidelines how to choose it. anyway the more similar pairs coincided with the most confusable by baseline classifier categories. but it could just be the feature of our data set.
"we employed similarity metrics from related work to compute the similarity of the legitimate and illegitimate transaction pairs. this analysis -used prevalently in past work and other binary classification tasks, like biometricsassumes some threshold, t, is able to separate both illegitimate and legitimate attempts. the computed similarities provide a range of thresholds to test for each metric, before calculating the equal error rate (eer) for each sensor -defined as the intercept of the false acceptance rate (far) and false rejection rate (frr). this threshold would subsequently be used by decision authorities, e.g. the trusted authority in figure 3, to determine whether the transaction devices are proximate. this way, the success/failure rate of a relay attack being conducted successfully can be determined in the presence of ambient sensing."
"subcategories \"food, cooking -other cooking\" (\"food other\") and \"pc other\" have no such fluctuations. \"food other\" and \"pc other\" subcategories has small changes throughout the year -from 0.225% to 0.258% and from 2.0% to 2.4% respectively. almost every category has \"other\" subcategory (like \"pc other\" in the \"it\" category) which itself are noisy because they contain all questions that possibly have no suitable subcategory or could be assigned to more than one subcategory. this drawback heavily violates categories structure, makes them coarse and indistinguishable between each other."
"the paper is organized as follows. the next section survey papers on question categorization within cqa context. section 3 describes otevty@mail.ru platform and the data set used in the study, including category structure, distribution of questions over categories, user activity throughout the year. the approaches to quantify closeness of categories are proposed in section 4. section 5 discuss classification methods and reports overall performance including our approach."
in our work we address to the problems that are not considered in previous works. first we address to the problem of target category structure disadvantages exploration. we explore its drawbacks through user experience. we do not try to discover new topics and to find their location in the category hierarchy but we try to find the most confusable to user categories and use this information about structure violations in the process of category prediction.
"an ambient sensor measures a particular physical/environmental property of its immediate surroundings, such as temperature, light or sound; modern smartphones and tablets are equipped with such sensors. the physical environment surrounding a smartphone or payment terminal can potentially provide a rich set of features that are reasonably unique to that location -the sound in a library, for example -which could be leveraged for proximity detection. we illustrate a generic approach for deploying ambient sensing as a proximity detection mechanism for mobile payments in figure 2, with the following variations: figure 2, both the smartphone and payment terminal collect sensor measurements independently of each other and transmit these to a trusted authority. this authority compares the sensor measurements, based on some predefined comparison algorithm with set margins of error (threshold), and decides whether the two devices are in proximity to one another. 2) payment terminal dependent reporting. this setup,"
a software toolbox in matlab to undertake the analysis in this paper is available for free download. the software includes a user guide and example scripts. it is available from: http://www.neurospec.org/ appendix a. algorithmic descriptions for two and three variable cases.
"the proof of this central result follows closely that for the bivariate case [cit], using parseval's theorem [cit] . adopting the same approach as in the bivariate case allows the overall dependence to be decomposed summatively by direction"
"for bivariate random processes (x, y) a scalar measure of overall dependence is given by the squared correlation coefficient [cit] . this is defined in terms of ordinary and residual variances as"
"some categories are fine grained in the subcategory level: the largest categories are \"food, cooking\" and \"legal advice\" since they have 14 subcategories which encompass a wide range of sub-topics. the smallest are \"science, technology, languages\" and \"style & fashion\" as they have 4 subcategories which are quite coarse. also some top-level categories such as \"humor\", \"adult\" and \"other\" do not branch."
"on the other hand user may confuse top-level categories. for example the question from 3 is asked in the \"animals and plants -wildlife\" 10, \"animals and plants -houseplants\" 11, \"family, home, kids -housekeeping\" 12 and \"animals and plants -gardening\" 13 categories. this question is related to different toplevel categories \"animals and plants\" and \"family, home, kids\". for some sort of questions user assumes some categories to be synonymous. in the current otvety@mail.ru categories structure some questions could be assigned to more than one category."
as a result the set of connected using klsim categories pairs is very similar to the set of pairs obtained using qsim described in the previous section. we give a short comparison of these measures in the section 4.4.
we describe an experiment on topical classification of a large data set of russian questions originated from otvety@mail.ru. the main purpose of this experiment is to learn to recommend appropriate category for the new arrived question. when posting a new question the user has to assign it to a category using drop-down lists; currently no hints are provided. by choosing the topically correct category for the posted question the user increases her chance of getting a good answer in the nearest future. in this paper we show that most users are not familiar with original category structure and rely on the experienced users is impractical.
another approach is to find similar subcategories using vocabularies. we assume that similar subcategories have similar set of words because users ask similar questions. the kullback-leibler divergence (kl-divergence) is a good measure to find subcategories that are lexically similar.
"generally topics of the categories represent the interests of the community. common quite understandable \"seasonal fluctuations\" on some topics could be traced over time. figure 2 shows an examples of user activity in four subcategories. in the \"education -homework\" (\"edu homework\") category the number of question decreases in summer starting from june till september. percentage of questions about homework per month range from 0.26% to 3.8% throughout the year. the maximum of asked questions in the \"travel, tourism -holidays abroad\" (\"travel abroad\") category asked in the july -usually the holiday season -and the minimum is in the december. percentage of questions varies from 0.18 % to 0.45 %. questions about holidays are asked 2.5 times more often in july than in december."
"both sets demonstrate strong coherence between ca3 and ca1 lfp signals, and exhibit a modulation of the coherence and the directional components after application of ka. both the strength of coherence and range of frequencies change after ka is applied. this can be seen more clearly in the sections illustrated in figures 7 and 8. these illustrate the frequency (left) and time domain (right) bivariate directionality analysis for fixed blocks 6 (top; pre ka injection), 50 (middle; soon after ka injection) and 150 (lower; well after ka injection), respectively, corresponding to ∼1 minute blocks centred at 5.4, 49 and 148 minutes into the data set. these figures represent vertical sections through the time-frequency plots in figs 5 and 6, at fixed times."
"where n refers to the number of datapoints in a sensor measurement, and a i,j refers to the j th datapoint of the i th sensor measurement on device a."
classifier needs relatively small amount of data for training. recall that we have about 10 million of questions for training but the accuracy stops growing after 500 thousands of training samples. figure 7 shows accuracy values depending on the size of training data set for different classification tasks.
"the rate of accepted transactions using the distant instrument -analogous to the rate of successful relay attacks -is estimated by inspecting the eer. the far is equivalent to the proportion of successful distant transactions (fas) to the total of fas and correctly denied distant transactions (trs). the rate of potentially successful relay attacks can be estimated from the eers, which we present in table iii . we estimate, for example, that using the accelerometer with mae will result in 49.4% relayed transactions being accepted using a threshold of 0.913ms −2 . table iv shows the results obtained using five different machine learning algorithm that induce different types of classification models from data. for each dataset/sensor and learning algorithm, the table shows the mean and standard deviation of the 100 equal error rate estimates obtained using 10-fold cross-validation repeated 10 times. the best result for each sensor is shown in bold."
"our framework has applicability to both time series and spike train data. as advances in multielectrode array recordings generate ever larger multivariate data sets with lfp and single unit recordings there is a need for appropriate signal processing tools to infer network dynamics. the novel non parametric method presented here has the flexibility to combine spike train and time series data in a single framework, and is free from any concerns regarding the use of low order auto regressive models to represent electrophysiological signals. the framework should have broad application to a wide range of data including human electroencephalography (eeg) and magnetoencephalography (meg)."
"a substantial portion of work surrounding relay-attack countermeasures for contactless smart cards relates to distance bounding protocols [cit] . however, these may not be feasible for nfc-enabled phones -at the current state of the art -due to their requirement of high time-delay sensitivity and specialised hardware [cit] . as such, alternative methods have been proposed to provide proximity detection, most of which use environmental and motion sensors present on modern mobile handsets. in section iii, we discuss how ambient sensors have been proposed to counter relay attacks in nfc-based mobile contactless transactions."
"recall that we have 10,739,727 [cit] and test data set includes 939,472 questions. after removing questions from useless categories we have 8,456,252 questions for training and 815,170 for testing."
"top-level classification by the structure without general categories like \"humor\" and \"other\" and specific \"about mail.ru project\" category exceeds the classification by original categories structure results by 5%. \"humor\" and \"other\" categories itself are noisy because they have questions from all possible categories. in the \"humor\" category users can post jokes of any topic. determining jokes is another scientific problem and it is not addressed in this paper. performance of classification by subcategories (llc*) without categories \"humor\" and \"other\" does not differ from classification by subcategories (llc) of original structure. classifier is often confused between subcategories of different top-level categories. the same is relevant to the users. hierarchical classification is an effective approach in such problems. in 91% cases a correct category is in top 3 predicted categories. in this case we even do not take into account the similar categories."
"directed network analyses are widely used in neuroscience to infer network structure in multivariate neural recordings [cit] . the majority of approaches are parametric, which rely on estimating the parameters of a model to describe the patten of interactions between the observed signals, typically using autoregressive (ar) models [cit] . once the ar parameters have been estimated different metrics relating to directionality can be constructed directly as a function of the model parameters [cit] . a number of concerns have been raised regarding the validity of ar models to accurately capture the complex structure present in multivariate neural and other time series typically encountered in scientific problems [cit] . a number of alternative non parametric approaches have been considered to describe directed interactions in neurophysiological signals [cit] . a recent article introduced a non parametric framework for directionality analysis of bivariate data [cit], with application to single unit spike train data."
our goal is to improve classification performance by finding categories that often confuses users. regardless of the baseline classification results we try to find these confusable categories. further in the paper we show that our approach allows to connect categories that often confuses baseline classifier. we assume that categories that confuses users probably will confuse the classifier too so we modify categories structure to make it more unambiguous and less confusable to the classifier and as a consequence to the user.
"an overview of the measurement recording process across the four devices is presented in figure 5 . three android applications were developed. devices ti and ti were run- ning the same application, and host-based card emulation (hce) [cit] was used to achieve nfc communication with tt and tt . the application for device tt included two connection interfaces. for the first interface -used for the nfc connection with the device ti -device tt was set to nfc reader mode, allowing it to interact with discovered nfc tags. the second -used for connection with the device tt over wifi -device tt would broadcast a udp packet in the local network. in both, the message transmitted on the nfc or wireless channel included the sensor to be measured in that transaction, and a random 7-byte transaction id. the transaction id was generated by device tt and used in the analysis phase in order to uniquely identify each transaction across the devices. in real-world scenarios, device ti would act as the device communicating with tt . however, since the scope of this paper is to evaluate the effectiveness of the ambient environment as an anti-relay mechanism, device tt was responsible for sending the information across the wifi channel for greater transaction synchronisation. the final results were not influenced by this. lastly, the application running on device tt featured a broadcast listener for udp packets from tt. devices tt and tt were connected to the same wireless hotspot, created for the requirements of the experiment. upon receiving a packet from device tt, device tt would be able to initiate a transaction with device ti, upon tapping the latter to the former. after the initiation of a transaction, devices tt, ti, and ti would start recording data using some predefined sensor, for 500ms. on the android operating system, data captured by a sensor is returned to an application in time intervals set by the application. the rate at which data was polled from the sensors was set at the highest available that was the same across all three devices."
"simple distance functions give equal weight to each measurement obtained from the sensors. to address this, we consider machine learning to perform the discrimination between legitimate and illegitimate transactions more effectively through modelling non-linear interactions between measurements. the experiments we conduct apply the same evaluation strategy as the first method, but the threshold is based on the probability estimate output by the learned classification model, i.e. the estimated probability that a transaction is legitimate. moreover, to avoid optimistic bias in the error estimate when applying machine learning, it is necessary to perform a train-test experiment. more specifically, the full set of transaction pairs is split into a training set and a test set. the machine learning algorithm is applied to the training set to build a classification model that can output class probability estimates. once the model has been been built, it is applied to obtain probability estimates for the test set. when we split the data into training and test sets, we ensure that two transactions with the same id (i.e., a legitimate and a fraudulent transaction that were recorded simultaneously) are either both in the training set or both in the test set, to avoid potential bias. moreover, instead of using a single train-test split, we use 10-fold cross-validation repeated 10 times, a standard estimation technique from machine learning that generates 100 different train-test splits based on shuffled versions of the data. the learning algorithm is run 100 times on the 100 training sets, to build 100 models, and these 100 models are evaluated on the corresponding test sets. performance estimates from the 100 test sets are averaged to obtain a final performance estimate."
here w i is the set of words occurring in s i subcategory; the parameter β is a positive number smaller than the minimum word probability occurring in either s i or s j subcategories and γ is a normalization coefficient and it is based on the requirement:
to find similar and duplicate questions we use simhash [cit] algorithm. simhash is based on comparison of bags of words and gives the same hash values for the same and similar questions. in our application questions are similar if they have the same vocabulary but may have different set of particles and stopwords.
"at a point in time a user taps ti to tt; at approximately this point, ti is also tapped against tt and initiates the ambient sensor measurements. thus, at approximately the same time, we have three separate ambient sensor measurements for the three devices (tt does not record any sensor values). overall, for an ambient sensor to be effective, the trusted authority should be able to distinguish the genuine pair from the illegitimate pair. to evaluate each ambient sensor's effectiveness for proximity fig. 4 . test-bed scenarios detection and to detect relay attacks, we analysed the collected data using threshold and machine learning based analyses."
for users who ask questions only in two subcategories we assume that they might not be sure what subcategory best suits the question. some users might post one question in two different but topically similar subcategories. we check this assumption in the section 4.3.
"finally we take 106 pairs of similar subcategories for qsim similarity result, 78 pairs for klsim, and 40 pairs for u sim. table 1 lists the number of connected subcategories pairs for each measure. generally u sim connects subcategories from one common category. this means that users who ask only two questions is interested in one common topic and address one question within one top-level category. qsim and klsim connect subcategories both from same and different top-level categories."
"we computed equal error rates (eers) to determine optimal similarities/thresholds where the rate of false acceptances (far) is equal to the rate of false rejections (frr) for each tested sensor. the following notions of true accepts (tas) and rejects (trs), false accepts (fas) and rejects (frs) are defined for each evaluation: evaluation 1. ta: the legitimate pair, transaction instrumentterminal (ti i,tt i ) is accepted correctly. tr: the distant instrument-terminal pair, (ti i,tt i ), is rejected correctly. fa: the distant-terminal pair is wrongly accepted as a legitimate transaction. fr: the legitimate transaction instrument-terminal pair is rejected incorrectly."
at otvety@mail.ru some similar questions belong to different categories because sometimes it is hard for the user to determine which category is more topically appropriate to the question. we assume that subcategories are similar if they share many similar questions. we denote this method of finding similar question by qsim.
"the paper is arranged as follows. section 2 presents the methods including sub-sections on algorithms and significance testing. section 3 describes results from application of the conditional non parametric framework to simulated cortical neuron networks, to artificial mixtures of gaussian time-series used to verify quantitative aspects of the framework and to the experimental data. conclusions and discussion are in section 4."
"this section describes application of the bivariate directionality analysis to local field potential (lfp) recordings from bilateral hippocampus (hpc). the data is from a larger study investigating intra-and inter-hippocampal connectivity in a model of kainic acid (ka) induced mesial temporal lobe epilepsy (mtle) in rat [cit] . isoflurane anaesthetised lister-hooded rats (300-400g) had microelectrode arrays positioned in the left and right hippocampus. a cannula was attached to the left electrode for local injection of saline or kainic acid (1 mm, 1 µl) following a 30 min basal period. multiple single-unit and local field potential activity (lfps; filtered at 0.07 -300 hz) were recorded simultaneously using a plexon multichannel acquisition processor (map) [cit] for further details. all procedures had ethical approval and were carried out in accordance with the animals (scientific procedures) [cit], uk."
"as the result u sim measure connects subcategories from one common category of the original otvety@mail.ru categories hierarchy. it gives only two pairs of connected subcategories which subcategories is assigned to different categories in the original structure. these pairs of connected subcategories are \"music\"/\"drama\" and \"drama\"/\"internet\"."
hierarchical classification is an effective approach in hierarchical classification task. we denote it by tlc/llc. in hierarchical classification approach we build one classifier to predict top-level category and classifiers for every top-level category to predict subcategory.
"the presented results provide a basis for quantifying the effectiveness of various mobile sensors as an anti-relay mechanism in nfc-based mobile transactions. in both evaluations, investigated the success rate of a potential relay attack by considering readings between the terminal-distant instrument and the terminal-transaction instrument. the results of the first evaluation are shown in table iii . the rotation vector sensor performs relatively better with an eer of 0.330, while the accelerometer has a 49.4% possibility of a success -one of the highest in our analysis. not only does the eer imply the potential success of the attack, but also the potential of a legitimate transaction being denied. the rotation vector eer indicates that 33% of relay attacks would be accepted and 33% legitimate transactions would be denied. denying 1-in-3 legitimate transactions would invariably cause annoyance issues for users in practice. from this analysis, it is difficult to recommend any of the sensors for a single-sensor deployment for high security applications, such as banking. such sensors might be appropriate for low-security access control, but we recommend that a thorough analysis of the sensors and their performance in the chosen domain is performed prior to deployment."
"shown as double-dot-dash lines in figure 2, involves the smartphone encrypting the sensor measurements with a shared key (between smart phone and trusted authority) and transmitting the encrypted message to the payment terminal. the payment terminal sends its own measurements and the smartphone's to the trusted authority for comparison. 3) payment terminal (localised) evaluation. the smartphone transmits its measurement to the payment terminal, which compares it with its own measurements locally; the payment terminal then decides whether the smartphone is in close proximity. regardless of how a user interacts with the payment terminal, e.g. touching or tapping it with their device, the overall deployment architecture falls under one of the above scenarios. it can be observed that there is a potential fourth scenario in which a smartphone (payment instrument) could perform the comparison. in this study, our sole focus is on conventional transactions, requiring no specific interaction with the terminal, e.g. double-tapping, a gesture, or otherwise."
"one reason that past work achieved different results could be due to the significantly larger sampling durations (see section iii). the sampling duration imposed in our experiments was in line with the performance requirements of an emv application, i.e. 500 milliseconds. transportation is another major application for contactless smart cards; in this domain, the recommended transaction duration is far lower, between 300-500 milliseconds. the 500 millisecond limit in our experiment was thus an upper bound of the recommendations of two significant application areas where contactless mobile phones may be utilised."
"three samsung galaxy s4 (gt-i9500) android devices running android 5.0.1 were used in the experimental phase for data collection.this specific device was found to include a large variety of sensors, covering the majority of sensors supported by the android platform [cit], excluding the geomagnetic rotation vector. a nexus 5 android device was used as tt, which was not collecting sensor data. table ii lists the available sensors on the samsung galaxy s4 device, the ones used in our experiments and the rational behind excluding some. for the majority of excluded sensors, no values could be captured in the 500ms timeframe in our initial tests. based on these initial tests, for less than 5% of the 500ms transactions any data is being returned by the bluetooth, gps, network location and wifi sensors. furthermore, during the initial evaluations of ambient temperature and relative humidity sensors, we discovered insufficient data was returned by the sensors during 500ms for any meaningful proximity analysis. the proximity sensor on samsung galaxy s4 returns only a true or false value when the sensor, located in the front of the device, is covered or uncovered, hence it could not be used effectively in the experimental phase. lastly, the sound sensor, i.e. the microphone, of the specific device was tested; we found that this could not initiate and record values within 500ms. finally, after our initial analysis we selected seven sensors and discarded the others."
"on the one hand user may be confused at the level of subcategories. in the example 1 user asks question about graphics card in \"computer, internet -other computer\" (\"pc other\") subcategory while the similar question is asked in \"computer, internet -hardware\" (\"hardware\") category (example 2). example 1. \"what graphics card is better? gtx 560 or gt 630\" 8 example 2. \"what graphic card is better?\" 9"
"as mentioned in section iv, four devices were used in the data collection phase, tt, ti, tt, and ti. during the experimental phase, the devices tt and ti were placed at a distance of 5ft from the devices tt and ti. when ti was brought in close proximity to tt, an nfc connection between the two devices would be established, initiated by tt, indicating the beginning of a transaction. according to the emv standard, tt and ti should be in proximity, less than 3cm apart [cit] . during the analysis process the pair tt-ti represented the genuine devices, where no relay attack was involved. the pair tt-ti represented the genuine devices, where a relay attack was active. so, device ti had a double role, acting as both a genuine, and a relay device. figure 4 depicts the experimental setup."
"topical classification of questions is an area of active research. question classification can be helpful in several ways. first, category prompt for arriving questions makes question asking process easier for the user, maintains topical consistency within categories, and increases utility of categories for potential answerers (which again benefits questioners). second, cqa archives contain a vast amount of topically labeled questions. though partly noisy, these data can be still a valuable resource for question classification in external question answering tasks."
the first problem solution would significantly improve user experience while the second makes possible to offer to the user similar questions from cqa archives and possibly avoid the user from posting the question. in our work we address to the first problem.
equations (14) and (15) mimic the frequency domain implementation applying pre-whitening filters to the processes x and y that was used in the bivariate case. the difference here is that the output of the filters are conditioned fourier transforms which are optimally pre-whitened. auto spectra estimated by replacing the expectation in equations (10) and (11) with ensemble or segment averaging will have the value 1 at all frequencies:
"another major problem is that people often ask at otvety@mail.ru exactly the same and very similar questions in different subcategories, so categories and subcategories overlaps. all this make categories structure hard to use for both questioners and answerers."
in this section we describe classification approach and evaluation methods. table 2 presents evaluation results for different classification tasks and in section 5.2 we describe all methods with its notations.
"our framework assumes that random processes have wide-sense (weak) stationarity [cit] . the approach can be applied to time series data and point-process data. point process data are represented using differential increments which count the number of spikes in a small interval, which we assume to be the sampling interval ∆t [cit] . point processes are also assumed to be orderly, i.e. only one spike can occur in each sampling interval [cit] . in the derivation below (x, y, z) refer to three random processes which can be either time series or point process differential increments, or mixtures of the two data types. we use the term multivariate in the manuscript, since we are considering the analysis of three simultaneous random processes. however, only a single predictor is used, the possibility of extending the analysis to multiple predictors is considered in the discussion."
in otvety@mail.ru all questions are organized in categories hierarchy that has 28 top-level nodes and 186 leaf nodes. 7 figure 1 shows part of the otvety@mail.ru categories hierarchy.
all those papers use yahoo! answers categories hierarchy as a target structure. our data set differs in many ways from yahoo! answers. the most remarkable difference is the total number of categories. we describe in details our category structure in section 3.1. moreover some papers use not full yahoo!answers categories structure what probably should overestimate classification performance. finally we have very large amount of source questions organized into much smaller number of categories.
in the case of top 3 evaluation we take three most probable categories predicted by the classifier and see whether correct category are in the predicted. so we give the user an opportunity to choose between recommended categories.
"injection of local ka into the left hemisphere also modulates the interaction between the ca3 and ca1 lfp sig-nals in the right hemisphere as shown by the sections in figure 8 . the range of frequencies that exhibit significant coherence decreases from 110 hz (block 6) to 85 hz and 75hz (blocks 50 and 150, respectively). the interaction tends to become more one directional, the percentage of r 2 yx;200, in the forward direction increases from 67% in block 6 to 86% in block 50 and 90% in block 150. for the right side in block 6 the coherence and forward components both have peaks around 30 hz, in contrast the reverse component has a peak around 50 hz. in later sections the magnitude of the reverse component is greatly reduced thus the forward component is very similar to the ordinary coherence, in block 150 the maximum is around 20 hz."
this violates categories structure and makes user experience with the cqa service much worse. the classifier trained on this data set will probably confuse the categories that confuses the user. our goal is to find similar subcategories to modify original structure. the approaches of categories structure modifications are described in details in section 4.
in addition to inexperienced users problem we explore that otvety@mail.ru categories structure has some drawbacks. this leads to a further category structure violations. some categories are ambiguous to the user and overlaps with others. again this leads question assignment to incorrect category.
"the next evaluation applied machine learning classifiers (see table iv ). the gyroscope sensor with random forest performs significantly better, with an estimated eer of 0.179; in this analysis, the gyroscope was the best sensor, but all sensors apart from the gravity sensor provided some discriminative power. this illustrates that ambient sensors may have potential as an anti-relay mechanism, but the detection accuracy is not high enough to provide sufficient security in a real-world deployment."
"the hat indicates that these are estimates constructed from a single realisation of the three processes. a different realisation will result in a different pair of pre-whitening filters, this will achieve the objective of pre-whitening the conditioned auto-spectral estimates to 1 at each frequency. the pre-whitened discrete fourier transforms for each segment, l are"
we find semantically similar subcategories and connect them so these connections form the new categories from the old one so ambiguous questions could be assigned to them. we use three similarity measures to find similar subcategories.
"most of the removed questions was asked in the \"newcomers\" category. figure 3 shows 10 [cit] . almost 10% of the total number of questions in the data set was asked in the \"humor\" category. these top 10 subcategories comprise 40% of all questions and the other 60% are asked in the rest 176 (!) subcategories. this percentages changes slightly from month to month, but top categories remain the same. we used questions asked in 11 month to find similar categories and to train classifier classifier. [cit] are used to evaluate classification results. originally december data set had 989,521 questions but after removing redundant categories we get 939,472 questions."
"the use of ambient sensors for proximity detection in nfc-based mobile services is expanding, as illustrated by the number of proposals that currently exist. in this paper we extended the discussion to a large set of ambient sensors and included real-world relay attack data -not only analysing their effectiveness as proximity detection mechanism but also as an anti-relay mechanism. table ii shows that we have undertaken a comprehensive evaluation of ambient sensors for proximity detection and anti-relay effectiveness. a point to note is that in previous literature, ambient sensors were not evaluated for their effectiveness as an anti-relay mechanism (table i) . without a strong ability to detect relay attacks, the proximity detection alone does not warrant their deployment as an effective mechanism in nfc based mobile transactions."
"cqa data and tasks attract numerous researchers. various methods for finding similar questions, search over large collections of questions and answers, experts search, etc. are proposed in the literature. recent works made an attempt to organize (classify) cqa questions into an existing category hierarchy."
"in this paper we learning to predict the most probable category for the question. section 5.1 describes baseline classification category prediction approach. for clarity, presenting classification results we also provide examples of categories which confuses the baseline classifier."
"we evaluating classification by modified structures for qsim, klsim and u sim measure. evaluation on structure built with u sim measure gives us the 6 . classifier accuracy depending on the selected measure threshold lowest accuracy. generally u sim connects subcategories belonging to the one common category in the original otvety@mail.ru hierarchy while qsim and klsim connect subcategories from different categories that users often confuse. u sim possibly reflect an areas of user's interest and not subcategories that users often confuse within one common category because they do not know which subcategory is more appropriate for a given quesion."
1. remove punctuation and lowercase questions. 2. lemmatize words using aot 15 . aot is a software for automatic text processing and is intended mainly for the analysis of the russian language.
"we evaluate question classification performance over original otvety@mail.ru categories hierarchy and three modified categories structures. evaluation methods of classification over modified categories structures in table 2 is denoted similarly with its measures: qsim, klsim and u sim respectively. we independently classify questions over top-level (tlc) categories and over lower-level (llc) subcategories of original otvety@mail.ru hierarchy."
"original 186 otvety@mail.ru subcategories produce 17,205 pairs and it makes no sense to connect all subcategories so we have to choose the most similar subcategories. we select thresholds for all three measures independently and if pair similarity value does not pass the threshold's value we connect them in new one. in the section 5.3 figure 6 shows the performance of classifier depending on the selected threshold of similarities for all measures. empirically selected threshold values corresponds to the moment where classifier performance begins to sharply increase."
"following the recording time period, devices ti and ti would send a response message, containing the transaction id and sensor used to tt and tt, respectively. device tt would then validate the received data. in case of inconsistencies, data would not be stored for the specific transaction. the three devices would store the recorded data in a local sqlite database, along with the transaction id, sequence number, timestamp and the pre-defined location in which the recording took place. separate database tables were used for each sensor. the recorded data was stored in xml format. during the data analysis phase, only transactions that existed in all three databases, based on their transaction id, were considered. a total of 400 transactions were collected for each sensor, distributed in 2 distinct locations on the university campus."
"the idea is to find similar categories and connect them together. these new categories can be accounted in question classification task. to do that we propose three different methods to calculate similarity of categories using the following features: sharing of identical questions, similarity distributions of words, and user activity."
"our baseline is a standard approach for text classification tasks -support vector machine with bag of words features vector. it classifies questions by original otvety@mail.ru hierarchy. figure 5 shows the hinton diagrams of baseline classifier's confusion matrices for flat top-level and lower-level classification. figure 5 (b) shows the part of confusion matrix obtained for flat top-level categories classification. it is interesting that generally humor is the most confusable category and it is less often confused with technical categories than with non-tech (more frequently it is confused with \"society\", \"philosophy\", \"love\" and \"adult\" categories). here we can see that classifier frequently confuses subcategories from one common category like \"hardware\" and \"pc other\", \"religion\" and \"pshychology\", etc."
"near-field communication (nfc) [cit] has opened smartphone platforms to a range of application domains, particularly those based previously on smart cards. through card emulation, users may use their smartphones in a range of payment, transport and access control applications -leading to the deployment of services such as google pay and apple pay. the android platform also provides host-based card emulation (hce) [cit], which enables any application to take advantage of card emulation mode via nfc. deloitte estimated that 5% of the 600-650 [cit] . in the same year, 12.7% of smartphone users in the usa were actively using contactless mobile payments according to statista, while the value of such transactions is projected to grow to $114 billion (usd) [cit] 1 . similar trends are being observed in other domains where mobiles are used to deliver smart card-type services, like transportation and access control [cit] ."
"otvety@mail.ru categories hierarchy is useful resource not only for internal question category recommendation task. it also can be used to determine topic of the question from external resource -a search engine query subject for example. recall that query topic identification is actively used in the question retrieval task. some categories from original structure is not useful for topic prediction task. these categories are \"humor\", \"other\" and \"about mail.ru project\". \"humor\" and \"other\" is not objective while \"about mail.ru project\" is meaningful only for otvety@mail.ru users. the category \"other\" has questions on many topics as well as \"humor\". we exclude these three categories from original otvety@mail.ru categories structure and evaluate classification performance over top-level (tlc*) categories and lower-level (llc*) subcategories."
"1. obtain a minimum of 8 wild-type mice in the sex, strain, and age group representative of the experimental mice that will be used (e.g., 6 to 9 week-old female and male c57bl/6 mice). 2. handle all mice daily for 1 min over the course of 3-5 days prior to testing. 3. divide mice into groups of 4 and, if they are not already singly housed, move them into individual clean holding cages. 4. bring them into the testing room and allow them to acclimate for at least 30 min."
"the datasets generated using our method have demonstrated their usefulness in the related studies. in the case of orthology data, the heterogeneity of the orthologyrelated datasets suggested us to extend the work done in ogolod. orthoxml is the most popular format for representing orthology data. we have recently reused our work to define a canonical mapping from orthoxml to the domain orthology ontology (orth), so we are providing a means for generating open datasets to the orthology community. each orthoxml resource could be automatically transformed and exploited jointly with the rest of content. in the case of ehr data, the rules for classifying the patients by level of risk of developing colorectal cancer were implemented in owl. this effort permitted to develop a semantic infrastructure for classifying patients by levels of risk, which could be reused for other source datasets by just adapting the mapping rules. this would also permit the joint exploitation of different datasets transformed to our semantic infrastructure. therefore, the use of such approach permits to create reusable and extensible datasets, which is a feature needed for biomedical research datasets. another lesson learned is that the same approach can be applied for generating public or private datasets, since they only differ in their access policy."
"both the olt and nort are highly sensitive to the intrinsic value of objects and thorough testing of object equivalence is necessary to ensure that there is no intrinsic bias that can confound results. figure 3f and 3g show an example of inappropriate object selection. in a pilot test with a sample size of 4, mice showed a trend towards spending less than 50% of investigation time with object a when paired against object b ( figure 3f) . when these objects were then used in a nort with object a as the novel object and a larger sample size of 16, mice spent significantly less than 50% of investigation time with object a (figure 3g) . this aversion to the novel object is easily recognizable here as a technical flaw in the experiment and exemplifies why pilot testing of objects for inherent preference/aversion is essential."
"there are four major types of data integration architectures [cit] : data warehouse, mediator-based, serviceoriented and peer-based. the data warehouse approach is more related to the semantic transformation methods, and the other three are more related to odba, since they perform a virtual integration. in the literature, we can find biomedical semantic integration approaches such as ontofusion [cit] or tambis [cit], which fall in the area of mediator-based systems or ogo [cit], which follows the data warehouse approach. bio2rdf uses a data integration approach based on links. this is a case of virtual integration that uses owl:sameas statements to identify instances referring to the same entity in other resources."
"1. the class protein has an identity rule associated, and the properties of the identity rule have the same value for protein a in both resources. 2. the class protein has an identity rule associated, but the properties of the identity rule have different value for protein a in both resources. 3. the class protein does not have an identity rule associated. in the case 1, the definition of the transformation scenario in swit will determine whether (i) the two individuals are linked through owl:sameas; or (ii) they are merged into a single individual. in the case (i), the two instances of protein a would co-exist, but owl:sameas would facilitate interoperability. this would be an appropriate decision if having rdf versions of each source dataset is desired. the integration capabilities offered by swit would facilitate the creation of those links if all the data are stored in the same repository. the decision of whether merging or linking is done in the behaviour of the identity rule. in the case (ii), a new decision has to be made, since each instance of protein a would have a different uri, but the merged instance would have one uri. for those cases, swit permits to use either the uri of one of the instances or a different prefix to which an identifier would be added. this is also included in the behaviour of the identity rule."
"1. redefinition of the uri. we can use the uniprot uri instead of the dataset uri, since swit permits to define which prefix has to be used in the transformation of each entity. 2. linkage of resources with owl:sameas. the transformation of the protein uses the dataset uri but creates an owl:sameas link to the uniprot uri. either action can be applied to transform the data with cross-references to external resources, but this requires to know which external resources will be used during the definition of the mappings. additional research on the identification of related datasets would permit our method to evolve to generate richer linked open datasets."
"this article showed that the chaotic encryption of an image can be enhanced when using chaotic data from an oscillator that has a high kaplan-yorke dimension d ky . however, maximizing d ky of a chaotic oscillator is not a trivial task because the design parameters or coefficients of the mathematical model can have huge search spaces, as shown by the three case studies documented herein."
"in addition to this, the meaning of our identity rules is similar to the identity criteria proposed by formal ontologists [cit], because they determine which conditions are sufficient for identity. the properties used in identity rules are those that would be included in owl key axioms 25 . key axioms associate a set of object properties and datatype properties with a class, so each individual is identified by the values of such set of properties. hence, when two individuals have the same values for such properties, they are considered the same individual. key axioms are only applied over those individuals with asserted membership to a class. such inferencing-related limitation made us to define our identity rules."
"the data published in the linked open data (lod) cloud are diverse in granularity, scope, scale and origin, and the lod cloud is constantly growing with information from new domains. berners-lee 6 suggested a fivestar deployment scheme for open data, where each level imposes additional conditions. the use of rdf and an appropriate use of uris permit the achievement of fourstars datasets. the fifth one can be achieved by linking your dataset to external ones. it should be noted that the community is trying to impose additional conditions to get such stars [cit] . the number of biomedical datasets in the lod cloud is still reduced in comparison with the number of existing biomedical databases, but our approach aims at facilitating biomedical communities to join and follow the lod principles and effort. we believe that the development of methods that permit to get fivestar datasets would contribute to the development of the semantic web."
"commercial software packages are also available for scoring object investigation. commercially available software can provide a wealth of data beyond the hand scoring data described here, including total distance traveled, amount of time spent in certain areas of the arena, and speed of movement comes with restrictions on how many users can access the software concurrently, limiting the data throughput and time savings. though there are many advantages to these software packages, they are not necessary to gather the essential information of time investigating objects from the olt and nort. the ability to manually acquire this data makes novel object tasks more financially accessible to a wider variety of researchers."
"our approach requires transforming entities, attributes and relations, so the mapping rules must permit to achieve congruence at those three levels. to this end, three types of basic mapping rules have been defined: entity rule. it links an entity of the input schema with an owl ontology class. it permits to create individuals in the owl ontology. let s be an entity of the input schema and t be a class of the target ontology. then, the entity_rule(s, t) means that for every instance s of s, there is a congruent individual t which is an instance of t. for example, an entity rule in fig. 2 serves to link the element gene of the xml schema and the class gene in the ontology. an entity rule can be complemented by a conditional statement that transforms only those instances of s holding a certain condition on the value of a particular attribute. using the classes of the previous example, let a1 be an attribute associated with s, and let c1 be a boolean condition over a1. the entity_rule(s, t, c1) means that for every instance s of the entity s fulfilling c1 there is a congruent individual t which is an instance of t. attribute rule. it links an attribute of an entity of the input schema with the datatype property of an owl ontology class. it permits to assign values to datatype properties in the ontology. let s be an entity of the input schema, t an ontology class, and a 1 and a 2 attributes/datatype properties associated with s and t respectively. then, the attribute_rule((s, a 1 ), (t, a 2 )) means that for each instance of s associated with a 1 from the same schema, there is a congruent individual of t associated with the datatype property a 2 from the ontology and that a 1 and a 2 have the same value. for example, an attribute rule in fig. 2 links the attribute id of the element gene in orthoxml and the datatype property identifier of the ontology class gene. relation rule. it links a relation associated with two entities of the input schema with an object property associated with two classes of the owl ontology. let s 1 and s 2 be entities of the input schema associated through r 1 and t 1 and t 2 be classes of the ontology associated through the object property r 2 . then, the relation_rule((s 1, r 1, s 2 ), (t 1, r 2, t 2 )) means that given s 1 and s 2 linked through the relation r 1, and given the entity_rule(s 1, t 1 ) and the entity_rule(s 2, t 2 ), then for each instance of s 1 and s 2 there will be individuals of t 1 and t 2 respectively that will be linked through r 2 . for example, a relation rule in fig. 2 would link the hierarchical relation between species and gene in the xml schema and the object property ro:in_taxon in the ontology."
"the olt can be used independently or in combination with an additional test of memory that draws on neural activity from multiple brain regions, the novel object recognition task (nort). the nort is identical to the olt until the test phase, when one of the objects is replaced by a novel object instead of being moved to a new location. as is the case with the olt, mice with good memory of the objects will spontaneously prefer investigating the novel object. in contrast to object location memory, which relies heavily on hippocampal substrates, object recognition memory appears to rely on a variety of brain regions and the involvement of the hippocampus is unsettled. many studies report that hippocampal lesions or inactivation do not affect novel object preference 10, 13, 16, 17, while others find the opposite 18, 19 . however, it is still a commonly used task to evaluate general memory function in rodents."
"consequently, we believe that swit contributes to the interoperability of datasets, since it includes mechanisms for unifying uris and defining links between uris that represent the same entity, especially when all these resources are transformed and integrated in a common repository. further research will permit to manage swit integration processes in distributed repositories."
"for each environment. cues can consist of large shapes or patterns (typically in black and white) that enable mice to spatially orient themselves during the olt. instead of placing cues at different locations across from each other, cues can also be mounted to the walls of the testing area. this protocol recommends dividing the testing room with a curtain to hide the researcher and computer during behavioral testing. during all habituation, inter-trial intervals, and active trials, researchers should close the curtain to separate themselves from the testing area. if this is not practical, the computer can remain in view of the mice, but the researcher must move out of view during the task. if the researcher is present, the mice may try to rely on her or him as a spatial cue."
"one limitation of state of the art approaches and tools is that they are not generic enough in the sense of their applicability to both xml and relational data. data integration has to overcome issues such as redundancy or inconsistency between the data sources. most mediator or link-based approaches aggregate the data from the different sources, but the availability of mechanisms for preventing redundancy or inconsistency is not common. those mechanisms are easier to include in data warehouse-oriented methods, which provide more control on the data. our approach will be mostly based on data warehouse, since the integrated datasets (from xml and relational resources) are assumed to be created in a common repository. besides, in order to preserve the original datasets in the integrated, semantic repository, the configuration of the integration process will enable to merge those equivalent instances or linking them through owl:sameas statements."
"our experience in semantic data transformation in the last years reveals that the semantic representation of the data sometimes needs additional content that is not made explicit in the xml schema or in the corresponding table, so the additional meaning is not provided by the canonical transformation methods. we believe that such additional meaning can be included during the etl process. in this context, our patterns are equivalent to ontology content patterns [cit], which are focused on the definition of templates for the semantically-rich, precise description of the meaning of the content. we believe that ontology content patterns are a solution for those situations in which the source data does not contain all the information needed to generate semantically-rich rdf/owl content."
"our transformation process follows a data warehouse approach instead of an obda one because of the following reasons. first, we believe that the availability of rdf/owl resources is the most natural way of developing the lod. in our opinion, efforts like bio2rdf or the ebi rdf platform are correct ways of proceeding for the practical exploitation of biomedical data and the development of the semantic web. second, we are interested in generating owl knowledge bases over which owl2 dl reasoning can be applied, which is not ensured by current obda approaches. that would be a limitation from our exploitation point of view. we aim to obtain datasets linked to external sources, which is also easier to achieve with our approach. third, to the best of our knowledge, current obda approaches do not facilitate the application of ontology patterns as we do in this work, which also permits a semantically-richer representation and exploitation of data."
"1. place environmental cues (as described in the discussion section) across from each other and facing the arenas in the testing area ( figure 1b) . 2. arrange the four testing arenas in a 2-by-2 manner either on the floor or sturdy table at appropriate distances from the cues and the camera to maximize visual input to the mice (figure 1b) . 3. verify the line of sight using a meter stick propped from each arena floor over the wall towards the cues to confirm that these distances between the arenas and the cues are appropriate. 4. determine the optimal optical path length that allows video documentation of all four arenas by adjusting the height of the camera or the height of the table. (figure 1b) . 5. connect the camera to a usb extension cable. 6. using cable raceways, run the cable across the ceiling and down a wall to a computer running video capture software. 7. hide the computer behind a curtain that will separate the mice in the testing area from the researcher (figure 1c) . 8 . assemble 4 each of at least 3 different objects that are approximately 2-5 cm in length and width and up to 10 cm in height to use for testing ( figure 1d )."
"the protocol presented here delineates the steps involved in initiating and executing the olt and nort, both of which use an open-field-testing arena. commercially available behavioral testing equipment can be cost-prohibitive, particularly for smaller labs. this protocol includes the design and simple steps to build arenas in-house at the minimal cost and without specialized tools. furthermore, this protocol details the ideal behavioral testing area, including placement of arenas, contextual cues, and video recording system that sets the stage for implementation of the olt and nort protocols. representative results for successful as well as flawed studies are presented, emphasizing the importance of optimizing all materials and procedures for each study."
"from a process perspective, the semantic transformation of data requires the execution of extractiontransformation-load (etl) processes. canonical transformation approaches apply the same etl process to all the data. the required information about the semantics of the data sources is sometimes missing in the data schema or coded in natural language [cit], which makes such canonical processes not effective enough to obtain semantically-rich datasets. ontology-driven etl processes use ontologies for giving precise meaning to the source data, which will be made explicit in the transformation phase. this also enables consistency checking in the transformation and/or load phases, which prevents from the creation of logically inconsistent content. tools like rdb2owl [cit] and karma [cit] are examples of tools that exploit mappings between relational schemas and ontologies to generate rdf content."
"hippocampal memory function has been a focus of study across many fields of neuroscience because it is exquisitely sensitive to perturbation. negative perturbations ranging from prolonged stress and aging to seizures and stroke are associated with hippocampal damage 5 . in contrast, positive interventions, such as social interaction, physical environmental enrichment, or exercise, improve hippocampal function 6, 7, 8 . rodent studies with proper testing of hippocampal memory can reveal insight into the cellular and molecular mediators of memory as well as the effects of different environmental interventions on hippocampal function."
"we have used swit to generate the integrated ogolod linked open dataset [cit] . the first version of ogolod was created with the purpose of providing an integrated resource of information about genetic human diseases and orthologous genes, given the increasing interest in orthologs in research [cit] . ogolod integrates information from orthology databases such as inparanoid [cit] and orthomcl [cit], with the omim database [cit] . the content of ogolod was generated using a method to transform the content of relational databases into an rdf repository. the ogolod repository uses the ogo ontology [cit] as a scaffold to link the integrated information about orthologs and diseases."
"the integration method is useful for scenarios that require the creation of a repository that includes portions of data from different resources. in case of wishing a link-based integration, the mechanism offered by swit to include links in the mapping rules could be sufficient. the key objectives of the integration method are (1) detecting equivalent data instances to reduce redundancy and (2) ensuring the consistency of the resulting repository. both tasks are supported by owl reasoning."
"next, we discuss how and to what extent swit promotes or facilitates the interoperability of the datasets. let us consider a resource about proteins, which uses a local uri for each protein, but also stores the uniprot accession numbers (ac). let us suppose that we want to link every protein to the corresponding uri in uniprot. it should be noted that the uniprot uri for each protein differs in the ac. for example, the uri for the protein p63284 is http://purl.uniprot.org/uniprot/p63284. swit provides two different ways for creating such link:"
"data transformation and integration are based on the definition of mappings between the data schema and the owl ontology. the difficulty and the complexity of mapping not only relies on finding the corresponding entities in the domain ontologies but also on being able to design the corresponding ontology content patterns. once the rules and patterns are designed, swit reduces the implementation effort by executing them and generating the corresponding semantic content. patterns are also used in populous [cit], which is focused on creating ontologies from spreadsheets. our experience reveals that semi-automatic mapping techniques would contribute to significantly reduce the mapping time, so efforts in this area are key to support the mapping process."
"notably, the presented protocol uses spatial olt before the nort, which may bias mice towards using hippocampal processes in the nort. thus, it is important to note that the order can be reversed, or each task can be run independently, depending on the experimenters' questions and needs."
"in this section we explain how we have used swit in three biomedical domains. our website contains more information about these efforts, including the mapping files and examples of content generated."
"in the cases 2 and 3, swit would generate two different individuals, since the sufficient conditions for identity are not met. the current version of swit does not discover equivalent instances, which is considered further work. discovery methods should be carefully used, since the identity rules defined in the ontology should be respected."
3. perform the olt. 1. move one of the objects used in the training trial to a new non-release corner and affix the object 6 cm from each wall of that corner with double-sided tape (figure 2c) . note: the other object should remain where it was during the training trial. 2. start recording the video. 3. place each mouse facing the walls in the release corner. 4. allow mice to investigate the objects for 10 min. 5. stop recording the video. 6. place mice back in their clean holding cages for an iti of 20 min. 7. clean all arenas and objects with animal facility recommended cleaning methods such as wiping with 70% ethanol to minimize olfactory cues.
"the definition of the mapping rules will be illustrated with the example described in fig. 2 . in this example, (1) the input schema is orthoxml [cit] (figure 2 top left), which is a standardised format for the representation of information about orthologous genes, (2) the ontology is the orthology ontology (orth) 12 (fig. 2 top right), which models domain knowledge about orthology. in the example, the entities of the input schema are represented with boxes, the attributes with @, and the relations with arrows. in the ontology, the classes are represented with rounded corner boxes, the datatype properties with a pin attached to the classes, and the object properties with arrows. mapping rules link entities, attributes and relations of the input model with the ontology classes, datatype properties and object properties. in fig. 2, dashed lines represent the mappings from the xml schema to the ontology. for simplicity, this figure does not include mappings involving relations or object properties. the ontology contains a series of prefixes, which refer to ontologies reused in the orth: ro (relations ontology 13 ), ncbi (ncbi taxonomy 14 ), cdao (comparative data analysis ontology 15 ), and sio (semanticscience integrated ontology 16 ) generally speaking, the application of a mapping rule to an instance of the input dataset generates one individual in the ontology. the instance and the individual must fulfill a relation of congruence. for us, an ontology individual t is congruent with an input instance s if t can be obtained from s by applying a mapping rule and t is consistent with the axioms expressed in the ontology and with s. the consistency with the axioms expressed in the ontology has to be understood in owl dl terms. the individual must be, in logical terms, a member of the class to which the membership is stated."
"the lyapunov exponents of the three chaotic oscillators were obtained by applying wolf's method [cit] . the initial conditions (x 0, y 0, z 0 ) matters to reduce computing time in computing lyapunov exponents and d ky . in this manner, the appropriate initial conditions are: (ae 0.5, 0.5, 0.75) for the chaotic oscillator with infinite equilibria (1), (0.5, 0.5, 0.5) for rössler (2), and (0.1, 0.1, 0.1) for lorenz (3) ."
"we showed that metaheuristics like de and pso algorithms are quite suitable to maximize d ky, both algorithms were run with the same conditions to find feasible solutions. the two highest values of d ky of each chaotic oscillator were selected to encrypt an rgb image and a correlation analysis was performed between the original image and the chaotic channel to identify the best masking. in this manner, after testing each state variable of each chaotic oscillator, the lowest correlation was provided by lorenz, as showed in table 5 . this confirms that the best chaotic encryption can be performed if one maximizes d ky, as shown herein applying de and pso."
"identity rules are fundamental in the integration process, because they control the redundancy of the individuals created. they describe which properties permit identifying an individual of a certain ontology class. for example, we could integrate two resources about proteins which use different identifiers for the proteins, and those identifiers are used in the uri of the individual. those resources might be using the human genome nomenclature for naming the gene associated with the protein. if the gene name is used in the identity rule, then swit would find that both individuals refer to the same protein."
"in this paper we have presented an approach that is able to generate open biomedical repositories in semantic web formats. the generation process is based on the ontology-driven transformation of data and integration is performed following semantic web principles. the method has been implemented in the swit tool, which automates and standardises the process of generating five stars, semantic, open biomedical datasets. we believe that the approach (1) enables the common transformation and integration process for heterogeneous biomedical data; (2) permits the design of reusable mappings driven by domain knowledge; and (3) applies linked open data principles to generate interoperable, semanticallyrich, open, biomedical datasets. future work will address the assistance on the recommendation of semi-automatic mappings and on providing more flexibility in the transformation process."
"the availability of biomedical datasets in open, semantic formats would facilitate the interoperability of biomedical data and would enable to carry out scientific studies with larger, connected datasets. in this paper we have presented a solution based on the transformation and integration of heterogeneous data resources in which ontologies play a central role. a series of important aspects of our work are discussed next."
"the statistical tests used in this protocol are representative of traditional methods of analysis. when comparing 2 groups of mice, a two-sample, two-tailed t-test is recommended to test the significance of the difference in percent time investigating the moved or novel object between groups. if the sample sizes are uneven, have exceptionally high variability, or show uneven variability between groups, a non-parametric test is recommended instead, as many of the underlying assumptions of the t-test will be violated in these conditions. when comparing 3 or more groups of mice, anovas are recommended to test whether group has a significant effect on percent time investigating the moved or novel object. error-corrected post-hoc tests, such as tukey's test or bonferroni-corrected pairwise comparisons, can then be applied to test the difference in percent investigation time between each pair of groups. with the olt, an additional way to analyze data is to test for a significant change in percent time investigating the moved object from training to olt trials. with only one group of mice, this test would take the form of a paired, two-tailed t-test, testing for significant changes in percent time with the moved object from training to olt in each mouse (keeping in mind that these are paired analyses and not independent measurements, since each mouse yields two data points). with two groups of mice, a repeated measures two-way anova would be used, with the repeated measure being the trial (training versus olt) and group assignment being the second factor. post-hoc tests for differences between treatment groups should compare the groups to each other within trials."
"on the medical and clinical side, the advent of electronic health records (ehrs) contributes to making more data available for computer processing, but suffers from similar problems. the heterogeneity of ehr systems can be assimilated to the one of biological databases. the semantic interoperability of ehr data not only has been identified as a need but also considered as a reason for inefficiencies within the healthcare system [cit] and for the waste of billions of dollars in the united states annually [cit] ."
"while the hippocampal dependence of the spatial memory functions tested in the olt are well-established, the nort may or may not rely on the hippocampus. interpretation of data from the nort should take this caveat in to account. the determining variables, for whether the hippocampus is involved, in object recognition memory are not agreed upon yet, but could include iti length or saliency of spatial cues 18 ."
"in summary, this protocol effectively tests memory in mice at minimal costs. recommendations for appropriate modifications to the protocol are included to ensure successful implementation with any small rodent model. application of this protocol to specific injury or therapeutic intervention models can reveal valuable functional relevance that complements the cellular and molecular mechanisms being studied."
"in this section we describe the main results of our work. first, we will describe the tool that implements the transformation approach. second, we will describe how the tool has been used in different biomedical scenarios."
"the joint semantic exploitation of data stored in xml files or in relational databases requires methods for the transformation of the data into semantic formats. both xml technologies and relational databases provide schemas which define the structure of the datasets. in our approach, such schemas will be used to define generic processing methods able to transform and exploit xml and relational data using semantic technologies. more concretely, xml schemas, adl archetypes and the schema of relational databases will be managed in our approach. in practical terms, adl archetypes play the role of xml schemas."
"open-field-testing arena assembly with part a corresponding to the inner wall, part b as the outer wall, and part c as the base. the finished arena will have two outer walls (parts b) that run the entire edge of the base and two inner walls (parts a) that fit between the outer walls on the adjoining edges of the base (part c). all walls will rest on top of the base. (b) representative arrangement of arenas on a 0.62 m high table, 60 x 90 cm environmental cues, lights, and a camera for a testing area that allows the capture of all four arenas simultaneously. (c) a curtain hides the experimenter and computer system from mice during trials. the overhead lights are on for the purposes of taking this photograph, but during testing, only the floor lamps are on. also, one of the environmental cues has been removed for this photograph of the testing area, but during testing, there is a fourth cue in front of the arenas, facing the all black cue behind the table. (d) a representative object (and ruler for scale) that is appropriate for olt or nort testing with mice. please click here to view a larger version of this figure."
"in this paper, we have used the mapping rules for creating owl individuals from xml or relational data, but the process can also be applied for the creation of ontology classes. this might be helpful in case the content of the owl ontology used is not sufficient for mapping the input schema. for this purpose, the mapping rules were extended to produce owl classes instead of individuals. this class-based approach also permits to use patterns. the only difference is that the set of variables associated with the pattern are bound to entities instead of instances. we have actually applied such approach for the generation of openehr archetypes from cem clinical models [cit] . in that study, the input schemas were owl ontologies corresponding to cem clinical models and the openehr owl ontology was the output schema. basically, the creation of the openehr clinical models was approached as extending the openehr owl ontology with the specific content of the clinical model. in owl, being an individual or a class can be seen as the role played by a given concept [cit] . the representation of knowledge may therefore be based on individuals or classes, this decision depending on the expected use of such knowledge. in fact, punning was included in owl 2 dl to enable different uses of the same term, so an individual and a class can have the same uri. from a formal ontology perspective, enabling this possibility might be reason enough for a criticism to our approach, but it is needed from a practical perspective. this situation can be exemplified in the orthology use case. orthology databases basically contain information about genes, proteins and organisms. in the database they are represented as individuals, but they semantically correspond to classes, since there are many instances of each gene, protein and organism."
"1. open the video file. 2. apply a transparent circle that provides a border of 2 cm around each object over the screen to help determine active investigation. use a video file image with a ruler placed in an arena to calibrate this grid. 3. observe mouse behavior and record the times the mouse is actively investigating the object, which consists of its nose pointed at the object at a maximum distance of 2 cm from that object. figure 3 provides examples of typical positive and negative results obtained with male and female adult c57bl/6 mice using this protocol 6 . interpretation of olt and nort data always applies to the aggregate data of a group (see discussion below). investigation time for a single mouse cannot be interpreted as memory or lack of memory. however, the performance of a group of mice (i.e., multiple samples) can be compared to other groups or to the fixed chance levels using statistical testing. during a typical training trial, groups of mice do not show a significant preference on average for either of the objects as they are both equally novel and do not have any intrinsically negative or positive value to the mice (figure 3a and 3b) . if the aggregate data of a group of mice show significant preference for one object over another during training, these objects should not be used because that inherent preference/aversion will confound results in the subsequent trials. additionally, the total investigation time of all the mice must meet a minimum standard (traditionally set at 20 seconds 21 ) and should be compared to ensure that there is no baseline difference in investigation that may affect subsequent tests of memory."
"all behavioral testing should be completed in a temperature-and humidity-controlled environment with dim, but even illumination at around 310 lux and minimal extraneous sound or strong environmental odor cues, such as perfumes on experimenters. between each trial and testing day, all arenas and objects should be cleaned with animal facility recommended methods of sterilization such as wiping with 70% ethanol or unscented bleach wipes to minimize olfactory cues. if chemical disinfectants are used, a final rinse with 70% ethanol is recommended because many chemical disinfectants can be irritating to the animals' feet. as with any behavioral task, handling mice for several days before testing is necessary to familiarize them with the individuals who will be performing the olt and nort and reduce stress during testing 20, 22 . as mice can experience acute stress due to unfamiliar individuals in the vicinity of the testing area, it is also recommended that all behavioral testing should be completed by the same individual(s). the testing parameters and conditions detailed in this protocol have been optimized for 6 to 9 week-old adult c57bl/6 mice and would be most useful in revealing memory impairments due to injury in this age group or memory impairments due age itself in older mice. if the aim is to test for memory improvements in young mice, a longer iti ranging from 1 hour to 1 day would be more appropriate to avoid ceiling effects on performance in the easier 20 min iti version. indeed, itis can range from 5 min (for immediate recall) to several hours or days (for remote memories), depending on the specific needs of the experiment and the strains and ages of the mice. importantly, regardless of the length, all itis should be consistent between sessions in an experiment. as different strains and ages of mice exhibit differences in behavior and learning, the timing for each trial and interval, testing area arrangement, and objects used can be modified according to the particular strain of mice, their age, and the specific injury/disease/intervention model being tested 9, 21, 23, 25 ."
"the main objective of the present work is to propose a method that could serve to simplify the process of generating integrated semantic repositories from heterogeneous sources. the approach will be able to work with relational databases, xml documents, and ehr data and will produce datasets described by means of ontologies. the transformation process is independent of the formalism used for capturing the data to be transformed. this process will be driven by the semantics of the domain to ensure the correctness and logical consistency of the resulting content. this will be achieved by defining mappings between the data schemas and the ontologies, which will provide the semantic content. our approach will be able to create a repository from multiple sources, which will require to define mechanisms for merging the data about the same entity contained in the different resources. besides, the resulting content will be generated according to the principles of linked open data. we will also describe our semantic web integration tool (swit), which implements the transformation and integration methods, and the application of our method in different use cases. the expected contributions of our research are (1) the common transformation and integration process for heterogeneous biomedical data; (2) enabling the design of reusable mappings between schemas driven by domain knowledge; (3) the application of linked open data principles to generate interoperable, semantically-rich, open, biomedical datasets."
"the integration of data is carried out through the transformation of each input resource. the mapping rules enable to generate owl content, and the identity rules are applied during the transformation process to limit redundancy and to merge data instances. their role is to identify which instances from different resources correspond to the same domain instance. obviously, individuals with the same uri are considered the same one. the integration method makes the following decisions concerning the usual problems in integration processes:"
"to the best of our knowledge, there is no standard language to define mappings from different types of input models to owl ontologies. we are currently using a language based on the former ontology alignment format 22, which has evolved into edoal 23 . the w3c has developed the r2rml mapping language 24 for mapping relational databases to rdf, but does not cover xml. our current language permits to express mappings from relational databases and xml schemas to owl ontologies, and it could be easily extended to cover new types of input models (i.e., owl ontologies). our mappings can be reused, especially for data transformation processes that use the same owl ontology, but the lack of standardisation in this area forces third parties to do some additional work in order to include the mappings generated with swit."
"let us revisit now the example introduced in the integration subsection, whose goal is to integrate data about the protein a from two resources which have different identifiers for that protein. let us suppose that the transformation and integration are driven by an owl ontology which contains a class protein. generally speaking, these are the possible situations:"
"most state-of-the-art transformation approaches and tools from xml or relational databases into rdf/owl are based on canonical transformations or are not based on mappings with domain knowledge (i.e., ontologies). such tools mainly perform a syntactic transformation of the traditional formats, making the semantic interoperability of the obtained datasets difficult. besides, there are no methods that can be applied to both xml and relational databases. our method provides a semantic representation of the input datasets by performing a transformation guided by domain knowledge using an owl ontology, so performing an ontology-driven etl process. this is similar to rdb2owl [cit] and karma [cit] . swit and rdb2owl have in common that the mappings between the input model and the ontology are manually defined, but rdb2owl is limited to input datasets in relational format and does not provide any solution for the problem of complexity on the manual definition of mappings when using complex ontologies or data integration. karma has the advantage of performing semi-automatic mapping of input databases and ontologies. however, this mapping process depends on the existence of a knowledge base of previous mappings."
"in the last years, bio2rdf has become the most prominent initiative for the generation of biomedical rdf datasets. bio2rdf has developed rdf versions for 35 datasets (release 3 [cit] ), and its website contains non-canonical transformation scripts for such resources. to the best of our knowledge, the links between the data and the domain knowledge are not made explicit in such transformation scripts. for instance, there is no guarantee that content about a protein from different resources is transformed using the same meaning, and this makes more difficult to expand the approach and to find errors."
"the semantic web community wishes to achieve the web of data, which would semantically connect datasets distributed over the internet. the linked open data (lod) effort 1 pursues the publication and sharing of biomedical datasets using semantic formats such as rdf 2 or owl 3 . the biomedical community is heavily involved in the development of the lod cloud [cit], since integration and interoperability are fundamental for biomedical data analysis [cit] . the lod cloud offers a promising infrastructure for such a goal. the availability of consensus ontologies generated by the biomedical community facilitates the publication of data in the lod cloud, since those ontologies can be used as vocabularies for the rdf datasets. most efforts in this area have been solved by in-house solutions, implementing resource-specific transformation scripts. hence, we believe that there is a need for methods and tools that contribute to standardise the process of getting biomedical datasets in semantic formats."
"1. using double-sided tape, affix objects 6 x 6 cm 2 away from 2 non-release corners such that they are counterbalanced in the arena. 2. start recording the video. 3. place each mouse facing the walls of the release corner as done during the habituation sessions. 4. allow mice to investigate the arena and objects freely for 10 min. 5. stop recording the video. 6. place mice back in their clean holding cages for an iti of 20 min. 7. clean all arenas and objects with animal facility recommended cleaning methods such as wiping with 70% ethanol to minimize olfactory cues."
"translational research aims at applying basic biological results and data into clinical activities and routine. in recent years, supporting data-driven medicine has been set as a challenge for translational bioinformatics [cit] . for this purpose, the integration and joint analysis and exploitation of heterogeneous data, both biological and medical becomes critical, hence, solutions in this area are required."
"next, we describe the two major approaches for data exploitation using semantic technologies: (1) the transformation of data into semantic formats; and (2) ontology-based data access, which works on traditional formats."
"an additional feature of this protocol is that the first session of habituation is essentially a trial in an open field, which can yield data on activity and anxiety levels. activity levels can be quantified using total distance traveled or average speed, if tracking software is available. similarly, with tracking software, anxiety measures can be derived from time spent in the center of the arena or distance traveled in the center. these data can also be acquired manually by overlaying the arena video with a grid and quantifying crossings. this open-field data can help rule out gross motor deficits or excessive anxiety that might interfere with object investigation later."
"biomedicine is a knowledge based discipline, in which the production of knowledge from data is a daily activity. current biomedical research generates an increasing amount of data, whose efficient use requires computing support. traditionally, biomedical data have been stored in heterogeneous formats in various scientific disciplines. since the need for facilitating the integrated use of such resources. unfortunately, there is also heterogeneity in the formats used for storing such data, since they are not usually the most machine-friendly ones [cit] ."
"in humans, specific tasks have been designed to assess the performance of targeted brain regions, such as the wisconsin card sorting task for prefrontal function or the paired associates learning test of the cambridge neuropsychological test automated battery (cantab) for hippocampal function 2, 3 . these tests are designed to study the functions of specific brain regions in humans by assessing behaviors that result from the neural activity of those regions. the end goal of most biomedical research is the improvement of human health; however, many studies of brain function in health or disease cannot be ethically performed with human participants. for studies that cannot use human participants, small rodents such as mice are often the model of choice. using mouse models allows for the direct control over experimental manipulations including alteration of gene expression, induction of injury or even modulation of circuit activity through optogenetic techniques. behavioral testing of mice, similar to human testing, aims to assess the effect of experimental variables on brain function by measuring behaviors that rely on specific regions."
"quantification of object investigation time can be accomplished in various ways either with or without expensive analysis software. if manual scoring is used, as described here, the most comprehensive data collection can be achieved by recording the time stamps on the video when the mouse is investigating the object. recording the start and stop times of object investigation during the three individual trials creates a permanent log of unbiased time stamps and facilitates accurate manual calculation of data as opposed to alternative methods such as using a stop watch to additively record the total time a mouse investigates each object."
"the transformation approach has been implemented in a web tool called swit 18 . swit provides a web interface through which the user is guided to perform all the steps of the process. swit is currently supporting mysql databases, xml schemas and adl archetypes as input schemas. swit permits to generate the output dataset in owl or rdf formats, which can be downloaded or directly stored in virtuoso [cit] or in a jena knowledge base 19 . the user can define the mappings between the input schema and the owl ontology. for this purpose, mappings created in other transformation processes can be uploaded and reused. once the mappings have been defined, they can be executed, thus generating the corresponding rdf/owl content. swit applies the mapping rules to the data source to generate the semantic content, checking the identity rules to guarantee that redundant individuals are not created. this process also uses automated reasoning to ensure that only logically consistent content is transformed. swit uses both the owlapi [cit] and the jena api for processing and generating the rdf/owl content, hermit [cit] as reasoner, and the patterns are implemented using oppl2. figure 4 shows a part of the mapping interface, which has three main parts. the left side shows the input schema using a hierarchical representation. the right side corresponds to the owl ontology. the lower part of the figure is a text box, which contains the mapping rules defined. for example, the third line defines the mapping of the attribute coorddimension of the entity molecule to the datatype property coord_dimension of the ontology class molecule. figure 5 is a screen snapshot of the definition of the mapping of entities of the input schema to a transformation pattern. in this case the input schema consists on openehr archetypes (left), which are mapped onto an ontology transformation pattern for histopathology reports. in the figure, we can see that the mapping would associate a particular element of the archetypes with each variable of the pattern. in this case, the expression corresponding to the mapping rule is not shown in the figure."
"the olt is a simple and effective test that provides a measure of hippocampus-dependent spatial memory 13 . the task relies on an animal's intrinsic preference for novelty without additional external reinforcement and can therefore typically avoid complications associated with differential emotional responses 13 . the present protocol for olt is presented for mice, but it is also effective in rats if the dimensions of the equipment are appropriately scaled. the protocol consists of acclimating a mouse to an open-field-testing arena and then allowing it to investigate 2 objects in relation to spatial environmental cues. the mouse is then removed from the arena, and during a delay (inter-trial interval or iti), one of the objects is moved. after the iti, the mouse is reintroduced to the arena and allowed to freely explore. in general, mice prefer novelty, and if they remember the location of the objects from their initial exposure, they will spend more time investigating the moved object. animals with hippocampal lesions have impaired spatial contextual learning and consequently demonstrate no preference for objects in the novel location 14, 15 ."
"on the other hand, the electronic health record of a patient stores all the information digitally recorded from the interactions of the patient with the health system. in the last decades, many efforts have addressed the development of ehr standards and specifications, such as hl7 [cit], openehr [cit], and iso en 13606 [cit] . such standards and specifications are based on the dual model architecture, which distinguishes two modelling levels. on the one hand, the information model provides the generic building blocks to structure the ehr information. on the other hand, clinical models are used to specify clinical recording scenarios by constraining the information model structures. in both openehr and iso en 13606, clinical models are named archetypes and they have been considered a promising way of sharing clinical data in a formal and scalable way [cit] . archetypes are used to specify clinical recording scenarios. an archetype may be used to record clinical data about a laboratory test, a blood pressure measurement, a medication order, etc. they constitute a standardised way of capturing clinical data according to the archetype model [cit] . they are usually defined in the archetype definition language (adl) 4 . ehr data extracts are usually represented as xml documents, whose content should satisfy the constraints specified in the archetype."
"different obda approaches for accessing xml and relational data can be found in the literature. on the xml side, xsparql 10 was proposed as a query language combining xquery and sparql for data transformation between xml and rdf, and xs2owl [cit] creates owl ontologies from xml schemas for allowing querying xml data using sparql queries. on the relational databases side, triplify [cit], d2rq [cit], virtuoso [cit], quest [cit], ultrawrap [cit] and ontop [cit] are likely to be the most popular obda systems nowadays. such systems differ in how they express the mappings, how they translate the queries and in the reasoning capabilities. current obda approaches are limited in their support for reasoning. for example, d2rq does not support reasoning and owl2 ql is the level of reasoning offered by ontop. obda tools are starting to provide support to rule languages such as swrl 11 for enabling users to exploit semantic web rules over data in traditional formats. given that our approach will rely on reasoning for guaranteeing the consistency of the transformation and integration of data, obda is not the best option for our work."
"the mapping rules, including the patterns, permit to transform input data into rdf/owl, which are the formats used by the semantic web community for the development of the web of data and the linked open data cloud. swit permits to achieve five-stars data repositories, because our method permits to include links to external resources in the mapping rules for integration and interoperability purposes."
"in this section we describe the methods included in our approach for the generation of the open biomedical datasets. figure 1 provides a generic description of the method for a single input data resource. our data transformation approach is based on the definition of rules between an input schema and an owl ontology. once defined the mapping rules, the transformation approach may also take into account identity rules defined over the owl ontology. identity rules establish which properties permit identifying an individual of a certain ontology class. thus, these rules permit to merge different individuals of the same class. besides, the transformation method will be able to detect and, therefore, avoid the creation of logically inconsistent content by checking the consistency of the owl ontology. this is done because the whole process is supported by owl ontologies and, therefore, automated reasoning techniques can be applied. in general, the approach can be applied to any input data model providing entities, attributes and relations. in this work, we will use xml and relational databases as input data models. a practical requirement for our approach is that the input schema and the ontology should have some domain content in common. in addition to this, the output data instances shown in fig. 1 can be expressed in rdf or owl."
"an important prerequisite to data collection is defining \"active investigation\", which is when a mouse engages an object with its nose pointed at the object no more than 2 cm away. a mouse moving over the top of the object or looking past the object does not qualify as active investigation. furthermore, since the olt and nort depend on a mouse remembering either the spatial location or actual features of the object, there must be sufficient study of these during the training trials. thus, the researcher must define a minimum investigation time and exclude any subjects that do not meet that baseline level of investigation, traditionally set at 20 seconds 21 ."
"the d ky requires evaluating the lyapunov exponents of a chaotic oscillator, for which several methods has been published [cit] . in this work we perform numerical simulations by applying ode45 to generate chaotic time series that are used to evaluate both le + and d ky by applying wolf's method [cit] . this process is performed within the optimization loops of the de and pso algorithms."
"our approach for the integration of heterogeneous resources is based on the application of the transformation approach described above to the different resources, using the same owl ontology as driver of the process. the construction of the integrated content requires mapping the schemas to the owl ontology. the owl ontology may have a series of ontology transformation patterns associated, which support the integration process. the use of patterns facilitates (1) reusing the transformation rules with different resources, and (2) overcoming the structural heterogeneity of input data schemas. table 2 shows the pattern that defines a protein in the owl ontology used in one of our use cases. this pattern not only avoids the user the need for knowing the structure of the ontology but also can be applied with minor modifications to data resources which store the relation protein-cds-transcript in different ways, or might even not be defined in the input schema. table 3 shows how parametrizing the variable ?cds from the variable ?protein, the pattern can be applied to data resources with a direct protein-transcript relation without cds."
"note: presence of the experimenter in the room for these 30 min will reduce stress on the mice during the task, particularly if the experimenter is male 20 . 5. after acclimation is done and the experimenter is ready to start, begin recording the video. 6. place each mouse facing the walls of one corner of the arena (called the release corner) (figure 2a) . allow the mice to explore the arenas freely for 10 min. 8. stop recording the video. 9. return mice to their clean holding cages for a duration of 20 min. 10. clean all arenas with animal facility recommended cleaning methods, such as wiping with 70% ethanol to minimize olfactory cues before the next use. 11. using double-sided tape, affix 2 different objects near 2 non-release corners such that the objects are counterbalanced in the arena, and 6 x 6 cm 2 from each wall of that corner (figure 2b )."
"we follow a data warehouse-oriented integration method, although our approach has features associated with the integration based on links, because our mapping rules permit to define links to external datasets. this architecture is similar to the one applied in bio2rdf, with the difference that our repositories may contain data from multiple sources. although that could also be possible in the bio2rdf effort, it is focused on transforming single datasets. in fact, we believe that an effort such as bio2rdf could benefit from our approach. currently, one transformation script has to be written to include a new dataset in bio2rdf. swit would reduce the implementation effort for relational or xml sources in the sense that only the definition of the mappings would be needed, since swit would execute the data transformation. besides, swit mappings could be reused for new datasets. using swit would have the cost of making explicit the mappings with an owl ontology, but it would also provide benefits in terms of consistency checking and homogeneity in both the richness of the semantic description and the structure of the data."
"the world wide web consortium has developed a series of semantic web standards for exchanging data (e.g., rdf), for representing their semantics (e.g., owl) and for querying these data (e.g., sparql 5 ). automated reasoners (e.g., hermit [cit], pellet [cit] ) can be used in conjunction with semantic web content to check for the consistency of data or to infer new information. semantic web technologies also offer mechanisms for storing semantic data called triplestores and whose performance for complex queries is continuously improving [cit] . linked open data is a semantic web initiative aiming to materialise the web of data through the publication and sharing of datasets using semantic formats. linked open data datasets meet four requirements [cit] : (1) use uris as names for things; (2) use http uris so that people can look up those names; (3) when someone looks up an uri, provide useful information, using the semantic web standards like rdf and sparql; and (4) include links to other uris, so related things can be discovered."
"the transformation rules define how the content of the input dataset is transformed into a semantic format, and play two major roles: (1) controlling that the information represented according to the input schema is correctly transformed into the semantic format; and (2) preventing redundancy in the set of output dataset. for this purpose, two major types of rules are defined in our approach, namely, mapping rules and identity rules. both are described in the next subsections."
"the objective of this effort is to use semantics to improve compound selection for virtual screening. virtual screening methods use libraries of small molecules to find the most promising structures that could bind with drug targets. one of such libraries is zinc [cit], a free database of commercially-available compounds for virtual screening. zinc data can be downloaded in xml format. in this effort, we created the xml schema and defined the mappings with an ontology developed by our group."
"during the olt, memory for object location is reflected by mice spending on average significantly more than 50% of total investigation time with the moved object ( figure 3a) . if the total investigation times of the individual mice vary greatly, results are better depicted as a discrimination index for the objects (figure 3b) . the significant increase in average discrimination index in figure 3b indicates that the mice spent more time with the object after it had moved. whether measured by increase in percent time or discrimination index, the increase in investigation of the object after it is moved suggests that the mice remember where the object was located during training."
"in rodents, several tests have been developed to study the hippocampus-dependent learning and memory 9, 10, 11 . they can be broadly subdivided into tasks that require a stimulus with emotional valence to elicit a change in behavior, and tasks that draw on the rodent preference to investigate novel stimuli 11 . contextual fear conditioning, for example, pairs an unpleasant stimulus (foot shock) with an environmental context and then later tests memory for the context by measuring fear-induced freezing behavior 9, 11 . the morris water maze and its dry counterpart, the barnes maze, use negative external reinforcement to promote spatial learning . tests that rely on an emotional response to stimuli may not accurately reflect impaired spatial memory if ventral hippocampal emotional regulation functions are also affected."
"the hippocampus is an essential structure for memory formation in humans and rodents 4 . more specifically, the hippocampus plays a critical part in declarative memory involving relational representations, but not procedural memory, which relies on the motor centers of the brain 4 ."
"this discussion is framed around p-values and significance cutoffs because these are the measures and analyses most typically reported for olt and nort data, and therefore are likely to familiar to both experimenters and reviewers. this reliance on p-values has been heavily criticized as statistically invalid 26 . however, though alternative analysis methods exist and are endorsed in some journals 27, none have been broadly adopted by the behavioral and biomedical fields as standard 26 ."
"identity rules define the set of datatype properties and object properties that permit to distinguish each individual in the ontology. these rules are useful to prevent the let ir be the set of datatype properties and object properties of the ontology that univocally defines the identity for the class c. the identity_rule(c, ir) means that all the individuals of c with the same value for the elements in ir are considered the same. we can define an identity rule for the class gene using the datatype property identifier and the object property gene, ro:in_taxon, ncbi:organisms . this identity rule means that two individuals of the class gene (see fig. 2 ), associated with the same individual of the class ncbi:organisms through the object property ro:in_taxon, and with the same value for the datatype property identifier are the same individual."
"the equilibrium points of equations (1)-(3), and their associated jacobians are shown in table 2, where ∂ f 0 ∂x can take the values from evaluating equation (9), so that each chaotic oscillator has three eigenvalues for each equilibrium point. for complex systems the eigenvalues can be calculated by applying cardano's method [cit] . table 2 . jacobian and equilibrium points of the chaotic systems given in equations (1)-(3)."
"on the one hand, biomedical databases contain large volumes of complex, dynamic information about biomedical entities. the information about a concrete biomedical entity, like a protein, is distributed along many different databases, which makes necessary to combine information from different sources to get all the information. these heterogeneous resources do not even share identifiers for the biological entities, although this particular aspect is being addressed by initiatives like identifiers.org [cit] . xml files and relational databases are popular formats used for the representation and sharing of biomedical databases. for instance, orthoxml and seqxml [cit] are two xml formats to standardise the representation of orthology data. relational databases have gained popularity in the last years because they are effective in retrieving data through complex queries. biomedical resources such as the gene ontology [cit] or chebi [cit] provide their data in relational format."
"start recording the video. 13 . place each mouse facing the walls in the release corner. 14. allow mice to investigate the arena and objects freely for 10 min. 15 . stop recording the video. 16 . place mice back in their clean holding cages for a duration of 20 min. 17. clean all arenas and objects with animal facility recommended cleaning methods such as wiping with 70% ethanol to minimize olfactory cues. 18. repeat training trials with 2 new objects affixed in the same locations until all the objects (at least three different objects if conducting both the olt and nort) have been tested with each mouse. 19 . exclude objects that allow mice to sit on top of the object. 20. analyze investigation time of each mouse with each object according to step 4. 21. exclude objects that have a negative or positive intrinsic value. 3. after acclimation is done and the experimenter is ready to start, begin recording the video. 4. place each mouse in the arena (one mouse per arena) facing the walls of the release corner (figure 2a) . 5 . allow the mice to explore the arenas freely for 6 min. 6. stop recording the video. 7. return mice to their clean holding cages during the inter-trial interval (iti). 8. clean all arenas with animal facility recommended cleaning methods such as wiping with 70% ethanol to minimize olfactory cues. 9. repeat steps 3.3-3.9 two more times for a total of 3 habituation sessions for each mouse. 10. return all mice to their home cages. 11. clean all arenas with animal facility recommended cleaning methods such as wiping with 70% ethanol to minimize olfactory cues before use the next day. 12. day 2: training trial, olt, nort note: the nort is an optional test. 1. after 24 h, bring the same group of mice in to the testing room and allow them to acclimate to the testing room for at least 30 min as done before the habituation sessions on the previous day. 2. conduct a training trial using 2 objects placed in the arena (figure 2b )."
"the term biomedical data covers a wide range of types of data used in biomedicine. such data are usually stored and represented in different, heterogeneous formats, which makes their joint exploitation difficult. in this work we are specially interested in the information contained in biomedical databases and in the content of electronic healthcare records because of their importance for biomedical and clinical research."
"the rest of the manuscript is organized as follows: section 2 describes the three chaotic oscillators that are used to maximize d ky . they are a chaotic system with infinite equilibria points [cit], rössler [cit], and lorenz [cit] systems. section 3 details the de and pso algorithms that are used to maximize d ky . section 4 details the maximization of d ky for the three chaotic oscillators and shows statistical results of 10 runs applying de and pso. section 5 shows the chaotic time series with the highest d ky that are used to encrypt a color image. finally, the conclusions are given in section 6."
"object selection is a critical aspect of both the olt and nort 22, 24 . ideal objects are heavy enough not to be easily displaced by a mouse and made of material, like glass or metal, that a mouse cannot damage by chewing or scratching. wooden, foam, or soft plastic objects are not appropriate as they are easily deformed and are difficult to keep odor-free. furthermore, the objects used in the trials should all be relatively similar in size, texture, odor and material. figure 1d gives an example of an appropriate object that could be used. this orange plastic figurine of a chick is filled with sand to give it enough weight and sealed to prevent leakage of cleaning reagents or other odor-causing agents. because of the shape of the top, mice are unable to climb on top of or sit on this object. for the olt or nort, this object is best paired with another object of similar size, weight, material, color and complexity, such as a similar plastic figurine of a rabbit. to ensure that object investigation truly reflects preference for novelty, all objects must be validated for equivalent intrinsic value with a minimum of 8 mice in the same strain, sex, and age of the experimental group as described in section 2.9 of the protocol. additionally, the objects should be randomized in terms of which object is the novel or moved object between mice in the same study to further ensure that inherent characteristics of the objects are not affecting preference. object placement can also greatly affect the success or failure of the olt and nort. objects must be counterbalanced in the arenas and not be too close to the walls. a corner crowded by an object is an attractive place for mice to hide and this will confound measures of investigation."
"the computational complexity of the full method depends on the number of individuals and the mean number of properties and relations per individual. as a consequence, for medium and large datasets, the transformation time may be longer than expected because of the number of instances of axioms to be generated. this might not be a problem in case of stable datasets or batched transformation processes. however, according to our experience with swit datasets, the intended exploitation of the dataset might permit to relax some conditions of the transformation process. in case of not performing integration processes, identity rules are only needed if, for instance, two entities from the input are mapped onto the same ontology class. in case of not requiring automated reasoning on the transformed dataset, the generation of some types of axioms might be omitted, saving time and space. owl:differentfrom axioms are an example of a time and space consuming type of axiom, but they might be skipped in some cases. the lesson learned here is that the optimal configuration of transformation depends on the use case, so the flexibility of the process is basic for getting the desired semantic dataset. all these aspects can be considered adjustable parameters for the execution of the process using tools like swit."
"a sequential forward search-based ga was used to select the list of surrogate genes (see figure 2 ). for this, we first identified co-regulated genes that have a similar direction (but not necessarily magnitude) of response irrespective of chemical treatment. ultimately, the genes were curated to define a subset of genes that reliably represented the full genome. the ga was coupled to three different classification modeling algorithms consisting of support vector machine (svm) [cit], random forest (rf) [cit], and artificial neural network (ann) [cit] . each one of these coupled methods (ga-svm, ga-rf, and ga-ann) was performed separately leading to three sets of selected genes. a combined set of surrogate genes was created by taking those genes that appeared in at least two out of three different coupled methods."
"we next checked if there were patterns between the overlap of surrogate genes identified using different machine learning approaches. the diagram in figure 5 shows the number of common probes between the 3 selected sets. this number was 869 for svm and rf, 776 between for rf and ann, and 865 between the sets of svm and ann. a total of 89 probes were common to all three sets. the probes which were present in more than one set have a higher likelihood of serving as predictors of the remaining genome than the ones which are present in only one set. a total of 2,332 probes [cit] surrogate probes) were present in at least 2 sets and these probes were then used to predict the remaining genome. interestingly, while the three machine learning approaches all produced predictive suites of surrogate genes, a large collection of genes (1,247, or 35%) were only identified by one algorithm. this behavior indicates that there is a degree of degeneracy of information in the transcriptome that htt approaches build upon, i.e., the expression levels of many transcripts are approximately equally predictive."
"one of the pioneering htt efforts was genometry's l1000 platform 1 . the landmark genes used in the l1000 platform were derived using available public human gene expression data to determine genes with the most correlated expression changes across a range of cell types and chemical stressors, primarily from studies with pharmaceutical compounds. this correlation analysis yielded a set of 978 genes that were then used to computationally predict the remainder of the transcriptome (the inferred probes). the l1000 platform has been shown to be highly reproducible, and suitable for computational inference of expression levels of about 81% of non-measured transcript abundance [cit] . results from the l1000 have been used successfully to predict molecular targets based on similarity analysis with responses to other pharmaceutical compounds [cit] . because of the large number of chemicals in commerce or under development for commercial use that have little to no toxicity data, the promise for this type of approach in environmental science is substantial."
the optimal feedback closed-loop control system is composed of m data packets. k represents the number of packets. the system transmission efficiency η can be obtained from the formula (3).
"in order to keep the probability of w l be greater than 0, we design the crowd feedback module in the optimal feedback loop control system. the value feedback vector x cr can be obtained by the formula (4)."
"one of the essential components of the proposed algorithm is the t m o −1 c, which refers to an operation, which will convert the jpeg decoded ldr image to an appropriate hdr version. in principle this operation can be anything in a generic situation, but the efficiency of the proposed algorithm largely depends on the choice of this operation. in particular, one could observe that a t m o is then fed into a prediction component, which will calculate the residual hdr portion of the input image. the prediction can be either in the form of a differential, a ratio, or a more complex mechanism. the residual image can then be compressed either in a lossless fashion using an appropriate entropy coding, if a lossless solution is desired, or transformed, quantized, and then entropy coded, if lossy compression is used. the enhancement layer (el) containing the residual hdr portion of the input image, along with the si is then embedded inside the jpeg compressed file format, using appn marker as indicated earlier."
"the comparison of pathway enrichment using gene expression results from the full genome vs. predicted expression classes in response to 100 µm 2,4-dinitrophenol is shown in figure 3 where the combined set of surrogate genes [cit] was used. prediction was made using a consensus of all three qualitative models (see methods). thirty-eight (38) out of sixty-one (61) significantly enriched pathways were found to be common the pathway similarity index (psi) indicates the similarity between the pathways found using actual and predicted expression classes."
"to further investigate the predictivity of the three sets of genes, all 3 qualitative models (svm, rf and ann) [cit], l1000, and s1500 surrogate probes. a decision on prediction was taken based on consensus of all three models. table 2 summarizes the results of this comparison showing that our set of surrogate genes [cit] provided somewhat higher psi than the other surrogate sets. we interpret this result, i.e., that all three sets provide psi values above 0.7, that there is no single set of surrogate genes that works in all cases and that the selection is likely to be technology and approach dependent. our set of selected surrogate genes were powerful predictors when used within our fitted 3-methods (ann-rf-svm) consensus model and optimized for our matlab code. the code is available on github 5 ."
"in this study, we explored the application of gene expression prediction models to a more diverse chemical space, focusing on two primary goals. first, rather than using a fixed candidate gene set as predictors, we developed a more robust, data driven predictor selection. this process is intended to permit high fidelity gene prediction from any pre-existing gene expression data, regardless of species or platform used to generate the relative gene expression measurements. such a data driven approach would allow for refinement of the predictor selection as new or additional data became available or could be tailored to particular exposure landscapes when existing predictors prove less than optimal. our second goal was to use data-driven predictors set to computationally infer whole genome equivalent transcriptomic expression and then process those expression estimates qualitatively to elucidate conventional ontology enrichment results for inferring toxicogenomic mode of action (moa). the robust data-driven predictor selection, independent of a specific gene expression technology, combined with whole transcriptome expression modeling and qualitative selection of differentially expressed genes for ontology enrichment could prove a valuable opensource approach to htt chemical screening."
"the focus in the work by park and montag 5 was on the scientific images (astronomic, medical, infrared, radar), where 9 tone-mapping algorithms were evaluated. subjective tests with 25 subjects were conducted in a typical controlled environment, where only default parameters were used. paired comparison was used to (i) find which tone-mapping is preferable perceptually, and (ii) to determined which operator is more \"scientifically useful\". results concluded that there were no correlations between perceptual preference and scientific usefulness. one method was selected as performing best for both criteria."
"one popular format is radiance, first proposed by greg ward in ref. 10 . this format encodes floating point hdr pixels represented in 32 bits rgbe format, with red, green, and blue mantissas, plus a common exponent each stored as 8 bits integers. this format was supported by released radiance software, 11 which is a comprehensive set of tools for image manipulation and rendering, implemented under public license, significantly boosting the popularity of the format. this format uses simple run length encoding to losslessly compress an image."
"one main novelty of this approach is evident when considering that the majority of current evaluation work in hdr ignores the context and environmental factors. by varying different environmental parameters, one could see how these factors affect the perceptual quality of the content."
high dynamic range (hdr) imaging is an increasingly popular topic and has been the focus of attention in both scientific and artistic communities for several years. several proprietary image and video compression algorithms have been proposed in the literature for compression of hdr content. among the most popular are those that are backward compatible with existing low dynamic range (ldr) image and video compression standards.
"in this paper, we propose a generic hdr compression algorithm backward compatible with jpeg format. the proposed solution relies on an important observation. by means of rigorous subjective assessments of various tone-mapping algorithms applied to typical hdr images and rendered in various controlled and uncontrolled environments and devices, it is shown that there is no universal tone-mapping algorithm that always stands out when compared to others. the choice of the best algorithm depends on the content, but also on the device used, and other environmental parameters such as back lit lighting, display type and size, environment illumination, etc. these parameters are explicitly taken into account in the proposed solution. illustrative implementations of the proposed solutions using two simple tone-mapping algorithms show that significant compression efficiency can be obtained when compared to state of the art."
"in addition to the validation of our models, we used the pathway enrichment analysis approach and the measure of psi to compare the performance of our selected set of genes with existing sets of surrogate genes developed by other methods. for this comparison, we chose the l1000 landmark genes and another list -the s1500 -that was designed to predict the whole transcriptome expression for toxicogenomic studies at the niehs national toxicology program. the genes in the s1500 list were based on a 5-step series of analyses to derive consensus gene sets that are highly correlated within a group of predictive genes, and which collectively represent known ontology associations of genes. the goal of the s1500 effort was to select gene sets with a high predictive capacity that are closely associated with defined ontology elements. to date, this approach has yielded 5,892 unique affymetrix probes (hg_u133plus2 array based) representing 2,737 human genes that are collectively associated with 674 reactome pathways 4 . all 978 landmark probes from genometry's l1000 platform are present among the 5,892 s1500 probes."
"we have conducted subjective tests as pairwise comparison of different tone-mapping operators. for pairwise comparison of five operators with four test images, 40 pairs in total is necessary."
the cellular/infostation integrated network was considered by the authors of an article [cit] that supports on-demand data service delivery. the novel approaches was proposed in an article [cit] that are based on data envelopment analysis (dea) to further optimize energy consumption in wireless multicast networks.
"compared to all these subjective tests, the goal of our study is not to find the best tone-mapping algorithm but to demonstrate the importance of other factors, such as display size, type of content, and environment on the quality of resulted tone-mapped image, etc. and the findings of these tests can guide the development of more efficient compression algorithms for hdr images and better file formats. although, there are many file formats for hdr images, as with tone-mapping algorithms, none stands out consistently, when compared to others. and there is a lack of widely used standard for both format and compression of hdr images."
seventy-five percent of the total samples in the tg-gates were used for the selection of surrogate genes while the remaining 25% samples were used subsequently to validate the performance of the predictive qualitative models which used the new surrogate genes as predictors. the method of selecting a set of surrogate genes by using tg-gates database involved several steps and machine learning techniques (figure 1 and supplemental figure s1 ).
"unchanged genes across samples for each type of chemical in the heterogeneous and diverse pool of chemicals. the future challenge can be using multiple toxicogenomics data and evaluate prediction performance across databases. any predictive transcriptomics technology that fails to keep pace with changes in respective species transcriptomic information will inevitably lose predictive power simply by ignoring emerging data that could be used for improving predictor selection and model training. for this reason, we plan to continually update our training sets to maximize the applicability domain of our models with the goal of increasing predictivity across a broader chemical space."
"many different subjective evaluations have been previously performed to compare different tone-mapping operators for hdr images and video. main focus of these studies was either on determining a more superior approach to tone-mapping or establishing an evaluation methodology for subjective evaluation of hdr content. as different evaluations result in different sets of best tone-mapping algorithms, it demonstrates that other factors may also affect perceptual quality of the resulted ldr images. this paper will start by analyzing the impact of contextual and environmental parameters on perception of quality in hdr image and video, such as display type, size, contrast, and brightness characteristics, as well as the type of content, in different surrounding lighting conditions. we have designed a comprehensive methodology for subjective evaluation of quality and conducted supporting set of subjective tests to build a model of the perception of quality of hdr content by human subjects in various contexts and environments. the test-bed and infrastructure used in the paper consist of displays with different sizes and characteristics, such as mobile phones, tablets, and large monitors. environmental conditions and contextual information include the amount of environmental lighting, the way subjects view the images, and the backlit light of displays, as well as their size and contrast."
"only qualitative models (up, down, and unchanged) were used throughout the selection of surrogate genes and the evaluation of their performances. the performance of each the three sets of surrogate genes was evaluated by training a model on 30% of the samples used in ga and validated on 25% of the holdout samples (validation set). svm, rf, and ann methods were used to validate the performance of surrogate genes found from ga-svm, ga-rf, and ga-ann respectively."
"gene expression microarrays and next-generation sequence technologies have been used to study functional changes from exposure to pharmacological, industrial, and agricultural compounds. however, a number of practical challenges have impeded the broader use of toxicogenomics for assessing hazards to human health, including the generally low throughput and expense of traditional microarray approaches. more limited, predictive transcriptomics gene sets should provide a viable alternative in leveraging the wealth of public gene expression data available to produce predictive models of transcriptomic change based on measurement of a small sub-sample of mrna transcripts."
"the experiments with mobile phones and tablets were performed in similar ways, only users were allowed to scroll through the pairs of images by themselves, enabling a more realistic active mode in subjective evaluations in such contexts."
"while htt approaches are promising for evaluating gene expression changes, their utility for assessing response to environmental toxicants has not been fully evaluated. our approach utilized tg-gates' toxicogenomic data of human hepatocytes treated with diverse chemicals to select a set of surrogate genes. a sequential forward search-based ga has been used to develop three different sets of surrogate probes using three different classification approaches. a combined set was created [cit] surrogate set consisting of 2,332 probes) using the probes which are present in at least 2 out of 3 surrogate sets. this combined surrogate set was used to predict expression classes (up-regulated, down-regulated, and unchanged) of the remaining genome using a consensus prediction approach. instead of directly comparing the expression levels, a pathway enrichment approach was used to validate the prediction performance."
"although, the proposed solution can cope with both lossless and lossy compression, in our illustration, we focus only on lossy scenario. the detailed process of hdr image compression in a jpeg backward compatible manner consists of the following steps:"
"our approach removed features with variance lower than the median variance across samples for each gene to reduce computational complexity (see methods, figure s1 ). this step reduced the number of probes from 54,675 to 27,338. the gene expression changes were then binned into three categories (up-regulated, down-regulated and unchanged) using a threshold of −0.1 for down-regulation and 0.1 for up-regulation. these 27,338 probes were further reduced in the next step that involved unsupervised clustering with combination of pca and k-means algorithm. the optimum number of clusters for the k-means clustering was found to be 10,000 using elbow method [cit] . the optimum k found here captures 75% of the total variance. a representative probe was selected (nearest to the center of each cluster) to get the desired reduction from 27,338 to 10,000. after that, the ga based on each of the three classification techniques provided a predetermined number (2,000) of surrogate genes from these 10,000. each of these gene sets contained 2,000 genes that should reliably predict the broader transcriptomic profiles irrespective of chemical treatment. a combined set was created [cit] surrogate set consisting of 2,332 probes) that contained genes which were present in at least 2 out of 3 surrogate sets."
"the test-bed and infrastructure used in the paper consist of displays with different sizes and characteristics, such as mobile phones, tablets, and large monitors. environmental conditions and contextual information include the amount of the backlit light of displays, as well as their size and contrast. in practice, the environmental ambient light can be captured by either frontal camera of the device (in case of mobile phone and tablet) or a camera (in case of monitor)."
"in this diagram, t m o c1 refers to an appropriate tone-mapping operation which converts the input hdr image into a format suitable for jpeg compression and decompression. in particular, the most widely used jpeg compression format relies on a yuv color image representation, with all three components coded with 8 unsigned integer bits, and where u and v components are sub-sampled by a factor of 2 in both horizontal and vertical directions, when compared to y component. the selection of t m o c1 is made by the encoder based on any consideration, and for the rest of this paper we simply assume that it can be any tone-mapping algorithm. in this sense, the proposed approach is designed to cope with any tone-mapping algorithm, as it is also the case with some existing solutions in the state of the art, such as jpeg-hdr. t m o c2 indicates an optional tone-mapping operation which could exist in the encoder, in case the representation of the input hdr image is different from the internal hdr content representation in the codec, or when the application requires a different hdr representation than that of the input image. in particular, the color components and bit depth representations of the input hdr image may be different from that used internally in the codec. this component brings an additional flexibility to the approach proposed and allows to cope with a wide variety of hdr images and situations. jp eg and jp eg context, no further assumption is made and the compression ratio, or any other jpeg compression parameters such as the choice of quality factor, quantization and entropy coding tables, or any pre-and post-processing for the purpose of jpeg compression and decompression are left to the encoder, with the largest degree of flexibility. the bl indicates the baseline portion of the resulting bitstream and consists of a fully compatible jpeg format, readable by any compliant jpeg decoder. as in many extensions of jpeg, the additional bit stream necessary for decoding the hdr image will be included in the baseline jpeg format thanks to an appropriate appn application marker as proposed in jpeg standard."
"to have a better understanding of the subjective tests results, we first computed the number of subjective votes for each compared pair of tone-mapping operators, as presented in fig. 2-5 . the pairs of tone-mapping algorithms are displayed on vertical axis, with \"drago\" denoted as \"dr\", \"mantiuk\" as \"ma\", \"reinhard\" as \"ra\", \"icam\" as \"ic\", and logarithmic as \"lg\". values representing horizontal bars are computed as follows (in the order from left to right): the number of subjects that favored first algorithm of the compared pair, the number of subjects voting that algorithms were the same, and the number of subjects that favored the second algorithm. all these values are divided by the total number of subjects that participated in tests for each device, resulting in probability value. each figure presents results for each image with three subfigures corresponding to three devices (in order of appearance): eizo monitor, ipad tablet, and samsung mobile phone, on which the evaluations were performed."
"gene expression changes have proven to be reasonable predictors of the dose-response for classical apical endpoints in vivo, i.e., the 2-year rodent bioassay [cit] . toxicogenomic responses are also being used successfully to categorize developmental toxicants [cit], and many approaches exist for evaluating similarities and differences in toxicogenomic responses across chemical groups [cit] . different suites of genes that serve as transcriptional biomarkers of genotoxicity have been identified [cit] . toxicology is now moving toward use of higher-throughput in vitro methods as a basis for screening compounds for subsequent testing and these screening analyses for transcriptomic changes are playing an increasingly prominent role in early stages of testing [cit] ."
"the results presented in this paper can be extended in several directions. first and utmost, extension of performance evaluation to include a larger set of typical hdr images and other more sophisticated tone-mapping algorithms. second, exploring other hdr image prediction strategies from jpeg decoded ldr image, as well as alternatives compression of residual images both in lossy and lossless fashions. finally, a rigorous subjective evaluation of both ldr and hdr decoded images obtained and comparisons to state of the art from subjective quality point of view rather than mse or psnr as reported in this paper."
"1. application of t m o c1 (gamma correction or logarithm-based) on the hdr input image and jpeg compression. for logarithm t m o c1, we will also maintain maximal luminance value, which is a constant value."
"the results of the subjective evaluation motivated us to develop an hdr compression algorithm that takes into account the statistical properties of the environment, the content, and used tone-mapping algorithm. we consider jpeg format for backward compatibility as it is the most popular format for images. we propose a generic compression scheme accommodating the above mentioned contextual and environmental parameters into the encoding and decoding design. we then implement a simple version of the compression scheme, which results in a more efficient encoding in terms of size of compressed data, demonstrating the advantage of utilizing the knowledge about the used tone-mapping algorithm."
"mb, bp, bf, sh, and bw generated and processed data for proof of concept. sh, mb, and pm processed data for the actual concept. ma, bw, and rc contributed in study, concept, and design. sh, mb, km, and pm designed methods and algorithm. sh, mb, bw, km, and pm wrote the manuscript. sh, mb, bp, bf, bw, ma, rc, km, and pm reviewed and approved the manuscript."
"for modeling purposes, we used primary human hepatocyte exposure data for 158 compounds in the open tg-gates [cit] database. gene expression for the primary human hepatocyte exposures were run on affymetrix hgu133_plus_2 microarrays, typically at three exposures (low, middle, and high), the actual values of which varied depending on the specific compound. each exposure series used its own vehicle controls. data were also typically sampled at 3-time points for most compounds: 2, 8, and 24 h. gene expression in response to the 158 compounds across concentration and exposure times gave a total of 941 experimental conditions. the full listing of cel files and samples available in open tg-gates is listed in supplementary material (data sheet 2)."
"even though the costs of full genome expression analysis technologies continue to fall, the large number of untested chemicals in commercial inventories have inspired the use of high-throughput transcriptomics (htt) approaches for assessing gene expression changes. these technologies are based on the presence of a high degree of correlation between the expression of related genes across the genome [cit] . leveraging this interdependence, some htt technologies measure the expression of relatively small subsets of \"surrogate\" genes and impute the balance of the genome using computational prediction models. the imputed equivalent to a whole transcriptome assay can then be used to make inferences about chemical targets using a variety of gene association techniques, followed by enrichment analyses to link gene expression profiles to known patterns of either cellular biology or of responses to chemical exposures."
"genes with a very low variance of expression across different cell lines and different experimental conditions contained very limited information. to minimize the computational burden, we removed the genes which had low variance across samples from downstream analysis. the criteria to remove low variance gene expressions depended on the distribution of expression from specific dataset. we then removed any gene with a variance lower than the median variance across samples. the tg-gates gene expression data were then categorized using thresholds of -0.1 for down-regulation and 0.1 for up-regulation. the threshold was selected because it provides a similar proportional distribution of up-regulated, down-regulated and unchanged genes across samples for each type of chemical in the heterogeneous and diverse pool of chemicals."
"the details of calculation of the psi were as follows. let n c be the number of pathways common between actual and predicted pathway enrichment, n be the number of pathways in the actual enrichment. then psi is calculated in equation 1 as follows:"
"as can be noted from these figures, there is a variety of scores across both devices and different images, with variety being more significant across images. it means that content of the images (the luminance range, whether it is a day or night shot, variety of details, etc.) plays more significant role in determining which tone-mapping suits better."
"a combination of principal component analysis (pca) and k-means clustering was used to cluster the relevant (other than the very low variance) features into k small clusters. representative features from each cluster were used to create a set of features that serve as inputs to a greedy algorithm (ga) to select the surrogate genes. the reduced set has k features (see supplemental figure s1 ). the optimum value for k for the k-means clustering was found using the elbow method [cit] . the elbow method computes the distortions using incremental cluster numbers. here, to reduce computational complexity we set the increment as 500."
"as technological developments decrease cost and increase throughput of full-genome transcriptomics, new toxicogenomics platforms are emerging alongside htt. novel sequence-based technologies that move beyond traditional next-generation approaches offer even higher throughput, such as biospyder's tempo-seq technology [cit] . nonetheless, predictive transcriptomic modeling approaches will remain valuable tools as the national toxicology program implements their s1500 + initiative [cit] 6 . furthermore, htt has applications in (1) efforts to align new data using emerging genomic technologies to legacy transcriptomics and (2) for data mining approaches of potentially useful gene signatures or more restricted gene sets for predicting the possibility of responses of human tissues exposed to various compounds."
"where x said emission signal intensity. t denotes the data transmission time. h (t) said linear vector data transmission. function f (t) said linear data transmission control vector. s is the linear transmission data sequence. d (s) denotes the size of the data. y (x) indicates the transmitting power signal. y (t) represents the linear time on the total transmit power. wireless data transmission is subject to various external disturbances, such as obstacles reflection, building diffraction, ground absorption, and other factors. an optimal feedback loop is formed between the sender and the receiver, which is the key to eliminate the interference factors. the formula (2) describes the optimal feedback closed-loop control systems."
the data transmitting end of the experimental platform is a cluster composed of five servers. the receiver runs five linux virtual systems in a parallel operation. between the transmitter and the receiver is a wireless communication network composed of 50 mobile nodes. the mapping relationship between the transmitter and two virtual systems of the receiver is an end to end mapping. the relation is used to establish the optimal crowd feedback closed-loop control system. the experimental structure is shown in fig. 3 . the server hardware and system software configuration is shown in table 1 .
"in an attempt to select an optimal set of surrogate genes, we used a high-throughput toxicogenomics database, open tg-gates, with expression of 54,675 probes in response to chemicals belonging to diverse classes. given our emphasis on htt for human risk assessment, we focused on human primary hepatocyte data in tg-gates to create a set of 2,332 surrogate probes [cit] to predict expression classes of the remaining genome. however, the data-driven predictor selection method presented here can be applied to any gene expression data, irrespective of species or platform used for data generation. this approach allows for refinement of the predictor selection as additional data become available."
"to illustrate the compression scheme presented in fig. 10 and fig. 11, we implemented two variants of a simple codec based on the proposed solution. the following simple tone-mapping algorithms were used in each variant: (i) gamma correction and (ii) logarithm-based operator. the main goal of this implementation is to demonstrate that the knowledge of tone-mapping used to produce the jpeg image from the original hdr can help achieving efficient compression even via simple means."
"the other sets of surrogate genes used for comparison in this study are also likely to have specific strengths as well. a deep analysis of redundancy based on a large transcriptomic database could reveal the degree of overlap in information. such study can be useful to extend and improve the set of surrogate genes. for example, a logical extension of the work presented in this paper would be to compare the power and concordance of chemical response prediction using our approach and the s1500 + predictive gene set as an independent assessment to the l1000 comparison presented here. this comparison could help clarify if there is some optimal predictive gene expression method, or some consistently highly predictive gene sets and ontology pathways that are more predictive of cellular changes associated with toxicity."
"and the suites of genes associated with particular pathways have provided the scientific community with publicly available ontology databases. here we used these publicly available ontologies to explore whether the incorporation of biological pathway information into the gene set analysis would improve prediction of whole genome response from the htt gene subsets. we used a visualization technique we have employed previously [cit],b) to perform traditional hypergeometric over-representation analysis for genes identified by our models as up-or downregulated. reactome is a curated biochemical pathway-based cell biology ontology with descriptions that progress from broad, collective functional categories (e.g., \"metabolism\" or \"cell signaling\") to more defined sub-collections of functionally related cellular pathways, and finally to discrete biochemical cellular process pathways 3 . pathways enriched in a toxicogenomics experiment can be summarized using a directed acyclic graph that captures these relationships between pathways in the ontology. intensity of the color of nodes to indicate relative significance of their enrichment, and node size captures the relative number of elements from the query gene set found among that category's elements. together, the enrichment analysis and subsequent visualization provided both a statistically rigorous and intuitive snapshot of the processes perturbed by the compound."
"in the experiments, we compared the proposed data transmission optimization-optimal crowd feedback (dto-ocd) with the data transmission optimization strategy based on the optimized feedback loop data transmission optimization-feedback loop (dto-fl) with the system efficiency and throughput performance. figures 4, 5, and 6 give the dto-ocd and dto-fl system efficiency with the change of the data packet size when the packet loss rate is 0.5, 1, and 5%, respectively. found that with the increase of the packet loss rate, the system efficiency of the two mechanisms began to significantly reduce. with the increase of the data packet, the system efficiency of the two mechanisms is chattering. however, the system efficiency of dto-fl decreased more obviously. its decline range is the same or two times as much as that of dto-ocd. dto-fl jitter is more efficient than that of dto-ocd. figures 7, 8, and 9 give the system throughput rate results with the change of the data packet size when the wireless network mobile speed is 0.5, 1, and 5 m/s, respectively. with the increase of the speed of node movement, we found that the external disturbance is the key factor to control the quality of data transmission. the system throughput rate of the two mechanisms began to significantly shake. the jitter is especially evident when the moving speed is 5 m/s. in addition, with the increase of the data package, the system throughput declined. however, the system efficiency of dto-fl decreased more obviously. the decline range of dto-fl is 2 times the one of dto-ocd. based on the study of the decline reasons of the data transmission performance in the wireless network environment and the influencing factors of the feedback mechanism, we put forward the optimal crowd control of the wireless network data transmission. the innovation of the scheme has also a crowd feedback mechanism, data encapsulation, mobile management, and sensory feedback. firstly, a closed-loop control system is formed between the transmitter and the receiver, which is affected by the interference of the wireless data transmission. the aim of the system is to eliminate the key factors of these factors. secondly, according to the time domain linear weight, crowd feedback module is designed to the optimal feedback closed-loop control system. finally, on the basis of the above researches, we proposed an adaptive optimization model of mobile data transmission. based on the experimental results, we found that the system efficiency and the system throughput rate of the proposed algorithm are higher and the system throughput is higher and smoother."
"the performance of the combined set of the surrogate genes was evaluated using a consensus of all three methods (svm, rf, and ann). prediction of the validation set was made using the combined surrogate genes with all three qualitative models and decision on final prediction was made on majority agreement of these models. when the three models predicted three different classes, the prediction was marked as \"unchanged.\""
"(gamma correction or logarithm-based) to the decoded jpeg image in order to obtain an approximation of the original hdr image, denoted by hdrj. the decoding process follows a similar but dual path when compared to encoding. to make it simple, during decoding, the residual for y component of the hdr image (represented in ycbcr format) is decoded only in an enhanced resolution, while chroma components are obtained directly from the jpeg version of the image."
"the rest of the paper is organized as follows. section 2 gives the optimal feedback model. section 3 discussed the optimization strategy of mobile data transmission with crowd feedback. the algorithm analysis results have been in section 4. finally, the section 5 concludes this paper."
"here, q ci is the number of query element in ith common pathway; and q oj is the number of query element in jth pathway in the actual enrichment."
"efficient high-throughput transcriptomics (htt) tools promise inexpensive, rapid assessment of possible biological consequences of human and environmental exposures to tens of thousands of chemicals in commerce. htt systems have used relatively small sets of gene expression measurements coupled with mathematical prediction methods to estimate genome-wide gene expression and are often trained and validated using pharmaceutical compounds. it is unclear whether these training sets are suitable for general toxicity testing applications and the more diverse chemical space represented by commercial chemicals and environmental contaminants. in this work, we built predictive computational models that inferred whole genome transcriptional profiles from a smaller sample of surrogate genes. the model was trained and validated using a large scale toxicogenomics database with gene expression data from exposure to heterogeneous chemicals from a wide range of classes (the open tg-gates data base). the method of predictor selection was designed to allow high fidelity gene prediction from any pre-existing gene expression data set, regardless of animal species or data measurement platform. predictive qualitative models were developed with this tg-gates data that contained gene expression data of human primary hepatocytes with over 941 samples covering 158 compounds. a sequential forward search-based greedy algorithm, combining different fitting approaches and machine learning techniques, was used to find an optimal set of surrogate genes that predicted differential expression changes of the remaining genome. we then used pathway enrichment of up-regulated and down-regulated genes to assess the ability of a limited gene set to determine relevant patterns of tissue response. in addition, we compared prediction performance using the surrogate genes found from our greedy algorithm [cit] with the landmark genes provided by existing technologies such as l1000 (genometry) and s1500 (tox21), [cit] . the ability of these predictive algorithms to predict pathway level responses is a positive step toward incorporating mode of action (moa) analysis into the high throughput prioritization and testing of the large number of chemicals in need of safety evaluation."
"the subjective evaluations results reported in the previous section can guide the design of an efficient jpeg backward compatible compression beyond the state of the art. the subjective scores indicate that to be opti-mal, and adaptive to different environments, devices and contents, a mechanism is required for selection of an appropriate tone-mapping operation when converting an hdr image to ldr for display on conventional devices available to consumers. jpeg is selected to represent ldr version of the hdr image, because of the wide-spread use of this format and availability of viewers able to handle images in jpeg. although a few proprietary jpeg backward compatible hdr compression formats have already been proposed in the state of the art, they often do not explicitly take into account the perceptual quality, nor context-and environmental parameters in their design. our subjective evaluations described above indicate that such parameters should be considered when selecting a tone-mapping operation to convert an hdr to an ldr image. fig. 10 describes the generic block diagram of the encoder architecture proposed in this paper for hdr image compression, with the feature of being jpeg backward compatible, while offering a more optimal solution when compared to state of the art. by jpeg backward compatibility, we mean that when the resulting bitstream is fed into a conventional jpeg decoder, the latter can decode it into an image and display the content as an ldr version of the original hdr image."
"the test images (see fig. 1 ) in original radiance format were resized to fit different resolutions of monitors, tablets, and mobile phones. then, the tone-mapping operators were run on each image with default settings to produce ldr versions in jpeg format, which were used in the pairwise comparisons."
"1 http://genometry.com/ for toxicogenomic interpretation, l1000 data has been coupled with a novel chemical association algorithm using the connectivity mapping (cmap) concept [cit] . cmap uses a large database of l1000 generated gene expression profiles derived from thousands of small molecule and genetic reagent exposures to multiple cell lines. novel compounds can in turn be assayed on the l1000 platform and their measured gene expression used to search for non-random associations with expression signatures of tested compounds to infer commonality in function and cellular effects. while the cmap concept was developed independently of the l1000 assay technology, the current public cmap database has been derived from l1000 data due to the high throughput nature of the l1000 screening system 2 . existing full genome prediction models like the l1000 have primarily used pharmaceutical compounds as their test sets. the chemical space of commercial compounds is much larger than that of pharmaceutical compounds. it is not at all clear that any single predictive gene expression model will be equally effective across this broader landscape of chemical structures. this chemical diversity makes it difficult for the inference of specific modes of action for adverse effects. highly adaptable or \"tunable\" modeling algorithms for predictive toxicogenomics that are computationally tractable and both time and cost effective would be more useful than any single, static platform."
"the goal of this section is to analyze the suitability of the most common image and video quality evaluation methods for the subjective evaluation of hdr content and to adapt/extend these methods to take into the account contextual and environmental information. we study the effect of the environmental conditions, display characteristics, and content types on the perceptual quality of hdr images. we have designed a comprehensive methodology for subjective evaluation of quality and conducted supporting set of subjective tests to build a model of the perception of quality of hdr content by human subjects in various contexts and environments."
"the same greg ward also proposed a jpeg backward compression algorithm and file format called jpeg-hdr, 1 which is one of the few lossy hdr compression schemes and is the main jpeg backward compatible scheme widely used. jpeg-hdr compresses an image into a jpeg tone-mapped version of that hdr image (any tone-mapping operator is allowed to be used) and a residual data stored as an extension of jpeg. this approach allows any conventional jpeg decoder to render a tone-mapped version, and for jpeg-hdr aware software to reconstruct the original hdr image. in this paper, we agree with the idea of having jpeg backward compatible hdr compression and format, but propose having a more general view on the compression algorithm that would allow to have both lossless and lossy compression, as well as to achieve a more efficient compression and flexibility, given how the research on compression has advanced since jpeg-hdr was first introduced."
"the random wireless sensor networks were considered, where nodes are distributed randomly and form clusters to transmit the packets to relay clusters using cooperative multi-input-multi-output technique [cit] . a simple and effective method is demonstrated to overcome the frequency splitting for an optimal efficiency [cit] ."
"to verify that the resulting psi values are not obtained by chance, we did a y-scrambling test where we randomly scrambled the samples in testing data to predict the expressions from the model created by non-scrambled training data. the average of these psi values was 0.41 with this y-scramble test which suggest that the result in our analysis was not obtained by chance."
"differences between expression changes of individual genes measured across different transcriptomic platforms can be reconciled by assuming a pathway approach [cit] ). here, we tested whether this concept extends to differences between htt and traditional transcriptomics experiments. table 1 shows the comparison of prediction performance of expression classes in response to 100 µm 2,4-dinitrophenol using surrogate genes found from our algorithm using 3 different versions of ga (ga-svm, ga-rf, and ga-ann). in all the 3 versions, the algorithm was stopped after selecting 2,000 surrogate genes (see figure 2) ."
"we developed a novel set of htt genes based on a broader suite of chemistries than previously investigated. toward this end, we developed a qualitative approach based on classification models predicting three classes of probes: up regulated, down regulated, and unchanged. the context of selecting qualitative model over quantitative has been provided in supplementary material. this approach uses machine learning to select a set of surrogate genes using a publicly available toxicogenomics database containing gene expression changes resulting from exposure to a wide range of heterogeneous chemicals. data resources, such as the tg-gates database have greatly expanded the chemical domain of transcriptomic data. the available tggates data was randomly split into a training set (75%) used to select the surrogate genes and fit the predictive model, and a test set (25%) used for model validation. the predictive performance of the resulting model was assessed based on pathway enrichment analysis comparing how major pathways were enriched using up-regulated and down-regulated genes for both the actual and predicted expression patterns. in a second step, we used the genometry l1000 and another wellestablished affymetrix whole genome toxicogenomics platform to gauge performance characteristics of the pathway enrichment approach."
"according to the different time domain linear weighting w l, the solution is divided into two kinds. according to the formula (2), it can be known that the optimal system efficiency can be obtained when the w l is greater than 0."
"the selection and configuration of wearable wireless sensors and smartphones devices that will securely and robustly capture and process streams of health data [cit] § the design of an architecture that enables health care providers and patients to sign up, start and terminate applications with minimal overhead or cost § the establishment of economic models that ensure a bawsn/ha approach is scalable to millions of patients the provision of high levels of security over patient's private health data §"
"run sa on main-scale with lower temperature the above algorithm has been tested for portfolio selection problem based on spin glass paradigm. however, the state of upper-scale spins represents the better assets but this may not be optimal portfolio; also, it is largely influenced on finding better assets and causes the performance of the optimization algorithm increase."
"a summary of the characteristics of the data sets (# of inputs, classes, samples etc. they contain) is given in table 1 . for comparison purposes with state-of-the-art methods, we, apart from our novel algorithms, apply the following methods:"
"the comparison with the first two variants is in conformity to check the advantage of the novel architecture as outlined in section 2 over direct multi-class classification (as in efc sm) and one-versus-rest approach (as in efc mm), as in all cases we apply flexfis-class as training engine. furthermore, in order to underline the impact of taking into account reliability aspects of the classifiers as described in section 4.2, we include the degree of ignorance into the classifications as defined in (16) and ignore conflicting samples when calculating the classification accuracies, both indicating whether the inclusion of classifier reliability in its prediction may finally increase classification accuracy."
"as shown in fig. (3), first, glass is placed in phase transition condition and renormalized. in this condition, an upper-scale glass along with several smaller glasses is produced. for all produced glass, sa algorithm is applied. the results are glasses that are locally optimal. now, the result of upper-scale glass is applied on main-scale glass, (by multiplying the amount of each spin in upper-scale glass), to demonstrate the glass's ground state."
"the early detection of medical conditions can lead to their prevention, more effective treatment and cost savings. for instance, the early detection of diabetic neuropathy was found to reduce complications and save substantial cost and suffering [cit] . remote monitoring of fluid buildup can lead to early detection of cardiovascular conditions with de-fribulated patients [cit] . sudden cardiac death often does not come 'out of the blue' and patients have symptoms for as long as two hours before cardiac death occurs [cit] . further, changes in heart rate variability have been associated with sepsis in hospital settings [cit] and sensors have been developed to detect biomarkers for prostrate cancer [cit] ."
"the patient were also given access to their data and can visualize them in real-time in the slave instance. in addition, they can collaborate with the doctors using the message board. in this way, the hospital minimize the overhead owing to maintenance of such heavily loaded functionalities of a ha. the slave instance are spawned dynamically based on the request from the hospital via patient by using boto applicaton programming interface (api) -a python interface to cloud services [cit] . please note, the slave instance is spawned one per hospital. if the patient from the same hospital makes another request -he/she will be added in the existing running instance."
"the likelihood of balanced learning problems is higher in case of all-pairs fuzzy classifiers, compared to the one-versus-rest classification scheme based on indicator entries, where k ts models for k classes using the whole data set are trained. this is because when training the kth model for the kth class, all samples not belonging to this class are assigned the value of 0, whereas the samples falling into class k are assigned to 1. this means that for the usual classes (classes with not an extraordinary high number of representatives) much more regression target values are 0 than 1. on the other hand, imbalanced learning problems often cause significant bias towards under-represented classes [cit] (no matter if used in batch or incremental training mode). in"
"security is maintained by having the provider request (by sms) that the patient permit the initiation of a monitoring request by responding to an sms. the health care provider is not required to set up or install any customized software but merely to subscribe to the hma provider. once commenced, the provider clinicians access the data stream through conventional web browsers."
"in this paper, we extended state-of-the-art evolving fuzzy classifiers (efc) with the concept of allpairs classification in case of multi-class classification scenarios. the all-pairs model architecture reduces complexity for model updates compared to the one-versus-rest concept and and causes decision boundaries which are more easily to learn compared to direct multi-class response architecture. furthermore, it allows some additional interpretation of the binary classifier outputs as these are stored in a preference relation matrix showing the preference degrees between all class pairs. the evaluation section shows that all-pairs evolving fuzzy classifiers can significantly out-perform the conventional state-of-the-art efc techniques with respect to classification rates. in fact, it is remarkable that the regression-based variant efc-ap-ts is the topranked method for five out of six high-dimensional, partly noisy real-world classification data sets. furthermore, it is also remarkable that by including the classifiers' reliability concepts in the preference degrees (matrix), the accuracies of the efc-ap classifiers can be significantly increased. this also shows the plausibility of the conflict and ignorance concept models described in section 4.2. future work includes the integration of the degree of non-linearity of the binary fuzzy classifiers into the ignorance levels and enhanced strategies for producing overall classification responses from the preference relation matrix including a more funded analysis from fuzzy relational point of view. table 2 : performance comparison of the various incremental learning methods on different shuffles of data of six multi-class classification applications, the first number denotes the mean accuracies over the different shuffles, the second number the standard deviation over the shuffles; first part includes results on the new methods, second on state-of-the-art methods for evolving/incremental (fuzzy) classifiers"
"there are many location-aware applications, such as object position tracking in smart spaces, and personal navigation, that require systems for indoor localization. the global positioning system (gps) is not available indoors, therefore new localization technologies are required. localization systems designed to work in indoor environments are known as local positioning systems (lpss) [cit] . these systems require the installation of several nodes at fixed positions (called beacon nodes) in the indoor environment. beacon nodes are usually positioned at the ceiling or on walls, and a mobile node is attached to the person or object to locate. in order to locate the mobile node using the trilateration method the position of the beacons must be known in advance. the determination of the beacons position is usually done manually by measuring the distance to the two closest walls of the building using measuring tapes or ultrasonic/laser rangers. this method is cumbersome and error prone, therefore different techniques have been proposed to address the problem of obtaining automatically the position of the beacons, also known as the autocalibration or auto-localization problem."
"in this paper, a new optimization algorithm that is named sams based on spin glass scaling and kadanoff's theory is presented and as a case study, the portfolio selection problem is solved. so first, the portfolio selection problem is mapped into long range spin glass as mentioned in paper [cit], and then the glass is placed in phase transition situation. in this condition, the scale of the glass is changed into a upper scale glass and some smaller main scale glass. for each generated glasses, ground state is found with one of the heuristic methods such as simulated annealing (sa) and finally renormalized again to show the found ground state of the problem."
"there is an inversion of responsibility in which a helper class, known as a delegate, is given the responsibility to execute a task for the delegator. the delegation pattern solves the common design problem for a class with excessive functionalities. if the class is highly loaded with different functionalities the delegation pattern creates a delegate to do some of its functionalities. a simple pseudo code is given in fig. 1 that shows the client program calls the delegator printer class, which in turn calls the realprinter class to print the output. to the client it appears that the printer class is doing the print, but the realprinter class is the one actually doing the print. in this way, the delegator (printer class) delegates the print function to the delegate (realprinter class). the conceptual idea of the delegation pattern is transferred across in designing the hma. the ha in the hma is heavily loaded with various functionalities -based on our previous work [cit] the functionalities such as registration and authentication of patients, remotely monitoring of intrinsic health data from wireless sensors and storage of health data are considered to be generic and also requires high maintenance and resources. therefore, the ha in the hma is designed as shown in the fig. 2, a master module (delegator) capable of registering and authenticating a patient and the slave modules (delegates) capable of remotely monitoring the patient's health data. the master module spawns (or delegates) as many slave modules for as many requests from the client (or health care providers). the spawned modules are terminated once the functionalities of that module are complete. in this way, the operational load of the master can be minimised owing to the delegation of functionalities to the slave modules. in the following section, we describe the architecture design of the proposed assistive patient monitoring cloud platform for active healthcare applications (appa)."
"to comprehend the above claims in a pragmatic way, one of the objectives in our research was to build an ha to realize the complete end-to-end hma. and also, in building an ha, it is imperative for our work to have a real-time test-bed to determine how the data generated and gathered from different bawsns is used at the other end of the hma in a real-time environment. hence, the appa is built to maintain the electronic medical record and simultaneously monitor two types of patients for medical conditions -those who have undergone knee procedures and recovering at home and elderly residents at home at risk of falling. these two types of patients were selected because:"
in our tests we found that the condition number of the linearized equations can be reduced by subtracting all equations by the last one. this procedure is based on the centering method which is used to improve the condition number of a given matrix. in this method the matrix a is reduced by dropping the column of ones and subtracting the mean (or other typical value) from the remaining columns [cit] . we can achieve this by locating the last virtual node near the center of the mobile node path in order to subtract the mean of the columns. the final auto-localization eqs. (11)-(13) are expressed as:
"ultimately, we can expect the emergence of a multitude of condition specific applications, each using different subsets of each patient's health data commissioned by diverse healthcare practices. for instance, a rehabilitation clinic may be interested in tracking a patient's gait, while a counselling service may be interested in tracking heart rate variability to detect suicidal depression and a hospital may be interested in detecting sepsis. in addition, the ha can be expected to send alert messages to the patient as well as to the doctors in case of emergency."
"compared to the direct multi-class classification structure, the training time for all-pairs classifier structures can be expected higher, as in the multiclass case only one single model is trained. although"
"with t denoting a t-norm in general. assuming that the preference degrees are reciprocal and the ignorance degree is the same for classifier on k over l as for classifier on l over k, we obtain: (16) i.e. the confidence of the preference of l over k is decreased by the same extent as the confidence of the preference of k over l."
"a generic electronic medical record (gemr) is devised capable of storing necessary health data for patients. using the gemr a master instance is built. the master module can be used as a framework to monitor a variety of medical conditions of the patient. however, in this work two slave instances are spawned to monitor the knee surgery recovery and elderly fall conditions, as shown in figure 3, in the cloud. any ha would require health intrinsic data only through bawsn. the bawsn uses smartphones to transfer the health data from the body sensors to the slave instance. therefore, an intelligent mobile application was developed to authenticate the patient as well as to collect the sensors data specifically for those medical conditions."
"(2)-(3) are two cost functions that should be solved with constraints (4) and (5) . i µ is the mean return of asset i in n intervals of time, i.e."
"the choice of the model architecture for each binary classifier depends strongly on the concept to be learned. we are dealing with fuzzy classifiers, as they can resolve any degree of non-linearity in the decision boundary between classes and also provide a natural and meaningful way how to represent conflict and ignorance in the preference relation (this will be handled in more detail in section 4). in this paper, we are concentrating on two fuzzy classification architectures, singleton class labels and regression-based classifiers based on takagi-sugeno type models (note the former is used as direct multiclass classification architectures in flexfis-class sm and eclassa approach [cit], the later in the one-versus-rest classification scheme as exploited by multi-model evolving fuzzy classifiers flexfisclass mm and eclassm [cit], allowing a direct comparison of novel all-pairs classifier structure in efc with former used structures possible -see section 5)."
"i.e. the score for class k is simply the sum of the single confidences for preferring class k over all the other classes, and outputting the class with highest score as final decision:"
"it should be noted that in the above equation, the costs c m and c s are variable and depend on the amount of load the system receives in a given duration of time. moreover, the hospital h has to incur the capital cost c c again after certain duration owing to wear and tear of the hardware. therefore, from equations (3) and (4), it is evident that for in-house model, the h has to incur overheads in running the monitoring system that has to be investigated further before deployment and also, the in-house model may not be scalable easily."
"in the present paper we propose a new closed-form solution for the position estimation of the beacons that neither requires a trial and error approximation (i.e., randomly generated positions) nor any external positioning information (such as dead-reckoning data). section 2 describes the auto-localization problem when no position information of any node is known. in section 3 a new method to locate three beacon nodes without initial estimations is presented. in section 4 the method is expanded for any number of beacons. the proposed solution is then evaluated by simulation in section 5, and also experimentally on an ultrasonic 3d lps system in section 6. the objective of auto-localization algorithms is to obtain the position of all nodes, beacons and mobile, using only a group of distance measurements between the beacons and the mobile node. the distance measurements are obtained at different unknown positions of the mobile node. in most lps inter-beacon distances are not available since they are only designed to measure their distance to the mobile node. besides, no external localization system is used, so no positioning or odometric information is available in any node."
"by default the shipped shimmer comes with 3-axis low noise accelerometer and wide range accelerometer array [cit] . in addition, the capabilities such as 3-axis gyroscopes (angular rate sensors) and magnetic sensor can be enable as well. however, the shimmer are capable of sending only parameter at any given time. in other words, the set in the sensor hardware is used to send either accelerometer, magnetometer or gyroscope sensor data. by default, the application will sample the 3-axis accelerometer at 51.2 hz and send the data over a bluetooth connection."
"the accelerometer gives the current position of the subject as x, y and z coordinates. the sensed data from the monitoring application is sent directly to the slave instance spawned by the master instance. below sub-section details the cloud instances in appa."
the global coordinate system is defined so that the y coordinate of the third beacon is always positive. with the proposed equations an estimated position of the initial subset of beacon nodes can be obtained based only on range measurements. before expanding the problem to more beacons we will evaluate the error propagated from the distance measurements in the linearized equations.
"1. knee surgery recovery patients typically have rehabilitation plans that involve sequences of exercises that should be stepped up as the knee recovers mobility. the challenge for most patients is to know when their knee has recovered sufficient mobility to step up to the next rehabilitation level. this is currently performed on advice from orthopaedist and/or physiotherapist. however, gaining timely access to specialist is difficult and so rehabilitation is delayed, leading to problems being detected too late to correct."
"in order to evaluate the performance of the proposed method, an lps was simulated using matlab. the simulation is also used to choose two components of the algorithm not fully defined in section 4: the mobile node path and the 2d localization algorithm used to locate the beacons once the inter-beacon distances are obtained. in the following simulations, unless stated different, the ranging data was generated with an additive gaussian noise with zero mean and a standard deviation of 1 cm. this standard deviation was chosen based on the precision typically obtained with ultrasonic lpss [cit] . all simulations were performed 100 times."
a high level requirement for a hma includes: § the facility for diverse ha's managed by different providers to execute on different subsets of the data § the ability for each ha to start and stop with very limited overhead resources or cost § the capacity to ensure that each ha accesses only the data it is authorized to access § the capacity for any health care provider to spawn a new ha with limited overhead or cost § the capacity to define or ramp up to ensure a high level of security § technologies that support business models include 'pay as you go' and other models that ensure sustainability
continuous monitoring of health conditions with the use of wearable sensors that stream data via wireless networks to repositories that are accessible by health care providers is emerging as a technology that promises to lead to new ways to realize early detection of conditions [cit] . the approach combines a body area wireless sensor network (bawsn) [cit] with healthcare applications (ha) that are customized to monitor for general health or specific diseases into a healthcare medical application (hma).
"2. observing the physical condition of elderly people or patients in personal environments such as home, office, and restroom has special significance because they might be unassisted in these locations. the elderly have limited physical abilities and are more vulnerable to serious physical damages even with small accidents, e.g. fall. the falls are unpredictable and unavoidable. in case of a fall, early detection and prompt notification to emergency services is essential for quick recovery [cit] . the two medical conditions of the patient provides a real-time test-bed to explore the concurrent continuous monitoring using appa as shown in the below figure 3 . also, both the mentioned medical conditions have efficient algorithm defined in the literature [cit] for early detection and prompt notification with off-the-self wearable sensors."
"there are several localization methods based on range measurements that can be employed to determine the beacons' position, such as the iterative trilateration. this method requires a group of anchor nodes with known"
"healthcare monitoring application is made up of emerging technologies for various heterogeneous components, therefore, the hardware and software used to build the hma is also very much heterogeneous. in this section we present in-depth details of the hardware, software and communication infrastructure used in each of the components used in hma. one of the major components of the hma is bawsn, which consists of wearable sensors to monitor the sensor data and the smartphone that acts as a gateway to transfer sensor data to the internet (or to ha). in this work, we used shimmer sensors [cit] and samsung tab [cit] as wearable sensors and the gateway respectively."
"the capital cost for the hardware components c c, maintenance cost for the hardware components c m, which includes any ongoing upgrades and minimal running cost for the hardware and support c s . please note the hospital h should have 24 x 7 services to replicate the master instance in appa in order to service the requests from the patient. therefore, the h should have proper infrastructure to host the server whose cost is represented by c i . based on the costs involved in running an in-house model, the total cost is given by the following equation:"
"therefore, linear parameters need to be updated in a regression setting. this is achieved by using local learning instead of global one (due to several advantages with respect to numerical stability, computation time and flexibility when joining new rules on demand, see [cit] ) and exploiting the fuzzily weighted recursive least squares estimator [cit] for each rule separately (here for the ith updating from time instance n to n + 1):"
"first the mobile node is moved on a plane trying to obtain at least m measurement distances shared by the beacon nodes where, based in (13) and (11), m is at least six, although adding redundant measurements improves the estimation. the path chosen in this paper is a circular route plus a measurement point at its center. the circular route was chosen to avoid aligned points and the point at its center to improve the condition number when applying the centering method of eqs. (11)- (13) . on large areas, or if the nodes' range is limited, multiple paths can be required. the goal is to obtain all possible inter-beacon distances applying the linearized equations on subsets of three beacons. every subset is tested to confirm it shares distance measurements with m virtual nodes or more. if so, the respective inter-beacon distances are calculated. to validate the obtained distances, the condition number of the matrix a is verified to be less than our defined threshold of 10 4 ."
"in the second stage, the information obtained from the preference relation matrix as defined in (2) is exploited in a meaningful way in order to achieve high classification performance. an obvious way for doing so is to apply a weighted voting procedure [cit], where the score for each class k is given by:"
"the architecture design depicts a real-time scenario that might happen when a patient requires continuous monitoring. as shown in figure 3, the doctors/clinicians can request the patient to start the monitoring process. the patient with the wearable sensors attached to their body uses a smartphone application to start the monitor after authenticating their credentials with the master instance. in this way, the hospital ict can eliminate the onus of authenticating the patient for monitoring. the master instance once authenticates the patient, spawns the slave instance with the required sensor data to be monitored based on the patient conditions and their requirements. the slave instance will be spawned if and only if there is no instance running for a particular hospital for that particular medical condition -this can be identified by using the hospital identification, for example h2 for hospital2, as shown in the figure 3 . please note the hospital identification will be entered by the patient along with their credentials during the authentication process. this authentication can be scaled up to large numbers of users."
"the shimmer ecg records the pathway of electrical impulses through the heart muscle, and can be recorded on resting and ambulatory subjects or during exercise to provide information on the heart's response to physical exertion [cit] . in case of elderly fall detection ecg may give some indication to confirm atrial fibrillation and conduction defects where there is a prolonged pr interval, inferior ischaemia or bundle branch block. the three lead, two channels shimmer ecg connects to the internal connector pin on the shimmer main board, and is enclosed within the shimmer unit, with the application to the skin via four conventional disposable electrodes as show in the fig.4 . in the below sub-section we detailed how the sensors data is captured using the smartphone monitoring application."
"the master instance have authentication only for the administrator and have the capacity to view, delete, stop and to take snapshot of the running slave instance. the master instance shown in the fig.11 shows the two slave instances running for the hospital capable of monitoring two conditions simultaneously. currently, the system has been implemented and running in real-time for planned field trials."
"the decision variable i x represents the proportion of capital to be invested in asset i; and in spin glass, we can define i x to be the state of spin i. so the problem of portfolio selection can be solved by minimizing the mapped function as in eq. (6) ."
"where d,j is the true distance between node i and j, and e is the measurement error modeled as a zero mean gaussian distribution with variance a 2 . since matrices a and b are composed by the squared measured distances, the resultant error distribution is not gaussian, though it can be modelled as such using a first order taylor series expansion [cit] :"
"where, c h is expressed in dollars per hour ($/h) as we assume t a is expressed in number of hours and cr is the initial registration cost in master instance for h. in in-house model, the h has to incur the following cost for running the same monitoring services."
"the appa approach addresses three of the challenges identified above: the selection of sensors, the design of an architecture that enables health care providers to sign up, start and terminate applications with minimal overhead, the alignment of technologies to viable economic models, and the provision of security."
"one of the main aims of the appa is to reduce the burden for the hospitals in deploying and maintaining the ha with their major functionalities such as patient authentication, continuous health data monitor, health data storage and real-time alert messages. the proposed appa architecture achieves this by delegating the functionalities among the master and slave instances. the hospital responsibility is to send the request for the patient to start the monitoring and to have a required sla with the cloud service provider. in the following section we detail the implementation of appa."
"the appa envisages that the data in the master instance is not intended to be part of the patient's medical record per se. the data monitored by a ha is a subset drawn from the master instance by a health care provider (or patient) so once downloaded, is ideally required to be integrated with other digital data related to the patient. data integration has not been attempted in the current prototype but the design places responsibility for the integration in the realm of each ha. this is also intended to facilitate scalability in alignment with a viable economic model."
"to evaluate the localization methods aforementioned in section 4.2 we simulate a lps composed by 49 beacons deployed on an area of 9 m x 9 m as shown in fig. 6 . we use a circular path for each group of nine beacons to obtain the inter-beacon distances between nearby beacons. the path is composed of 12 virtual nodes and has a radius of \\ m in order to encircle the beacons in each group. we limit the measurement range of the mobile node (4, 5 and 9 m) to obtain a different number of inter-beacon distances. our goal is to verify which method performs better regardless of the number of inter-beacon distances available."
"the shimmer emg measures and records the electrical activity associated with skeletal muscle contractions and can be used to analyze and measure the biomechanics of human movement. the shimmer emg is non-invasive (surface emg) and therefore the activity it measure is a representation of the activity of the whole muscle or group of muscles who electrical activity is detectable at the electrode site. the shimmer ecg offers a wireless solution to a host of muscle, gait and posture disturbances in an easy to integrate and ergonomically valuable arrangement [cit] . each emg board connects to three electrodes, namely, positive, negative and neutral. as shown in fig. 4, the emg is attached in the forearm of the patient."
"the basic conditions for the mobile node path were presented in section 3: at least six noncollinear measurement points on a plane. to avoid aligned points, a circular path is used plus a measurement point at its center. the best parameters of such path: path radius r, the number of virtual nodes n, and the height h from the nodes plane to the beacons plane are evaluated using the node configuration shown in fig. 4 in fig. 5a we evaluate the inter-beacon distance mean error with different path radii. since the error on the matrices of the linearized equations increase with the range distance, it was expected that the best radius value will be the one that minimized the distances from each virtual node to all beacons. however, the simulations show that the best path is the one that encloses the beacon nodes even with a radius much bigger than the inter-beacon distances. we observe that for very small radius the mobile node positions are too close and not much information is obtained with the measurements (as if we were measuring from only one mobile position). as the radius increases more information is obtained (the farther the points the more information we obtain). however, (9) shows that as the distance between beacons and the mobile increases the precision of the solution diminishes. so a mobile trajectory with big radius that does not place the mobile too far from the beacons is ideal. as observed, for this particular configuration, increasing the radius over 3 m also increases slowly the distance estimation error."
"simple estimates illustrate that the provision and maintenance of a hma and bawsn facility by a single health care provider is unrealistic. for instance, the royal melbourne hospital in victoria australia discharges around 60,000 patients every quarter [cit] . in a context of widespread use of remote patient monitoring, we may assume that 10% of patients discharged are provided with wearable sensors for an average duration of 30 days. if the sole data collected is heart rate variability with an emotion device [cit] that typically generates a 7mb text file per hour of continuous monitoring and we assume monitoring for 20 hours per day, then we can expect 280,000 mb of data to be generated per quarter for the hospital from patients wearing this one sensor. if a single health application is running for each patient as it would be for example with remote monitoring of heart rate variability, then over 60 hma applications will be running in parallel processing the data as it streams in, for this one hospital alone. this requires considerable computational resources and built in redundancy that would severely challenge in-house it services. in addition, other health care providers are likely to emerge with a desire to provide ancillary services such as emergency fall alert services with the same data streams."
"in this article, a new optimization algorithm (sams) for solving problems based on spin glasses is presented. this algorithm, by using multi-scaling property of material, tries to break problem into smaller problems. since scaling can not be done at any condition, kadanoff's theory has been used to show the best conditions of scaling. based on kadanoff's theory, the best condition of scaling is in phase transition condition at critical temperature of material. so, this algorithm uses these two properties and solves the portfolio selection problem as case study. the experiments show, this algorithm, in addition to confirm kadanoff's theory in application, has more convergence speed than traditional methods such as sa and also, provides possibility of using parallel processing in optimization problems."
"in this paper a new solution for the auto -localization problem of 3d lpss was presented based on the distance measurements between co-planar beacons nodes and a mobile one. both, beacons and mobile locations, were unknown. while other techniques require an external positioning system (e.g., a second lps or an odometric sensor) to estimate the positions of the mobile node, the proposed method uses only the measurements available on the lps being located. the method is based on the linearization of the trilateration equations for three beacons by grouping nonlinear terms in additional variables. since the linearized equations provide a closed-form solution, the method does not present local minima errors. by moving the mobile node on different paths the distance between more than three beacons can be obtained."
"the classifier structure of all-pairs learning in multiclass classification scenarios is based on a decomposition of the whole problem into several binary sub-problems. formally, this can be expressed by a classifier c k,l which is induced by a training procedure t i,j when using (only) the class samples falling into classes k and l:"
"in fig. (1), a summery of sams algorithm is shown. this algorithm is a combination of sa and multi-scale techniques regards to the renormalization process and used for solving optimization problems. sa is used for finding optimal state of each glass and scaling is used for creating glasses with different scale."
"this means that the ts model type represents a mixture of completely linguistic antecedent part and functional consequent parts (in form of hyperplanes), achieving a tradeoff between accuracy and linguistic interpretability. the rules are combined by a weighted inference scheme to produce a regression output value [cit], upon which a binary classification statement can be obtained, see section 4."
"experiments show, the optimal glass's state, in regards to sams algorithm, is closer to ground state; but it is not ground state yet. the reason is clear: the renormalization formulas are approximate and accuracy is reduced from main-scale to upper-scale. even, these conditions are worse for more scaling levels that cause glass can not specify the ground state clearly. 0 set the glass in phase transition position 1"
"evolving classifiers are serving as powerful datadriven design tool in today's real-world decision support and classification systems in order to cope with on-the-fly modeling scenarios in on-line environments and building models from huge data bases which cannot be loaded at once into the memory. single-pass incremental learning capability, where models are built up in a step-wise fashion by using single data samples or blocks of data [cit], plays an important role in evolving classifiers in order to keep the update time and the virtual memory usage at a low level [cit] . evolving classifiers are not only able to adapt their parameters, but also to extend their structures and expand their memory on-the-fly in order to account for varying, expanding system states/behaviors. a specific type of evolving classifiers are evolving fuzzy classifiers [cit], which exploit the concepts of fuzzy sets in order to account for a possibilistic evolving modeling approach for any uncertainty in the (classification) data. furthermore, as using a non-linear fuzzy classifier structure in rule-based form, they are able to represent classifiers with high accuracy [cit], to express reliability in a natural way [cit] and to allow some sort of interpretability [cit] ."
"the master/slave soa approach enables each health care provider to spawn a slave instance configured to perform only the action relevant to that provider. the fall detection instance takes ecg, emg and accelerometer data and implements a fall detection algorithm whereas the knee rehabilitation instance solely uses accelerometer data."
let us consider a hospital h that has n number of patients to be monitored in real-time for a particulate disease. the total amount of time t for which the monitoring system was running in order to monitor the n patients is given by the following equation:
"in software engineering, a design pattern represents a solution to a problem or class of problems that one can put to work at once in the programming code. design patterns were not invented based on theories. rather the problem situation occurred first based upon the requirement context, some design solutions were evolved. design patterns often provide design solution for common design problems faced by the application developer. there are many design patterns available related to object oriented programming world, and in time, a particular set of them has become accepted as the standard set [cit] . one of the standard design patterns is the delegation pattern in object-oriented programming where a class, instead of performing one of its stated tasks, delegates that task to an associated helper class."
"table (1), shows the time of convergence speed to finding spin glass ground state for two sa and sams algorithm. as shown, productivity (the ratio of the time of finding ground state that initialize with applying sams algorithm to the time of finding ground state with random initializing) is significant. for example, in hang seng stock market, pro-ductivity is 0.81 percent."
"for finding the minimum of optimization function eq. (6) with regard to constraints (7)-(8), we first randomly place the possible assets into a long range glass (full connection glass). all of the spins in this structure are initialized to random values. therefore, we can use some heuristic algorithm or methods such as sa to select the best assets by finding the ground state or minimum energy of the glass."
"once spawned, the slave instance will be ready to receive data from the patient's smartphone. the functionalities of the slave instance are to receive the monitored data from the patient and to store the sensor data temporarily. the temporarily stored data can be transferred either to the master instance or directly to the hospital based on the service level agreement (sla) between the cloud service provider and the hospitals. in addition, doctors from the hospital can login into the slave instance in order to visualize the patient health data and trigger any alert message based on the gravity of the health data."
"we selected the following sensors accelerometer, electromyogram (emg) and electrocardiogram (ecg) because these variables are used in fall detection algorithms [cit] and accelerometer data is useful for knee rehabilitation monitoring [cit] . the samsung tab 2 was used as a gateway to the australian nectar research cloud (www.nectar.org.au). the following subsections present in-depth details of the hardware, software and communication infrastructure used in each of the components in hma."
"another problem presented with the optimization algorithms, and also with bayesian methods, is that their convergence depend heavily on the initial conditions used. here we propose a new method based in the linearization of the trilateration equations by expanding these equations and grouping all nonlinear terms in additional variables, therefore circumventing the convergence problem."
n n t n t (11) where n represents the number of epochs. the stop condition of algorithm is the iteration of single result in number of defined steps continually with regard to defined precision. for example all experiments' results have been measured by ten same results with a precision of 7 10 − .
"to present our method, we initially map the portfolio selection problem into a spin glass computational model and then find its ground state by looking at the objective function, eq. (6), of the portfolio selection problem and comparing it with the spins energy function eq. (1) of the spin glass model. we obtain the values for the interaction strength as follows [cit] :"
"after the spin glass was placed in phase transition condition, renormalization process is performed. in this case, the glass is divided into several smaller glasses. also according to formulas (2) and (3), a upper-glass scale is produced. now, sa algorithm applies for upper-scale glass and all main-scale glasses. the results show that the time to reach to optimal state reduced effectively. so that, if the starting temperature without using renormalization process is 0 t, then temperature, for the case with using renormalization process is approximately 0 1000 t . this means, the sa algorithm needs lower temperature for initializing and is caused increasing convergence speed of the algorithm."
"in fig. 11 is close to the one obtained with the inverse positioning though there is a small offset between the estimated positions. this solution could be used as a first estimation of the beacons' position that later can be improved if necessary. here, with an offset around 1.5 cm, any optimization algorithm will easily find a more accurate solution. fig. 12 shows the average and maximum auto-localization difference, on each axis, of the linearized algorithm compared with the inverse positioning method. since beacon one is used as origin no errors are present. on the other beacons the higher differences are seen on axis z. although the cell structure of the 3dlocus is positioned parallel to the floor, it seems that there is a small inclination that causes the altitude variation on the beacons. since on the linearized method it is assumed that the beacons are on a plane parallel to the mobile path, any difference of altitude is reflected on z axis. on the other axes the average difference is below one centimeter, and the maximum obtained on all tests is below 1.5 cm. the results indicate that the linearized method has a good accuracy in the autolocalization of beacon nodes. fig. 9 . 3dlocus acoustic localization system."
"it is apparent that the time duration for monitoring n patients will have overlaps because several patients will be monitored at the same time for h. therefore, the actual time t a for which the monitoring system would be running is given by:"
"a phase transition typically refers to a transformation of a thermodynamic system from one state to another. this transformation is typically marked by a sudden change in some physical property that is the result of some small change in what is called an order parameter (e.g., temperature) [cit] . phase transition temperature in spin glasses is a sudden change that occurs in a state of glass. in other words, at this temperature, the probability of finding the ground state of the glass decreases significantly and glasses' states are unlikely to be a ground state. fig. (8) illustrates phase transition temperature of spin glasses applied to the portfolio selection problems. as shown in this figure, the probability of reaching ground state ( min e e ) is equal to 1 in low temperatures, where e is the amount of glass energy and min e is the actual minimum of the energy function. as temperature rises, glass energy also changes and the probability of reaching ground state is expected to gradually decrease. however, this drop is delayed until phase transition, where a sudden drop occurs. after phase transition temperature (critical temperature), the probability of reaching ground state decreases significantly. as shown in fig. (8), this temperature is approximately 5.8* 10 -5 for all of the five mentioned stocks. at this temperature, system is not in ground state and selecting any state as answer of the problem is unlikely to be a ground state."
"is the total spin of each cluster and a is the total spin in glass in upper-scale. so, the total number of spins of main scale glass is equal to"
"cloud computing allows simple and easy maintenance of information technology (it) infrastructure. in essence, cloud computing can be considered as a set of internet services provided by a third party who owns the it infrastructure and offers its functionality over the internet through various innovative business pricing models. in this proposed model, the healthcare monitoring application leverages the advantages of the cloud service to the extend that minimizes the operational and monetary cost overhead for the hospitals. currently, the system has been implemented and running in real-time for planned field trials."
"so first, in section 2, concepts of renormalization and multi-scaling are described, in section 3, the proposed sams algorithm based on the kadanoff's theory and multiscaling is explained. in section 4, the portfolio selection problem and markowitz model are defined and in section 5, the portfolio selection solution by using long range spin glass, which is described in paper [cit], is expressed again. finally, in section 6, experimental results and conclusions are presented."
"with the inter-beacon estimated distances all the lps' beacons can be located using a 2d localization algorithm. different localization methods were evaluated in order to obtain the best solution with lpss deployed on small and large areas. the simulations showed that the lamsm method was best suited for large areas (where the inter-beacon distance estimations were few) and also on small areas (where most of the inter-beacon distances were available). besides, the solutions obtained can be later improved by using them as initial conditions on other auto-localization techniques based on bayesian or optimization methods. finally the method was evaluated on an acoustic lps, the 3dlocus, and compared with the inverse positioning method. the results showed mean positioning differences below 1 cm on axes x and y for each beacon. higher differences are found in axis z due to the assumption that the beacons are on the same plane. the proposed method showed a good estimation of the beacons' position compared with one that requires a costly second positioning system, like the high-precision staübli unimation industrial robotic arm. the obtained results are very promising for a practical auto-calibration method that only requires to move a node around the localization area, without needing an initial estimation of the beacons' position or any external equipment."
"the appa is designed in such a way that it can leverage the cloud's pay-per-usage model. the master instance will spawn the slave instance for the hospital only if there is a request from the patient's from the hospital. if there is a slave instance already running for the hospital -any further request from the patient from the same hospital will be added in the existing instance. in this way, the cloud service provider can charge the hospital only based on the number of hours the instance was running. the formulation below shows the cost estimation for appa and inhouse implementation of healthcare application for hospital h."
the solution of the linearized equations is affected by the errors in the measured distances used in the matrices a and b. the magnitude and type of error depends on the technology used in the lps system. the algebraical manipulation applied to the trilateration equations further increases the complexity of the error propagation analysis. consider the case where the distance measurements are given by
the appa approach is aligned with a scalable economic model because it provides the technology for start-up companies to provide hma services. hma suppliers can readily be imagined to offer a range of payment models including sourcing the full payment from the client on a per use basis or a combination of per-use and subscription plans from each health care provider and patient. the architecture envisages a plethora of hma startups ensuring viable competition and also ensuring single vendors do not dominate the market place.
where d¡¡ k is defined in (3) . the solution of the inter-beacon distances d 12 d\3 and d 2 3 are the same specified in (6).
"the remaining of this paper is organised as follows: the requirements for a hma are described in the next section. following that, a design pattern, for the implementation is presented in section 3. before describing the implementation results of appa using shimmer sensors and the australian nectar research cloud in section 4. the architecture design of appa is detailed in section 5. the formulation of the cost and the justification are given in section 6 and 7 respectively. the conclusion and future work are described in section 8."
"we finally evaluated the distance (height) between the beacons plane and the mobile path plane. as expected, the closer the virtual nodes get to the beacons the better is the solution. in this case, the distance between beacons and mobile plane will be limited by the emission/reception pattern of the nodes, since it is required that all points are visible to all beacons."
"in this section the auto-localization problem when there are more than three beacon nodes is considered. our approach is very straightforward, we move the mobile node in order to obtain as many inter-beacon distances as possible, using the equations proposed in section 3. since it is assumed that the beacons lie on a plane, their positions can be solved using a 2d localization algorithm. a flowchart of the proposed algorithm is shown on fig. 3."
the master and slave instances are shown in the fig. 11 and fig. 9 respectively. the slave instance not only facilitates the doctors to visualize the condition of the patient in real-time but also allows the doctor to send alert message manually to the patient in case of any abnormalities.
"health care providers including hospitals and health care practices are unlikely to be in a position to maintain the data repositories required to receive, process and store streams of patient data. this represents an additional overhead to their existing ict operations and stretches already thin budgets. in addition to technical challenges including connectivity and security issues inherent in the ha with bawsn approach, the health economics case looms as a large obstacle."
"in hmas, the health data gathered by the bawsn from the patient is delivered to the ha, which can be defined as a sophisticated application assisting the doctors/care staff to monitor the patients' health condition and consult with the patient 'on the fly', regardless of where they are located. although the bawsn achieves the critical function of gathering trustworthy health data from the patient, the ha provides the visualisation of the patients' progress to the doctor and can have many functionalities. examples are maintaining the electronic medical records in the database, alerting the concerned clinicians about the condition of the patients, the ability to provide a common ground for the patients and the care staff to discuss their needs in detail and also in private; it can also have an intelligent algorithm to predict any forthcoming emergency situation. the general functionalities mentioned above are under the perspective of the user of this application. however, from the application developers' perspective, the specific implementation of these functionalities differs considerably depending on the health care requirements. for instance, the design of the electronic medical records differs considerably for patients who are suffering from lymphoma 1 and heart disease, and for those with other functionalities associated with an intelligent algorithm to predict any situation."
"as expected, for small areas all mds based methods perform better than the trilateration. since most inter-beacons distances are available, the mds algorithms have a better distribution of the errors compared with the accumulative effect showed by the iterative trilateration. as the area increases, and more inter-beacon distances must be estimated by hops, the mds-map rapidly decreases its performance. when only one hop is required for the beacons distance estimation, the mds-map (p) performs exactly as the mds-map. with large localization areas, the mds-map (p) and the lamsm performs better than the trilateration and the mds-map, since the inter-beacon distances are never estimated using more than one hop. based on the simulated tests the lamsm is chosen to be used on the proposed auto-localization algorithm."
"once the inter-beacon distances are validated, another three beacon subset is chosen. the process is repeated until every possible subset is evaluated. since in the process a given pair of beacons is chosen more than once, multiple distance estimations for each pair of beacons can be obtained. in these cases the mean of all distance estimations is used. once all groups of three beacons are evaluated, a 2d localization algorithm is used to locate every beacon position."
"another condition evaluated in this section is the number of virtual nodes used in the path. as seen in fig. 5b the estimation of the inter-beacons distances improves with more redundant information obtained by more virtual nodes. however, it seems that the improvement obtained per added node decreases noticeably for more than 15 virtual nodes."
"in fig. 7 the cumulative error of the estimated beacons' position is presented for three different range limited (c) virtual nodes height measurements. in fig. 7a the range limit simulated is 9 m, therefore almost all inter-beacon distances are obtained. it can be seen that the trilateration method performs worst because of its accumulation of error. the performance of lamsm and mds-map (p) is almost the same, obtaining location errors under 4 cm on 90 % of the beacons. mds-map performs slightly worst than the other mds methods obtaining location errors under 5 cm on 90% of the beacons. it seems that the lack of some inter-beacon distances, even a small number, affects the mds estimation. this can be further observed in fig. 7b where the limit range is 5 m. in this case with less inter-beacon distances available, the trilateration algorithm performs better than the mds-map (errors on 90% of the beacons under 12 cm for the trilateration method, 15.5 cm for mds-map). finally, in fig. 7c a range limit of 4 m is simulated. in this case the mds-map (p) still performs better than the mds-map, but not as good as the lamsm method (errors on 90% of the beacons under 17 cm for mds-map (p) and 13 cm for the lamsm). the lamsm method does decrease its performance compared with the previous simulations but it is still better than the other algorithms. on fig. 8 the cumulative error at 90% is shown for different localization areas keeping the distance between beacons constant at 1.5 m. the simulated areas in m 2 are: [cit] and the corresponding number of beacons are: [cit] . the range limit used on this test is 5 m."
"in this article, we outline requirements for a scalable, secure, cloud based approach to the implementation of a hma. the business model is assumed to be a pay-per-use model of the cloud services and repositories that will enable convenient, on-demand network access to a shared pool of configurable computing resources (e.g. networks, servers, storage, applications, and services). these can be rapidly provisioned and released with minimal management effort or service provider interaction. the hospital can make use of this business model to host the has in the cloud and have service level agreement (sla) for the usage. this paper not only emphasis and applies this business model in healthcare application but also proposes an innovative way to leverage the cloud resources based on the healthcare monitoring needs."
"this experiment shows that dividing glasses into two separated glass cause higher convergence speed in finding its ground state but it may not find the ground state of the main glass respectively. however, the upper-scale glass ground state help to provide better initialization condition."
"ga is an adaptive method to solve the search problem. it is based on parallel search of the chromosome group, selecting operations with guessing, switching operations, and mutation operations. so, ga has the following characteristics. ga begins its search from the set of problem solutions, rather than the single solution. this is a great difference between ga and traditional optimization algorithms. the traditional optimization algorithms get local optimal solutions easily because they obtain the optimal solution from a single initial value iteration. ga begins its search from the set of problem solutions. so, it covers a wide area and it is good for global choice."
"in the paper, ga for suppression ici energy of sp-wofdm is proposed. the optimal interference subcarriers for suppression ici that the un-lu causes to the lu can be obtained by a new algorithm. moreover, the parallel ici suppression algorithm is designed to improve the calculation speed of ici suppression based on ga and meet the practical requirement of cr. the new algorithm does not only realize the trade-off between reducing the interference power and maintaining the bandwidth of the unlicensed system, but also consider the calculation speed and practicability of the algorithm."
"to test the performance of the ici suppression algorithm based on ga, we construct spectrum pooling with 32 subcarriers. spectrum pooling is divided into 8 subbands and each subband matches 4 subcarriers. the daubechies-4 (db (4)) is implemented to modulate the subcarriers of spectrum pooling. qpsk is used to realize symbol constellation."
"in addition to facilitate visual motion parameters estimation, the achieved optic flow model in a previous frame helps providing an initial guess to the optic flow estimation algorithm in the next frame by projecting the points in question using (1) and (2) . by doing this both the reliability and the performance of the optic flow estimation is enhanced especially when the magnitude of optic flow becomes large due to large displacement or when the depth becomes small."
"ga need not search spatial knowledge or other auxiliary information. it utilizes the fitness function value to evaluate individuals and carry out genetic operation. the fitness function is not limited to continuous differentiable functions, and its definition domain can be set arbitrarily. this feature expands the application range of ga greatly."
where ( ) is the original data transmission rate of un-lu before the interference is suppressed. bit is the number of bits assigned to each subcarrier. (2) is selected as the fitness function of the un-lu's subcarrier. the fitness function can be described as follows:
"in this case the foe is the only point where the optic flow vectors all coincides; hence it will be the only vanishing point of the optical flow vectors. we can find the image point coordinated x f oe, y f oe of the foe from (3) and (4) is the distance from the eye to the surface generating the visual features on the retina. image angular velocities in both image directions ω x, ω y can be defined by using v x, v y respectively in equation 9. it is clear that in the case of a downward looking camera the image angular velocities are the scaled lateral velocities presented as the first terms in (1) and (2)."
"cognitive radio (cr) technologies based on the dynamic spectrum access concept can utilize temporarily idle spectrum resources [cit] . cr can improve the current tension of spectrum resources and enhance wireless communication performance [cit] . spectrum pooling is a very effective spectrum utilization technology of cr. the licensed spectrum of different businesses is merged into a public spectrum pool by spectrum pooling [cit] . unlicensed users can utilize the idle spectrum which is not occupied by licensed users in spectrum pooling. spectrum pooling based on orthogonal frequency division multiplexing (ofdm) has been widely accepted [cit] . however, the side lobe interference of ofdm signal is the shortcoming of spectrum pooling based on ofdm [cit] . spectrum pooling based on wavelet-based orthogonal frequency division multiplexing (sp-wofdm) can not only obtain the same perfect performance of ofdm but also configure subchannels flexibly and suppress intersymbol interference (isi), pulse interference, and narrow band interference more effectively. and it can realize multirate signal transmission to meet the requirements of different businesses and business quality more conveniently [cit] . moreover, sp-wofdm owns better bandwidth effectiveness without side lobe interference because it does not need guard interval and pilot."
"the parallel ici suppression algorithm can run on a multicore computing platform and it has the advantage of multithreads. so, the parallel ici suppression algorithm can not only improve the calculation speed of the original ici suppression algorithm based on ga, but also meet the practical requirement of cr."
"the fitness evaluation of the selected subcarriers takes up most of the execution time and there is no dependence on each other. so, fitness evaluation computing of originally selected subcarriers can be assigned to different cores in a multicore processor."
"if the camera focal length f is known then the angular velocities of the observer can be deduced as follows: (13) the equations for ttc (t c ), lateral ventral flows (ω x, ω y ), and the foe location (x f oe, y f oe ) can be found as follows: (14) ego-motion model equations (11), (12) have six unknowns thus optic flow estimation at a minimum of three points are required to solve for the model parameters. this assumes that there is no depth variation in the scene, however with large number of optic flow measurements a minimal variation in the depth could be tolerated if the solution is found in a least square sense. the following system of equations could be written for n points: (15) and the values of model parameters a 1 to a 6 can be found using the least squares solution of the over-determined system defined in (15) ."
"the accuracy of the estimation is tested under two different motion profiles. it is clear that the method produces very good estimates in the first motion profile while the quality of the estimated degrades slightly in the second motion profile. this reduced accuracy is due to invalidating the assumption of a uniform depth values for all image points when the camera tilts. however if the camera orientation is not expected to vary significantly the achieved accuracy is suitable for the purpose of robot navigation. side-view of the simulated environment in order to quantitatively measure the estimation accuracy, the root mean square error (rmse) of the five estimated visual motion parameters against their theoretic cal values is calculated and shown in table i for the two motion profiles defined above. the horizontal and vertical directionhx, hy in degrees are included for convenience. in this paper a simple closed form method for estimating visual motion parameters, namely the time-to-contact, focus of expansion, and image angular velocities from a general 6 degrees of freedom camera motion. the proposed method uses sparse optic flow estimates at arbitrary image location allowing exploiting image textures in each frame. all parameters are estimated simultaneously rather than in stages to prevent error accumulation. the method managed to accurately estimate the required parameters in real-time. future work should find ways to address degradation in estimation accuracy due to variation in depth due to multiple planar objects or slant surfaces possibly due to camera orientation without adding the complexity of resolving depth itself."
"in a multipath environment, the orthogonality of wofdm signal is lost so that intersymbol interference (isi) and intercarrier interference (ici) occur [cit] . the performance of sp-wofdm can be destroyed by ici and isi. masking one or more un-lu's subcarriers can mitigate the ici of sp-wofdm. but it can sacrifice bandwidth in the rental system. so, it is important to research the trade-off between reducing the interference power and maintaining the bandwidth of the unlicensed system. the genetic algorithm (ga) is an efficient, practical, and robust optimization technique. its essence is parallel, efficient, and global search methods. it can obtain and accumulate the related knowledge of the search space automatically and control the search process adaptively to get the optimal solution. the ga has characteristics including 2 wireless communications and mobile computing operating on the encoding of the parameter, no deducing and additional information, the uncertainty of optimization rules, self-organization, self-adaption, and self-learning, compared with the traditional optimal algorithms."
"(6) obtaining optimal interference subcarrier sequences. when the fitness of optimal subcarrier reaches the given threshold, the optimal interference subcarrier sequence is obtained. the interference suppression of cr is realized by masking the optimal interference subcarrier sequence."
"the paper is organized as follows. the system model and ici energy of sp-wofdm are provided in section 2. in section 3, ga for ici suppression is proposed. the parallel ici suppression algorithm is designed in section 4. the simulation results are described in section 5. section 6 concludes the paper."
"ga is self-organizing, adaptive, and self-learning. when ga uses the information of the evolution process to organize the search, the individual with large fitness has a higher probability of survival and can obtain a more adaptive genetic structure."
"the cr is a new method to share spectrum resources with more flexibility and efficiency. however, the orthogonality of lu signal and un-lu signal is disrupted because of multipath fading impulse. so, ici between lu and un-lu occurs. ga can obtain and accumulate the related knowledge of search space automatically and control the search process adaptively to get the optimal solution. this paper proposes and discusses the ga used to obtain the optimal interference subcarriers"
"parallel computing is the process of using multiple computing resources to solve computing problems. traditional serial computing is the process of performing single operations one by one on a single central processing unit. but parallel computing can simultaneously perform multiple operations on a set of processor units. parallel computing can quickly solve large and complex computing problems. in the multicore era, better performance can be obtained by designing a parallel computing algorithm based on multithreads on a multicore computing platform. the spectrum pooling ici suppression parallel algorithm based on multithreads is designed by us to improve the calculation speed of spectrum pooling ici suppression algorithm based on ga and make it more practical. the spectrum pooling ici suppression algorithm based on ga can be parallelized as follows:"
"most big data platforms, including hadoop and spark, have been designed without any specific consideration of spatial properties. a few platforms provide spatial functionalities [cit], but they generally lack spatial properties and do not support advanced geospatial operations such as geospatial joins and geostatistical operations, which are imperative for advanced geospatial analytics. spatialhadoop [cit] has been developed to enable spatial operations by extending hadoop layers: language, storage, mapreduce, and operations. although spatialhadoop has been known to overcome the limitations of existing spatial big data platforms and perform better than traditional hadoop, it is still not sufficient in supporting in-depth geospatial analysis."
"there are five target spatial operations in our experiments: obtaining minimum bounding rectangle (mbr) and creating spatial index, range query without index, range query with index, and spatial join with index. mbr has been most commonly used to approximate spatial objects and is one of the fundamental operations for spatial analysis. a query for obtaining mbrs is therefore included in our experiment. spatial index also plays an important role in geospatial domain to speed up retrieving certain objects in a spatial database, making measuring the performance of creating spatial index essential for this evaluation. range query allows one to search for spatial objects located in a specified spatial extent and is therefore a fundamental type of query in spatial databases. to see the impact of using spatial index on range query performance, we investigated two cases-with and without an index. lastly, spatial join is one of the most important operations for combining spatial objects and serves as building blocks for processing complex spatial analysis. even with the support of spatial index, spatial join is particularly complex and time-intensive. thus, efficient processing of spatial join is crucial to increase query performance."
"the term data model refers to an abstract model that arranges the structure of data and regulates their association with each other. figure 3 shows the data model used in marmot that corresponds mostly with the model in a standard relational database management system (rdbms). marmot supports seamless interactions between spatial and nonspatial operations within a solid framework, as shown in figure 2b . this is possible because marmot is designed and developed by considering the data model and operations for processing spatial information from the initial phase of the development process. most nosql systems, however, are designed with no consideration of spatial functionalities. this leads them to adopt plug-in based architecture when they want to embed spatial functionalities later on. because both spatial and nonspatial operations are managed by one system, it is feasible to improve workflow. in addition, since temporary files do not need to be generated, the input/output overhead also decreases considerably regardless of file size or the number of operations for analysis."
"where sh and m are execution times required by spatialhadoop and marmot, respectively. table 4 shows the execution time of each test case and the pir. in designing the experiment, we included only fundamental spatial operations due to the infrastructural limits of spatialhadoop. although spatialhadoop has built-in spatial analytic functions, it supports few spatial operations and lacks many useful functions such as coordinate conversation, exporting spatial data, and raster processing functions. in addition, spatialhadoop was not designed to read shapefiles directly, which is a very popular geospatial vector data format used in spatial domain. instead, spatialhadoop reads spatial data represented in well-known text (wkt) or well-known binary (wkb). because of this, in this paper we focused on measuring only the performance of fundamental spatial operations."
"(3) literature on the structural layers of twitter [cit], including (a) that adding a hashtag sends the tweet to the macro larger community of twitter, (b) that not including a hashtag and not starting with @user sends the tweet the meso layer -limited to all followers so audience relies on follower numbers, and (c) that starting a tweet with an @user sends the tweet to the micro layer, with a very small audience; and (4) a content classification of tweets [cit] demonstrating how to compose tweets strategically, including for: (a) conversational tweets (mentioning another user, for conversation), (b) news (for announcements and journalism), (c) pass-along tweets (for sharing links to other internet content), (d) social presence tweets (for connecting personally with a twitter audience) and (e) status broadcast tweets, reflecting twitter's use as a soapbox where users communicate their thoughts, feelings, experiences, and ''diary of a daily life''. an online training mode was selected to enable participation from any location in australia, and consisted of a powerpoint slideshow saved as a pdf and emailed to participants one week prior to training. this could be printed or saved to a local device and accessed at any time offline prior to and after the training. additionally, training included an individual 2 h tutorial on the strategic use of twitter, delivered via skype."
"limitations of the study include (1) @user1 took part in the training but not the follow-up interview for insights into their use of twitter for finding information or the rapid return to baseline low levels of using twitter, (2) we did not capture any data relating to any changes made to the participants' twitter profiles over time (e.g. updates to biographical statements, pictures) which could also have impacted on twitter interactions, and (3) we did not examine ''likes'' or ''media'' data on the participants' profiles over time. future research could investigate these aspects, and also the impact of other strategies for increasing tweeting frequency (e.g. by participants tweeting in a hashtag chat, which involves several tweets and replies, inserting links and using hashtags frequently), particularly in the first month following the initial training in order to support persistence in ''sticking with it'' while skills are being established. in this study, the researchers deliberately avoided public interactions with participants (e.g. in the form of liking, retweeting or responding to participants). with the option of ''liking'' tweets, twitter might also be a useful way for trainers to affirm trainees as they put their developing twitter skills into practice. the prominence of event tweets in both @user2 and @user3's data suggests that timing twitter training to immediately precede conferences might also facilitate a burst of tweeting more frequently, if including additional material about ''live tweeting'' is included, and might also enable @users to build up the size and reduce the density of their twitter networks."
author contributions: kang-woo lee designed and implemented marmot; kang-woo lee and junghee jo conducted the testing of marmot and analyzed the results; junghee jo wrote the manuscript; and kang-woo lee revised the manuscript.
"the general consensus among researchers from various domains is that \"80% of data is geographic\" [3, [cit] . the united nations initiative on global geospatial information management (un-ggim) reported that 2.5 quintillion bytes of data is created every day and a significant portion includes location components [cit] . google generates approximately 25 pb of data daily, a large portion of which consists of spatiotemporal characteristics [cit] . the mckinsey global institute estimates the portion of spatial aspect data was about 1 [cit] and is growing at an annual rate of 20% [cit] ."
"in marmot, operators in recordsetfuntions are classified into one of three categories: mapreduceterminal, mapreducejoint, or neither, depending on their own features. the term mapreduceterminal implies operators not generating a recordset as the result of their executions; operators involved in recordsetconsumer (e.g., storeascsv) usually belong to this category. the term mapreducejoint is defined as those operators in recordsetfuntions generating a recordset after grouping input data based on specific columns and then processing the grouped data. aggregation operators (e.g., groupby) performing an operation on a given set of values and returning a recordset usually belong to this category. lastly, there are operators in recordsetfuntions not involved in either mapreduceterminal or mapreducejoint-most operators belong to this category. among the three categories, mapreducejoint is an important separation point to produce the map phase and reduce phase. an example of mapreduceterminal and mapreducejoint is shown in table 2 ."
"for the experiment, we used two tiger files including real spatial data for korea. one file contained a nationwide continuous cadastral map with a size of 16gb (38,744,510 polygons); the other file included major urban areas created for management of land use with a size of 0.3gb (53,087 polygons). figure 12 shows the visualization of these two tiger files including real spatial datasets of korea. particularly, figure 12a of the five test cases in our experiment, three cases (i.e., obtaining mbr, range query without index, and range query with index) use only the cadastral data; one case (i.e., creating spatial index) uses only the major urban data and one case (i.e., spatial join) uses both the cadastral and major urban data."
"marmot receives a plan and automatically constructs one or more mapreduce jobs based on the features of each of the recordsetfunctions. in general, to create a mapreduce job for a required operation in hadoop, a programmer needs to specify both a map function and a reduce function. a spatial analysis task usually requires multiple mapreduce jobs to achieve a result, thereby making it cumbersome to write iterative code to describe complex mapreduce workflows [cit] . the main idea of marmot is to provide advantages which enable developers that have no detailed knowledge of mapreduce to build high performance geospatial analysis applications by simply combining builtin recordsetfuntions. the specific process of mapping a plan to a sequence of mapreduce jobs is described in figure 7 ."
"twitter use enables both individuals and groups to access their right to ''freedom of opinion and expression'' [cit], article 19) in their online and real-world communities, as evidenced in the outcomes of twitter campaigns for positive political and social change, including disability advocacy [cit] . according to the universal declaration of human rights [cit], article 27) ''everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits''. in addition, everyone has a right to education [cit], article 26) . accordingly, educating people who use aac on using twitter could facilitate their right to freedom of expression, and their communication rights enshrined in the convention on the rights of persons with disabilities [cit] . articles 2, 4, 9 and 21 of the convention on the rights of persons with disabilities focus on communication and communication technology and emphasise that people with disability not only have the right to use a range of communication modes but also to learn and use new technology that will increase their participation and access to information at an affordable cost."
"the tweet text from the spreadsheet twitter data was exported in plain text format, converted to all lower case, and used as input to the kh coder program [cit] to analyse and visualise the text content as co-occurrence network (con) and multidimensional scaling (mds) plots. frequently cooccurring terms in the visualisation are connected by lines/edges, the relative frequency of terms is indicated by the relative size of their node, and the relative frequency of co-occurrence of terms is indicated by the relative thickness of the edge connecting their nodes. words/terms clustered close together in the resultant mds visualisation are found more frequently close together in the source text, and may reveal key themes in the tweet data. for @user2 and @user3, con and mds plots were generated for the three separate time periods of interest: (1) three months pre-training; (2) three months post-training; and (3) three-six months post-training."
"the words captured in kh coder graphics in figures 3 and 4 give some insights as to the broad range of topics of conversation or expression in twitter and substantiate participants' views and experiences of using twitter. an example of the content of tweets as analysed and visualised in kh coder software is presented in the three months pretraining and three-six months post-training, with a con network presented in figure 3 for @user2 and an mds visualisation presented in figure 4 for @user3. potentially identifying words (names or brands) are removed by masking the labels."
"the training was piloted with two adult colleagues of the first author who wanted to increase their use of twitter for a range of purposes similar to the participants' stated goals (e.g. obtaining information, participating in discussion online, promoting their own work). the pilot study was conducted to check the (1) duration of the training, (2) level of information being suitable for people who wanted to learn to tweet more confidently and strategically and (3) clarity of the training. on the basis of the pilot, minor changes were made to the order of slides, and to clarifying technical details on the strategic use of the structural layers of twitter (i.e. the difference between micro, meso and macro layers) for sending tweets to engage with different audiences. during the interactive tutorial, the first author explained each slide and used the ''share screen'' feature of skype to demonstrate features of tweets and twitter settings on the twitter website. participants also had the opportunity to do the activities outlined in the tutorial and were encouraged to continue doing these after the tutorial. [cit], three months after baseline data collection for each of the participants, and follow-up data was collected for six months."
"during the process of constructing m when marmot finds an operator belonging to mapreducejoint, it constructs another list which is a set of recordsetoperators to be executed in reduce phase (i.e., r in figure 7 ). the specific procedure of constructing r is as follows. marmot decomposes the operator belonging to mapreducejoint and extracts two suboperators: the mapping operator and reducing operator. for example, in the case of picktopk which is one of the recordsetoperators in mapreducejoint, sort (cols) and take (k) are mapping and reducing operators of picktopk, respectively. the extracted mapping operator is attached at the end of m and the reducing operator is inserted into the initialized r. marmot then extracts the next recordsetoperator from the plan and resumes examining whether the operator belongs to the category of mapreducejoint. if the operator belongs to mapreducejoint, marmot inserts it in r and extracts the next recordsetoperator. this process is repeated until the last recordsetoperator is inserted in the r; then finally a mapreduce job is created consisting of one map phase and one reduce phase. during the process of constructing r, when marmot meets an operator belonging to mapreducejoint, marmot iteratively constructs another mapreduce job by treating the remaining recordsetoperators as a new plan. figure 8 shows an example of the process of creating mapreduce jobs in marmot. the given plan in the example consists of five recordsetoperators where operator3 is the only operator belonging to mapreducejoint. during the process, operator3 is decomposed into two operators-a mapping operator and reducing operator. the plan is eventually transformed into mapreduce jobs consisting of several map tasks and reduce tasks. each map task performs operator1 and operator2 and the reduce tasks perform operator4 and operator5. operator3, which is defined as a mapreducejoint, is broken down into two suboperators and they are carried out at map tasks and reduce tasks, respectively. the number of map tasks is heavily dependent on the size of the input dataset. in the default configuration, each map task takes care of 64mb of input dataset. the number of reduce tasks is, however, is usually decided by the parameters of the mapreducejoint operator, which is operator3 in this example. marmot then extracts the next recordsetoperator from the plan and resumes examining whether the operator belongs to the category of mapreducejoint. if the operator belongs to mapreducejoint, marmot inserts it in r and extracts the next recordsetoperator. this process is repeated until the last recordsetoperator is inserted in the r; then finally a mapreduce job is created consisting of one map phase and one reduce phase. during the process of constructing r, when marmot meets an operator belonging to mapreducejoint, marmot iteratively constructs another mapreduce job by treating the remaining recordsetoperators as a new plan."
"though the plug-in offers a convenient way to add new features to an existing system, it carries several limitations as described in figure 2a . first, it is inadequate to support seamless interactions between spatial and nonspatial operations. a nosql system basically treats plug-ins as legacy and generates temporary files while executing spatial and nonspatial operations by turns, obstructing improvements in workflow. second, its performance is limited due to the high number of input/output operations while reading or writing temporary files. furthermore, this input/output overhead increases with increases in file size, as well as the number of operations. from this, we assume that if we reduce the data transfer overhead between nosql and spatial plugins, we could improve data processing performance. marmot supports seamless interactions between spatial and nonspatial operations within a solid framework, as shown in figure 2b . this is possible because marmot is designed and developed by considering the data model and operations for processing spatial information from the initial phase of the development process. most nosql systems, however, are designed with no consideration of spatial functionalities. this leads them to adopt plug-in based architecture when they want to embed spatial functionalities later on. because both spatial and nonspatial operations are managed by one system, it is feasible to improve workflow. in addition, since temporary files do not need to be generated, the input/output overhead also decreases considerably regardless of file size or the number of operations for analysis."
"the majority of existing nosql systems supporting geospatial data processing on hadoop embed spatial functions as a form of plug-in to support extra features within their systems. this plug-in based approach suffers from severe performance problems due to the massive volume of data transfer between nosql and spatial plugins. in order to reduce data-transfer overhead, we have developed a new geospatial data processing framework on hadoop, named marmot. marmot executes both spatial operations and nonspatial operations in the same session to avoid massive data transfer between operations, and employs a new method that maps a sequence of operations, either spatial or nonspatial, into a series of mapreduce jobs. we also show the proposed method outperforms spatialhadoop with performance experiments."
"participants were recruited through social media and organisations providing services to people with disability. in total, six adults with no cognitive impairment consented to take part in the training study; [cit] . after randomisation and study commencement, but before their training commenced, three participants declined to complete the training, owing to competing study demands or personal health problems. therefore, this study included three adults (@user1, @user2 and @user3) aged between 35 and 50 years with little or no functional speech, who used aac systems, computers and the internet. all had previously used twitter and wanted to increase their strategic use of twitter. to protect their identity, the gender of the participants is not reported, details about their aac device are withheld and the plural pronoun is used to refer to the individual."
"each test query was run five times and the average, after excluding the highest and lowest values, was chosen to evaluate performance. the formula to express the performance improvement rate (pir) of marmot is:"
"in the future, we first plan to investigate what additional factors create performance differences between marmot and spatialhadoop and conduct extensive experiments using more large-scale geospatial big data. second, we are going to finalize implementing an apache spark-based geospatial big data processing system, which is currently underdeveloped. to address the aforementioned limitations of marmot, we have selected spark as a framework for our next system for analyzing geospatial big data since spark supports both in-memory and real-time data processing. finally, we will design and conduct experiments comparing performance between marmot and the new system."
"already, twitter is part of a wider use of social media for political and collective action, providing marginalised groups with a ''voice'' in wider society [cit] . [cit] reviewed the literature on social media and aac, noting that the majority of research to date focuses on facebook and its use by children and young people, and/or people's views and experiences of social media. they called for sociotechnical research [cit] into publicly available social media data produced by people who use aac to: (1) understand about how people using aac used social media platforms, and (2) identify increased opportunities for improved participation and inclusion online. despite the potential of twitter for the communication and technology rights of people using aac, little is known about the impact on people who use aac of learning to use twitter to increase their social networks, or to enter public discourse on topics of their own choosing. therefore, the aims of this sociotechnical research [cit] were to determine the impact of teaching people who use aac to use twitter, on follower count, frequency of tweeting, tweet content and the development of social networks in twitter."
"in both @user2 and @user3, the number of nodes and the number of edges increased posttraining. so, irrespective of the number of tweets, both @user2's and @user3's networks included more communication partners (@users) and more directed communication paths post-training. additionally, the network density decreased for both @user2 and @user3 post-training. a high social network density is generally taken to be an indicator of a homogeneous network where participants know each other. a lower social network density is generally taken as an indicator of network diversity and increased ''reach''. so, on these metrics, both @user2 and @user3 participants increased the size and sophistication of their twitter networks posttraining, as reflected in their gephi visualisations. thus, their twitter data supports experiences of improved social connectedness in reflecting reduced density with greater reach, and larger networks. that @user2 did this without substantially increasing the number of tweets sent compared to baseline suggests a degree of sophistication in twitter use developing as a result of the training followed by sustained use."
"for all @users, basic network statistics were compiled for the three separate time periods of interest: (1) three months pre-training; (2) three months post-training; and (3) three-six months posttraining. the weekly total twitter activity (i.e. tweets and retweets from, and mentions of) was computed and plotted on a timeline, highlighting the three separate time periods of interest. the individual participant twitter activity timelines were combined onto a common timeline, with a vertical line added to indicate the time of training for each participant; and network density was calculated for @user2 and @user3."
"the process of mapping a plan to a sequence of mapreduce jobs, specified in figure 7, is as follows. when marmot receives a plan from an analysis application, marmot extracts the first recordsetoperator from the plan and checks whether the operator belongs to the category of mapreducejoint. if the operator does not belong to mapreducejoint, marmot inserts it in a separate list storing recordsetoperators to be executed in map phase (i.e., m in figure 7 ) and extracts the next recordsetoperator. this process is repeated until the last recordsetoperator is inserted into the m then, finally, a mapreduce job is created consisting of only one map phase."
".build(); the plan is composed of six recordsetoperators: 'load', 'centroid', 'spatialjoin', 'groupby', 'picktopk', and 'storeascsv'. as shown in figure 10, using the 'load' operator, marmot reads the boundaries of each subway station and 'centroid' operator computes their center coordinates. with the 'spatialjoin' operator, marmot finds a city whose boundary contains the centroid of each subway station. then, the 'groupby' operator groups the output records based on each city and counts the number of their records. the output records should be '(city_id, count)' pairs, which contain the number of subway stations for each city. finally, the 'picktopk' operator sorts the record on the basis of numbers and picks the top five cities. these five records are stored in the hadoop distributed file system (hdfs) by the 'storeascsv' operator. in the process of transforming the plan to a sequence of mapreduce jobs, marmot recognizes two 'mapreducejoint' operators: 'groupby' and 'picktopk'. since each 'mapreducejoint' generates one mapreduce job, the plan is transformed into two separate mapreduce jobs, as depicted in figure 10 . for the first mapreduce job, the 'groupby' operator is the breakpoint producing the first map in the process of transforming the plan to a sequence of mapreduce jobs, marmot recognizes two 'mapreducejoint' operators: 'groupby' and 'picktopk'. since each 'mapreducejoint' generates one mapreduce job, the plan is transformed into two separate mapreduce jobs, as depicted in figure 10 . for the first mapreduce job, the 'groupby' operator is the breakpoint producing the first map and reduce tasks, and 'picktopk' operator is another breakpoint to produce the second mapreduce job. the output of the first mapreduce job is stored in a temporary hdfs file-so that the second mapreduce job can continue to execute the rest of the given plan. marmot deletes the temporary file when the whole plan has been executed."
"in this study, people who use aac and wanted to learn to use twitter approached the learning opportunity with a range of communicative goals for participation online. their goals demonstrate that they perceived twitter as an avenue for achieving the rights articulated in article 19 of the universal declaration of human rights [cit] and in the convention on the rights of persons with disabilities [cit], such as being used for accessing information (article 9), or providing a platform for free expression (article 21). furthermore, article 4 states that technologies should be affordable. twitter is a free platform available for any any adult able to access the internet and use the platform for a range of purposes including participating in communities and groups, albeit on-line. education is clearly defined in article 24. facilitating adults to learn to use social media and participate is important, particularly as many currently popular platforms were not available when the adults who may now want to use them were at school. according to the quality standards of single case multiple baseline design studies [cit] the study met the methodological criteria of including at least three cases, at least three data points in each phase of the study, and randomisation in the order of participants. in order to account for an expected variation in use of twitter across participants, all had at least three months baseline data collected. furthermore, the data were analysed using objective methods in computational coding using gephi and kh coder. however, the study did not meet the criteria of at least three cases showing an effect in terms of percentage of non-overlapping data (determined visually) in terms of pre-and posttraining frequency of either tweeting, retweeting or being mentioned in twitter. therefore, it is not possible to conclude that a 2-h online training based on information drawn from the twitter platform how to use twitter, including specific information on the way tweets are seen by different audiences in twitter, and customised to user goals, is effective in increasing twitter activity for people who use aac."
"the process of mapping a plan to a sequence of mapreduce jobs, specified in figure 7, is as follows. when marmot receives a plan from an analysis application, marmot extracts the first recordsetoperator from the plan and checks whether the operator belongs to the category of mapreducejoint. if the operator does not belong to mapreducejoint, marmot inserts it in a separate list storing recordsetoperators to be executed in map phase (i.e., m in figure 7 ) and extracts the next recordsetoperator. this process is repeated until the last recordsetoperator is inserted into the m then, finally, a mapreduce job is created consisting of only one map phase."
"in the environment of the internet of things (iot), various sensors have been mounted on objects in diverse domains, generating huge volumes of data at high speed [cit] . a significant portion of sensor big data is geospatial data describing objects in relation to geographic information [cit] . in general, geospatial big data refers to geographic data sets that cannot be processed using standard computing systems [cit] ."
"to show how marmot handles geospatial big data, this section presents an example of spatial analysis. figure 9 is a sample code of a plan retrieving the number of subway stations per city. for this analysis, nationwide datasets of cadastral and subway stations in korea are used."
".build(); the plan is composed of six recordsetoperators: 'load', 'centroid', 'spatialjoin', 'groupby', 'picktopk', and 'storeascsv'. as shown in figure 10, using the 'load' operator, marmot reads the boundaries of each subway station and 'centroid' operator computes their center coordinates. with the 'spatialjoin' operator, marmot finds a city whose boundary contains the centroid of each subway station. then, the 'groupby' operator groups the output records based on each city and counts the number of their records. the output records should be '(city_id, count)' pairs, which contain the number of subway stations for each city. finally, the 'picktopk' operator sorts the record on the basis of numbers and picks the top five cities. these five records are stored in the hadoop distributed file system (hdfs) by the 'storeascsv' operator. in the process of transforming the plan to a sequence of mapreduce jobs, marmot recognizes two 'mapreducejoint' operators: 'groupby' and 'picktopk'. since each 'mapreducejoint' generates one mapreduce job, the plan is transformed into two separate mapreduce jobs, as depicted in figure 10 . for the first mapreduce job, the 'groupby' operator is the breakpoint producing the first map the plan is composed of six recordsetoperators: 'load', 'centroid', 'spatialjoin', 'groupby', 'picktopk', and 'storeascsv'. as shown in figure 10, using the 'load' operator, marmot reads the boundaries of each subway station and 'centroid' operator computes their center coordinates. with the 'spatialjoin' operator, marmot finds a city whose boundary contains the centroid of each subway station. then, the 'groupby' operator groups the output records based on each city and counts the number of their records. the output records should be '(city_id, count)' pairs, which contain the number of subway stations for each city. finally, the 'picktopk' operator sorts the record on the basis of numbers and picks the top five cities. these five records are stored in the hadoop distributed file system (hdfs) by the 'storeascsv' operator."
"marmot has been developed as one component for constructing a national geospatial big data platform [cit] promoted by the ministry of land, infrastructure, and transport of the korean government. marmot is publicly available including github [cit] and will be integrated into other components of the platform to be used for public services such as transport, real estate, or disaster prevention. based on the long-term plans of the korean government to promote the geospatial industry, marmot also will be integrated with the national geospatial information open platform, v-world [cit], to provide a variety of map-based spatial analysis services. in addition, other ministries dealing with spatial data also plan to use marmot as an infrastructure platform to provide big data based analysis services."
"mapreducejoint storemarmotfile, store, storeascsv mapreduceequijoin, picktopk, reducebygroupkey figure 8 shows an example of the process of creating mapreduce jobs in marmot. the given plan in the example consists of five recordsetoperators where operator3 is the only operator belonging to mapreducejoint. during the process, operator3 is decomposed into two operators-a mapping operator and reducing operator. the plan is eventually transformed into mapreduce jobs consisting of several map tasks and reduce tasks. each map task performs operator1 and operator2 and the reduce tasks perform operator4 and operator5. operator3, which is defined as a mapreducejoint, is broken down into two suboperators and they are carried out at map tasks and reduce tasks, respectively. the number of map tasks is heavily dependent on the size of the input dataset. in the default configuration, each map task takes care of 64mb of input dataset. the number of reduce tasks is, however, is usually decided by the parameters of the mapreducejoint operator, which is operator3 in this example. although marmot has been built on a centos 6.9 environment, it can easily be executed on any operating system where hadoop is installed due to the characteristics of java's platform independency. as a hadoop system, marmot uses hortonworks data platform (hdp) 2.6 version. table 3 shows the specific environment of implementation and testing of marmot. although marmot has been built on a centos 6.9 environment, it can easily be executed on any operating system where hadoop is installed due to the characteristics of java's platform independency. as a hadoop system, marmot uses hortonworks data platform (hdp) 2.6 version. table 3 shows the specific environment of implementation and testing of marmot."
marmot is a functional architecture that processes geospatial big data in parallel with mapreduce jobs for both batch and streaming services. a mapreduce job usually divides big data sets into separate chunks which are processed by map tasks in parallel. the framework sorts the results of the maps which are later input to reduce tasks.
"as @user1 only interacted with six other @users following training, computational analysis (i.e. using gephi and kh coder methods) was not needed. the gephi networks reflected growth in the size and complexity of networks developed by @user2 and @user3 over time. an example of the computational analysis of twitter networks using gephi is presented for @user3 in figure 2 ."
"nosql systems are very well-adapted to the environment offering high-performance information processing on a massive scale and so are increasingly used to deal with heavy demands of big data applications. various nosql systems are currently available; however, only few provide spatial functionalities. mongodb supports spatial indexing which allows users to execute spatial queries on a collection involving shapes and points, but it does not support many advanced geospatial operations which are essential for advanced geospatial analytics. postgis has more comprehensive geospatial functionalities but still provides only limited number of spatial data types and very few functions to handle them [cit] . thus, existing nosql systems needing to newly include or extend spatial functions generally use a form of plug-ins to support the extra feature within their systems. for example, cassandra is extended to enable spatial data retrieval by extending its query language capabilities [cit], and hbase is extended to support spatial indexes of quad tree and k-d tree to enable range/knn queries and point datasets [cit] ."
"the mixed results of this study suggest that (1) targeted twitter training might be useful for people with communication disability who already use twitter and want to enhance their strategic use of its functions to build safe and enjoyable networks with more people online; and (2) social media platforms, such as twitter may be powerful in supporting adults with disability access their rights according to the universal declaration of human rights [cit] and the convention on the rights of persons with disabilities [cit] . @user2's goal of increasing others' awareness of disability issues online supports the notion that twitter might benefit both people who use aac being included in disseminating and generating information about disability; and benefit society through ''greater awareness of disability, improved attitudes, and increased knowledge about disability and its impacts on people with disability and the wider community'' [cit] ). @user3's use of twitter to advance their professional networks and business opportunities, and to engage with commercial entities in twitter for access to their consumer rights."
"participants' social media data produced in twitter was collected continuously on a weekly basis, amassing all tweets sent any day from the start of baseline to six months after the training (i.e. 9 months of twitter data). [cit], multiple hand content coding and computational analytic methods were used to determine: (1) the quantitative effect of the training on the participants' twitter skills, including frequency of tweets, retweets and mentions in twitter; (2) the strength of the networks developed for communication in twitter, in terms of their size and density; and (3) the content of tweet expressions either authored or retweeted by or mentioning the participants."
"nosql systems are very well-adapted to the environment offering high-performance information processing on a massive scale and so are increasingly used to deal with heavy demands of big data applications. various nosql systems are currently available; however, only few provide spatial functionalities. mongodb supports spatial indexing which allows users to execute spatial queries on a collection involving shapes and points, but it does not support many advanced geospatial operations which are essential for advanced geospatial analytics. postgis has more comprehensive geospatial functionalities but still provides only limited number of spatial data types and very few functions to handle them [cit] . thus, existing nosql systems needing to newly include or extend spatial functions generally use a form of plug-ins to support the extra feature within their systems. for example, cassandra is extended to enable spatial data retrieval by extending its query language capabilities [cit], and hbase is extended to support spatial indexes of quad tree and k-d tree to enable range/knn queries and point datasets [cit] ."
"six months after the training, a follow-up conversational-style interview was conducted with two of the participants (@user2 and @user3), and @user1 did not respond to the invitation to participate in the interview after taking part in the training. during their 30 min face-to-face interviews, the participants were shown graphic visualisations of their own twitter data networks and asked to reflect upon their twitter experiences since the training. the first author verified her interpretations of their experiences for reporting purposes. results of the follow-up interview were analysed descriptively and individually."
"to show how marmot handles geospatial big data, this section presents an example of spatial analysis. figure 9 is a sample code of a plan retrieving the number of subway stations per city. for this analysis, nationwide datasets of cadastral and subway stations in korea are used. inf. 2018, 7, 399 12 of 18"
"in marmot, operators in recordsetfuntions are classified into one of three categories: mapreduceterminal, mapreducejoint, or neither, depending on their own features. the term mapreduceterminal implies operators not generating a recordset as the result of their executions; operators involved in recordsetconsumer (e.g., storeascsv) usually belong to this category. the term mapreducejoint is defined as those operators in recordsetfuntions generating a recordset after grouping input data based on specific columns and then processing the grouped data. aggregation operators (e.g., groupby) performing an operation on a given set of values and returning a recordset usually belong to this category. lastly, there are operators in recordsetfuntions not involved in either mapreduceterminal or mapreducejoint-most operators belong to this category. among the three categories, mapreducejoint is an important separation point to produce the map phase and reduce phase. an example of mapreduceterminal and mapreducejoint is shown in table 2 ."
"in twitter, an individual tweet can represent the connection between the sender and receiver. a single tweet that mentions another @user is considered as ''directed'' towards that user; if a single tweets does not not mention any other account, it may be considered ''undirected''. in this study, undirected tweets were considered as being sent to a common notional ''user'' node labelled as ''undirected'', to permit a complete network to be formed from the available twitter data. the spreadsheet twitter data of @user1, @user2 and @user3 were imported into the gephi program [cit] for network visualisation. in the twitter network diagrams presented in this study, edges are presented as curved lines, the direction of tweets is clockwise around the edge, and the width of an edge is proportional to the total number of tweets recorded between the two nodes in that direction. the yifan hu layout algorithm [cit] was used for the twitter network visualisations with the data of @user2 and @user3 for the three separate time periods of interest: (1) three months pre-training;"
"spatial analysis is the process of investigating the positions, attributes, or relationships of objects based on spatial data to obtain valuable knowledge or to answer a question. in marmot, spatial analysis using big data is defined as a chain of one or more units of recordsetoperators, called a plan. in a plan, a recordsetloader and a recordsetconsumer are generally placed in the first and last position, respectively; each recordsetfunction is placed in the middle of the plan. in order to process a given plan, marmot sequentially processes each recordsetoperator comprising the plan. figure 6 shows an example of a plan consisting of a total of five recordsetoperators: one recordsetloader, three recordsetfunctions, and one recordsetconsumer. as mentioned in the previous section, the recordsetloader transforms external geospatial big data into an internal form of marmot which is a recordset. then, one or more numbers of recordsetfuntions process the recordset and create a new recordset containing the desired contents. finally, the recordsetconsumer stores the result in the output files on the file system."
"although marmot shows good performance results compared to other existing research, it still needs further improvement. first, marmot is restricted to batch processing only. batch processing is efficient for handling huge amounts of data, but outcomes can be delayed depending on the input data size and computing power of a machine. many recent geospatial data analytics require real-time data processing; however, marmot does not yet support real-time processing of streamed data. second, many machine learning algorithms read modestly sized source datasets iteratively, e.g., k-means and dbscan. for those applications, in-memory data processing offers the best match, but the current version of marmot lacks this. although marmot utilizes memory, in part, along with disks during data processing [cit], it cannot take full advantage of in-memory computation to increase processing speed."
"all participants were adults who used aac, had access to a computer and used the internet for over two years, and currently used the internet daily. the pre-training interview revealed participants' use of multiple social media platforms. twitter was used for reading and occasionally writing tweets (@user2 and @user3), and @user1 only read tweets throughout the baseline period. @user1 used facebook and youtube regularly, and twitter infrequently or rarely, and their goal was to use twitter for both writing and reading tweets to find information on topics of interest. @user2 used facebook regularly, and used twitter primarily for reading. their goal was to improve writing of tweets, and using twitter for: professional purposes, getting quick answers to questions, finding information on a particular topic, connecting socially with friends, participating in conversation, growing a community audience/network (e.g. raise profile with likeminded people), self-promotion or promotion of any aspect interest, and lobbying for causes/advocacy. @user3 used facebook and instagram regularly, and wanted to use twitter to: contribute to political discourse in public activism online, connect multiple social media platforms with a larger audience, and follow the news or current."
"the results for @user2 are mixed, with an initial and overall drop in twitter data in terms of frequency of tweets, retweets and mentions, compared to the full three-months of pre-training data, and increased when compared only to the twomonth period prior to the training. @user2's pattern of tweeting in this study could reflect cyclical variation in tweeting. there was an overall ''dampening'' in frequency of twitter data, with more activity during events when sharing the real-life experience with other tweeters in the audience. overall, even in the context of dampened frequency of tweeting, @user2 increased the size of their twitter network, sharpened or honed their ''voice'' on twitter, and displayed increased finesse and confidence in applying twitter functions to selfexpression in tweets and retweets."
"experiments were conducted to compare the performance of marmot to that of spatialhadoop, one of the top mapreduce frameworks supporting spatial functionalities. since the geospatial operators currently supported by spatialhadoop are limited compared to marmot, it is difficult to conduct our experiments based on complex spatial analysis tasks. therefore, we decided to perform"
"though the plug-in offers a convenient way to add new features to an existing system, it carries several limitations as described in figure 2a . first, it is inadequate to support seamless interactions between spatial and nonspatial operations. a nosql system basically treats plug-ins as legacy and generates temporary files while executing spatial and nonspatial operations by turns, obstructing improvements in workflow. second, its performance is limited due to the high number of"
"the raw network data, including tweets, retweets and mentions for all participants is presented in table i and in a concurrent graph in figure 1 ."
"twitter is a popular microblogging social media platform used by 3 million australians [cit], representing 0.9% of 319 million monthly active twitter users worldwide [cit] . twitter enables users to communicate with the public in short messages of 140 characters in length. messages require few keystrokes, and spelling errors and common abbreviations are tolerated, thus twitter is ideal for people who use augmentative and alternative communication (aac) [cit] . twitter use increases visibility, influence and reach online [cit] . people with communication disability associated with motor neurone disease [cit] or traumatic brain injury [cit] and their support associations have used twitter strategically for awareness raising campaigning (e.g. #alsicebucketchallenge, #braininjuryawareness month). however, despite the apparent benefits of twitter, currently people who use aac have relatively small twitter networks [cit] ."
"although mapreduce has been a highly popular framework for conducting parallelized distributed computing jobs, it is still not easy for developers to build complex geospatial big data applications. in fact, users of mapreduce often have analysis tasks which are too complicated to be transformed to mapreduce jobs. such spatial tasks require handling the chain of multiple mapreduce steps where the output path of the data from previous mapreduce jobs becomes the input path to the next mapreduce job. to address this issue, marmot supports automatic construction of one or more mapreduce tasks by taking on a spatial analysis task. when the task is transformed to mapreduce jobs, marmot automatically controls the number of mapreduce jobs in a way to achieve better performance by decreasing the overhead of mapping and reducing. figure 1 shows a brief structure of marmot which is based on a hadoop framework. the core of marmot consists of four main components: a plan scheduler, operation flow execution, geospatial big data storage management, and index management. this paper focuses on explaining how marmot transforms a sequence of operators for spatial analysis to map and reduce functions. specific content about other components is beyond the scope of this paper. applications. in fact, users of mapreduce often have analysis tasks which are too complicated to be transformed to mapreduce jobs. such spatial tasks require handling the chain of multiple mapreduce steps where the output path of the data from previous mapreduce jobs becomes the input path to the next mapreduce job. to address this issue, marmot supports automatic construction of one or more mapreduce tasks by taking on a spatial analysis task. when the task is transformed to mapreduce jobs, marmot automatically controls the number of mapreduce jobs in a way to achieve better performance by decreasing the overhead of mapping and reducing. figure 1 shows a brief structure of marmot which is based on a hadoop framework. the core of marmot consists of four main components: a plan scheduler, operation flow execution, geospatial big data storage management, and index management. this paper focuses on explaining how marmot transforms a sequence of operators for spatial analysis to map and reduce functions. specific content about other components is beyond the scope of this paper."
"the sequence of recordsetoperators assigned during map and reduce phases is executed in a pipeline manner by a number of mappers and reducers. the number of mappers is dynamically determined by the size of input data whereas the number of reducers is statically fixed in a plan. during the process of a given analysis, four mappers with two reducers are used in the first mapreduce job and two mappers with two reducers are used in the second mapreduce job, as shown in figure 11 . and reduce tasks, and 'picktopk' operator is another breakpoint to produce the second mapreduce job. the output of the first mapreduce job is stored in a temporary hdfs file-so that the second mapreduce job can continue to execute the rest of the given plan. marmot deletes the temporary file when the whole plan has been executed. the sequence of recordsetoperators assigned during map and reduce phases is executed in a pipeline manner by a number of mappers and reducers. the number of mappers is dynamically determined by the size of input data whereas the number of reducers is statically fixed in a plan. during the process of a given analysis, four mappers with two reducers are used in the first mapreduce job and two mappers with two reducers are used in the second mapreduce job, as shown in figure 11 ."
"prior to developing the training, the three participants were interviewed about their use of the internet and their goals for participating in the training. a semi-structured survey (available from the authors) was used and analysed descriptively to determine the patterns of use in terms of frequency and purpose on each type of social media platform (e.g. facebook, instagram, twitter) and any adverse events they had experienced in using social media. prior to their twitter training, participants were also asked about their knowledge and use of twitter and their goals for using twitter strategically."
"prior to the training, all data for @user1 reflected @user1 being mentioned in tweets sent by others, with only one original tweet from @user1. @user1's follow count did not change although @user1 demonstrated skill immediately in writing direct conversational tweets (i.e. starting with an @user) mentioning six other twitter handles and being mentioned by one other user. however, following this initial burst of twitter activity @user1 did not change from the baseline pattern of non-use of twitter as an expressive mode of communication after the training. both @user2 and @user3 increased in terms of follower counts, and network nodes and edges following the training. only @user3's graph (figure 1 ) reflects a notable increase in frequency of tweeting, retweeting and being mentioned."
"marmot receives a plan and automatically constructs one or more mapreduce jobs based on the features of each of the recordsetfunctions. in general, to create a mapreduce job for a required operation in hadoop, a programmer needs to specify both a map function and a reduce function. a spatial analysis task usually requires multiple mapreduce jobs to achieve a result, thereby making it cumbersome to write iterative code to describe complex mapreduce workflows [cit] . the main idea of marmot is to provide advantages which enable developers that have no detailed knowledge of mapreduce to build high performance geospatial analysis applications by simply combining built-in recordsetfuntions. the specific process of mapping a plan to a sequence of mapreduce jobs is described in figure 7 ."
"the twitter application programming interface (api) allows publically available twitter data to be retrieved on the basis of searches (e.g. @user name, one word, date); and the web browser add-on ncapture [cit] allows the results of a twitter search to be recorded in a machine-readable data format. over the relevant periods of interest, for both pre-and post-training data for @user1, @user2 and @user3, the twitter api was used to extract data about: (1) follow count (number of people following the user and number of people being followed by the user); (2) tweets and retweets sent from their account; and (3) mentions of their account. the nvivo program [cit] was used to convert the captured twitter data into microsoft excel [cit] spreadsheets for further processing and analysis."
"compared to existing systems, marmot is distinguished by the following. first, marmot supports seamless integration between spatial and nonspatial operations within a solid framework, thereby improving workflow performance. second, marmot outperforms existing top-tier, plug-in based, spatial big data frameworks, especially for complex and time-intensive queries involving a spatial index. third, marmot provides a variety of spatial operators, which allow executing further complex spatial analyses. finally, once application developers recognize a set of nonspatial and spatial operators, they can implement desired spatial analysis tasks without having to possess detailed knowledge of big data technologies."
"during the process of constructing m when marmot finds an operator belonging to mapreducejoint, it constructs another list which is a set of recordsetoperators to be executed in reduce phase (i.e., r in figure 7) . the specific procedure of constructing r is as follows. marmot decomposes the operator belonging to mapreducejoint and extracts two suboperators: the mapping operator and reducing operator. for example, in the case of picktopk which is one of the recordsetoperators in mapreducejoint, sort (cols) and take (k) are mapping and reducing operators of picktopk, respectively. the extracted mapping operator is attached at the end of m and the reducing operator is inserted into the initialized r."
"the rest of the paper is organized as follows. section 2 shows our related work and section 3 describes a systematic overview of marmot including integration between spatial and nonspatial operations. section 4 shows marmot's data model and section 5 presents the main algorithm which maps a sequence of operators for spatial analysis to mapreduce jobs and discusses implementation of marmot. an example of a spatial application using marmot and its performance evaluation is described in sections 6 and 7, respectively. finally, the discussion and conclusion along with our future work are presented in sections 8 and 9."
"for the experiment, we used two tiger files including real spatial data for korea. one file contained a nationwide continuous cadastral map with a size of 16gb (38,744,510 polygons); the other file included major urban areas created for management of land use with a size of 0.3gb (53,087 polygons). figure 12 shows the visualization of these two tiger files including real spatial datasets of korea. particularly, figure 12a shows just part of the continuous cadastral map because the entire data are too large to load onto the visualization tool. to demonstrate what all the data look like, partial data with a size of 1.24gb (4,720,616 polygons) involving seoul city and gyeonggi province are plotted using red on a grayscale osm basemap as the one by stamen. there are five target spatial operations in our experiments: obtaining minimum bounding rectangle (mbr) and creating spatial index, range query without index, range query with index, and spatial join with index. mbr has been most commonly used to approximate spatial objects and is one of the fundamental operations for spatial analysis. a query for obtaining mbrs is therefore included in our experiment. spatial index also plays an important role in geospatial domain to speed up retrieving certain objects in a spatial database, making measuring the performance of creating spatial index essential for this evaluation. range query allows one to search for spatial objects located in a specified spatial extent and is therefore a fundamental type of query in spatial databases. to see the impact of using spatial index on range query performance, we investigated two cases-with and without an index. lastly, spatial join is one of the most important operations for combining spatial objects and serves as building blocks for processing complex spatial analysis. even with the support of spatial index, spatial join is particularly complex and time-intensive. thus, efficient processing of spatial join is crucial to increase query performance."
"to show how marmot handles geospatial big data, this section presents an example of spatial analysis. figure 9 is a sample code of a plan retrieving the number of subway stations per city. for this analysis, nationwide datasets of cadastral and subway stations in korea are used."
"each of the @users responded differently to the training. @user1's follow count did not change throughout the three months of baseline data or up to six months following training. both @user2's and @user3's follow count increased from flat baseline numbers rising at three and six months following the training. @user1's main priority in the training was to use twitter for writing tweets, and this goal was not realised. in terms of frequency of tweeting, retweeting or being mentioned, @user1's twitter activity rose briefly after training, but rapidly returned to baseline levels, and it is not known whether twitter was a useful source for accessing information. @user1 might not have tweeted or retweeted sufficiently to build up their follow count or develop a useful network of information sources in twitter, which might yield information that could not be found more easily on an already familiar social media platform used, namely facebook."
"experiments were conducted to compare the performance of marmot to that of spatialhadoop, one of the top mapreduce frameworks supporting spatial functionalities. since the geospatial operators currently supported by spatialhadoop are limited compared to marmot, it is difficult to conduct our experiments based on complex spatial analysis tasks. therefore, we decided to perform a comparison for each of the fundamental spatial operations supported by both spatialhadoop and marmot."
"in this work, a process model is developed to reconstruct watertight and textured 3d maps from indoor environments by using a rgb-d sensor. the models are stored in a standard format to allow further use, such as virtual exploration, robot teleoperation and 3d printing. most of the current visual odometry and mapping techniques do not consider model re-distribution and visual quality, because more coarse representations can be used in navigation tasks. in our workflow, photorealistic 3d models are produced by using a laptop with a low-end gpu and a rgb-d sensor."
"our contributions are the following. 1) a scanning process is developed from raw rgb-d measurement stream into a textured and watertight 3d model, which is stored compactly in a standard wavefront format. 2) a photometrical refinement phase is developed, which improves the model keyframe quality by fusing hundreds of rgb-d measurements together. the sweeping process produces all necessary data for generating re-usable photorealistic 3d models. the core function is the camera tracking module which operates in real-time on a low-end gpu (nvs4200m with 48 cuda cores) [cit] . watertight and textured polygon mesh is generated in an automatic post-process. the model is stored in wavefront format, which also converts also to wrml, and therefore enables online 3d visualizations."
"we have shown how photorealistic 3d models can be captured using a rgb-d sensor using a laptop and low-end gpu. the main benefit is in storing the models compactly in a standard format to enable their further use in different applications. two mapping modes were discussed, which do not always require global optimization. the incremental mode fits trajectories, which have only a small number of potential loop closures. otherwise the slam mode is recommended. a watertight polygon mesh was generated using the poisson method and mesh appearance was directly mapped from the keyframe images. the resulting models are photorealistic despite that they have significantly lower memory footprint than raw point clouds. all phases in the process are automatically executed after a rgb-d video has been recorded. in the future, our goal is to experiment with 3d printing and develop easy-to-use tools for lighting normalization and global keyframe model refinement."
"after recording a video, gpu-boosted rgb-d tracking is executed which produces 3d trajectory [cit] . whether or not rgb-d tracking uses keyframes, only the trajectory is stored. the model keyframes are selected by looping the trajectory and storing a keyframe whenever user-specified angular or translational distance to the existing model is exceeded. the neighboring rgb-d measurements to the keyframes are efficiently localized (timestamp or frame index) and depth map fusion is executed. in depth map fusion, keyframe depth maps are filtered using all rgb-d measurements available. then, optionally, bundle adjustment is possible for the full model. the results presented in this paper do not use bundle adjustment at all, because the sequences are relatively short and pose error remains small. finally watertight polygon model is generated from the rgb point cloud using poisson reconstruction method. keyframe images are stored into a single texture and uv coordinates are generated for each polygon. the textured mesh is then stored in a simple wavefront format which can be loaded into various standard 3d modeling programs for further refinement. the video 1 illustrates the full process for sequence room a."
"where ω is a set of potential candidates, ∆t is a relative transformation between the frames, and k s is the nearest keyframe. the test point set p is a sparse representation of the view frustum. in particular, the frustum is approximated by n sparse 2d grids, each having uniformly sampled depth coordinates in the overall depth range of the rgb-d sensor."
"many application domains, such as 3d visualization and printing, benefit precise models. despite that relatively accurate rgb-d pose estimation and mapping approaches exist, most of the methods do not automatically output photorealistic 3d models which are compatible with standard modeling softwares. whelan, in fact, is the only one to suggest a specific technique [cit] to generate textured polygon meshes using rgb-d mapping process [cit] . we presented a realtime gpu implementation of a rgb-d camera tracker [cit], for which we now develop a process to produce photorealistic 3d models."
"in our previous work, we have concentrated on realtime augmented reality in a studio environment [cit] . to prevent camera tracking drift and to allow foreground actors, a static keyframe model was used as reference. however, several application domains exist where the digitized 3d model can be directly valuable. in robotics, tele-operation tasks benefit from a 3d model. websites support online 3d product catalogs and 3d printing devices are becoming available for consumer market. also apartment renovation and re-organization can benefit from a realistic 3d model. currently there is no standard process, which would produce visually pleasing 3d content for further use using commodity hardware. the available systems store the content in internal formats such as a point cloud (figure 1), a gaussian mixture model or a voxel grid on a gpu, which are not directly usable in standard softwares. the final memory consumption may also become unnecessarily high due to naïve reconstruction methods which introduce multiple surface instances."
"rgb-d tracking can be executed in three different modes: slam (algorithm 1), incremental (algorithm 2), and keyframe mode. the main difference is in selecting the reference keyframe for tracking. in slam mode, the keyframe database is built concurrently while tracking the camera. loop closures are maneuvered whenever re-visiting scenes that are already mapped. in incremental mode, a recent rgb-d measurement is taken as a reference, and memory consumption remains extremely small. in keyframe mode, the reference is selected as the nearest neighbor in a pre-recorded database. keyframe mode is only usable after an environment has been mapped."
"2 http://youtu.be/afrvrolja38 3 http://youtu.be/mfc1kv7ddpi fig. 2 . kitchen sequence. a raw point based model is generated using the slam mode. the camera re-visits same keyframes many times, and the errors cumulated during the loop are avoided 3 ."
"temporal correspondence problem between feature points fully avoided, and the warping function will produce the dense correspondence after the correct pose is found. the estimation is robust due to gradient-based pixel selection and a m-estimator. inverse compositional approach is used for minimization, which allows pre-computing the jacobian [cit] . minimization enables precise camera tracking. accuracy and comparison with kinectfusion were documented in our earlier work [cit] ."
"the task is to find the correct pose increment t(x), that minimizes scalar cost function 1 2 e(x) t e(x), where the residual is defined by"
"oriented points are transformed into a reference coordinate system. the poisson method finds a scalar function whose gradients best match the vector field, and extracts the appropriate isosurface. the algorithm uses openmp for multi-threaded parallelization and octree data structure to reduce memory consumption [cit] . to further avoid memory limitations, the mesh could also be done piece-by-piece using a single, moving reconstruction volume. the most interesting parameters are octree grid resolution, point weights and minimum point count in an octree node. because octree resolution is limited, the resulting mesh may become oversmooth at complex regions. the reconstruction accuracy depends mostly on the precision of the oriented point cloud. with microsoft kinect sensor, the depth noise increases with distance. in our tests, we measure distance to the camera and set quadratically decaying point weights. poisson reconstruction result without and with depth fusion can be observed in figure 6 . notice how poisson method generates the floor and fills holes despite that measurements do not exist (compare with fig. 1 )."
"where uvkey is the index of the best uv-mapping keyframe (fig. 8) . because frequent switches in uv-mapping directions can cause visually disturbing seams, mapping can be improved by enforcing the locally dominant keyframe. one method to do so is to recursively enumerate connected polygon neighbors in n passes, and then prefer the mapping direction which has the largest number of votes. finally the selected uv-coordinates are converted into global texture coordinates and stored. the keyframe images are not undistorted to better maintain maximum texture quality. the resulting meshes can be observed in figure 9 . when final texturing is patched together from keyframes, some color banding effects may occur if brightness varies across the images. manual camera settings reduce global lighting variation across images. to reduce the problem further, averaging or median filtering could be attempted. also more sophisticated laplacian pyramid filtering has been suggested for the task [cit] ."
"polygon models are compact in their memory consumption and are better supported by standard 3d modeling programs than point clouds. a polygonization phase generates a polygon mesh from a point cloud. in our context, the method should take into account noise and missing data. a common approach is to fit the points to a surface using the zeroset of an implicit function, such as a sum of radial bases or piecewise polynomial functions. we select the poisson method, because it produces a watertight surface based on a photometrically refined, oriented point cloud [cit] . the point normals are derived from the depth fused maps. a depth map is converted into a point cloud by"
"the memory consumption is shown in table i . the consumption is separated into geometry and texture consumption for poisson meshes. the datasets room a, room b and kitchen have 47, 23 and 14 keyframes. after the trajectory has been estimated, a raw point cloud is generated. the minimum requirement per vertex is 9 attributes (position, normal and color). after poisson reconstruction and texture mapping, the vertices require only 3 floats (x,y,z) and n triangles have in total 9 * n attributes (3 * 2 uv coordinates + 3 * 1 index). in addition texture map requirement is computed directly as k * 320 * 240 * 3, where k is the number of keyframes. table i shows memory footprint for 2 7, 2 8, and 2 9 octree grids. the corresponding geometric quality is poisson reconstruction produces watertight mesh, whose texturing is photorealistic as it is directly mapped from the keyframe images. the cost of reduced memory footprint is over-smoothing, which may occur at thin surfaces such as the shelf in 9a. also lighting changes can be detected at seams where texture data source switches from one keyframe to another. otherwise the models are photorealistic and in metric units. illustrated in figure 10 . the reconstructions in figure 9 are generated using 2 9 grid."
"because the keyframes are sparsely selected from the stream of rgb-d measurements, they do not utilize all available information. the intermediate point clouds, which are not selected as keyframes, are warped into a nearest keyframe and the final maps are filtered in post-processing. in effect, this improves depth map accuracy and fills holes. a method is required for determining the correct mode, because the warped depths may also produce a multi-modal distributions in case of occlusions. we assume the correct mode is near the median of the depth values. note that area covered by a pixel grows as a function of distance. this should be taken into account by avoiding points which are too far away from the ray through pixel center point (figure 4) . a large portion of the outlier points can be neglected based on color deviation to the keyframe pixels. thus, prior to any filtering, it is useful to discard the points whose color difference to the reference color is larger than a threshold. if the number of remaining points is small, the depth value should not be filtered, but completely discarded as noise. the median depth is computed from the remaining sample points, which determines a distance range around a surface. the t ⇐ minimize(i, p *, c *, i cur ) (eq. 1)"
"the remainder of the paper is structured as follows. section ii reviews the related works in cache replacement, section iii details the system outline and assumptions, section iv presents the cache admission and replacement policies used, section v explains the simulation scenario, section vi includes the analysis of results and section vii concludes the paper."
"caching is a very popular technique to enhance the performance of both wired and wireless networks. it is well studied for wired web applications. due to the space limitation, the mobile nodes can store only a subset of the frequently accessed data. a good replacement mechanism is needed to distinguish between the items to be kept in cache and that is to be removed when the cache is full. the important factors that can influence the replacement process are access probability, recency of request for a data item, number of requests to a data item, size, cost of fetching data from server, modification time, expiration time, distance etc. most of the replacement decision proposals are based on the above factors."
"we have developed a simulation model in java. in our simulation, nodes are randomly placed in an area of 800x800 m 2 . each node is identified by a node id and a host name. the data server is implemented as a fixed node in the simulation area. the data server contains all the data items requested by the mobile nodes. the size of each data item is uniformly distributed between s min and s max .the nodes in the network move randomly based on a random path. the nodes within a transmission range of 100m are taken as the neighboring nodes. the nodes that generate data request are selected randomly and uniformly. the time interval between two consecutive queries generated from each node/client follows an exponential distribution with mean of 10sec. each mobile node generates a single stream of read only queries. after a query is sent out, the client does not generate new query until the pending query is served. the data access pattern follows a zipf distribution [cit] with a skewness parameter as 0.8. a. performance metrices our performance metrics include cache hit ratio and average query delay. the evaluation of these parameters are done by varying the number of cache locations with respect to number of nodes and the behavior of cache hit ratio for different cache sizes. the hit ratio is defined as the percentage of requests that can be served from previously cached data. since the replacement algorithm decides whether to cache the data or not, it affects the cache hits of future requests. the query delay is the time interval between the query sent and the data transmitted back to the requester. average query delay is the query delay averaged over all queries."
"we compared the performance of our method with lru for different cache sizes. different cache sizes were used, ranging from 20% to 80% of the total size of the database. fig 2 and fig 3 shows the result with a set of 6 cache sizes which are 20%,30%,40%,50%,60% and70% of the total size of the database, respectively. fig 2 depicts the comparison of cache hit ratio for different cache sizes. this figure shows that cache hit ratio of e-lru is more for all cache sizes than lru. at small cache sizes e-lru shows significant improvement in cache hit ratio compared to lru. for large cache sizes the difference in hit ratio of the two policies became less significant. fig 3 is the comparison of average query delay for the same and it shows that average query delay is always lower for e-lru than by lru. this is due to the fact that e-lru uses the cache space more effectively and the number of data requests send to the server is reduced. in this paper, we have explored cache replacement issues for ad hoc networks and presented a new cache replacement policy for cooperative caching. the algorithm takes in to account the inter arrival time of recent references, size and consistency for page replacement. in lru only the last time of reference is taken and the numbers of references are not considered. since the inter arrival time of the recent reference is taken more preference is given to objects that have been accessed more than once. hence we are able to distinguish between data that are frequently referenced with that of occasionally referenced. as we are not caching data with bigger size cache space can be saved. since the algorithm is based on a key based approach it is simple to implement. experimental results show that the proposed replacement algorithm can significantly improve the cache hit ratio and lower the data access delay when compared to lru."
"in this paper we present a coordinated cache replacement policy, e-lru (extended lru) which evicts data based on size, time interval between recent accesses and consistency. almost all of the replacement algorithms proposed for cooperative caching are function based policies which involves the calculation of a value function based on different parameters, which uses complex data structures that makes the implementation difficult. least recently used (lru) on the other hand is the simplest replacement policy suggested from early days, for data caching. the disadvantage of lru is, it considers only too little information for replacement. the replacement policy we propose is an extension to the lru policy, aims at replacing the data objects based on the time difference between the recent references. the novelty of our approach lies in the key based replacement where we first consider size of the data item, then takes in to account the time interval between the recent references and the pages with longest reference interval time is dropped first. the advantage of our scheme is that the pages with shortest access time interval, which have more probability of reference in the future, will be kept in the cache. simulations shows that our replacement policy out performs lru in cache hit ratio and average query delay."
"the data items with maximum inter arrival time is considered for replacement. in both cases if more than one data item have the same value, the ttl parameter is taken and the one with lower ttl value is removed as the data with lower ttl will be outdated soon."
"due to limited sized cache, nodes cannot store all the content they need, but are forced to choose some items to keep and someone to discard every time when newly retrieved data fill up their memory. in cooperative caching if data replacement decision is made by individual node by considering the local cache, the performance is degraded because the data may be present in the neighboring node. if the cache is full, appropriate data from the cache is evicted to make room for the incoming one. the algorithm illustrated here considers recency and the number of references for a particular data item and gives more significance to data items that are referenced more than once. when we have data items referenced only once, it is given more priority for replacement. at this time lru is used for replacement. if an item is referenced more than once the inter arrival between the most recent two references is considered for eviction."
"the advancement in the new wireless technology has lead to the wide popularity of mobile ad hoc networks (manets). the primary attraction of a wireless ad hoc network is the fact that these networks can be formed spontaneously without any fixed infrastructure. this makes the ad hoc networks ideal for applications where initial connection setup is not possible or unreliable like military and disastrous areas. it has also wide range of commercial applications like personal area networks, sensor networks, emergency networks and vehicular communication. in these types of networks, devices generally have limited energy reserves and processing capabilities. bandwidth is also a scarce resource, limited by the nature of the wireless medium. in a data-management point of view, these restrictions introduce several issues that need to be addressed. data transfers must be reduced and mechanisms must be deployed in order to confront the frequent disconnections and low bandwidth constraints. therefore it is a challenging task to present the data efficiently by reducing the delay or waiting time to the end user."
"data caching is widely used in various domains to improve data access efficiency, by reducing the waiting time or latency experienced by the end users. in wireless mobile network, holding frequently accessed data items in a mobile node's local storage can reduce network traffic, response time and server load. caching in ad hoc networks is effective because a few resources are requested often by many users, or repeatedly by a specific user, which is known as the locality of reference. to have the full benefits of caching, the neighbor nodes can cooperate and serve each other's misses, thus further reducing the wireless traffic. this process is called cooperative caching. since the mobile nodes can make use of the data stored in another node's cache the effective cache size is increased. however, since the mobile nodes have limited memory, the cache area defined for storage is also limited. whenever the cache memory is full, we have to find an efficient method to replace some data from the cache to make room for the newly arrived data. the cache replacement strategy decides which data item has to be removed to place the new data items. cache replacement algorithm plays a central role in response time reduction by selecting suitable subset of data for caching. numerous cache replacement algorithms were proposed in web caching but only a few were proposed for ad hoc networks."
in cooperative caching nodes share the cache contents of neighboring nodes to utilize the full advantage of caching. the available cache replacement mechanisms for ad hoc network can be categorized in to coordinated and uncoordinated depending on how replacement decision is made [cit] . in uncoordinated scheme the replacement decision is made by individual nodes. coordinated cache replacement considers the information present in neighboring nodes for replacement.
"in order to improve the content diversity in the cooperative cache, our scheme does not cache any data coming from the neighboring nodes. this increases the availability of information for the user, as more data items are cached and also avoids additional request to the server. previous studies [cit] have shown that the requests for smaller objects are more compared to bigger objects, so the probability of achieving high hit rate is increased if we store more number of small objects. in our proposed model we set a threshold value for the size of the data item admitted to cache. the threshold value is set as 50% of the total cache size. any object bigger than this threshold is'nt brought in to cache."
"we consider a network architecture in which the number of n audio sensors are deployed in an environment and they separately detect and communicate the event information to the sink node. sensor nodes are assumed to use a fixed transmission power level. therefore, they have a fixed transmission range, i.e., r, in which they communicate with each other. when an event occurs in the environment, the average number of s audio sensors that sense the audio signal act as source nodes to sample the event signal and generate data packets. this initiates the data transmission from sensor nodes to the sink node. the source nodes are assumed to form the average number of h clusters each of which has a cluster head. each cluster head aggregates the source node packets and directly transmits to the sink node in one hop."
x y z u iso */u eq o1 0.74647 (10) 0.54064 (8) (7) 0.0004 (5) 0.0040 (5) 0.0033 (5) c17 0.0428 (7) 0.0480 (7) 0.0334 (6) 0.0006 (6) −0.0017 (6) 0.0035 (6) c18 0.0499 (8) 0.0469 (7) 0.0339 (7) 0.0001 (6) 0.0061 (6) −0.0047 (6) c19 0.0492 (8) 0.0371 (6) 0.0379 (7) 0.0059 (6) 0.0104 (6) 0.0001 (5) c20 0.0363 (7) 0.0437 (7) 0.0543 (8)
"second, we believe that our experimental results suggest that a template-based learning engine is quite competitive. building a more competitive tool based on our techniques for the svcomp competition is an interesting future direction. we believe choosing a template from a class of templates based on extracting feaures from a simple static analysis of the program and using priors gained from the experience of verifying similar programs in the past would make our approach efficient."
the proposed fcnn methodology consisted of multiple parameters which were carefully selected and validated through extensive experimentation. various experiments were designed to evaluate the effects of different parameter values under controlled conditions on the validation and testing sets.
"x, v y, v ì x ′, v y ′ for ì x, y, ì x ′, y ′ that makes the above implication false. however, notice that the above cannot be formulated as a simple implication constraint. the most natural constraint to return to the learner is"
"where m is the number of members in the cluster. if the sampling frequency determined by the cluster head, i.e., g-sensor, is f (hertz), then the total number of samples taken (in a second) is f.p[sample k is taken]. on the other hand, the expected number of the n-sensors which take any sample, i.e., d, can be given as"
"in our setup, invariants are synthesized simultaneously but verification is modular (i.e., the vcs are local to a process/function) and the configurations learned from them are local. one can in fact think of our solution as a simultaneous synthesis of contracts for all functions (in the sequential program setting), where the synthesis engines communicate and are simultaneously constrained through horn clauses."
"communication of wide-band event signal such as audio signal containing high frequency components requires high transmission bandwidth. once the event signal is sensed by a number of sensor nodes within the event radius, significant amount of traffic is injected to the network, which may easily cause severe packet losses and quick depletion of scarce network resource. therefore, in order to realize promising wasn applications, the additional communication challenges posed by the unique features of wide-band event signal, i.e., audio signal, 1 must be addressed. in the existing literature, there are many research efforts on reliable communication protocols that are devised for wsn domain and aim to efficiently communicate the event information to the sink [cit] . moreover, recently, a cross-layer rate control scheme is also introduced in order to fairly maximize the quality of the communicated multimedia streams by minimizing distortion [cit] . however, the primary objective of these existing protocols is to achieve reliability and energy efficiency. they do not take the spectral characteristics of the event signal to be communicated into consideration in the determination of communication parameters. in fact, different event signal characteristics may need different treatment in terms of communication requirements such as different communication bandwidth demand and diverse reliability objectives. furthermore, they do not address the communication of high-bandwidth event signals as in the case of wasn and do not perform or consider an exact reconstruction of the event signal from the delivered samples rather than averaged samples at the sink node."
"where k is the packet loss probability observed by the cluster head, i.e., g-sensor. according to the nyquist sampling theorem, if the event signal has bandwidth b 2 in hertz, it is imperative for n-sensors to take more than b number of distinct samples (n d ) within a second to satisfactorily reconstruct the event signal at the sink node, that is,"
"the paper is structured as follows. in sec. 2 we present an overview of horn-ice invariant synthesis; in sec. 3 we describe the decision tree based algorithm for learning invariant formulas from horn-ice samples; in sec. 4, we describe the algorithm that propagates the data point classifications across horn constraints; we describe the node/attribute selection strategies used in the decision tree based learning algorithm in sec. 5 and the experimental evaluation in sec. 6. we discuss related work in sec. 7, and conclude with directions for future work in sec. 8."
"single-step down-sampling was used in our network to minimize the computational burden of large input images. comparisons were made between max-pooling and average pooling for this step. the pooling factor in the global pathway was also tuned to ensure that the network sufficiently retained the relevant global information while maintaining a low number of parameters. 5, 7, 9 and 11 sized filters were tested for pooling the global patch while the local patch remained fixed throughout the experiments. since our proposed network required the dimensions of the global and local pathways to match during merging, the global input resolution was adjusted accordingly when testing different pooling factors. the unpooling layer was tested by comparing nearest neighbor, bi-linear and bi-cubic interpolation."
"the accuracy of atrianet could possibly be further improved by applying shape constraints which would be imposed on the either the intermediate layers or the output to control the generated 3d geometry. this would especially improve the segmentation at the mitral valve which connects the la with the left ventricle as currently, this region is arbitrarily cut by a straight line in the ground truth masks. atrianet attempts to segment the mitral valve region with a smooth rounded shape, leads to a poor performance value when evaluated. this issue could potentially be alleviated by manually re-labeling the ground truths masks to improve the definition of the mitral valve, which in turn, will improve the quality of the samples provided to atrianet during training. the accuracy of the la 3d reconstruction could also be improved by considering the 3d geometry and continuity between slices. a simple method to achieve this is to incorporate multiple slices as additional channels at the input of atrianet, however, our preliminary experiments showed that atrianet with three channeled or five channeled inputs performed worse and had substantially greater computational and memory costs. further attempts at considering the continuity of the la geometry in 3d warrant future investigation. in the future, we would like to apply atrianet for segmenting both atrial chambers and fibrosis since af is a bi-chamber disease [cit] . we are currently progressing towards creating a dataset that contains manual segmentations of both atrial chamber masks, which could potentially be used to train atrianet."
one of the strengths of bgfit is that it allows to easily expand the dynamic models. modeling the data in the application is performed through a representational state transfer (rest) web-service that receives a set of parameters as input and returns the function's result.
"a vital functionality of many biological organisms is the ability to maintain a stable internal state although the environmental conditions may change rapidly [cit] . this functionality is called homeostasis, and it is the leading feature of an organism to sustain its autonomy. by means of homeostasis, the organism self-regulates its growth and development, and maintains itself in the stable state. to sustain homeostatic stability within an organism, the nervous system, endocrine system, and immune system behave as one large, unified and complex system. the interaction and communication among all three systems are provided by the specific receptors on the cells [cit] ."
"several dynamic models based on differential and algebraic equations have already been proposed and are extensively used in these fields. these include sigmoid-http://www.biomedcentral.com/1471-2105/14/283 like curves such as logistic, gompertz, richards, schnute and stannard [cit] . the fitting of these curves to growth data is usually performed using in-house software or freely available tools. these include dmfit (available at http://www.ifr.ac.uk/safety/dmfit/) and ginafit [cit], which offer an excel add-in to model data according to several implemented dynamic models, along with packages provided for software r, namely grofit [cit] and cellgrowth in bioconductor. other tools such as microhibr (http://www.microhibro.com/) are available as a webapplication but with limited functionality. recently developed databases such as combase [cit] and labbase [cit] try to aggregate and store time-series of bacterial growth under several experimental conditions, serving as benchmarks for predictive microbiology."
"rxnorm is a standardized, controlled terminology for medications in the united states. it includes multiple components -medication name (both generic and brand), dosage, route of administration, ingredients, and fully-specified \"common dose forms\" (i.e., what a physician might enter as part of a prescription to a pharmacy) [cit] . these multiple components are linked together through a relational file structure, easily portable into database format [cit] . rxnorm was originally developed as part of the unified medical language system (umls) effort to integrate and map diverse and competing controlled medical terminologies in order to facilitate interoperability and data exchange across healthcare providers [cit] . early work that led to the development of rxnorm in umls was spurred by efforts of the hl7 [cit] 's [cit] ."
"rxnorm currently lacks any such built-in categorization scheme.. grouping medications into categories, such as by linking to ndf-rt, enables cross-organizational, cross-population, cross-provider, and crossdiagnostic comparisons, among other capabilities. however, there is still progress to be made in this arena, particularly in achieving better coverage/linkage of medications between rxnorm and drug categorization schemes [cit] ."
"on the other hand, due to the lack of the autonomous operations in these protocols, reliable sensor communication partially or fully depend on feedback messages from the sink. however, the feedback may not reach in time to provide reliable communication of event information and it may lead to waste of scarce network resources. hence, it is imperative to develop an autonomous communication protocol for wasn that enables the sensor nodes to cooperatively and effectively communicate the event signal without the need for any coordination with the sink. this necessitates the sensor nodes to have the capability of self-organization rather than the control of a central entity such as the sink node or base station. with this self-organizing feature, the sensor nodes are expected to reliably communicate their readings with minimum energy expenditure despite any communication problems such as packet losses, link errors, and excessive delays so that the sensor network operates in a relatively stable state."
"this automated process can be applied to single dataset, or to a collection that aggregates similar or complementary data, such as replicates of an experiment. this provides both a global view on aggregated data and a fine control on specific measurements."
"in order to illustrate the organization of the data and how to retrieve the available information, we will exemplify the application of bgfit tool in two different projects: 1) bacterial growth fitting and 2) tumor cell growth. the data is available at the webpage, along with all the necessary documentation. (see also additional file 1)"
"v. discussion direct segmentation of atrial chambers from 3d lge-mris is a challenging task. this is due to a lack of distinguishing features on the lge-mris that enable the separation of atrial tissue from non-atrial regions, in addition to the poor image quality of lge-mris due to motion artifacts, noise interference and significant variations in image intensities."
"af, especially persistent af, is driven by complex substrates, which are widely distributed throughout both atrial chambers [cit] . repeated episodes of af also produce further changes in the structural properties of the atria, i.e., atrial structural remodeling (dilatation, myofiber changes and fibrosis) [cit] . as a result, direct study of the atrial structure and its changes in patients with af is vital to the understanding and treatment of af."
", asking the learner to meet this requirement when coming up with predicates in the future. the above is best seen as a (non-linear) horn implication counterexample (horn-ice). (plain implication counterexamples are linear horn samples.)"
"the aim of direct capture, at the suggestion of active clinicians, was to emulate the functionality of the medicare part d website formulary search (http://plancompare.medicare.gov/pfdn/formularyfinder/). this website allows active searching for medication names, an auto-complete suggestion feature as the user types (familiar to many readers from google search), brand/generic identification, and modal popup functionality with common dosage information specific to each selected medication (to facilitate accurate dosing information capture)."
bgfit web-application serves both as: (i) an automated fitting tool for experimental data using an extensible set of dynamic models through a distributed architecture and; (ii) a data repository that stores and manages experimental data.
"before marking a node as łtruež or łfalsež the learner would like to ensure that the consistency of the decision tree with respect to the horn constraints is preserved. for this he calls the horn solver, which reports whether the proposed extension of the partial valuation is indeed consistent with the given set of horn constraints, and if so which are the data-points that are łforcedž to be true or false. for example, let us say the learner has constructed the partial decision tree shown in fig. 4b, where node n 4 has already been set to łtruež and nodes n 2 and n 5 are unclassified. he now asks the horn solver if it is okay for him to turn node n 2 łtruež, to which the horn solver replies łyesž since this extended valuation would still be consistent with the set of horn constraints in fig. 4a . the horn solver also tells him that the extension would force the data-points d 12, d 8, d 7, d 5, d 4, d 3, d 1 to be true, and the point d 2 to false."
"3) data integrity -rxnorm data should provide high-quality, reliable data with consistent mappings (from the ehr data) back to the original source (rxnorm/umls), even in the face of flexibility (#1) needs."
"where the penalty is proportional to the number of horn constraints cut by the attribute split. however, we do not penalize a split when it cuts a horn constraint such that the premise of the constraint is labeled negative and the conclusion is labeled positive. we incorporate this in the penalty function by formally defining penalty(s, s a, s ¬a, h ) as"
"the computation of c clearly depends on the statistical properties of n(k) such as mean and variance. however, if a mean estimate for n(k), i.e.,n, is available, an approximation for c, i.e., b c, can be given as"
"consider a system with variables ì x, with initial states captured by a predicate init(ì x), and a transition relation captured by a predicate trans(ì x, ì"
"nowadays, cnns are widely used for image classification and segmentation tasks. u-net [cit], dilated u-net [cit] and v-net [cit] have been successful in many medical segmentation tasks whereas vggnet [cit], inception [cit] and resnet [cit] have achieved state-of-the-art performances in image classification. however, due to the single scaled inputs/information utilized in these aforementioned architectures except inception, all lack the ability to directly process multi-scaled information -an aspect atrianet excels at. the difficult task of la segmentation from lge-mris requires more input information to capture its complex geometry. in this aspect, atrianet can essentially process twice the amount of information due to its dual-pathway, resulting in its superior performance. inception was the only architecture that contained multi-scale processing. however, it is done internally within only one image patch. this suggests that cnns such as inception [cit], which uses internal multiscale processing, learns substantially less information than explicit multi-scale processing, such as those used in atrianet. furthermore, our experiments using various cnns suggest that atrianet can generalize to many unseen patient data and produce robust segmentations, lessening the chance of overfitting, compared with other cnn approaches."
"bgfit's repository of dynamic models allow users to apply their own models, as well as take advantage of an existing and expandable set of contributed models, each bestowing to a richer environment. with this functionality it is possible to compare the results of different fittings in a single dashboard. the models currently implemented are baranyi, gompertz, logistic and richards models [cit], first and second order polynomial regression, exponential decay, lumry-eyring -lenp type ib (ode) [cit] for modeling the kinetics of irreversible protein aggregation, hyperbolastic growth model of type iii (h3) [cit] and live cell fraction model [cit] . to complement the dynamic modeling feature, users can also apply manual regression on the data, traditionally performed as a linear fitting in logarithmic scale."
"despite the argument above, it turns out that implication counterexamples are not sufficient for learning invariants in program verification settings. this is because reasoning in program verification is more stylized, to deal compositionally with the program. in particular, programs with function calls and/or concurrency are not amenable to the above form of reasoning. in fact, it turns out that most reasoning in program verification can be expressed in terms of horn clauses, where the horn clauses contain some formulas that need to be synthesized."
"where b is typically 2. we know the p(x i ) to be 1/(total number of different combinations) for the general, non-reformatted case of rxnorm, and for the reformatted case the joint entropy p(x i ) to be the multiplicative of 1/(search size), 1/(effective branching factor of common dose form, and 1/(effective branching factor of dose units). totaling h(x) for the various probabilities in each case gives us approximately ~15 bits in both cases (as expected theoretically). in other words, we reduce the needed cache size/search space and provide a bounded limit on the number of round trips to the database without loss of information. in plain language, for such an application, restructuring rxnorm can convey the same information content more efficiently, which, obviously, has a positive impact on speed and performance regardless of system configuration."
"number of n-sensors becomes cluster head at the end of an n-sensor clustering phase. if we assume that s, h, and k are known a priori by the system setting, b c can be computed as"
"we have developed learning-based black-box algorithms for synthesizing invariants for programs that generate horn-style proof constraints. we have overcome several challenges in this process that non-linear horn constraints bring, giving new and efficient decision tree algorithms that build small decision trees consistent with samples. our algorithms come with robustness guarantees that they will always succeed in building a tree when one exists while working in polynomial time, and with convergence guarantees that they will find an inductive invariant if one expressible in the logic exists. we have implemented and evaluated our technique, and shown that our tool favorably compares with state-of-the-art tools on a large class of benchmarks for several styles of programs that compile to non-linear horn constraints."
"the above framework would be extensible to any standardized terminology implemented as part of a practical computing application in some provider ehr system. generally, the framework would apply across most settings, though medications of interest may differ (e.g., in this study, in a behavioral healthcare setting, we are primarily interested in coverage of psychotropic medications and medications for common chronic health disorders, such as diabetes and hypertension) [cit] . it is important to note that the four factors may be ascertained independently (e.g., demonstrating data integrity only), though such an approach is not typical in many real-world implementations. in such settings, the factors may compete (e.g., flexibility vs. data integrity), and the principle goal is to evaluate the factors as a whole, not individually."
hac aims to communicate a sufficient number of data samples from the event signal to enable the exact reconstruction of the wide-band event signal with minimum energy expenditure.
"surely,n has to be determined in order to find p opt . however,n depends on some system parameters such as desired number of cluster heads and the average number of time slots during which overall clustering process is expected to be completed. if we assume that the overall clustering phase is completed within k slots, using the markov inequality, an upper bound for the probability, that an n-sensor cannot become a cluster head, can be given as prðc p kþ 6 e½c k ð6þ"
the obtained fittings and corresponding comparisons and simulation results are fully available at the webpage under projects tumor growth and comparing bgfit with r respectively.
"the second benchmark suite consists of 12 concurrent programs obtained from the literature on concurrent verification, including the work of . note that some of these programs use non-linear expressions over numerical variables as ground terms. consequently, the annotations our tool generated were also non-linear."
"here, we describe an approach to direct capture utilizing rxnorm in a live ehr system deployed in a large, multi-state outpatient behavioral healthcare provider in the united states (centerstone), serving over 75,000 distinct patients each year across 130 clinical locations. the overarching goal is to understand if and how standardized terminologies like rxnorm can be used to support such practical computing applications in live ehr systems."
a detailed post analysis of the segmentation errors was performed for each test patient data to compare the predictions to the ground truths for each image slice throughout each 3d
"input data is stored using a hierarchical-based organization with three different layers. the top-level layer, http://www.biomedcentral.com/1471-2105/14/283 project, defines global properties for the project, such as user permissions and whether it is publicly available. the middle layer, experiments, aggregates the different results in folders. the bottom layer, measurements, is the user's actual data and can store 3-dimensional annotated data, although only the first two dimensions are used in the modeling extensions for now (figure 1 ). bgfit represents a central repository for data, models and fittings."
"(1) a robust decision tree learning algorithm that learns using horn implication counterexamples, runs in polynomial time (in the number of samples) and has a bias towards learning small trees (expressions) using statistical measures for choosing attributes."
"while not intended to be exhaustive, this list implements a wide set of algebraic and differential models that are used in many areas or research and serves as a support for future expansions by users."
"1. spectrum estimation phase: n-sensor estimates the spectrum bandwidth of the event signal, i.e., b, to determine how many samples must be taken and delivered to satisfactorily reconstruct the event signal at the sink. hac algorithm uses a periodogram-based welch method [cit] to estimate the spectral bandwidth of the event signal. the details of this procedure is given in section 4.1."
"polyphospharic acid was prepared by mixing orthophosphoric acid (15 ml) and phosphorus pentaoxide (23.5 g) followed by heating on a water bath for 1.5 hr. a catalytic amount of polyphosphoric acid (160 g) was added to resorcinol (11 g, 100 mmol) and ethyl acetoacetate (13 ml, 100 mmol) and was heated on a water bath (75-80 °c) for 20 min. with stirring."
"data augmentation was also used to artificially increase the amount of data. elastic deformations, affine transformations and warping were found to be effective in increasing the performance in previous studies [cit] . the proportion of the training set to augment was tuned as to introduce a sufficient amount of new data but not cause overfitting."
"in the molecule of (i), the mean planes of the two coumarin units make a dihedral angle of 88.07 (2). in one of the coumarin units, the the dihedral angle between the least-squares planes of the pyrone and benzene rings is 3.36 (6)°. in the other coumarin unit the pyrone ring adopts an envelope conformation and the dihedral angle with the aromatic ring is 13.23 (6)°."
"all of these factors are discussed in more detail in the results. however, it should be noted that speed -measured as response time -was estimated by running the application using different medications with different medication component values (number of dose forms, number of dose units, etc.). higher values presented a larger search space and thus longer run-times."
"in this section, we first argue the need for building learners that work with non-linear horn-ice examples and then give an example of how our horn-ice invariant synthesis framework works on a particular example. fig. 1 shows the main components of our horn-ice invariant synthesis framework. the teacher has a program specification she would like to verify. based on the style of proof, she determines the kind of invariants needed (a name for each invariant, and the set of terms it may mention) and the corresponding verification conditions (vcs) they must satisfy. the learner conjectures a concrete invariant for each invariant name, and communicates these to the teacher. the teacher plugs in these conjectured invariants and asks a verification engine (in this case boogie, but we could use any suitable program verifier) to check if the conjectured invariants suffice to prove the specification. if not, boogie returns a counterexample showing why the conjectured invariants do not constitute a valid proof. the teacher passes these counterexamples to the learner. the learner now learns new invariants that are consistent with the set of counterexamples given by the teacher so far. the learner frequently invokes the horn solver to guide it in the process of building concrete invariants that are consistent with the set of counterexamples given by the teacher. the teacher and learner go through a number of such rounds, until the teacher finds that the invariants supplied by the learner constitute a valid proof."
"the structure of rxnorm in its umls download structure is not directly amenable to use as the basis for such a system [cit], in particular with consideration for i/o performance in a large, high-volume ehr. for such an application to function effectively without noticeable lag-time, it is imperative to cache as much data on the server-side web application. with the addition of the dynamically defined lists and search boxes, this means that minimizing the number and time of round-trips to the database over the network (the \"i/o\" in this case) is of principal concern. this entails a strategy of normalizing the data into the smallest possible subsets that could be searched quickly via efficient indexes and used to construct the minimal web caches in order to support front-end functionality with minimal lag time, even in the face of potential hundreds or thousands of concurrent users. in other words, we operationalized the basic premise behind modern relational databases [cit] of particular interest was separating dose amount and dose units into separate, unlinked fields; such a design allows clinicians to manually edit either field independently to enter an atypical dosage (e.g. off-label use). this separation is useful for evaluating specific changes in dosage for research or analytical purposes, as well as algorithm development for cdss purposes. otherwise we are comparing string fields of dosage information for variation or to identify medication changes, which can be imprecise and difficult to parse across patients. data integrity can be maintained by delimiting the possible dose units that a clinician can enter for a given medication to those units known to be applicable (derived from rxnorm itself)."
"bgfit supports collaborative projects by providing a central repository which can be used by several teams simultaneously, handling large experimental datasets through a clean and hierarchy-based organization of the data. bgfit allows users to implement and reuse an ever growing network of models, to improve validation of their methods, thus supporting model comparison and selection procedures."
"for concurrent programs, we have used both rely-guarantee proof techniques [cit] ] and owicki-gries proof techniques [cit] to verify the assertions. all benchmarks were manually converted into boogie programs, essentially by encoding the verification conditions for rely-guarantee-and owicki-gries-style proof requirements, respectively. table 2 shows the results of running horn-dt-boogie on these programs. the column łinvž reports the number of invariants that need to be synthesized in parallel for a particular benchmark. the column łdimž refers to the learning dimension (i.e., total number of predicates over which invariants are synthesized)."
"the crucial property of our learning algorithm is that if the given set of constraints c is satisfiable and if the data points in x are łseparablež (as defined below), it will always construct a decision tree consistent with c. we say that the points in x are separable if for every pair of points d 1 and d 2 in x we have a base predicate ρ which distinguishes them (i.e., d 1 ρ iff d 2 ρ). this result, together with its time complexity, is formalized in theorem 3.1. below, by the size of a horn formula we mean the total number of occurrences of literals in it."
"the decision tree algorithm in section 3 returns a consistent tree irrespective of the order in which nodes of the tree are processed or the heuristic used to choose the best attribute to split nodes in the tree. if one is not careful while selecting the next node to process or one ignores the horn constraints while choosing the attribute to split the node, seemingly good splits can turn into bad ones as data points involved in the horn constraints get classified during the construction of the tree. we experimented with the following strategies for node and attribute selection:"
"the right-hand-side of figure 6 compares the runtimes of horn-dt-boogie and ultimate automizer. horn-dt-boogie was able to verify 28 programs very quickly, requiring less than one second each. by contrast, on half of the programs (i.e., 26), ultimate automizer required more than 10 s to finish. on benchmarks that both tools successfully verified, horn-dt-boogie is about two times faster than ultimate automizer in terms of the total time taken. in fact, we find it surprising that our prototype without optimizations is competitive to the best tool for this track of sv-comp. we believe that this demonstrates lucidly that template-based black-box invariant synthesis is a promising and competitive technique for program verification."
"the entire source code for bgfit and the implemented models are available online, as well as the instruction to setup a fully functional installation locally. this addresses data confidentiality by allowing each laboratory to keep a local bgfit version for private projects."
the rest of this section presents the three benchmarks suits in detail and discusses our empirical evaluation. all experiments were conducted on a intel core i3-4005u 4x1.70ghz cpu with 4 gb of ram running ubuntu 16.04 lts 64 bit. we used a timeout of 600 s for each benchmark.
the data modeling features allow users to choose a dynamic model and estimate the parameters that best describe the dataset. with this information bgfit simulates the estimated curve and presents the results in a chart along with the original dataset and goodness-of-fit measures.
"the modeling extension should implement these functions to be fully compliant. this approach forces a strict interface for communication, but on the other hand, it offers flexibility on the implementation of the model as it is technological agnostic."
"our learner uses a decision tree based learning technique. here the internal nodes of the decision tree are labeled by the base predicates (or łattributesž) and leaf-nodes are classified as łtruež, łfalsež, or ł?ž (for łunclassifiedž, which happens during construction). each leaf node in a decision tree represents a logical formula which is the conjunction of the node labels along the path from the root to the leaf node, and the whole tree represents the disjunction of the formulas corresponding to the leaf nodes labeled łtruež. the learner builds a decision tree for a given set of horn constraints incrementally, starting from the root node. each leaf node in the tree has a corresponding subset of the data-points associated with it, namely the set of points that satisfy the formula associated with that node. in each step the learner can choose to mark a node as łtruež, or łfalsež, or to split a node with a chosen attribute and create two child nodes associated with it."
"in the crystal, the molecules are linked by o-h···o hydrogen bonds (fig. 2, table 2 ) forming rings with four molecules, graph-set motif r 4 4 (32), according to the etter's graph-set theory [cit], centered about inversion centres."
"machine learning is a class of algorithms which learn from a given set of data and labels by creating their understanding in a process known as feature separation. classification and segmentation are two typical problems solved with machine learning algorithms. in traditional machine learning classification algorithms such as support vector machines [cit], random forests [cit] and k-nearest neighbor [cit], a set of features is generated manually from the raw data, and fed into a classifier. this requires domain expertise in the field of the task at hand, as a rigorous feature selection procedure is required to find the optimal feature combination for learning. optimization is then performed on the features to minimize an objective function, which results in the linear separation of the data from different classes. however, despite the effectiveness of these algorithms over the years, the manual feature engineering and algorithm selection processes are major bottlenecks for improving the performance on classification tasks."
"for a series of such calls to procedure horn-forced-2, in which the constraints are fixed and the partial valuations are successive extensions, the total time taken will be bounded by o(h · n)."
"the second prototype is named horn-dt-chc. its teacher component is a fresh implementation consisting of roughly 2500 lines of c++ code, while the decision tree learning algorithm and the horn solver are taken from horn-dt-boogie. horn-dt-chc takes chcs in the smtlib2 format as input and learns a predicate for each uninterpreted function declared. we use horn-dt-chc to evaluate the performance of our learning algorithm as a chc solver."
"by designing the application based on a distributed architecture that separates heavy calculations from the data management and repository, computational load is distributed through different locations and network of models. this allows bgfit to scale as the userbase grows."
"mapping data from legacy systems into rxnorm post-hoc, or 2) direct capture of data in rxnormcompatible format at the point of data entry. challenges with the former scenario in real-world ehr systems include the fact that only a certain percentage of medications map correctly, even with sophisticated algorithms [cit] or intensive, post-hoc manual matching by researchers, and that many successful mapping efforts have been limited to well-defined subsets containing only fully-specified prescriptions incorporating national drug codes (ndcs) [cit] . even the most sophisticated algorithms cannot overcome all the limitations of poorly collected medication history at the point-of-care, such as open-ended free text fields, non-standard abbreviations, or invalid combinations due to uncontrolled data capture (e.g., recording a medication in a non-existent dose unit or dose form). limiting medication data to particular subsets or engaging in post-hoc manual mapping efforts is not feasible for real-world systems. direct capture avoids some of these issues; however, there are challenges with this approach as well. for example, a medication missing in a search list can prevent data entry from occurring (i.e., \"medication coverage\"). the direct capture approach also necessitates transforming the complex relational structure of the original rxnorm tables into a bare-bones structure to populate fields/search boxes in a web screen that can support the high-speed input/output (i/o) required for such applications, maintain high data quality, and allow for flexibility in data capture [cit] . additionally, any direct capture system must be capable of handling multiple potential clinical scenarios, such as allowing for partial data capture when the patient does not recall specific information or lacks the faculties to report complete information (e.g., an individual experiencing psychotic schizophrenia). search functionality used in direct capture approaches can also open up further issues around search optimization and information retrieval filtering, commonly applied to web search (see discussion)."
"in oncology, it is also crucial to model tumors growth and understand their dynamics under different internal and external perturbations. for example, linking growth parameters with pharmacokinetic-pharmacodynamic (pk/pd) models can help predicting the responses of tumor dynamics when exposed to distinct drug regimes [cit] . this knowledge can be further used to optimize the design of new experiments and support preclinical development of oncology drugs."
"the final result was searchable information for 18708 known medications and over 42,000 common dose forms thereof. this information allowed clinicians (and patients) to search for previously prescribed medications and assist with capturing accurate dosing information (as seen in figures 2 and 3). the response time on the medication auto-complete search feature was measurable in milliseconds and generally imperceptible to the end user during human factors testing (described in forthcoming publications). where the question mark above is replaced by the actual med_list_id of the medication selected in the search box. the \"common_form\" field is shown in the popup. once the user makes a selection (including a dummy option for \"unknown\"), med_name, dose_amt, and dose_units are loaded into the appropriate fields. the user can still manually adjust the dosing information at that point, which necessitates delimiting the possible dose_units for the selected medication for that dropdown field."
"although dynamic models and databases for biological growth data have now reached a mature state, there is still no easy-to-use software, to our knowledge, that allows experimentalists not familiar with computational tools to extract relevant parameters in an easy and automated way and simultaneously efficiently manage their data. this constitutes the main motivation for the development of bgfit, which further allows to integrate more sophisticated and complex models, both algebraic and differential, due to its flexible and expandable architecture. its main utility is thus to provide a user-friendly web-service that couples database management with model inference, with expected applicability in several areas of research. the examples here provided (data available for illustrative purposes in the webpage) include microbiology projects, with the estimation of bacterial growth curves under different sugars, and oncology, where models for the time evolution of carcinoma weight are inferred."
"in the end, such a balance was deemed necessary to provide end users with the flexibility needed to capture medication history data across all possible clinical scenarios while still maintaining reasonable data integrity. one takeaway is that capturing medication data in fully-specified rxnorm form 100% of the time may not be a realistic goal, but capturing components of rxnorm that can be used to reconstruct an approximation of rxnorm format is achievable. as such, the system design was able to maximize data integrity without loss of flexibility."
"(85,795/92,058) of all prescriptions during that year-long time period. many of those non-matching medications were due to slight variations in naming (such as for \"extended release versions of some medications, e.g., venlafaxine vs. venlafaxine er, seroquel vs. seroquel xr) that could be easily correctly using a more complicated matching algorithm such as one based on regular expression pattern matching [cit] . the other non-matching medications tended to be uncommonly prescribed medications and/or alternative therapy options such as nicotine cessation products, multivitamins, and dietary supplements (e.g., omega-3 fatty acids). these non-matching patterns are similar to findings reported elsewhere [cit] . the overall coverage was thus judged to be adequate for the intended purpose in this case -capturing medication history that might impact current treatment decisions and/or the evaluation of current treatment effectiveness -given that 1) the coverage rates of prescription orders were in line with previously reported values [cit] and, 2) the non-matching prescriptions were generally of a nature (e.g., spelling variations, vitamins) that would not interfere with that intended purpose."
"in this phase, the sink node forms the complete sample set of the event signal f s (t) using the samples generated and transmitted by n-sensors using sampling frequency f and sampling probability a. here, we assume that each n-sensor i also generates and transmit an indicator set i i cluster head cluster 1 cluster head cluster 2"
"the data-management features supports the modeling process and facilitates the collaboration by creating a central point of access. one of the motivations for this application is the need to have a better workflow for collaboration, avoiding the exchange of files using traditional methods, such as emails and shared folders. thus, bgfit features a hiearchical-based data storage where users can define their own teams and attribute read/write permissions accordingly. additionally the public scope can also be defined, allowing to openly share and publish the data online."
"as introduced in section 4, hac uses the irregular sampling scheme in which each n-sensor takes a sample from event signal with probability a while it samples the event signal with the sampling frequency f. each cluster head of n-sensor clusters acts as a g-sensor to determine a in its cluster. using a, the probability that a specific sample of the event signal, i.e., sample k, is taken (p[sample k is taken]) at least by one n-sensor is expressed by"
"since the mentioned architectures contained only one pathway, the impact of patch size equivalent to the local/global resolutions in atrianet was tested to evaluate its performance during the benchmark study. to ensure for a fair comparison of the different architectures, the same training, validation and testing datasets were used for la endocardium segmentation. due to the random initialization of the weight parameters, biases can be introduced in different training sessions where the accuracy can differ by a slight margin for the same network trained on the same data. to avoid this, each architecture was trained three times independently, and the results were averaged. all architectures were implemented using tensorflow and trained from scratch to eliminate any source of bias from existing implementations. the default hyper-parameters, initializations and training schemes recommended by the original authors were used."
"there are several techniques for finding inductive invariants, including abstract interpretation [cit], predicate abstraction [cit], interpolation [cit], and ic3 [cit] ]. these techniques are typically white-box techniques that carefully examine the program, evaluating it symbolically or extracting unsatisfiable cores from proofs of unreachability of error states in bounded executions in order to synthesize an inductive invariant that can prove the program correct."
"in particular the calls made by the decision tree algorithm (algorithm 1) of sec. 3 are of this type, and hence the total time across those calls is bounded by o(h · n)."
"rxnorm has taken on increased importance with development of meaningful use standards in the united states, which specified the use of rxnorm for medication data in early drafts of the standards [cit] . although the explicit requirement was removed in the final draft for stage 1 meaningful use, the standards still encourage rxnorm use, and there is a strong belief that rxnorm or an equivalent tool will be required at stage 2 or 3 of meaningful use when those standards are released in order to achieve the overarching goal of interoperable health records."
"each n-sensor i decides whether or not to become a cluster head by sending request to cluster (rtc) message with a probability p at the beginning of each time slot. when n-sensor i sends a rtc message, n-sensors that are in the correlation radius of n-sensor i and receive the rtc message form a cluster whose head is n-sensor i. in order to provide quick formation of the clusters that is required for the real-time communication, it is essential to avoid possible collision of rtc messages. the optimal selection of p can provide an optimal performance in the cluster formation phase. hac provides a nearly-optimal solution for the cluster formation phase as follows. we assume that the cluster formation is initiated with the detection of an event signal by n-sensors. we also assume that the event signal is detected in time slot 1 and n-sensor i has n(k) neighbors that are neither a cluster head nor a member of a cluster at the kth slot. in this case, the expected number of time slots during which n-sensor i successfully transmits a rtc message and becomes a cluster head, i.e., c, can be given as"
"in the future, patient medication history information will theoretically be available electronically via data exchange across provider ehrs. however, such electronic exchange is not imminent, and current methods for collecting medication history present broad challenges [cit] . for the time being, systems are needed that are capable of capturing such history in a standardized format in the current provider's ehr. such systems would also establish the necessary data infrastructure for future electronic exchange of standardized medication history data across providers, once such capabilities become widely available. additionally, since medication history is typically captured via patient selfreport, techniques for applying rxnorm for capturing medication history in live clinical settings and ehrs are directly applicable to capturing similar information in research study settings [cit] ."
"for further comparison, a previously proposed method for fully automatic la segmentation was also compared with atrianet by executing an existing implementation of the algorithm on the testing data in our study and evaluating the generated segmented masks [cit] . this allowed for a direct comparison of the effectiveness of our proposed method against a previous method used for the same task in terms of both the accuracy of segmentation and the computational efficiency."
"it history directly from patients, which was later extended to interact with provider/patient collaborate eprescribing tool called \"myrxpad\" [cit] summarized many of these efforts. other researchers have focused efforts on utilizing rxnorm as part of specific research projects [cit] . all of the aforementioned efforts have produced insights regarding application of rxnorm toward developing web-based tools, personal health records, and specific research studies. this paper extends these studies by exploring the necessary steps toward implementation of rxnorm in an adapted form in ehrs."
"the learning algorithm (algorithm 1) can be seen to run in time o(h · n). to see this, observe that in each iteration of the loop the algorithm produces a tree that is either the same as the previous algorithm 2 finding variables forced to true and false 1: procedure horn-forced input: horn constraints c over x, partial valuation u over x consistent with c. output: forced-true(c, u), forced-false(c, u)."
"these rings are linked, with each molecule participating in two rings, forming a three-dimensional network. the structure is stabilized further by weak c-h···o hydrogen bonds."
"it is tempting to think that the learner can learn invariants using positively and negatively labeled configurations, similar to machine learning. however, [cit] argued that we need a richer notion of samples for robust learning of inductive invariants. let us recall this simple argument."
"the first benchmark suite consists of the entire set of łtrue-unreachž (error is unreachable) recursive programs of the software verification competition [cit] ]. this benchmark suite contains 52 programs, including both terminating and non-terminating programs. for recursive programs we used a modular verification technique, in the form of function contracts for each procedure. we run horn-dt-boogie on these programs by manually converting them into boogie programs. for three of the 52 programs we used non-linear expressions over numerical variables as ground terms, for rest of the programs we used numerical variables as ground terms."
"in this study, we have developed and evaluated a dual fully convolutional neural network for robust automatic la segmentation from lge-mris. our algorithm enables the reconstruction of la in 3d with a dice accuracy of 94% as well as accurate estimates of key clinical measurements. the extensive evaluation of our pipeline demonstrates that it is superior to previously proposed state-of-the-art cnns, setting a new benchmark for future studies. our study may lead to the development of a more accurate and efficient atrial reconstruction and analysis approach, which can potentially be used for much improved clinical diagnosis, patient stratification and clinical guidance during ablation treatment for patients with af."
"the stable state of a wasn corresponds to a network operating condition in which the sensor network autonomously, reliably, and yet energy-efficiently communicate a wide-band event signal at the sink despite any communication problems such as packet losses, link errors, and excessive delays. in wasn, the spectral characteristics of event signal impose a sampling frequency rate constraint on source nodes to enable successful reconstruction of the audio signal according to nyquist sampling theory [cit] . spectral characteristics determine the total number of samples to be transmitted per unit time over the network and hence, the traffic load over the forward paths. when the traffic load over the network is excessively high, this leads to possible collisions and packet losses on the forward paths. thus, it is required to estimate the spectral bandwidth of the event signal for energy efficient and reliable communication of sensor measurements. similarly, in biological homeostasis, the properties of an external stimuli detected by neuron cells determine the response of organism to this external stimuli. when an external stimuli is detected by neuron cells, according to the properties of the external stimuli such as kind and the magnitude, neuron cells trigger the endocrine system to initiate the hormone secretion. based on this similarity, we consider source nodes that sense and sample the event signal as neurons in neural system and call them as n-sensor. like a neuron in neural system, in wasn, the aim of an n-sensor is to sense and sample the event signal and estimate the spectrum of the sensed event signal."
"a horn clause (or a horn constraint) over x is disjunction of literals over x with at most one positive literal. without loss of generality, we will write horn clauses in one of the three forms:"
"in this paper, we propose and evaluate a novel cnn for fully automatic la segmentation. our method is developed and validated on the largest 3d lge-mri dataset from 154 patients with af from the university of utah. this exciting development is a very important step towards patientspecific diagnostics and treatment. ii. methods direct la segmentation from raw lge-mris is challenging due to the massive imbalance between positive (thin wall la) and negative (background) pixels. to overcome this, we propose a novel dual fully convolutional neural network (fcnn), with the alias atrianet (fig. 1), that contains two parallel pathways to process both local and global information. the details of the configuration of atrianet are provided in table. i."
"in table 1 some of the models implemented in bgfit are shown. figure 2 illustrates several features of bgfit. panel a) represents a specific model overview webpage (hyperbolastic growth), where the user can define the algebraic or differential equation, along with all the detailed description regarding the parameters, such as the expected search range values and which of them represent initial conditions. the source code for estimation and simulation is automatically generated, thus expanding the model collection currently available. in this page the user can also download all the statistical data of the fittings performed. panel b) shows one measurement fitted with different models, including a manual regression, allowing to compare them graphically and numerically. the simulations are plotted along with the original experimental data, further supporting visual inspection of the results. panel c) illustrates the simultaneous estimation of different measurements of the same experiment. this allows to fit a model to several replicates, useful for finding an average model for similar experimental conditions. panel d) shows all the estimation results obtained for a given model, allowing the user to download the information in a csv file."
"this counterexample takes the form of a non-linear horn implication counterexample, and cannot be represented as an ice counterexample. it relates one configuration in the precondition and two configurations in the postcondtion, all satisfying the current conjectured contract, to a configuration in the postcondition that does not satisfy the current contract."
"in this paper, the homeostasis-inspired autonomous communication (hac) protocol for wireless audio sensor networks (wasn) is introduced. using the spectral properties of the event signal, the aim of hac is to enable sensor nodes to achieve energy-and bandwidth-efficient it computes the estimate of power spectral density of f s (t) as the average of these k periodograms (i k (f n )) as follows"
"the adaptive moment estimation (adam) optimizer [cit], a type of gradient descent algorithm, was used for optimization. the learning rate was kept constant at 0.0001 without adjustment and the exponential decay rates of the 1 st and 2 nd moment estimates were set to 0.9 and 0.999 respectively. during training, the accuracy was evaluated on the validation dataset after each iteration of all the training data through the network. this was repeated until the validation accuracy stopped increasing, and the best performing model was selected for evaluation on the test set. the network was developed in tensorflow [cit], an open-source deep learning library for python, and was trained on an nvidia titan x-pascal gpu with 3840 cuda cores and 12gb ram. the training phase took approximately four hours and predictions on each 3d lge-mri took around one minute to compute."
"the web-service should support two functions and a baseline for comparisons between different models, e.g., root mean square error (rmse): 1) parameter estimationwhich takes the data-points, such as time series, a range set for the parameters and outputs the estimated parameters using linear/nonlinear regression and 2) model simulation -that receives a set of parameters for the model as input and returns a simulated curve."
"a major goal in developing rxnorm was to provide a consistent, standardized way to identify the essential components of prescriptions for purposes of facilitating electronic capture of such data in electronic health records (ehrs) to support the exchange of patient information across providers (i.e., interoperability), development of clinical decision support systems (cdss), quality improvement efforts, and other research endeavors. early efforts to utilize rxnorm have primarily focused on: 1) navigating between names/codes across differing vocabularies and home-grown legacy systems, 2) exchanging data across providers, and 3) developing medication-related cdss [cit] . many of these efforts have taken the approach of mapping existent ehr data into rxnorm format post-hoc [cit], although there are an increasing number of efforts to develop \"direct capture\" methods where prescription data are captured in rxnorm format at the point of data entry [cit] . however, there is limited evidence about the use and implementation of rxnorm in actual clinical practice outside of such research settings [cit] ."
"the nature of the legacy medication history data in the system, as well as new data captured directly from patients, is problematic in terms of quality and consistency. patients often cannot remember the exact dosage taken, let alone other aspects of prescriptions that rxnorm dictates (such as dose units, dose form, and route of administration). additionally, other requirements (e.g., state, payer, accreditation bodies) require only medication name and dosage (amount and units). meaningful use itself does not currently specify the medication components to be captured in medication history."
"n-sensors that sense and sample the event signal estimate the spectral bandwidth of the event signal f s (t) and obtain the bandwidth b. for the estimation of the event signal bandwidth, the n-sensor uses the periodogrambased welch method [cit] . the details of the method is given in appendix a."
"the robustness and superiority of atrianet was demonstrated by comparing its performance with current widely used cnn architectures for the task of la endocardium segmentation using the same lge-mri dataset and same prior/post processing procedures. the networks investigated here included u-net [cit], dilated u-net [cit], deeporgan [cit] and v-net [cit], which are popular for medical image segmentation; and vggnet [cit], inception [cit] and resnet [cit] which have obtained state-of-the-art performances in image classification. popular fcnns for semantic segmentation such as fcn-8 [cit], deconvolutional neural network (deconvnet) [cit] and segnet [cit] were also investigated. since the three classification networks were traditional cnns, they were converted into fcnns by replacing the fully connected layers at the end of the network with convolutional layers for pixel-wise segmentation prediction. the intermediate pooling layers were also removed to avoid significant losses in dimensionality. it should be noted that vggnet with its fully connected layers removed, is the single-pathed version of our proposed fcnn, hence, the effect of having a dualpathway was tested implicitly."
"where z is allowable unsuccessful transmission attempts for a packet before it is dropped. similarly, k c is the packet loss probability observed in the link between g-sensors and the sink node and using p c and p f, it can be expressed as"
"false (a) horn constraints given by the teacher point ⟨p4, 2, 1⟩ and d 2 is the data point ⟨q4, 2, 1⟩. we use the convention that the data points are vectors in which the first component is the value of a łlocationž variable łlž which takes one of the values łp0ž, łp1ž, etc, while the second and third components are values of x and y respectively. this horn constraint is shown in the bottom of fig. 4a ."
"most of the current wireless sensor network (wsn) applications do not have high bandwidth demands, and are usually delay-insensitive. more recently, the technological advances in cmos cameras and microphones have fostered the development of wireless multimedia sensor networks (wmsn) for many emerging applications such as multimedia surveillance sensor networks, advanced health care delivery, traffic avoidance, and person locater services [cit] . they will also increase the capabilities of traditional monitoring and surveillance systems by virtue of the distributed system of multiple cameras and sensors. among these, wireless audio sensor networks (wasn), i.e., networks of wirelessly interconnected audio sensors, are also promising for the realization of efficient audio surveillance applications [cit] ."
"the second comparison performed was with the results obtained by state-of-the-art grofit package in r [cit] . we have used the testing datasets provided upon installation and compared bgfit with grofit results on gompertz and richards models (see all seven measurements at http://kdbio.inesc-id.pt/bgfit/projects/62/experiments/ 146, along with figures and tables), thus demonstrating the consistency of our results."
"procedure horn-forced in algorithm 2 shows our procedure for identifying the subset of variables forced to true or false, by a partial valuation for a set of horn constraints. intuitively, the standard linear-time algorithm for horn satisfiability by dowling and gallier [cit] in fact already identifies the minimal set m of variables that are forced to be true in any satisfying valuation, and assures us that the others can be set to false. however, the other variables are not forced to be false. our algorithm essentially runs another set of sat problems where each of the other variables are set to true; the instance returns sat if and only if the variable is not forced to be false. in our algorithm, variable x being set to true is modeled by x being marked ł * xž, and this instance being sat corresponds to the fact that false is not marked ł * xž after all marks have been propagated. fig. 5 illustrates algorithm 2. the final marking computed by the procedure is shown above or below each variable. variables set to true in the partial assignment are shown with a ł+ž above/below them. variables forced to true (respectively false) are shown with a ł(+)ž (respectively ł(-)ž) above/below them. theorem 4.1. let (x, c) be a horn sample, u be a partial valuation consistent with c. then procedure horn-forced correctly outputs the set of variables forced true and false respectively by u in c."
"for its packets to inform the sink node about which samples are taken from f s (t) by it. i i is a vector that includes 1 for the taken samples and includes 0 for the missed samples. hence, using i i, the sink node knows which samples are taken by n-sensor i during a sampling interval t (s) if the data packet including these samples and their indicator set is received. if i i is not received with a data packet, indicator set of this packet is set as 0. furthermore, i i is simply transmitted as a dft e length bit stream because it includes a string of 0 and 1 bits. therefore, the usage of i i incurs a considerably low transmission overhead. in the sink node, the sample set generated and transmitted by n-sensor i, i.e, k i, can be given as"
"commonly prescribed dosage for each medication. dose frequency (e.g., q.d., b.i.d.) was also captured independently of this rxnorm construct using snomed ct codes derived from umls. we extracted snomed terms for the most commonly used ranges (as agreed upon separately by a health information exchange centerstone is part of comprising six of the largest outpatient behavioral healthcare providers in the united states). this included terms such as qd -\"once a day\", qam -\"once a day, in the morning\", qpm -\"once a day, in the evening\", \"qhs -once a day, before bed\", bid -\"twice daily\", tid -\"three times daily\", \"qid -four times daily\", qod -\"every other day,\" prn -\"as required\", and mdu -\"as directed\". this list could be amended as needed to cover additional terms, including terms not covered by any terminology in umls."
"several authors of this paper were also authors of the original ice learning framework [cit] ]. the present work came about in our realization that implication counterexamples are just not sufficient. it is now commonly accepted that constrained horn clauses are the right formulation for most verification tasks. this includes programs with function calls (that cannot be inlined, say due to recursion, or due to deep nesting) and concurrent programs. learning to solve these clauses gives rise to horn-implication counterexamples naturally."
"the modelling extensions only require the implementation of the necessary interface and for it to be deployed on a location that is accessible by bgfit. this approach allows for every component of bgfit to be deployed online, encouraging collaboration and the reutilization of these tools. it can also be used in a local installation while keeping the access to all the developed models."
"return p ′, n ′ step (but with a leaf node labeled), or splits a leaf to extend the previous tree. at each step we maintain an invariant that the collection of data points in the leaf nodes forms a partition of the input set x . thus the number of leaf nodes is bounded by n, and hence each tree has a total of at most 2n nodes. when the algorithm returns (successfully or unsuccessfully) each node in the final tree has been processed at most once by calling the labeling/splitting subroutines on it. furthermore, the main work in the subroutines is the calls to the horn-sat and horn-forced procedures. each call to horn-sat takes o(h) time, and hence the calls to horn-sat totally take o(h · n) time. the calls to horn-forced (which is called only for the leaf-nodes), can be seen to take a total of o(h · n) time (see sec. 4). it follows that algorithm 1 runs in o(h · n) time."
"the final table structure was comprised of three tables (med_list, med_list_common, and med_list_dose), populated by embedding the sql code in the appendix into an etl process (via kettle)."
the application is designed using a model-view-controller architecture effectively separating data-management and dynamic modeling that is performed using extensions that are decoupled from the web-application.
"it is easy to see that each time the procedure horn-forced marks a variable x with a mark m, the sequence of markings till this point forms a valid c ′ -pebbling of (x, m) from true, and the final sequence of markings produced does indeed constitute a complete pebbling. then v c ′ ."
"in endocrine system, gland cells are stimulated by neurons to secrete hormones so as to keep the organism in the biologically stable state. similarly, in wasn, source nodes, i.e., n-sensors, form cluster to transmit event information to the cluster head in order to stimulate the cluster head for data transmission. hence, we consider the cluster heads of the n-sensor clusters as gland cells and call them g-sensors. in immune system, t-cells sense any malfunction in the organism and trigger the neural and endocrine systems to take an appropriate action. similarly, in wasn, some sensor nodes must detect and notify any malfunction such as collisions and channel error and their symptoms such as packet losses and excessive delay. with this regard, we consider these sensor nodes that detect any malfunction in wasn as t-cells and call them as t-sensors. the analogies and the mappings between the homeostatic system elements and sensor networks are also outlined in fig. 1 . note that each sensor node may jointly act as n-sensor, g-sensor, or t-sensor according to its current state such as source node and cluster head. based on the analogies established in this section, in the next section, we present homeostasis-inspired autonomous communication (hac) algorithm."
"the main contributions of bgfit are delivering a platform for automated data modeling of large time-series dataset and providing a baseline for comparison between different models, either novel or already described in the literature."
"a biological organism is open to various external stimuli. neurons in the nervous system takes the stimuli, e.g., taste, smell, vision, etc., via the sensory parts, and triggers an output reaction at the effectors, e.g., tissues and muscles. after the nervous system detects an input stimulus, the endocrine system produces and releases hormones through gland cells. thus, the interaction between the nervous and endocrine system is the homeostatic response behavior of the organism to sustain its stable internal state. any malfunction that adversely affects the operation of the organism is detected by the immune system of the organism. the immune system is the defense mechanism for the maintenance of homeostatic stability. this system responds to foreign substances that are generally called pathogens. the response is applied through b-cells and tcells of the adaptive immune system. b-cells and t-cells are white blood cells and have the capability to bind to and eliminate pathogenic material, i.e., antigens. in conclusion, there is a constant interaction among the neural, endocrine, and immune systems, and the collaboration of them provides a promising model for the construction and development of self-organizing, highly-functional, and adaptable intelligent systems."
"we assume that each sensor node uses a number of different frequency channels and selects and switches its radio front-end to one of these frequency channels in order to transmit and receive data packets. the selection of frequency channels used for the intra-cluster communication of sensor nodes is assumed to be made by cluster head. as will be introduced, hac protocol allows the neighbor clusters, that may interfere with each other, to use different frequency channels. this aims to avoid high level of interference among the neighbor clusters. for the communication between cluster heads and the sink node, a single unique frequency channel, that is not used in intra-cluster communication of sensor nodes, is assumed to be set initially and used by the cluster heads to transmit the aggregated data packet to the sink node. thus, the sink node also use the same frequency channel with the cluster heads in order to receive data packets from the cluster heads."
"the wireless channel is assumed to be shared in fixedduration time slots, which are, in turn, captured by sensor nodes in order to transmit. the duration of a time slot consists of two different intervals named packet transmission and acknowledgment (ack) transmission interval. in the packet transmission interval, each sensor node transmits a packet to its destination and an ack frame is transmitted back by the destination if the packet is successfully received. if at least two or more sensor nodes, that are in the communication range of each other, transmit using the same time slot, their packets collide and the current transmission attempt become unsuccessful."
"overfitting was a potential issue in larger neural networks due to the large number of parameters. to minimize this issue, dropout rates of 0%, 25%, 50% and 75% in atrianet were evaluated to find the most effective number of nodes to remove while still keeping enough nodes for sufficient feature learning."
"x ′ ), and assume we want to prove that the system does not reach a set of bad/unsafe states captured by the predicate bad(ì x). an inductive invariant i (ì s) that proves this property needs to satisfy the following three constraints:"
"for the two datasets of ground truth (g) and prediction (p). the diameter and volume of the la endocardium were evaluated to compare the potential measurement errors from the segmentation between the predictions and ground truths. the la diameter, measured in millimeters, was calculated by finding the maximum distance from the anterior to the posterior of the la endocardium. the la volume, measured in cm 3, was calculated by counting the total number of voxels within the endocardium and then scaling the sum by multiplying the original resolution of the lge-mri."
"where trans s captures logically the semantics of the snippet s in terms of how it affects the post-state of ì x. in the above, all three of the predicates i pre, i post and postfoo need to be synthesized. when a learner proposes concrete formulas for these, the verifier checking the above logical formula may find it to be invalid, and find concrete valuations v ì"
"algorithm 1 decision tree learner for horn samples 1: procedure decision-tree-horn input: a horn sample (x, c) output: a decision tree t consistent with c, if one exists."
"in order to evaluate the success of using rxnorm as the basis for a direct-capture medication history tool, as well as to understand if and how it could be used as the underlying data structure for such practical computing applications, we propose a four-factor framework of analysis. these factors are:"
"the sink node combines the sampling sets of all n-sensors, i.e., k i, \"i, to generate a complete sample set of event signal, i.e., k. then, the sink node reconstructs the event signal f s (t) using the complete sample set k. here, the reconstruction method of the signal is not in the scope of this paper. we only assume that the event signal can be reconstructed from the complete sample set of the event signal if a sufficient number of distinct samples can be taken and delivered to the sink by n-sensors."
"hence, the optimal rtc transmission probability, i.e., p opt, that minimizes the average formation time of a cluster, i.e., b c, can be given as"
"as such, an increasing need exists to look for alternate methods (averse to, e.g., post-hoc mapping) for capturing and transforming ehr medication data into rxnorm format. this includes new e-prescriptions as they are ordered (i.e., e-prescribing), as well as medications previously prescribed (i.e., medication history) [cit] . medication history in a clinical (not pharmacy) context includes such history (at least within some reasonable time frame, e.g., the past two years) for a patient prior to the current provider's treatment episode, often provided by the patient themselves. collection of such information is specified as part of meaningful use, and such data can be critically important for identification of proper treatment, adjusting baseline outcomes, and reducing medical errors due to drug interactions [cit] . for example, a patient suffering from major depression who was on citalopram 20 mg until 3 months prior to the start of treatment with a new provider may not represent a true baseline due to residual effects of the previous treatment [cit] . in order to evaluate such a patient, as well as to properly account for potential outcome improvement (or lack thereof), it is thus essential to know the patient's medication history. this sort of baseline adjustment would also be a necessity in any pay-forperformance paradigm."
"synthesizing inductive invariants, including loop invariants, pre/post contracts for functions, and rely-guarantee contracts for concurrent programs, is one of the most important problems in program verification. in deductive verification, this is often done manually by the verification engineer, and automating invariant synthesis can significantly reduce the burden of building verified software, allowing the engineer to focus on the more complex specification and design aspects of the code."
"we believe our results show that our extension of decision tree-based ice learners to horn-ice learners is quite efficient, and favorably compares with state-of-the-art tools for solving non-linear horn constraints. our technique is able to prove a large class of programs correct drawn from a variety of styles (sequential programs with and without recursion, concurrent programs) that result in non-linear horn clauses."
"proof. let us fix x, c, and u to be the inputs to the procedure, and let x ′, c ′, p, n, p ′ and n ′ be as described in the algorithm. it is clear that there exists an extension of u satisfying c if and only if c ′ is satisfiable. furthermore, the set of variables forced true by c with respect to u coincides with those forced true in c ′, less the variables in dom true (u). a similar claim holds for the variables forced to false."
hyper-parameter tuning of the dropout rates at the last layers of atrianet showed that 50% dropout provided the best balance between alleviating overfitting while still maintaining sufficient number of nodes in each layer for feature learning. this resulted in the 0.942 dice score. experiments showed that having no dropout produced the lowest dice score of 0.927 and having 75% dropout resulted in a dice score of 0.937 due to the excessive removal of parameters.
"node selection: breadth-first-search, depth-first-search, random selection, and selecting nodes with the maximum/minimum entropy. attribute selection: based on a new information gain metric that penalizes node splits that cut horn constraints; based on entropy for horn samples obtained by assigning probabilistic likelihood values to unlabeled datapoints using model counting."
"in microbiology studies, maximal growth rate and maximal biomass are probably the two best studied bacterial growth properties. in a nutshell, these physiological properties provide a rough reflection of how well a bacterium cell benefits from a particular set of nutrients. thus, they can be used to guide a myriad of applications. one such example, is the utilization of growth rate maximization as an objective in constraint-based reconstruction and analysis of metabolic networks."
"the necessary technical documents, templates and examples are fully described in the model blackbox public repository (https://github.com/averissimo/model_ blackbox), providing a starting point for users to create and implement their own interface-compliant models."
"2. n-sensor clustering phase: n-sensors form clusters according to their locational proximities with each other. heads of the formed n-sensor clusters are selected as g-sensors to collect and directly transmit the data packets, received from n-sensors, to the sink as will be detailed in section 4.2. 3. sampling probability determination phase: g-sensors, i.e., cluster heads of n-sensor clusters, determine the sampling and communication parameters of n-sensors to ensure that a sufficient number of samples can be timely delivered to the sink node. this phase is elaborated in section 4.3. 4. channel frequency selection phase: in order to avoid excessive interference among neighbor n-sensor clusters, neighbor clusters select different operating frequency channels used for intra-cluster communication of sensor nodes. t-sensors detects frequency overlapping among neighbor clusters that use the same frequency channel and inform the clusters about that. using this information, each cluster selects a different frequency channel in order to prevent excessive interference. this mechanism is explained in section 4.4. 5. signal reconstruction phase: the sink finally incorporates all samples of event signal, that are taken and delivered to the sink node, into a single sample set that is expected to include a sufficient number of samples that are needed for accurate reconstruction of the event signal. detailed operation in this phase is introduced in section 4.5. this phase completes the overall operation of hac algorithm."
"bgfit is developed using open-source frameworks and free libraries allowing for a high degree of flexibility and creating a modular system constituted by ruby on rails, mysql, octave, mathjax and google chart tools."
"extending decision tree learning to handle horn samples turns out to be non-trivial. when a decision tree algorithm reaches a node that it decides to make a leaf and label it true, in the ice learning setting it can simply propagate the labels across the implication constraints. however, it turns out that for horn constraints, this is much harder. assume there is a single invariant we are synthesizing and we have a horn sample (s 1 ∧ s 2 ) ⇒ s ′ and we decide to label s ′ false when building the decision tree. then we must later turn at least one of s 1 and s 2 to false. this choice makes the algorithms and propagation much more complex, and ensuring that the decision tree algorithm will always construct a correct decision tree (if one exists) and works in polynomial time becomes much harder. furthermore, statistical measures based on entropy for choosing attributes (to split each node) get more complicated as we have to decide on a more complex logical space of horn constraints between samples."
"similar to wasn, a major functionality of many biological organisms is the ability to autonomously maintain a relatively stable equilibrium state for the operation of vital functions [cit] . this functionality is called homeostasis, and it is the principal quality of an organism to conserve its autonomy. hence, this analogy between biological homeostasis and wasn gives inspiration to develop autonomous communication algorithm for wasn. the homeostasisinspired autonomous communication (hac) protocol devised for wasn is the main contribution of this work. the salient features of hac are:"
"if we were given (say by the user) inductive pre/post contracts for all functions, we can use ice learning to synthesize the required loop invariants, and would not need to deal with non-linear horn-ice samples. however, we assume a completely automated verification setting where such contracts are not given and need to be synthesized as well."
"finally, we would like to apply our invariant synthesis technique for particular practical domains, where domain-specific templates involving complex atomic predicates can be used to synthesize invariants (much like gpuverify [cit] ] does for learning conjunctive invariants to prove gpu programs race-free)."
"hac algorithm has several unique features which allow sensor nodes to collaboratively and effectively communicate the event information based on the principles of biological homeostasis as described in section 3. hac takes the spectral characteristics of the event signal into consideration in determination of the sampling and communication parameters used by sensor nodes. based on the spectral bandwidth of the event signal estimated by the n-sensor, hac enables g-sensors to determine the number of samples which must be delivered in order to enable accurate reconstruction at the sink. furthermore, it also provides significant energy conservation by means of an efficient sampling scheme named irregular sampling [cit] . in this scheme, each n-sensor takes samples from the event signal with a probability (a) called sampling probability while it samples the event signal with sampling frequency f. a portion of irregularly sampled event signal is illustrated in fig. 2 . since the irregular sampling scheme decreases the number of samples transmitted over the network by a sensor node, it decreases excessive traffic load which may cause possible packet losses, and therefore, it provides more accurate reconstruction at the sink with significant energy conservation [cit] . the overall hac operation is composed of five distinct phases as briefly introduced below:"
lge-mri. fig. 4 illustrates the segmentation results for the la epicardium and endocardium by atrianet compared with the ground truth for selected slices at the same depth for a test 3d lge-mri. the results shown are representative of the errors seen in other test lge-mris. the relative depth of each slice from the bottom of the lge-mri scan is provided in millimeters.
"the viscous mixture was then poured into ice cold water and the resulting solid (18 g, m.p. 180 °c) was crystallized with etoh as shinning crystal (7 g), it was characterized as 7-hydroxy-4-methyl coumarin by comparison with authentic sample."
"all the input data and results, such as the time series, estimated parameters, model simulations and charts, are available for direct download to further analysis."
"since t-sensors are also in the communication range of each other, they first determine two different frequency channel ids and sends these ids to the cluster heads. if the cluster heads accept these channels, they immediately switches to these new channels. even if one of the cluster heads does not accept the new frequency channels, t-sensors find two new frequency channel ids to send the cluster heads. the acceptance of the newly proposed frequency channels depends on whether or not cluster heads already have some neighbor clusters that use these channels. if such a situation is not arisen, the proposed frequency channels are immediately accepted by the cluster heads for the intra-cluster communication. moreover, once a cluster head switches to a proposed frequency channel, it does not change this channel anymore. this mechanism prevents a cluster head to uncertainly oscillate different frequency channels."
"bgfit is designed as a parameter estimation platform for any type of two-dimensional data. despite being described in the context of cell growth data, the application can easily be used in other areas with different dynamic and algebraic models, such as physical chemistry and econometrics."
"verification using owicki-gries proof rules requires adequate invariants at each program point in each thread. in comparison, rely-guarantee additionally requires two-state invariants for each thread for the rely/guarantee conditions. these additional invariants make learning for rely-guarantee proofs more difficult. nonetheless, our tool successfully learned invariants for all of these programs in reasonable time, with most verification tasks finishing in less than 10 s. two of the 12 programs (16.6 %) required disjunctive invariants."
"the remainder of the paper is organized as follows. in section 2, the network model and assumptions used in hac protocol operation are introduced. in section 3, biological homeostasis is reviewed and then, a connection between biological homeostasis and wasn is introduced. hac algorithm is introduced in section 4 based on the analogies between wasn and homeostasis given in section 3. in section 5, performance evaluation results of hac are presented. finally, concluding remarks are given in section 6."
"a trial fibrillation (af), leading to an irregular and rapid heart rate, is the most common sustained heart rhythm disturbance. af is associated with substantial morbidity and mortality, causing 1 out of 5 strokes in people aged over 60 years [cit] . the current overall prevalence of af is about 2% in industrialized countries and is projected to more than double in the following couple of decades. current clinical treatments for af perform poorly due to a lack of basic understanding of the underlying atrial anatomical structure, which directly sustains af in the human atria [cit] ."
"hence, it is relevant for experimentalists to extract these key parameters from curves in order to characterize cell and tissue physiology, such as maximum growth rates, lag phase and asymptotic maximum od. furthermore, it is also interesting to be able to compare fittings obtained with different models. due to the development of high-throughput techniques, the amount of data being generated is growing fast, hindering their management in large collaborative projects and also hampering model identification procedures."
"(2) we show that algorithm guarantees that a decision tree consistent with all samples is constructed, provided there exists one. an incremental maintenance of horn constraints during tree growth followed by an amortized analysis over the construction of the tree gives us an efficient algorithm. (3) we show that we can use our learning algorithm to learn over an infinite countable set of predicates p, and we can ensure learning is complete (i.e., that will find an invariant if one is expressible using the predicates p). (4) an implementation of our algorithm and an automated verification tool built with our algorithm over the boogie framework for synthesizing invariants. we evaluate our algorithm for finding loop invariants and summaries for sequential programs and also rely-guarantee contracts in concurrent programs."
"since atrial tissue is continuous and smooth, the raw segmentation output from atrianet was passed through a 3d gaussian filter to enhance the segmented tissue mask. isolated mask islands were removed automatically, keeping only the largest connected tissue in 3d as the final la segmentation. thresholding was applied to restrict the pixels to binary values, ensuring the atrial geometry was smooth and sharp."
"the popular medical image segmentation architectures [cit] and the recent state-of-the-art image classification architectures [cit] adapted for segmentation were all outperformed by atrianet in both dice and hd evaluation. the single path version of atrianet, vggnet, was the second-best performing architecture, and showed the use of an additional pathway in atrianet improved the performance by a dice score of 0.078. the evaluation metrics indicated that the use of a dual pathway in atrianet resulted in a significantly more effective architecture for performing accurate la segmentation compared to other existing neural networks."
"proof. at each iteration step, the algorithm maintains the invariant that the partial valuation u is an extension of the partial valuation u t induced by the current (partial) decision tree t, and is consistent with c. this is because each labelling step is first checked by a call to the horn-sat algorithm, and subsequently the horn-forced procedure correctly identifies the set of forced variables, which are then used to update u. it follows that if the algorithm terminates successfully in line 15, then u t is a full valuation which coincides with u, and hence satisfies c. the only way the algorithm can terminate unsuccessfully is in line 11 or line 14. the first case is ruled out since if n.dat is singleton, and by assumption u t is consistent with c, we must be able to label the single data point with either true or false in a way that is consistent with c. the second case is ruled out, since under the assumption of separability the select-attribute procedure will always return a non-trivial attribute (see sec. 5)."
"we select nodes in a breadth-first search (bfs) order for building the decision tree. bfs ordering ensures that while learning multiple invariant annotations, the subtree for all invariants gets constructed simultaneously. in comparison, in depth-first ordering of the nodes, subtrees for the multiple invariants are constructed one after the other. in this case, learning a simple invariant for an annotation (e.g., true) usually forces the invariant for a different annotation to become very complex."
"in fig. 8, the effect of the average number of clusters in the network, i.e., h, on the performance of hac is shown. n d is shown with h in fig. 8a . while h increases from 2 to 10, which increases the number of n-sensors in the network 10 times, n d slightly increases over 5000 that is the event signal bandwidth in hertz. similarly, in fig. 8d, the total number of samples taken by all n-sensors within a second also remain almost the same level, which gains significant energy conservation over the network. this provides a great stability that remains the data traffic in an almost the same level. this can be achieved by hac protocol operation reducing the sampling probability a as shown in fig. 8c."
"the dice score for each test patient data is shown in fig. 3 . the results show that atrianet was very consistent in reconstructing both the la epicardium and endocardium as seen from the small variation of the dice scores across different patients. the standard deviation of the dice scores for both epicardium and endocardium segmentation was 0.014. table. vii shows the predicted and ground truth measurements for the la diameter and volume, as well as their absolute and relative errors. the predicted masks were accurate within 1.59 mm and 4.01 cm 3 of the ground truths on average for the diameter and volume measurements respectively. overall, the segmentations for the pre-ablation patients were more accurate as seen from the higher dice scores and lower relative errors of the estimated la dimensions."
"the available templates offer two approaches implemented in octave and matlab's numerical computing environments, either as a script for octave, making it possible to deploy the modeling extensions without any licensing issues, or as a standalone application for matlab, taking advantage of sbtoolbox2 [cit] functions."
"while many of the issues described elsewhere were similar to those presented here -e.g., coverage, flexibility, speed -some of the requirements for such an ehr application proved to be distinct, there are multiple potential avenues for extension of this work, particularly in regard to search optimization and filtering as well as integrating drug classification schemes into rxnorm. the search results could be improved via the use of web-search-like indexing and filtering, similar to commonly used information retrieval techniques utilizing click-throughs and page ranking [cit] . such algorithmic approaches to search optimization can also incorporate user profiling to personalize medication search results based on individual user behavior or behavior of users similar to the current individual [cit] . in other words, the search results that a particular user (patient or provider) sees when entering medication information may be ranked or organized based on other medications they have entered during the current session or based on past user behavior. for example, medications that are commonly prescribed in \"clusters\" for co-occurring disorders may be pushed to the top of the list if the patient is identified as fitting that profile or if one medicine from such a cluster is entered. additionally, search functionality could be enhanced through the use of \"sounds like\" technology, such as metaphone, that allows users to phonetically spell medication names and find them via search even if they do not know the correct spelling [cit] . on other fronts, efforts to classify rxnorm data into medication categories hold great promise for research, clinical decision support, and business intelligence purposes, whereas"
"in this section, the basic principles of biological homeostasis are first reviewed. then, the analogies between biological homeostasis and wasn is presented, based on which hac protocol is introduced."
one avenue where black-box learning engines for invariants have been particularly useful is in the context of programs where checking validity of verification conditions is itself undecidable [cit] ]. it would be interesting to extend our technique to such domains.
"the title compound, (i), fig. 1, has one chiral carbon atom (the c11 atom). both enantiomers are present in the crystal structure, forming a racemate."
(1) the first suite consists of 52 recursive programs from the software verification competition [cit] ]. we compared horn-dt-boogie on this benchmark suite with ultimate automizer [cit] recursive programs track.
"this approach for implementation and visual display of rxnorm information is similar yet different from what has been proposed elsewhere [cit] particularly in regards to utilizing both restructured production tables of rxnorm data and optimized search functionality. in this case, we were essentially re-structuring rxnorm data into specialized production tables to allow for improvements in speed and flexibility. this is in essence the same as the general design differences necessitated in a transactional (online transaction processing -oltp) production system relative to an online analytical processing (olap) system, such as a data warehouse."
"a crucial step in our decision tree algorithm when we decide to label a node as true or false is to compute the set of variables forced to be true or false respectively. in this section we describe an efficient algorithm to carry out this step. our algorithm is an adaptation of the łpebblingž algorithm of dowling and gallier [cit] for checking satisfiability of horn formulas, to additionally find the variables that are forced to be true or false. we begin with a conceptual extension of the pebbling algorithm in section 4.1 and describe in section 4.2 how this algorithm can be implemented more efficiently."
"hac aims to provide real-time communication of the wide-band event signal. hac regulates its communication parameters according to the spectral characteristics of the event signal. hac does not need any feedback message from the sink node. hac allows the self-organization capability of the wasn, inspired by biological homeostasis, to maintain the sensor network in a relatively stable operating state in which the sensor nodes autonomously, reliably and, yet energy-efficiently communicate wide-band event signals to the sink."
"the search functionality utilizes the autocomplete extender from the ajax control toolkit for asp.net, set up to suggest possible complete medication names once the user has entered at least two letters, displaying only the top twelve results. string matching is unanchored and can match any segment of the medication name, including beginning, middle, or end. the end user can continue typing to refine the search, with the results automatically filtered/updated with each keystroke. medication name information was cached on the server side with a 2 hour expiration policy (requiring cached data to be deleted or updated every two hours). a 70 millisecond delay was set on the search box text for updating results, which allows the results to filter seamlessly as the user types."
"is that the quality of the data can be compromised when coding back into standard rxnorm common dose forms due to missing or non-standard data. however, the capability exists to reconstruct rxnorm format from the component parts via sql query, and at a minimum the medication name can still be mapped using the cui/aui identifiers. importantly, these identifiers are stored and associated with specific, controlled terms at different levels of the hierarchy, so it does not involve evaluating text after the fact. the data are mapped as soon as collected. in other words, even if some elements of the medication data are omitted, we can always map to some level of the original source (rxnorm/umls)."
"note the query returned both the \"common_form\" field for selection display as well as fields for populating individual fields (e.g., dose_amt and dose_units) and returning a unique key for populating the backend tables after a selection was made. selection of a common dosage, as seen in figure 3, would then populate the remaining appropriate fields of the overall screen (medication, dosage). the end user can still manually edit any field, and an option always exists in the modal popup if the exact dosage is unknown. additionally, an \"other units\" dummy option is provided for all medications for situations when the standard dose units are not inclusive of some atypical dosing regimen, which can occur with some less commonly used medications. generally, such an approach allows for flexibility, at least in terms of dosing. the trade-off"
"as evidenced by the work presented here, rxnorm is a suitable terminology for capturing medication history in live ehrs. a restructured dataset of rxnorm satisfies the necessary requirements."
"as seen from fig. 4, at the depth of 27.5 mm to 40 mm (middle sections of the 3d atria), atrianet produced more accurate reconstructions of the la geometry. atrianet was able to successfully capture the curvature of the la tissue in detail, while also showing a clear gap between the epicardium and endocardium denoting the la wall tissue. the pulmonary vein regions were the main sources of error within these slices, as the segmentation was less accurate as seen from the bottom left part of la at 33.75 mm and the bottom right part of the la at 40.00 mm, where the segmentation was non-smooth and the thickness of the vein was underestimated. lge-mri test dataset. the metrics of all single pathway networks when using the local resolution as input are shown. benchmarking experiments on the architectures using the global resolution (not shown) resulted in substantially lower performances for all single-path approaches (∼0.1 lower dice score) compared with using the local resolution. the superiority of the local resolution as input for single pathway networks was due to the ability to achieve sharp segmentations given the high resolution of the local patches, while the global resolution alone is inferior as the image is low in resolution due to pooling."
"so as to clutter the paper not too much, we here only describe the best performing combination of strategies in detail. the experiments reported in section 6 have been conducted with this combination."
"step 1. calculate available bandwidth per node in the network. in this improvised range calculation algorithm, adaptive parameters and other calculation will be made only on the suspicious data flows in the traffic. according to this algorithm, initially for the first data flow entry in the network whole parameters for the calculation of range (rl-ru) and making the decision will be computed. but for the next data flows, it will check for the current active bandwidth i.e. if the current data stream bandwidth is less than the available bandwidth per node then there is no need to recalculate all the parameters as there are no chances of attacks like dos (denial of service), ddos etc. due to the fact that bandwidth consumption is well within the legitimate range of available bandwidth. this mechanism implements the concept of fast switching and fast forwards the data flows which in-turn reduces the overhead of recalculation over the controller and increases the performance of our network. however, if the bandwidth utilization by the current flow is higher than the available bandwidth, only then adaptive parameters will be recalculated dynamically and values will be updated for the anomaly detection for the next data flows to maintain accuracy for detection."
"for accuracy of anomaly detection using dynamic threshold range (rl is the lower range and ru is the upper range) calculation algorithm, the graphs for detection of anomalies with respect to prefix aggregation mask length are generated. these graph are obtained using the dynamic rule updating algorithm and also upon implementing threshold range calculation algorithm for different simulation times using different capturing intervals. these graphs are compared with the graphs obtained by using existing static threshold range calculation to the graphs obtained using new dynamic threshold range calculation algorithm for dynamic rule update algorithm."
"the rest of paper organized as follows: in ii, some work related to anomaly detection by other researchers is given. in iii, the methodology used for our algorithm is defined for accurate anomaly detection. in iv, implementation details and gathered statistics is shown. v represents the accuracy of the results obtained using our algorithm. in vi, the effect on performance is evaluated. finally in vii, we conclude this research work by discussing the importance of adaptive technique and future work on its further improvement."
"the performance of the network is the most important aspect to be taken care of, as networks need to perform better for proper and real time data delivery. packet loss must need to be minimum for real-time data flows while monitoring the statistics for making the decision on the basis of these statistics. in this algorithm, adaptive parameters are re-calculated each time for new data flow entry, this can results into the greater overhead over the controller. so, to overcome this problem, it is required to implement such scenario that only mandatory or suspicious data flows can be monitored and values of range and other adaptive or dynamic parameter must need to be recalculated in case of suspicious data flows only."
"in this algorithm, a threshold range of values for making the decision for dynamic rule updating algorithm is calculated by utilizing the bandwidth efficiently. these values of range rl and ru will be re-calculated each time a new user will connect to the network or whenever any user leave the network or isolated from the network. initially available total network bandwidth is assigned to the network and minimum required per node is set. after this, this find out the total number of active data streams inside the network. available network bandwidth per node is calculated by deducting the essential data header field bandwidth form total allocated bandwidth and then bandwidth for each current data flow is calculated. according to these obtained values, the value of the dynamic threshold range rl and ru are updated by comparing the currently active data stream bandwidth for each data flow so that internal data flow fluctuations can be ignored and allows them to safely transfer data without raising alarms for detection of anomaly in case of legitimate users. this mechanism helps in detecting anomalies accurately inside the network. step 1. some total bandwidth is assigned to the network and also set a minimum bandwidth required for each node."
"with the emergence of the sdn's technology, the concept of programmable networks has again come into the picture. network flow path decisions are made by the controller present in the control plane of sdn. sdn can improve the functionality of the networks and change the way they operate. also, openflow is treated as a new revolutionary idea in networking that provide secure transfer of control flows [cit] . sdn is currently capturing a greater amount of academia and industry attraction. a number of network operators, vendors, and service providers had recently laid down an industrial driven organization named \"open network foundation\" [cit] to promote sdn. security of sdn is the most important area of concern for its rapid deployment in today's fast growing enterprises. with the totally changed security architecture, it is a challenge to learn the whole new security paradigms and implement them. sdn open interfaces and known protocols also provide opportunities to invaders for various attacks. including with its benefits of higher innovation, separation of the control plane from the forwarding plane also being a security problem for various enterprises using sdn. all three layers of sdn can be targeted for attack due to that extra level of complexity added to it. centralized sdn controller still provides us the whole view of the network, that view can be used to easily apply various security policies and intrusion detection system inside the network for making it more secure. using sdn it is easier to monitor the network traffic and make the dynamic decision, according to the traffic patterns. but due to new encapsulation and overlying infrastructure techniques for hiding the data-flows, most of the existing security tools are unable to monitor and inspect the sdn traffic. various new monitoring policies and mechanisms need to be implemented on the network traffic while maintaining the performance of the network. virtualization and abstraction hide the underlying infrastructure so it is harder for an attacker to attack the forwarding elements directly because they are not exposed directly to the attacker."
"in this work, it further improvises the range calculation algorithm to the higher extent to maintain the performance of the network. the improvised algorithm is given as algorithm 2."
"the performance of this algorithm is evaluated by generating the similar graphs on implementing the improvised range calculation over the above algorithm1. similar graphs generated by implementing algorithm 2 upon algorithm 1 are shown in figure 6.1 and 6.2 at simulation times 30s and 90s respectively. the similarity of these graphs with the improved accurate detection graphs represent that we had achieved higher performance of the network while keeping the accuracy of the anomaly detection intact. vii. conclusion and future work security of modular sdn architecture relies on ensuring the integrity of network traffic. to ensure the integrity of the data, it is required to find out various malicious flow entries present in the network. hence, it is required to implement efficient anomaly detection algorithms. but on implementing such algorithm on the network, it is also required to manage the monitoring overhead to keep performance matches. for that, an adaptive rule update algorithm is provided in this work, with greater efficiency and reduced complexity. its dynamic nature provides zooming of some flow aggregates on expanding them which helps in applying finer granularity rules on our network traffic and detect anomalies with greater accuracy. the existing algorithm also considers fluctuations of data as anomalies even for legitimate users and raise the alarm. however, a dynamic threshold range calculation algorithm is provided in this work, which dynamically update the threshold range values according to the bandwidth utilization and accurately make decisions of anomaly detection. results show that upon implementing this improved algorithm, the accuracy of anomaly detection increases, and performance is also improved using fast switching concept by recalculation of dynamic parameters in case of suspicious flows only. in future, other certain parameters like header space divider and sampling rate can be considered for further improvement of this technique. also, trusted users like internal server, managerial data flows from management servers etc. can be added to a trusted ip addresses list on the basis of some parameters. so that, they can also be fast forwarded even without applying any monitoring policies on internal data so that processing speed can be further increased."
"however, this range of values considers the fluctuations of data as anomalies when applied on the aggregated data as used in the existing algorithm and causes a problem for valid users. so using this approach anomaly count comes out to be much higher than the original anomalies present in the network. to overcome such problem, a new paradigm is implemented which replaces this method for calculating the threshold range and helps in accurate detection of anomalies. in this paradigm, this dynamic threshold range values can be calculated using the efficient bandwidth utilization. this approach is represented in algorithm 1 and implemented for improving the accuracy of the adaptive anomaly detection method."
"in this work, we proposed a developed u-net architecture for brain images segmentation intended for alzheimer disease and brain damage detection. we found-out that u-net improves clearly state-of-the-art. the benefits of our method were demonstrated in comparison to different relevant methods. our main contribution is providing an automatic and exact alzheimer detection using an advanced full neural network on a 2.5d context to ensure the performance of segmentation whatever the used system, by reducing memory costs while processing 3d images. moreover, our method performs brain segmentation then classification for ad detection. to our best knowledge, our proposed cad system is the first one 2.5d mri analysis for alzheimer's disease detection using such learning techniques. our developed u-net can automatically segment a 2.5d mri image and offer an accurate analysis of brain structures. the network to segment brain images was trained from scratch, for that we expect that it will be applicable to many brain analyses especially 2.5d segmentation tasks."
"ere are also some other methods performed by calculating inverse kinematics of exoskeleton to implement accurate position and trajectory planning [cit] . however, these methods are only for series exoskeleton configurations. in order to achieve friendly human-exoskeleton interaction, some force control methods [cit] are used, such as the admittance control [cit] . unfortunately, these methods do not establish and combine the dynamics model of exoskeletons. in addition, some adaptive robust control algorithms [cit] are also applied in exoskeleton control, but the adaptive parameters are usually hard to be tuned and identified. although some force control algorithms, such as the impedance control [cit], establish the dynamics model of the exoskeleton and apply them to the control, these methods are usually used for series exoskeleton and cannot be applied directly to our exoskeleton with parallel actuated joints. erefore, for obtaining accurate motion control and good dynamic performance of our parallel exoskeleton, we establish the dynamics model of exoskeleton by simplifying it as a serial equivalence configuration firstly in this paper. en, a conversion algorithm is proposed to map the motor motion to the joint motion. afterwards, a torque control method combined with the conversion algorithm and position decoupling is designed for our exoskeleton. for verifying the effectiveness of the proposed algorithms, a polynomial trajectory with fifth interpolation is simulated. finally, the simulation results prove the validity of the proposed algorithms. e main contributions of this paper are as follows: (1) by converting motion relationship between motors and joints of the designed exoskeleton, the parallel exoskeleton is equivalently simplified to a serial exoskeleton to establish inverse dynamics; (2) a novel parallel upper limb rehabilitation exoskeleton with modular structure was designed; (3) a trajectory tracking control method for the exoskeleton was designed based on the proposed conversion algorithm of equivalent simplified inverse dynamics. e paper is organized as follows: section 2 is the mechanical design of the 6-dof upper limb rehabilitation exoskeleton based on parallel actuated joints. section 3 introduces conversion algorithm which maps motor motion to joint motion. section 4 discusses the torque controller design, system stability analysis, and trajectory tracking simulation results. section 5 discusses the simulation results. at last, conclusion is shown in section 6."
"in contrast to all the previous solutions, the proposed method takes into consideration the incorporation of 3d spatial information in a 2.5d context using a deep convolutional network to ensure segmentation tasks. our choice for u-net is based on the need to boost the segmentation accuracy and the performance of image processing while studying a sensitive disease such as alzheimer."
"stroke is a neurological disease with a high prevalence and often leads to disability. one major symptom after stoke is the decrease in the function of patients' upper extremities, which seriously affects their daily life [cit] . e most effective therapy plan is the rehabilitation training for stroke patients [cit] . however, the traditional treatment process is carried out by professional therapists, which is time-consuming and expensive."
"since € e t _ e � _ e t € e and _ e t k p e � e t k p _ e, equation (15) can be rewritten as"
"in order to better observe the driving torque of each motor of the exoskeleton after mapping motor motion to joint motion, this section designs a controller shown in figure 3 based on joint angle decoupling introduced in detail in our previous paper [cit] and conversion algorithm with the computational torque method for parallely connected exoskeleton robots and performs trajectory tracking control simulation based on the planned motor trajectory. e control law of the controller we designed is shown below:"
"the point cloud c defines a set of points in a 3d volume like the example illustrated in fig. 1, c can be written by c (xs, ys, 3), where s is the spatial sampling step of the system and (x, y) are positive integers belonging to ℤ2. therefore, the transformation of c into 2.5 projections can be performed without loss of data. each point c (xs, ys, 3) will be converted to a pixel p (x, y). indeed, the value of the gray level of this pixel p will be z -zmin, where z min is the minimum depth of c. next, in volume 2.5, the value of each pixel corresponds to the vertex of the point upper of zmin. in contrast, the 2.5d transformation in 3d corresponds to a simple conversion of the acquisition parameters to assemble the adjacent images and form the 3d volume."
"the cnn network consists of different layers able to analyze and capture the quantity of the different structures present in the image. the extension of cnns has contributed to the development of u-net that allows the segmentation of biomedical images. the u-net is based entirely on a convolutional network, with a modified u-shaped architecture, which can perform the required tasks with fewer training images to produce more accurate segmentation swiftly using a gpu [cit] . as for medical imaging, it is evident that u-net architecture was introduced to promote precision and objects' localization in microscopic structures. u-net combines a fully convolutional network [cit] with a deconvolutional network [cit] . consequently, the resolution of the output can be ensured thanks to the number of features in the up-sampling phase, which ensures the propagation of context information to higher resolution layers. this paper describes an improved cad system for alzheimer detection using 2.5d modalities. the ultimate contribution of our work is to provide a different alzheimer detection process from several perspectives. first, the use of a robust neural network that can process the mri scans on a 2.5d spatial context to improve the performance of segmentation and so that the whole system. second, our method simultaneously performs brain structure segmentation for lesion detection and ad detection. third, we tested our work on public data. finally, we developed a novel 2.5d fully deep convoluted segmentation based on the u-net architecture to increase the results accuracy. this paper is ordered as follow: section 2 states the literature review regarding alzheimer detection techniques. section 3 explains our proposed technique in detail. section 4 illustrates the experimental results and section 5 presents the conclusion of the work."
"a newly designed modular upper limb rehabilitation exoskeleton with parallel actuated joints is described in this paper. e three modules represent the three joints of the human upper limb, which can achieve rehabilitation training for any joint of the left arm or right arm of the human upper limb. to obtain the relationship between motor torque and joint torque of exoskeleton, a decoupling jacobian matrix is calculated to map the joint torque to the motor torque. en, for solving the joint torque, a dynamics model of a serial equivalence configuration of the exoskeleton is established. afterwards, a torque controller used to control our exoskeleton is designed, which especially contains the proposed conversion algorithm and joint position decoupling. e system stability is also analysed. for proving the validity of the proposed algorithms, a trajectory tracking control is simulated. from the simulation results, we find the trajectory tracking performance is good, which shows the proposed algorithms are valid."
"in medical imaging, some tasks remain complicated such as labeling the learning images. this operation can be expensive and involve delicate and ambiguous decisions. for instance, in brain image processing, annotating the locations and scales of objects is often difficult, hence the need for a consistent way that annotates all the objects segment and detects their natures. www.ijacsa.thesai.org"
"in our process, firstly the down-sampling and training are made. then the up-sampling outputs are classified. visually the results indicate that our proposal can segment brain images to diagnose ad; the task that requires physician's interactions in the general clinical routine; by offering blended annotation on the studied images. moreover, our quantitative results reveal further efficiency with the additional classification layer. this layer facilitates ad detection with less additional efforts."
"as shown in fig. 3, our method involves of a fully cnn containing two main parts: the u-net to segment the brain images and the classification layer to classify the detected areas. the input image will first go through the contracting path to generate feature maps, then the expanding path to generate the output segmented image. after that, the detected objects in the segmented images would be classified to detect their nature and improve the network model's adaptability."
"in order to observe the effect of the disturbance, perturbation force of 5 nm is applied to each joint of the exoskeleton at t � 2 s on the basis of simultaneous movement of the shoulder joint and the elbow joint. after adding the disturbance, the actual motor torque curve is shown in figure 8 . e trajectory tracking effect and rmse of the shoulder joint and elbow joint motors are shown in figure 9 and table 2 . once the disturbance is present, the torque of the motor fluctuates. e actual trajectory also deviates from the expected trajectory. e maximum position tracking rmse of motors reached 0.6696 rad, and the minimum position tracking rmse of motors reached 0.0795 rad."
"each module is a differential mechanism, and therefore, the coupled relationship exists between two degrees of freedom for each joint. if a set of desired exoskeleton angular velocities are known, the required motor angular velocities can be obtained by"
"e exoskeleton robot shown in figure 1 for rehabilitation training of upper limb uses the cable-actuated parallel mechanism to achieve 6-dof movement of the human upper limb. we approximated the shoulder as a spherical joint, the elbow as a rotary joint, and the wrist as a 2-dof nonspherical joint."
"our deep u-net network has been able to obtain competitive results in detecting damaged regions of the brain and defining the presence of ad. the proposed method generates a model of automatic segmentation of brain lesions as well as the diagnosis of patient-specific ad, which can potentially facilitate different clinical tasks such as diagnosis, treatment planning, and patient monitoring."
"the proposed network offers accurate results in less time (less than 2 hours for training and testing). these accomplishments are superiors than all the tested methods that can take more hours or even days for brain images analysis. our method is an appropriate automatic solution for brain image segmentation and ad detection. yet, the current work still has some limitations. first, our method was evaluated to segment 2.5d images using a t1 longitudinal dataset, but running our method on a t2 dataset can produce a more objective evaluation. secondly, the evolution of neural networks continues to increase, so the addition of new parameters can improve the performed tasks by our network."
"in this paper, we have proposed and developed a cad system using deep convolutional networks to analyze brain damage and detect alzheimer's disease by segmenting 2.5d images. as long as we have added a classification layer in our network, we need accurate measures to calculate the proportional values between the studied samples. for this purpose, we considered three metrics: accuracy, sensitivity, and specificity [cit] . these metrics are defined as:"
"where g is the gibbs function, and the time derivative is denoted by two dots. let the dynamics of the studied system satisfies (1) and is described by equations (3)."
"the automated computer-aided diagnosis of alzheimer remains significantly a challenging task that requires advanced technical practices such as deep learning (dl) algorithms. recently, deep learning has shown promising methodologies with great progress in the segmentation, identification, and classification of image patterns [cit] . among the most widely used dl architectures, convolutional neural networks (cnns) perform machine learning tasks without manual functions [cit] . cnn has a strong ability to solve complex vision problems, such as classification [cit], segmentation [cit] and object detection [cit] ."
"according to the gauss principle at every time t the dynamic system moves in such a way that constraint [cit] corresponding to actual way is minimized by the accelerations s q, so that, 0 z g (2) where s m is the material point mass; s q -is the coordinate of the material point relative to a static cartesian coordinate system; s q is the resultant force applied to the material point; n is a number of degrees of freedom of the dynamic system. from (2) the appell equations [cit] follow in the form:"
"as illustrates the example in fig. 5 the results demonstrate the efficacy of our method. another important outcome is the number of wrong region detection without the classification stage. the methods in fig. 5(b) and fig. 5(d) show good results, however they can segment other objects different from the zones attacked by the alzheimer which can create a kind of confusion during the diagnosis. in fig. 5(c) the method ensures the segmentation of the brain by determining different objects other than lesions or ad. this phenomenon is mainly due to the fact that even the semantic segmentation is not only sufficient to diagnose ad. that's why it's preferable to combine several techniques to achieve higher accuracy."
in this work it is shown that the application of combined-maximum principle methodology for finding a required condition for the expanded functional minimum as a convolution product of the objective criterion and the gaussian constraint [cit] provides a synthesis of quasioptimal controls. in contrast to known results this leads to a solvable boundary problem and doesn't require to build the synthesis function.
"in the mechanical design, each joint exists as an independent module, so the exoskeleton has three modules representing the three joints of the human upper limb, that is, the shoulder joint, the elbow joint, and the wrist joint. each module is a differential mechanism and has two coupled motors to achieve two dofs of the human upper limb. e shoulder joint module achieves shoulder abduction/adduction freedom and flexion/extension freedom. e elbow joint module achieves shoulder internal/external rotation and elbow flexion/extension. e wrist joint module achieves forearm supination/pronation and wrist flexion/ extension. e three modules have the same structural design principle, so the shoulder joint module show in figure 2 is used as an example to analyse joint motion. s is the shoulder joint rotation center. e main driving mechanism consists of two small straight wheels (ssw1 and ssw2), two large straight circles (ssc1 and ssc2), two second-order shafts (sso1 and sso2), and two small wheels (sw1 and sw2) each with a second-order shaft."
is the given small finite time interval; 0 ! 't . then the full variation of the functional has the following form: the relations on the trajectory ends are the transversality conditions: the developing of this equation should be performed separately for each specific case of synthesis problem.
"the solution (22) unlike the known solution (24) [cit] did not contain singularities at the finite time. as a result, the discontinuity of the control generalized force at the final time is absent. this allows to use it in practice without additional transformation."
"since h(q) is the same symmetric and positive-definite matrix as m(q) [cit], h(q) is invertible. for this reason, equation (11) can be simplified as"
"our method offers an ad diagnosis framework that extracts the characteristics of brain images, performs correct lesion detection, classifies their natures, and indicates the presence of ad. the operations performed are designed to maximize the content of the information from different views. the details of this process are related to the following concepts:"
"for verifying the performance of the algorithm proposed above, elbow flexion/extension motion is simulated below. at the initial moment, the exoskeleton flexion angle is zero; that is, from the visual point of view, the exoskeleton arm naturally hangs down, and the three joints of the arm form a straight line perpendicular to the earth. en the flexion of the elbow joint is performed, during which the shoulder joint and upper arm remained stationary, and the exoskeleton arm is stopped at the position where the flexion angle of the elbow joint is 90°. figure 4 shows the actual motor torque curve obtained by the proposed conversion algorithm. in the elbow joint, there are two situations that need to be noticed; that is, when the two motors are at the same speed with the same rotation direction, the exoskeleton can achieve the internal/external rotation movement of the upper arm; when the two motors are at the same speed with the opposite rotation direction, the exoskeleton can achieve the flexion/extension of the elbow joint. erefore, as the angle of elbow flexion increases, the more gravity the elbow joint needs to overcome, and the torque of two motors of the elbow gradually increases in the opposite direction."
"in our system, the application of cnns consists of convoluting a 2.5d image with the kernel in order to extract the maps of existing characteristics in the studied images. in fact, each map is connected to the previous layer of the network through the appropriate weights during the training to improve the input. the same kernel is converted on all the data of the image which leads to the use of several functions as will be explained in the following subsections. the proposed process is summarized in fig. 2 . we used the u-net architecture for each view of the 2.5d brain mri after parsing them into transversal views. the rationale behind the choice of this architecture is based on the sensitivity of the zones studied, and the limitation of memory facing the performed operations. consequently, we have been able to optimize the formation of convolutional networks by limiting the problem of mri images to a 2.5d domain."
"with the advent of computer assisted diagnosis systems, especially these, relying on deep learning techniques, the cnns [cit] are frequently used to solve complicated problems from both computer vision and medical imaging field. indeed, these models learn to establish a list of image characteristics. since cnns are much used for mri images studies to detect or predict ad, it is necessary to develop methods ensuring the learning from a large-scale training package [cit] . the robustness of the capsule or capsnets [cit] networks allows a fast, precise and thorough learning of data images. capsnet requires less data for training by ensuring shorter learning curve [cit] . other methods have used different auto-encoders or cnn 3d to detect ad [cit] ."
"when the two motors are at the same speed with the same rotation direction, the exoskeleton can achieve the flexion/extension of the shoulder joint. when the two motors are at the same speed with the opposite rotation direction, the exoskeleton can achieve the abduction/adduction movement of the shoulder joint. when the rotation speed of motor 1 is faster than that of motor 2, then the exoskeleton can achieve the abduction and flexion or adduction and extension of the shoulder joint. when the rotation speed of motor 2 is faster than that of motor 1, then the exoskeleton can achieve the abduction and extension or adduction and flexion movement of the shoulder joint. for more structural design details, one can refer to our previous work [cit] ."
"the estimation of efficiency of the suggested solution is performed on the basis of comparison with the quasioptimal law of \"soft\" terminal control [cit] the results of the mathematical simulation are shown in the fig, 1, where the number 1 denotes the phase trajectory of the system (19) with the right part (22), and the number 2 denotes the phase trajectory of the system (19) with the right part (24). fig, 2 presents the structure of the controlling generalized forces. there are the following notations: 1 -the control (22), 2 -the control (24). it can be seen that singularity (24) at the end time leads to a sharp increase of control force in opposite to (22). the results of modeling are illustrated on fig. 3 . the solution based on the pontryagin's maximum principle allow getting these results. this confirms the validity of the developed method."
"in the shoulder joint, the reduction ratios from the output shaft of the motor 1 reducer and motor 2 reducer to the large straight circle corresponding to those motors are i 1 and i 2, respectively. and c 1 and c 2 represent the reduction ratios from large straight circle to small wheel corresponding to that large straight circle, respectively. in the mechanical design, we let c 1 � c 2 . by decoupling analysis and calculating the shoulder joint, we can get the following equation:"
"taking into account the 2d and 3d modalities in image processing, we choose to adopt a 2.5d method which designates a set of techniques that take advantages of 3d features with fewer complexities. this involves extending the dimension of the lowest resolution of the mri volume images into the rgb dimension providing different benefits:"
"alzheimer designates an incurable disease that attacks brain tissues and influences mental functions, as well as memory. this neurodegenerative disease is characterized by brain damage, including β-amyloid peptide (aβ), neurofibrillary tangles and neuronal degeneration, which chronically damage the brain in an irreversible way [cit] . however, it took years to develop the appropriate algorithms to analyze information processing in the brain. consequently, in the last decades, several systems based on cad were proposed [cit] . however, most of the proposed systems have been based on manual functionalities requiring too much precision and concentration to study mri modalities."
"in the controller, the desired motor trajectory of the exoskeleton is planned. en desired joint trajectory of the simplified serial robot is calculated according to the trajectory decoupling algorithm. by calculating the inverse dynamics of the serial robot, each joint moment is obtained. erefore, the actual driving torque of each motor can be obtained by using the proposed conversion algorithm, resulting in the movement of the upper limb rehabilitation exoskeleton. it should be noted that the unmodelled errors and disturbances in robot dynamics are suppressed by the feedback parts of the controller, i.e., k p and k d . with regulation of the control gains k p and k d, the joint motor can follow the desired trajectory for motion. each motor trajectory planned in this paper is based on fifthorder polynomial as follow:"
"once we have completed the design of our network, tested the structure we start the training process. for that, extracting global characteristics of mri images requires large training and involves costly computation, given a large number of test parameters crossing the input/output layers. to evaluate our 2.5d approach, the soft dice metric was used as a network cost function during the formation phase, this metric representing a differentiable form of the dsc (dsc) [cit] . for better efficiency, in the processing of mri images, optimization based on a stochastic gradient is essential during the training phase to minimize the cost function according to its parameters. after training, the intensity is normalized by providing a linear transformation of the original intensities between two features into the corresponding learned ones. this allows the similarity of the histogram of each sequence between subjects. the time required was of the order of ten minutes."
"in our article, we deal with a learning network to segment mri images and form lists of the contained objects. as long as we rely on the u-net architecture, we have introduced some changes. first, we introduce a function in the pooling layer that assumes the possible location of the segmented objects. secondly, the cost function retrieves the information from the image and signals the location of objects or their distinctive part in the studied images."
"e fifth-order polynomial can minimize the jerk of the motion trajectory. e researchers have found that normal human movement follows a trajectory that minimizes total movement jerk based on observations of voluntary motion [cit] . it is expected that the exoskeleton will move along a trajectory similar to that of a normal human body to ensure its comfort and fitness for rehabilitation. and the use of polynomials has great advantages in real-time applications, especially in rehabilitation. e control of the human trajectory can be enhanced by the ability of replanning coefficients of polynomials or superimposing a new trajectory based on the previous one in real time by using this method. and, the angular velocity and angular acceleration of the fifth-order curve are both zero at the initial point and the end point. e coefficients of the polynomial can be obtained as follows: a i(i�1,...,6) � [0, 0, 0, 0.1257, − 0.0377, 0.003]."
"since only elbow flexion/extension movement are performed, the trajectory tracking effect shown in figure 5 of motor 3 and motor 4 at the elbow joint is analysed in this paper. it is assumed that the initial positions of both motors at the initial time are zero. as shown in figure 5, the angles of two motors have the same tendency to move in the opposite directions which corresponds to the elbow joint torque obtained above. position error and root-meansquared error (rmse) of elbow joint motors are shown in figure 6 and table 1 . e error between the expected position and the actual position of each motor is almost zero, which meets the purpose of rehabilitation training of the exoskeleton and shows the proposed algorithms are valid. e actual motor torque curve of simultaneous motion of multiple joints is shown in figure 7 . in figure 7, in the shoulder joint, an abduction movement is performed from 0°t o 90°. simultaneously, in the elbow joint, flexion movement is performed from 0°to 90°. when the shoulder joint and the elbow joint move synchronously, the maximum driving torque of motor 1 to motor 4 is 0.1206 nm, − 0.1205 nm, − 0.0225 nm, and 0.016 nm, respectively."
the research results show [cit] that the convolution product of the objective functional and the action integral gives the control structure accurate to a synthesis function. the function can be built using the stationarity conditions of the energy invariants which can be used to determine the switching surface. this allows to find that solutions to the extreme problem which satisfy to variational principle (the dynamics base) and provide the stability of the controlled motion in accordance with a.m. lyapunov's theorem.
"it is required to synthesize the law of optimal control of the dynamic system (19), transferring it from initial state to the phase space point 0, 0 fulfilling the condition of minimum of the objective functional min;"
"in the first experiment, height hundred images are used in order to train and validate our model. an example of the results is shown in fig. 4 which illustrates the detection and segmentation of endometrial areas of the brain are part of the semantic segmentation. after the training phase, we carried out different sets of evaluations. this allowed us to add new examples to our initial training. in this phase, we compared the segmentation result with the classification of detected objects in cerebral images. the network can detect cases of brain damage or lesion as shown in case a. they may affect brain abilities and cause different problems. in these cases, the patient may have similar signs similar to those of ad, yet these lesions can be treated. case b shows both brain damage and alzheimer disease presence in the early stages. finally, case c denotes the clear presence of ad, which shows a big difference comparing to healthy aged brains."
"compared to the quantitative results of the different mentioned methods, the validation of the proposed system demonstrated good results for the complete brain segmentation. using our method allows us to achieve an accuracy rate of 92.71%, sensitivity of 94.43%, and a specificity rate of 91.59%. to quantitatively assess the effectiveness of the methods tested we present, in a random order, the results of the evaluation methods as shown in table i . to ensure better comparison we used the same datasets from oasis to assess them. we conclude that the perfect segmentation of cerebral images is not granted by any method. however, the proposed method was able to take first place in terms of segmentation performance taking into account the use of the same evaluation basis. according to these results, we report the high-efficiency of our proposed network."
"where e � q d − q, _ e � _ q d − _ q, and € e � € q d − € q are the position error, velocity error, and acceleration error, respectively."
"take the one-sided driving mechanism as an example. when motor 1 actuates the small straight wheel ssw1 to rotate, the large straight circle ssc1 will also rotate by the actuation of ssw1. ssc1 and the second-order shaft sso1 are integrated structures, so the small straight wheel sw1 can be driven by using sso1 to achieve the rotation of the exoskeleton. specifically, sso1 and the second-order shaft on sw1 are driven with a pair of wire ropes represented by the green line and red line shown in figure 2 (b). when sso1 rotates, one of the two wire ropes that are entangled in sso1 is released to wrap around the corresponding shaft of sw1. at the same time, the other of the two wire ropes is released from sw1 to wound on the corresponding shaft of sso1. by using this method, we can achieve the corresponding rotation between sso1 and sw1. finally, since the upper arm is attached to the small wheel sw1, the small wheel rotation drives the upper arm rotation."
"where  is a positive constant. this equation will smooth a curve, eventually shrinking it to a circular point [cit] . the use of the curvature deformation has an effect similar to the use of the elastic internal force in parametric deformable models. the properties of curvature deformation and constant deformation are complementary to each other. constant deformation can create singularities from an initially smooth curve while curvature deformation removes singularities by smoothing the curve. the basic idea of the geometric deformable model is to couple the speed of deformation (using curvature and/or constant deformation) with the image data, so that the evolution of the curve stops at roi's boundaries. the evolution is implemented using the level-sets method."
"we note that the only purpose of the level-sets function is to provide an implicit representation of the evolving curve and the topological merging and breaking are well defined and easily performed. instead of tracking a curve through time, the level-sets method evolves a curve by updating the level-sets function at fixed coordinates through time. a useful property of this approach is that the level-sets function remains a valid function while the embedded curve can change its topology. the motion is analyzed by convecting the  values (levels) with the velocity field f . this elementary equation is:"
"an initial function   0,  t x  must be constructed such that its zero level-sets correspond to the position of the initial contour or surface. a common choice is to set:"
where 0 v is a coefficient determining the speed and direction of deformation (shrinks or expands). constant deformation plays the same role as the pressure force in parametric deformable models. curvature deformation is given by the so-called geometric heat equation [cit] :
"since the motion equation eq. 13 is derived for the zero level-sets only, the speed function  k f, in general, is not defined on other level-sets. hence, we need a method to extend the speed function   k f to all of the level-sets. a re-initialization of the level-sets function to a signed distance function is often required for level-sets schemes."
"in this section, we describe a modeling technique based on a level-sets approach for recovering shapes of objects in two and three dimensions. the modeling technique may be viewed as a form of active modeling such as \"snakes\" and deformable surfaces . the model which consists of a moving front, until is plated on the desired shape, by externally applied stop criteria synthesized from the image data (fig. 2.) . specially, deformable models are curves or surfaces defined in a digital image that can move under the influence of external and internal forces. external forces, which are computed from the image data, are designed to keep the model smooth during deformations. the external forces are defined from the deformable curve or surface like curvature in order to move the model to the boundary of a region of interest (roi) in the digital image. using these two forces, deformable models offer robustness to both image noise and boundary gaps, by constraining extracted roi's boundaries to be smooth and incorporating other prior information about the roi shape. moreover, the resulting boundary representation can achieve subpixel accuracy which is considered a highly desirable property for medical imaging applications."
"in the previous approach,the segmentation quality is not good, it means that the gradient information which is local information insufficient to control the evolution of the level-sets model. an alternative is to integrate statistical information related to regions in the brain mri volume to improve the quality of brain tumor segmentation. technically speaking, the new proposed method is similar to the segmentation with a deformable model with a two phased image [cit] . the general principle is based on the evolution of a surface  which partitions the volume data into several regions of different statistical characteristics."
"where  x d is the signed distance from each grid point to the zero level-sets. for example, when the zero level-sets can be described by the exterior boundary of a circle, the signed distance function can be computed as follows:"
"where . this scheme can work well for objects that have good contrast. however, when the object boundary is indistinct or has gaps, the geometric deformable contour may leak out because the multiplicative term only slows down the curve near the boundary rather than completely stopping the curve. once the curve passes the boundary, it will not be pulled back to recover the correct boundary."
"to segment the brain tumor using this approach (fig. 14.), we initialized an initial surface through its boundary. then this surface evolves until reaching the actual border of the tumor. several criteria can be incorporated to stop the process of segmentation: when the area of the deformable surface becomes constant or the volume of the region bounded by the deformable surface becomes constant or energy function    e reaches its minimum value. the latter criterion is sufficient but it has a problem of computational cost. the convergence of the deformable surface to the tumor border implies that the area and the volume of deformable surface becomes constant. however, area and volume computational is less. for this, we used as a stopping condition, area and volume of the deformable surface at a time. we present above a flowchart designed in our research to isolate a brain tumor using levelsets method based on region informations: this method consists in initializing a small sphere through the border of the brain tumor. then the level-sets model evolves according to related region information in the image in order to plate itself on the surface of the tumor. we show in the following the results of the 3d reconstruction of the brain tumor surface relayed to this approach. the following figure shows different stages of evolution of the deformable surface until reaching the final surface of the tumor and some projections in 2d slices (fig. 15. and fig. 16.) . these results show that this approach combine the following advantages: arbitrary initialization of the object anywhere in the image, no need for gradient information, self adaptation for inward and outward local motion. the following representation (fig. 16) shows the segmentation result on some slicers."
"despite the advantages cited above related to segmentation based on stacking a sequence of 2d contours detected in the parallel cross-sectional images, it has many disadvantages: there is information loss because the third dimension is not taken into account, broken boundary in one slice and overlapping intensities usually lead to poor detected results. this approach supposes that the distance between the slices is very small and the reconstruction of the surface and its properties from 2d contours may lead to inaccurate results. however, an evolution was necessary to the glance of its defects. the following developed approaches come to improve segmentation quality, based on carrying out the computation in 3d space and detects the 3d tumor surface directly using 3d level-sets method. first, the 3d level-sets model evolves according to information related to contours on irm volume specifically the data gradient information. second, the level-sets model evolves according to related regions information in the volumetric mr image."
the first stage of this method is to initialize a small sphere around the border of the brain tumor. then the level-sets model evolves according to information related to edges in the volumetric brain mr images. this movement comes to its end when the deformable surface found the actual border of the brain tumor. 3d discrete evolution equation of the level-sets model is the following:
"various numerical implementations of deformable models have been reported in the literature. for examples, the finite difference method, dynamic programming (amini & al., 1990), and greedy algorithm [cit] have been used. in this section, we present the finite difference method implementation for level-sets method as described in ."
"presented research was provided with a general goal to develop 3d segmentation algorithms of brain tumor from volumetric mri images. we have presented a variational method, 3d level-sets applied to automatic segmentation of brain tumor in mris, using boundary and region based information of tumor to control the deformable surface propagation. the first approach used, is the 3d reconstruction from its 2d contours using a sequence of 2d contours, detected by 2d level-sets method in the parallel cross-sectional mri images. this method goes very well but it has two major defects, there is no interaction between the slices and surface must be cylindrical. this approach is the most simple that one can make. it makes it possible to use active contours in the field 2d method which showed its robustness. however, an evolution was necessary to the glance of its defects related to the results obtained and the tumor shapes that were being able to be treated. the second approach comes to improve the segmentation quality, based on carrying out the computation in 3d space and detecting the brain tumor region directly using 3d level-sets method. in the first volumetric approach 3d level-sets model evolves according to information related to contours on irm volume. in the second level -sets model evolves according to related regions descriptors in the volumetric mr image. evaluations were performed on a set of volumetric mri images obtained from (medeisa) database."
"the discretization of equation eq. 13 is given as follows; noting   j i, is a position in the tow space dimensions image data:"
"the algorithm stops when all the cross-sectional mri images are processed. after all tumor boundaries are stacked and 3d tumor shape is reconstructed. the following figures (fig. 10. and fig. 11.) show various views of the surface of the tumor obtained by 3d reconstruction of its 2d contours and some projections of 2d tumor contours related of somes cross-sectional mri images. this approach is similar to an expert reasoning. this approach has several advantages such as simplicity to implement, it is fast, it requires less time than manual segmentation and based on 2d level-sets method that has shown robustness in the segmentation of mri images."
"here f is the desired velocity on the interface, and is arbitrary elsewhere. actually, only the normal component of f is needed. the inward unit normal to the levelsets curve is given by:"
to stop the evolution of 3d level-sets model in the desired boundaries we used 3d version of the anisotropic diffusion filter in order to reduce noise without removing significant parts of the brain mri volume and without evolving the deformable surface toward the brain tumor borders. we show in the following figure the results of the 3d brain tumor surface reconstruction using 3d level-sets based contour's information. the following figure shows different stages of evolution of the deformable surface until reaching the final surface of the tumor and some projections in 2d slices (fig. 12. and fig. 13.) . segmentation of 3d tumor in mr images using volumetric approach based on the level-sets as the surface detection mechanism. we note that the main problem with this approach is related to leakage or overflow of the deformable surface in regions where overlapping intensities are present and that usually leads to poor detected results.
"in three space dimensions), time, the geometry of the interface and the external physics. the interface is captured as the zero level-sets of a smooth function:"
"in this work, we describe various segmentation tools for segmenting brain tumor from volumetric mr images based on level-sets method. figure 1 shows a general diagram of developed segmentation tools. we develop a first technique of brain tumor segmentation by stacking a sequence of 2d tumor contours, detected by 2d level-sets method in the parallel cross-sectional mri images. it consists on applying to each brain mri slice the 2d level-sets method and to propagate the result by taking as initial data the result of the preceding slice. the first approach is similar to an expert reasoning. this approach has several advantages such as simplicity to implement; it is fast, it requires less time than manual segmentation but it has a major disadvantage : the information loss because the third dimension is not taken into account. however, an evolution was necessary to the glance of its defects. the second and third developed approaches come to improve segmentation quality, based on carry out the computation in 3d space and detect the 3d tumor surface directly using 3d level-sets method. in the second approach the 3d level-sets model evolves according to information related to contours on irm volume. the third proposed method is similar to the segmentation with a deformable model with two phased image. explicitly no need for gradient information, the level-sets model evolves according to related regions information in the volumetric mr image. in order to evaluate the proposed segmentation tools, mri volumetric images have been used. they can be downloaded from the well known medeisa database « medical database for the evaluation of image and signal processing algorithms », [cit] ."
"given by eq. 14. it has values that are closer to zero in regions of high image gradient and values that are closer to unity in regions with relatively constant intensity. iˆ denotes the image convolved with a gaussian smoothing filter whose characteristic width is  . in some image slices, the boundary feature of the tumor is not salient enough and the image gradient information is weak. it usually causes the \"boundary leaking\" problem when we apply the level set method to detect the 3d tumor surface. the problem of the gaussian filtering is the smoothing of the entire image, destroys and moves edges. so we need to limit or prohibit the smoothing operation of the contours in the mr images. for this, we must choose a filter aiming at reducing image noise without removing significant parts of the image content, typically edges, lines or other details of the mr image. the anisotropic diffusion filter proposed by p. perona and j. malik [cit] ) meets our needs. the action of such filter is given by the following nonlinear equation:"
"noise and boundary irregularities, as well as the ability to incorporate knowledge about the roi. however, parametric deformable model must be re-parameterized dynamically to recover the object boundary and that has difficulty in dealing with topological adaptation such as splitting or merging model parts. a level-sets deformable model, also referred to as a geometric deformable model, provides an elegant solution to address the primary limitations of parametric deformable models (taheri & al., 2009), (taheri & al., 2007), (lefohn & al., 2003) . [cit] . advantages of the contour implicit formulation of the deformable model over parametric formulation include: no parameterization of the contour, topological flexibility, good numerical stability and straightforward extension of the 2d formulation to n-d."
"the purpose of curve evolution theory is to study the deformation of curves using only geometric measures such as the curvature and the unit normal. let us consider a moving curve  t , where t represents the time;"
"since the curve or the surface of roi is recovred from the zero level-sets only. we must therefore detect the zero values of the function  .we can only detect differences in sign between two consecutive points in either direction, horizontal and vertical. the detection of points  j i p, of zero level-sets by malladi is done according to the following algorithm: recovering interface algorithm:"
"where pr err (c i, q) denotes the probability of a transient fault originating from a collection charge with strength q at hit node c i being latched by one flip-flop."
"where # gate denotes the total number of gates susceptible to hits by radiation particles in the circuit. note that the transient fault caused by a particle hit may propagate and be captured by different state-holding elements, resulting in numerous soft errors."
"where pr sig (c * i ) is the probability of logic-0 (logic-1) when a positive (negative) transient fault is generated at c i, and c k, which is neither c i nor d j, is another gate along the path (c i d j ). pr sig (c k ) represents the signal probability for a noncontrolling side-input that does not impede a transient fault propagating through gate c k ."
"on the other hand, the electrical-masking function, f e-mask (), reflects the pulse-width change of transient faults passing through a gate and can be defined as the following."
"the first basic part of mcc is mobile internet, also known as wireless networks, that is a series of computing networks for achieving wireless communications by connecting network nodes and using wireless protocols. a variety of mobile internet techniques are available for different demands [cit] . main techniques of wireless networks include wireless personal area networks (wpan), wireless local area networks (wlan), wireless metropolitan area networks (wman), wireless wide area networks (wwan), and cellular networks. among these techniques, the cellular network is an approach of deploying wireless communications, such as 3g and long-term evolution (lte)."
"since we have deduced the first-hit model ψ hit and the propagation model ψ prop, the pulse width of a transient fault can be approximated using (10)."
"the remainder of this paper is follows. we provide explanations about decm in section 2. following the description of the model, an example is given in section 3 in order to demonstrate the implementation of decm in practice. section 4 proposes and evaluates decm algorithm. the experimental results are represented in section 5. the conclusions are given in section 6."
"it is worth noting that because the timing information of transition signals is preserved, the issue of reconvergence can be analyzed in a manner that would be impossible in traditional sser methods [cit] ."
"the propagation stage starts after the generation stage and can be divided into three steps: in the first step, the breath-first search is employed to acquire the propagation tree g prop of the transient fault starting from c i and terminating at any po or ppo. once a gate is visited, it is added to g prop and the flag is set as visited so that any gate on the reconvergent gates will not be added again. after g prop is built, all gates in g prop are ranked according to their topological orders."
"to derive the pulse width of reconvergent transient faults in the same direction, we define the same-direction mix operation as a worst-case operation in which the new pulse comprises the latest transition signal and the earliest transition signal among these reconvergent transient faults. before performing same-direction mix operations over two reconvergent transient faults, the existence of overlapping is checked. as shown in fig. 8(a), in the event of overlapping, the earliest transition and the latest transition are selected to form a new pulse; otherwise, the width of the new transient fault is the sum of the widths of the two convoluted transient faults, as displayed in fig. 8(b) ."
"details of each step are organized as follows. after introducing the first-hit model and propagation model in section iv-a, the distributions of the width in a transient fault are estimated by the mme [cit] in section iv-b. the two issues related to correlation and reconvergence are discussed in sections iv-c and iv-d, respectively."
the overall ser can be defined as the accumulation of soft errors (s e(·)) resulting from particle hits at each individual gate (c i ) in the circuit. that is
"in this paper, we propose an advanced dynamic model, dynamic energy-aware cloudlet-based mobile cloud computing model (decm), which uses cloudlets technique to assign, manage, and optimize the cloud-based infrastructure usages and services for achieving green computing. this model uses dynamic programming to assist cloudlets cloud computing resources within a changing operational environment. the intention of decm matches practical demands of mobile industry because various elements can have major influences on the cloud services quality. for example, mobile users who are using map services highly rely on the speed of wireless communications while the mobile devices are rapidly moving. nevertheless, unstable and inefficient wireless connections usually shorten the battery life."
"furthermore, as one of the core techniques in cloud computing, virtual machine (vm) is considered an efficient approach for building up cloud-based datacenter to achieve green computing [cit] . nevertheless, vm is only a service representation approach that does not bring much technical innovations, even though vm has been broadly applied in deploying green information technology (it) industry, such as green data processing, storage, and transmissions [cit] ."
"in the generation stage, the first-hit model ψ hit is used to deduce the distribution of the particle-induced transient fault on the output pin of the hit gate c i . the initial transient fault is then split into a rising-transition signal and a falling-transition signal, denoted as t 0 r and t 0 f, respectively, and their moments can also be deduced by ψ hit ."
"traditional monte carlo methods for sser analysis are known to suffer from long simulation times when deriving the pulse-width distribution for particle hits and transient-fault propagation. therefore, this paper employs a parameterized first-order closed form for these two distributions. we simply"
"the principle of decm can enable green computing because the model is designed for mainly reducing energy consumptions, which matches one of the characteristics of green computing as an energyaware feature [cit] . the approaches of achieving green computing are from software and hardware to management, policy, and legal issues. our model focuses on technical side that leverages a few cloud-related techniques, such as vm, wireless networks, and dynamic programming."
"1. this research is the first attempt on the functionality of cloudlets in order to achieve energy-aware performances in the dynamic networking environment. 2. the results of this research provide theoretical supports and explorations. the model may be migrated and applied in multiple domains. the model may be able to be migrated and applied in multiple industries, which requires further research for identifying and proving."
"the remainder of this paper is organized as follows. in section ii, previous studies related to ssta and sser are reviewed. in section iii, we propose the outline of the closedform framework for sser analysis. a parameterized firstorder closed form of transient faults is detailed in section iv. section v presents the experimental results, including the accuracy of our models, the ssers, as well as the runtimes over a variety of iscas'85 benchmarks, a series of multipliers, and several industrial circuits. section vi concludes this paper."
"transition signal t arrives at the input of a gate with delay d, where t and d can be expressed in linear closed form as after the timing signal t passes through the gate, the output timing signal t is updated as t + d, enabling us to deduce t by a sum operation of two normal jointly-distributed random variables, as described in section ii-a. hence, a rising signal t in r and falling signal t in f at the gate input can be propagated to the gate output and modeled by ψ prop . accordingly, the two output timing signals become"
"figs. 11 and 12 compare the results from the probability density function (pdf) of transient faults induced by four particles of different charge strength in the proposed models and those of monte carlo spice simulation for one and gate and one or gate, respectively. the solid line represents the pdf results of the monte carlo simulation while the pdf results from our models are denoted by a dotted line. the means by which pdf results are derived using our models are very close to those derived using monte carlo spice simulation, while the variances of pdf results derived by our models are slightly smaller (6.76% on average). table ii summarizes the accuracy of the first-hit models and propagation models. the first column lists the name of the cell libraries, and the following four columns denote the mean and variance errors of first-hit models and those of the propagation models, respectively. the average mean and variance errors of our first-hit model are all less than 2%, as is the average mean error of the propagation models. moreover, except for the variance of the propagation model, both proposed models are more accurate than the svm models in the latest framework [cit], and in particular, the variance of the first-hit model (1.90% versus 12.27%)."
"given the first-hit model ψ hit and the propagation model ψ prop, the final distribution of pw in fig. 5 can be further expanded according to (10) . that is (13) where the superscript is the corresponding topological order originating in the hit gate."
"correlation is a major concern when using a first-order closed-form method to approximate the behavior of transient pulses. this is because the pair of transition signals t r and t f are mutually dependent rather than completely uncorrelated. intuitively, the solution to this issue is to iteratively split and merge the transient faults during propagation. as illustrated in fig. 6, a transient pulse is reconstructed by merging t r and t f after both transitions pass through a gate, and then splitting them again before they are propagated toward the succeeding gates."
"due to process variation beyond the deep submicrometer era, traditional static approaches are no longer effective for analyzing sers. this is because soft errors originate from particle hits with small charges, which can easily be overlooked in traditional static analysis, resulting in an underestimation of sers compared to monte carlo spice simulation. in recent years, numerous sser frameworks have been proposed; however, simulation-based methods still suffer from extremely large timing costs, even when accurate sser results were achieved. on the other hand, learning-based methods have been developed to overcome the problems of timing costs while sacrificing the accuracy of sser."
"in this section, we review the first-order closed-form statistical static timing analysis and the frameworks used to analyze statistical soft error rates (ssers) in sections ii-a and ii-b, respectively."
"using our proposed schema can reduce the energy consumption in wireless communications. the system reliability is also considered in the proposed schema, which aims to guarantee the performance of the system. saving the energy costs of the communications can achieve green computing. the goal of performing green computing is to apply decm to reduce energy consumptions on mass mobile devices without weakening the performance of cloud services."
"mobile cloud computing (mcc) is an emergence of multiple internet-based technologies development, which enables mobile users to acquire benefits of cloud computing and achieve green computing by using their mobile devices [cit] . the technology mainly derives from three hemispheres, including mobile computing, mobile internet, and cloud computing. combing the advantages of multiple techniques allows users to offload data processing and storage to the cloudbased servers [cit] . however, behind the benefits of adopting this approach, the implementations of mcc are still facing a few challenges that limit its performance, such as energy over consumptions while the wireless communications are weak [cit] . keeping searching wireless signals can dry out the power of mobile devices, which may cause unexpected energy waste [cit] ."
"in the second step, the initial transition signals t 0 r and t 0 f are propagated along g prop using the propagation model ψ prop in a block-based fashion. during propagation, the two conditions are handled in different ways. for the case in which the output pin of the current gate c j is a reconvergent fanout node (rfon), sum and mix (introduced in section iv) operations are deployed to deal with the issue of convolution of transient faults. for the opposite case, only the sum operation is required. in the final step, the transient faults arriving at one ppo or po are reconstructed by merging t r and t f, and combined pulse-width distributions are used to compute ser, accordingly. details regarding ψ hit and ψ prop are described in the following section."
"2) updating logic probability: the logic probability at reconvergence fanout nodes should be updated to reflect the phenomenon of reconvergence. for convoluted transient faults, the result of logic probability is the union of the logic probabilities of input transient faults, because this condition is equivalent to all of these transient faults being able to pass through the reconvergent node. taking fig. 10 as an example, the logic probabilities of transient faults at the output pins of gate g1 and gate g2 are denoted as pr1 logc and pr2 logc, respectively. the logic probability of a transient fault at the output pin of gate g3, denoted as pr3 logc, as illustrated in fig. 10 ."
"accordingly, efficient and accurate models, ψ hit and ψ prop, become the most critical components due to the difficulty in integrating the impact of process variation on soft errors. in this paper, both ψ hit and ψ prop are derived in first-order closed forms; therefore, the deduction over ψ hit and ψ prop (to approximate ψ hit and ψ prop, respectively) can be conducted using the method of moment estimation (mme) [cit] . accordingly, the estimated electrical-masking function in (9) can be modified as"
"ith increased scaling in cmos technology, the issue of reliability is becoming increasingly important for memory devices [cit] as well as soft errors, which are a major failure mechanism for logic circuits. the cause of this type of error is radiation-induced transient faults, which are latched by state-holding elements causing nonpermanent damage to data. soft error rates (sers) are much higher than those typically associated with reliability mechanisms, and with recent increases in circuit speeds, soft errors occur even more frequently [cit] ."
"during monte carlo simulation, the pulse width of the arrival transient faults was measured at all po/ppo for all input-pattern combinations. due to the long runtime associated with monte carlo spice simulation (with 100 runs), we were only able to perform tests on small circuits of up to 26 gates, 31 hitting nodes, and five inputs. the runtime for such monte carlo spice simulation required more than one day."
"where # ff and d j represent the total number of flip-flops in the circuit and the j th flip-flop, respectively. pr logc and pr elec, respectively, denote the logic-masking probability and the electrical probability related to the electrical-masking and timing-masking effects. corresponding details are elaborated in the following sections."
each se(c i ) can be further formulated by integrating the products of the particle-hit rate and the error probability over the range of charge strength from q min to q max as
"the reason that the variance error associated with the propagation models is worse is that the shape of the hitting pulse becomes irregular during propagation. as shown in fig. 13, because the sinusoidal shape of a hitting pulse is transformed into a trapezoid, the variance of the flat part (like f 1 and f 2 ) of the trapezoid is hardly considered in the proposed framework, leading to an underestimation of variance."
"for reconvergent transient faults in opposite directions, the pulse width is determined according to interactive behavior. in fig. 9, if the positive transient fault appearing at one input of an and gate does not overlap with the negative transient fault appearing at the other input of the and gate, the pulsewidth result is the width of the positive transient fault pw, because the negative transient fault is completely masked by the controlling value on the side input. in the event of overlapping, the result is computed as the width of positive transient fault pw subtracted by the overlapping period (d) between the positive and negative transient faults due to the negative transient fault masking part of the positive transient fault. other gate types can be derived in a similar manner."
"3) pulse-width reconstruction: once both signals reach po or ppo, they are merged to reconstruct a new transient pulse to determine whether or not a soft error has occurred. the reconstruction step uses the idea proposed in (10) . note that, when we split one transient fault into two transition signals, the related important information, such as its amplitude is also embedded implicitly in the timing models (t r and t f ) to correctly estimate the behavior of a transient fault. to take fig. 5 for example, the original transient pulse generated by a particle hit at the output of g0 is split into two transition signals, which then individually begin their propagation. finally, both signals end at g2 and are merged to reconstruct the transient pulse based on t r and t f ."
"where the superscript is the corresponding topological order originating from hit gate g0, τ 0 r/ f denotes the slope ratio defined as the slope of the rising signal to that of the falling signal, and ρ t 0 r t 0 f, pre-characterized into a table, is the correlation coefficient of t 0 r and t 0 f . after obtaining the distributions of the two initial transition signals, the linear timing model ψ prop is deployed to propagate both signals toward the primary outputs. the derivation of the linear timing model ψ prop, computed by typical statistical static timing analysis, is given as:"
"mcc is the conceptual architecture that combines three technologies, including mobile internet, mobile computing, and cloud computing, to enable mobile users to offload data processing and storage onto clouds via wireless networks and mobile devices [cit] . the motivation of applying mcc is gaining benefits of cloud computing technologies by leveraging mobile techniques. the dynamic networking environment results in more complicated service deployments and implementations, comparing with basic cloud computing."
"the number of transient faults doubles if there is a reconvergent structure along the propagation path in the circuit, resulting in an exponential increase in the complexity of the sser analysis. as shown in fig. 7, a particle hits the output of g0 and induces a transient pulse. the transient faults then propagate along the paths in a block-based fashion, finally reconverging at the inputs of u0 and u1. consequently, two positive transient faults appear on the output of u0, and two transient faults with different directions appear on the output of u1."
"because the quality statistical model has been the bottleneck in all previous sser frameworks, sser the sser results, long simulation time was required, resulting in a simulation-based method that is inapplicable for industrial circuits. both of the studies described above deal with the computation of electrical probability, but differ in the computational methods used to derive the transient-fault distribution. the aim of this paper was to achieve high efficiency and accuracy simultaneously during the computation of the transient-fault distribution, through linear closed-form formulation, as shown in fig. 3 . after acquiring the distribution of transient faults, the occurrence of soft errors on the flip-flops can be determined by checking whether these transient faults are smaller than the error-latching window of the flip-flops. if a transient fault is wide enough, a soft error is captured; otherwise, it is masked."
"to resolve this problem of reconvergence, we propose a two-stage approach. in the first stage, transient faults are classified into two groups according to their directions. the outcomes of the pulse width and the logic probability of these convoluted transient faults are then derived in the second stage. the pulse-width distribution of convoluted transient faults is derived using a newly-defined mix operation in which the logic probability is updated as the union of the logic probabilities associated with these transient faults."
"the results derived using the traditional max operation in ssta may lead to an underestimation of the pulse-width associated with reconvergent transient faults. taking fig. 8(a) as an example, we denote the latter transient fault and former transient fault as p 1 and p 2, respectively. the result deduced by the same-direction mix operation performed on p 1 and p 2 should be the latest transition and the earliest transition among them, respectively, denoted as t r1 and t f 2 . however, the results derived using the traditional max operation performed on p 1 and p 2 are t r2 and t f 2 . similarly, in fig. 8(b), the pulse-width result deduced by ssta's max operation is pw 2 rather than pw 1 + pw 2 ."
"the overall analysis is outlined as follows. 1) transient-fault generation and decomposition: initially, the first-hit model ψ hit is used to look up the distribution of the initial pulse width pw 0 from a precharacterized table according to the output load of the hit gate and the strength of the hitting charge. then, the estimated pulse width pw 0 is decomposed into two initial transitions t 0 r and t 0 f according to the ratio of their slopes. 2) block-based propagation: two timing signals are updated by ψ prop whenever they are propagated through one gate, reflecting the gate delay. this step repeats until both the rising and falling signals arrive at one po or ppo."
"to apply the first-order closed-form statistical static timing analysis, two operations, sum and max, are required. the procedure of the sum operation of two jointly-distributed random variables is described as follows."
"monte carlo spice simulation, the svr-learning approach, and casser were used to evaluate ser accuracy on five benchmark circuits (t1, t2, t3, c17, and adder 2bit ). information related to the five benchmarks is listed in table iii . the name of each circuit is shown in column 1, and the following four columns denote the number of gates, the number of primary inputs (pi), the number of primary outputs, and the maximum topological level, respectively. fig. 14 compares the ser analysis results of these five circuits. our findings lead to two conclusions. 1) the svr-learning framework does not typically yield results of satisfactory accuracy for ser compared to those using monte carlo spice simulation due to a lack of quality models. moreover, the 16% difference in the result of two-bit adder (adder 2bit ) is due to reconvergence, which was not considered in that framework."
"for example, the width of a transient pulse hitting the output of a gate decreases as the output load of the gate increases (because the charging/discharging time of capacitors increases). another example is that a hitting charge with greater strength causes a wider transient pulse. hence, for the first-hit model ψ hit, x includes charge strength, the type of driving gate, and output loads; y contains the distribution of initial pulse width, correlation coefficients, and slopes of the two transitions. similarly, for ψ prop, x consists of the same components as x in ψ hit with an additional component -the slope of the transition signal; y contains the transition slope, the distribution of gate delay, the correlation between transition signal and the corresponding gate delay, and the correlation between transition signals."
"2) the proposed closedform ssta-based framework casser yields more accurate sers with differences of less than 3%, demonstrating that the proposed idea is capable of achieving superior accuracy. the results of adder 2bit were quite accurate, despite the inclusion of many reconvergence fanout nodes, demonstrating the effectiveness of our reconvergence handling strategy. moreover, fig. 14 also shows that the ser obtained by monte carlo spice simulation are 19% ∼ 35% above that obtained by static spice analysis and proves that we need to consider the process variation for estimating ser again."
process variation often results in unpredictable transient fault behavior that cannot be accurately estimated using static approaches. both learning-based and simulation-based methods of sser analysis have been studied in the literature.
"based on the industrial needs, our advanced deployment model, decm, offers a unique mechanism to avoid the energy waste when users are suffering a complicated and unstable networking environment. the model is a type of web service that focuses on efficient communications between user devices and cloud servers. figure 1 represents a conceptual model of decm. three main components of decm include mobile device, cloudlets with dynamic searching, and cloud computing. the relationship between cloudlet and dynamic searching is that the cloudlets provide an operating platform in which the dynamic search is executed."
"to consider both efficiency and accuracy simultaneously, this paper proposed a framework named casser, which includes a novel idea for sser analysis, in which a transient pulse was partitioned into two transition signals (one is rising transition and the other is falling transition). because the two signals were expressed as timing quantities in closed form, they can be analyzed using a block-based ssta-like method, which considers the correlation of timing. according to experimental results, the runtime of analysis using casser is small and sser differences are within 3%, compared to monte carlo spice simulation. moreover, the timing cost of casser is about 286 times faster than that of a previous sser framework [cit] ."
"information related to other benchmark circuits and their sser results as well as runtimes derived using the two methods are listed in table iv . columns 1-5 denote the name of each circuit, the number of gates, the number of pi, the number of primary outputs, and the max topological level, respectively. the remaining four columns show more sser results and runtimes derived by svr-learning framework [cit] and the proposed framework casser on a variety of circuits, respectively. the last column computes the improvement in timing cost. the last six test cases were aborted because the runtime exceeded one day. the runtime of each test case using casser was less than ten minutes except for bench7 and approximately half of the test cases were completed in one second. in addition, the timing cost grows slowly even if the circuit size grows rapidly, while that of the svrlearning method increases rapidly as the circuit size increases. the runtime of casser was approximately 286 times faster than that of the svr-learning method. moreover, because the proposed idea is built upon a closed-form ssta-like analysis, the longer logic depth will induce a longer runtime. for this reason, c6288 and some multipliers (mul_16 to mul_32) required a slightly longer runtime."
"many researchers and scholars have done various achievements in energy-aware mobile cloud computing in previous research. the research is diverse in different perspectives [cit] . zhu and his team [cit] ) developed a real-time tasks oriented virtualized cloud computing system that was designed to achieve energy-aware scheduling in their recent works. the proposed solution [cit] intends to integrate various energy-aware scheduling algorithms by employing a rolling-horizon optimization policy. however, this approach did not consider mobility usage and the similar research focusing on energy-aware cloud computing systems has been accomplished by other scholars [cit] ."
"we develop a motivational example explaining the fundamental methods of adopting decm. the implementation of the example is a simulation followed by the proposed model. figure 2 illustrates the fundamental concepts of decm. compare with classic web services, business logics are refereed by cloudlets that use dynamic programming to search efficient cloud services. this difference is a core session in our model, which is optimizing the utilizations of cloud resources in mobile cloud. detailed information of fundamental concepts for decm is given in the following section."
"the reason for defining a new mix operation for the two timing signals is that the pulse-width result of transient faults is underestimated and incorrect, if the traditional max operation is used to deduce the result of these convoluted timing signals."
where ρ td denotes the correlation coefficient of t and d. [cit] further used the concept of tightness probability to deduce the result of the max operation of two timing quantities in closed form. the definition of the max operation is described as follows.
"the procedure of the service delivery is followed by the directions of the arrows. mobile devices send the service requests to the closest cloudlets before the requests reach the cloud servers. the cloudlets allocate the cloud servers for better service performance determined by a group of constraints, such as nearby server locations or networking stability. dynamic programming is applied in cloudlets for adapting to the constant changing context. selecting the best solution directed by the dynamic-based cloudlets is the core component of decm, which is expected to avoid energy waste when switching cloud servers or wireless networks."
"in the first step, ψ hit is responsible for approximating the distribution of t 0 r and t 0 f and the corresponding computations can be enumerated as"
"in this section, we review the analysis of soft error rate considering the impact of process-variation beyond the deep submicrometer era [cit] . overall analysis comprises three main components: 1) computation of logic probability; 2) electricalpulse propagation; and 3) the accumulation of soft errors. a flowchart of the overall process is shown in fig. 3 . the following sections deal with each component in detail and the global view of such a linear closed-form formulation, respectively."
"experimental results show that this process can be skipped because the impact of the correlation between transition signals on sser is small. in table i, the name of each circuit is listed in column 1; the remaining two columns show the results derived by the closed-form block-based sser framework with independent transition signals (a) and with correlated transition signals (b), respectively. the last column computes the difference of the sser results derived using these two methods. according to table i, it is clear that the difference between the ser results derived by the two methods is negligible on four iscas'85 benchmark circuits (where the signal correlation is strong) and a 5-to-32 decoder circuit (where the correlation is weak). in other words, the correlation between transition signals is independent of ser estimation and thus can be overlooked in our framework."
"this allow us to resolve the relationship between the variance of and penalty. if, then implies, but the opposite is not true. this shows that for a given download rate, just reducing the variance of refresh intervals, without enforcing, is insufficient to improve the penalty across all functions . as an example, recall the special case of with in (54), where the penalty was reduced iff the variance of was; however, no such causality exists for or . on the other hand, if reduction in penalty holds for all measures, then stochastic ordering between and follows and thus variance has to decrease (i.e., ordering of variances is necessary, but not sufficient)."
"an svc algorithm was standardized as the annex g of h.264 [cit] to cover the needs of scalability. in this standard, the video compression is performed by generating a unique hierarchical bit-stream structured in several levels or layers of information, consisting of a base layer and several enhancement layers. the base layer provides basic quality. the enhancement layers provide improved quality at increased computational cost and energy consumption. because the energy consumption depends on the particular layer to decode, an h.264/svc decoder is a very well-suited solution for managing the energy consumption by selecting the appropriate layer."
"consider a single source driven by an update process and a single replica with the corresponding download process, which is independent of . our first contribution is to propose a general framework for modeling staleness under arbitrary stochastic processes . since staleness age and various penalties derived from it are usually defined in terms of sample-path averages [cit], questions arise about their existence and possible variation across multiple realizations of the system. we address this issue by identifying the weakest set of conditions for which the distribution of staleness age exists and converges to a deterministic limit."
"while (3) is convenient, it is unclear whether these limits exist, if they are finite, and under what conditions they are deterministic. we investigate these issues next."
"the first restriction is that collection within each sample-path have some limiting distribution . if this fails to hold, staleness in (3) does not exist either. the second prerequisite is that not be a random limit. this condition ensures that almost all sample-paths produce the same result. finally, the third condition is that an fraction of cycles must consume an fraction of length as . allowing otherwise would be a problem because, being a limiting distribution, does not capture these intervals, but still lands there with a non-diminishing probability as (we discuss an example demonstrating this effect shortly)."
the second interpolated pel (i 1 ) is calculated using the same algorithm but in this case employ the pels from p -1 to p 4 . all these pels are stored in data1 and data2 so no additional loads are necessary. the results for this pel are stored in m 2 and m 4 variables.
"secondly, table i indicates that some modules have been optimized achieving an improvement greater than 70%. but the global improvement shown in table ii is around 40%. this difference is justified by the flow chart of the decoder presented in fig. 5 . the decoder executes the decoding phases (entropy decoding, mc and deblocking filter) frame by frame generating data cache misses and increasing the number of cycles used to decode each picture. currently the flow chart is being modified to reduce the cache misses."
"the left side of (19) is concave in, which means that must be either concave or degenerate at . from (18) and (19), we know that: (20) where letting establishes that the latter case is impossible. therefore, must be concave, for, and finally . from helly's selection theorem [cit], there exists a further subsequence of along which (18) holds and:"
"this result shows that is determined by the positive deviation of the generalized lag from zero, or equivalently by that of, where the weight applied to each deviation is given respectively by and . the only caveat is that simplification (49) requires weight functions that can explicitly handle negative arguments, e.g., a constant penalty would be rather than just . throughout the rest of the paper, we avoid the extra notation dealing with, but keep this in mind."
"a similar result holds under update penalty . note that all with the same are equivalent here, which is why we state only half of theorem 8, and is less restricted. theorem 9: assume the conditions of theorem 7. for a given and fixed, iff for all nonnegative . proof: since, where is a measure for all non-negative, lemmas 1-2 yield that decreases iff gets stochastically larger in second order."
"this section describes the decoder performance, measured as the number of cpu cycles employed to decode a frame of a sequence layer, after the optimization process. while in subsection v.a, the performance improvement of each optimized module is summarized, in subsection v.b the decoder performance is analyzed using quality-spatial and spatial-quality sequences. finally, subsection v.c presents the decoder performance using sequences with different bitrates."
"to put these models in perspective, we use wikipedia's distribution of, which happens to be quite heavy-tailed (i.e., zipf shape ). the average update rate across all pages is updates/day; however, 98% of them exhibit less than 1/day, 90% less than 1/week, and 50% below 8/year. using this distribution in (73) and (74) shows that optimizing staleness of the entire wikipedia under uniform page access requires 46 times less bandwidth than under zipf. this can be explained by the fact that keeping frequently modified pages fresh costs more bandwidth. this effect is related to the variance of :"
"suppose a replica is a system whose goal is to synchronize against information sources, apply certain processing to downloaded content, and serve results to data consumers. one challenge of this architecture is that sources not only require pullbased operation, but also lack the ability to predict future updates, which makes real-time estimates of remaining object lifetime (i.e., ttl) unavailable to the replica."
"this pull-based replication (also called optimistic or lazy) improves both scalability of the service and availability of the data, but at the expense of increased age of manipulated content [cit] . this model of operation has enjoyed ubiquitous deployment in the current internet (e.g., http, dns, network monitoring, web caching, rss feeds, stock-ticker aggregators, certain types of cdns, sensor networks); however, it still poses many fundamental modeling challenges. our goal is to study them in this paper."
"we conclude the paper by noting that internet applications often combine the last two scenarios, i.e., and replication, into a single framework. however, these problems are usually separable into subproblems that can be reduced to the analysis above. for example, suppose we are interested in the probability that a query to a random subset of replicas finds at least one of the sources fresh. first, we compute the staleness probability for each source based on the aggregate synchronization process from replicas. second, since each source is independent, we multiply these probabilities to deduce the likelihood that all sources are stale. taking the complement of the result, we get the desired probability."
the use of the dma in the motion compensation process achieves an improvement lower than 4% in the global performance. this improvement is smaller than expected because the cpu must wait for the end of transfers before processing the transferred data.
"the limiting distribution in (5) is again a constant equal to 1, but this time (6) converges to, which makes this process age-measurable. however, for, the sum in (26) oscillates between 25 and 30 as increases, while the corresponding integral is . as a result, is not measurable by and does not converge as . from this point on, we omit explicit conditioning on the sample-path since results do not depend on for age-measurable processes. however, we keep in mind that all probabilities and expectations involving are still taken in the sample-path sense."
"our first objective is to derive the probability of staleness. theorem 4: assuming that and are age-independent, the probability of staleness at time converges in probability as to:"
"we now present the reverse (necessity) proof. assume that for some and is bounded. our goal is to show convergence of (5) and (6) to such deterministic limits that satisfy (9) . from the bolzano-weierstrass theorem [cit], every subsequence contains a further subsequence such that (18) where with probability 1 from being bounded. note that this limit may depend on the subsequence. to show finiteness of, notice that (13) and implies that for all :"
"since is -measurable, (27) shows that this expectation converges and its limit equals . by (25), this is also . to compare against prior results, consider poisson and constant . then, (50) produces for and for, both of which match previous analysis of these special cases [cit] . generalizing to exponential, we obtain from (50) respectively and ."
"theorem 2: for a process that is age-measurable by, the sample-path expectation of converges in probability as : (27) to understand this better, consider another counter-example:"
"this leads to the main result of this section. theorem 10: when the conditions of theorems 8-9 hold, constant inter-synchronization delays are optimal under the corresponding staleness metric."
"we next aim to establish a minimal set of conditions under which analysis of staleness admits closed-form results. consider a point process with cycle lengths, where each is a random variable. in order for the age of this process to have a usable limiting distribution as, one must impose three constraints on, which we discuss informally and motivate next, followed by a more rigorous definition."
"returning to the topic of information staleness, our goal is to determine the condition under which both types of penalty can be reduced without changing the refresh rate. define and to be the source penalties corresponding to random synchronization intervals and, both with mean . for the opposite problem, i.e., finding the worst update distribution, define and to be the penalties that correspond to update intervals and under a fixed . the next result shows that stochastic (rather than variance) ordering is needed to improve staleness penalty. define to be a measure if it is non-negative, non-decreasing, and rightcontinuous with for . similarly, for a fixed, gets stochastically larger in first order iff becomes stochastically smaller. again applying lemmas 1-2, penalty increases iff becomes stochastically larger in second order."
"an h.264/svc decoder based on a commercial dsp has been implemented by porting the open svc decoder from the pc to the dsp environment. several optimizations techniques have been applied to reach real-time performance for cif sequences. up to the best of our knowledge, no other h.264/svc decoder based on dsp has been reported. this optimized decoder will be used in a multimedia terminal to trade-off between quality and energy consumption."
"which immediately leads to (42) after expansion of and . with, (42) reduces to staleness probability already discussed above. for the other case seen in the literature, we obtain the expected staleness age by which the replica trails the source. under poisson and constant, we get from (42): (47) and when both distributions are exponential: (48) these special cases are consistent with [cit] . simulations in fig. 7 additionally confirm that (42) is accurate under general renewal processes. also observe in the figure that the combination in (b) continues to offer inferior performance to that in (a); however, the difference between the two scenarios is now more pronounced. for example, using the same considered earlier, search-engine clients encounter indexing results outdated on average by 0.06 days (1.5 hours) in the left subfigure and by 0.8 days (19 hours) in the right. this example shows how drastically the cost changes based on the shape of and, which emphasizes the importance of utilizing models that can accurately handle any underlying processes . we now offer a more intuitive look at source penalty. modifying to be zero for negative, we can rewrite (42) in a more compact form:"
"even though constant is optimal from the staleness perspective, it unfortunately fails to guarantee age-independence (30) against all underlying . we now deal with principles related to asta (arrivals see time averages) [cit], placing them in our context. in general, asta can be viewed as a condition that allows discrete and continuous sample-path averages of a process to be equal almost surely:"
"finally, the performance improvement achieved with the \"snr\" module for the layers 1 and 5 is negative. this module is only used if a quality enhancement layer is decoded (layers 2 or 3). the rest of the enhancement layers do not use this module so the global improvement should be zero. however, the integration of this module modifies the allocation of the code and the data in memory and therefore the number of cache misses varies. this situation generates a negative improvement in the decoder performance for layers 1 and 5."
"to calculate the interpolated pels a 6-tap filter must be applied in horizontal and vertical directions. the fig. 7 shows the pels used to calculate the intermediate values for an 8x8 pel block (shadowed pels) with a fractional motion vector only in the horizontal direction. thirteen pels (from p -2 to p 10 ) must be read for each row. this pels are stored in four 32-bit variables (data1 to data4) using 4 double-word load instructions. 8 summarizes the optimized algorithm used to calculate the interpolated pels for the first row. the first interpolated pel (i 0 ) uses the pels stored in data1 and data2 (from p -2 to p 3 ). each pel is multiplied by a constant coefficient, all the results are accumulated and finally the average is calculated. all these operations can be optimized using a specific simd instruction available in the dsp (_dotpsu4). two of these instructions are needed and the final results are stored in two 16 bits variables (m 1 and m 3 ). the average between both data must be calculated as will be show later."
"the majority of the literature on source penalty is limited to poisson, either constant or exponential, and or [cit] . there has been only one attempt to model under a renewal process, in which [cit] assumed and the entire sequence of refresh instances was known. while appropriate in some cases, this model is difficult to evaluate in practice when is given by its statistical properties."
"the last row presents only for reference the average improvement achieved. these results demonstrate that real-time performance has been achieved for all the layers of the generated streams. later, the decoder performance is measured using the spatial-quality sequences. table iv contains the percentage of cpu cycles needed to achieve real-time processing using the un-optimized and the optimized versions and the percentage of improvement achieved for each layer. the average improvement achieved for each layer is showed in the last row. in this case, the real time performance is not achieved for layer 5. the results presented in table iii and table iv demonstrate that the performance is higher if the first enhancement layer is a snr layer instead of a spatial enhancement layer. real-time performance is achieved for the first subset of sequences presented in subsection iv.a but not for the second subset."
"the fixed-point dsp core has two levels of internal memory (l1 and l2). the l1p memory/cache consists of a 32 kb memory space and the l1d memory consists of an 80 kb memory space. both memories can be configured as cache memories, general-purpose memories or a combination of both. finally, the l2 memory/cache consists of a 64 kb memory space, shared between the program and data. l2 memory can be configured as a general-purpose mapped memory, a cache memory, or a combination of both."
"let and . then, if (64) holds for all, it follows that the distribution of refresh age sampled in update points equals that sampled in uniformly random instances, which in turn is equivalent to our earlier formulation (30). while we have given conditions for the right side of (64) to exist and equal a constant almost surely, existence of the left side or its equality to the integral is not guaranteed. asta analysis focuses on the properties of points and their relationship to that allow (64) to hold; however, this normally requires conditions that are difficult to verify in practice (e.g., laa, wlaa, lba [cit] ). we therefore discuss guidelines for ensuring that (64) is satisfied, without becoming engrossed in unnecessary rigor."
"to shed light on the complexity of real, we plot in fig. 9 (a) the tail cdf of inter-update delay for the most frequently modified article -\"george w. bush\" with 44,296 updates in 10 years (mean delay hours). the figure is a close match to pareto tail with and . in fig. 9(b), we show the corresponding auto-correlation function with a power-law fit, which suggests long-range dependence (lrd) with hurst parameter 0.81. of course, lrd effects might be caused and/or compounded by non-stationarity. to address this question, fig. 9(c) shows the update rate throughout the day, clearly indicating non-stationary dynamics."
"theorem 12: assume and let refresh delays be optimal (i.e., constant). then, the solution to (66) using guarantees that . proof: using lagrange multipliers, we get that all partial derivatives of must equal some constant :"
it is worth noting that the open svc decoder has been developed for a pc-based platform. the decoder has been ported to the dsp as follows:
"3) the average cycle length converges to in probability as : (6) note that any renewal process satisfies this definition since all are the same, which from the weak law of large numbers trivially leads to and . furthermore, condition (6) resembles mean-ergodicity, which is normally stated with a stronger type of convergence (e.g., mean-square or almost-sure) and only for stationary processes. for other cases, the fact that indicator variables are uniformly bounded allows application of the dominated convergence theorem (dct) [cit] to show that is the limiting average of individual distributions:"
"the next intermediate result is the distribution of the first lag, which relies on in (36). theorem 5: if and are age-independent, the cdf of converges in probability as to: (38) proof: consider the on/off staleness process in fig. 5(a) and suppose the query time falls in the on period. then, since is uniformly random within this cycle, the backward delay is symmetrical to the forward (residual) delay, meaning they have the same distribution. note that it is important to condition on since residual depends on age, i.e.,"
"to explain how optimization with can be used, we assume constant and, with the goal to maximize . solving (66), the optimal download rate of each page is proportional to the square root of :"
"since is uniform in, the probability that it falls into an interval of length is simply : (12) where the upper limit is reduced from to since for . recalling that all probabilities and expectations are dependent on the sample path, it follows that is a random variable. our goal below is to show it converges to a constant as . to this end, first observe that it can be bounded as: (13) where we use the fact that for all and . next, notice that (5) implies that for all bounded, continuous functions [cit] : (14) which leads to:"
"replicas operate independently of the sources and perform one of the two general functions shown in the figure-manyto-one aggregation in part (a) and one-to-many replication in (b). the former case arises when the replica executes certain processing on multiple objects to provide the consumer with results that cannot be obtained otherwise. these applications include search engines, data-centric computing, and various web front-ends that cache queries against back-end databases. the purpose of the latter case is to handle failover during source crashes and/or ensure scalable load distribution under heavy customer demand. applications in this category include cdns, large websites, data centers (e.g., amazon ec2), and various distributed file systems."
"while theorem 1 establishes when has a limiting distribution, convergence of expectation or suitability of for computing it are not guaranteed. furthermore, given that consumers may apply generic weights to the various age-related metrics, it is important to identify when exists as . to build intuition for the next result, assume is a non-negative variable and define its age to be a random variable with cdf in (9) . then, we are interested in the relationship between and . to this end, suppose for any locally integrable function, we set and then recursively integrate the result times to define:"
"this creates a problem, however, because exponential requires noticeably more overhead than constant to achieve the same staleness penalty. for example, using our model for and discussion after (54), this difference is by a factor of 2 for and by a factor of 6 for, which shows that a distributed cluster of replicas may need to consume 100-500% more bandwidth than a centralized solution for a given level of qos (quality-of-service)."
"a commercial prototyping board [cit] (fig. 6 ) based on this processor has been used to test the open svc decoder and its performance has been measured. the board has 256 mb of sdram external memory, 256 mb of flash external memory and several interfaces. note that the clock frequency of the gpp and dsp cores is 600 mhz and 500 mhz, respectively."
"armed with these results, our second contribution is to model interaction between the age processes of . for staleness to be a function of inter-update delays, we discover that sample-path ages of both processes, examined at the same random time, must be asymptotically independent. interestingly, this condition does not automatically follow from independence of and, their stationarity, ergodicity, or even all three constraints combined. instead, we show that it translates into a form of asta (arrivals see time averages) [cit], where the download process must observe the sample-path distribution of update age."
"what is more important is the performance of the model in providing an accurate assessment of the download bandwidth needed to achieve a given . we invert the formulas to solve for as a function of and plot the result in fig. 10(a) . these results show a much more dramatic difference. for example, 20% staleness requires 95 downloads/day according to previous poisson models, while in reality this can be achieved with just 8. to illustrate this better, we show the ratio of these two curves in fig. 10(b), where the amount of poisson overestimation varies from one to almost two orders of magnitude depending on the desired ."
"under the condition of age-independence, our third contribution is to derive the distribution of time by which the replica trails the source, the fraction of consumers that encounter a stale copy, the average number of missing updates from the replica at query time, and the general staleness cost under all suitable penalty functions . our results involve simple closed-form expressions that are functions of limiting age distributions of both processes."
"since both bounds in (13) have the same limit, converges in probability 1 to the ratio of (15) to (16): (17) where the second integral follows from expanding the function and integrating by parts."
"let be an indicator variable of and the complementary cdf (cumulative distribution function) of . we are now ready to summarize our discussion. definition 4: a process is called age-measurable if: 1) for all, except possibly points of discontinuity of the limit, sample-path distribution of variables converges in probability as : (5) 2) function is deterministic with mean ;"
"theorem 1: process is age-measurable if and only if is almost surely bounded and converges in probability to . proof: we start with the forward (sufficiency) proof. consider an age-measurable and let be the -th arrival point of this process. note that almost-sure boundedness of immediately follows from (6) and the fact that . the rest of the proof deals with convergence of . assume some constant . then, event is equivalent to the existence of some such that belongs to the -th interval, under the condition that starting point and age . defining (10) where, we get:"
"the optimization techniques presented in subsection iii.c have been applied to the dsp-based decoder implementation. each layer of the sequences described in subsection iv.a has been decoded and the profile data of each module has been analyzed. table i presents the average performance improvement achieved in each of the optimized modules for the foreman quality-spatial sequence. the entry \"others\" in table i includes functions optimized for bit-stream parsing, intra-prediction and motion vectors storage. to obtain these measurements, each layer of the sequence foreman, encoded with the parameters presented in subsection iv.a, has been decoded with a decoder that encompasses all optimized versions of the modules shown in table i . in table ii, the average performance improvement percentage per module and sequence layer is shown. these values have been obtained as follows. first, different optimized decoder versions that comprehend optimizations for only one module have been generated. afterwards, each layer of the quality-spatial foreman sequence, similarly encoded with the parameters presented in subsection iv.a, has been decoded with each optimized decoder. at last, the average number of cpu cycles per layer frame is compared to that of the non-optimized decoder."
"it is worth noting that the gap between the module and global improvements of the ported decoder is mainly due to data-cache misses and the increasing number of cpu cycles employed at the frame-by-frame decoding phase. furthermore, the motion compensation process achieves smaller ameliorations than could be expected when the dma is in used. in addition, the optimized open svc decoder accomplishes higher enhancements for quality-spatial test sequences than for spatial-quality ones. finally, the main performance decrease at increasing bit rates is observed when decoding base layers."
"we assume a model of distributed data generation, replication, and consumption shown in fig. 1 . during normal system operation, sources sustain random updates in response to external action (e.g., new posts on facebook, traffic congestion in google maps) or possibly some internal computation (e.g., mapreduce [cit] indexing with periodic writes to disk). in either case, each update represents certain non-negligible information that manipulates the current state of the source."
"h.264/svc specifies three types of scalabilities: spatial, temporal and quality. in a temporally scalable video sequence, several frame rates (temporal layers) of a video sequence can be chosen when decoding. fig. 1 shows an example of a group of pictures (gop) where the user can select three frame rates. if the device decodes the four frames of the gop (i1, b1, b2, b3), a full-frame-rate sequence will be obtained. if the decoder discards b1 and b3 frames and only decodes i1 and b2, a half-frame-rate sequence will be achieved. the third case is a quarter-frame-rate sequence, which will be obtained when the decoder discards b1, b2 and b3 frames and only decodes i1. in a spatially scalable video sequence, several spatial resolutions (spatial layers) of the video frames can be chosen when decoding. fig. 2 depicts an example of a spatial scalable bit-stream containing three possible resolutions. as can be seen, the information related to the three resolutions of a frame is contained in the field reserved for such frame in the bit-stream. in a quality-scalable video sequence (or signal to noise ratio -snr-sequence), it is possible to select several quality levels (quality layers) when decoding. fig. 3 shows an example of a quality scalable bit-stream with three qualities. the information related to the three qualities of a frame is contained in the space reserved for this frame in the bit-stream. finally, the three types of scalability specified in h.264/svc can be combined into a unique bit-stream. as an example, consider an encoded video sequence that has three temporal layers, three spatial layers and three quality layers. an h.264/svc decoder that has a medium charged battery may decode, for instance, the third spatial layer to get full spatial resolution, the second temporal layer to get half temporal resolution and the first quality layer to get a lowquality level. a decoder that has a fully charged battery might decode the complete bit-stream to get the full temporal and spatial resolution as well as the higher quality."
"a set of tests has been carried out to verify the decoder conformance and to characterize its performance using different combinations of scalability values and bitrates. a block diagram of the test-bench is shown in fig. 10 . as can be seen, first, a test stream is read from a file and written into a stream buffer allocated in external memory. then, the decoder reads the stream from the memory and decodes it on a picture basis. at last, the decoded picture is written into a buffer and also into a component yuv video file. the test-bench has been executed in the prototype board used in pccmute project (see fig. 6 ). in order to assess the decoder performance with the test-bench depicted in fig. 10, six well-known video sequences (akiyo, coastguard, flower, foreman, mobile and news) have been encoded using a commercial h.264/svc encoder [cit] . the following subsections summarize the generated sequences."
"(66) where refers to either or . for and certain choices of, solutions to (66) using cost are known to completely starve frequently modified sources in favor of those that are updating slowly [cit] . since (66) does not have a closed-form solution under even in the simplest cases, specific conditions for starvation are not clear. complete loss of synchronization for sources whose is above some (typically unknown) threshold may be an unwelcome surprise for many applications. this naturally leads to the question of whether suffers from the same drawback. we address this next."
"consider extrapolating these results to sources and keeping the expected consumer lag below updates. we use the two models above as lower/upper bounds on the actual search-engine crawl rate. the first case requires download capability thousand pages per second (pps), while the second one million pps. for and 25 kb per page, these translate into 2 and 92 gbps, respectively. results can be easily adjusted to non-wikipedia situations as long as and are known."
"the issue of redundant replication from a single source, as in fig. 1(b), to nodes is quite different from the opposite case considered in the previous subsection. when the source fails, suppose the goal is to deduce the expected penalty afforded by the freshest member of the entire collection of replicas. the issue at stake is how this case compares to a single replica with some refresh rate and optimal . to keep comparison fair, assume that each of the replicas is allowed budget in synchronization with the source. decentralized operation leads to much better robustness under failure, but is it possible that this causes reduced freshness? if so, what is the amount of extra download bandwidth needed to keep both scenarios equally stale?"
"we next model interaction between a single source and a single replica, which is a prerequisite to understanding system performance. suppose the source undergoes updates at random times and define to be a stochastic process that counts the number of updates in . when referring to the entire process, rather than its value at some point, we omit and write simply . for the replica, denote its random download (synchronization) instances by and the corresponding point process by . this formulation neglects processing delays and treats all events as instantaneous. we additionally assume that both processes are simple and independent. now, suppose the inter-update delays of are given by a random process and inter-download cycles of by, which are illustrated in fig. 2 . each of these sequences may be of fairly general nature, e.g., correlated and/or non-stationary."
"motivated by (54) and consistently worse performance of pareto, the goal of this section is to understand the impact, if any, of on penalty and determine whether there exists an optimal distribution that, for a fixed download budget, provably results in the lowest cost for all and all suitable functions ."
"as information evolves at the source, which we call data churn, the replica may become stale and provide responses to that do not reflect the true state of the system. in such cases, we assume that user satisfaction and system performance are directly rated to the amount of time by which the replica is lagging behind the source. to convert time units into cost, suppose the application applies some weight function to the age of stale content to determine the penalty associated with a particular refresh policy and data-churn process. then, the goal of the system is to optimize the expectation of penalty observed by a stream of arriving customers."
"we start with general concepts from economics and game theory that are useful for understanding optimality. for two nonnegative random variables and, let their cdf difference be: (58) whose generalization is given by (24) . then, we have the following definition."
"contrary to the jsvm which decodes the upper layer of a given scalable bit-stream, i.e. the enhancement layer with the highest spatial, temporal and quality scalability, the open svc decoder can partially decode the bit-stream until a specific layer is required with a specific temporal scalability. this particularity provides an adaptability of the decoder over different platforms by selecting the right layer in order to have a real-time decoding."
"in fig. 5, a simplified flow chart diagram of the decoding process for an h.264/svc compliant bit-stream is shown. the decoder reads the h.264/svc bit-stream from an input buffer and decodes the nal units in sequence. after decoding the nal header, the nal unit content is identified as a slice header or another syntax element (i. e. a sequence parameter set -sps-or a picture parameter set -pps-). when the nal unit contains a slice of interest for the selected layer, the decoder extracts all the syntactical elements from the bit-stream and stores them in intermediate buffers. if the processed nal must be displayed, each macroblock (mb) is completely decoded, however, if the nal must not be displayed the mb is partially decoded."
"moreover, the dma controller has been used to improve the data transfers between internal and external memory during the motion compensation process. the data used in the mb loop (\"mb fully decoded\" block in fig. 5 ) are allocated in internal memory to increase the execution speed. the reference data pointed by the motion vectors are moved from the reference picture buffers to a buffer in internal memory (ref_y). the prediction is added with the residual mb and stored in a ping-pong buffer (rec). to move the reference and reconstructed data from/to external memory to/from internal memory, explicit dma transfers are used. the fig. 9 shows the buffers allocated in internal memory for the motion compensation process."
"more sophisticated cases are also possible. suppose the source runs some computation, with updates representing certain intermediate states that are written to disk. a crash at time requires computation to be restarted, which means that the penalty is determined not by, but rather by the duration of the computation that was lost due to staleness. services that charge per cpu time-unit (e.g., amazon ec2) may want to optimize against this metric rather than . furthermore, if the difficulty of recovering each update from other storage is proportional to the delay since the update was made, then staleness cost may be based on the combined lag of all missing updates at time ."
"finally, table v shows the influence of the bitrate in the decoder performance. as described in subsection iv.b three quality-spatial streams has been generated with different bitrates (0.5, 1 and 2 mbps) using the same parameters for the encoder configuration. the sequence foreman has been used to analyze the decoder performance."
"therefore, in order for to exist, one must ensure that both and do. note that the latter does so by (6), but the former requires an additional constraint. note that age-measurable by a constant is equivalent to simply age-measurable since in that case (26) becomes (6) . we omit the proof of the next result as it follows that of theorem 1 pretty closely."
"where the limit is a proper cdf from prohorov's theorem [cit] . what remains to prove is that limits (18), (21) are independent of the subsequence and establish their relationship to . using an analog of (14) for subsequences and applying (15): (22) invoking (19), this limit equals . assuming is some cdf, a function can be represented in the form of (9) using a unique pair . therefore, must be, which shows that for every subsequence there exists a further subsequence such that and . but this means that the full sequence and, i.e., (5), (6), and (9) hold. to elaborate on this result, consider independent variables: wp wp (23) whose limiting distribution in (5) is a constant with . however, in (6) converges to 2. consequently, the distribution of age cannot be determined based on . even worse, (9) suggests the age is uniform in, while is asymptotically finite only with probability ."
"the paper introduced a novel model of sampled age under general non-poisson update/synchronization processes and applied it to obtain many useful metrics of staleness. we additionally established that constant inter-refresh intervals were optimal for all considered cases and provided guidelines for achieving asta even in those cases. we finally considered a family of related problems stemming from and replication, showing that they can be easily solved from the preceding analysis of the 1 1 case."
"the library contains also several mechanisms to switch of layer during the decoding process which allows the user to select the layer to display by specifying commands. in the case of a partial decoding of a bit-stream, the decoder will dismiss discardable layer. fig. 4 shows the dataflow graph of the decoding process when the top layer of a 4-layer stream is not decoded. variable length coding and texture decoding are processes for the first three layers but not for the fourth. the pc-version of the open svc decoder has been compared to the jsvm 9.19 to benchmark and to test the conformance of the library using conformance test sequences. the benchmarks were executed on a pc with a dual core processor at 2.4ghz and show the speed up between the open svc decoder and the jsvm decoder on several conformance test sequences with different configurations. indeed, the performance of the library is up to 14 times faster than the jsvm decoder [cit] ."
"we finish the paper with our last contribution that considers the practical aspects of staleness, including experimentation with wikipedia page updates, error analysis of previous poisson models, estimation of search-engine bandwidth requirements, and generalization to multiple sources/replicas."
"for example, produces the first two metrics discussed above, i.e., via and via . both (1) and (2) are random variables, which suggests that system performance should be assessed by their average values. but as neither nor is assumed to be stationary, the expected penalty requires additional elaboration. instead of considering and, which may depend on time, it is more natural to replace them with sample-path averages [cit] : (3) where consumers are modeled as being equally likely to query the replica at any time in ."
we now examine the presence of poisson updates in real data sources and show how to apply the developed models to solve several classes of multi-source/replica problems.
"the main caveat in solving this problem is that staleness at different replicas is no longer independent. this happens because updates at the source simultaneously make all copies outdated, which means that reliability does not benefit exponentially with increased . to overcome this issue, let be the download processes used by the individual replicas. then, observe that the entire collection can be replaced by a single replica that implements a refresh pattern, which is a superposition of all point processes . therefore, the source can be recovered during the crash with a probability determined solely by . if we assume centralized scheduling between the replicas, then it is possible to run the system optimally (i.e., using a perfectly spaced out round-robin) and thus keep the overall penalty exactly the same as with a single replica. under fully decentralized (i.e., independent) replica operation and, each rate and thus likely converges in distribution to a poisson process with rate (palm-khintchine theorem [cit] )."
"in near future the work will be focused on two lines. the former consists in the distribution of data and code in the different levels of memory and the flow chart reorganization to reduce the number of cache misses, while the latter will concentrate in evaluating the correlation between the decoded layer and the dsp energy consumption."
"since source penalty sums up the ages of all missing updates, it allows usage of decaying functions such that their integral is increasing. we demonstrate this effect using, for which . this cost function increases rapidly for small, but then becomes less sensitive to staleness as the age of replicated content grows. since, constant yields:"
"in addition, a set of sequences has been generated in order to evaluate the influence of the bitrate in the decoder performance. the foreman sequence has been selected to analyze the dependency between the bitrate and the decoder performance. the sequence has been encoded as a qualityspatial stream using the codec parameters described in subsection iv.a. table v provides the performance results for three different bitrates (0.5 mbps, 1 mbps and 2 mbps) and in subsection v.c some conclusions are derived."
"while sampling constant update cycles with constant synchronization intervals sometimes leads to phase-lock, we next discuss how to achieve asymptotic age-independence in such cases. to build intuition, suppose and for all . then, from the equidistribution theorem, is a uniformly random variable in, meaning that has the same distribution as . the key observation is to ensure that puts its download points uniformly across the cycles of . in general, sequence, where is rational, is irrational, and, is uniformly distributed in, in which case for any riemann-integrable function, an asta-like condition automatically holds:"
"(65) while using to sample works well, there is a possibility that itself happens to be a multiple of . to preclude these cases, must exhibit enough randomness to prevent from becoming deterministically an integer. one option for doing so is to require that either process employ non-lattice cycle lengths. recall that non-lattice distributions may be entirely continuous, including the classical pasta (poisson arrivals see time averages) [cit] and the uniform distribution often suggested for network measurement [cit] . however, they can also be entirely discrete. in such cases, cycle lengths must distribute mass across at least two values, where is irrational, e.g., pairs or . by bringing spread closer to zero, it is possible to obtain a variety of approximations to the optimal (constant) synchronization delay with mean . note that a non-lattice distribution may still enter phase-lock if its cycle lengths follow a deterministic pattern, e.g., both updates and downloads strictly alternate between 1 and . to rule out these cases, it is sufficient to require that the non-lattice process randomize its delays, leaving the other one general. this produces the following."
"expanding leads to the result. to perform a self-check against prior results with poisson, observe that (36) simplifies to under constant and under exponential, which are consistent with [cit] . simulations in fig. 4 examine model accuracy in more interesting cases of general renewal processes. we use pareto cdf with and mean . observe in the figure that the model matches simulations very well, with constant download intervals performing significantly better against pareto update cycles in (a) than the other way around in (b). for example, synchronizing pages at their update rate (i.e., ) serves stale copies with probability 33% in the former case and 66% in the latter. furthermore, for the same, the scenario in (a) requires roughly 4 times less bandwidth than in (b)."
"proof: suppose is the fixed mean of all distributions under consideration. let be the cdf of a constant and be the cdf of another random variable such that . our goal is to show that . when, we have trivially:"
"on sample-path staleness in lazy data replication convolutions of single-hop notification delays. in other cases, however, scalability and administrative autonomy require that sources operate independently and provide information only based on explicit request, especially when they are unable to track their replicas or adopt modifications to existing protocols."
"the final element of fig. 1 is the consumer, which sends a stream of requests that represent either queries for information or attempts to recover the most-recent state of the source after it has crashed."
"in this paper, a dsp implementation of a real-time h.264/svc decoder is explained. the h.264/svc standard and the open svc decoder are outlined in section ii. in section iii, the dsp decoder implementation is described. in section iv the test-bench created to measure the decoder performance is outlined. the results of the profiling tests are discussed in section v. finally, section vi concludes the paper."
"our fourth contribution is to analyze conditions under which produces provably optimal penalty for a given download rate. we show that penalty reduces if and only if inter-refresh delays become stochastically larger in second order. this leads to constant synchronization delays being optimal under all and . this, however, presents problems in satisfying asta and creates a possibility of worst-case (i.e., 100%) staleness due to phase-lock between the source and the replica. to this end, we discuss broad requirements for ensuring that avoids these drawbacks while remaining optimal."
the preceding results set up motivation to ask the question of whether there exists a distribution that dominates all others in second order. we answer this next.
"in pccmute 1 project, our research is focused on the energy and power consumption control in multimedia terminals. a multimedia terminal prototype with a dvb-h receiver, an h.264/svc decoder and an audio decoder is going to be used to validate the experiments. in this context, the h.264/svc decoder will be used to achieve a trade-off between user quality of experience (qoe) and energy consumption [cit] . the multimedia terminal architecture is based on a commercial chip [cit] having a general purpose processor (gpp) and a digital signal processor (dsp). the"
"in the next step, if a frame has been completely decoded, the deblocking filter is applied. finally, the decoded pictures are stored in images buffers and presented in the right order using the pc simple direct media layer (sdl) library."
"the columns of table ii present the layers included in the sequence where s indicates the picture size of the frames, t the temporal resolution in frames per second and q the level of quality (high or low). the rows present the percentage of improvement achieved when an optimized module is integrated. finally, the last row shows the global improvement when all the optimized modules are integrated. the two following conclusions can be drawn from the analysis of table i and table ii: first, table ii shows that the global improvement achieved is not the addition of the improvement of each module. the reason of this loss is that the allocation of the modules in memory changes after each optimization and the number of data-cache misses increases when the code is optimized."
"definition 2: for a stale replica at time, define lags to be backward delays to each unseen update, i.e, . this concept is illustrated in fig. 3(a) for the first two lags. to keep the model general and cover the various options already seen in the literature [cit], we assume that the consumer is sensitive to either just lag, i.e., how long the source has been stale at time, or the entire collection of lags, i.e., how long each uncaptured update has been stale. since it is usually difficult to predict the value of information freshness to each customer, one requires a mapping from staleness lags to actual cost, which we assume is given by some non-negative weight function . definition 3: at time, the source penalty is given by the weight of the delay since the replica was fresh last time: while the update penalty is given by the aggregate weight of all staleness lags:"
"the gpp processor has two levels of cache memories (l1 and l2). the program (l1p) and data (l1d) caches, within the microprocessor unit (mpu) subsystem, consist of a 16 kb memory space. the l2 cache consists of a 256 kb memory space, shared between program and data. in addition, the mpu integrates a coprocessor, optimized for multimedia applications, with its own multiplication-accumulation unit (mac) and support for floating-point operations."
"first, the decoder performance is measured using the quality-spatial sequences. table iii contains the percentage of cpu cycles needed to achieve real-time processing out of those available, using the un-optimized and optimized decoder versions and for all layers. moreover, the percentage of improvement achieved for each layer is presented. these results have been obtained using a dsp running at 500 mhz."
"if and are age-independent, while is age-measurable by, the source penalty converges in probability to: (42) proof: first, observe that (43) where . working back from (38), the tail cdf of can be written more compactly as: (44) since, conditioned on, it suffices that only be measurable by . in that case: (45) or equivalently:"
"the m i results are stored in 16 bits variables. package instructions can be used to store two of them in a 32-bits variable. m 12 and m 34 variables store the packed data. using the addition (_add2) and right shift (_shr2) instructions is possible to calculate the average between m 1 and m 3 and, m 2 and m 4, at the same time. interpolated pels i 0 and i 1 are obtained after these operations with 16-bit resolution and packed in a 32-bit variable. these interpolated pels must have a resolution of 8 bits, so four pels (from i 0 to i 3 ) can be packed into a 32-bit variable. after the optimization process, the algorithm needs only 12 instructions (2 loads, 4 products, 3 packages, 1 addition, 1 shift and 1 store). a similar methodology have been applied to cabac entropy decoding, frames upsampling, motion compensation, iict, coefficients interpolation and deblocking filter. in subsection v.a, table i presents the average improvement achieved in all the optimized modules."
"it is pretty clear that (5) is not implied by (6) . if are uniformly bounded, the reverse can be inferred (i.e., (6) follows from (5)); however, this does not hold universally. in fact, many random variables used in practice (e.g., exponential and pareto) are not bounded and thus require an explicit assumption that convergence in (6) take place. additionally, even if this limit exists, it does not generally equal, which is why we require that as well."
"if since is non-increasing, larger implies larger . finally, since for all, it follows that its inverse has the same properly and thus no positive can achieve . this means the optimal allocated rate must be strictly positive (i.e., no starvation)."
"two different types of test sequences have been generated to evaluate the influence of the specific layers embedded on the stream in the decoder performance. for each type of set, sequences that consist of six layers extracted out from the eight possible combinations among two spatial resolutions (qcif and cif), two frame-rates (12.5 and 25 frames per second) and two qualities (low and high) have been generated. furthermore, the bitrate of these sequences is 512 kbps and the base layer of each sequence has been encoded with 102 kbps (20% of a total bitrate of 512 kbps)."
"we start by performing a convenient transformation of (3) to remove the integrals. define to be a uniform random variable in, which models the random query time of consumers. suppose is independent of and, in which case (3) is the limit of and as . to keep formulas manageable, we sometimes omit explicit conditioning on processes ); however, it should be noted that all expectations and probabilities are still computed within each sample path (i.e., with respect to only). at time, suppose age processes and, shown in fig. 3(b), specify delays to the previous update and synchronization event, respectively. using this notation and observing that is equivalent to, define an on/off staleness process:"
"the cpu percentage needed to achieve real time performance is presented for all the layers included in the bitstreams. moreover, the percentage of increase in the number of cpu cycles needed to decode the 1 mbps and 2 mbps streams respect to 0.5 mbps stream is shown. all the results have been obtained after decoding 100 frames. the results presented in table v show that the bitrate has a higher influence in the decoder performance for the base layer than for rest of the layers. moreover, the increase in the number of cpu cycles needed to achieve real time performance is not linear with the bitrate, if the bitrate is doubled; the number of the cpu cycles increase in about 15%."
"noticing that (34) 2 a random variable is called lattice if there exists a constant such that is always an integer. and applying theorem 1, we get using (30): (35) which is (32) with the two sides swapped."
"from this and definition 7, it follows that implies and vice versa. as given by the next lemma, first-order stochastic dominance allows one to determine the relationship between expected utilities and . while it is possible to establish a more general version of this result using -th order dominance, it would restrict to a narrower class of functions and thus would be less useful in practice."
"when a single replica tracks sources, as in fig. 1(a), performance is assessed by its ability to provide usable aggregate information to the consumer. if sources are independent, many results are relatively easy to obtain. for example, consider a system that selects a replica and loads it with a mapreduce job that has to execute over the data of all sources. a computation may be considered successful if at least one source is fresh at the time of job request. then, the fraction of successful attempts is, where in (36) is the probability of staleness for source . alternatively, application consistency may require that all sources be simultaneously fresh, which leads to the probability of success via . a more interesting problem is optimal allocation of download rates to different sources. suppose is the probability that an incoming query requests data from source and is its update rate. then, the goal is to allocate refresh rates so as to optimize the expected staleness cost for a given bandwidth budget :"
"we first discuss possible reasons for the frequent use of memoryless source-update processes in the literature. if indeed this is universal, extensions to non-poisson dynamics may be unnecessary. while modeling convenience is one possible explanation [cit], there is certain belief in the field that updates to individual web pages can be accurately described by a poisson process, which has fueled this line of modeling for over a decade [cit] ."
"intuitively, there is no fundamental reason why a single source should exhibit poisson dynamics, especially when modified by humans. a more likely scenario would be heavy-tailed behavior observed in many areas of computer networks [cit] and user-driven distributed systems [cit] . another intuitively reasonable inter-update distribution is constant, where certain information is injected into the system periodically by design or is obtained from an on/off source (e.g., sensors trying to conserve energy)."
"even though certain measurement studies [cit] have found non-poisson updates among web pages, they also lack ground truth. these pitfalls can be avoided if model verification is performed over sources that expose information about each update. one particularly interesting source with public traces of all modification timestamps is wikipedia [cit] . from a search-engine perspective, this website represents a realistic example of data churn stemming from user interaction with each other (e.g., edits from other people), flash crowds in response to external events, and diurnal activity patterns of the human lifecycle. wikipedia is also well-suited for purposes of model validation and discussion."
"in the last years, a speed-up in the deployment of all kinds of telecommunication networks supporting multimedia services and applications has been produced in many parts of the world. in this context, the consumer multimedia terminals play a central role. in these terminals, video decoding is one of the most demanding tasks in terms of computational load and energy consumption."
"the poisson assumption on allows easy computation of the various metrics of interest. outside these special cases, superposition of non-memoryless processes produces much more complex behavior."
"the scalable video coding (svc) techniques [cit] can be used in multimedia terminals to achieve a trade-off between quality and energy consumption. though svc techniques have been defined in most video coding standards [cit], the svc capabilities included in h.264 [cit] have overcome the ones in former standards."
"we now return to examining (4). in order to determine when the replica is stale, one requires comparison of with, which may not be independent random variables, even if and are. to prevent such cases, which are called phase-lock [cit] unconditionally, however, the ages are independent:"
"since and are age-independent, we can condition on without impacting the distribution of or . following the proof of theorem 1, define and to be the lower/upper boundaries within synchronization interval such that if, then and . see fig. 5(b) for an illustration."
"to understand the penalty of outdated content, suppose counts the number of updates missing from the replica at time (e.g., in fig. 2, ). this is a discrete-state process that increments for each update and resets to zero for each synchronization. definition 1: a replica is called stale at time if . otherwise, it is called fresh."
"for example, pareto produces in (54): (55) which for is quadruple that of constant and double that of exponential . another peculiar case is, where tends to infinity regardless of . in fact, the update process itself may exhibit, but the expected number of updates by which the replica falls behind will still become unbounded as approaches 2."
it is well-known that a renewal [cit] or regenerative [cit] assumption on yields as . our next result produces a condition that is both sufficient and necessary for this to hold.
"-an effective pcm write algorithm called maxpb. we observe that existing write schemes are not aware of low power budget utilization and experimental results of 10 multithreaded and six multiprogrammed workloads show that the utilization is only 27.4% without power asymmetry. with maxpb, we can maximize power budget utilization by using the least number of write units. -a variation of maxpb named maxpb-asy. we observe that the power budget utilization decreases to 14.9% considering the power asymmetry of set and reset. by leveraging power asymmetry, maxpb-asy can obtain more performance improvement and energy consumption reduction compared with the original maxpb scheme. -efficient hardware circuits design to support maxpb and maxpb-asy. the circuits are slightly altered with extremely low overhead compared with fnw and two-stagewrite designs."
the area under roc (auc) is computed on the observed scale and is a measure of the efficacy of prediction of phenotype using a test classifier. the receiver operating characteristic curve is a plot of sensitivity and specificity.
"the experimental results of power budget utilization are shown in figure 9 . maxpb and maxpb-asy show much higher utilization than fnw. maxpb earns 46.9% power budget utilization and maxpb-asy shows more than 40.2% utilization on average. in comparison, fnw gets only 27.4%/14.9% power budget utilization without/with power asymmetry on average. one workload shows more than 80% utilization (facesim) when using the maxpb method and five workloads indicate more than 70% utilization (dedup, ferret, freqmine, streamcluster, and vips). one workload exhibits more than 70% power budget utilization (facesim) when using maxpb-asy write scheme. the reason is that our designs package more data units into one write unit and the power supply is fully used in these benchmarks. under the same dataset, higher power budget utilization means more data bits are written in parallel with higher current use efficiency and write parallelism, which reduces the write service time of pcm-based main memory."
"energy consumption is an important issue in both current big data centers and smart devices, such as smartphones, pads, etc.. in data centers, high-energy consumption put servers into security risks caused by heat, and numerous refrigerating devices are deployed to cool down the whole data center, which uses millions of watts of power per year. in smart devices, high-energy consumption leads to the reduction of use time and we have to charge the devices anytime anywhere, which leads to the decline of usability. energy consumption reduction can bring significant benefits both to the environment and the economy. even though maxpb and maxpb-asy do not reduce the amount of written data, that is, decrease the dynamic write energy consumption compared with fnw, our design can significantly reduce the cache line service time, the queueing time of each request, and hence reduce the standby power of the main memory system. as shown in figure 15, maxpb shows potential in energy consumption reduction. compared with the dcw baseline, maxpb shows 73.7% energy consumption reduction. compared with the state-of-the-art fnw and two-stage-write schemes, maxpb outperforms them by 23.3% and 11.1% on average, respectively. moreover, maxpb-asy outperforms the fnw and two-stage-write by 30.6% and 18.4% on average, respectively."
"in this paper, we describe a novel hybrid approach that connects binary animal migration optimization with random forest (rf) algorithm to predict the extracellular matrix proteins via four various features represents. binary animal migration optimization is a new heuristic optimization algorithm inspired by the behavior of animal migration that is utilized to select a near-optimal subset of informative features that is most relevant for the classification. amorf experiments on a dataset including 145 ecm and 3887 non-ecm proteins. our method performs 86.4700% accuracy, a sensitive of 84.9655%, a specificity of 86.5261%, an mcc of 0.3627 and an auc of 0.877804. the results show that the proposed method is promising. it can select small subsets of features and still improve the classification accuracy."
"maxpb write logic gets the data to be written together with flip bits and wup bits, and then decides which data units should be packaged into one write unit. the write control logic is illustrated in figure 6 . the shared finite state machine (fsm) is the key component of our maxpb write logic design. fsm consists of data units selection logic and write signal generation logic. in detail, fsm decides which data units should be packaged into one write unit to get the maximum power budget utilization with the minimum number of write units. in order to boost the accessing efficiency, all data units, that is, dx in figure 6, are indexed by fixed offsets. offset 0 corresponds to data unit 0 (d0) and d1-d7 are indexed by 17, 34, 51, 68, 85, 102, and 119 considering the flip bits, respectively. the units packaging and choosing processes deliver extra overhead when finishing a cache line write service."
"random forest algorithm is a well-known data processing method for classification, which works by building a multitude of decision trees at training step and generating the class [cit] . this algorithm also performs a kind of crossvalidation based on an out-of-bag sample. in the training process, the algorithm uses different bootstrap sample to create each tree from the original data."
"the conventional write scheme is under pessimistic assumptions about the power demand of each unit, regardless of the power and time asymmetries in pcm write. typically, as shown in equation (1), it strictly writes s writeunit bits per write unit until finishing the write service of s total bits. as illustrated in figure 2, the service is finished at t5 under the parameters presented in table i ."
"existing state-of-the-art pcm write schemes, such as fnw (flip-n-write) or twostage-write, aim to address the poor performance problem by improving the write parallelism under the power constraints. the parallelism is obtained via reducing the data amount with ingenious data encoding and leveraging both power and time asymmetries, respectively. however, we observe the following: -fnw is quite simple and effective but faces low power budget utilization problems."
"in animal migration optimization [cit], there are mainly two processes: migration process and population updating process. suppose that we have a discrete optimization problem in a binary space and a population of candidate individuals. the individual is denoted by a d-dimensional real coded vector."
the data units packaging algorithm is shown in algorithm 1. the key idea behind the maxpb algorithm is to decide which data units should be executed in parallel under power constraints to earn better power budget utilization and lower number of write units. maxpb is a greedy algorithm and the key working processes are as follows:
"in the final, the proposed four amorf methods are compared with the well-known classifier ecmpred. we obtain the result of ecmpred from the paper [cit] . we use the test benchmark dataset including 145 positive and 3887 negative sequences to test all compared algorithms. the compared results are tabulated in table 7 . from table 7, we can find the amorf4 can give the best sensitive solution."
"the write driver logic is shown in figure 6 . like fnw and two-stage-write design, an extra control signal, called prog-enable, is used for individual bit control. once fsm sends a writing signal to a data unit dx, the write driver logic compares the data dx with the data already stored in the pcm array, that is, 16 bits old dx, by leveraging a simple xor gate logic. a dmux gate logic is used for deciding the stage of writing \"0\" or \"1,\" that is, reset and set enable, respectively."
"we set the value for w a between 0.6 and 0.9, which is the weight of the classification accuracy of random forest algorithm. after that, the w b is equal to 1 − w a . as we know, these two different weight is relative to the effect of the accuracy of random forest algorithm and the number of chosen features, respectively. because the accuracy of random forest algorithm is very important compared with the number of chosen features in our paper, the w a is larger than the value of w b . rf accuracy is the results of random forest algorithm. d is the dimension of the original data, and r indicates the number of features, which is chosen by the proposed algorithm."
"pcm takes advantage of the properties in resistance of the storage material (such as ge2sb2te5, briefly called gst). the material shows great diversity and resistance gap when the material shows different states, that is, crystalline state and amorphous state. in general, amorphous-state material shows several orders of magnitude higher resistance value than crystalline-state material. if given the same voltage level, the current value adopted by the sense amplifier varies four or more orders of magnitude when the material shows opposite states. the striking differences in resistance and current status can be used for presenting binary information, that is, digital \"0\" and \"1.\" a typical mushroom structure of a pcm cell is illustrated in figure 1 cell typically adopts mushroom architecture including electrodes, heater, and phase change material. simplified read and write mechanisms and current-time sparkline of pcm are illustrated in figure 1 (b) . pcm has obvious asymmetries [cit] :"
"in order to address the poor write performance problem, fnw [cit] ] uses a thin read-before-write scheme, encodes the data with an extra bit to reduce the amount of written data, and improves the write parallelism by writing the different bits only. in general, the concurrent serviceability is doubled and the write service time is halved compared with the conventional write scheme, as shown in equation (2)."
"we give a new metric named power budget utilization to measure the efficiency of current supply utilization. the definition of power budget utilization is given in equation (4) and the term actual poweru se without/with power asymmetry is calculated by equations (5) and (6), respectively. in general, power budget utilization means the rate of actual power use per write unit and the maximum power supply provided by the charge pump. under the same dataset, higher power budget utilization means that more data bits are written in one write unit and higher chip-level write parallelism, which reduces the write service time of pcm-based main memory."
"to further enhance the parallelism of stage \"1,\" new data are inverted if the number of \"1\" bits is more than half of bits to be served. thus, the number of units that served in parallelism at stage \"1\" is doubled again under the same power constraint. there is no extra read operation overhead compared with the fnw scheme. power-token-based method [cit] ] improves the write concurrency by leveraging fine-grained power tokens management. under the power limitations of one chip or a set of pcm chips, memory controllers can send more write commands to pcm by leveraging the bank-level parallelism, and the overall latency can be significantly reduced. to monitor the actual power consumption of each write back operation, the llc is modified to record the count of different bits compared with data stored in pcm. fpb [cit] ] tries to migrate the power-token-based write scheme to mlc pcm. by combining with the special program-and-verify iterations, the write parallelism can be significantly improved and fpb can improve the overall throughput of mlc pcm. bit-mapping [cit] b] tries to make the data distribution among cell groups in a balanced way in order to get almost identical service time among different cell groups. three-stage-write [cit] tries to combine fnw with two-stage-write to get further parallelism improvement as well as write latency reduction."
"read is on the critical path of the whole system performance and short read latency could deliver good system corresponding time. maxpb and maxpb-asy can minimize the number of write units and hence reduce the read latency under conventional memory scheduling algorithms, such as fcfrfs-wqf in our experimental environment [cit] ] . figure 11 shows the read latency results of 16 benchmarks. in general, maxpb outperforms dcw, fnw, and two-stage-write in all benchmarks. however, the improvements of dedup, facesim, vips, and zeusmp are not significant because the power budget utilization is already high in those benchmarks as shown in figure 4 and there is not much room for parallelism improvement. overall, maxpb can get 67.7% read latency reduction compared with the baseline dcw on average, and shows 24.6% and 12.9% more read latency reduction on average compared with the fnw and two-stage-write, respectively. moreover, by leveraging power asymmetry of set and reset, maxpb-asy can get 32.0% and 20.3% more read latency reduction on average compared with the fnw and two-stage-write schemes."
"comparisons of existing pcm write schemes are concluded in table iv . in general, dcw, flip-n-write, two-stage-write, three-stage-write, power-token-based, maxpb, and maxpb-asy methods are designed for slc pcm devices. fpb is designed for mlc pcm and the bit-mapping method can be used for slc and mlc pcm. powertoken-based, fpb, and bit-mapping methods are designed at the memory controller level; they focus on write concurrency to pursue more write commands or requests be executed in parallel. however, other methods, like fnw, two-stage-write, maxpb, and maxpb-asy, are focusing on how to finish the chip-level write parallelism and how to write data from the on-chip buffer to pcm physical array quickly."
"in other words, only a small number of bits is changed and consumes power by \"removing the redundant bit-write\" method [cit], and the power is often excessively supplied but underutilized. according to our experimental results of 16 parsec and spec workloads, the power budget utilization is only 27.4%/14.9% without/with power asymmetry on average under fnw scheme and the system environment is shown in table i . -two-stage-write is highly efficient but suffers from performance degradation when the asymmetries are not significant. according to our theoretical analysis shown in figure 3, if set is four times (or more) slower than reset, two-stage-write gets significant write service-time reduction over fnw. otherwise, the write performance improvement is not that significant. moreover, two-stage-write needs extra control circuits and the modified write driver logics to support the separation of writing ones and zeros."
"maxpb can significantly reduce the total number of write units and the overall write service time of a cache line can be reduced. the results of write latency of all workloads are illustrated in figure 12 . in general, maxpb outperforms 44.9% more write latency reduction compared with conventional dcw on average. moreover, maxpb also shows 21.4% and 10.7% more write latency reduction compared with the state-of-the-art fnw and two-stage-write, respectively. maxpb-asy shows 26.5% and 16.1% more write latency reduction on average compared with fnw and two-stage-write, respectively. one workload (streamcluster) gets little improvement or even performance degradation. there are two reasons that lead to this result. on one hand, streamcluster is not a memory-intensive workload; there are fewer memory read or write operations. there are only a small number of write requests in this workload. on the other hand, read requests are prior to write requests under the high-performance schedule algorithm. all these reasons result in the longer write latency compared with the baseline."
"then, the accuracy of random forest algorithm is marked as the fitness of each animal. the value of the ith animal is calculated as follows:"
"(1) calculating the actual power requirements based on the number of bits that have to be written, that is, the number of different bits between new and old data. (2) rearranging all data units in descending order according to their actual power requirement. since there are only a few data units (eight data units in our study), the ordering and selection process is fast and efficient. end for 26: end for (3) from the data unit with the highest power requirement to the data unit with the least power need, trying to put a current data unit into an existing write unit and recording the power budget residue of each existing write unit. (4) if the remaining power supply of one write unit is enough, the data unit is marked that it should be written into this write unit. if all write units cannot satisfy the processing data unit, another write unit is enabled and the data unit is marked with the new write unit."
"dipeptide composition is popularly employed for reproducing the protein sequence [cit] . for each protein sequence, this method can generate 420 elements to represent the protein. in all elements, the conventional amino acid composition (aac) is denoted by the first 20 elements. therefore, for each protein sequence, the dipeptide composition method can be computed as follows:"
"in this section, we evaluate the effectiveness and efficiency of maxpb and maxpbasy write schemes and analyze the experimental results. maxpb and maxpb-asy can significantly improve the power budget utilization and reduce the number of write units, that is, significantly improve the write performance of pcm. the experimental results include power budget utilization, the number of write units, read latency, write latency, cpi speedup, running time, and energy consumption."
"the mcc is a correlation coefficient between the observed and predicted binary classifications, which returns a value between −1 and +1. the mcc can be described as follows:"
"where x neighborhood,g is the neighborhood of the current position i. in the population updating stage, some individual in the current population will migrate the current group. meanwhile, some new animals can find the current population to add it. in other words, some of the population will be renewed by some news with the probability p a . as we know, for the best individual, the probability p a is set to 1. for the worst individual, the probability is set to 1 np . after creating the new individual x i,g+1, it will be computed and compared with the original individual. the better value is stored in the population. in this algorithm, the binary value ''1'' expresses that the feature is chosen while the binary value ''0'' expresses the non-selected feature [cit] ."
"the write service time of two-stage-write is highly relative to the current ratio and time ratio, as illustrated in figure 3 . when the current ratio l is 2, the time ratio k is 8, two-stage-write finishes its cache line write service at t2 (two-stage-write i in figure 2 ). however, if the time gap between set and reset is not huge, the theoretical performance of two-stage-write is close to fnw. when the time ratio is 4, the write service is finished at t3 (two-stage-write ii in figure 2 )."
"there are many schemes focusing on the write performance improvement of pcm especially based on the power budget model. fnw [cit] ] is designed for write performance improvement on pcm. fnw uses a lightweight read-before-write scheme to reduce the amount of written data. fnw first reads the data to be written and encodes the data with an extra bit if the number of different bits is more than half compared with the original data. under the same power limitation, two times the amount of data can be written in parallel, that is, the size of the write unit is doubled. two-stage-write [cit] ] splits the pcm write process by taking into account the pcm write properties. the write is formed by two independent stages: stage \"0\" and stage \"1.\" in stage \"0,\" all zero bits in every unit are processed very quickly for reset is much faster than set. in stage \"1,\" all ones are served with improved parallelism since the current need of set a cell is only half of reset a cell."
"on one hand, fnw is simple and effective but faces low power budget utilization problems. in general, only a small number of bits is changed compared with original data (old data) and consumes power, and the power is often excessively supplied but underutilized. low power budget utilization limits parallelism upgrade and write performance improvement. [cit] workloads illustrated in figure 4, the power budget utilization is only 27.4%/14.9% without/with power asymmetry on average with fnw scheme and the system environment and benchmark details are shown in table i and table ii, respectively. on the other hand, two-stage-write is highly efficient but suffers from performance degradation when the asymmetries are not significant. according to our theoretical analysis shown in figure 3, if set is four times (or more) slower than reset, two-stage-write gets huge enhancement over fnw. otherwise, the performance improvement is not that significant. moreover, two-stage-write needs extra control circuits and modified write driver logics to support the separation of writing ones and zeros."
"in this paper, to show the effective and efficient of our proposed algorithm, we use the independent test dataset crossover-validation [cit] as the test method, which has been increasingly adopted and widely known by researchers to check the performance of different methods. the wrapper approach uses the inductive algorithm to estimate goodness of a given feature subset. a new swarm intelligent algorithm is employed for choosing some features that are most important for the classification. for the parameters, the population size of binary animal migration optimization is 15, and the stopping condition is 30 for the generation. four rf models with animal migration optimization (amorf) based on statistical factors (amorf1), chou's pseaac (amorf2), dipeptides composition (amorf3), and reduced amino acids with physicochemical properties (amorf4) are constructed respectively. because of animal migration optimization is a random algorithm; we run the algorithm for ten times for every amorf model. the experimental results are summarized in table 2 -5. as shown in tables, we can find the amorf3 can provide the best specificity, overall accuracy, mcc, and auc. for the sensitivity, the amorf4 gives the best solution; however, this feature method cannot perform well on three evaluation metrics including the specificity, overall accuracy, and mcc. meanwhile, as can be seen in table 4, the amorf3 can obtain 86.4700% accuracy using 127 average features with the sensitive of 84.9655%, the specificity of 86.5261%, the mcc of 0.3627 and the auc of 0.877804. this result shows that our binary animal migration optimization can reduce the correlated and noisy features. to demonstrate the impact of the four features represents, receiver operating characteristic curves is plotted by deriving from the false positive rate and true positive rate for the classifiers shown in figure 2 ."
"the remainder of this article is structured as follows. section 2 describes the background, the details of existing write schemes, and motivations of our designs. section 3 describes the implementation of circuit designs. section 4 presents and analyzes the experimental results. section 5 introduces the related work. finally, section 6 concludes our article."
we use the cpi (cycles per instruction) to perform the system performance measurement. we use the dcw's cpi as the baseline and define variable speedup as the following equation:
"in the migration part, three rules should be considered for an animal. for the first rule, we require that the position of each in the group should be different. for the latter two rules, we need that the individual should run to a new place according to the current situations of its friends. to illustrate the idea of the local community of an animal, we apply the ring topology method [cit] . when the ring topology has been built, one neighbor is selected randomly and then generated the position of the animal according to its neighbor:"
"the extracellular matrix (ecm) protein is constructed and covered by cells and builds an intricate extracellular meshwork in which cells are transferred to form tissues [cit] . in biology, the extracellular matrix (ecm) protein is the extracellular component of animal tissue which provides structural support to the cells. moreover, the extracellular matrix offers structural support for cells within a tumor affording anchorage for cells and departing tissues; however, it also serves homeostatically to mediate communication between cells and presents survival and signals [cit] . many proteins of the ecm is connected with cells via cell surface. the resulting focal associations are critical for the preservation of tissue architecture and for holding a kind of cellular processes. thus, extracellular matrix proteins provide numerous opportunities as therapeutic objectives or diagnostic markers [cit] ."
"based on these key observations, we propose a novel pcm write scheme named maxpb (maximize the power budget utilization) based on the slc pcm for its better write performance and intuitive power budget model [cit] ] . our design goals are to maximize the power budget utilization with minimum changes in the circuits design. maxpb is a \"think before acting\" method. the main idea of maxpb is to rearrange all data units according to their actual power needs and package all data units into the least number of write units under the power constraints. maxpb can significantly improve the write parallelism and reduce the critical number of write units."
"to meet the proposed design goals of maxpb, we implement altered hardware circuits based on an industrial pcm prototype published by samsung [cit] and fnw write scheme [cit] . the data path of our design is shown in figure 5 . the original prototype supports an eight-word prefetch method with a big buffer to reduce the data transmission overhead."
"maxpb circuit designs do not deliver any extra overhead on the critical read path, which is the key bottleneck of the system performance. moreover, maxpb can significantly reduce the number of the write units and improve the write performance with extremely low overhead."
"-asymmetries between read and write. the power needs and the service time of read and write vary significantly. in general, read operation consumes less power with shorter service time [cit] ]. -asymmetries between reset and set. the power needs and the write time of reset and set vary significantly. in general, set operation needs higher current level but its service time is much shorter than set operation [cit] ."
"to address these problems, we propose a novel pcm write scheme named maxpb based on the insight that existing pcm write schemes are unaware of their low power budget utilization. our design goals are to maximize the power budget utilization with minimum changes to the circuits design. maxpb rearranges the executed sequence of all write units and maximizes the power budget utilization with the least number of write units. the main idea of maxpb is \"think before acting,\" that is, we rearrange all data units according to their actual power needs and package more data in one write unit to minimize the number of write units rather than writing down the data directly."
"based on the pcm power asymmetry, we propose a new method called maxpb-asy to further improve the write parallelism. the primary difference between maxpb-asy and maxpb is the statistical approach of \"actual power use.\" there exists power asymmetry in pcm write that reset operation consumes much more current than set operation. the data path and write control logic of maxpb-asy are illustrated in figures 7 and 8. compared with 24 wup bits overhead in maxpb, the buffer of maxpb-asy needs 24 wup_reset bits and 24 wup_set bits to store the number of reset and set operations of all data units. wup calculation module is modified to count the number of reset and set operations of all data units after data inversion. only the buffer is extended compared with the original fnw design and it is not necessary to write down all wup bits to pcm array. the write control logic of maxpb-asy is virtually identical to maxpb design and the only difference is the algorithm inside the finite state machine."
"to show the performance of these algorithms, in this part, we adopted twenty experimentally supported extracellular matrix proteins as the same as the paper [cit] . we experimented the performance of ecmpred, ecmpp [cit] and our four different amorf models using these 20 proteins in table 6 . as depicted in table 6, amorf2, amorf3, and amorf4 can predict 16 proteins correctly. ecmpred and amorf1 can predict 15 proteins correctly, while ecmpp predicts six proteins for extracellular matrix protein. based on the above analysis, we can discover that all predictors cannot predict the proteins p27487. it expresses this protein that is hard to predict."
"workloads completion time is one of the key metrics to measure the whole system performance. as maxpb and maxpb-asy can significantly reduce the number of write units and shorten the write service time of a cache line under the power constraints, the workload running time can be also shortened. as shown in figure 14, the experimental results show that maxpb can get 45.5% running time reduction compared with dcw. more importantly, maxpb outperforms the state-of-the-art schemes fnw and two-stage-write by 19.3% and 9.6% on average, respectively. by leveraging power asymmetry, maxpb-asy can further shorten the running time. maxpb-asy outperforms the state-of-the-art schemes fnw and two-stage-write by 24.3% and 15.6% on average, respectively."
"in animal behavior environment, migration is a widespread aspect of the animal kingdom, which has been investigated intensively. the migration is determined and straightened-out movement affected by the animal's locomotory exertions taking them to new environments [cit] . in the migration process, the simple numerical patterns of animal aggregations direct the individual to follow three rules: 1) move to the same direction of your friends; 2) continue adjacent to your friends; 3) avoid collisions with your friends."
"unlike the state-of-the-art chip-level pcm write schemes, such as fnw, two-stagewrite, and three-stage-write, maxpb aims at different design goals. the differences are observed in table iv . the key idea of fnw is to utilize the dissimilarity between stored and to-be-written data. if more than half of the total bits have to be written, the new data will be flipped with an extra bit to index it. fnw can double the write unit size and improve the write parallelism. two-stage-write focuses on both the time and power asymmetry of writing zero and one to accelerate write. maxpb exploits insights that the number of bits changed in each data unit is little and the power is often excessively supplied. maxpb tries to write more data in one write unit to maximize the power reduce the chip-level write service time budget utilization and hence reduces the total number of write units, which is the key performance bottleneck of pcm write."
"feature selection plays a critical performance in various pattern recognition problems for eliminating unnecessary and irrelevant features [cit] . feature selection is a problem of combinatorial optimization in machine learning, which lessens the number of features, excludes unnecessary, noisy and irrelevant data and produces in the satisfactory analysis precision. in classification, feature selection approach can be classified into three classes: filter methods, wrapper methods, and embedded methods. filter method determines the importance of feature by studying the characteristics of the data. wrapper method is attempted to the duty of the training system in the evaluation process. if the feature selection and learning algorithm are interleaved, it is an embedded method. popular evolutionary-based feature selection methods are based on the genetic algorithm, differential evolution, and particle swarm optimization and so on. in this paper, we will use a new binary animal migration optimization [cit] as the feature selection method."
"in this study, five evaluation methods are used to measure the performance of different algorithms. they are sensitivity, specificity, accuracy, matthew's correlation coef-volume 5, 2017 ficient (mcc) and area under receiver operating characteristic (roc) (auc). following these five evolution methods, true positive (tp), false negative (fn ), true negative (tn ) and false positive (fp) are considered."
"recently, matlab version of the random forest algorithm is accessible at http://code.google.com/p/randomforestmatlab/. this matlab tool includes two major roles: one is ''classrf_train'', which trains provided data, and the other is ''classrf_predict'', which is predicting the new dataset by using the previous model [cit] . in this paper, our algorithm is designed by combining the random forest algorithm."
"in this paper, we introduce an amorf algorithm to predict extracellular matrix proteins based on four different features represents. the results obtained on training and testing datasets are better than other algorithms. we developed four models with four different feature selection approaches. the results show that the dipeptides composition (amorf3) is promising, which can balance the number of features and the classification accuracy. then, our four different models perform better than the ecmpred and ecmpp on experimentally supported ecm proteins. our model can also contribute to the understanding of some diseases relative to the ecm proteins. in the future, we shall make efforts to implement a web-server for the proposed method."
"after the preceding process, we get the total number of write units and keep in mind which data units should be packaged in one write unit, respectively. according to the process results, fsm selectively sends the write control signals to each data unit's write driver using a simple multiplexer as illustrated in figure 6 ."
"to use the animal migration optimization in feature selection, the algorithm needs to change the continuous space into the binary space. then, in this paper, we propose a binary animal migration optimization."
"to support the proposed maxpb write scheme, we add a middle circuit design, that is, maxpb write logic, between the write buffer and s/a write driver compared with the original design. it is worth noting that we do not increase the read operation overhead and the length of the read path is the same as the original and fnw designs. moreover, the overall length of the write path is similar to fnw and two-stage-write designs. we extend the write buffer size to 160 bits and the write buffer is composed of eight data units. actually, the data are flipped similar to the data conversion of fnw. extra 8 flip bits are added to index the data have been flipped or not similar to fnw. to monitor the actual power consumption of eight data units, 24 wup bits are used to store the power requirements of data units, that is, the number in the brackets as shown in figure 2 . we assume the size of the write unit is 16 and 3 wup bits (2 3 equals to 8) are used for per data unit as the maximum number of bits changed is 8 after the inversion process. our design does not need extra pcm array compared with fnw design as we just need to extend the write buffer size, which can be easily implemented in the pcm chip."
"in our sample, the cache line service can be finished at t4 as illustrated in figure 2 . two-stage-write [cit] ] accelerates pcm write parallelism by leveraging the asymmetries of writing \"0\" and \"1\" (time and power demand) as shown in figure 1 (b) . by adding carefully designed stage control circuits, the write is finished in two steps, that is, \"stage 0\" and \"stage 1.\" assuming the time ratio of writing \"0\" and \"1\" is 1/k, the current ratio is l. the average service time of a cache line is concluded in equation (3)."
"the algorithm of maxpb-asy is shown in algorithm 2. the algorithm input is extended for some necessary information such as the number of set and reset bits changed of each data unit and power demand for set and reset one bit. maxpbasy uses fine-grained power demand calculation and the power consumption of each data unit is counted by leveraging the power asymmetry of set and reset. after the fine-grained power consumption calculation, the dealing processes are the same with maxpb."
"the number of sequentially executed write units is one of the key metrics of pcm write latency and write performance, and less write units mean less write time consumption. we measure the average number of write units among 16 different workloads under the maxpb and maxpb-asy write schemes, as shown in figure 10 . in general, the number of write units, that is, the number of write-executed times to finish a cache line service, varies from 1.2 to 3.1 and is 2.0 on average under maxpb write scheme. when adopting maxpb-asy write scheme, the number of write units can be further reduced. the average number of write units is 1.4 when using maxpb-asy write scheme. in comparison, the average number of write units when implementing fnw is almost 4 while two-stage-write's average number of write units is close to 3. maxpb and maxpbasy can maximize the power budget utilization and minimize the number of write units, hence improving the overall system performance."
"recently, a multivariate statistical analysis on 494 amino acid attributes [cit] has been used to reach a small set of five multidimensional mathematical models, which illustrate the highly interpretable covariation among the original characteristics. the five statistical agent scores for each amino acid are presented in table 1 . factor 1 explains the simultaneous covariation in the portion of exposed residues. factor 2 is a secondary structure factor, which expresses the relation of different amino acids. factor 3 relates amino acid composition in various proteins. factor 4 reflects the relative amino acid composition in different proteins. factor 5 refers to electrostatic charge with the high coefficient. to transform the protein sequence to the fixed length vector, the fraction of each amino acid in the given protein sequence is first computed as follows:"
"we also considered other combination techniques. a very simple method is to estimate the overall accuracy of all g dj graphs, and chose the best one as a g combined . interestingly, this combination technique performed the best on our datasets, which might not always be the case. it is important to note that not always the same function performed the best."
"source code repositories stores a wealth of information that is not only useful for managing and building source code, but also provide a detailed log how the source code has evolved during development. information regarding the evidence of source code refactoring will be stored in the repository. also as bugs are fixed, the changes made to correct the problem are recorded. as new apis are added to the source code, the proper way to use them is implicitly explained in the source code. then, one of the challenges is to develop tools and techniques to automatically extract and use this useful information."
research regarding the development of such applications for mobile terminals is still confronting with essential questions such as: is it preferable to have a web-based application to a standalone one?
"firstly, the app is integrated with the underlying lms system. besides avoiding the hosting and maintenance of additional systems and infrastructure, this also integrates mobile learning into existing education and training programs. furthermore, the reuse of existing infrastructure allows utilizing existing scorm compliant learning material whenever possible without additional overhead."
mobler cards is designed to complement existing web-based adl courses with exercises for repetitive practicing. three core requirements were considered for the app. these requirements are key for scaling up mobile learning in security and defence organizations.
the second challenge addresses the legal frameworks and operational regulations of security and defence organizations. for this purpose it is necessary to understand the legal frameworks and the security needs to which mobile learning solutions relate. this is of particular relevance for innovating adl solutions across organizations. in order to meet the legal and operational requirements of different organizations common denominators for the use of mobile technologies in related organizations. the contribution of hodges and stead provided a first analysis of the factors that influence research ethics for testing mobile learning solutions in defence organizations. they indicate a great variation of regulations between organizations and nations that influence how research ethics can and have to be applied for studying mobile learning.
"arlearn implements a simple data model that enables the definition of several kinds of messages including multiple choice messages, video messages, audio messages, etc. messages can be bound to a location and/or a timestamp in the game. furthermore a flexible dependency mechanism enables the author to define the game logic. an author can for instance define through this framework that 60 seconds after all players have read the introduction message, a video message will become available."
"for each region we estimated the accuracy. figure 1 shows the accuracy values for k − means generated regions, for the similarity function f3, for the person \"cohen\", in the www'05 dataset. the accuracy values varied significantly for all functions. even if the actual values might depend on the actual dataset, the variation of accuracy is a common phenomenon. note that the accuracy estimations are based on the small training set and not the entire data, so computationally the method remains feasible."
"arlearn is and will probably always be under development. however at regular times we schedule releases of new functionality. by implementing real pilots we learn a lot, and new ideas take shope on making simulations better. the runs we organized so far suggest the following extensions."
"the matching itself can be usually conducted through the use of sub-graph isomorphic comparison between the code and the pattern graphs, and thus it is a form of supervised learning (patterns are already known), using graph comparison or similarity metrics between the examined graphs and the graph patterns."
"in the current version of the framework, the game logic runs both on the server and on the mobile device. this insures that a simulation can go on even when a player's device is temporarily not connected to the internet. because of this, inter-device communication has been postponed. the arlearn framework is currently being extended so that actions taken on one device can implicate other players. depending on the actions that the team leader role takes, the other roles will experience a different game play and could for instance suffer from ill taken decisions."
"one can define such regions based on some properties of the input (i.e. pair of entities) or based on the reported function value. we discuss here our experiments, where we defined the regions based on the similarity value. we considered two methods:"
data mining assists with software engineering tasks by explaining and analyzing in depth software artifacts and processes. based on data mining techniques we can extract relations among software projects. data mining can exploit the extracted information to evaluate the software projects and/or predict software behavior. a number of data mining methods have been proposed to satisfy the requirements of different applications. all of them accomplish a set of data mining functionalities to identify and describe interesting patterns of knowledge extracted from a data set. below we briefly describe the main data mining tasks and how they can be used in software engineering.
"-graph mining on the mailing lists of oss projects. based on the information provided by the mailing list of the project we could build author and message graphs. then applying mining techniques to these graphs we can extract useful information regarding message exchange or the users that contributes to projects. an interesting research approach will be to exploit graph processing techniques, like pagerank or spreading activation so that we rank nodes in each graph. the extract results can assist with rank users and measure relatedness between important users and important messages. -pattern mining form source code. another interesting perspective in the category of design pattern mining approaches in software engineering, could be to apply a graph clustering approach, or in general an unsupervised method, and examine what design patterns would be produced from the analyzed code. this would imply the definition of a graph clustering model, where in this case the graphs could be abstract syntax trees (asts) or abstract semantic graphs (asgs). the model should allow for the computation of the similarity between graphs, as well as for the computation of cluster representatives, i.e., the centroid graph of the graphs included in each cluster. the process would then be able to extract patterns, and which in turn could give an insight, after post-processing, about the design patterns used, as well as the design decisions made. -mining bug reports. the bug report database contains useful information regarding the quality of the software. analyzing the data collected from the bug fixing procedure, we could extract information about i) average impact on code change (i.e. % of files or % of lines changed), ii) estimate mean time before bug fixing developers involved in the bug fixing procedure. iii) temporal bug distribution in relation to project release dates."
"in this paper we discuss some data engineering techniques, which help to improve the decision we can make about whether two entities shall be considered as the same. our strategy is to define regions (i.e. intervals) in the value space (that is [cit] ) and estimate the accuracy of the functions in each region. in other words, we partition the interval [cit] into disjoint sub-intervals and with simple machine learning techniques we estimate how well does the similarity functions predict the equivalence in each sub-interval, based on a small training set. then, we use both the similarity values and their accuracy estimations to decide whether we should consider two entities equivalent. we also study how to combine these decisions if we have multiple similarity values and multiple accuracy estimations."
"the names correspond to real persons, but the set of real persons p is not known, even the size of p is unknown. there are multiple documents about a person with the same name in d. the person names are non-unique, therefore some of the documents might talk about different persons, if they share the names. for each name, we would like to partition the documents in the collection, such that two documents refer to the same person if and only if they are in the same partition."
"1. select the number of clusters into which the dendrogram will be divided. 2. examine the individual clusters for homogeneity by choosing the two executions in the cluster with maximally dissimilar profiles. if the selected executions have the same or related causes, it is likely that all of the other failures in the cluster do as well. if the selected executions do not have the same or related causes, the cluster is not homogeneous and should be split. 3. if neither the cluster nor its sibling is split by step 2, and the failures were examined have the same cause then we merge them."
"the platform team developed a cross-platform toolset that enabled mobile learning content to be deployed to apps on both android and ios (apple) platforms, and worked with jko (the us dod e-learning platform) to integrate the mobile platform with their back end infrastructure"
"large software systems, and especially open source software, offer mailing lists as a means of bridging users and developers. mailing lists constitute hard data since they contain a lot of free text. message and author graphs can be easily pulled up from the data, but content analysis is hard since probably messages constituting replies need to consider initial and/or previous discussions in the mailing lists. data mining applications in mailing lists among others include but are not limited to text analysis, text clustering of subjects discussed, and linguistic analysis of messages to highlight the developers personalities and profiles."
"quite often in a simulation, players need to gather on a location. when all players are present the game continues. currently a game facilitator tracks the players and manually triggers the next message to be broadcasted to all players. this puts some some burden on the game facilitator and prevents multiple simulations to take place on the same time. this game script was implemented in two phases. [cit], a dry run was organised in budapest with staff members of the organisations. [cit], the actual pilot was organised in entebbe, uganda. here, 3 game runs were ran at the same time featuring 3 roles per run."
"the different functions report similarity values with very different value distribution as they capture different aspects of similarity. thus instead of combining the similarity values themselves, we try to combine the decisions (whether or not to consider two entities as equivalent) and the estimated accuracy values."
"on the semantic web person names might be annotated with a globally accepted ontology. this direct link between the ontology helps to disambiguate the person names. however, such globally accepted ontologies are not present in the emerging semantic web. instead, ontologies are very often used as local schemas, thus one needs to relate the existing annotation to the ontology one would like to use. the semantic web community has developed a plethora of such techniques, see [cit] . the okkam project suggests a different approach, [cit] . they propose a service, which provides globally unique identifiers on large scale for entities, for (semantic) web applications. their approach relies on the existence of a large and clean (i.e. resolved) collection of entity profiles. entity profiles collect relevant attributes of real world entities. our techniques can contribute to create or extend such an entity profile collection."
"the mobile learning environment (mole) project was based on a requirement by the commander, u.s. naval forces europe (cne)/commander, naval forces africa (cna)/commander, sixth fleet (c6f) to effectively operate in the largest maritime area of operations (aor) where the most difficult challenge is the ability to train and communicate. it was intended to help mitigate the long-standing challenge of delivering low-bandwidth, on-demand and training by using mobile devices where there was a limited internet connectivity and limited infrastructure."
"the workshop participants highlighted that integrating mobile learning with existing infrastructures remains one of the main practical challenges on the way of scaling up the use of mobile devices for education and training. this does not only include the user interfaces of these environments, but also touches the communication between the mobile devices and the main adl system. concepts such as extended offline periods or push messages are typically not well considered by the existing infrastructure. furthermore, it is necessary to revisit the data-traffic footprint of adl systems for communicating with their mobile clients."
"in order to prove the concept of mobler cards it has been tested for delivering course material for two courses. the first course \"introduction to nato\" had no question pool available so an entirely new question pool has been created that was specifically tailored for mobile delivery. the second course \"building defense organizations\" [cit] . this course included a question pool for assessing the achievement of the learning objectives. both courses were successfully delivered on a range of test devices."
"many of source code version repositories repositories are examined and managed by tools such as cvs (concurrent versions system) and (increasingly) its successor subversion. these tools store difference information access across document(s) versions, identifies and express changes in terms of physical attributes, i.e., file and line numbers. however, cvs does not identify, maintain or provide any change-control information such as grouping several changes in multiple files as a single logical change. moreover, it does not provide high-level semantics of the nature of corrective maintenance(e.g. bug-fixes). recently, the interest of researchers has been focused on techniques that aim to identify relationships and trends at a syntactic-level of granularity and further associate high-level semantics from the information available in repositories. thus a wide array of approaches that perform mining of software repositories (msr) have been emerged. they are based on statistical methods and differencing techniques, and aim to extract relevant information from the repositories, analyze it and derive conclusions within the context of a particular interest."
-warnings are flagged for return values that are completely ignored or if the return value is stored but never used. -warnings are also flagged for return values that are used in a calculation before being tested in a control flow statement.
"data mining provides the techniques to analyze and extract novel, interesting patterns from data. formally, it has been defined as the process of inducing previously unknown and potentially useful information from data collections. thus mining of software engineering data has recently attracted the interest in this paper, we present an overview of approaches that aim to connect the research areas of data mining and software engineering leading to more efficient techniques for processing software. in section 2, we provide an introduction to the main data mining concepts and approaches while in section 3 we describe the different types of software engineering data that can be mined. in the follow up, a discussion takes place in section 4 concerning the methods that have applied data mining techniques in the context of software engineering. it surveys the current research that incorporates data mining in software engineering while it discusses on the main characteristics of the respective approaches. specifically, our study is based on the following main features of the approaches: i) data mining technique used (section 2), ii) the software engineering data to which they are applied (section 3), and iii) the software engineering tasks that they can help (section 4). thus, this paper aims to introduce practitioners to the fundamental concepts and techniques that can use in order to obtain a better understanding of the software engineering processes and potentially perform them more efficiently by applying data mining. in parallel, researchers can exploit the recent techniques to better understand the project data, to identify the limitations of the current processes and define methodologies that facilitate software engineering tasks."
"due to its capability to deal with large volumes of data and its efficiency to identify hidden patterns of knowledge, data mining has been proposed in a number of research work as mean to support industrial scale software maintenance, debugging, testing. the mining results can help software engineers to predict software failures, extract and classify common bugs, identify relations among classes in a libraries, analyze defect data, discover reused patterns in source code and thus automate the development procedure. in general terms, using data mining practitioners and researchers can explore the potential of software engineering data and use the mining results in order to better manage their projects and to produce higher quality software systems that are delivered on time and on budget. in the following sections we discuss the main features of mining approaches that have been used in software engineering and how the results can be used in the software engineering life cycle. we classify the approaches according to the software engineering tasks that they help and the mining techniques that they use."
this section provides an overview of mining approaches used to assist with development process. we summarize the main features of these approaches in table 2 .
"arlearn, originally a tool for audio augmented reality, has grown over the past few years from a standalone smartphone app to a fully-fledged mixed reality application platform taking into account field-trips, serious gaming, augmented virtuality and a notification system [cit] . in order to support the creation of simulations, arlearn builds on two important concepts:"
"the second step is to build a bug detector driven by these findings. the idea is to develop a function return value checker based on the knowledge that a specific type of bug has been fixed many times in the past. briefly, this checker looks for instances where the return value from a function is used in the source code before being tested. using a return value could mean passing it as an argument to a function, using it as part of calculation, de-referencing the value if it is a pointer or overwriting the value before it is tested. also, cases that return values are never stored by the calling function are checked. testing a return value means that some control flow decision relies on the value."
"where n t is the size of t and j(t) is the average value of j in t. -each node t is split into two children t r and t l . the split is chosen that maximizes the reduction in deviance. that is, from the set of possible splits s, the optimal split is found by:"
"in addition to the core research activity, the mole project sponsored several external science and technology (s&t) projects in order to develop and deploy a sustained capability -and it is well-known that s&t plays an important part in developing and testing new technologies that will meet defense-related requirements. through s&t research programs, the defense community is able to develop capabilities that closely link the human and physical worlds in order to meet emerging demands. this linkage can only be achieved through rigorous testing, evaluating, experimenting and conducting trials that will demonstrate if new technology provides an operational capability otherwise not available."
"cudré [cit] take a different approach to entity resolution in the web context. they propose a graphical modelbased probabilistic framework to capture the relations among the entities. their framework also includes trust assessments about the providers of the entity equivalence assertions. these trust assessment values are later adjusted as their probabilistic reasoning framework eliminates the detected inconsistencies. while this approach has many advantages, it is not fully applicable to our case, as the underlying factor graph model would have very large cliques, as subgraphs, which could easily lead to poor convergence of the probabilistic reasoning."
"the majority of web-based adl systems are optimized for desktop computing systems, reflecting the infrastructure that is available to learners and trainers in these organizations. however, with the current generation of smart phones the \"mobile web\" gained momentum and become relevant on a large scale. the mobile web refers to a number of technologies that support a wider range of interaction modes beyond the \"keyboard-mouse-screen\" interaction of desktop computing. this includes support for \"responsive\" [cit] arrangement of information for a wide range of screen sizes and touch interaction, but also interactions that go beyond active manipulation of information on a personal screen. these new forms of interaction include location-based and gesture-based interactions that are used for but not limited to information filtering. like many legacy ict are adl systems not designed to support these new forms of interacting with information. moreover, the related educational resources are often unsuited for the delivery to different platforms or prepared to benefit from these new interaction forms. yet it is clear that desktop computing systems remain an important part of the ict infrastructure in security and defence organizations. this raises questions and doubts about the needs and benefits for supporting mobile technologies for education and training in security and defence."
"the office of the united nations high commissioner for refugees (unhcr) leads and co-ordinates international action to protect refugees and resolve refugee problems worldwide. as this organisation is sometimes confronted with kidnappings of their co-workers, employees are trained on how to deal with such situations. an arlearn \"decision making\" game was designed that presents the participants a real-time simulation of a hostage-taking situation. the game script was created taking into account several roles (head of office, security official and staff welfare). depending on the role, participants receive different tasks and information. for instance, the head of office receives calls from journalists, while staff welfare receives a call from a distressed hostage's family member. therefore different educational scenarios and collaborative scripts have been implemented in arlearn to simulate complex hostage taking scenarios and their management with different roles."
"issue-tracking or bug reporting databases constitute the most important cesspool of issue reporting in software systems. structured data (database tuples) containing the description of an issue, the reporter's details and date/time are the standard three types of information that can be found in issue-tracking databases. machine learning techniques have been successfully used in the past to predict correct assignments of developers to bugs, cleaning the database from manifestations of the same error, or even predicting software modules that are affected at the same time from reported bugs."
"initially, mole focused on currently available mobile devices in order to assess which mobile platforms and solutions 'best' meet the operational needs and requirements since not every participant in this proof of concept has a state-of-theart mobile device. this approach proved to be a very cost-effective approach since a majority of mobile devices (e.g., gsm, 3g and 4g capabilities) are accessible worldwide. therefore, the proof of concept, which would involve 22 nations, ensured time and financial resources were focused on identifying and developing an effective operational capability rather than the procurement of specific types of mobile technologies."
"secondly, the app minimizes the overhead for content authors. this lowers a significant barrier for scaling mobile learning in security and defence organizations by enabling content authors to use their knowledge about web-based courses and web-based assessment. this has been achieved reusing the authoring capabilities of the lms for content creation. as mobler cards reuses the available question pool function provided by the lms it allows the immediate use of existing course resources for learning. as a side effect this allows to repurpose components of existing scorm packages for mobile learning."
"source code for data mining in software engineering can be proved an important source of data. various data mining applications in software engineering have employed source code to aid software maintenance, program comprehension and software components' analysis. the details of these approaches are discussed in section 4. an initial preprocessing of the available source code is always a caveat, since a parser for the respective source code language must be available. once parsed, the source code can be seen as structured text. central aspects of applying data mining techniques in source code among others include prediction of future changes through mining change history, predicting change propagation, faults from cached history, as well as predicting defect densities in source code files."
the evaluation of software is based on tests that are designed by software testers. thus the evaluation of test outputs is associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification.
"the recent advances in data management technology provide us with the tools and methods for efficient collection, storage and indexing of data. large amount of data is produced in software development that software organizations collect in hope of extracting useful information and obtain better understanding of their processes and products. also a significant amount of open source software (oss) project metadata is collected and maintained in software repositories. for instance, the flossmole 1 project integrates data from several oss projects and freely provides them in several formats. then there is an increasing abundance of data stored in software engineering (se) repositories which can play a significant role in improving software productivity and quality. specifically, data in software development can refer to programs versions, execution traces, error/bug reports and open source packages. mailing lists, discussion forums and newsletters also provide useful information about a piece of software. this explosive growth of software engineers' ability to collect and store se data has created a need for new, scalable and efficient, tools for data analysis."
"we performed our experiments on a 2gb ram, genuine intel(r) t2500 @ 2.00 ghz cpu. linux kernel 2.6.24, 32-bit machine. we implemented our methods using matlab and java."
"to summarize, we used a simple entity resolution framework, while there are more involved frameworks known in the literature. we applied data engineering techniques, which improved the quality of our results. these techniques (explicit analysis of the accuracy of similarity functions) can be used in combination with other techniques. creating a large and resolved collection of entity profiles can open new perspectives for semantic web applications. our work also contributes to this line of research."
"the first step in the process is to identify the types of bugs that are being fixed in the software. the goal is to review the historical data stored for the software project, in order to gain an understanding of what data exists and how useful it may be in the task of bug findings. many of the bugs found in the cvs history are good candidates for being detected by statistic analysis, null pointer checks and function return value checks."
"a problem that we have to tackle in software engineering is the corrective maintenance of software. it would be desirable to identify software defects before they cause failures. it is likely that many of the failures fall into small groups, each consisting of failures caused by the same software defect. recent research has focused on data mining techniques which can simplify the problem of classifying failures according to their causes. specifically, these approaches requires that three types of information about executions are recorded and analyzed: i)execution profiles reflecting the causes of the failures, ii) auditing information that can be used to confirm reported failures and iii) diagnostic information that can be used in determining their causes. below we present the various data mining approaches used to facilitate software maintenance (see also table 5 )."
"finally, it became evident that mobile learning also requires revisiting the concepts of instructional designs for adl. this is not only required for novel concepts such as mobile team simulations, but also for more conventional concepts such as formative tests and content delivery. although existing learning material remains accessible if appropriate interfaces are available, its attractiveness and usefulness appears to change with the move from desktop computing environments to mobile devices. therefore, it is necessary to develop a better understanding about the relation between mobile technologies and micro-level instructional design patterns."
"mobler cards provides an alternative interface for accessing and using the question pools that are stored in the lms. by doing so, the app complements web-based training but it does not build on existing interfaces of the lms. the app reuses the lms' components for user management and preferences, for course management, and for test objects. in order to minimize the network traffic for the mobile app, the app contains all parts of the user experience and exchanges only the relevant data with the lms through a set of rest services."
"given a single similarity function, we can consider two related persons equivalent if their similarity value is higher than an appropriately chosen threshold. indeed, for each function we have chosen such a threshold, using the estimates from a small training sample, where we know the equivalence relations. we have chosen a threshold, which -based on the training set-maximizes the number of correct decisions."
"although the transfer of factual knowledge plays an important role when training personnel that operate in potentially dangerous conditions, mobile learning techniques to support security trainings are often underused. there is a great value in simulating dangerous situations. in contrast to learning from a textbook, a simulation leaves room customization and surprise, as players do not know what will happen in advance. by experiencing a series of events that occur in a simulation, participants learn in authentic context and are trained to respond to events as they occur. this is different from studying factual knowledge. organising these simulations in the real world can often become very expensive and resource intensive. for instance, a real-life military simulation can require the use of weapons and explosives to engage the participants in simulation. many dangerous situations require personnel with different roles (e.g. team leader, communication expert) to cooperate. managing these different roles during a simulation puts extra work on the simulation facilitator."
"over the last decade mobile information technologies have become a ubiquitous part of daily life. mobile learning research dates back not even 10 years further [cit] . as such it is among the latest research areas for educational technologies. given the overwhelming success of smart mobile devices on a global scale this technology appears to be suited to extending the reach and continuity of educational programs. for many security and defence organizations these new opportunities are problematic because much of their content and infrastructure has been optimized for web-based desktop computing environments. much has been invested into the training of instructors and authors to make good use of the available adl solutions many organizations have a rich pool of educational resources available in the scorm format [cit] . however, some criticism of scorm highlighted that it is not sufficiently supporting mobile learning. this raises the question for security and defence organizations, whether they have to create new educational resources and programs if they want to introduce mobile learning on a large scale."
"further research includes the proof of effectiveness for learning. this will include a field test using the \"introduction to nato\" course. additional work is required for providing more flexible ways for providing course specific learning analytics and learning badges. this is closely related to integrating support for the tin-can api [cit] into mobler cards."
"-use annotation graphs that provide more detailed annotation information -ignore comments, black line, format changes, outlier bug-fix revisions in which too many files were changed -manually verify all hunks in the bug-fix changes"
from the contributions it appears that the core technological concepts are well understood and are ready for mobile learning applications. the workshop indicated three key challenges for future mobile learning research in security and defence organizations.
"in the same study ( [cit] ), the authors also attempted the use of decision trees, and more specifically the j.48 implementation of weka, which is essentially the implementation of the c4.5 decision tree algorithm [cit], in order to analyze the same data. the application of the c4.5 decision tree algorithm in this was made in a manner so that the authors were able to identify the most important features from the 27, by conducting attribute removal experiments. more specifically, they studied what would be the root node of the tree in each case, if at each time the most important attribute is removed (i.e., the root node), and the tree is rebuilt without considering that attribute. this methodology allowed them to identify weak attributes (i.e., attributes that appear in any non-root node after several removals of root node attributes), as well as barely supportive attributes (i.e., attributes that, once root nodes, if removed and the tree is rebuilt disregarding them, the classification accuracy remains the same)."
"peace support operations, crisis and disaster management, law enforcement and civil protection are increasingly of multinational concern. the personnel of international organizations often are confronted with managing complex logistic and cooperation scenarios under the threads of military offense, terrorist attacks, or environmental devastation often outside their own cultural and legal settings. armed forces, international organizations and ngos who operate in crisis areas face similar educational challenges although the content of their training might differ. even for domestic activities of the police or the customs the underlying legal frameworks are complex and subject to change and adjustment. working under such conditions requires flexibility, frequent relearning, and situational awareness. this leads to a high demand of just-in-time information, higher-order procedural skills and competences, frequent re-learning, and recontextualization of practices and habits. mobile technologies do not only hold great potential for learning, training, information access, and knowledge management, but are already part of the practice found in security and defence organizations."
"various approaches have been developed to accomplish the above mentioned data mining tasks and deal with different types of data. they exploit techniques from different aspects of data management and data analysis, including pattern recognition, machine learning, statistics, information retrieval, concept and text analysis."
"currently, there is a wide variety of smart phones that can access and surf on the web through installed browsers. this variety can be seen from both perspective, also as a good thing but sometimes as something that causes problems for adapting software for users. therefore, it is necessary to use standards, but in this domain of mobile technology standards are still in developing phase."
-generate candidate feature-sets and use each one to create and train a pattern classifier to distinguish failures from the successful executions. -select the features of the classifier that give the best results.
"the general conception of the second product is not new, being used by other systems of this kind. the novelty factor of it is given by access to 3g networks and the possibility of adding events and new procedures in real time. thus, the knowledge base is accessed as if it is a living organism and grows with each step made by any of the users."
"the rest of the paper is organized as follows. section ii gives a precise problem definition, section iii discusses similarity functions. section iv gives details on how do we combine multiple evidences for entity resolution, while section v presents experimental results, section vi contains related work and finally section vii concludes the paper."
"a hidden layer l, consists of nodes representing conjunctions of values of the first l input attributes, which is similar to the definition of an internal node in a standard decision tree. the final (terminal) nodes of the network represent non-redundant conjunctions of input values that produce distinct outputs. considering that the network is induced from execution data of a software system, each interconnection between a terminal and target node represents a possible output of a test case. figure 3 presents an ifn structure."
"the practicing mode offers the typical flash card learning experience of a question and an answer. in order to monitor the learners' progress they have show that they are able to answer the question correctly. in addition to this questionresponse activity, the learners receive immediate feedback on their answer and can evaluate their answer in comparison with the correct answer. each answer can have three levels of achievement: \"excellent\" if the correct answer has been provided, \"partially correct\" if the parts of the answer were correct, and \"wrong\" if the provided response were not matching the correct answer at all."
"2 unhcr aimed at giving their trainees an authentic learning experience. an incoming video message with a plea for help from the hostage created a sense of immersion. through overloading the participants with many messages and tasks, the game designers wanted to create a level of stress. for this purpose the possibility to trigger notifications automatically, was extended with the possibility for a game operator to trigger them manually. this way, the operator can better estimate when a message (with additional work) should be dispatched. this game script was implemented in two phases. [cit], a dry-run was organised in budapest with staff members of the organisations. [cit], the actual pilot was organised in entebbe, uganda. here, 3 game runs were ran at the same time featuring 3 roles per run."
"through tag scanning (e.g. qr codes, nfc tags), players can indicate their presence to the system. when they arrive on a location they check-in by holding their phone next to an nfc tag, or by scanning a barcode. by doing so, the system can automatically take action and proceed with the next phase of the game."
"arlearn currently comes with an authoring environment that covers only part of the features that are offered. features that are not supported by the authoring tool are hand coded and require programming capabilities. by including the creation of dependencies and other features, we expect the authoring environment to become more suitable to create complex security simulations."
"the current development and uptake of mobile technologies in the security sector challenges existing advanced distributed learning (adl) and knowledge management approaches. yet, research and practice of mobile learning for supporting security and defence learning is in its infancy, scattered, and weakly connected."
"one of the main problems of compatibility between the content of education in classical and electronic platforms for mobile devices, is characteristic of display and multimedia. content for desktop computers is not automatically compatible with mobile devices and their display screens. creating different content for mobile and for desktop on the same subject is quite expensive, that why most elearning solutions on the market are only aimed at the desktop. contrary to this aspect, the elearning systems users are most out of time and usually in motion. providing access to educational content, not just in a fixed location is a key objective for developers of elearning solutions. the technology is evolving very rapidly and there is a very close competition between developers to create apps and to offer many features and applications to end-users. mobile devices involve unique requirements and challenges. they are usually limited in terms of processing power, battery life and user interface estate."
"however, lower similarity values might have many reasons, such as real dissimilarity, missing or incomplete information on the pages, the function does not capture the differences (only in special cases), the input to the functions (generated by information extractors) is uncertain or erroneous, or the inaccuracy of the function itself. a possible way to improve threshold-based decisions is to consider the accuracy of the function. one can estimate the overall accuracy (percentage of correct decisions) of a function, based on a small training set. if a function has an overall low accuracy, the reported high similarity values are not informative. however, our experiments indicated that the such overall accuracy estimations do not work very well, as the accuracy shows significantly different values for various subsets of the input. even if a function has an overall acceptable accuracy, in some regions it might perform particularly well. we tried multiple ways to divide the input into regions and compute accuracy estimations separately for each region."
"in this section we discuss how data analysis techniques can contribute to educe or trace system requirements. the works for requirement analysis refers to data mining in its broadest sense, including certain related activities and methodologies from statistics, machine learning and information retrieval. table 1 summarizes the main features of the techniques discussed below."
"mobler cards optimizes the time frame that is available for learning by hiding most of administrative tasks from the learners. this includes authentication, data synchronization, and course navigation. furthermore, mobler cards allows the learners accessing supportive features such as learning statistics at any time. this feature requires that all functions have to be implemented in the app instead on being provided by the lms."
"clustering is one of the most useful tasks in data mining, applied for discovering groups and identifying interesting distributions and patterns in the underlying data. the clustering problem is about partitioning a given data set into groups (clusters) such that the data points in a cluster are more similar to each other than points in different clusters [cit] . in the clustering process, there are no predefined classes and no examples that foretell the kind of desirable relations being valid among the data. thus it is perceived as an unsupervised learning process [cit] . clustering can be used to produce a view of the underlying data distribution as well as automatically identify data outliers. in software engineering, clustering can be used to define groups of similar modules based on the number of modifications and cyclomatic number metrics (the number of linearly independent paths through a program's source code)."
"the content team worked with a wide range of medical, and training stakeholders to design, convert, import and create mobile content to support the needs of the target users"
"-scm annotation does not provide enough information to identify bug-introducing changes. also we have to trace the evolution of individual lines across revisions in order that the functions/methods containment can be determined. -all modifications are not fixes: there might be changes that are not bug-fixes. for instance, changes to comments, black line and formatting are not bug-fixes, even though based on scm are flagged as such."
". however, the similarity values do not have a uniform distribution in the [cit] interval, thus choosing the regions as equal size intervals is not the best option, as some intervals contain significantly more values than others. 2) we clustered the similarity values corresponding to the training set using the k − means clustering technique, the output of which are k-cluster heads and each cluster head representing a region. based on the training set, for each region we compute an accuracy estimate. from the training sample set, each region would contain certain sample points corresponding to link existence and non-existence. accuracy for a region is then defined as the percentage of the sample points representing link existence. if this value is lower than 0.5 then it suggests that the majority pairs should not be considered as a link."
"data mining approaches can be used for extracting useful information from the tested software which can assist with the software testing. specifically, the induced data mining models of tested software can be used for recovering missing and incomplete specifications, designing a set of regression tests and evaluating the correctness of software outputs when testing new releases of the system. a regression test library should include a minimal number of tests that cover all possible aspects of system functionality. to ensure effective design of new regression test cases, one has to recover the actual requirements of an existing system. thus, a tester has to analyze system specifications, perform structural analysis of the system's source code and observe the results of system execution in order to define input-output relationships in tested software. table 3 summarizes the main data mining techniques that are used in the context of software testing."
"-a classification of the approaches used to mine software engineering data, according to the software engineering areas that assist. -matrix based analysis framework bridging software engineering with data mining approaches."
"2) methods: given the dataset, we use 10% of the complete dataset as the training set. the performance of the er algorithm depends on how well the training set represents the features of the complete dataset. in order to avoid any bias, we repeated the experiments for 5 runs and the averages of the observed results are presented. on each run we randomly choose the training subset from the complete dataset."
"this contribution introduces the mobler cards app for learning management systems (lms). mobler cards is basically a variation of a flash card learning app that uses test questions for repetitive practicing on smart phones. the unique feature of mobler cards is that it synchronizes itself with a lms while offering all functions even while the learners are offline. after learners installed the app on their smart phone, they connect to the lms. after the authentication mobler cards identifies appropriate learning resources for the courses into which a learner is enrolled. for each of the learners' courses the app has two modes: a practicing mode and a statistics mode."
program logic errors rarely incur memory access violations but generate incorrect outputs. a number of mining techniques have been used to identify logic error and assist with software debugging (see table 4 ).
"each similarity function compares two webpages based on a particular feature(like concepts, urls etc) using a similarity measure(like cosine similarity, number of overlaps etc) [cit] . we use common observations in coming up with the following similarity functions. two webpages are about a same person, if the concepts or organizations or person names etc mentioned on the pages are similar/overlap, or if the pages urls are on a same webdomain."
"1 from the graph g fi w we would like to obtain a graph g dj, a (not-weighted) graph, where an edge between two nodes shall indicate whether the entities corresponding to the nodes are the same. this transformation depends on the decision criteria d j . these decision criteria include to chose values above a threshold or also consider accuracy estimations, as it is explained in section iv-a. once we have all the graphs g dj, for all functions f i and all decision criteria d j, we obtain a combined graph g combined, which is explained in section iv-b. for this we also use accuracy estimations acc(g i d j ), based on the training set. finally, we apply clustering techniques to obtain the final entity resolution. in our recent implementation we compute the transitive closure of the graph g combined, but we also experimented with several other clustering techniques, such as correlation clustering [cit] . the overall procedure is summarized in algorithm 1."
"compiled code constitutes in its form of object code one of the alternative data sources for applying static analysis in software engineering. compiled code has also been used as a data source from data mining techniques in order to assist malicious software detection. furthermore, web mining principles have been widely used in object-oriented executables to assist program comprehension for means of reverse engineering. when the software modules and components are tested, a chain of events occurs which are recorded in an execution trace. execution pattern mining has also been used in execution traces under the framework of dynamic analysis to assist with the extraction of software systems' functionalities."
"the basic concept was that the mole project would leverage the global cellular network infrastructure, mobile technologies and emerging mobile application/service models to build a mobile learning (m-learning) capability that integrated into the deputy director, joint staff (j-7) for joint and coalition warfighting (dd j7 jcw) joint knowledge on-line (jko) portal to facilitate the sharing of educational content between us and multi-national partners. through the proof of concept it would demonstrate an enhanced interoperability and yielded high benefits, to all services, combatant commanders (cocom), the international health division, force health protection & readiness organization, and others using the dd j7 jcw internet protocol (ip) by providing medical education and training to military and related civilian personnel of foreign countries in need of humanitarian and civil assistance or during joint exercises and force training. this would, in turn, be shared by the international community to support their education and training in the medical area."
"250 users across the partner nations downloaded the apps onto their personal devices, and spent between 1 and 10 hours working through the mobile courses and content. most were using the app in english, though spanish, georgian, french and german versions were available too."
"a recent study on the mobile learning readiness of the swiss armed forces [cit] unpublished) analysed the responses of almost 500 learners and trainers regarding the availability of mobile technologies among and the expectations towards using mobile technologies for education and training. the results indicate that more than 90% of the respondents own a recent smartphone, 27% own a tablet computer and 71% own a laptop. this indicates that providing support for mobile devices can increase the outreach of learning. the data also indicated that it is common to have access to several devices that are capable to connect internet. furthermore, the study results indicate that the use of mobile devices is ubiquitous without major differences across age groups or organizational ranks and that the majority (70%) of the respondents expect to access all their relevant information also from their mobile devices. the wide availability and adoption of mobile technologies in daily life indicates the exiting demand for mobile alternatives to existing web-based services. the same time. the contributions to this workshop analysed four aspects of mobile learning in security and defence organizations."
the mobile learning management system and mobile knowledge management system solutions open up new opportunities for learning environment corresponding to the skills of the future generations and the requirements of a dynamic economic environment.
"unhcr aimed at giving their trainees an authentic learning experience. an incoming video message with a plea for help from the hostage created a sense of immersion. through overloading the participants with many messages and tasks, the game designers wanted to create a level of stress. for this purpose the possibility to trigger notifications automatically, was extended with the possibility for a game operator to trigger them manually. this way, the operator can better estimate when a message (with additional work) should be dispatched."
"the final product of the project is an experimental model for an integrated mobile learning system that allows access for different users, from formal educational systems or from professional system, to knowledge bases and real time learning, according with \"anywhere and anytime\" principle. the mlearning integrated project aims to cover two major directions of interest: access to education (mobile learning management system-mlms) and access to knowledge resources (mobile knowledge management system -mkms) together with a study conducted about the role of these new technologies in a knowledge society in relation with market opportunities to adopt such solutions."
"any return value passed as an argument to a function before being tested is flagged, as well as any pointer return value that is de-referenced without being tested."
"daniel beligan, ion roceanu and dragos barbieru carol i national defence university, bucharest, ro m-learning has represented a huge field of interest in the last 10 years. the research and development efforts are especially oriented towards m-learning applicability in formal education as a complementary solution of the methods and forms of organizing learning and developing communication capabilities in groups. mobile devices allow people to be less dependent on a certain place in order to be involved in economic or social activities."
"3. the profiles of reported failures are analyzed using cluster analysis, in order to group together failures whose profiles are similar with respect to the features selected in phase 2. 4. the resulting classification of failures into groups is explored in order to confirm it or refine it."
"we note here that the above measures rely on the fact, that we know the ground truth, which is unrealistic in the web context. we could apply them for the document collections in our experiments, as we had this information available. figure 2 shows the performance of the individual similarity functions on the entire www'05 dataset. the figure shows three metrics, namely f p -measure, f -measure and randindex. the final column, depicted as black in the figure, is the combined performance of our proposed technique, which clearly shows improved performance. similarly, figure 3 shows the experimental results on the weps dataset. table iii contains the achieved f p values, for each individual person, by each individual function in the www'05 dataset. one can observe that each function performs differently for different persons. for example, for \"voss\" the function f8 has the highest f p -value, while for \"mulford\" the best function is f6."
"1. the software is implemented to collect and transmit to the development either execution profiles or captured executions and then it is deployed. 2. execution profiles corresponding to reported failures are combined with a random sample of profiles of operational executions for which no failures were reported. this set of profiles is analyzed to select a subset of all profile features to use in grouping related failures. a feature of an execution profile corresponds to an attribute or element of it. for instance, a function call profile contains an execution count for each function in a program and each count is a feature of the profile. then the feature selection strategy is as follows: fig. 4 . a clusters' hierarchy."
-bringing together data mining and software engineering research areas. a number of approaches that use data mining in software engineering tasks are presented providing new work directions to both researchers and practitioners in software engineering.
the checker does a data flow analysis on the variable holding the returned value only to the point of determining if the value is used before being tested. it simply identifies the original variable the returned value is stored into and determines the next use of that variable.
"the influence of laws and organizational regulations cannot be underestimated in the security and defence sector. for bringing mobile learning solutions from prototypes to practice it is necessary to understand the context in which mobile learning will be applied. specifically defence organizations have rigid rules for how and where to use mobile devices. these rules go clearly beyond the level of research ethics. therefore, it is necessary to develop a better knowledge about the influence of organizational, national and even international regulations on implementing mobile learning in this sector."
"quite often in a simulation, players need to gather on a location. when all players are present the game continues. currently a game facilitator tracks the players and manually triggers the next message to be broadcasted to all players. this puts some some burden on the game facilitator and prevents multiple simulations to take place on the same time."
"one of the areas that was particularly challenging was agreeing a common frame of reference for research ethics, given the wide range of stakeholders, and the mix of educators and medical practitioners. medics have their own set of stringent guidelines for any trials. the eu has a secondary set. the us government has it's own guidelines too. the partners needed to define a subset of these three and seek approval via the appropriate channels, to agree an appropriate protocol."
"the statistics mode allows the learners to analyse their performances for the course. four analytical measures are available for the learners: the number of questions handled during a period of 24h, the average score that has been achieved during the same period, the progress to answer all questions correctly, and the average speed for answering each questions. the difference between the average score and the progress is that the average score includes partially correct answers as well as fully correct answers, while the progress includes only fully correct responses. in addition to these performance-based learning analytics, the app offers two learning badges that are based on the effort learners using the app. the first learning badge indicates that the learner handled all available questions for a course. the second badge is awarded after the learner answered a large number of questions in one sitting. for both learning badges, the performance is irrelevant."
"in case of software failure classification problem, we consider two classes, that is success and failure. the classification and regression tree (cart) algorithms was used in order to build the classification tree corresponding of software failures. assume a training set of execution profiles"
"we say that two entity references (names) n i and n j are equivalent (n i ≡ n j ) if they refer to the same person. clearly this relation is transitive. the relation of the entity references can be represented as a graph, in which for each entity reference there is a vertex in the graph, and two vertices are connected by an edge whenever the two corresponding entities are equivalent. we refer to this graph as the entity graph. the goal of the entity resolution algorithms is to reconstruct this entity graph as accurately as possible. note that the entity graph has very specific properties: it is not a connected graph, it is a union of pairwise disjunct connected components and each component is a clique, i.e. a complete graph, because of the transitivity of the equivalence relation."
"similarity functions associate a value from the interval [cit] to a pair of entities. in our case, instead of comparing the entities themselves, we compare the related web-pages. as a preprocessing step we apply information extraction tools, so the input to the similarity functions is the extracted information and not the pages themselves. in other terms, we apply (dictionary-based) named entity recognition techniques."
"given the classification tree, we have to traverse the tree from the root to a leaf in order to classify an object. at each step of the traversal prior to reach a leaf, we evaluate the expression at the current node. when the object reaches a leaf, the predicted value of that leaf is taken as the predicted class for that object."
where each x i represents an execution profile and j i is the result (success/failure) associated with it. the steps of building the classification tree based on l are as follows:
"the recent explosive growth of our ability to collect data during the software development process has created a need for new, scalable and efficient, tools for data analysis. also there is strong requirement for mining software repositories and extracting hidden information. this extracted knowledge is expected to assists the software engineers with better understanding the development processes and predict the future of software products. the main focus of the discipline of data mining in software repositories is to address this need. in this paper we review the various data mining methods and techniques used in software engineering. specifically our objective is to present an overview of the different data sources in software engineering that are interesting to be mined. also we discuss how the data mining approached can be used in software engineering and what software engineering tasks can be helped by data mining. the main characteristics of data mining approaches used in software engineering are summarized in tables 1-6 ."
"to cover the widest possible range of mobile devices and provide a consistent experience with mobile device capabilities, educational content packages must include two forms of educational content, in terms of quality and functionality: one with a low quality and functionality and an interactive format, and one with high quality and interactive functionality. both formats should be wrapped and stored in the repository of educational content. formats will be delivered according to the capabilities of the device either by selection or by automatic detection capabilities."
"in general terms, it has been observed that the bugs cataloged in bug databases and those found by inspecting source code change histories differ in type and level of abstraction. software repositories record all the bug fixed, from every step in development process and thus they provide much useful information. therefore, a system for bug finding techniques is proved to be more effective when it automatically mines data from source code repositories."
"many entity resolution techniques rely on pairwise similarity functions, which report the similarity based on some features of the pages. it is unlikely that one can design a single similarity function, which could be used for all pages in any larger collection to decide whether they are about the same person or not, because of the heterogeneity of the pages. typically, the functions work better in some cases and worse in others."
"the first challenge emphasizes that educational technologies are not new to the security and defence sector and that mobile devices are likely to be used together with traditional web-based systems. the contributions suggested two approaches to this challenge. the first approach is to provide alternative interfaces for mobile devices. alternate interface enable learners and teachers to access all functions of adl systems from mobile devices. these interfaces are designed for meeting the specific constraint of the small screen estate of mobile devices. while the screen resolutions of contemporary smart phones would be sufficient to display most types of web-content, it is the constraints of the human body special attention for enabling learners and trainers to interact with this content. the second approach is to provide complementary learning opportunities based on the available learning material through mobile devices. this approach considers mobile devices not as an alternative way for accessing the same functions as with desktop computers. instead, glahn considers that mobile devices offer new affordances that can be utilized for alternative learning experiences. for example, it is possible to improve the continuity of learning through casual exercises that are mediated through mobile devices."
the key challenge that has been addressed by the research related to this app is how to make use of scrom compliant learning material for supporting learning and extending the continuity of learning. the objective was to identify whether the reuse of existing learning material for new learning experiences can get achieved with existing scorm compliant resources.
"the main contributions of the paper are that even with our simple techniques we could achieve results comparable to state-of-the-art methods and with our accuracy estimations we could demonstrate systematic performance improvements. we believe that such explicit analysis of accuracy of similarity functions can be used in combination with other entity resolution techniques, to improve their performance."
"systematic software reuse has been recognized as one of the most important aspects towards the increase of software productivity, and quality [cit] . though software reuse can take many forms (e.g., ad-hoc, systematic), and basic technical issues such as development of software repositories, and search engines for software components in various programming languages are on the frontier of research in the area of software reuse, recently there have been attempts to incorporate data mining techniques in an effort to identify the most important factors affecting the success of software reuse. the motivation behind those approaches stems partially from the fact that previous surveys showed possibility of projects' failure due to the lack of reuse processes introduction, as well as modification of non-reuse processes [cit] ."
"finally, the app provides full flexibility for mobile learners in order to support the continuity of learning. for the learners' perspective the objective of mobler cards is to enable learning in suitable moments as they occur. these \"learning opportunities\" can vary in their duration and in their contexts. such as waiting for a bus or commuting in the train. also it was presumed that the internet connectivity is not a reliable factor of the learning experience. furthermore, it should be possible that learners still have access to the learning material during extensive offline phases. yet, the learning activities must not be disconnected from the lms with it features for learning support."
"one of the strategies that has been proposed for refining initial failure classification relies on tree-like diagram (known as dendrograms). specifically, it uses them to decide how non-homogeneous clusters should be split into two or more sub-clusters and to decide which clusters should be considered for merging. a cluster in a dendrogram corresponds to a subtree that represents relationships among its sub-clusters. the more similar two clusters are to each other, the farther away from the dendrogram root their nearest common ancestor is. for instance, based on the dendrogram presented in fig. 4 we can observe that the clusters a and b are more similar than the clusters c and d. a cluster's largest homogeneous subtree is the largest subtree consisting of failures with the same cause. if a clustering is too coarse, some clusters may have two or more large homogeneous subtrees containing failures with different causes. such a cluster should be split at the level where its large homogeneous subtrees are connected, so that these subtrees become siblings as fig. 6 shows. if it is too fine, siblings may be clusters containing failures with the same causes. such siblings (clusters) should be merged at the level of their parent as fig. 5 depicts."
"both products have been tested with several types of mobile devices, from simple phones with internet access, smart phones and tablets. depending on the operating system of each device, the responses were from excellent to satisfactory. studies on the impact on knowledge-based society and the market research on mobile devices and services revealed that in the coming years these specialized software offers becomes much larger and increasingly more individuals users or organizations will adopt such solutions."
"given the heterogeneity of the web, we cannot expect that we can design a single similarity function which would perform optimally in all cases. thus we need to compute several similarity functions and try to make our decision based on a combination of the similarity functions. to find an optimal way of combination involves a lot of challenges."
"arlearn is and will probably always be under development. however at regular times we schedule releases of new functionality. by implementing real pilots we learn a lot, and new ideas take shope on making simulations better. the runs we organized so far suggest the following extensions."
"we studied entity resolution methods for web data collections, in particular to realize web people search. our techniques rely on pairwise comparisons by similarity functions. by estimating the accuracy of the similarity functions and by combining multiple functions we could demonstrate improvements in performance."
"mobile learning certainly has a viable contribution to play in defense training and performance support, if users are allowed to use their own smartphones. there are many technical challenges in ensuring the mobile content can be deployed over multiple channels, as well as practical issues to consider around the areas of research ethics, and personal privacy, but with careful management these can be dealt with clearly and effectively"
being able to use existing learning material for initial courses can significantly reduce the barrier for providing mobile learning offerings on a broad scale. instead of the necessity for creating entirely new educational resources this approach relies on adapting existing educational material.
"another important aspect of data mining in software engineering is the process of mining design patterns from software, in order to allow for the reuse of software system design expertise. more specifically, the process of mining focuses on extracting patterns by analyzing the code or the design of the software system in order to trace back the design decisions made, which are usually buried inside the source code. typically, during the software system design, the system components are not tagged with the respective design patterns applied, and, thus, the design decisions are no longer connected with the existent system, often leading to lack of understanding of the software's details. in this direction, a number of techniques and tools have been proposed in the past, which attempt to mine the design patterns from a software system."
the ifn algorithm is trained on inputs provided by rtg and outputs obtained from a legacy system by means of the test bed module. a separate ifn module is built for each output variable.
"the primary purpose of this project was to explore the viability of m-learning, and create a transition plan to move it into the mainstream of defense training. this has already been achieved, since several of the partners are already adopting some of the system and processes, and all content created by the project is being transitioned into an ongoing mobile deployment."
"for extracting features from the webpages we used several information extraction tools, including \"alchemy api\" [cit] to extract named entities, \"gate\" [cit], \"opencalais\" [cit] to extract other types of entities, such as organizations and locations. we also extract wikipedia-based concepts using \"semhacker\" [cit] . finally for representing a webpage as document vector we use the services provided by lucene [cit] . the similarity functions we consider are summarized in table i note that the similarity functions are not transitive, in fact, it is very hard to define transitive functions. we use the similarity functions to identify equivalence relations among the entities. as equivalence relations are transitive, we must cope with our inability of designing transitive functions."
"the second product of the project, mkms is designed for business which by nature has employees who are in constant motion, but they need a permanent connection to a portal of knowledge to solve certain tasks."
the above described strategy provides an initial classification of software failures. depending on the application and the user requirements these initial classes can be merged or split so that the software failure are identified in an appropriate fashion.
"the proposed model, named \"mobile learning -net centric based on knowledge access\" -\"access to knowledge and learning database using mobile technologies\" aimed to integrate the cutting edge technologies in the field of computers, wireless communication, educational and knowledge management advanced software. the most important aspect of this enterprise is to find the solution to access knowledge databases on different types of wireless terminals (pda, smartphone, ipod, ultramobile pc, handheld military radio stations etc.) that connect through different communication networks using different specifications and communication protocols."
"however, although research is critical to bringing forth the benefits that technology can bring to both society and the military environment, the process for providing the positive benefits must first take into consideration the importance of research ethics."
"mobiles can play an important role here. although they won't match the immersive feeling a participant gets when entering a real-world simulation (e.g. flying a full-scale flight simulator), mobile devices are very suited to orchestrate a flow of events. furthermore they offer many advantages when dealing with a crisis requires different participants to act together. arlearn is a toolkit that supports the creation of such mobile serious games. in this article, we discuss how arlearn's features can support a mobile simulation. we proceed with an explanation of how this toolset has been used to simulate hostage-taking simulation with employees of unhcr. at the end we conclude with an outlook of new features that we are about to integrate to support more diverse simulations."
"the main goals of data mining are prediction and description. prediction aims at estimating the future value or predicting the course of target variables based on study of other variables. description is focused on patterns discovery in order to aid data representation towards a more comprehensible and exploitable manner. a good description suggests a good explanation of data behavior. the relevant importance of prediction and description varies for different data mining applications. however, as regards the knowledge discovery, description tends to be more important than prediction, contrary to the pattern recognition and machine learning application, for which prediction is more important."
"3) measures of interest: various measures are considered to assess the quality of entity resolution. precision, recall and f -measure are widely used in information retrieval. we also measure the rand-index [cit] and the f p -measure [cit], which is the harmonic mean of purity and inverse purity."
"the mlms elearning application is a n-tier web application (presentation, application and data tiers) built mostly on top of open source technologies. it is truly database independent; [cit] /2008 or oracle 11g. the application tier is divided in two different tiers: the content repository server and the learning management system server. the content repository is a lightweight content management system customized for the elearning activities. it stores lessons, tests, and documents, and offers different features which exist in a document management system like storing information in a files and folders structure, versioning operations, clipboard like features (copy, cut, and paste) and archiving. the learning management system offers features that make easy the interaction between a trainer and a trainee in a remote environment, like: authoring tools (html editor, editors for standard compliant scorm lessons and ims qti tests), managing learning activities in both synchronous (virtual classroom module) and asynchronous modes, assignment of work to other users, system notifications and personal messages between users and also a reporting area."
"source-code repositories contain differences between versions of source code.thus it would be interesting to mine source code repositories, identify and analyze the actual source-code differences."
"-supervised learning approaches, like text classification, based on predictive modeling techniques, for the purposes of predicting future bugs and/or possibly affected parts of code. a measure of future influence of bugs in the source code, associated with a weight and a prediction ranking can show a lot for the software quality. -text clustering of the bug reports, and cluster's labeling can be used to automatically create a taxonomy of bugs in the software. metrics in that taxonomy can be defined to show the influence of generated bugs belonging to a specific category, to other categories of bugs. this can also be translated as a metric of bug influence across the software project. -online mining. the data mining techniques that have recently been developed in software engineering conduct offline mining of data already collected and stored. however, in modern integrated se environments, especially collaborative environments, software engineers must be able to collect and mine se data online to provide immediate feedback. thus a challenging issue is the adaptation or development of stream mining algorithms for software engineering data so that the above mentioned requirement are satisfied. -quality project classification. a classifier will be built to categorize projects as successful or nonsuccessful based on the data collected about projects. these data provide information about features of projects related to the popularity, ranking of projects. the quality of the classification (i.e. accuracy of classifier) depends on the training set. then the requirement is to select the appropriate set of data features based on which we will build an accurate classifier of projects. -association rules extraction from oss project data. there is useful information provided for open source software projects regarding the number of downloads, the number of developers, the popularity, the vitality of the software, etc. these are considered to be metadata of the oss project. analyzing thus information we can extract useful knowledge about oss projects and new quality metrics could be defined. an interesting direction would be to find correlations between the metadata provided for oss. we assume that each project can be represented by a vector (projectid, selected metadata), where metadata refers to the oss development metrics i.e., popularity, activity, number of downloads. the subsequent project's evaluations are stored in the repository as transactions. then an association rule extraction algorithm can be used to discover correlations or co-occurrences of events in a given oss environment."
"however there are types of functions that lead the static analysis procedure to produce false positive warnings. if there is no previous knowledge, it is difficult to tell which function does not need their return value checked. mining techniques for source code repository can assist with improving static analysis results. specifically, the data we mine from the source code repository and from the current version of the software is used to determine the actual usage pattern for each function."
"in this section we present a simple entity resolution framework, which relies on pairwise similarity functions. first we explain how do we combine the accuracy estimations with the similarity values (section iv-a), then we discuss how to combine multiple similarity values (section iv-b). finally we summarize the overall technique in section iv-c."
"in this way, for each function f i we obtain a graph g dj, together with accuracy estimations, where d j is the decision criteria, i.e. whether we decide upon a single threshold or also consider the accuracy estimations. our goal is to combine the the individual graphs g dj into a single graph g combined . first we obtain a multi-graph, where the multiple edges between two nodes correspond to the edges from the individual graphs. we weight the edges with the individual accuracy estimations, which we consider as estimations of the probability of a link. then we compute a weighted average and obtained an optimal threshold, based on our training set. if the combined value is above this threshold, we add an edge to g combined ."
"the evaluation team developed an evaluation process, and script that was embedded into the app, so that users in multiple nations would be able to send feedback on their app usage"
"entity resolution is a well studied problem in the context of relational databases [cit], for a survey see [cit] . even if the papers are dated back quite early, this topic has also regained in importance recently. this is most likely because it is more and more common and easy to combine independent data sources and in this scenario identifying duplicate records is essential. one faces a very similar problem on the web: for example, person names are not unique, but it is often important to identify which person name corresponds to which real world person. such situations include web people search or aggregating information from web-extracted data even if this problem looks very similar to the entity resolution problem in databases, there are important differences. in a database typically one has to identify duplicate records, which is very different from person names. indeed, one can verify that for example the model for fuzzy duplicates [cit] does not hold in our setting. the information what could help here is the content of the webpage, where the name appears. they are on the one hand a rich source of information, but on the other hand this source is often not so straightforward to exploit, because it is very hard to distinguish the relevant information from noise and the relevant information might be even missing."
"weps-2 test data is provided by the web people search clustering task [cit] . the test data consisted 30 web page collections, each one corresponding to one ambiguous name. these 30 person names were chosen from three different sources: wikipedia, acl'08 (association for computational linguistics program committee members) and us census data. each person name was queried using yahoo search api and the top 150 results were included into the dataset. we have evaluated our techniques on weps-2 dataset. we report the performance figures we observed on the 10 person names chosen from the acl'08."
"the ifn algorithm takes as input the training cases that are randomly generated by the rtg module and the outputs produced by ls for each test case. the ifn algorithm repeatedly runs to find a subset [cit] input and desired output logical errors svm classification [cit] program executions logical bugs classification, execution profiles & decision tree of decision trees [cit] result(success/failure) failed executions frequent pattern mining source code patterns and association rules [cit] of call-usage of input variables relevant to each output and the corresponding set of non-redundant test cases. actual test cases are generated from the automatically detected equivalence classes by using an existing testing policy."
"many security and defence organizations and affiliated institutions already make good use of web-based adl technologies for providing trainings for performance support and career development. over the past years, adl solutions have become part of the standard procedures for training and development in these organizations."
"one of the main issues in software engineering is the evaluation of software project and the definition of metrics and model that give us an indication of the future of a project. though a number of mining approaches have been used to assist with software engineering tasks, an open issue is if and how data mining techniques can be exploited to define novel quality metrics in software engineering."
"the nature of the data being used by data mining techniques in software engineering can act as distinguishing means of the underlying methods, since it affects the preprocessing as well as the post analysis. below we present the various sources of software engineering data to which data mining has been applied. the presentation also tries to reflect the difficulty of preparing the data for processing."
"appropriate research protocols must be developed and utilized to ensure that each volunteer is aware of the purpose, procedures and processes, testing process, risk and discomforts, potential benefit, voluntary participate and withdrawal and confidentiality. all of these requirements, as demonstrated by the mobile learning environment (mole) project, proved to be quite daunting and challenging when trying to determine if technology can address defense-related priorities."
"the educational content that is dedicated specifically to mobile devices can have many forms, depending on the capabilities of the targeted mobile device. for modern mobile devices containing an advanced web browser with support for multiple media formats and macromedia flash or javascript programmability, the educational content could have the same complexity and form like in the standard educational content. for mobile devices that do not support a modern browser, the content must be created using specialized standards for structuring and delivering content."
"in addition, results of the research are still in the process of being reviewed. [cit] which will allow this workshop to be the first place that the results are disseminated"
"it can be defined as the time it takes for the created message to reach the destination node. the lower the average latency, the better the performance of the network. the 1-day ttl using epidemic, prophet and scorp routing protocol shows an average latency of 32, 28 and 27ms; with the buffer scheme being implemented the latency is 25ms. the 3-day ttl using epidemic, prophet and scorp routing protocol shows 32, 30 and 28ms of average latency, and with eehimbs the average latency decreases from 25ms to 23ms. the 1-week ttl using the same protocol shows an average latency of 35, 31 and 28ms, and with eehimbs it has an average latency of 18ms. the 7week ttl using the same protocols shows the average latency of 39, 33 and 29ms respectively, and with eehimbs it has an average latency of 18ms. the average latency when using the buffer scheme has reduced the average latency in comparison to the one used without enacting the buffer scheme. the epidemic routing scheme floods the data to throw-boxes and nodes and does not have buffer scheme implemented and so it has high average latency. similarly, the prophet and scorp routing protocols do not flood the data so they have less latency compared to the epidemic routing protocol. the proposed buffer scheme deletes data with high diffusion or high popularity level, and more space is created in the throw-boxes while the average latency is reduced. the results are shown in fig. 4 ."
"the energy efficiency of the proposed routing protocol is compared to the epidemic, prophet and scorp routing protocol. the results are shown in fig. 6 . initial energy of the nodes in all routing protocols is 2100 mah when the number of nodes is 100. as the number of nodes is increased to 200, the epidemic routing protocol has the lowest remaining energy of 1700 [cit] mah, prophet routing's remaining energy is 1800 [cit] mah. as the number of nodes is raised from 200 to 300, the epidemic routing has the lowest remaining energy of 1400 mah, prophet routing has 1500 mah remaining energy, scorp's remaining energy is 1700 mah and eehimbs has a remaining energy of 1800 mah. when the number of nodes is increased from 300 to 400, epidemic routing has lowest average remaining energy 1000 mah, prophet routing's remaining energy is 1300 mah, scorp's routing protocol remaining energy is 1400 mah, and international journal of engineering works vol. 7, issue 02, pp. 143-148, [cit] www.ijew.io eehims has 1500 mah remaining energy. overall, eehimbs performed best due to the buffer management scheme, as the buffer is always free, the time is not wasted in scanning and energy efficiency is achieved. scorp forwards data on interest metric, so it performs well compared to prophet and epidemic routing protocol. the epidemic has lowest remaining energy due to its flooding approach-it sends data to all nodes as it receives it so it wastes all energy in scanning and transmission of data."
"content-oriented routing schemes consider the nature and the interest of data, instead of the host and the location of a node, which increases the performance of the network [cit] . social networks have played a vital role in improving the performance by leveraging the challenges of the huge amount of content existing in the network. because an entry is created in the routing table for each content item, even if we are capable of compressing data, we must cope with the situation of fastmoving content which is produced, moved or deleted with a high speed and occurrence. the paper utilizes the social benefit of different nodes, and the processed information is forwarded to the node if their interest profile matches, or else, the data is forwarded to the static throw-box. while mobile throw-boxes implement the transportation system and increase the performance of the network, static throw-boxes are placed in different locations by the point of interest [cit] ."
"an sdae [cit] consists of multiple daes, which stacks each dae in a deep structure, as shown in fig. 3 . an sdae is able to overcome the difficulty of determining highly nonlinear and complex patterns by learning hierarchical features. the training process of an sdae includes multiple unsupervised pre-training steps and a supervised fine-tuning step. in the unsupervised pre-training steps, an effective way to obtain good parameters for an sdae is by using greedy layer-wise training, which is implemented by training stacked daes in an encoder network by training a layer each time before starting the next layer. as shown in fig3. (b), given a set of training data, the first layer is trained to obtain the hidden representation h 1, then the second layer is trained on the hidden representation h 1 for higher representation h 2, and so on for subsequent layers. in this way, sdae can extract a robust nonlinear representation from input data in an unsupervised manner."
"3) the proposed framework greatly improves the generalization of implementing tsa with nonlinear data synthesis and ensemble cost-sensitive learning compared with existing imbalanced learning methods such adasyn, smote and ros, etc."
2) the proposed ensemble cost-sensitive sdae can effectively mine transient patterns in data and increase attention to unstable samples so as to improve their recognition accuracy.
"where l is the loss of the dae and m is the number of input data samples. once trained, the dae can conduct nonlinear transformation and extract robust features from input data with noise for classification or regression."
"dae is a type of autoencoder (ae) [cit] . as shown in fig. 3 (a), dae is a three-layer neural network which is trained to try to copy input to output. by adding widearea noise to the input, dae learns to remove noise and to approximate the original input data. as a result, dae can learn more stable and meaningful features, which constitute a more robust representation of the input data. in general, dae consists of two parts: the encoder and decoder. the encoder can extract higher order features from input data and the decoder can transform the higher-order features to input data. in other words, the structure of the encoder and decoder are symmetrical. the function of the encoder can be express as:"
"here we have shown the identification of the most common security attacks (sinkhole attack and dos attack). however other security attacks like selective forwarding attack, acknowledgment attack, and wormhole attack can also be identified after an acute study. threshold and time period can be set by the administrator and it depends on the type of network; threshold for multimedia based sensor network will be high than for temperature monitoring based sensor network."
the lepr buffer management scheme [cit] deletes the data based on predictive delivery probability; the data will lower predictive delivery probability is deleted first which frees buffer space.
"the nodes have smartphones for forwarding and receiving data, though most research findings do not consider the energy aspect of smartphones. our contribution in the paper is to provide options and demonstrate that an efficient hybrid buffer management policy might be implemented in throw-boxes to reduce the congestion and improve the energy efficiency when the throw-boxes get full and new data is arriving. for every user, interests are defined and data is relayed if the interest of the receiver node is the same as that of the sender. if any data arrives at the throw-box when the same is full, the data which is going to be deleted first is the one that has a popular interest or increased diffusion level and high hop count as it would have already been propagated to many nodes and other throw-boxes. this will reduce the congestion and increase the energy efficiency, and new data can be spread through the network while the previous one will have the chances to be stored somewhere else. in this hybrid buffer scheme, data with lower interest will also reach the destination node."
"3)the ensemble cost-sensitive stacked denoising autoencoder (sdae) is improved to extract different patterns by employing multi-sdaes with a dropout layer for tsa, and it can pay more attention to unstable samples by increasing the cost of the unstable class."
"is the decoder parameter, w 2 is the weight matrix, and b 2 is the bias vector of all neurons in decoder. when the norm of the difference between the decoded data and the original data is small, it is believed that the hidden layer output of the encoder can represent the characteristics of the input data. the training process finds the optimal [θ e, θ d ] to minimize the square reconstruction of x andx by the gradient descent algorithm:"
"in the scenario of internet and manets, there is always end-to-end connectivity between the source and the destination nodes. the network operates through a global protocol due to the connectivity of all nodes in the network [cit] . delay-tolerant networks are challenged networks in which there is no end-toend connectivity between the source and the destination nodes, mostly due to the mobility of those nodes. in the scenario of dtn, the routing protocol is to forward data by using the storeand-carry approach. in this method, data loss prevention is ensured by storing the data in the buffer until it is transmitted to another node and reaches its destination [cit] ."
"as the first step is completed, apply step two: delete all those messages that have highest diffusion level and hop counts. the following equation is implemented for finding the highest value of pa."
"international journal of engineering works vol. 7, issue 02, pp. 143-148, [cit] www.ijew.io another buffer technique that is used is shli [cit] which deletes the data whose remaining ttl is much lower; this data cannot reach the destination so it is deleted to free up buffer space. this technique performs well until the throw-box becomes full; in that case, the data with high ttl cannot be deleted so this technique fails."
"the growing use of powerful mobile devices, together with the current global, fast-paced development of technology and consumption habits, has raised the users' expectancies and criteria; people are craving for reliable connectivity while on the move. this has resulted in a networking situation where the mobile industry focuses on overcoming the power, cpu, and memory constraints by proposing more sophisticated and better, heterogeneous devices and wireless networks. nonetheless, the intermittent connectivity even in urban scenarios is still a problem since the availability of wireless shadowing, the expensive internet services and closed access points would contribute to the potentially low throughput and the significant delay. with the improvement of the internet, the mobile ad hoc networks (manet) have begun to operate by themselves, and most people already maintain those continuously connected to the internet, thus forming a dynamic and autonomous topology of a self-configuring network of mobile and portable devices. because they are linked wirelessly, those devices rely on almost no infrastructure, and because of the capacity of manets to be forwarding unrelated traffic constantly, they also function as routers. the challenge is to equip each device in such a consistent manner as to permit it to continuously maintain the information required to route traffic properly."
"this section shows the output of our developed tool lands when it is in the middle of the live traffic the monitoring stage. as discussed in the previous sections that, at an instance, the tool monitors a part of an already deployed wsn using an ethical sniffing technique by exploiting the intrinsic characteristics of wireless medium and, unlike other existing diagnosis and monitoring tool, it does not result in rapid energy depletion or additional control packet dissemination through the network."
"2) in order to adapt to the environment of the power system, the denoising autoencoder (dae) is improved by adding wide-area noise to the input layer. adasyn is further introduced to synthesize unstable samples in the hidden space of the dae to accomplish nonlinear synthesis to handle non-temporal data for imbalanced learning tsa."
"with access to a high proportion of renewable energy, modern power systems operate under a variety of conditions, so as many typical operatiing conditions as possible are considered. in order to find the boundary of instability, this paper only considers the most serious contingency, which is the three-phase short circuit. datasets include thousands of samples that can be generated with different operation conditions and contingencies."
"the results of a pathway enrichment can further be visualized in metapathways (e.g. the 'metabolic pathways' map), together with mrna expression data and enriched sub-pathways. all pathways are visualized using keggtranslator [cit], and incromap extends these pathways by visualizing expression data from each single platform therein. therefore, node colour is changed according to mrna expression, and small boxes are added and coloured according to each protein modification's expression value. micrornas are added as small coloured triangles to the graph and are connected to their targets with edges. dna methylation data are indicated with a black bar that shows the maximum differential peak in each gene's promoter (stretching from the middle to the left to indicate hypomethylation and to the right for hypermethylation). this is an interactive graph, therefore, allowing users to modify the layout and selecting nodes to get more detailed information and plots of the associated expression data."
"a simple wams consists of several pmus and a phase data concentrator (pdc). typically, a pmu is installed at a substation and the data sampled by all pmus is sent to a pdc at a location where the data are aggregated and analyzed. as a result, the wams can report data at the frequency in which the power system works for situational awareness to provide complex analysis, control and protection in realtime. in order to effectively utilize the real-time information provided by a wams, many studies have conducted much meaningful work [cit] . this research generally transforms measurements into statistics, such as maximum value of voltage, and then uses these statistics as feature inputs of the tsa model. however, information loss is inevitable in such artificial feature engineering. the model proposed in this paper is an end-to-end analysis process, which avoids information loss in the powerful feature extraction ability without the feature conversion process. some elements that can reflect system dynamics are chosen to construct the input features for tsa; they include generator active power, generator reactive power, bus voltage magnitude, bus voltage angle, branch active power, branch reactive power, load active power and load reactive power."
"in practice, pmu failure, phasor data concentrator (pdc) failure and communication delay may cause missing data both in cyber and physical power systems, resulting in poor performance for tsa. therefore, the information characteristics of wams data in real time will be considered in future works. in addition, the topology changes of power system is another problem to be studied, for which would affect the adaptability of the data-driven model."
"a recent review article on anomaly detection in wsns [cit] focuses on data anomalies, mainly due to security attacks, and the statistical approaches for detecting them. because of their tight coupling to often harsh physical environments, wsns and other networks used in extreme conditions (e.g., in space [cit] ) are more likely to experience anomalies related to connectivity or hardware failures than conventional networks. recent work also focuses on devising detection strategies that target network level [cit], data level [cit], or node and data level [cit] anomalies."
"a first approach to integratively investigate data from any two platforms is the 'data-pairing' procedure. this procedure shows two datasets next to each other, thus, simplifying common lookup task, such as investigating the effect of a differentially *to whom correspondence should be addressed."
"here too, threshold and time period can be set by administrator and it depends on type of network; threshold for multimedia based sensor network will be high for temperature monitoring based sensor network."
"in order to overcome the aforementioned drawbacks to accomplish accurate and rapid tsa considering the serious class-imbalanced problem, a deep imbalanced learning framework for transient stability assessment of power systems is proposed in this paper. the main contributions of this paper are listed as follows:"
dos attack has severe impact on network as it tries to exhaust the resources available to the victim node by sending extra unnecessary packets and thus preventing legitimate network users from accessing services or resources. its identification can be done through a simple approach:
"the hyperparameter of the model is determined by grid search optimization, which includes the number of sdae and the cost of the unstable samples. fig. 9 shows the optimal parameter search process of the proposed method. in order to highlight the distribution of various evaluation indices, contour is plotted. it can be observed that tur increases as the cost increases, but as attention to the stable samples decreases, tsr also decreases. and because the number of stable samples is much larger than unstable samples, so the overall accuracy of samples is diminished. g-mean can be used to make trade-offs between tur and tsr. on the premise of ensuring of maximum g-mean, a lightweight model is obtained in this paper. finally, the number of sdaes is set to 2, and the cost of unstable samples is set to 14."
"1) the proposed framework designs a nonlinear data synthesis method and an ensemble cost-sensitive classifier to implement imbalanced learning for tsa, so that the recognition rate of unstable samples and overall accuracy can be effectively improved."
"for evaluation and testing purposes of our lands tool, we carried out an experiment on an isa100.11a network in a local lab environment. a group of usb dongle sniffers without m2m interface were used as sniffer device. figure 5 shows the basic network and sniffing model. multiple sensor nodes were connected to a gateway, while, on the other hand, a set of sniffing devices (usb dongles) each for a particular ieee 802.15.4 channel sniffed the ongoing wireless communication between the neighboring isa100.11a based sensor nodes. laptop running our tool lands interpreted and prepared packets after receiving the sniffed data sequentially from each sniffer data stream. finally the network statistics were viewed on the laptop screen in three different tabs as explained in the previous section."
"the rest of this paper is divided into five parts. section ii briefly introduces the implications of transient stability assessment. section iii presents the models of adasyn, dae, sdae and ensemble cost-sensitive sdae. section iv proposes a deep imbalanced learning framework for tsa and section v discusses numerical results on two benchmark power systems. finally, the conclusions are drawn in section vi."
"until now, many methods have been proposed for online tsa, and they can be summarized into two modes: mode a and mode b. mode a generates a large number of transient samples offline to train the model. when the real-rime data arrives, the trained tsa model can be deployed immediately, and the model is updated periodically as many new transient samples are collected. mode b trains the tsa model with a massive number of samples produced by time domain simulation offline as well, but it can update itself online as it conducts tsa for a power system. however, the fast update speed of the model in mode b sacrifices generalization, so mode a is used in this paper, which is also called ''offline training, online application''."
"lands is quite a user friendly tool and the output is presented in three different categories separated by tabs. we cannot only see parameters of each header in the form of tree, but different graphs in the statistical view tab tells us different aspects and hidden anomalies present in the network. moreover it also shows a network topology view which can further assist us to monitor the live network quite efficiently."
"in this paper, we propose a portable and user friendly diagnosis and monitoring tool \"latent network diagnosis system (lands)\" that observes, monitor and evaluates the isa100.11a [cit] based industrial sensor network using freescale mc1322x usb dongle. motivated from wireshark, a network protocol analyzer, our java based application receives the ongoing transmission in the surrounding area of m2m enabled sniffer device through wimax interface and shows each osi layer parameters in a live view tab in a tree form. from this view, the user can perform deep packet inspection (dpi). moreover there is a statistical view tab which, as the name suggests, outputs the live sniffed data in the shape of charts and graphs. from the graphs we can see the size and number of data and acknowledgment packets sent in the network, the composite packet rate, channel activity statistics and the number of packets each participating node has sent. a topology view tab shows virtual network topology showing nodes interconnected with other nodes based on their communication pattern (i.e., data packets sent to and from). based on the output from the live view, the statistical view, and the topology view tab, we can get a quick and complete picture of the network environments. we can also identify the problematic node, inspect message contents down to the bit level, and share scenarios with vendors. through the heedful analysis, we can keep an eye on network anomalies (loss of connectivity, intermittent connectivity, and broadcast storm) and hardware anomalies (node failure and node resets). moreover, we have also devised ways not only to identify common security attacks like denial of service (dos) and sinkhole attack [cit], but also to identify the source malicious node responsible for the attack."
"an interest-based buffer management scheme is proposed in throw-boxes to increase the performance of the network. when the throw-box is full, we delete the data with high diffusion level instead of deleting the data with less popularity. the proposed scheme has good energy efficiency compared to other routing protocols. this hybrid approach increases the delivery probability of the network by also delivering the data with less popularity. the overall average latency and overhead ratio are also decreased with this buffer management policy, and we also double-checked and prevented some critical data discrepancies. in future research, this buffer management scheme can be compared with other buffer management policies."
"(a) crucial area coverage: one approach is to put the sniffer devices only at particular areas in a network called crucial areas. we can place these devices at areas where there is high node density, where data delivery is crucial to delay and loss or where we anticipate an intermittent connectivity or data loss."
"ß [cit] . published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution license (http://creativecommons.org/licenses/by/3.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. methylated promoter on mrna level. further, this view is especially suitable to inspect the effect of microrna expression on target mrnas. an arbitrary amount of data from different platforms can be inspected, using the 'integrate heterogenous data' procedure. to keep the clarity, only the most relevant information, that is, the expression values (as fold changes or p-values) are shown. therefore, one row is created for each gene and one column for each platform. a hierarchical representation of the table allows for expanding nodes to get more information, such as all micrornas targeting this gene's mrna (fig. 1b) . a popular method for a generic analysis of expression data is performing a gene set enrichment. we have extended this procedure to an integrated gene set enrichment that is able to perform enrichments across multiple platforms. the user can choose the datasets and thresholds for each dataset to calculate a p-value, using a hypergeometric test for each predefined gene set [cit] . incromap supports gene sets from the kegg pathway database [cit], gene ontology and any gene set from the molecular signatures database (www.broadinstitute.org/gsea/msigdb/). furthermore, biopax level 2 and level 3 pathways can be imported for visualization in incromap."
"in this paper, to overcome the imbalanced-class problem in tsa, a deep imbalanced learning framework consisting of nonlinear data synthesis method syndae and ensemble cost-sensitive sdae is presented. simulations of the volume 7, 2019 proposed framework on benchmark power systems demonstrate superior performance for tsa compared to traditional imbalanced learning methods. the main conclusions can be drawn as follows: 1) compared with the methods based on linear interpolation, the proposed nonlinear data synthesis method syndae demonstrates improved capability in extracting transient characteristics for tsa with non-temporal data."
"although much research has been done in various aspects of wireless sensor networks, little work has been done towards a diagnosis tool for monitoring the statuses of operational systems in the field. this paper proposes a new technique used for remote monitoring of already deployed industrial isa100.11a based sensor network using lands. lands receives the data through the wimax link from m2m enabled sniffer devices and later processes it to give a detailed insight of the network. lands system can be thought as a passive diagnosis tool that exploits the wireless medium characteristics, but it can be efficiently applied to any already deployed industrial sensor network system giving live traffic analysis and statistics in the form of text and figures. what makes lands system unique is that unlike other tools it does not periodically pull the status and other predefined parameters from the nodes; thus it does not result in early depletion of already scarce available energy of sensor nodes. moreover, it does not disseminate additional control packet throughout the network, and hence it does not have adverse effects on network efficiency either. the only limitation is that it provides monitoring only to a part of a network at a time and, in order to get an overview of the entire network, we have to install multiple mobile devices which add to cost and complexity."
"to realize an accurate and rapid tsa, many data-driven methods have been proposed, which mostly employ artificial intelligent (ai) technology to build prediction models offline using massive training datasets. at present, previous relevant research in this area can be divided into two categories. in the first category, researchers use machine learning algorithms to construct the mapping relationship between wams data and transient stability. once a fault occurs, the stability status of a power system can be found according to the mapping relationship with real-time measurements [cit] . works in this area focused on the performance of various classification algorithms and application scenarios, such as decision trees (dt) [cit], support vector machines (svm) [cit], extreme learning machine [cit], and neural networks (nn) [cit] are applied to online tsa. time-adaptive tsa has also been proposed in recent years, which uses continuous prediction via the mapping relationship between post-fault trajectories and stability status with long short-term memory (lstm) [cit] . in the second category, time series prediction methods are applied to predict the future trajectory of the system directly, and the stability status of the system can be assessed [cit] ."
"tsa can be regard as an imbalanced learning problem as the power system is too strong to lose its stability. therefore, if tsa does not consider the imbalance in the power system, it tends to regard the system as stable. once the unstable situation is judged to be stable, it will cause security risk to the power system. in other words, the cost of stability and instability are different. to mine the pattern of instability effectively, an ensemble cost-sensitive sdae is designed, as shown in fig. 4 . from left to right, there are input layers with selected features of a power system, a supervised learning process is represented by ensembled daes to learn from input layers, and a fusion layer and cost-sigmoid layer are deployed to merge higher order representations in the daes. at the left of the ensemble-cost sdae in fig. 4, m sdaes are the feature extractors for unlabeled data. a single sdae can only capture a specific feature of the input data. however, the power system stability pattern is not dominated by just one mode, so a dropout layer is added for every sdae to obtain different patterns. the features learned by the sdaes are heterogeneous because the features represent different physical attributes that humans do not understand. then, the captured features of multiple sdaes are merged by the fusion layer using successive cost-sigmoid layers, and the loss function of the cost-sigmoid layer, can be written as: (10) where l 1 is the loss function of the cost-sigmoid layer, y i is label of i-th sample, t i is the prediction of the i-th sample, w s and w l are weight of unstable and stable samples, respectively. in order to increase the cost of the classification of unstable samples, w s is set larger than w l and w l is always set to 1 as a benchmark so that the ensemble costsensitive sdae can effectively mine the unstable pattern. finally, the cost-sigmoid layer can fine-tune the features learned by the sdaes and fuse the layers using labeled data. in short, the different patterns of features in data are extracted via multiple sdaes and a fusion layer, while the cost-sigmoid layer is regard as a classifier."
"attack. the statistical view tab not only shows the graphical representation of the network, nodes, channels, and packet types, but we can also exploit it for detecting successful security attacks from inside nodes. security based monitoring through lands can provide the following:"
"the output of lands is divided into three tabs each showing different representation of sniffed data. the first tab shows live packets sniffed in a tree form. every packet is associated with the packet number and the channel number on which it was captured as shown in figure 7 . in this view, we can have a deep packet inspection by expanding the target layer header and can view the parameters and addresses present there."
"in a confusion matrix, f 11 represents the number of actual stable samples that are predicted to be stable and so on for f 12, f 21 and f 22 . the index to evaluate the performance of the model is defined as follows:"
"the ieee 39-bus system has been widely used to test tsa algorithms. a line diagram of an ieee 39-bus system is shown in fig. 6 . the system is composed of 39 buses, 10 generators, 19 loads and 34 transmission lines. the generator on the bus-39 bus is the equivalent of the external large power system, and its rotor angle is used as a reference for other generators. the required time domain simulation is performed with digsilent powerfactory [cit], which is a power system simulation tool."
"in this section, we will briefly describe the software level system design and the user interface of our network monitoring tool \"lands\". lands is an all-in-one tool tailor made for wsn and its reliable, efficient, and user friendly output makes it quite different and unique from other existing tools of the same kind. m2m based sniffer devices exploit the wireless channel properties and send the sniffed packets to remote server having our lands monitoring tool running."
"with the extensive installation of phasor measurement units, it is possible to provide online monitoring for power systems. specifically, modern tsa is designed to predict the stability status of power systems with real-time measurement after clearance of a fault when it is subjected to severe disturbance. therefore, rapid and accurate assessment is the core requirement of tsa, to leave enough time for emergency control. fig. 1 shows the dynamic curves of all generators in the stable case when bus 16 of an ieee 39-bus system [cit] has suffered a three-phase short-circuit fault. and fig. 2 shows the unstable case. once the fault is eliminated, the dynamic measurement will be transferred to the control center to predict the stability status for the power system, and then emergency control is conducted. the process of assessment is called tsa. it can be found that the power system loses its stability when some generators lose synchronization. however, with the wide deployment of control devices, it is difficult to cause modern power systems to lose transient stability, which means that there are few unstable cases in the power system database. as a result, data-driven tsa has a huge obstacle in detecting instability patterns."
"we have defined a new term called \"ethical sniffing\" in this paper. as the name suggests, we have incorporated a sniffing technique in order to monitor the network and in turn come up with detailed analysis, statistics, and graphs. at any given time, the coverage of our m2m enabled sniffer device will be limited to a small part of a sensor network due to the small radius of the sensor node radio signals; therefore our goal is to devise other ways to extend our monitoring range to a complete or partial (significant part) network area. we have suggested some of the ways how to implement these m2m based sniffer devices in order to get a wider monitoring scope of the network."
"transient stability is the capability of a power system to maintain synchronization when subjected to large disturbances [cit] . with the interconnection between large power grids, access to high-penetration renewable energy and the construction of power markets, the dynamic characteristics of power systems are becoming increasingly complicated and the risk of transient instability increases correspondingly [cit] . therefore, real-time and accurate assessment of post-disturbance transient stability is crucial for power system security."
"the limitation of existing routing protocols is that they do not consider any buffer management scheme in the nodes and throw-boxes. epidemic [cit] routing protocol floods the data to every node it encounters and considers buffer capacity unlimited. prophet [cit] routing doesn't flood the data to every node it encounters, instead, it forwards the data based on past node records, i.e., if node a frequently encounters node b, there is a higher probability that they will meet again, and data is forwarded based on the history of encountered nodes. this routing protocol also considers the buffer unlimited. the scorp [cit] routing protocol forwards the data based on interests: if the interest of the encountered nodes is the same, data is relayed to the node, else the weight of that interest in that time interval is evaluated, and if it is greater than the threshold the data is relayed, otherwise it is not. this approach is good in terms of reducing the congestion of the network, but scorp does not consider that the buffer of the nodes is limited. the limitation of the protocols explained above is that they consider the nodes to have unlimited space, and so data is forwarded to the nodes as it is received, which is not a practical approach. in practice, the smartphones have limited buffer space, and so a buffer management scheme needs to be implemented. another limitation is that the above routing scheme does not consider that the battery of the nodes can be depleted and an energy efficient algorithm needs to be implemented. the proposed algorithm considers the above two limitations of routing protocols and offers a routing protocol that takes into account the buffer's limited space and the node's smartphone limited battery capacity. until now, very little work has been done on implementing the buffer management technique in throw-boxes, and in this research, the buffer scheme is implemented in throw-boxes and nodes. the proposed algorithm is compared with scorp [cit], prophet [cit] and epidemic [cit] routing schemes for evaluation. above is the algorithm of the proposed routing protocol, and its flow chart is shown in fig. 2 . but we only consider bluetooth for data forwarding when nodes encounter each other, as these are challenged networks and wi-fi facilities are not available everywhere. the range of a bluetooth is 10 meters. in fig. 1, suppose that node a is in range of node b (within 10 meters) so that data in bundle format will be relayed to node b, and similarly, node b is in range of node c (within 10 meters), so node b will forward the bundle to node c and data can reach the destination."
"on the network side we used twelve isa100.11a based sensor nodes and a gateway as shown in figure 6 . these gateway controlled nodes transmit data packets haphazardly to each other using the frequency hopping technique as per isa100.11a standard. in our network scenario (a) at ieee802.15.4 layer, 16-bit short addressing has been used with pan id compression and security disabled;"
"inspired by the approach of some cellular operators to maintain a list of legitimate users and implicitly prevent unauthoritative access, we have proposed to maintain a whitelist containing addresses of all devices deployed in our network, and if during sniffing, we encounter any external node (node address not mentioned in the whitelist), it gets highlighted in the topology view and the next step would be to eliminate it from the network. similarly, we have also maintained a blacklist through which nodes having a particular mac address (known attackers) can be identified."
"2. nodes are defined as different interests. the data in the buffers of the throw-boxes is labeled with interests. one of the techniques is deleting those messages that have less popularity or fewer subscribers, but in this method, the disadvantage is that data with less popularity will never reach the destination."
the deep imbalanced learning framework for online tsa proposed in this paper can be summarized in four steps: 1) feature construction for tsa; 2) index for model performance evaluation; 3) offline training; and 4) online application. the deep imbalanced learning framework is shown in fig. 5 .
"(1) when the power system collects records of some contingencies without a topology change, it retrains the model with the parameter initialization of the existing model, and repeats this step periodically."
"as the working conditions change dynamically, the model requires incremental learning to maintain adaptability. a model updating mechanism is designed to implement incremental learning as follows:"
"it can be defined as the number of messages created and the number of messages delivered to a destination. the higher the distribution probability, the better the performance of the network. the simulation results are shown in fig. 3 . the first scenario, in which buffer scheme eehimbs is implemented, shows 0.85 delivery probability with ttl of 1 day, compared to epidemic, prophet and scorp routing scheme which shows 0.73, 0.74 and 0.80, respectively. the 3-day ttl of epidemic, prophet and scorp routing protocol shows 0.76, 0.77 and 0.82 delivery probabilities, respectively, and with the buffer scheme being implemented-the delivery probability increases to 0.87. the 1-week ttl of epidemic, prophet and scorp routing protocol is 0.78, 0.79 and 0.84, respectively, but with buffer scheme implemented, the delivery probability increases from 0.85 to 0.90. the 7-week ttl performed on epidemic, prophet and scorp routing protocol shows 0.82, 0.85 and 0.87; with the buffer scheme implemented, it reaches 0.94. the increase in the delivery probability is due to the implemented interest-based buffer scheme; the scheme uses the hybrid approach in which even the data with less popularity reaches to the destination node. on the other hand, epidemic routing protocol has low delivery probability due to its flooding approach; the prophet routing protocol does not implement any buffer management protocol but neither does it flood data to all nodes, so it has higher delivery probability than the epidemic routing protocol. scorp routing protocol forwards data based on interests, so it has a higher delivery probability as data reaches the area where the node has high chances of receiving data. the proposed routing scheme achieves highest delivery probability due to buffer management scheme implemented; the data with less priority also gets delivered to the destination."
"to obtain excellent representation of the input, the dimension of the encoder is set to 100 because the dimension of the encoder is generally smaller than the input, and the overall training epoch is 40. fig. 7 shows the training process of syndae which presents spatial changes of synthetic data dynamically. according to the direction pointed by the arrow, the data synthesized by the 1-th, 11-th, 21-th and 36-th training generations are compressed into two-dimensions v 0 and v 1 by tsne [cit] respectively, where the blue points are the original stable samples, the red points are the original unstable samples and the green points are the synthetic unstable samples. from fig. 7 it can be observed that the data synthesized by syndae becomes more and more nonlinear as the training progresses, which indicates that it captures the nonlinear unstable patterns and effectively synthesizes it. as syndae encodes the original data to hidden space where mined patterns of transients related to physical characteristics of transient processes of the power system, then these patterns are to be balanced with synthesis of adasyn. therefore, it can enhance the transient characteristics of the data when unstable patterns are decoded to balance the original data."
"multiple networks can be monitored remotely by a single server having multiple instances of lands running. lands is a java based application and therefore can be run on a variety of operating systems including smart phones and tablets. programmatic layout has been summarized in figure 3 . when the application runs, the interface is loaded by calling the \"gui\" class. when the user clicks on the start button to initiate the sniffing process, \"captureoptions\" java class is triggered and a new window appears. in this window, the user is prompted to select the target channels from the radio button as shown in figure 2 . since isa100.11a runs on the principle of channel hopping, target channels here refer to the ieee802.15.4 channels on which sniffing is to be performed. upon pressing \"start,\" an object of sniffer class is made and \"startsniffing()\" process is executed. for every single channel to be sniffed, a separate freescale mc1322x sniffer usb dongle is required and a separate agent (object) is created as \"snifferagent\". each thread corresponding to its agent is parsed sequentially via the \"initserialreader()\" function. this continues in a loop and every data or ack packet received on each channel is read serially and the header parameters are arranged in a tree-like structure and later displayed in the live view tab of the main topology view tab. within the loop boundary, another function \"drawfigures()\" is also called repeatedly whose sole purpose is to update the graphs and charts in the statistical view tab."
"in order to verify the effectiveness of the proposed method on tsa, this paper compares several traditional methods which are smote, adasyn, ros, random under sampling (rus), conditional generative adversarial network (cgan), to highlight its advantages. all results are obtained by using training and testing data."
"as smote and adasyn only linearly interpolate the unstable samples, so transient characteristics of the data are not effectively extracted, and the recognition rate of the stable sample is reduced with elm after synthetically increasing the number of unstable samples. ros simply repeats the unstable samples, resulting in a decrease of tsr and damage to the generalization of elm. rus delete some stable samples to balance the unstable samples, however, leading to harming the ability to recognize the stable samples. and cgan is difficult to be trained to get the excellent performance for imbalanced tsa. while syndae mines transient patterns effectively by synthesizing the unstable samples in hidden space, so that tur can be improved while maintaining tsr. the ensemble cost-sensitive sdae further enhances tur with powerful feature extraction capabilities and increases the cost with misclassification of unstable samples. this indicates that the proposed method can effectively identify unstable samples and maintain a very high g-mean and acc."
"different data synthesis methods, including smote, adasyn, ros and syndae, respectively balance the original data and compress these balanced sets into a two-dimensional spatial distribution as shown in fig. 8 . fig. 8 (d) also shows the final training result of syndae. compared with these other three methods, syndae clearly belongs to nonlinear data synthesis methods by mining complex patterns from original data. the synthetic unstable data of syndae and the original data form a stable boundary with a significantly larger interval than other methods. this is because smote and adasyn are based on linear interpolation, so then the synthetic unstable samples are distributed between the original unstable samples. in other words, the data synthesized by smote and adasyn are not implemented according to the transient patterns, so the correlation with the physical characteristics of the power system is not strong enough to form a stable boundary as large as syndae. ros randomly copies unstable samples to balance the original data results in the dispersion of the spatial distribution of unstable patterns, which undermines the generalization to a certain extent. therefore, syndae can use the synthetic unstable patterns to help classifiers more effectively distinguish transient data."
"this paper assumes that data of wams is not polluted in the previous discussion; however, pmu measurements are accompanied by noise that affects the performance of the model used for tsa. therefore, this section will discuss the impact of wide-area noise on the proposed methods and compare the robustness of different methods."
"the purpose of security manager is tomonitor the device abnormality and to detect various security threats. in order to provide industry-level wireless network system, isa100.11a should guarantee robustness from security threats. security manager in proposed framework detects denial of service attacks, channel jamming, node impersonation, and wormhole attacks and notifies the network administrator with special warning methods. after the following processes are complete, sniffed data is finally posted to the lands gui categorized as live, statistical, and network view tab. more detailed explanation along with figures is provided in section 6."
"on the sniffer side, in order to capture all channels of ieee802.15.4, we used 16 freescale mc1322x usb dongles mounted on a 16-port usb hub as shown in figure 6 . the usb hub was connected to a pc running lands and it captured the live packets as they were being sent by the target nodes."
"a wsn, unlike an enterprise network, is featured by its hierarchical multilevel structures, which can hardly be approximated by the bipartite graph model used in most of the enterprise network monitoring tools, for example [cit] . it is also impractical to maintain the network dependencies as stable inputs in highly dynamic and self-organized sensor networks."
"research on wsn monitoring and diagnosis of already deployed networks has not gained much attention while it is a critical issue as it has a direct influence on successful, efficient, and secure network operations. one of the reasons for this being neglected is that they are notoriously difficult to develop and debug. most existing tools for wsn diagnosis are built on proactive approach, in which each sensor employs a debugging agent to collect the relevant status information and reports to the sink by periodically transmitting specific control messages which in turn decreases the network efficiency and can accelerate the energy depletion rate of sensor nodes. some researchers propose to monitor sensor networks by scanning the residual energy [cit] of each sensor node and collecting the aggregates of parameters of sensors where the in-network processing is leveraged. by collecting such information, the sink is aware of the network conditions. periodic transmission of metrics from nodes to the sink is not a new idea. mintroute [cit] includes periodic transmission of neighbor tables to aid in debugging at the sink. however, it neither includes other metrics nor performs failure analysis at the sink."
"buffer management policies improve the performance of the network, but little research has been done about implementing it in throw-boxes. in the fifo (\"first in, first out\") policy, the message is deleted as soon as the buffer is full. the disadvantage is that the priority messages will also be deleted [cit] ."
"at first, a serial reader reads sniffed packets from a single or multiple k1322-sniffer interfaces and sends special commands to sniffing protocol for resetting the sniffer cpu, setting capturing channel and capturing mode, and so forth. after that, the signal merger receives multiple data streams from the serial reader and combines them to serialized frames. each frame contains captured packet from a single interface. since k1322-sniffer interfaces capture signals from multiple channels, correctly differentiating these signals into frames is an important task. network manager processes capture packets, extracts device and network information, and prepares statistical information related to network. this component also estimates efficiency of network based on collected statistical information. the parameters of network efficiency include tcp/udp throughput, header compression efficiency, packet loss rate, packet delivery latency, and so forth. channel manageris responsible for quantifying channel usage. isa100.11a uses three types of hopping schemes which are (i) slotted hopping, (ii) slow hopping and (iii) hybrid hopping. channel manager estimates how each channel is efficiently used. channel manager also evaluates interference and intrusions, and suggests which channels should be avoided for efficient data exchange."
"therefore, we developed incromap, a user-friendly and interactive application with a graphical user interface that is specialized on an integrated analysis of cross-platform microarray and pathway data. incromap supports dna methylation, messenger rna, microrna and protein modification datasets. besides these platforms, it is possible to import data from any platform that contains expression values that can somehow be assigned to genes. a special emphasis has been put on the usability of the application. hence, all required files, for example, for mapping gene identifiers to gene symbols, annotating mrna targets to micrornas or pathways to visualize, are either directly included in the application or downloaded dynamically in the background."
"the overhead ratio using epidemic, prophet and scorp routing protocol with a 1-day ttl is 0.70, 0.64 and 0.63, and with eehimbs implemented, the overhead ratio is 0.60. the overhead ratio of the above same protocols with 3-day ttl shows 0.77, 0.69 and 0.64, and with eehimbs implemented, it drops from 0.64 to 0.59. the overhead ratio with 1-week ttl using the above routing protocols shows 0.80, 0.76 and 0.78, and when using eehimbs, it drops from 0.59 to 0.55. the overhead ratio using the same protocols with a 7-week ttl has the values 0.81, 0.76 and 0.69, and when using eehimbs, it decreases from 0.55 to 0.54. the epidemic routing protocol has highest overhead ratio due to its flooding nature, compared to scorp and prophet routing protocol. scorp routing protocol forwards data based on interests, so the overhead ratio is less compared to epidemic and prophet routing protocol. eehimbs has lowest overhead ratio due to deleting the data as the buffer gets full, and data will ttl less than 5 hours also gets deleted. the overhead ratio can be defined as the number of extra bytes sent in the network for data to reach the destination node. the lower the overhead ratio, the better the performance of the network. the results are shown in fig. 5 ."
"besides those integrated analysis methods, incromap allows plotting region-based dna methylation data in a genome plot with boxes for gene bodies, which in turn can be coloured according to mrna expression. further, all enrichments can also be performed on any single dataset, which is straightforward for mrna or protein datasets, but implementations that can also handle dna methylation or microrna data are less common."
"in this paper, an sdae is used as an excellent feature extractor to learn robust nonlinear representation for classification from noisy input data. to obtain accurate tsa, an ensemble-cost sdae is proposed in next section while considering imbalanced learning."
"since tsa has the defect of ignoring unstable samples, it is unreasonable to use only the accuracy of all samples as the evaluation index for the model. assume that stable samples account for 99% of the dataset, then the tsa model can achieve an accuracy of 99% by judging all samples to be stable. in order to effectively evaluate the performance of the model under the class imbalance, this paper utilizes the confusion matrix [cit] shown in table 1."
"the algorithm searches for data that has highest pa value a, has highest number of copies spread and has passed a large number of hops."
"as seen in table 4, the original data has the poorest ability to detect unstable samples while syndae can maintain the generalization for stable samples with moderate performance of unstable samples; therefore, syndae achieves the highest acc among the eight imbalanced learning methods. when the ensemble cost-sensitive sdae is trained with data synthesized by syndae, the proposed method efficiently extracts stable and unstable patterns in the datasets, resulting in desirable performance of acc and g-mean. cgan shows good performance at larger power system, but its ability to capture the unstable situation is not stronger than proposed method. by intuitively repeating or deleting unstable samples, ros and rus creates a bottleneck by recognizing them. additionally, methods based on linear interpolation, such as smote and adasyn, cannot capture enough transient characteristics of the power system to achieve state-of-the-art performance."
"tsr represents the proportion of correct results predicted to be stable from all stable samples, tur represents the proportion of correct results predicted to be unstable for all unstable samples, g-mean is the geometric mean of tsr and tur, which can effectively evaluate the performance of the imbalanced data, and acc represents the overall accuracy."
(d) divide and monitor: real-time active monitoring of entire network can be done efficiently by dividing the network into logical subnets such that each subnet has a dedicated mobile sniffing device. figure 1 exhibits the scenario.
"one shortfall of the existing strategies is that none of them comprehensively addresses network, node, and data level anomalies in wsns. moreover, these problems are often not encountered during predeployment tests also, because the environmental conditions that trigger these problems are hard to simulate in the lab. one common difficulty is determining which metrics should be used to evaluate the health of the sensor network. many of these sensor networks are used to report measurements about the environment, such as temperature, light level, and sound level. these nodes also have awareness of internal metrics such as processor utilization, current draw, and battery voltage level. when fielded to monitor the environment, these nodes are usually configured to sense and report on only a few of these data streams. the lack of comprehensive anomaly detection strategies for wsns contributes to slower adoption and more frustration in deploying and maintaining these networks. therefore, sensor networks have to be inspected in-situ on the deployment site to identify and locate failures and their causes. determining the health of a sensor network is a difficult, yet important task. it is crucial that all sensor data reported by a sensor network is accurate so that it can be trusted by its user. nodes may malfunction due to loss of power, extreme environmental conditions such as temperature or precipitation, or physical damage caused by falling debris or wildlife. monitoring the health of a sensor network helps increase its trustworthiness by reporting nodes that may be malfunctioning."
"the datasets are constructed via time domain simulation, the generator is a six-order model, and the load uses the constant impedance model. considering a load level ranging from 75% to 125% in a step size of 5%, with the generator output level adjusted accordingly. in other words, the load level and generator output level change at the same ratio. then, power flow is calculated, and if the power flow has converged, the operation condition is preserved."
"the rest of this paper is organized as follows. section 2 gives an abridged version of related work done in the past. monitoring scenario proposed by us for efficient and large scale monitoring of industrial sensor network through lands has been proposed in section 3. section 4 describes the design and description of our developed tool. isa100.11a network deployed in a lab environment to test lands has been explained in section 5, while the final experimental evaluation has been done in section 6 with special focus on common security threats and their detection scenarios. finally we have concluded our paper in section 7."
(b) network of sniffer devices: we can also place multiple sniffing devices in the network haphazardly or separated by some distance and create a separate network of sniffer devices. hence every device will keep an eye on its surrounding area and monitoring of the entire network can be done.
"in order to effectively mine instability patterns from massive power system datasets, this paper considers the factors affecting the mining of instability patterns from two directions. due to the small amount of unstable data, the datadriven tsa will consider the unstable case to be an abnormal point, so a feasible method is proposed to increase the weight of unstable samples through synthesis. in addition, it can also ensemble models and increase the cost of classification in the unstable case to improve the model's performance in detecting instability, which will help tsa effectively mine unstable patterns. the imbalanced learning framework proposed in this paper is composed of these two aspects, which will be introduced in detail in the following sections. (1) calculate imbalanced degree of class:"
"the nucleus network management system (nms) infrastructure helps sensor network applications export debugging and monitoring information [cit] . nucleus' support for exporting statistics and recording application metrics is not only easy to use but also to lightweight, but the limitation of nms is that it does not provide infrastructure to analyze these metrics. furthermore, these metrics consume more than double the ram required for the rest of the stack."
(i) dhdr subheader specifies that slow hopping offset and daux subheader are included whereas signal quality in ack is excluded. (ii) isa100.11a compress feature has been set to null as per drout sub-header;
"typical workflows for the analysis of microarray data involve several steps, namely, the preparation of samples and arrays, their hybridization to arrays, scanning the array and processing the image to read out the raw probe intensities. depending on the array type, several quality control and low-level data analysis steps are then performed in silico. these steps mostly include normalization, annotation of gene identifiers and the calculation of diverse measures of differential probe-level intensities (such as p-values, fold changes or log ratios). mostly, these tasks are performed in r, a statistical programming language (www .r-project.org) or by using derived applications with a graphical user interface (e.g. mayday; [cit] ). the processed datasets can then be used in various high-level data analysis tools for further evaluation and data mining. a popular example is the commercial ingenuity pathway analysis software (www.ingenuity.com), which links processed microarray datasets with pathway analysis. however, most of these high-level analysis tools are specialized on single platforms, and only a few approaches are available for an integrated analysis of high-throughput data from heterogenous platforms. furthermore, not many software tools are freely available that offer suitable and easy-to-use analysis and visualization techniques for microarray platforms, other than mrna expression arrays."
"network monitoring using lands can be further enhanced to detect active malicious attacks and to pin point the source at node level. as a result, the network once attacked can be quickly brought back to normal routine and the sensor network can be protected from prolonged internal and external attacks."
(c) mobile sniffing devices: another approach can be employed using mobile sniffing devices that will sniff the data required for monitoring as it traverses through the network in a fixed predefined path or as directed by the network administrator through wimax downlink. the mobile sniffer devices can also be programmed to be active and monitor the network periodically.
"for an ensemble cost-sensitive sdae, the m is larger, and the performance of the model will improve and the training time will increase correspondingly. to speed up learning with m as large as possible, the batch stochastic gradient descent (sgd) method with momentum, which is called adam [cit], can be introduced to update the weight of the model. in this way, an ensemble cost-sensitive sdae can extract robust nonlinear features from data and perform excellent imbalanced learning to realize balanced tsa."
"in order to test the scalability of the proposed method, a larger power system is employed to be analyzed, namely south carolina 500-bus system. the same database generation method used in the ieee 39-bus system is utilized to construct datasets that includes 34725 samples with 1243 unstable samples, and the datasets are split into training data (80% of datasets) and testing data (20% of datasets) for the proposed method."
"once a fault is detected, the online application program is triggered. after a cycle of a fault clearing, real-time measurements are acquired in observation window of a cycle, and the critical features are selected from them. then, we input these critical features to the ensemble cost-sensitive model, and the model will perform tsa for the power system. if the power system is assessed as unstable, emergency control will be performed immediately; otherwise, the trained tsa continues monitoring the stability status of the power system during the next cycle."
"to be more specific, lands consists of five major components: (i) serial reader, (ii) signal merger, (iii) channel manager, (iv) network manager, and (v) security manager. detailed system design summarizing the complete process from frame sniffing to data display stage has been shown in figure 4 ."
"in the fine-tuning process, as shown in fig. 3(c), all hidden layers trained during pre-training are stacked and a sigmoid layer is added on the top of the stacked deep learning architecture. to train an sdae efficiently, the parameters in all layers of the sdae are connected to corresponding parameters learned in the pre-training phase, then the sdae is fine-tuned with label information by the back propagation (bp) algorithm [cit] ."
"while significant work on conventional network management tools exists [cit], wsn counterparts have been slow to gain such efficient and useful tools. one of the main challenges for wsn anomaly detection is determining where to embed the intelligence for detecting and localizing anomalies. another key requirement for any anomaly detection strategy is to cater for the needs and feedback of the human operator. a user-friendly detection strategy should provide several modes of notification, such as email and sms alerts, and adapt its frequency of alerts to user feedback [cit] ."
"where pa is the equation used for deleting the messages from the throw-boxes, p is the number of copies spread in the network, and hp is the hop count factor of the data."
"by studying typical protocols used in sensor networks, we found that a great deal of information about the state of the sensor network can be inferred from a message trace. for example, we can detect node failures and node reboots without modifying the protocols used in the sensor network. we can even infer routing topologies or detect the existence of network partitions without touching the sensor network. lands was originally designed for diagnosis and monitoring of wsn, but it also provides some additional benefits against detection and elimination of active security attack. generally, if an attack is successful on a wsn, it is very difficult to identify and to eliminate the attack, but, through lands, we cannot only identify the threat but also reach the source of the attack in a short time. from the security point of view, some of the protection scenarios are mentioned below."
"in the procedure as figure 5 shown, the datasets with selected features are divided into training and testing data. dae converts training data into hidden space, and the hidden space data are synthesized by adasyn to balance the class amounts, which means the ratio of numbers of stable samples and unstable samples is set to 1:1 by synthetically increasing the number of unstable samples. then, the synthetic data are converted back to the original space of the training data. finally, the decoded data are used as input for the ensemble cost-sensitive sdae, and it is trained using the method introduced in section iii. testing data are utilized to evaluate the trained model with the indices introduced in section iv for good performance in subsequent online application."
the mofo buffer management technique deletes the data that has most copies or is propagated the most in the network since that data would have travelled in the network for a period of time and so it has had higher chances to reach its destination [cit] .
"the experiment is implemented to explain the physical meaning of the nonlinear data synthesis method proposed in this paper, and the methods including dae and adasyn are hereafter referred to as syndae."
"the other two tabs shown in figures 8 and 9 are more of a graphical representation of data. in the statistical view tab, there are four graphs showing network statistics (figure 8 ). the first graph shows the channel utilization. a bar chart shows the number of packets sent through each channel. the second graph is a line graph showing the network efficiency; it shows the number of packets transmitted as the time progresses. another graph on the bottom left classifies the packet according to the packet type. a pie chart shows the distribution of broadcast, multicast, and unicast packets. the last graph shows the packet size; each node has transmitted since the sniffing by lands began. the third tab, the network topology view tab, shows the topology view of the surrounding network. sensor nodes sending data to each other will be interconnected with each other as shown in figure 9 . in this exemplary view, we can easily infer that the middle node is broadcasting packets to the surrounding nodes."
"wireless sensor networks (wsns) enable users to interact with the physical environment at an unprecedented level. their tiny, cheap, energy efficient, robust, and scalable properties have resulted in their deployment for a wide range of applications, such as military, health monitoring [cit], data acquisition in hazardous environments; and habitat monitoring [cit] . much of the research has been done on wsn in general issues like routing, reliability, qos, energy consumption, and security, but very little work has been done targeting the in situ network diagnosis for testing operational sensor networks. existing tools (debugger, testbed, simulation, and emulation) do not work for deployed networks, thus it is of great importance to provide network developers and administrators with some useful information on a system's working status. wsns are by nature error prone and have unsatisfactory reliability, encountering various faults and failures during their operation. diagnosis and evaluation of the deployed networks will enhance the applicability, reliability, and efficiency of wsns."
"the pseudo code for rough set clustering is stated as follows: for all transactions obtained in module2 from fig.1, 1 . construct similarity matrix 2. find similarity class for each transaction 3. perform upper approximation by considering objects in k neighborhood, by which all transactions whose intersection with class considered is not equal to ø are combined. repeat step (3) until result of two successive iterations are same."
"apriori algorithm finds frequent k-item sets using apriori property that all subsets of frequent item sets are also frequent and an anti monotonic property that all supersets of infrequent item sets are also infrequent. for every learner session, the user id, each page viewed and time spent on each page is available. at very first step, all pages are placed as candidate 1-itemset, and their support count is recorded. from fig.1, the large (l 1 ) item sets are generated by eliminating those pages that do not have minimum support or those referred less frequently as they are not useful for mining risk. the large items are also called frequent 1-itemset. in the subsequent stages candidates (c k ) are generated by joining l k-1x l k-1 in lexicographic order. the candidate item sets 0.17 0.14 0.14 0.17 0.14 0.14 1 0.14 0.14 0.14 0 0.17 0.14 0.14 0.17 0.14 0.14 0.14 1 0.14 0.33 0.17 0.17 1 0.6 0.4 0.33 0.33 0.14 0.14 1 0.14 0 0.4 0.14 0.14 0.17 0.33 0.33 0.14 0.33 0.14 1 whose subsets are infrequent are pruned. and the remaining c k item sets are used for finding frequent k+1 item sets (large item sets).this process is repeated until k+1 frequent item sets are generated, for active learner session containing k previous browsing history pages [cit] ."
the proposed e-learning recommendation work involve page that takes boolean values as viewed or not viewed. hence it requires generation of boolean association rule. the variant of apriori algorithm for finding frequent k item set is proposed to find frequent learning sub sequence of pages. the proposed approach is advantageous in two ways.
"in this paper mining is performed using only page view details. hence it is sufficient to retrieve only page view details. the effect of web crawlers and web spiders which run indefinitely are totally irrelevant to mining task. such items are filtered out. similarly images and erroneous information such as noise are also removed. only the ip address, page view and total time spent on every user is retained by preprocessing phase. the total time spent by user on all his reference to a particular page in a single session is determined. this parameter is retained in order to calculate average time spent by user in a page. this is used for resolving ambiguity when contradictions arise in association rule mining. very short and very long transactions are eliminated as they affect cluster validity. transactions containing only least referenced pages, that is, those with poor support are eliminated. similarly transactions having only pages that appear in all transactions are eliminated, as they do not provide useful information. hence a sample transaction after preprocessing phase is represented as,"
"clustering is done automatically, every learner is assigned to suitable cluster and most recent learning behaviors are considered for personalization. using similarity measure instead of distance measure reduces computational complexity. for every prediction, only one cluster to which current user belongs to is considered. it reduces computational complexity and improves recommendation accuracy."
"e-learning is the process of learning through web, a vast information resource. however e-learning is advantageous regarding huge subject content, presentation of materials, discussion facility through forums and chats etc., the main drawback is poor tutor-student interaction and choosing appropriate learning material from the enriched data source. this paper attempts to provide good e-learning personalization by web usage mining."
"the clustering of web sessions is the key aspect as it groups similar learning patterns. hence in making e-learning recommendations, instead of considering all click stream sequences, it is necessary and sufficient to consider similar learning patterns. the highlights of this approach are as follows considering learning activities of learners of similar interest (in a cluster) can improve recommendation accuracy."
"association rule mining using apriori is applied for personalization. the recommendation accuracy is improved in this stage in following ways, (i)all k th order approach is proposed by which the longest possible browsing sequence is considered for association rule mining, (ii) contradictory predictions are reduced. as only small, highly similar data set (cluster) is considered, consequently the number of rules generated are very less, (iii) ambiguous predictions if any, can be resolved by average number of bits per session and average access time user per page. the most referenced page is selected for recommendation. (iv)minimum support and minimum confidence parameters are set in such a way to eliminate false discoveries. when minimum support is too small, every rule will get a chance to be true, leading to wrong recommendation and when minimum support is too large, for small data set, wrong predictions may occur due to poor coverage. but in the proposed approach, as clustering phase produces only similar patterns, the impact of minimum support and maximum confidence is not explosive regarding accuracy of prediction, (v) one of the major drawbacks of associations rule mining is that too many rules are generated and no guarantee for all generated rules to be relevant. in the proposed work, as clustering reduces the input data set to be small for association rule mining, consequently the number of rules are reduced and the extracted rules are highly relevant and meaningful."
"the patterns of learners of similar interest are discovered by clustering. clustering is the process of grouping data objects such that, objects within a cluster are highly similar and objects in different clusters are dissimilar to each other. a roughest is defined by pair of sets which give lower approximation and upper approximation of the set. lower approximation includes elements that definitely belonging to a concept where as upper approximation of a set includes elements that possibly belonging to a concept [cit] ."
"the performance of e-learners after providing recommendations is good. from fig 2, the number of poor students is reduced. most of them are moved to average performance. as recommendations are more useful to slow learners, the overall learning effectiveness is improved. the accuracy of learning recommendations is defined as percentage of learners followed the recommended page."
updating course content of web site according to previous usage information. (vi) identifying groups of learners of similar interest and sending personalized course materials to interested groups.
"in the proposed work homogeneity of clusters is improved leading to determining highly similar user groups through simple calculations. khalil proposed markov model for web access prediction in which for analysis of every test session, all training data are considered, some of them can be less relevant to test session that affects prediction accuracy [cit] .moreover, markov model uses only strict consecutive and sequential page access for matching session during prediction. it might lose some of the loosely connected but interesting sessions."
in this paper it is proposed to combine upper approximation based rough set clustering [cit] and dynamic support pruned selective association rule mining using apriori for e-learning recommendation.
"the proposed work achieves good recommendation accuracy, with less computational complexity. the recommendations are more useful to beginners in web based learning systems. in this work association rule mining is tried in different approach and overall learning effectiveness is improved through recommendations. clustering played a vital role in reducing complexity of mining process and its contribution on making recommendations is also good. for future researchers, it is left out that, this work can be extended on different portals with different mining strategies."
"the active learner can make use of these recommendations for further learning. if no matching antecedent found, then next lower order predictions using only latest 1-page \"course\" are used. in this way, proposed approach improved the effectiveness of learning."
"data mining is the method of extracting implicit and useful patterns automatically from databases. web mining is a task that extracts hidden information from web relevant information. web mining is divided into web content mining, web structure mining and web usage mining. web content mining deals with discovery of useful knowledge from actual content of the web pages, in the form of text, audio or video. example -search engines. web structure mining deals with analyzing link structure and topology of hyperlinks. example -adaptive web server. web usage mining deals with analysis of various log file data including web server log, client log and proxy server log and discovering useful knowledge regarding web usage. web usage mining can be applied to e-learning domain as the site records information recording learner profiles, web access information, academic details of students and evaluation results. web usage mining can track learning activities and identifies web access patterns and user behaviors [cit] ."
"the input data used for the proposed web usage mining model is the web server's access log. the web server records all learning activities carried out in the learning portal in the access log. it includes ip address,url,referrer url, response code, size of files downloaded, date and timestamp etc., for experimentation of proposed work a sample log of www.e-learningcentre.com is used."
"(i) instead of finding longest possible frequent item set, frequent(k+1) item set is generated. value of k is determined by active learner sessions browsing hit. (ii) instead of using whole data set, only one cluster is used."
"the recommendations are provided in the decreasing order of confidence. no ambiguous predictions occurred. if the learner followed \"index/course/?\", then frequent 3itemsets(c 3,l 3 ) are generated and next set of recommendations are provided in decreasing order of confidence as follows."
"the rules that satisfy minimum support and minimum confidence are called strong rules. using frequent k+1 item sets determined by apriori algorithm, association rules are generated as follows for frequent (k+1) item set, calculate table 2 . frequent item set generation using dynamic apriori for cluster 0"
"for an e-learning portal under analysis, 100 learning sessions are tested. recommendation accuracy of 84% is achieved. it means, out of test sessions considered, 84% of learners followed the recommendations made by the proposed approach. figure 2 . learning effectiveness analysis before and after recommendations however, it is not necessary that new learners should follow the recommendations. but, in this experimentation, out of 100 learning sessions considered, 84 learners viewed recommended pages. thus, recommendation accuracy represents the usefulness of recommendations while learning. in order to evaluate the effectiveness, two online tests were conducted for same set of students. first test after self-learning and second test after learning through recommendations."
"ecdh have many advantages over rsa and dh. some of the advantages that come with ecdh systems can be briefly explained in terms of its resistance from attacks, strong encryption with less number of bits in key etc. these differences are given in brief below: [cit] 6.2.1."
"in this work i have proposed a new group key management approach in distributed network. this protocol is based on logical key hierarchy. i have proposed usage of symmetric cryptosystem along with asymmetric cryptosystem. for asymmetric key, elliptic curve diffie-hellman key agreement is introduced."
"these solutions have attacked the problem from multiple angles, with feature matching [cit], χ 2 minimisation [cit], and cross correlation [cit] the common three methodologies employed. usually these solutions are bespoke software packages designed for a specific instrument. we examine the redshifting requirements of two new surveys in this paper, ozdes and 2dflens, and from these requirements evaluate existing solutions and detail the creation of a new software package now in use by the surveys. the resulting marz software is a webapplication that, thanks to its ease of use (drag and drop a fits file), may be adaptable as a generic viewer of spectroscopic fits files."
"mobile devices have limited battery life so the requirement of key management algorithm is that it has to be computationally fast in order to reduce the power consumption of key management process to insure maximum battery life. our approach hierarchical key management uses elliptic curve cryptography for key exchange between leaf nodes. elliptic curve cryptography provides greater security with small key size and scalar multiplication is computationally fast. so use of ecdhsa protocol will provide more suitable and efficient technique for key management in manet. now i'm going to present my efficient approach, ecdhsa, for distributed secure group communication. the inspiration of this approach is to decrease re-keying overhead at join and leave operation of nodes. ecdhsa focuses on member collaboration for key calculation instead of key delivery by centralized sponsor or co-distributor. for this reason, i'm introducing three basic characteristics of ecdhsa."
since we have seen using elliptic curve cryptography have enhanced the security level with small size of key because it provide same level of security that rsa and dh provide with large size of keys. we have also seen that scalar multiplication of ecdh makes it very suitable to be used for manet because mobile devices have limited battery life and scalar multiplication takes less time in computation. as a future work authentication capability can be added to nodes that reply to the new node. for this purpose we can use ecdsa or any other suitable protocol for authentication purpose of new nodes. we can use variance of ecdh protocol that will provide more advance security mechanism.
"a command line interface has also been developed for marz. it requires node.js 6 to be installed, and the two node dependencies are installed when running install.sh -minimist 7 and q 8 . the run script provided (marz.sh) take a minimum single argument -the path of the file or folder to analyse. if the path points to a fits file, marz runs against the file. if the path specifies a folder, marz will run against all fits files inside the folder (not recursively). specific configuration options, such as the output folder for .mz files, can be configured either through the command line or by modifying autoconfig.js. more detailed usage instructions and command line parameters are detailed in the project github readme, or by the application itself when run without input parameters. 9"
"hierarchical approach with diffie-hellman and symmetric algorithm), uses hierarchical key tree to manage the keys logically. in this protocol, the combination of diffie-hellman key agreement and symmetric key is used. diffie-hellman key agreement is introduced to the leaf nodes of the key tree where the members are assigned, and symmetric key is introduced to intermediate nodes."
"performance testing for marz was conducted by looking at two sets of distinct data -one from the ozdes team with low signal-to-noise data at high redshift, and one from the 2dflens team with medium signal-to-noise data. in both cases, manual redshifting was performed by experienced redshifters in runz, and the automatic matches produced by runz and the automatic results returned by marz were compared to the manually assigned redshift for all results assigned a qop of 4. comparisons were also made with the autoz program, which is the software being used for the gama survey. these surveys have a smaller number of object types and smaller redshift ranges than ozdes, and therefore have simpler requirements for the redshifting software. these high and low signal-tonoise results are shown in figures 7 and 8 . the redshifting accuracy of marz for high signal-to-noise data gave the correct redshift for 97.4% of qop4 spectra, a failure rate far less than that offered by runz and comparable to autoz. for the low signal-to-noise (high redshift) ozdes data, the accuracy of marz was 91.3%. this is in comparison to the best runz algorithm giving a total accuracy of 54.6%. the lower success rate of autoz, 48.0%, is because the redshift ranges and object types found in the ozdes data are outside the matching capacity of autoz."
"given the above motivation, we drew a set of minimum requirements a modern software replacement would need to satisfy: liable, when compared to existing redshifting solutions. 2. the interface should be intuitive to allow fast manual checking and correction or verification by the user. 3. the software should be easy to install, operating system independent, and be able to be updated without user prompting."
"the code base, named marz, is hosted publicly on github, 4 allowing for open issue management, feature requests, open collaboration, forking of the project and instant web-hosting. 5 as a web page, marz updates automatically, and changes to the matching algorithm and output are reflected in an internal variable which stores the software version. significant upgrades will also be released as product versions via the tagging capacity of git."
the key management scheme that was previously being used was dhsa [cit] and in this scheme diffie-hellman key agreement was used to deliver the group key between nodes. the diffie-hellman key agreement scheme has following issues:
"2 the resulting decrease in automatic matching capacity by the legacy software created an undesirable workload for the members of the ozdes team to manually redshift observed spectra. additionally, the legacy nature (fortran, pgplot, cshell, figaro libraries and starlink libraries) of the runz code base makes code updates difficult, installation and usage complicated, especially for new users faced with having to compile, install and learn the software. the sum of these factors prompted a search for alternative software, leading to the development of a modern software replacement."
"in manet nodes are mobile in nature, due to the mobility, topology changes dynamically. due to its basic ad-hoc nature, manet is vulnerable to various kinds of security attacks. so the secure key management scheme is prime requirement of mobile ad-hoc network."
"the interactive interface consists of five primary screens: the overview, detailed, templates, settings, and usage screen. the first two screens -the overview and detailed screens, are where users will spend the vast majority of their time, and thus screenshots of them have been provided in figures 11 and 12 . the overview screen provides users with a high level view of the spectra in the loaded fits file, detailing what they have been matched to and the quality assigned to the matches. filtering for this screen allows users to sort results or to filter by categories, for example only displaying matches of quality (qop) 4 or all matches to quasar templates. matching results can be downloaded as comma-separated variable (csv) files, and those same files can be used to load the matching results back into marz on different machines by simply dropping the results file into the program the same way as a user would load in a fits file. this was added to allow easy verification of redshift results by different users on different machines. a progress bar at the top of the screen keeps track of current file completion and file quality."
implementing scalar multiplication in software and hardware is much more feasible than performing multiplications or exponentiations in them. as ecdh makes use of scalar multiplications so it is much more computationally efficient than rsa and diffie-hellman (dh) public schemes. so we can say without any doubt that ecc is the stronger and the faster (efficient) amongst the present techniques.
"the templates screen is mostly non-interactive, and simply displays all the templates used by the system with the option to enable or disable specific templates at will. the settings screen gives options to explicitly set how many processing threads to create, whether results should be saved in the background, and offers the ability to clear all saved results in the system, or to simply clear results for the currently loaded fits. the usage page gives instructions on how to use the program, an explanation of the purpose of each screen, how to raise issues or feature requests via github, and provides several example fits files for users who simply want to test out the system without having to source a fits file themselves. it also provides a list of keyboard shortcuts for those users whom are not familiar with runz."
"in this section, we detail the fits file format that can be consumed by marz. fits files are loaded into the application via the fitsjs library [cit], and the data extraction algorithm then searches for extension names. spectrum intensity is expected to found in either the primary header data unit (hdu) or one named intensity. spectrum variance is searched for using the extension name variance, and similarly the sky and fibres should contain the sky spectrum and details on the fibres respectively. although the primary use case of the software is with the the left hand subplot shows the smoothing convolution filter used in marz on the quasar spectra. on the right hand side, the black dashed line represents an underlying signal, modelled as an exponential decay. independent poisson noise is added to this signal to give the algorithm input (shown in black), and the output of the convolution of this signal with the smoothing filter is shown in blue. we can see that, despite the occurrence of subpeaks, the original signal peak is recovered, unlike the peak discovered when using boxcar smoothing (a uniform convolution), which is shown in red. this is due to the increased contribution at lower pixel separation, and increases the chance that a smoothed peak will be located at the same pixel in the noisy data."
"redshift determination is a key component in many cosmological surveys. whether the goal is to analyse supernovae, large scale structure, peculiar velocities, lensing, or a host of other interesting astronomical phenomenon, it is critical that the redshifts of target objects are determined to the highest resolution and free of unknown systematic effects. of interest in this paper is the use of spectroscopic data to determine redshift, and prior spectroscopic surveys, such as the two degree field [2df, 20], six degree field [6df, 17], wigglez [cit], sloan digital sky survey [sdss, 24], galaxy and mass assembly [gama, 16] and deep extragalactic evolutionary probe 2 [deep2, 22], have used a variety of different software solutions and pipelines to attain redshift measurements."
"we chose to implement the matching software as a web application, since this allows access to the software from any laptop or desktop with an internet connection with no installation. the interface utilises google's angularjs framework [cit] for its application scaffold, as angularjs allows dynamic two-way binding between interface and application variables for easy user interface creation, easy server communication, and has a vast amount of existing resources publicly available."
"the quasar processing steps illustrated. the top panel depicts in input spectrum intensity, with variance shown in red. the polynomial subtraction removes the almost constant continuum and is shown in the second panel from the top. the third panel shows the output after several steps, including an initial tapering of the spectrum, rolling point mean of the spectrum. the variance undergoes median filtering, boxcar smoothing and addition of minimal variance as described in text. the final spectrum after dividing the intensity by the variance is shown in the bottom panel. we can see that the spectrum is cleaner, is properly tapered, and emission features are accentuated."
"as marz simply requires generic fits files to work, it can be used for simple fits file inspection and visualisation, or as a redshifting tool for other surveys. the overview screen, showing data from a fits file courtesy of chris blake and the 2dflens survey. users can switch between a sortable tabular view and a graphic tile view, filter on object types, redshift ranges, templates and qop values. the top of the screen shows the navigation menu, file completion progress bar and input for user initials. visible at the bottom of the screen is the application footer, which shows the program's progress through automatic matching (automatically matched templates are shown in red in the graphical tiles). the bar changes colour depending on progress -green for preprocessing, red for matching and blue for completed. during the first two stages, a pause button is available to the user. if any results exist, a download button is available, which saves the current results to the file system. figure 12 : the detailed matching screen, showing spectrum 7 seen in the overview screen in figure 11 . the menus at the top of the page allow the user to toggle data on or off (variance, templates and whether to use the raw data or processed data). the menu bar also allows the user to reset to automatic or manual results, smooth the data, select which template to compare against, toggle between the best five automatic results, change the visual offset of the template and manually set the displayed redshift. the user can mark spectral lines by selecting a feature in the plot (either in the main plot or in any callout window) and then select the desired transition (either via keyboard shortcut or by selecting an option in the bottom row of the menu). users can also change redshift by clicking on peaks in the cross correlation graph found between the spectra plot and the menu bars. quality values for redshifts can also be assigned via keyboard shortcuts or via the vertical button menu on the left, and assigning a quality saves the result in the background and moves to the next spectrum. in the case where the \"qop 0 only\" option is selected in the left hand bar, the user is taken to the next spectrum without a quality flag set, or else it simply takes them to the next spectrum ordered by id."
"bad pixels have their intensity replaced with the mean to four pixels either side of the flagged pixel. pixels flagged as a cosmic ray have a 9 pixel window (centered on the flagged pixel) replaced by a constant value, which is given by the mean intensity of pixels in a 19 pixel window (centered on the cosmic ray), discounting pixels in the cosmic ray."
"1. binary code: this code will be used for member position discovery., g x2 respectively, and their associated parent binary code is 000. since there is no sibling member for u 3, the list just shows its public key, g x3, and the associated parent binary code, 00. moreover, each intermediated node code is calculated by the formula below."
this paper will detail how the marz software satisfies the above requirements. in section 2 we briefly review prior software to provide motivation for a new redshifting application then in sections 3 and 4 we justify our choice of software platform and input fits file format respectively. sections 5 and 6 detail the redshift matching algorithm and templates used. the performance of our algorithm is assessed in section 7. in section 8 we explain how to utilise the software using both the interactive user interface and command line interface. conclusions are presented in section 9.
"finally, the gama survey utilises the autoz code for redshifting low-redshift galaxy and stellar spectra [cit] by cross correlating input spectra with a range of templates. the autoz idl code does not feature a user interface, however the algorithm utilised outperformed the runz algorithm for all spectra types barring quasar spectra. unfortunately, idl introduces an unwanted impediment to utilising the software, as it must be licensed. due to the dependence on idl and lack of an interface, the autoz software did not meet the ozdes requirements."
"data processing in browsers has only recently become possible due to html5's web worker api, 3 which allows for multi-threaded processing by sending tasks to independent workers. communication to web workers and any future potential server communication will use json (javascript object notation) format [cit], and conformation to the rest (representational state transfer) interface [cit] will allow for easy end-point construction by combining the instant serialization and deserialization of any javascript object via json with the simple but structured api system provided by rest. this also has the benefit of making all messages human-readable and and able to be linked to web services easily due to existing support in all modern server frameworks for rest api's. the applications primary challenge to overcome with data processing was a lack of scientific javascript libraries. this forced many basic mathematical and scientific functions to be reimplemented from scratch or translated from python or idl libraries, creating unwanted development overhead. reimplementation and translation errors were checked via the creation of multiple test suites."
"ii. u7 updates the member discovery list by deleting the leaving node's public key, and changes his/her parent binary code. u7 also informs the other nodes about the updated information."
"for both high and low signal-to-noise data, marz performs consistently well at automatically redshifting spectra. marz also provides an enhanced and intuitive user experience, allowing the user to visualise spectra, and manually redshift spectra by cycling automatic matches, manually entering desired redshift, or marking emission or absorption features. the web-based nature of the application means that installation and updating are now no longer of any concern at all, and unlike many web-based applications, marz also saves users' work in the background, so that work is not lost with unintentional browser closure."
"to supplement angularjs and allow rapid prototyping, existing interface element libraries were imported to reduce the amount of reimplementation of common elements and boilerplate code required to produce a functioning application. to this end, angular ui's bootstrap [cit] reimplementation of bootstrap [cit] components was added to the project code base, to allow both the functionality of angularjs and rapid prototyping of using bootstraps premade components. as an example, this allowed marz's usage page, with its accordion layout, to be created in minutes, instead of the hours that would be required if no pre-existing components were available."
"as we wish to preserve broad features found in quasar spectra, we require the quasar error spectrum to be sufficiently smooth that broad features are not destroyed when we apply the variance onto the spectrum intensity. a median filter is applied to the variance, and then the result is smoothed with boxcar smoothing. the strength of these filters was not found to impact results, so long as fine pixel detail is removed from the variance plot. in order to preserve even more broad shape by creating a more uniform variance that does not have the possibility of creating false features by having small variance, the variance of the spectrum is increased by addition of five times the minimum spectrum variance, where again the results are not highly sensitive to the amount of variance added. the quasar intensity is then divided by the adjusted variance to produce a spectrum that retains broad features and shapes, but down-weights sections of higher variance which are commonly observed at wavelengths close the end of spectroscopic ccd range. the loss of resolution entailed by the variance modifications detailed above results in any sharp emission lines present in the spectrum to be more significant when compared to emission lines in the general spectrum, however the relative lack of sharp emission line features in the quasar template makes this issue not significant. these steps are illustrated in figure 3 ."
"the general spectrum then has error adjustment applied, where each pixel has its variance set to the maximal value of itself and the variance of its two neighbouring pixels. following [cit], this is to allow for uncertainty in the sky subtraction and any underestimation of errors next to sky lines. the variance spectrum is then widened again, where each pixel is set to a maximum of its original value or 70% of a local median filter, which serves to remove from the variance any points of sufficiently low variance that division of the intensity by the variance would create a fake emission feature. the intensity of the general spectrum is divided by the variance spectrum to down-weight pixels with higher uncertainty."
"two main error safeguards have been implemented in the program to stop unnecessary loss of work. the first is a confirmation request when attempting to close down the application, which solves the issue of closing the whole browser with an open tab of marz. the second and more robust solution is to use the local storage capacity available in modern browsers to save results in the background after every automatic or manual redshift is assigned. this allows users to close the program, and resume where they left off simply by dragging the original fits file back into the application."
"up signal-to-noise, and therefore the spectra have to be put a consistent wavelength solution prior to stacking. this means the heliocentric correction (which differs from night to night) must be done before the spectra are stacked, and thus before marz receives them. however, heliocentric corrections can be enabled in marz by setting the header property do_helio to true. upon finding this flag, marz will correct for the heliocentric velocity, which requires the header to contain the modified julian date of exposure (utmjd), epoch of exposure (epoch), and the observatory's longitude, latitude and altitude (long_obs, lat_obs, and alt_obs respectively). the longitude and latitude are taken to be in degrees, and the altitude in meters above sea level. furthermore, heliocentric velocity correction requires each spectrum have an associated right ascension and declination (ra and dec), which would live in the fibres extension as explained above. a correction into the rest frame of the cmb can also be performed, by setting the header do_cmb to true. marz will then compute the cmb velocity using the ra and dec values. [cit] and fk5 respectively. given a computed heliocentric velocity v hel and peculiar velocity of the 3k background rest frame v cmb, the correct redshift z is related to the observed redshift z obs by"
"the features of this protocol are that, at join no keys are needed to be exchanged between existing members, at leave only one key, the group key, is delivered to remaining members."
"this initial round of continuum subtraction is not intended to be high enough quality for the automatic matching process, it is done to give the user the option of manually redshifting spectra without continuum, allowing them to focus on the emission and absorption features of the spectrum without the broad shape of the continuum to distract. in order to limit the effect that singular emission features can have on spectrum matching, all features are clipped at a distance of 30 standard deviations from the mean."
"the preprocessing stage is designed to remove any bad pixels and cosmic rays from the data before being returned back to the interface, so that the user can manually redshift using the cleaned data."
the public key of each member along with binary code the corresponding parent node is stored in a list shared by group members. this list will be updated on each membership change and from time to time.
"in hierarchical approaches, the members of group are mapped with the leaves of a logical binary key tree. each member maintains all the keys along the path from his/her leaf to the root, hereinafter called the path set. the root key is the group key. at join/leave, all the keys in the path set need to be changed to new ones."
"in ecdhsa, the group key at join is sent to new member being encrypted by the shared key with his/her sibling member. however, the current members can calculate it by applying one-way hash function to previous one. when f is a given one way hash function, and k g is the previous group key, the new group key k' g is calculated as follows. when a new member wants to join a group, he/she sends a hello message to discover the group members. members, who receive the signal of this member, look up the list to know which member does not have a sibling member. a member who does not have a sibling member in his/her branch replies to this signal. but when each member has his/her corresponding sibling member in his/her branch, the member with lowest parent binary id replies to that member. he/she exchanges the public key generated by elliptic curve diffiehellman key agreement. here, a member who replies is responsible to authenticate new member. we assume that each group member is equipped with some authentication capability [cit] ."
where both the size of the window and strength of the exponential decay are not sensitive to small changes. pixel values were selected to maximise matching efficiency over test spectral data. this method of smoothing was selected and tuned so that the location of broad peaks remained unchanged whilst still providing sufficient smoothing of the spectrum to increase similarity to the template. this convolution is illustrated in figure 2 .
"the development of such a large and technical application, without significant supporting scientific libraries, has been a massive challenge. whilst, from a usability and interface perspective, web-applications may excel, recent development of the ability to perform large computations on background processes via web workers means that the mathematical library support for javascript is still in its infancy. whilst this can be overcome with effort, it provides a significant barrier for application development. this barrier has been overcome with marz, and as such marz presents a strong redshifting application, and a large step forward in the demonstration of web frameworks as a platform for non-intensive computational analysis."
"the preprocessing algorithm detailed above takes an input intensity and variance spectrum, where the variance spectrum is typically computed in the data pipeline using poisson and readout noise. the output of the preprocessing algorithm is an adjusted intensity and variance spectrum, and this becomes the input of the matching algorithm. in the matching algorithm, marz first duplicates the intensity spectrum so that two copies exist internally. this is necessary because the matching of the broad-featured quasar template differs to matching of the other templates, and the copy of the intensity spectrum used for quasar matching shall now be referred to as the quasar spectrum, and the other spectrum -used to match all other templates, shall be referred to as the general spectrum."
"u3 downgrades his/her position from 00 to 001, updates the member discovery key by replacing the new parent binary code and new member's public key ( vi. u3 generates new group key as below. as you notice just one key is delivered to new member. this is an important feature for distributed group communication in wireless network. since members are mobile, in addition to dynamic join/leave, simultaneous join may occur in such networks. in order to solve such problem, the overload of join operation must be minimized. the features of ecdhsa provide this task with just one key delivery. the flow of data for join operation of node can be seen from the figure 3."
"the main alternative considered was the popular rvsao 2.0 software package, which utilises cross correlation techniques, in addition to providing feature matching capacity [cit] . unfortunately, the software operates only in command line and without any interactive user input. [cit] release of the xcsao program (the cross correlation matching algorithm in rsvao 2.0) takes up to 67 input parameters [cit], which, whilst allowing great specificity in redshifting, introduces technical overhead to cosmology groups attempting to use the software. due to this, and the lack of modern interface, this software did not satisfy ozdes's requirements."
"the general spectrum undergoes a cosine taper, and then undergoes a second step of continuum subtraction, where a boxcar smoothed median filter (121 pixel window of boxcar smoothing and 51 pixel window median filter) is subtracted from the spectrum, with pixel values following autoz and correspond to a wavelength distance of 125 and 53 angstroms respectively for typical aaomega spectra. the cosine tapering of the spectrum ensures that features are not introduced at points of discontinuity at the ends of the spectrum in the case the input spectrum is zero-padded. this second step of continuum subtraction is not applied to the quasar spectrum, as the fineness of the smoothed median subtraction would result in broad features being completely removed from the spectrum. figure 1 illustrates each step in this process, in addition to the original polynomial continuum subtraction."
"departing from the autoz algorithm, the quasar spectrum undergoes smoothing via a rolling-point exponential normalised intensity figure 5 : a high quality emission line galaxy spectrum matched against the high redshift star forming galaxy template. the two strongest cross correlation peaks and the corresponding redshifted template have been displayed beneath the original spectrum for illustrative purposes."
"as of the eighth data release (dr8) of the sloan digital sky survey (sdss), the redshifting software used in their survey utilises a minimum χ 2 algorithm [cit] . prior to dr8, a cross correlation algorithm was used [cit], where the same results were found when matching using the different methods for 98% of spectra [cit], as expected, given that maximising the cross correlation strength gives a minimum χ 2 value [cit] . this potential software system also does not satisfy the ozdes requirement of easy manual redshifting and confirmation."
"in this section, multiple numeric values have been presented, in terms of pixel width, polynomial degree and other limits. these values, chosen by testing them against aaomega data, may not be optimal for other surveys which use other spectrographs and wavelength resolutions. to this point, all these values are contained in the config.js file, and thus are easily modifiable upon forking the marz project."
"the local application state is preserved via utilisation of local storage, where local state is preserved in json format. this gives the benefit that results are not lost when exiting the application or refreshing the browser, negating one of the major disadvantages of stateful web applications. changes to application are persisted via setting 3 https://html.spec.whatwg.org/multipage/workers.html cookie properties instead of using local storage, to provide a concrete demarcation between user preferences and user redshifting data."
"these templates were sorted, compared, and a selection of twelve representative templates were extracted, consisting of 5 stellar templates, 1 agn template, and 6 galactic templates. inclusion of a greater number of templates was found to have a minimal impact on matching performance -see figure 7 for a comparison, and future improvements to matching performance will be looked for in the areas of eigentemplates or archetypal matching, rather than simply increasing the number of available templates."
"the simulations are performed using network simulator (ns-2), [cit] particularly popular in ad hoc networks. the mac layer protocol ieee 802.11 is used in all simulations. the destination sequence distance vector (dsdv) routing protocol is chosen for the simulations. the simulation parameters used are summarized in table 4 : no. of nodes 100"
u7 checks his/her list and use elliptic curve diffie-hellman key agreement to share a key with one of the member in each branch. then it will unicast new group key to each of them. in this a node is being deleted and its sibling node becomes parent node and now this node is responsible secure generation and delivery of group key using ecdh. table 3 shows the updating of member discovery list after a member leave a group. this is necessary to insure the backward secrecy. i t can be seen from above table after deletion of node u8 its corresponding public key g x8 was deleted.
"these pixel values were the minimum window found to produce sufficient quality means and remove the majority of cosmic rays. the continuum is initially subtracted via the method of rejected polynomial subtraction, where a 6-degree polynomial is iteratively fitted to the spectrum and, as with autoz, all points greater than 3.5 standard deviations from the mean are removed from the fitting process. as soon as an iteration discards no extra pixels, or after fifteen iterations (to ensure the final array of values is not excessively sparse), the loop is terminated and the final polynomial should closely follow the continuum, and is thus subtracted out."
"a mobile ad-hoc network (manet) consists of a number of mobile wireless nodes, among which the communication is carried out without having any centralized control. manet is a self organized, self configurable network having no infrastructure, and in which the mobile nodes move arbitrarily."
"the detailed screen allows for better verification of automatic matches and also offers the possibility of manually redshifting spectra. verification of the on screen displayed redshift is done simply by assigning a qop value, and the top five automatic matches can be cycled if the best match is visibly incorrect. keyboard shortcuts are available for almost all actions, where key mappings are based off the shortcuts available in runz in order to make transitioning from runz to the marz as easy as possible. users can click on features in the detailed plot and then mark them as spectral lines. matches can be updated by by automatically fitting to the best cross correlation peak value within a small deviation window. the user can also toggle whether to display the raw data or the preprocessed data, whether to render a template under the data plot, and whether to display continuum or not. boxcar smoothing is available to help spectrum legibility."
"ecdh requires much lesser numbers (and thus less number of bits) for its operation thanks to ecdlp. the security level of a 160-bit ecc, 1024-bit rsa, and (160/1024)-bit dsa are similar. table 6 [cit] gives detail about this:"
"the top subfigure shows an example input spectrum from which the continuum will be removed. the sixth degree fitted polynomial is shown dashed in this subplot, and the spectrum after subtraction of this polynomial fit is shown in the middle subplot, where we can see that broad continuum features are removed. the middle subplot also shows the output of the smoothed median filtering (dashed), and spectrum after subtraction of this filter is shown in the bottom subplot, where we can see even fine continuum detail has been removed from the spectrum. we have deliberately chosen to show a spectrum that was processed with an earlier version of the 2df data reduction pipeline (2dfdr), since spectra that were produced by this version of 2dfdr often have inaccurate continua with complex shapes. later versions of 2dfdr more faithfully produce the continua of galaxies."
"both the general and the quasar spectra undergo cosine tapering and root-mean-square normalisation, the former to remove ringing in a fourier transform [apodization; 19], with pixel width following autoz but insensitive to change, and the latter to ensure comparable cross correlation values between different templates. the spectra are oversampled and then fourier transformed. the quasar spectrum's transformation is then cross correlated with the quasar template, and all other templates are cross correlated with the general spectrum. cross correlation results are then inverse transformed, with the inverse transformed array representing cross correlation strength over a redshift range. peaks within allowed redshift ranges for each template are selected, and if prior information on the object type is accessible in the fits file, the peaks for each template are then weighted. peaks from all templates are then sorted together, and the ten highest correlation value peaks have a quadratic fit applied around the peak for sub-pixel determination of local maxima. the sub-pixel location of the peaks are converted into a redshift value, and these are returned to the user, with the highest peak representing the best automatic matching found. a potential quality is returned to the user, which is a function of the cross correlation strength of the two greatest peaks, v 1 and v 2 respectively. these peak values are used to construct a figure of"
"extra target types can be added in the future, however there are three challenges for marz when attempting to handle a large number of templates. firstly, users desired to be able to fully replicate the matching capacity of the automatic system when manually redshifting, and a large number of templates complicates the user interface and slows down the process of the user assigning an object type to the spectrum. it would be possible to only display to the user a restricted set of templates, however user feedback indicated this was an undesired solution. due to the interpreted nature of javascript and its lack of vector processing capability, computational performance is roughly an order of magnitude worse than on typical compiled code. as the computation time for each spectrum was roughly proportional to the number of templates to match, the number of templates was also kept relatively small to ensure that the automatic matching performance was still acceptable on low-end machines. the final concern when adding more templates is the download size of the web application, such that the size of the template dependency remains small enough to be easily redownloaded on page refresh. a potential solution to this is to enable javascript caching of the template file, such that it only needs to be downloaded once."
"at each iteration t, the action selected by the base station bs l is the power to allocate to the currently transmitting secondary user su l . the set of all possible actions is therefore given by equation (27)."
"in this article, we use two multi-agent q-learning algorithms. the first one is used to allocate the secondary user sensing times and the second one is used to allocate the secondary user transmission powers. in the sensing time allocation algorithm, each secondary user is an agent that aims to learn an optimal sensing time allocation policy for itself. in the power allocation algorithm, each secondary base station is an agent that aims to learn an optimal power allocation policy for its cell."
"here, we set the constraint that h is larger than the maximal dwell time. in this case, the normal stopping (dwell process) of a train at a station will not affect its following trains unless mis-operations, network failures, or other terrorist activities occur. these special situations are not considered in this study."
"this section presents the performance of the proposed optimization algorithm by investigating the effects of number of sensors, path-loss exponent, and transmission power on the system, through simulation experiments. in addition, the system performance is verified in a comparative manner in the context of the number of ips available in the system, and jain's fairness ratio. also, to further substantiate the contributions of this work, two networks at unequal distances to the bs are investigated, to showcase the improvement in network performance. based on simulation experiments, the computational efficiency of the proposed optimization algorithm is shown, while the simulation settings presented in table 2 are employed to configure the proposed wpsn system. this work assumes similar network parameters as in a recent reference work [cit], for comparison purposes."
"from the second derivative test, r a, b (j, ζ, ξ) is a concave function and it can be solved through any known standard convex method."
"where h su l bs l is the link gain between su l and bs l . in this article, we consider free space path loss. therefore, the link gains are computed as follows:"
"in this model, the flow conservation constraints of both trains and stations are considered. according to the illustration of the train operations and passenger flow, as shown in figure 2, the flow conservation constraint of passengers per train is represented as follows."
"the complexity of the decentralized power allocation q-learning algorithm can be compared to a reference gradient descent centralized power allocation algorithm, similarly to the analysis performed in section 1. the conclusion is the same as for the sensing time allocation algorithm: the centralized allocation algorithm has a lower computational complexity than the decentralized q-learning algorithm whose main advantage is therefore that the base stations do not need to exchange control information."
"the cost c and the new state s' generated by the choice of action a in state s are used to update the qvalue q(s, a) based on how good the action a was and how good the new optimal action will be in state s'. the update is handled by the following rule:"
"the water quality sensors are made up of four essential modules, namely sensor, micro-controller, power supply, and communication. the sensor module is used for measuring the desired parameter of water quality in the form of analog information, and converting the measured information into a digital form through an analog-to-digital converter (adc)."
"we have applied our method to the demand data of the line for many weekdays, and have observed that the passenger waiting time and the number of the larger headways correspond to the lower demand, while the high demand pattern is associated with the short headways. in figure 12, we show the relationship between the fitness and number of iterations for our method. we observe that our method converges with a decreasing speed as the number of iterations increases. the iteration error we use is 0.01. for clarity, we run and plot 400 iterations here. in table 5, the statistical results of different timetable methods are presented."
"although a base station could determine the sensing window lengths that minimize function (11) and send these optimal values to each secondary user, in this article we rely on the secondary users themselves to determine their individual best sensing window length. this decentralized allocation avoids the introduction of signaling overhead in the system."
"in this article, we compare the performances of the power allocation system for two different cost functions c l, t . we first define a competitive cost function in which the cost decreases if the secondary sinr at the base station increases, provided that the aggregated interference generated on the primary protection contour does not exceeds the acceptable level:"
"for q-learning implementation the states have to be quantized. therefore it is assumed that x l, t takes one out of the following ξ values:"
"in the dl phase, the energy a sensor node b harvests from an ips c, in a given time-slot, is formulated in (14) as follows:"
"in order to protect the primary receivers from receiving harmful interference from the secondary users, a protection contour is defined around the primary emitter as a circle on which the received primary sinr must be superior to a given threshold sinr p th . the secondary cells are located around the protection contour. as the primary cell ray is assumed to be much larger than the secondary cells ray, the protection contour can be approximated by a line parallel to the secondary base stations line."
"note that for both the sensing time and power allocation problems, the agents have an imperfect knowledge of the state of the environment. the state represented by an agent at each iteration of the q-learning algorithm is actually an imperfect estimation of the environment state. in this case, the convergence demonstration of single agent q-learning [cit] does not hold. however, multi-agent q-learning algorithms have been successfully applied in multiple scenarios [cit] and in particular to cognitive radios [cit] . numerical results will show that both q-learning algorithms presented in this article converge as well."
the optimal value of r will be determined in section 5. let s denotes the ratio between the duration of a time slot and the sampling period:
"where sinr s t,l denotes the secondary sinr measured at iteration t at bs l in the distributed learning scenario and sinr s t,l denotes the secondary sinr measured at iteration t at bs l in the optimal centralized scenario."
"in this work, a new wpsn is proposed. for efficient resource allocation, and to also realize a more practical system, network heterogeneity is considered. heterogeneous wsns is a class of networks where sensor nodes have different properties in terms of distance specification and resources allocation. more often, wsns are treated as homogeneous, whereas in real scenarios, the networks may have different properties. consequently, realistic wsns may not be achieved in homogeneous sensor networks. heterogeneity is a key design consideration for the realization of efficient and workable systems that are capable of solving several needs. therefore, the concept of heterogeneous networks is employed in this work to classify sensor nodes based on their distance specifications and deployment strategy. as a consequence, a multi-class network is formed, containing class a and class b networks. class a network sensor nodes are distributed in a predetermined manner to meet some specific design goals, while the sensors in class b are deployed in a random pattern."
"step 1: two points are selected at random as the cross_start and cross_end respectively. the segment between the cross_start and cross_end is selected as a candidate for cross operation, as shown in figure 7 . a) the overall structure of a chromosome (s1 -station 1, s2 -station 2) b) the departure times of trains at the start station congested stations can be decreased. since the demands of different weekdays have a similarly periodic property, here we take a typical weekday data as an example to show the function of our method in detail. figure 10 shows the demand profile of the line on a typical weekday, and table 1 shows the parameters used in the case study. in reality, the departure time, the dwell time, and the running time of a train may be influenced by some situations such as irregular and short halt of a train or over-crowding at stations. this will result in a stochastic timetable to some extent. in this paper, for simplicity and convenience of calculations, the dwell time of trains at each station and the running time of trains between two adjacent stations are supposed to be invariable after being optimized. the designed platform capacity of the largest station \"xinjiekou\" is 700 persons for and other stations it is 500 persons according to the design manual of the line. at the start station, the operation period of the metro line is from 6:30 a.m. to 9:30 p.m."
"the rest of this article is organized as follows: in section 2, we formulate the problems of sensing time allocation in the secondary network. in section 3, we formulate the problem of power allocation in the secondary network. in section 4, we present the decentralized q-learning algorithms used to solve the sensing time allocation problem and the power allocation problem. in section 5, we present numerical results allowing the discussion of the performance of the q-learning algorithms for different exploration strategies, cost functions and execution frequencies."
a congestion event occurs and is counted if the number of waiting passengers exceeds the platform capacity of a station when a train leaves the station. c min -minimum number of congestion events on platforms. given inputs: tc -the capacity of a train; pc -the capacity of a platform; k -the total number of trains departing from the start station during the study period; h -minimum headway; a -minimum average passenger load rate; j -the set of trains that need to satisfy the minimum average passenger loading rate requirements.
"where p p is the power that is received on the protection contour from the primary transmitter, s 2 is the noise power and h su k i l is the link gain between su k and the point i l on the protection contour."
"moreover, in order to provide a solution to unfairness in energy harvesting as a result of the transformation, we formulated a new problem as (p3) to guarantee the optimality of j and t, which is indicated as j* and t*. consequently, these values (j* and t*) are employed in (p1). the formulation of the minimization problem for addressing the unfairness in energy harvesting among the sensors is expressed in (22) as follows: (p3):"
this article aims to illustrate the potential of q-learning for cognitive radio systems. for this purpose two decentralized q-learning algorithm are presented to solve the allocation problems that appear during the sensing phase on the one hand and during the communication phase on the other hand. the first algorithm allows to share the sensing times among the cognitive radios in a way that maximize the throughputs of the radios. the second algorithm allows to allocate the secondary user powers in a way that maximize the signal on interference-plus-noise ratio (sinr) at the secondary receivers while meeting the primary protection constraint. the agents self-adapt by directly interacting with the environment in real time and by properly utilizing their past experience. they aim to distributively learn an optimal strategy to maximize their throughputs or their sinrs.
"in figure 2, the dwell time of train j is defined as the time interval between ta j u and td j u . a set of nodes associated with the latest arriving moment node (corresponding to p j u ) separates the time coordinate into several mutually exclusive intervals for each effective passenger-loading period. step 2: use the genetic algorithm to solve the multi-objective function denoted by equation 11 . if the genetic algorithm fails to find a feasible solution that satisfies c n, go to step 3."
"indicates how many times a learning loop is executed during one tdma time slot (i.e., for a fixed secondary transmitter su l in cell l). it is assumed that every secondary cell uses the same tdma time slot length t tdma as well as the same learning iteration length t l . however, the secondary transmissions as well as the learning iterations are assumed asynchronous, as illustrated on figure 3 ."
"each randomly generated gene corresponds to a possible time point that represents a decision. we propose a special chromosome to represent a decision of our method. the chromosome includes three parts: departure time of each train, the dwell time of the trains at each station, and the running time of the hours. we denote the set of trains that need to satisfy the minimum passenger loading rate requirements by j. by limiting the loaded passengers within a reasonable range, the passenger loading requirement constraint is presented as follows:"
"this last cost function penalizes the actions that lead to a realized average throughput that is higher than required, which should help the disadvantaged nodes (i. e., the nodes that have a low data rate c h 0,j ) to achieve the required average throughput."
"the licensed band is assumed to be divided into n sub-bands, and each secondary user is assumed to communicate in one of the n sub-bands when the primary user is absent. when it is present, the primary network is assumed to use all n sub-bands for its communications. therefore, the secondary user can jointly sense the primary network presence on these sub-bands and report their observations via a narrowband control channel."
"where n h 0,t−1 denotes the number of time slots that have been identified as free by the base station during the (t -1)th learning period."
"this section investigates the performance of the system, using a three-sensor example, by varying the fraction of average power of transmission of the sensor nodes. consequently, the number of both the network sensor nodes and the ipses are fixed, as the average transmission power is varied. the system model is developed such that energy consumption for other networking processing has been optimized; therefore, it is possible to increase the amount of energy resources for information transmission. this is the basis for varying the fraction of average power of transmission. from fig. 11, it is noticeable that the system throughput rate increases as the information transmission power of the sensor nodes in the network increases, as could be expected. the reason for this is that the sensor nodes can now spend higher energy on information transmission during the ul period, which in turn improved the system overall throughput. in addition, the results obtained in fig. 11 are compared to the existing system with the same configuration and it can be inferred that there is a significant improvement in the throughput rate of the proposed system. this is an indication that the proposed system is more energy-efficient in terms of energy consumption."
"as a result of the surge in water contaminants, water consumed from either the water polluted by natural processes or man-made activities is dangerous to humans and the ecosystem, because of their high levels of heavy metals and microbes. the microbes and heavy metals cause havoc to human health. examples of some of the disruptions they cause are diarrhea, epigastric pain, organ damage such as renal and hepatic failure, and cancer [cit] . for example, around 1,500,000 children die due to diarrhea every year [cit] . heavy metals are highly toxic and also create a lot of environmental concerns [2, [cit] . to address these issues, there is an urgent need for effective systems for frequently monitoring water quality parameters. to achieve this, the adoption of wireless sensor networks (wsns) technology has been proposed as a promising solution. unfortunately, wsns are faced with several challenges that range from energy, memory, and processing capability. as a result of these limitations, both academia and industry are currently making efforts toward seeking solutions to the aforementioned problems. among the issues raised in wsns, the energy scarcity problem is the greatest of all, as the operation of other modules depends on energy [cit] . the problem of energy scarcity in wsns has been in existence for a long time on the account of the limited energy budget of the batteries that are typically used for powering the sensor nodes in wsn systems [cit] . to meet the objective of wsn-based water quality monitoring systems in the context of timely monitoring without interruption in energy supply, harvesting energy from energy sources, which is technically referred to as energy harvesting (eh) technique in practice is a promising approach [cit] . the technique has been exploited by the energy harvesting research community in wsns to replace the utilization of battery power, which is associated with several problems that include short life span, cost of battery replacement, and difficulty in battery replacement. energy harvesting from sources that include solar [cit], radio frequency (rf) signals [cit], and wind [cit] have been exploited. however, the most interesting energy source among the above-mentioned sources is rf eh from intentionally stationed power sources [cit] . the main reason for this is that eh from intended rf power sources (ips) is controllable, as well as suitable for continuous monitoring of water distribution networks [cit] . similarly, eh from ips is suitable for energy transmissions over larger areas because of its far-field characteristics of energy radiation [cit] . as a consequence of the benefits of eh from ips, it is an attractive energy solution for water monitoring applications in enclosed environments, although the energy solution can be employed in any location."
"in this section, an efficient resource allocation algorithm is presented and is defined as algorithm 1. the essence of the proposed algorithm is to ensure fairness in eh-dl timing schedules among the system sensor nodes. in addition, it is aimed to achieve an enhanced rate of information transfer among the network sensor nodes in the ul. to achieve this, the proposed algorithm optimizes the energy and information transfer timing schedules in a joint fashion, according to the mathematical models presented in section 3 such that optimal time periods are allocated for both eh and information transmission to classes a and b in the network. as a consequence, algorithm 1 optimally allots an ips c to individual sensor nodes at a calculated optimal time period. in a similar vein, to make sure that the sensors in the network are provided with sufficient time for communications in the ul phase, an optimal information transmission time period is calculated and allotted. the implementation of the algorithm is done on the system controller to achieve the optimal control of the switching of the ips and optimally allocating them to the sensors for enhancing the attainable throughput of the wpsn system."
"step 3: in reality, headways are not constant during the operation, and the average headway cannot be used to prove safety requirements [cit] . to avoid train collision, we here consider that the descendants obtained after the cross operation should satisfy the condition that the minimum headway between two adjacent dispatches is not less than h, as shown in figure 8 . an unsafe descendant will be discarded and re-generated until it satisfies the safe condition."
"with this cost function, every node tries to achieve the maximumr j,t with no consideration for the other nodes in the secondary network. we secondly define a cooperative cost function in which the cost decreases if the difference between the realized average throughput and the required average throughput decreases:"
"in this article, the impact of the frequency of the learning algorithm is also analyzed. if t tdma denotes the length of a tdma time slot and t l denotes the length of a learning iteration, then"
"in this experiment to investigate the convergence of the system, both the number of sensors and ipses in the network are fixed, while the number of iterations is changed during the simulation. figure 4 depicts the convergence of the proposed oaera algorithm in the context of attainable system sum-throughput per sensor nodes against iteration number. for this experiment, a system configured with two sensors and a system configured with three sensors were investigated. the two systems are powered by five ips, which is consistent with the reference work [cit] . from fig. 1, it can be noticed that it takes the proposed oaera algorithm an average run time of 500 iterations to realize an optimal solution, as the iteration number is observed to enhance the system attainable sum-throughput of the two systems. as a result, it is reasonable to utilize 500 iterations for averaging the performance of the system. thus, results are obtained based on an average of 500 iterations."
"in this article, we have proposed two decentralized qlearning algorithms. the first one was used to solve the problem of the allocation of the sensing durations in a cooperative cognitive network in a way that maximize the throughputs of the cognitive radios. the second one was used to solve the problem of power allocation in a secondary network made up of several independent cells, given strict limit for the allowed aggregated interference on the primary network. compared to a centralized allocation system, a decentralized allocation system is more robust, scalable, maintainable and computationally efficient."
"as shown in figure 6a, the chromosome includes three parts, and the crossover and mutation process for different parts are executed in their own domains. for the departure time, our custom designed crossover process works as follows."
"this paper presents a timetable optimization model that aims to reduce the passenger travel time and congestion events at platforms for a single metro line under time-dependent demand. the model considers the passenger travel time, the capacity of trains, and the capacity of platforms systematically. a multi-objective function is proposed, and a recursive method incorporated by the genetic algorithm is presented to optimize the model variables. these variables include the departure time of each train at the start station, the dwell time at each station, and the running time between two adjacent stations. the method is applied to a real example and it is compared with two existing methods, a half-regular timetable and an irregular timetable. the results show that our method enables us to obtain a train timetable with minimum passenger travel time and minimum congestion events on platforms. moreover, by increasing the number of dispatches, the phase transition from congestion state to free-flow state and the optimal timetable with minimal passenger travel time for avoiding congestion on platforms are obtained by our method. the demand-sensitive timetable obtained by our method can be used to coordinate the train resources. a stochastic timetable that derived from the optimal solution with dense dispatch of trains for the peak hours and sparse dispatch of trains for the off-peak hours could be efficient in real applications."
"this section investigates the influence of transmission power on the attainable throughput rate of a system with three-sensor, powered by a variable number of ipses. to achieve this, the transmission power of the ips is varied from 100 mw, 500 mw, 1000 mw, to 3000 mw. as depicted in fig. 9, a great surge in the performance of the system is noticed as the ips transmission power increases. based on this observation, it can be corroborated that the ips plays a crucial role in the attainable system sum-throughput rate in the context of transmission power value. similarly, with a larger number of ips, the attainable overall throughput of the system performance gets better as more resources are being efficiently allocated to the network sensors in an optimal fashion. furthermore, when the system is operated with 100 mw transmission power, which is quite low, the system performance is satisfactory. this is an interesting observation that depicts the proposed system's capability to efficiently utilize energy resources, with reliable network communication."
"at the moment, there are commercial ips solutions [cit] . this development is as a result of the advances in wireless energy transmission technology. one of the leading energy solution providers is powercaster®. forms of rf transmitters from powercaster® are the battery-powered ips [cit], and the tx91501 ips [cit] . they are reliable solution for transferring rf energy in a wireless manner and they can cover a distance of about 24 m. they are compatible with the unlicensed bands of the industrial, scientific, and medical (ism) model for communications [cit] . in addition, the powercaster® energy solutions come with a compatible rf harvester, which is employed at the sensor node for energy harvesting."
"the scarcity of available radio spectrum frequencies, densely allocated by the regulators, represents a major bottleneck in the deployment of new wireless services. cognitive radios have been proposed as a new technology to overcome this issue [cit] . for cognitive radio use, the assigned frequency bands are opened to secondary users, provided that interference induced on the primary licensees is negligible. cognitive radios are established in two steps: the radios firstly sense the available frequency bands and secondly communicate using these bands."
"the implementation of the algorithm was done on the following proposed wpsn system. two classes of network are considered at a distance of 3 m apart. in class a, a distance of 6 m is considered for the placement of one or two sensor nodes by taking the data in the reference work into consideration. in class b, the water quality sensors are distributed at a random distance of 2.5-4 m from each other-as typical in monitoring the quality of water. in addition, it is possible to vary the distance among the sensor nodes. furthermore, it is worth mentioning that during optimization, different strategic positions are considered for the ipses."
"when the total travel time of passengers z is considered, the waiting time of passengers inside the trains should be added. for temporary peak-hour demand, the congested platforms may always exist. therefore, the multi-objective function is proposed as follows. the above multi-objective function has the property of non-dominated solutions. a conventional method for solving multi-objective functions is using adaptive weight approaches which set adaptive weights on different objective functions and transform them into a single-objective function. here, since we mainly focus on c n, inspired by the idea of preference-based optimization methods, a recursive method to solve equation 11 is proposed. in our method, it is first assumed that there are no congested stations; thus the first ideal point of c n is 0. then, ga method is used to optimize z. if the ideal point cannot be realized after a number of iterations such as 400, i.e. there is no feasible solution that satisfies the ideal point, the ideal point is increased by step 1 and z is optimized again. based on this process, we can finally obtain the minimized z under the condition of minimized c n . our method runs in a recursive way as follows."
"from (15), a part of the energy obtained by each sensor b is consumed for information communication in the ul phase and is formulated in (16) as follows:"
"when a new population is obtained, we calculate the fitness described by equation 12 of each individual in order to choose the best individual as the solution of the current generation and provide the basis for the selection in the next iteration. in our study, the stopping criterion for terminating the genetic algorithm is when the algorithm reaches a maximum number of iterations."
"each secondary bs is an agent in charge of sensing the environment state, selecting an action according to policy (16), performing this action, sensing the resulting new environment state, computing the induced cost and updating the state-action q-value according to rule (17) . in this section, we specify the states, actions, and cost function used to solve the power allocation problem."
"there are two improvements in our further research. (1) quick optimization algorithms and advanced computation methods should be developed to solve large-scale timetable problems. (2) our method is based on the known passenger demand data. we will use big data methods to accurately predict the passenger demand, and combine it with our method for practical applications."
"assumption 2. in order to facilitate the problem formulation and simplification, this paper assumes that all the waiting passengers follow the first-in-first-out-(fifo) principle to board a train, and the passengers on the arriving train alight first. two situations, under-saturated conditions and over-saturated conditions, for an urban rail station are considered. for under-saturated conditions, all passengers waiting at a platform can always board the arriving train, and there is no left-behind passenger on the platform after the train leaves. under over-saturated conditions, only a part of the passengers who arrive early can board the arriving train. for the boarded passengers, we assume that all of them can alight from the train at their destinations."
"the environment of the application is assumed to be a static environment. as a result, the wireless channels between the sensor nodes and the ips c are modeled using a quasi-static fading model. in addition, each ips is assumed to have knowledge of the channel state information (csi), and as a result employs the csi knowledge to ensure the transmission of optimal energy to individual sensors in the two classes in an adaptive fashion."
"each secondary user is an agent in charge of sensing the environment state, selecting an action according to policy (16), performing this action, sensing the resulting new environment state, computing the induced cost and updating the state-action q-value according to rule (17) . in this section, we specify the states, actions and cost function used to solve the sensing time allocation problem."
"in (22), the minimum energy received by a and b is defined by e a and e b, and is calculated based on (23) and (24) ."
"it can be inferred that the free-flow state (there are no congested events at platforms) for high demand can be obtained if we increase the number of dispatches of trains. it is meaningful to determine the solution with minimal number of dispatches for obtaining the free-flow state and minimal passenger travel time. in the optimization, the fleet-size may be increased for finding the optimal solution since the number of dispatches is limited under a fixed fleetsize. table 7 shows the results for different numbers of dispatches with application of our method. the influence on the passenger travel time. to avoid this influence, the original designed operations must be changed. a feasible method is to dynamically adjust the running time of trains. the results obtained by our method can provide a potential solution for dynamically coordinating train resources."
"we assume that the instantaneous noise at each node n ji can be modeled as a zero-mean gaussian random variable with unit variance n ji ∼ n (0, 1) . let g j be the signal-to-noise ratio (snr) computed at the jth node,"
"the effect of equal network distance to the bs between class a and class b networks is investigated in this section. as a result, the distances of the network classes are fixed, while the number of ipses is varied in the course of the experiment. class a and class b were considered at an equal distance of 7.5 m to the bs. each class contains two sensors and they are powered by a variable number of three w ips sources. the same investigation was carried out for three sensors. as illustrated in fig. 10, it is noticeable that class a network only has a slight enhanced attainable sum-throughput compared to class b network for two sensors, regardless of the unequal distances among the randomly placed class b sensors, while the attainable throughput rate for three sensors was almost similar for classes a and b networks. as a consequence, it is apparent that the proposed optimization algorithm is able to efficiently handle resource allocation among the two classes of network in a fair manner by providing different optimal timing to class b sensors based on their calculated distances to the available energy resources, as well as equal optimal timing to the sensor nodes in class a."
"this distributed allocation of the sensing times and the node powers presents several advantages compared to a centralized allocation [cit] : (1) robustness of the system towards a variation of parameters (such as the gains of the sensing channels), (2) maintainability of the system thanks to the modularity of the multiple agents and (3) scalability of the system as the need for control communication is minimized: on the one hand there is no need for a central authority to send the result of a centralized allocation to the multiple nodes and on the other hand these nodes do not have to send their specific parameters (sensing snrs and data rates for the sensing time allocation, space coordinates for the power allocation problem). in addition, a centralized allocation is not a trivial operation as the sensing time and the power allocation problems are both essentially multi-criteria problems where multiple objective function to maximize can be defined (e.g., the sum of the individual rewards to aim for a global optimum or the minimum individual reward to guarantee more fairness)."
where 2m-2 is the number of segments in each service cycle of a train because the trains run empty at start station and return station.
"in the third exploration strategy, the algorithm does pure exploration during the¯ f first learning iterations of each tdma time slot, then pure exploitation during the remaining (1 −¯ ) f last learning iterations of the time slot (see, figure 4 ):"
"where +∞ represents a positive constant that is chosen large enough compared to sinr s l,t . with this cost function, every agent tries to achieve the maximum sinr s l with no consideration for the other secondary cells in the network. second, we define a cooperative cost function in which the cost decreases if the difference between the secondary sinr at the base station and the required secondary sinr threshold decreases, provided that the aggregated interference on the protection contour is acceptable:"
"recently, an rf eh method based on wireless energy transmission and wireless information transmission (wipt) has emerged as a promising solution for powering sensor nodes in wsns. the technique is suitable for conveying the signals of the sensor nodes to a local base station. such a system is typically referred to as a sensor network that is based on wireless powering (wpsn). with this wipt technique, abundant rf energy could be efficiently transferred from an ips to a large number of sensor nodes in a network to achieve a stable supply of energy without any interruption in communication that may result due to energy depletion [cit] . this method employs harvesting of rf energy from an ips, compared to the traditional eh methods that harnesses energy from ambient energy sources, which may include ambient rf. this wipt method is envisioned to provide a lasting solution to the problem of energy scarcity in the future wsn applications and internet-of-things (iot) sensors [cit] . also, by employing this wipt method, a more reliable wsn system with an unwavering quality-of-service (qos) experience in the context of high throughput, and stable energy supply, can be realized [cit] . unfortunately, existing systems based on wipt methods are faced by an unfairness issue in eh caused by a problem referred to as doubly-near-far. this situation is a fundamental problem in wpsn systems and affects the information transmission rate of the system [cit] . the doubly-near-far situation in wpsn systems initiate unfairness in the energy received by individual sensor nodes in a wpsn system, as the sensor nodes in a network receives varying amount of energy based on a key factor, namely the distance from an ips. similarly, the unfairness issue among network sensor nodes is exacerbated by the distance to a base station (bs), where sensor nodes near the bs spend less energy to transfer their separate signals, while the farther sensor nodes consume more energy to transfer their signals. consequently, some of sensor nodes that are not privileged to be nearer to the bs may not have adequate energy to transfer their separate signals due to energy inadequacy. thus, based on the unfairness problem in eh among the sensors in a network, the overall throughput rate of a system is impacted negatively. this similarly translates to the inability of some of the sensor nodes in participating in transferring their vital signals in their allocated time. to enhance fairness among the sensors during eh, as well as to enhance the system throughput, the energy obtained by the individual sensor nodes in the network, including their information transmission rate, can be optimized in a joint manner. correspondingly, these improvements will circumvent the energy scarcity issue, since throughput qos is improved with less power."
"this section gives a simple overview of the building blocks of a sensor node for monitoring water quality parameters. the water quality sensors are portable, but powerful tools used for monitoring the microbial and the chemical parameters of water quality at water stations. an integral component of a water quality sensor is the communication technology. communication technologies can be classified into two categories, namely local communication technology and remote communication technology. the local communication technology is used to connect a sensor to another sensor, as well as a bs. the remote communication technology is responsible for delivering water quality information to a remote center. the remote communication technology acts as an internet gateway in the network. an internet gateway simply means an internet access point via which the system is connected to the internet."
"where q -1 (x) is the inverse function of q(x). as illustrated on figure 1, we consider that every t h seconds, each node sends a one bit value representing the local hard decision about the primary network presence to the base station. the base station combines the received bits in order to make a global decision for the nodes. the base station decision is sent back to the node as a one bit value. the duration of the communication with the base station is assumed to be negligible compared to the duration t h of a time slot. in this article, we focus on the logical-or fusion rule at the base station but the other fusion rules could be similarly analyzed. under the logical-or fusion rule, the global detection probability p d (defined as the probability that the coordinator node identifies a time slot as busy when the primary network is present during this time slot) and the global false alarm probability p f (defined as the probability that the coordinator node identifies a time slot as busy when the primary network is absent during this time slot) depend, respectively, on the local detection probabilities p d j and false alarm probabilities p f j [cit] :"
"it is observed that the cost decreases with respect to sinr s l until sinr s l reaches the threshold value sinr s th, then the cost increases with respect to sinr s l . this should prevent secondary users from selfishly transmitting with a power higher than required, which would remove transmission opportunities for other secondary users."
"to optimize the energy consumption of each sensor node a, only a fraction of the energy obtained by each of them in (4) is allowed to be consumed for information transmission. consequently, an average transmission power is defined for the sensor nodes as modeled in (5) as follows:"
we consider a cognitive radio cell made of n + 1 nodes including a central base station. each node j performs an energy detection of the received signal using m j samples [cit] . the observed energy value at the j th node is given by the random variable:
"in equation 12, z max and z min denote the maximum and minimum of values z, respectively, in the previous generations. in the current generation, we should select the minimal z and then update z min . the value offitness will increase and approach 1 with the increase of iterations."
we consider a large circular primary cell made up of one central primary emitter and several primary receivers whose positions are unknown. the primary emitter could be a dtv broadcasting station that communicates with multiple passive receivers.
"numerical results have demonstrated the need for an exploration strategy for the convergence of the sensing time allocation algorithm. it has also been observed that the strategy of keeping the exploration parameter constant in the power allocation algorithm is less efficient than using a linearly decreasing parameter or implementing an alternance between full exploration and full exploitation, this latest exploration policy leading to the fastest convergence of the power allocation algorithm."
"the integer-programming model proposed in section 2 deals with a complex, non-linear programming problem. this problem is difficult to solve by gradient-based methods or commercial optimization solvers. we adopt the genetic algorithm which is based on natural selection to solve the optimization problem. the key variables are the departure times of trains at the start station, the dwell time for each train at each station, and the running time for each train between station pairs, which can be represented by a chromosome. in figure 5 we plot a simple metro network with two lines to explain the application of our method. at first, we use ga method to optimize the departure time, the dwell time, and the running time for line 1. then we can apply these optimized variables to line 1 to decrease the total passenger travel time on line 1. for the transfer station s4 on line 1, the passengers that transfer to or from line 2 can be considered to leave or enter s4 for simplicity. when for line 2, the process is the same with that of line 1."
"in tables 2 and 3, we show the optimized dwell time and the optimized running time. in table 4 we show the number of passengers at the congested stations and the corresponding trains. the number of passengers written with a bracket, corresponds to the station number written with a bracket. in figure 11, we plot the corresponding timetable obtained by our method. we can see that the departure time headways of trains are inhomogeneous, and essentially consistent with the time-dependent demand patterns in figure 10 . (550) 12 (21) - figure 11 -the optimized timetable scheme for the metro line obtained by our method number of dispatches is 88. other parameters are shown in table 1 and given in section 4.2. we run each setting 10 times, and average the results. as table 6 shows, the average travel time decreases with the pullout time and minimum headway. this indicates that low pull-out time and headway can give more adjusting time for realizing higher utilization of train resource and obtaining lower passenger travel time. however, it also brings the difficulty for operations. we can see from table 6 that the computation time increases with the increase of population size. larger population size increases the computational burden, and leads to long convergence time. it is noted that the individual diversity is also influenced by very small population size. in the tests, it was found that it is difficult to obtain the global optimal solution when the population size is less than 30."
"the maximization of the wpsn system attainable throughput is described in this segment. to achieve this, a sum-throughput optimization strategy is employed. based on the optimization technique, the timing schedules for the harvesting of energy and transmission of information by sensor nodes a and b were optimized in joint fashion. with this, an improved fairness in the allocation of harvesting timing, as well as fairness in the rates of the sensor nodes information transmission, is achieved. consequently, an enhanced system overall throughput rate is achieved with minimal energy consumption. the general representation of the system attainable throughput is formulated as a maximization problem in (p1). from (1), we have: (p1):"
"in this section, we investigate three timetable methods. they are a half-regular timetable method, an irregular timetable method [cit] that only focus on the passenger waiting time and the capacity of trains, and our method. for the half-regular timetable method and the irregular timetable method [cit], the dwell time and running time are set based on the data provided by nanjing metro, as tables 2 and 3 show. for our method, the dwell time of a station is in the range of [0.2 1.00] minutes, and the running time between the departure times to satisfy the constraint that the minimum headway between two adjacent dispatches is not less than h. otherwise, the mutation will be re-implemented."
"as an important public transit system, the metro networks have many advantages for passengers such as large capacity, high speed, and high reliability. the improvement of the service of the metro networks, especially the reduction of passenger waiting times, has attracted many recent efforts in cities. some dynamic operation strategies, such as dynamically adjusting train speed or dwell time at stations [cit] are proposed. however, these dynamic operation strategies usually imply complex operations and are difficult to implement in real applications. compared with the dynamic operation strategies, the timetable methods, which are relatively stable and secure, are accepted as the most straightforward and effective solutions [cit] ."
"we apply our method to the first stage project of line 1 of nanjing metro in china, which has 16 stations and is 21.72 km long, passing across 5 districts, as shown in figure 9 ."
"the micro-controller module is responsible for the coordination of the processes that integrates the sensor module with other modules in a way to execute instructions that relates to the measurements of the sensor module. other key functions carried out by the micro-controller involves the collection of the information measured by the sensor unit, storing of the gathered measurements in its storage chip, and transferring of the information collected using the communication technology of the communication module to a bs. the communication module is important in the water quality sensor node architecture as it provides a suitable platform for water quality information transmission, and reception of important control signals. the communication module is usually implemented as an rf transceiver. the rf transceiver is equipped with an antenna, and has the capabilities for both information transmission and reception. the cc2420 zigbee radio is an example of a communication technology for local information transmission, and is defined in the ieee 802.14.4 specification [cit] . the zigbee radio is considered suitable to be employed in this work because of its low-cost and low-power features. each of the zigbee-based water quality sensors communicates directly with a local bs over the license-free ism bands (such as 2.4 ghz and 915 mhz). through a remote communication technology employed at the bs, which acts as a gateway to the internet (such as 2g, 3g, or lte networks), the water quality information received from the sensors is delivered to the remote monitoring stations [cit] ."
"we divide the study period equally into several extremely tiny time intervals (e.g. 0.2 second), and then refer to any particular time interval as time and use t to index it. the modelling time interval length is sufficiently small, which allows only one passenger from the outside arriving at a station during a single time interval. without this assumption, if the remaining train capacity at time t is 1, and if 2 or more passengers arrive at a station during a time interval, then none of passengers can board the train in order not to violate the capacity constraints. furthermore, all passengers leave the station as soon as they arrive at their destinations."
"most often, the time-multiplexing receiver model is employed in wpsn systems because of its installation simplicity, portability, and suitability for efficient harvesting of energy from rf signals [cit] . unfortunately, the current wpsn solutions which are developed based on time-multiplexing are confronted with a number of issues when there are no efficient strategies in place [cit] . such issues range from unfairness in eh time allocation, interference problems caused by energy transmission in the context of multiple ips, to unequal information communication rates within the sensors in a network [cit] . a multi-class, multiple-intended-source (mcmis) wpsn system is proposed for monitoring the quality of water in water stations, to address the above-mentioned issues. as well, this paper is intended to ensure that individual sensors in the network are efficient enough to obtain adequate energy for delivering their acquired signals with the desired qos. the major contributions of this paper are fourfold as highlighted below: there is no prior work on a mcmis wpsn system for monitoring water quality that studied this problem, to the best of our knowledge. the structuring of this study is highlighted as follows. following from the review of related works in literature in section 2, the structure of a sensor node devoted to monitoring water quality is presented in section 3. the section expounds the proposed mcmis wpsn system architecture and the proposed tdma protocol for the system. the proposed model for the new system wireless channel is described in section 3. this section also contains the optimization of the energy and information transmissions rate problem, as well as an efficient algorithm for multiple ips allocation and information transmission timing. the discussion of the proposed wpsn system sum-throughput and fairness results are considered in section 4, which validate the formulated sum-throughput optimization problem. the conclusion of the paper is contained in section 5."
"reinforcement learning algorithms such as q-learning are particularly efficient in applications where reinforcement information (i.e., cost or reward) is provided after an action is performed in the environment [cit] . the sensing time and power allocation problems both allow for the easy definition of such information. in this article, we make the assumption that no information is exchanged between the agents for each of the two problems. as a result, many traditional multi-agent reinforcement learning algorithms like fictitious play and nash-q learning cannot be used [cit], which justifies the use of multi-agent q-learning in this article to solve the sensing time and power allocation problems."
"it has furthermore been shown that the implementation of a cost function that penalizes the actions leading to a higher than required throughput in the sensing time allocation algorithm gives better results than the implementation of a cost function without such penalty. similarly, the implementation of a cost function that penalizes the actions leading to a higher than required secondary sinr in the power allocation algorithm gives better results than the implementation of a cost function without such penalty."
"mutation helps the generations to escape from a local optimum by altering one or more gene values in a chromosome from its initial state. the mutation points on a chromosome are selected at random. then, the selected mutation points on the chromosome mutate from 0 to 1 or from 1 to 0. after mutation, safety checking is also implemented for the chromosome part of trains between each station pair. for the departure time, the binary string '1' indicates a departure and '0' indicates no-departure at the corresponding moment. for the dwell time and running time, the binary string '1' means their corresponding value. the interval of the binary string is a scale problem. here, the interval of the departure time of each train is 1 minute, and the interval of dwell time and running time is 0.05 min. in figure 6, an example of the chromosome is plotted. as the example shows, the departure times of trains at the start station are 6:32, 6:34, 21:28, etc. the dwell time at a station is 0.25 min. and the running time between a station pair is 1.10 min. for high resolution, the density of chromosome points needs to be increased and the time scale modified."
"wherer j denotes the throughput required by node j. it is observed that the cost decreases with respect to r j until r j reaches the threshold valuer j, then the cost increases with respect to r j . this should prevent secondary users from selfishly transmitting with a throughput higher than required, which would reduce the achievable throughputs for the other secondary users."
"the objective function of the optimization problem is given in (21), while the constraints of the optimization problem are (21a) to (21e). constraint (21a) is the timing schedules for energy harvesting and information transmission. the non-negative constraints (21b), (21c), (21d), and (21e) are defined for the decision variables, while variables j, t, ζ, ξ are unknown in (p1). the maximization problem in (p1) is a non-convex problem since (9) and (18) contain a log function. by exploiting the structure of the problem, variable t c ξ 0 is changed to ξ 0, c, and the natural log form of the log function is obtained. they are substituted in (9) and (18) respectively. based on this development, the optimization problem in (p1) is transformed to a convex problem. the newly generated problem from the original problem is defined as (p2). the proof for the new problem is provided in appendix 1. consequently, the newly transformed problem is solvable by employing any standard convex approach [cit] ."
"in the system architecture, the sensor nodes a in class a are provided with equal optimal eh time, because of their nearness to the ips. unlike the sensor nodes in class a, there are different distances within the sensor nodes in class b because of the random approach employed for their deployment. therefore, there may be some significant variations in the energy a sensor node in class b is able to harvest in a dl-eh block. this situation is an inherent issue in wpsns that is typically referred to as the doubly-near-far problem. when this problem is encountered in a network, the energy that a particular sensor node which is not far from a bs is able to harvest is significant compared to the energy that another sensor node which is far from the bs is able to harvest. this can be attributed to the condition of the wireless channels. to tackle the doubly-near-far issue in this paper, unlike the same optimal eh time that is allotted to class a sensors, different optimal eh time is provided to the individual sensors in class b. in addition, in the ul stage, an optimal information communication period is provided to class a sensors, as well as class b sensors, based on their distances to the bs, to ensure completeness in the transmission of their individual information to the bs. to achieve this, in"
"the effect of unequal network distance to the bs between two classes of network is investigated in this section to emphasize the contributions of the newly existing system with the same configuration, and it is apparent that a substantial increase of 25.68% and 26.67% in transmission throughput rate is attainable with the proposed system for class a and class b, particularly when the available energy is constrained by a small number of ipses. furthermore, class a in the proposed system, which has a smaller distance to the bs, must spend lower energy on information transmission to the bs in the ul, and consequently, the network achieved a significant improvement in the attainable average sum-throughput compared to class b."
"attainable sum-throughput as the value of path-loss exponent increases. the reduction experienced in the system attainable throughput due to rise in path-loss exponent is valid in the two-sensor, three-sensor, and four-sensor systems. another observation is that, a system with four-sensor had a higher average attainable sum-throughput compared to the systems with two-sensor and three-sensor. therefore, it is confirmed that the system performs better when it is configured with a lower path-loss exponent value."
"at each iteration t, the action selected by the secondary user j is the duration m j, t of the sensing window to be used during the t l seconds of the learning iteration t. it is assumed that one learning iteration spans several time slots:"
"it is noted that only focusing on minimizing passenger waiting time may lead to congestion on trains and stations, especially for peak-hour demand. the capacity of trains and stations, which is closely related to congestion, is a key problem to the station passenger flow organization, station device schemes, and train operation [cit] . the definition of capacity has been regarded as a significant issue for decades in the railway industry [19,20, 21] . the capacity of trains and stations may vary according to factors such as train speed, commercial stops, train heterogeneity, distance between railway signals, and timetable robustness [cit], and has been studied from local [cit] and systematic [cit] points of view. besides the capacity, it should be also pointed out that the distribution of passengers along the platforms and inside the trains is complex and heterogeneous in reality, especially for crowded situations [cit] . for the platforms, the level of service may range from large space to personal space which is equivalent to the approximate area of the body ellipse. similarly, the situation in trains may range from many vacant seats to the over-crowded case when no one can move freely [cit] . moreover, it was found that the dwell time of trains can be influenced by boarding and alighting, or total interchanging passengers [cit] ."
"where r s is the ray of the secondary cells, f c is the transmission frequency and c is the speed of light in vacuum."
"each information transfer block, the distances to the bs of the sensor nodes a and b in classes a and b, respectively, are considered, and based on this distance, an optimal time is allotted to an individual sensor to transfer its individual information. the new tdma protocol described here is summarized in fig. 3 ."
"the passenger loading requirement is used to trade off the benefits between the transit service provider and the transit service users, which can be quantified by the ratio of the overall number of on-board passengers to the total utilized train capacity [cit] . it is reasonable to allow a few trains to have lower load rates than required during early morning or late night"
"we have also investigated the sensitivity of our method. table 6 shows the average passenger travel time and computation time for different pull-out times, minimum headways, and population sizes. the free-flow state is meaningful for determining the optimal dispatch of trains that has minimal cost and can avoid congestion. when the number of dispatches arrives at 92 with fleet-size 10, the free-flow state with minimal passenger travel time is obtained. figure 13 shows the timetable scheme for the 92 dispatches obtained by our model. related parameters are shown in table 1 . we concentrate on the transition from congestion to free-flow state. as table 7 shows, the number of congestion events decreases with the increase of the dispatches. when the number of dispatches increases from 90 to 91, an abrupt decrease of the number of congestion events is observed. the critical point from congestion to"
"for a certain train, it must arrive at the start station before we re-dispatch it. we use t to represent the minimum recovery allowance time for the finished train at final destination to be ready for a new service at the start station, which is called pull-out time here. the corresponding constraint is"
"the parameters of (21) such as log function, rendered the optimization problem as a non-convex function, and it was transformed through problem structure exploration technique by changing variable t c ξ 0 to ξ 0, c, and obtaining the natural log of the log function in (9) and (18) respectively, to obtain a new problem (p2). from (9) and (18), (27) and (28) were derived as follows:"
"in this article, we compare the performances of the sensing time allocation system for two different cost functions c j, t . we firstly define a competitive cost function in which the cost decreases if the average throughput realized by node j increases:"
"in recent times, there has been an upsurge in the need for efficient sensing systems for monitoring the parameters of water quality that include bacteria load and ph values, in a timely fashion [cit] . most times, escherichia coli is considered as an indicator organism for microbiological analysis of water [cit] . the main reason for the upsurge in seeking efficient sensing systems is because of the devastating impact of unclean water on human, plant, and animal. the systems are intended to complement the existing traditional systems for effective monitoring of water and its quality, in order to combat the problem of contamination in water [cit] . across the globe, an approximate estimate of 250 million cases of disease caused by polluted water is reported annually [cit] . these diseases are responsible for human death and claims up to about ten million lives across the globe annually [cit] . this is an indication that water problems caused by contaminants are major issues in this dispensation. the alarming rate of human death on a global scale caused by water pollution is as a result of a surge in water and environmental contaminants. these contaminants are due to two major factors, namely natural processes and man-made (anthropogenic) activities [cit] . examples of natural phenomena that adversely influence the quality of water through climatic factors are run-off caused by hydrological conditions, rock weathering, soil leaching, depositions caused by wind, and evapotranspiration, while some of the key man-made activities that negatively influences the quality of water are mining operations, deforestation, agricultural run-off, and industrial effluent [cit] ."
"from table 1, we can see that the computational costs and memory requirements for both sdcut-qn and sdcut-sn are linear in m, which means that our methods are much more scalable to m than interior-point methods. in terms of n, our methods is also more scalable than interior-point methods and comparable to spectral methods. especially for sparse/structured matrices, the computational complexity of sdcut-qn is linear in n. as sdcut-sn cannot significantly benefit from sparse/structured matrices, it needs more time than sdcut-qn in each descent iteration for such matrices. however, sdcut-sn has a fast convergence rate than sdcut-qn. in the experiment section, we compare the speeds of sdcut-sn and sdcut-qn in different cases."
"one main advantage of quasi-newton methods over newton methods is that the inversion of the hessian matrix is approximated by analyzing successive gradient vectors, and thus that there is no need to explicitly compute the hessian matrix and its inverse, which can be very expensive. therefore the per-iteration computation cost of quasi-newton methods is less than that of standard newton methods."
"we consider image segmentation with two types of quadratic constraints (with respect to x): partial grouping constraints [cit] and histogram constraints [cit] . the affinity matrix w is sparse, so arpack is used by sdcut-qn for eigen-decomposition."
"for the fastflow parfor, we use the notation \"ff chunk\" where chunk is the chunk size used for the considered test. all fastflow tests have been compiled on both platforms with the flag \"-dno default mapping\" except for the 2 micro-benchmarks tests on the intel nehalem where the default thread pinning policy was enabled."
"the quasi-newton algorithm for (6) (referred to as sdcut-qn) is summarized in algorithm 1. in step 1, the dual problem (6) is solved using l-bfgs-b [cit], which only requires the calculation of the dual objective function (6) and its gradient (8) . at each iteration, a descent direction for ∆u is computed based on the gradient ∇d γ (u) and the approximated inverse of the hessian matrix:"
"the main objective of the scheduling policy proposed, is to try to obtain a good trade-off between workload balancing and chunk-to-thread affinity assignment. maintaining as much as possible loop iterations affinity is an important performance factor on shared-cache spms to increase the probability to find the data needed for the computation in one of the cache level hierarchies, thus reducing somehow the communication overhead incurred by addressing non-local data on smp numa platforms. the chunk-to-thread affinity assignment policy, which implements the idea proposed in the early '90s of loop affinity scheduling on shared memory multiprocessors [cit], is implemented totally in the forsched using a task table having one entry for each worker thread."
"algorithmic skeletons have been around since the '90s as a viable and effective solution to support parallel application development. an algorithmic skeleton is a general purpose, efficient, reusable, parametric parallelism exploitation pattern [cit] . application programmers may instantiate skeletons (or proper composition of skeletons) to encapsulate and exploit the full parallel structure of their applications. business code may be passed as a parameter to the generic skeleton, thus turning the generic skeleton into a part of a parallel application. algorithmic skeletons are usually provided to the application programmers as library entries and therefore a complete separation of concerns is achieved: application programmers are in charge of the design of the most convenient parallel application structure through proper selection of the skeletons (skeleton compositions) among those provided by the skeleton framework, while system programmers are in charge of target architecture specific, efficient and scalable implementation of the parallel skeletons."
v. conclusions this paper presents the parallelfor skeleton implementation provided within the fastflow parallel framework. the proposed skeleton allows to parallelise independent loops with a moderate programming effort similar to that required by wellknown parallel frameworks such as openmp and intel tbb.
"a simple yet effective dynamic tasks scheduling policy has been studied and implemented. by using a set of benchmarks, we demonstrated that the parallelfor implementation proposed is able to obtain comparable or even better performance results with respect to those achieved parallelising the same code using openmp and intel tbb. all tests have been executed on two different platforms: a intel xeon phi many-core and a intel nehalem 12-core numa multi-core."
"as target architectures we considered the many-core intel xeon phi 5110p (hereinafter intel phi) featuring 60 cores running at 1056 mhz interconnected by a bi-directional ring bus. it is a smp computing node connected to the host domain through a pci express (pcie) bus. it runs an embedded linux x86 64 os that provides basic functionalities such as process/thread creation, scheduling and memory management. intel phi cores run independently of each other, having support for 4-way hw multi-threading being able to execute 4 threads by interleaving instructions. each core has a 32 kb l1 data cache and a 512 kb l2 cache and 512 bit-wide vector unit used to execute single/double precision simd instructions. the total amount of memory is 8 gb (gddr5) comprising 8 memory controllers."
"spectral methods are effective for many computer vision applications, such as image segmentation [cit] and motion segmentation [cit] . the optimization of spectral methods eventually lead to the computation of top eigenvectors. nevertheless, spectral methods may produce loose relaxation bounds in many cases [cit] . moreover, the inherent quadratic programming formulation of spectral methods is difficult to incorporate certain types of additional constraints [cit] ."
"in the parfor implementation, no shared data structure among threads is maintained in order to avoid locking over-heads. the main drawback of using a centralised thread for iterations scheduling is that it may become a bottleneck when an high number of worker threads is used. although this is not a big issue in principle because multiple scheduler threads can be used together (for example organised in a tree topology -exchanging latency with bandwidth), we will see in the experimental section that the forsched is not a bottleneck for current state-of-the-art multi/many-core."
"furthermore, when using a skeleton-based parallel approach for a given parallel problem, a proper selection of the appropriate implementation skeleton (typically totally in charge of the application programmer) together with a correct implementation of the sequential wrapper code is of foremost importance for obtaining the best performance."
"wheref(, u) is a smoothing function of f(u), which is constructed as follows. firstly, the smoothing functions for π d and π s n + are respectively written as:"
"from table 7, we can find that smqc is faster than our methods. however, smqc does not scale well to large problems since it needs to compute full eigen-decomposition. we also test sdcut-qn and smqc on problems with a larger number of superpixels (9801). both of the algorithms achieve similar segmentation results, but sdcut-qn is much faster than smqc (23m21s vs. 4h9m)."
"the parallel_for splits the half-open range [0, niter) into sub-ranges and processes each sub-range r as a separate task using a serial for loop in the code. the range and subrange are implemented as blocked_range objects. the function template parallel_for maps a functor across range of values. it is provided in several forms, in this paper we used the range-based algorithm version with c++11 lambda expressions. as an example of use, consider the following serial code:"
"sdp relaxation has been shown that it leads to tighter approximation than other relaxation methods for many combinatorial optimization problems [cit] . in particular for the max-cut problem, goemans and williamson [cit] achieve the stateof-the-art 0.879 approximation ratio using sdp relaxation. sdp relaxation has also been used in a range of vision problems, such as image segmentation [cit], restoration [cit], graph matching [cit] and co-segmentation [cit] . in a standard sdp problem, a linear function of a symmetric matrix x is optimized, subject to linear (in)equality constraints and the constraint of x being positive semidefinite (p.s.d.). the standard sdp problem and its lagrangian dual problem are written as:"
"as we can see in table 8, sdcut-qn takes 10 times more iterations than sdcut-sn, but still runs faster than sdcut-sn especially when the size of problem is large (see \"face\" data). the reason is that sdcut-qn can exploit the specific structure of matrix a in eigen-decomposition. sdcut-qn runs also 5 times faster than lowrank. all methods provide similar upper-bounds (primal objective values), and the score vectors shown in fig. 5 also show that the evaluated methods achieve similar visual results."
"the scheduling policy currently implemented, works as follow: loop iterations are divided into equal pieces of size chunk; contiguous chunks are logically assigned to the same thread trying to equalise as much as possible the number of chunks for each thread in the pool; the forsched sends chunks to workers upon request; during the computation of the for loop, if the forsched does not have any chunk of iterations to schedule to the requesting thread, it tries to \"steal\" a chunk (if available) from another thread. to implement this simple policy, a task table containing the number of currently available tasks and the minimum and maximum iteration indexes yet to be executed is maintained in the forsched as a private data structure (see fig. 1 right-end side) ."
"lower-order control method of joints in eight dofs quadruped robot is shown in figure 6, which builds intercoupling mapping relations between hip joint and knee joint. the control signals of cpg output are used to control the four corresponding hips directly and control the four knee joints indirectly by coupling relationship. cpg oscillation control system couples with hip joints, and hip joints couple with corresponding knee joints. this makes up the intercoupling control system."
"two typical gaits are discussed in this paper, including walk and trot. walk belongs to a still gait, and each leg puts up and down in turn; the phase difference between legs is a quarter cycle. trot means that the two diagonal legs put up and down at the same time and is better at energy consumption and belongs to dynamic gait, of which the phase difference between legs is half of cycle. the relative displacement between two legs is a quarter of walk cycle during walking, and each connection among oscillators is inhibitory connection in a full symmetric cpg network. but excitatory connections are adopted in trot. thus, let the excitatory connection value be +0.1, which is a smaller positive value in, and let the inhibitory connection value be −0.1, which is a smaller negative value. the network topological structure is shown in figures 3 and 4, respectively. cpg equation is solved by using the four-order rungekutta. the initial values of matrix are random numbers which are one order of magnitude larger than and v :"
"in stability control methods, liu and chen proposed stability control method of gait based on zmp (zero moment point) theory to control a quadruped robot, which achieves some success in stability control but lacks efficiency and flexibility of gait planning and gait switch [cit] ."
a step size ρ is found using line search. the algorithm is stopped when the difference between successive dual objective values is smaller than a pre-set tolerance.
"the average f-measure of all evaluated methods are reported in table 7 . our methods outperforms graph cuts and smqc in terms of f-measure. as for the running time, sdcut-sn is faster than all other sdp-based methods (that is, sdcut-qn, sedumi, sdpt3 and mosek). as expected, sdcut-sn uses much less (1/6) iterations than sdcut-qn. sdcut-qn and sdcut-sn have comparable upper-bounds and lower-bounds than interior-point methods."
"solving (3) is in general np-hard, so relaxation methods are considered in this paper. relaxation to (3) can be done by extending the feasible set to a larger set, such that the optimal value of the relaxation is a lower bound on the optimal value of (3). the sdp relaxation to (3) can be expressed as:"
(1) is introduced as amplitude limiting coefficient to adjust the outputs of and v; out is an output of linear synthesis to control the movement of corresponding leg.
"as shown in fig. 1, the first data set (the first row) contains two sets of points with different densities, and the second set contains an outlier. ratiocut and ncut fail to offer satisfactory results on both of the data sets, possibly due to the poor approximation of spectral relaxation. in contrast, our sdcut-qn achieves desired results on these data sets."
"that the application programmer succeeds modelling the parallel structure of the application at hand using a proper composition of the available skeletons. unfortunately, structuring the parallelism of an application with skeletons requires an extra effort by the application programmer. the amount and kind of effort required in the different skeleton programming frameworks often impaired the acceptance of the frameworks despite the encouraging performance values demonstrated. the problem is often exacerbated taking into account that single parallel patterns may be trivially expressed in other state-ofthe-art non structured parallel programming frameworks such as openmp [cit] ."
"the mrf models for chinese character inpainting are obtained from the opengm benchmark [cit], in which the unary terms and pairwise terms are learned using decision tree fields [cit] . as there are non-submodular terms in these models, they cannot be solved exactly using graph cuts. in this experiments, all models are firstly reduced using qpbo and different algorithms are compared on the 100 reduced models. our approach is compared to lpbased methods, including trws, mplp. from the results shown in table 11, we can see that sdcut-sn runs much faster than interior-point methods (sedumi and mosek) and has similar upper-bounds and lower-bounds. sdcut-sn is also better than trws and mplp in terms of upper-bound and lower-bound. an extension of mplp (refer to as mplp-c) [cit], which adds violated cycle constraints iteratively, is also evaluated in this experiment. in mplp-c, 1000 lp iterations are performed initially and then 20 cycle constraints are added at every 20 fig. 6 : image deconvolution. qpbo cannot label most of pixels (grey pixels denote unlabelled pixels), as the mrf models are highly non-submodular. sdcut-sn and mosek have similar segmentation results. trws and mplp achieve worse segmentation results than our methods."
"assume that the barycentre of robot is in its geometric center and the ground is plane, so the height coordinate of robot centroid above the earth is a constant, and coordinates of zmp can be obtained by the following formula: where is robot barycentre coordinate of -axis, is robot barycentre coordinate of -axis, is robot barycentre coordinate of -axis,̈is the acceleration of -axis, andï s the acceleration of -axis."
"based on proposition 3, first-order methods (for example gradient descent, quasi-newton), which only require the calculation of the objective function and its gradients, can be directly applied to solving (6) . it is difficult in employing standard newton methods, however, as they require the calculation of second-order derivatives. in the following two sections, we present two algorithms for solving the dual (6), which are based on quasi-newton and inexact generalized newton methods respectively."
"the configuration form of the robot joint is an inward knee-elbow form. the walk gait matrix is walk . the motion tracks of hip joints are controlled by cpg and knee joints are controlled by half-wave functions. the movement curves of hip-knee joints are shown in figure 9 by matlab simulation. the zero lines of motion curves in figure 9 are the balance states of joints. swing amplitude of hip joints near the balance state is ℎ, while unilateral swing amplitude of knee joints is . the motion curves of front legs' knee joints are in the positive axis and those of the hind legs are in the negative axis. the movement curves of hip-knee joints for trot gait are shown in figure 10 when gait matrix is trot ."
zmp is in the upper left of support diagonal when the left hind leg 2 and the right front leg 4 are the supporting legs. zmp is in the upper right of support diagonal when the left front leg 1 and the hind front leg 3 are the supporting legs. so zmp crosses support diagonal twice in one movement cycle.
"openmp uses a directive based approach, where the programmers annotate their programs with pragmas that instruct the compilers about the parallelism to be used in the program. despite the introduction of task-based parallelism in recent versions of the standard, loop parallelism is the most important part of many openmp programs."
"lower-bounds, number of iterations and time achieved by sdcut-qn and sdcut-sn, with respect to different values of γ. there are several observations: i) with the increase of γ, upper-bounds become smaller and lower-bounds become larger, which implies a tighter relaxation. ii) both sdcut-qn and sdcut-sn take more iterations to converge when γ is larger. iii) sdcut-sn uses fewer iterations than sdcut-qn. the above observations coincide with the analysis in section 4.5. using a larger parameter γ yields better solution quality, but at the cost of slower convergence speed. the choice of a good γ is data dependant. to reduce the difficulty of the choice of γ, the matrices a and b i of equation (1) are scaled such that the frobenius norm is 1 in the following experiments."
"finally in step 3, the primal optimal variable x is discretized and factorized to produce the feasible binary solution x, which will be described in section 4.3."
"together with the sequential code wrapper, it provides two basic algorithmic skeletons: i) a farm skeleton, applying in parallel the function modelled by an inner skeleton composition (the farm worker) to all the items appearing on its input stream, and delivering results to its output stream; and ii) a pipeline skeleton, applying in sequence the functions implemented by its inner skeleton compositions (the pipeline stages) to the items appearing on the pipeline input stream, and delivering the results to the pipeline output stream. both pipelines and farms, when used at the topmost level in the skeleton composition, support a feedback-channel providing the programmer with the possibility to move data back from the output stream directly to the input stream."
"overall, the algorithmic skeleton approach guarantees efficiency, scalability and some kind of functional and performance portability across different target architectures (possibly including hardware accelerators and coprocessors) provided this work has been partially supported by fp7 strep paraphrase (www. paraphrase-ict.eu)."
"to bqps, we propose a quadratically regularized version of the original sdp formulation, which can be solved efficiently and achieve a solution quality comparable to the standard sdp relaxation. 2) we proffer two algorithms to solve the dual problem, based on quasi-newton (referred to as sdcut-qn) and smoothing newton (referred to as sdcut-sn) methods respectively. the sparse or low-rank structure of specific problems are also exploited to speed up the computation. the proposed solvers require much lower computational cost and storage memory than standard interior-point methods. in particular, sdcut-qn has a lower computational cost in each iteration while needs more iterations to converge. on the other hand, sdcut-sn converges quadratically with higher computational complexity per iteration. in our experiments, sdcut-sn is faster for dense or medium-sized problems, and sdcut-qn is more efficient for large-scale sparse/structured problems. 3) we demonstrate the efficiency and flexibility of our proposed algorithms by applying them to a variety of computer vision tasks. we show that due to the capability of accommodating various constraints, our methods can encode problem-dependent information. more specifically, the formulation of sdcut allows multiple additional linear and quadratic constraints, which enables a broader set of applications than what spectral methods and graph-cut methods can be applied to."
"the second architecture we considered is a standard dualsocket numa intel multi-core xeon e5-2630 nehalem microarchitecture (hereinafter intel nehalem) running at 2.30ghz featuring 12 cores (6+6) each one 2-way hyperthreading. each core has 32kb private l1, 257kb private l2 and 16mb shared l3. the operating system is linux 2.6.32 x86 64 shipped with red hat enterprise 6.3."
"sdcut-qn in general, quasi-newton methods converge superlinearly given that the objective function is at least twice differentiable (see [cit] . however, the dual objective function in our case (6) is not necessarily twice differentiable. so the theoretical convergence speed of sdcut-qn is unknown."
"computational intelligence and neuroscience the change of zmp in direction is in curve, which shows pose of robot is adjusted in the right or the left continually."
step 2: discretization: z is discretized to a feasible bqp solution (see table 2 for problem-specific methods). output: x is assigned to the best feasible solution.
", n are eigenvalues and the corresponding eigenvectors of c(u). supposing problem (5) is feasible and denoting u as the dual optimal solution, we have:"
"the walk gait simulation of quadruped robot based on webots with the improved cpg in the paper is shown in figure 15 . quadruped robot walks in the 1-3-2-4 order (1: left foreleg, 2: right foreleg, 3: right hind leg, and 4: left hind leg) and there are three legs on the ground which are in stand phase at any time from the simulation chart. the simulation of walk shows that robot moves at a constant velocity and the pose is steady, so the algorithm has good practicability. figure 16 . the left foreleg and the right hind leg are in support phase when the right front leg and the left hind leg are in swing phase. the simulation shows that the speed of robot in trot is obviously higher than the speed in walk, but the stability of body is lower."
"the remainder of the paper is structured as follows: sec. ii briefly introduces the background. sec. iii discusses the fastflow parallel loop implementation details. sec. iv discusses results obtained by using a set of tests. finally, sec. v draws conclusions."
"as future work, we are currently planning to extend the applicability of the current parallelfor skeleton implementation and to study dynamic loop iterations offloading to general purpose many-core and gpus."
"(3) external feedback of cpg control network is introduced, where is the reflection information, is coefficient of, and and v are the adverse vectors of refection coefficients."
"over the past 35 years, there have been many theoretical and experimental studies on the zmp. to summarize, the zmp criterion states that if the zmp is within the support polygon made between the foot and the ground, then stable dynamic walking is guaranteed [cit] . the schematic diagram of zmp is shown in figure 11 ."
"for testing our proposed controller, we designed a robot, which has 16 dofs, and each leg has 3 actuated rotary joints. each rotary joint is controlled by a steering engine. in addition, an imu sensor is attached to the robot's body to measure the orientation (roll-pitchyaw angles), body linear acceleration, and rotational velocity. furthermore, each leg is equipped with a load cell. to evaluate the efficiency of the proposed controller, we performed some tests under rough terrain using a walk gait. the desired velocity is set to 0.08 ms −1 . the robot can adaptively adjust velocity from 0.03 ms −1 to 0.08 ms −1 according to movement environment and its posture. experiment results show the cpg-zmp controller adapts to the environment change very well. the overview is shown in figure 22 ."
"based on the above result, the dual problem can be solved by quasi-newton methods directly. furthermore, we also show in section 4.2 that, the second-order derivatives of d γ (·) can be smoothed such that inexact generalized newton methods can be applied."
"zmp can be calculated by datum from force sensor and gyroscope. the control system can tell whether the zmp is outside of safe area. if true but still not up to the critical point of turnover, zmp detector sends a signal to reduce the global threshold quickly for restraining roll, and then the neural signal activity in cpg is lowered. if not outside of safe area, the control system will detect whether the current global threshold is smaller than the preset value; if true, the system will increase the global threshold. if zmp is outside of safe area and the robot is in the state of turnover, which is mainly made by external impact and disturbance, cpg stops working and the control system will recalculate balanced foothold. to recover the pose of robot, the angle of robot's each joint is recalculated by inverse kinematics. and cpg is back to work until robot pose is normal. the theory of zmp not only can be used as the stability criterion of robot gait, but also can be used to plan the corresponding gait when robot is in the state of turnover. parameters of walk gait and trot gait are shown in table 2 by cut-and-try method."
(1) the improved cpg system based on wilson-cowan model shortens the oscillation time and makes the system respond quickly and enhances the real time; meanwhile the gait switch is more smooth and rapid.
"we can see from fig. 6 and table 10 that qpbo [cit] leaves most of pixels unlabelled and lp methods (trws and mplp) achieves worse segmentation accuracy. sdcut-sn achieves a 10-fold speedup over interior-point methods while keep comparable upper-/lower-bounds. using much less runtime, sdcut-sn still yields significantly better upper-/lower-bounds than lp methods."
the tbb parallel_for and parallel_foreach methods implement the map pattern so they may be used to parallelise independent invocation of the elemental function body of a for loop whose number of iterations is known in advance. c++11 lambda functions can be used as arguments to these calls so that the loop body function can be described as part of the call rather than being separately declared.
"in this paper, we have presented a regularized sdp algorithm (sdcut) for bqps. sdcut produces bounds comparable to the conventional sdp relaxation, and can be solved much more efficiently. two algorithms are proposed based on quasi-newton methods (sdcut-qn) and smoothing newton methods (sdcut-sn) respectively. both sdcut-qn and sdcut-sn are more efficient than classic interior-point algorithms. to be specific, sdcut-sn is faster than sdcut-qn for small to medium sized problems. if the matrix to be eigen-decomposed, c(u), has a special structure (for example, sparse or low-rank) such that matrix-vector products can be computed efficiently, sdcut-qn is much more scalable to large problems. the proposed algorithms have been applied to several computer vision tasks, which demonstrate their flexibility in accommodating different types of constraints. experiments also show the computational efficiency and good solution quality of sdcut. we have made the code available online 3 ."
"data parallel applications in general, and sequential iterative kernels with independent iterations (parallel loops, from now on) in particular, have been proved to be easily implemented on multi-core platforms using the fastflow framework by streamisation of loop iterations implemented using the taskfarm parallel pattern [cit] ."
"for all these reasons, we decided to implement a set of parallel patterns on top of the basic fastflow skeletons to ease the implementation of parallel loops using a skeletonbased run-time. almost like in openmp pragma compiler directives [cit], with the new parallelfor pattern the programmer is only in charge to identify parallel loops, without the need to rewrite or change the loop body. currently, the new pattern is implemented using c++ macros, thus covering only a (significant) subset of all possible parallel loop cases. an extension of this pattern is currently under development."
"output curves of walk gait with the improved central nervous oscillator model are shown in figure 2 . the adjusting time of the improved model is less than 0.5 t, but that of the traditional wilson-cowan model is about 1.5 t [cit], so the adjusting time of the improved model is shortened greatly."
"in the openmp tests we used the auto scheduling policy because it allows to obtain the best performance (we tried many combinations between static, dynamic, guided and grain size). for the fastflow parfor we used a chunk size of 8 for both platforms. for small matrices the fastflow parfor obtains a significant performance improvement with respect to both openmp and tbb on the intel phi platform and comparable performance results on the intel nehalem multi-core. for the bigger matrix, the maximum speedup on the intel phi is 41 using 136 worker threads corresponding to an overall execution time of 417ms. the same test executed on the intel nehalem, obtains a minimum execution time of 730ms using 23 threads and a speedup of 10.8."
"transition process takes about 0.5 t, but that of the traditional wilson-cowan is about 1.5 t [cit] . because the adjusting time of the improved central nervous oscillator model is short, gaits transition is rapid and smooth."
"step 1 is equivalent to sampling z from the gaussian distribution n(0, x ), which has a probabilistic interpretation [cit] : x is the optimal solution to the problem"
"the reduction pattern can be accessed via the parallel_reduce construct. it allows the specification of an arbitrary combiner function. however, in order for the result to be computed deterministically the reduction function needs to be associative and commutative."
"note that the above random sampling procedure does not guarantee that a feasible solution can always be found. in particular, this procedure will certainly fail when equality constraints are imposed on the problems [cit] . but for all the problems considered in this work, each random sample z can be discretized to a \"nearby\" feasible solution ( step 2 of algorithm 3). the discretization step is problem dependant, which is discussed in table 2 ."
"the mapping function of knee joint is defined as formula (5), which indicates that the knee joint has movement in swing phase and has only tiny passive movement in support phase as a halfwave function. the tiny movement of knee joint in support phase is ignored in order to simplify control algorithm:"
"loop parallelism is a topic that has been repeatedly investigated over the years using different approaches and techniques for iterations scheduling [cit] . in this paper we concentrate for performance comparison on openmp [cit] and tbb [cit], which represent to a major extent, the most widely used and studied frameworks for loop parallelisations."
"for tbb, on both platforms, we found that, for the tests considered, the best result is obtained by using the affinity partitioner, so we used the notation tbb-ap for tbb runs."
"in this work, a new controller algorithm based on cpg-zmp (central pattern generator-zero moment point) is put forward in order to realize smooth gait planning and stability control at the same time. at first, a new cpg model controller and its gait switching strategy based on wilson-cowan model are presented in order to generate smooth gait and shorten the adjusting time of the model oscillation system. the control signals of knee-hip joints are obtained by the improved multi-dof reduced order control theory. and, then, adaptive speed adjustment and gait switch are completed by real-time computing of zmp to realize stability control. simulation results show that quadruped robot's gait planning is efficiently generated and the gait switch is smooth in the cpg control algorithm. meanwhile, the stability of the robot movement is improved greatly with cpg-zmp algorithm. the algorithm has been applied on joint quadruped robot, which greatly improves the stability of movement and the flexibility of gait generation and switch."
"in the following, a few desirable properties of (5) are demonstrated, where x denotes the optimal solution to (1) and x γ denotes the optimal solution to (5) with respect to γ. the proofs can be found in section 7."
"to show that the proposed sdp methods have better solution quality than spectral methods we compare the graph-bisection results of ratiocut [cit], normalized cut (ncut) [cit] and sdcut-qn on two artificial 2-dimensional datasets."
"the duty ratio of walk gait is 3/4, and the motion order of four legs is 1-3-2-4, which realizes reciprocating motion of four legs. the support phase of every leg costs three quarters of the cycle time, and swing phase costs a quarter of the cycle time. the duty ratio of trot gait is 0.5, which means the time of support phase and the time of swing phase are the same. the other two legs sway when the diagonal two legs step on the ground. the whole processes from the beginning of support phase to the end of swing phase for walk gait and trot gait are shown in figures 7 and 8, respectively."
"to evaluate the fastflow parfor implementation we performed a set of experiments using 2 simple synthetic micro-benchmarks and 3 benchmarks: standard dense matrix multiplication algorithm, inner product computation and the iterative jacobi computation. we compared our implementation against openmp 3.1 [cit] (using intel icc 13.0.1 and gcc 4.8.1) and intel tbb [cit] suite). for both compilers we used the optimisation flag -o3. all tests have been repeated 5 times then the the average value considered."
the farm skeleton can be instantiated in several different forms allowing to fully customise the task scheduling and gathering policies. for the implementation of the parfor pattern we used the one in which the emitter thread (the scheduler thread) has been completely programmed to schedule loop iterations and where farm workers are sequential wrappers executing chunks of the loop iterations.
"the coordinated movement control of multilegged robot has been a difficult problem [cit] in the field of robot because a robot needs to make a quick response to the change of external environment and various stimulus. the control strategy based on biological induction is a new control idea that has been gradually carried out in the multilegged robot researches [cit], in which the alternate rhythmic movement of each leg of the quadruped robot is the most common. biological studies show that rhythmic movement is usually achieved by cpg (central pattern generator) and can be applied to four-legged robot motion control [cit] ."
"the simulation results are shown in figure 20 . the robot's stability decreases when the speed is higher. but the robot adjusts gait adaptively by cpg-zmp control and is kept from falling over and the rollover effectively. the height change of robot centroid in the direction of -axis during the movements is shown in figure 21 . when the speed is up to 0.312 ms −1 (1.3v), the robot jolts violently in the direction of move, and then the robot becomes unstable gradually. cpg stops working and gaits of robot are replanned by inverse kinematics when zmp of robot is out of the safe range and is turning over. after about one and a half motion cycles, the cpg restarts working when zmp is in safe range; then the robot's gait becomes normal."
"on the one hand, this approach provides the programmer with great flexibility, allowing also to fully customise the scheduling policy and/or to nest multiple level of parallel computations. on the other hand, it requires a significant re-factoring of the original sequential code thus introducing possible new bugs and not preserving sequential equivalence."
"where is the excitatory connection strength of neuron and is the inhibitory connection strength of neuron, is the inhibitory connection strength of v to, is the excitatory connection strength of to v, and v are the external signals and the inputs usually, is the rise-time constant of step input, v is the fatigue time constant, ( ) is the coupling function, and is the gain of ( )."
"lp iteration. mplp-c performs worse than sdcut-sn under the runtime limit of 5 minutes, and outperforms sdcut-sn with a much longer runtime limit (1 hour). we also find that sdcut-sn achieves better lower-bounds than mplp-c on the instances with more edges (pairwise potential terms). note that the time complexity of mplp-c (per lp iteration) is proportional to the number of edges, while the time complexity of sdcut-sn (see table 1 ) is less affected by the edge number. it should be also noticed that sdcut-sn uses much less runtime than mplp-c, and its bound quality can be improved by adding linear constraints (including cycle constraints) as well [cit] ."
it may be rewritten in tbb using c++11 lambda as follows: c. fastflow fastflow 1 is a c++ based parallel programming framework built on top of posix threads aimed at providing the parallel programmer with a set of pre-defined algorithmic skeletons modelling the main stream-parallel patterns [cit] .
"the oscillator model presented by wilson and cowan is shown in figure 1, which is composed of excitatory neuron and inhibitory neuron v. a stable limit cycle shock is formed by the intercoupling of and v. the model can be described by the following differential equation [cit] :"
"in openmp, two constructs are used to parallelise a loop: the parallel and the loop construct. the parallel construct, introduced by the parallel directive, declares a parallel region which will be executed in parallel by a pool of threads. the loop construct, introduced by the for directive, is placed within the parallel region to distribute the loop iterations to the threads executing the parallel region (thread team). openmp supports several strategies for distributing loop iterations to threads. the strategy may be specified via the schedule(type[,chunk]) clause, which is appended to the for directive. the type of scheduling policy can be:"
"it is widely accepted that interior-point methods [cit] are very robust and accurate for general sdp problems up to a moderate size (see sedumi [cit], sdpt3 [cit] and mosek [cit] for implementations). however, its high computational complexity and memory requirement hampers the application of sdp methods to large-scale problems. approximate nonlinear programming methods [cit] are proposed for sdp problems based on low-rank factorization, which may converge to a local optimum. augmented lagrangian methods [cit] and the variants [cit] have also been developed. as gradient-descend based methods [cit], they may converge slowly. the spectral bundle method [cit] and the log-barrier algorithm [cit] can be used for large-scale problems as well. a drawback is that they can fail to solve some sdp problems to satisfactory accuracies [cit] ."
"the flow chart of hybrid cpg-zmp control algorithm is shown in figure 13 . the motion track of each joint is generated by the improved cpg and motion mapping; meanwhile we specify the global threshold of cpg ( in). rhythmic motion of robot is realized by cpg gait generator, and cpg can receive feedback signal from body sensor while working."
"after solving the dual using l-bfgs-b, the primal optimal variable x is calculated from the dual optimal u based on equation (7) in step 2."
"1) initial vigilance stage: the encounter situation between the target ship and own ship will be judged after the target ship enters the observation stage of own ship. whether there is collision risk with the target ship and own ship is the give-way vessel will be determined. at this stage, the nearest collision avoidance turning point is decided. meanwhile, the maintenance period for the original sailing velocity and direction can be gained. the above information can be used as warning sign at the early steering stage."
"collision t is the time when the distance between our ship and the target ship is the shortest when our ship moves along the original route. in order to guarantee safety of the ship, steering collision avoidance must be conducted before collision t ."
"ship collision avoidance system is a complicated decision-making process, as shown in fig. 1 . covering various dynamic or static data, certain or uncertain information, complex mathematical calculation, and plentiful theoretical derivations, it requires a large amount of disciplinary knowledge including navigation, computer science and other fields. moreover, it also involves different techniques like data collection, data preprocessing, division of encounter situation, calculation of collision risk degree, judgment of encounter conditions, selection of collision avoidance mode, optimization of collision avoidance action, establishment of navigation restoration decision, and multi-object collision avoidance. therefore, determination for safe collision avoidance path of ships is a multi-standard and nonlinear path planning issue. thereby, research scholars begin to apply various kinds of artificial intelligence technology to ship collision avoidance field [cit], so as to realize a balance between navigation\" safety \"and \"economy\". this means that the adopted ship collision avoidance mechanism should not only maintain relevant risk assessment and collision avoidance measures in navigation, but also consider deviation from the original path."
"in the process of planning a collision avoidance path, safe collision avoidance should be guaranteed first and then \"economical\" collision avoidance may be considered. the collision avoidance optimization algorithm is required to complete collision avoidance along the shortest path with the premise of ensuring safety; the objective function is as follows:"
"step7: execute migration operation for all bacteria with low fitness after copy operation according to the migration probability, and acquire the random new position in two-dimensional space."
"step6: after all bacteria pass the complete chemotactic operation based on bacteria reproduction operation of quantum behavior, acquire the best position and global optimal position of different bacteria in the population. after calculating the best vector, update the population position according to formula (8) ."
"this paper applies qbfo algorithm to ship collision avoidance and path optimization.first,the pso algorithm is applied to the bfo to optimized the bacterial tumbling direction and the new position for the next chemo taxis to improve the local convergence rate; second, the adaptive sliding step which deceases linearly is adopted to strengthen the convergence ability of local search; thirdly,the bacteria individual is described in the quantum space and a potential well model is created. using monte carlo method to achieve the reproduction of bacterial reproduction, and which makes the population are able to search the whole space. then,the objective function based on the shortest collision avoidance path is established to optimize the avoidance and steering angle. [cit] simulation interface, which displays the comparison parameters of collision avoidance and verifies directly and visually the feasibility and superiority of applying this algorithm to intelligent ship collision avoidance. this research lays a foundation for the next step of the implementation of a multi-ship collision avoidance process."
"chemotactic optimization operation. chemotactic movement of bacteria directly affects local convergence rate of the algorithm. by combining with the characteristic of cosine function that its slope changes slowly in the interval of (0, ) π, decreases substantially in the middle, and then tends to be gentle, cosine function is used to adjust the movement step length of bacteria. this meets the dynamic search rule of bacteria population more. bacteria distribute randomly at the initial stage of foraging and they are far away from the optimal solution. therefore, they need to search in the foraging region with a great step length. the function value of cosine function presents a decline trend in the interval of (0, )"
"based on bacterial foraging algorithm, this paper self-adaptively adjusts the movement step length in chemotactic operation and sets up a quantized potential energy well model in copy operation. copy operation is completed via monte carlo random sampling. in this way, the bacteria are able to search in the whole space and quantum bacterial foraging algorithm is obtained. based on this algorithm, by combining with colregs and relevant knowledge in ship safety field, the optimal ship collision avoidance path is acquired. besides, the dynamic collision avoidance process is gained through simulation experiment."
"where, the value of g d is determined by the zero boundary of the space risk index. based on the above analysis, in order to accelerate operation of the algorithm, ship collision avoidance path planning decision-making based on quantum bacterial foraging algorithm can be treated as encoding of 2 parametric variables. as for the individual bacterium i,"
step 4: execute the process of chemotactic operation circulation; execute flipping and movement operation for each bacterium i in the two-dimensional space to generate the random new position ( )
"the target ship and own ship enter the stage of navigation observation after judging the encounter situation between ships. the encounter situation of two ships is judged according to colregs, and then whether own ship is the give-way vessel will be determined. if own ship is the give-way vessel and collision risk exists between the target ship and own ship, the intelligent decision support system will execute collision avoidance path planning and recommend a safe and economic path, which can give an early warning and auxiliary decision, be used as a reference of collision avoidance scheme for sailor. and collision avoidance path planning for ships is divided into 3 stages according to different task stages."
"quantum bacteria foraging optimization algorithm. bacterial foraging (bfo) algorithm [cit] realizes powerful local search of the individual via continuous alternation of movement and flipping in chemotactic operation. therefore, the chemotactic velocity affects convergence precision of the algorithm. secondly, the scale of bacteria population is guaranteed through copy behaviors of bacteria. however, limitation of population will be caused easily and the algorithm might fall into local optimum. finally, it comes to migration operation. sudden changes might happen to local regions where individual bacterium lives, which might result in collective death of bacteria population in this region. perhaps they might migrate to a new local region according to the given migration probability, and the ability of global search will be enhanced. bacterial foraging algorithm completes optimization of individual bacterium based on nested loop of the above three steps. see fig. 3 for the realization process of bacteria foraging optimization algorithm."
"3) resumption stage: when decisions about resumption time and resumption action are made, the optimal resumption angle should be adopted, so as to complete the entire collision avoidance process."
rule 17 the stand-on vessel: the stand-on vessel may take action to avoid collision if it becomes clear that the give-way vessel is not taking appropriate action.
"2)the stage of collision avoidance sailing：taking into account specific character of safe ship control process, characterized by great course changes in range from 20° to 90°. furthermore, the change of course (avoiding angle)is better over 25° to make the targets ships to attention and reduction of speed is no more than 30% in order to meet requirements of rule 8b of colregs."
"besides, its stable slope change at the beginning meets the requirement of great step length at this stage. as a result, the bacteria will not center on the initial value and move around it within a certain chemotactic period due to the small step length, and local precocity phenomenon can be overcome. after foraging for some time, the bacteria population approaches the optimal solution, so the chemotactic operation needs to transform from great step length into fine search stage. this rule matches the characteristic of cosine function that its value decreases substantially in the middle. at the end of chemotactic operation in bacterial foraging, the bacteria population is almost near the optimal solution and the slope of cosine function tends to be gentle again. at this time, the bacteria population can further enhance the optimization precision by adopting a stable and small movement step length. thereby, the change rule of cosine function is added into movement step length, and the following formula is gained after improvement:"
"collision avoidance path which is produced by ship collision avoidance decision support system must ensure that the target ship sails beyond the own ship field .it is necessary to define the encounter situations of vessels to apply with colreg [cit] . when two ships approach at sea, there can be several situations, including head-on (f), crossing (a, b, e), and overtaking (c, d), as shown in figure 2 . rule 13 overtaking: an overtaking vessel must keep well clear of the vessel being overtaken. rule 14 head-on situations: when two power-driven vessels are meeting head-on both must alter course to starboard so that they pass on the port side of the other."
"in order to improve group information sharing mechanism in bacterial foraging algorithm and enhance global search performance of the algorithm, probability density function of individual bacterium under quantum space is established by utilizing shared information of bacteria at the copy stage, and position of different individual bacteria is updated through monte carlo random sampling. thus copy operation of the population is realized, and loss of population diversity caused by direct copy is avoided. in this way, quantum bacterial foraging optimization (qbfo) algorithm is formed. in qbfo, a quantized potential well is required to restrain individuals and endow the population with aggregation state. bacteria under quantum bound state can appear at any point in the space at a certain probability density, and the probability tends to be 0 when the distance of particles and the center is infinite. suppose in a d -dimensional search space, at iteration t, the position of individual i is: substituting it into quantum mechanics equation, wave function and probability density function of each dimension can be obtained:"
"finally, given the low overlap of member genes between individual ags, it is important to establish how ags-level biomarker panels would practically summarize gene-level information and organize the accompanying statistical framework. ways to compile and employ multi-platform agss, optimal fgs design, and construction of nea-based biomarker panels should therefore become the topics of future studies."
"the advantages of two-stage vision-based object recognition system are as follows: by using haar-like features, the first-stage classifier can detect efficiently candidate areas. unfortunately, the haar-like algorithm suffers from higher false positive rates (see the purple rectangles in figure 17 ). therefore, the second-stage pca-hog algorithm classifier was utilized to compensate for the higher false positive rates of the first-stage result. the detection results of the vision-based object recognition subsystem are shown as yellow rectangles in figure 18 . the results of the rainy-day experiment are shown as green rectangles in figure 19 . the detection results of the vision-based object recognition subsystem are shown as yellow rectangles in figure 18 . the results of the rainy-day experiment are shown as green rectangles in figure 19 ."
"the advantages of two-stage vision-based object recognition system are as follows: by using haar-like features, the first-stage classifier can detect efficiently candidate areas. unfortunately, the haar-like algorithm suffers from higher false positive rates (see the purple rectangles in figure 17 ). therefore, the second-stage pca-hog algorithm classifier was utilized to compensate for the higher false positive rates of the first-stage result."
"where ( ) is the estimation of distance, while denotes the object coordinates v of the image. the error and standard deviation of our proposed particle filter tracking algorithm and the internal algorithm of the radar sensor were compared to the ground truth to verify the tracking results. the error is defined as the absolute value of the estimated position from the algorithm and the ground truth. the average error is the sum of the errors divided by the number of times of detections. as shown in table 1, the proposed algorithm had better performance considering the average error, the maximum error, and the standard deviation of error of the longitudinal or lateral direction. in addition, the number of times the proposed algorithm effectively detected objects was also greater than that obtained by the sensor internal algorithm."
"due to the high sensitivity to light sources, the performance of camera sensor depends on the condition of light sources. for example, suffering in an insufficient light source, the vision-based systems cannot extract completely the features of objects at night. on the other hand, in rainy weather experiments, the raindrops adhering to the camera lens block the object in front of the vehicle. thus, the system cannot effectively identify the information of the target, leading to the failure of the image subsystem. therefore, the worst detection rates are achieved at night and on rainy days."
"the individual molecular phenotypes of cell lines and tumors were characterized with agss compiled using a number of alternative methods. the analysis provided a primary comparison of their relative performance butat the current stage -did not enable definite conclusions about performance of the different ags classes. indeed, ags of fixed size (top.n) versus variable size (significant) compared differently in the cell lines versus the tcga data (suppl. table 1 ). further in the analysis of consistency in vitro versus clinical results, these classes were almost equally represented (suppl. table 2 ). we have also seen differences between different filtering approaches in agss of classes significant.mini and significant.maxi (suppl. fig. 2) . therefore an issue to be investigated further is the comparative performance and robustness of different feature classes, platforms etc. importantly, multiple platforms' data can be integrated into combined agss. although in our analysis such agss did not perform much better than platform-specific ones (most likely due to the domination of transcriptomics data), a more detailed evaluation should be done, including new platforms from tcga and elsewhere, such as dna methylation, protein phosphorylation etc. given the diversity of carcinogenesis routes and the multiplicity of respective molecular mechanisms, combining platforms appears essential and most promising. incorporation of approaches from sparse linear regression modeling, gsea 26, spia 27, and paradigm 52 certainly represent promising ways in this direction. the statistical power of nea was obviously far from full. as an example, there were 13 drugs for which the numbers of tested cell lines and patients treated in tcga cohorts were sufficient for a significant estimation. for four drugs out of these 13, no reliable correlates could be found. one instructive example could be irinotecan, prescribed to 25 and 22 patients in coad and gbm cohorts, respectively. the interesting feature of irinotecan is that its pharmacokinetic pathway involves the same enzymes as that of gemcitabine (fig. 6b), namely ces1, ces2, cyp3a4, cyp3a5 and some others (https://en.wikipedia.org/wiki/irinotecan#interactive_ pathway_map) -although the enzymes here work in an opposite direction: they activate irinotecan rather than degrade as they do to gemcitabine. nonetheless, relevant gnea scores might have been informative for response to irinotecan. the patients' response was sufficiently differential, too: while all the irinotecan-treated patients relapsed, the time to relapse varied from 78 to 1265 days. however, we did not observe almost any sensible correlation of the pathway genes neither as gnea features nor as raw gene expression profiles. in regard of gnea, this elucidated a lack of network linkage between the agss of responders (or non-responders) to the irinotecan pathway."
"we also compared our results with existing related works. the comparison results are listed in table 6 . the parallel sensor fusion architecture proposed in this study exhibits the advantages of compensating for the disadvantages of relying on a single sensor. it improves the scene in case of subsystem failure and significantly increases the system detection rate and stability, as listed in table 4 . regardless of the weather conditions, better detection rates were achieved by the sensor fusion system than those obtained when relying on a single subsystem. table 5 lists the detection results of each system for the three object categories under different weather conditions. the sensor fusion system can achieve a detection rate of more than 90%. we also compared our results with existing related works. the comparison results are listed in table 6 ."
"the image captured by the camera can easily be affected by lighting and weather conditions. furthermore, the estimated distance of the front object derived from the camera image has a low precision. a sufficiently large velocity relative to the front object is necessary for the mmw radar to stably detect it. accordingly, these two sensor subsystems were combined in a parallel connection to compensate for the limitations of each sensor and improve the robustness of the detection system. the overall architecture of the proposed detection and recognition system is shown in figure 1 ."
"correlation between drug sensitivity and molecular features. in each of the four drug screens, we quantified correlation between the cell line sensitivity to each drug and each of the molecular features f according to a general model of the form:"
"the proposed parallel architecture system depends on the confidence index of each sensor. the system can compensate for each other's sensors and avoid the limitations of series fusion architecture. 3. three kinds of scenario conditions (daytime, nighttime, and rainy-day) were implemented in an urban environment to verify the proposed system's viability. the experiment results can provide the baseline of comparison for future research."
"selection is carried out in each time step. selection is carried out on single or multiple traits by threshold selection or truncation selection. threshold selection is based on the phenotypic observation(s) with associated threshold. truncation selection is based on one of the following criteria: phenotype, blup (based on phenotype and pedigree information), gblup, single-step gblup when only parts of plants are genotyped or phenotyped [cit] or bayesian models [cit] . the breeding values are estimated using dmu version 6 [cit], which is a package for analyzing multivariate mixed models, including prediction of breeding values."
"the goal of most plant breeding programs is to hybridize and select best elite lines or varieties with the best combination of desired characteristics, viz., yield-attributing traits, quality traits, and insect and pest resistance [cit] . the success of selection based on yield-related traits has been attributed to classical ps methods by which superior individuals are selected based on their individual phenotypic performance or combined index. besides, with decreasing costs of snp genotyping, mas was widely used in particular for biotic and abiotic stress resistant traits [cit] . however, the success of selection can either be limited with ps if the trait under selection has low heritability or with mas if the trait is govern by many quantitative loci (qtls) with small effects. the introduction of gs has provided the opportunity to overcome these limitations. gs refers to selection decisions based on gebvs, which are calculated as the sum of effects of dense genetic markers in ld with one or more qtls across the entire genome [cit] . the key advantages of integrating of gs in plant breeding decisions are an increase in genetic gain per breeding cycle and a reduction in the length of the breeding cycle [cit] . in addition to applying gs, there are many other factors, which also affect the genetic gain in a plantbreeding program. these factors include breeding objectives, experimental design (e.g., plot size and number of replicates per family), selection strategy (e.g., individual/family selection and recurrent selection) and biological aspects (e.g., mode of pollination, self-incompatibility, heritability and genotypeenvironment interactions). consideration of all these factors means that the number of alternative breeding programs can be numerous. the choice of breeding program is based on how plant breeders are able to test the consequences of selected alternatives. comparing alternative breeding strategies in largescale field experiments can be labor-intensive in terms of time and effort needed. instead of field experiments, simulation studies can serve as an efficient means to model different breeding strategies and predict their performance. moreover, simulation studies can provide us with a deep understanding on the impact of different factors on the genetic gain and other variables of interest (e.g., selection accuracy and genetic variance) of breeding programs, which facilitates the development and choice of better breeding programs. from a methodological point of view, quantifying the expected genetic gain of breeding programs can be done using either stochastic simulation or deterministic methods. in the cases where a number of factors need to be accounted for, however, it may be difficult to derive accurate deterministic methods. the advantage of stochastic simulation in this situation is that, it can be used to simulate an entire population of individual plants, so that one can mimic the actual artificial plant breeding programs in any detail desired. this enables stochastic simulation to be able to provide very precise prediction of consequence of alternatives. hence, a tool that is capable of simulating a large range of practical breeding programs with sufficient feasibility and flexibility needs to be developed."
"for breeding plan a, the current study simulated a 25-year commercial wheat-breeding program using phenotypic selection (figure 4) . the generation time is 1 year and a new breeding cycle started every year, so in total there were 25 breeding cycles initiated. each breeding cycle was initiated by selecting 20 parental lines and completed at generation 8 (f8) after seven generations of selfing. in the first seven cycles, the genome of the 20 parental lines were randomly sampled from the 988 stored genomes without replacement and these 20 parental lines were used for crossing. genotype-by-environment interaction was considered for the yield trait by taking f5 yield and f6/f7 yield as different traits. therefore, three traits were simulated: bvp, yp in f5 and yas in f6 and f7 (ya). the term \"preliminary yield trial\" means that the population is tested in one replicate in sparse field plots with reduced phenotyping. an important use of the yp, therefore is replication of seeds. the term \"advanced yield trials\" means that the population is tested when the amount of seeds for the population is sufficient to conduct multiple location trials in dense yield plots with possibilities for extensive phenotyping. the genetic variance of bvp, yp, and ya was set to 1 (standardized unit). heritability of bvp was 0.1, and the plot heritability for yp and ya were 0.2 and 0.3 given prior knowledge of analysis using real dataset [cit] . the current study investigated the consequence of the breeding program considering different correlations between the traits. negative correlations between the traits were not considered. four levels of correlation between yp and ya (0.1, 0.3, 0.5, and 0.7) and two levels of correlation between bvp and the other traits (0 or 0.1) were tested, resulting in eight scenarios. the selection was on different traits in different selection stages as follows:"
"the supervised learning algorithms was used to learn the relationship between the mmw radar coordinate and image coordinate system. before the coordinate transformation, the radar coordinates (x, y) and image coordinate (u, v) needed to be recorded synchronously to be considered as training samples for offline learning. an mmw radar uses electromagnetic waves as a medium, and it exhibits better reflective property to metal objects. hence, a triangular metal reflector was used as a target object to gather data obtained from the radar and the camera, as shown in figure 8 . a metal reflector"
"haplotypes were used as parental lines considering the real ld patterns in a commercial wheat-breeding program, and the schemes simulated were similar to current commercial breeding programs. therefore, these results indicate that combining speed breeding with gs is a very promising tool in plant breeding. when the speed breeding and gs are considered, the initial facilities investment such as a growth chamber with appropriate supplemental lighting and temperature control capabilities can be substantial . [cit] ) ."
"from the collected training samples, the longitudinal and lateral distances from the radar were considered as the input of the rbfnn, and the corresponding u coordinate of horizontal direction in the image was considered as an output. this network architecture allows for obtaining the coordinate conversion relationship between these two sensors. the network architecture is shown in figure 9 ."
"drug sensitivity models in tcga patients. we used the follow-up time profiles for which both status records \"relapse/relapse-free\" and \"dead/alive\" were available, which allowed creating \"relapse-free survival\" and \"overall survival\" variables. depending on the cancer aggressiveness and chemotherapy type, different timeframes could become informative in the analysis of the eight tcga cohorts. the follow-up timeframes were defined as 1/5 th, 1/2 nd, and full available (up to 18 years) intervals. for the analysis reported in \"statistical power to detect correlates of drug sensitivity\", we used 42 drugs which were applied to at least 10 patients in one of the eight cohorts. in fig. 3 we report fractions of adjusted p-values (fdr) from this analysis calculated by benjamini and hochberg. for the analysis of \"agreement between in vitro screen and clinical data\" we only considered 14 of the compounds, which were found in the in vitro sets. the p-values from this analysis were bonferroni-adjusted in the cross-comparisons between the in vitro and clinical results."
"alternative methods of pathway and/or enrichment analysis. we evaluated a number of existing multivariate, enrichment-based, and/or network analysis methods that could be potentially useful in the proposed analysis, accounting for their complexity, applicability to different experimental designs, and the ability to analyze individual samples rather than the whole cohort. various statistical algorithms have been proposed to quantify functional relevance of pathways and other gene sets by accounting for gene network topology."
"the two-stage vision-based object recognition system was similar to in our earlier work [cit] . in the first stage, the haar-like features algorithm was used to identify the candidate regions of object from foreground segmentation. the second stage is responsible for object recognition. three kinds of objects (i.e., pedestrians, motorcycles, and cars) can be identified by svm classifiers. the scheme of the two-stage vision-based object recognition process in shown in figure 6 . the object recognition results are shown in figure 7 ."
"the mmw radar signals are electromagnetic waves. both reflection and refraction will occur when the electromagnetic waves occur on the medium. in addition to the reflected wave from the medium itself, some noise signals of non-real objects are also prone to appear. the relationship between relative distance and echo intensity information was statistically analyzed using a vast amount of data collected during experiments. the statistical results are shown in figure 3 . the statistical results of the signal distribution indicate that both real objects and noise show respective concentrations, and only a small part of the distribution of both overlaps. accordingly, a noise filtering operation was performed. as shown in figure 3a, after the signal on the left side of red curve was filtered, the subsequent target tracking and particle filter algorithm were performed. densitybased spatial clustering of applications with noise (dbscan) algorithm [cit] was used to cluster the radar data, and the number of possible front objects was estimated."
"the radar detection subsystem uses mmw radar to perceive the environment ahead. [cit], 13, 116 13 of 18 non-object noise. the radar subsystem experiments tested three different categories of objects under different conditions. the detection results are shown as green circles in figures 15 and 16. the tests primarily involved a single target in a lane. if there were multiple targets, the alert was reported for closest target to the experimental vehicle. other targets continued to be tracked."
"after the weight of each particle is obtained, the relative position of the object detected by the mmw radar can be estimated. the expected value of the target estimation is expressed as follows:"
"in this work, we use acronym nea to refer to a specific approach for network enrichment analysis, which ascends to the idea of accounting for the node degrees of individual genes 22 . using that approach of significance estimation via comparing network connectivity to a null model, nea 23, 24 can characterize experimental and clinical samples with pathway scores by accounting for sample-specific gene set relationships in the global gene interaction network. the pathway-level output could be used in downstream analyses against arbitrary phenotype models. the ability to summarize rare alterations that cause the recurrent cancer phenotypes into pathway profiles provides higher statistical power, more information on the underlying biology, and robustness in phenotype prediction. however, neither nea nor alternative methods of pathway enrichment had been systematically applied to the task presented above: the discovery of biomarkers suitable for individual outcome prediction."
"agreement between in vitro screens and clinical data. a more challenging task was to identify a conservation of associated features between the in vitro drug screens and clinical application of the same drugs. any trustworthy setup of such an analysis would be very complex, so that even cross-validation and adjustment for multiple testing could not guarantee an unbiased probabilistic estimation. thus, the final judgment should be made after a biologically independent ad hoc validation from the in vitro to the clinical domain. even though the tcga collection did not provide correctly balanced, randomized cohorts for estimation of relative risks, error rates etc., our task was simplified by only needing to compare the methods' performance. in the eight largest tcga cohorts, we counted how many significant in vitro-detected features correlated with survival of patients who received same drug 30, (https://tcga-data.nci.nih.gov/docs/publications/tcga/; suppl. table 4) 41 . more specifically, molecular features of each class that significantly correlated with sensitivity to a drug in cell lines were required to also significantly correlate with patient survival in a tcga cohort.our survival analysis accounted for clinical covariates available from tcga (suppl . table 4), which facilitated the 'net' effect estimation."
"fgs. the functional gene sets, fgss, were ags counterparts in the analysis. the main collection of 328 fgs was based on the kegg pathways, the full collection of which was complemented with a number of separately published cancer pathways as well as specific go terms corresponding to cancer-relevant signaling or hallmarks of cancer (around 70 cancer-and signaling-related gene sets from reactome, gene ontology, wikipathways and literature). another approach was applied to enable compatibility with gsea and spia. these methods were designed and are most suitable for analyzing expression data and, apart from that, spia was applicable only to pathways with well characterized intra-pathway topology. we therefore employed a special set of 197 kegg pathways for which the topology was available in kgml files and tested on it spia, gsea, ora, and pwnea exclusively gene expression data (these results were separately labeled as ora.kegg, spia.kegg, ssgsea.kegg, zgsea.kegg, egsea.kegg, and pwnea.kegg). the analysis on the fgs collection is referred to as pathway-level nea (pwnea)."
"as shown above, the original gene profiles were poorly preserved across drug screens. therefore, we compared the ctd results with those from act screen in a more relevant multivariate approach using the \"elastic net\" method 39 . starting from all available features, each model was finally reduced to a much smaller subset. multivariate models are notoriously prone to over-fitting when the number of variables exceeds the number of samples. for this reason, validation on independent sets has become an essential requirement in such studies 40 . the ctd-based models were thus created using cell lines not found in the act screen. the comparison was also streamlined by using only the data from ccle affymetrix and point mutation datasets versus two respective figure 6 . clinical performance of nea features discovered in drug screens. each tcga cohort was split into four categories by two factors: administration of the specific drug (as \"treated/untreated\") and a threshold for predictive feature (pathway or individual gene score, indicated in the plot header). while the primary feature evaluation, we calculated the factor interaction p-values without binarizing the cohort by the nea score, i.e. in the continuous score space. then for the visualization the binary classifications by the both factors were applied (\"optimal threshold\" value for the quantitative nea feature). therefore both continuous and binary p-values are indicated the legends. the plots present differential survival upon treatment with topotecan in ovarian carcinoma (a,c), gemcitabine in lung adenocarcinoma (b), and vinorelbine in lung squamous cell carcinoma (d). feature ags classes mutations.mgs and significant.affymetrix_ccle. using other classes produced similar results (data not shown). figure 5 demonstrates that by applying the same parameters for elastic net training, in each case it was possible to obtain a descriptive model from ctd drug screen data with a number (4…36) of non-zero terms and then substantiate the model (possibly with a poorer performance) using the act data in a smaller cell line set. for each model, we compared observed and predicted drug sensitivity values. the most important observation was that in all instances the signs of these correlations were consistent between ctd model and act validation, i.e. negative correlations in the training set remained negative upon validation."
"(1) the genetic model, which is the method used to generate breeding values, is specified by the user. (2) a founder population is simulated if genomic model with ld between qtls and markers is used. this creates desired ld in the genome. this founder population is used as the basis for subsequent stages. (3) the selected population is generated. it initiates with a base population that is assumed unrelated based on the pedigree and followed by selection in subsequent generations. the user needs to specify the number of replicates of the selected population, selection strategy used for each selection stage and the type of propagation used for seed reproduction. if genomic model is used, the simulation of the selected population can initiate from the same or a unique founder population, depending on the choice made by the user. (4) the output variables are recorded and analyzed. not all output variables are required in any simulation. the user specifies what to output (e.g., genetic gain, genetic variance and accuracy of ebvs) in each time step. these results are also summarized when more than one replicate have been obtained. if desired all the data generated on the molecular as well as phenotypic level in each time step can also be saved for further analysis."
"the weight of each particle in the space region is normalized. the normalization method is based on dividing the weight of each particle by the sum of all particle weights, as shown by equation (6):"
"in the nighttime experiment, the scenarios included flashing brake lights of front vehicles, headlight reflections, and poor lighting environments, as shown in figure 13 ."
"in the nighttime experiment, the scenarios included flashing brake lights of front vehicles, headlight reflections, and poor lighting environments, as shown in figure 13 ."
"for the fusion architecture of series type, any single sensor failure causes whole system failure. the proposed parallel architecture system depends on the confidence index of each sensor. the system can compensate for each other's sensors and avoid the limitations of series fusion architecture."
"point mutations. ccle provided point mutation data on sequencing of 1667 genes in 904 cell lines. in addition, we downloaded cosmic data from exome sequencing of 1023 [cit] 9 gene symbols. mutation data from the both screens were used in the binary form, i.e. all specifying attributes were neglected."
where is the number of times the object tracked by particle filter. is a constant. the confidence index of the image subsystem was calculated as follows:
"three kinds of scenario conditions (daytime, nighttime, and rainy-day) were implemented to verify the proposed system. all the scenarios were carried out on urban roads. the mmw radar and camera were mounted on the front bumper of the experimental car, as shown in figure 11 ."
"this study proposed a sensor fusion technology integrating mmw radar and camera for front object detection. the proposed system consists of three subsystems, including a radar-based detection system, vision-based recognition system, and sensor fusion system."
"we finally decided to include in our testing, in parallel with pwnea (pathway level nea) and gnea (gene node level nea), the following methods: (1) using original gene profiles from respective omics platforms; (2) ora, over-representation analysis which was capable of working on exactly the same ags and fgs as pwnea; (3) gsea on full ranked gene lists, applying two alternative methods: a. ssgsea, ranking by absolute gene expression value, b. zgsea, ranking by deviation of gene expression from the cohort mean; (4) spia, measuring the pathway perturbation via known intra-pathway topology. (5) egsea, that combined existing enrichment methods, using five out of the total twelve ones, which were capable of producing individual sample scores."
"the rate of genetic gain after the burn-in stage was the highest in breeding plan c with spb (0.28-0.46), which was 2-2.5 times higher than breeding plan a with ps and was 7-20% higher than brreding plan b with gs ( table 1 ). the total genetic variance was the highest with ps and the lowest in gs. the genetic variance was not influenced by the genetic correlation. all the three breeding plans were replicated 60 times and performed on a linux server. the running time for each replicate was 2 h 15 ∼2 h 30 min for breeding plan a and b, and 6 h 40 min for breeding plan c."
"a technology that allows rapid generation advancement, called \"speed breeding, \" can be used to achieve 4-6 generations of wheat per year [cit] . speed breeding utilizes extended period of light to accelerate growth rate of a plant, which greatly shortens generation time and accelerates genetic improvement. adam-plant enables the simulation of speed breeding by allowing the user to specify the number of time steps or years each generation takes. for instance, in each cycle, the first 4 generations can be assumed to achieve within a year instead of 4 years (figure 3) ."
"we matched correlates of same data types in ccle and tcga (possibly obtained using different omics platforms, e.g. affymetrix microarray from ccle could be matched to rna-seq from tcga etc.). then we determined whether correlation p-values of individual features, in their turn, correlated between in vitro and tcga data, i.e. if genes or fgss with high (respectively low) correlation with drug response in vitro tended to correlate in the same manner with the patients' response. due to the testing of alternative ags classes, respective numbers of matching pairs in ora, pwnea, or gnea were an order of magnitude higher than in raw data (column 2 in table 3 ). therefore we coupled this calculation with a significance test by randomly permuting feature and sample labels. altogether, the permutation tests indicated that point mutation and copy number data had zero true discovery rates (tdr), i.e. their correlation p-values were preserved not more than expected by chance (see column 3 in table 3 ). on the contrary, the tdr levels were substantial (0.02…0.805) for gene expression data and for agss processed with each of the enrichment analyses."
"in our view, the advantages of our nea approach are due to the following features of network-based data interpretation: (1) combining major types of molecular interactions in a biologically relevant way, (2) summarizing seemingly disparate molecular alterations at the level of pathways and processes, and (3) enabling lower-dimensional statistical analysis. in addition, a network context, with different types of evidence behind the edges provides better grounds for biological interpretation 36, 50, 51 . the poor performance of the individual gene analysis and alternative enrichment methods could be explained by the excessive dimensionality of the former and poorer sensitivity of the latter. in addition, the ability to use smaller and hence more specific ags could have provided extra advantage of nea over ora and gsea. on the other hand, nea could also deteriorate on ags of insufficient size when using sparser networks (around 10 4 …10 5 edges) and networks with many missing nodes. these potential limitations were established earlier 36 and we tried to avoid them in the present work by using e.g. a denser network from data integration. we admit that a future, more comprehensive version of nea might adopt advantages of the alternative enrichment methods by employing full gene lists (as in gsea) and intra-pathway topology (as in spia). indeed, at certain steps of our analysis these methods demonstrated performance comparable to that of nea."
"step what was evaluated measure scheme figure 1 statistical power to detect correlation between omics-based features and sensitivity to anticancer drugs testing the multiple alternative methods implied different input, processing and output ( fig. 1, table 1 ). accordingly, our data analysis procedure included the method-specific steps for sample/patient characterization, enrichment analysis, and phenotype modeling. in order to maximally adapt gsea to our applications, we tested two different ways of ranking gene lists, ssgsea and zgsea (methods) as well as two options of egsea, a method that combined multiple existing algorithms. in sections 3…5 of results, we report the outcome of systematic analyses of the experimental datasets under these alternatives ( fig. 1f and table 2) ."
"there are four options for seed propagation: cloning, crossing, selfing, and dhs. different types of seed propagation can be used in different propagation stages. crossing can be performed either within families, across families or in the population. when crossing is used, the user needs to specify the maximum number of crosses where a plant can be used for crossing. double-haploids are created by allowing recombination of the two haplotypes of an individual, randomly sampling one of the recombined gametes and then doubling this gamete to create a new diploid individual."
"adam-plant allows simulation of multiple traits. taking finite-locus model as an example, the correlated traits are characterized by the same set of qtls, although some of these qtls may have near-zero effect on one or more traits. the qtl effects on multiple traits are sampled from a standard multivariate normal distribution with a user-specified matrix of additive genetic variance and covariance between the traits."
"most of the consistent nea features were obtained for ags based on gene expression data (suppl. table 2 ). they were identified for docetaxel, gemcitabine, and paclitaxel in brca (see the cancer cohort notation in table 3 ); for dexamethasone, erlotinib, and topotecan in gbm; for gemcitabine in luad; and for gemcitabine, paclitaxel, tamoxifen, and topotecan in ov. while using gene copy number data, consistent pwnea and gnea features were found only for gbm (dexamethasone and topotecan). consistent features that correlated with the response to cisplatin (lusc) belonged to the combined, multi-platform types. one gnea feature was based on somatic mutation analysis (gemcitabine in lusc), although it did not match all the criteria. below we present four promising findings predictive of survival in a tcga cohort (fig. 6) ."
"in this analysis at act (\"advanced cancer therapies\") centre, we used 20 cancer cell lines for which molecular data could be found in the ccle affymetrix set as well as in both ccle and cosmic point mutation sets: a375, hct116, hdlm2, ht29, jvm2, k562, l428, mcf7, mdamb231, mv411, nb4, pl21, raji, rko, sjsa1, skbr3, sknas, sw480, t47d, and u2os. eight of these cell lines had also been included in the ctd screen (a375, hct116, ht29, mcf7, pl21, rko, sw480, and u2os). in order to avoid overlap in the multivariate models, we excluded these eight cell lines while training the original models from the ctd data and only used them in the validation set."
"using gsea and spia was restricted to only transcriptomics data. spia, in addition, could only be run on pathways with known topology, which limited the set of available fgs to 197 kegg pathways available in kgml format. this created an additional, specific line of testing on a limited collection of input data and fgss for the methods ora, ssgsea, zgsea, spia, and pwnea (see figs 3,4 and table 3 )."
"if a single sensor in the sensor fusion of cascade architecture fails, then the entire system will inevitably fail. meanwhile, the sensor fusion of parallel architecture determines which sensor should be trusted based on the decision mechanism. although one of the sensors might not detect an object or gives a false alarm, if the other sensor correctly detects the object, then the confidence index of each sensor can be calculated via a scoring mechanism, and a credible subsystem can be determined based on the confidence index."
"in the nighttime experiment, the scenarios included flashing brake lights of front vehicles, headlight reflections, and poor lighting environments, as shown in figure 13 ."
"to cover all the potential object positions, n pieces of particles were randomly distributed within the radar detection area. each particle represents a potential position of a real object, where the weight of the particle indicates the probability that the object is at this location."
"the main principle of nea (fig. 1d ) can be understood via comparison to the simplest method for detecting enrichment called overrepresentation analysis (ora) 31 ( fig. 1a) . first, an experimental or clinical sample should be characterized by a set of altered genes (ags), such as top ranking differentially expressed genes, or a set of somatic mutations, or a combination of these. the second component of the analysis is a collection of functional gene sets (fgs): pathways, ontology terms, or custom sets of biological importance. enrichment scores of the agss can thus be used as the samples' coordinates in a lower-dimensional fgs space. in both ora and nea significance can be evaluated with appropriate statistical tests. in ora, enrichment is measured by the number of genes shared between the fgs and ags, normalized by the gene set sizes. since nea considers a third, network component -via counting network edges that connect any genes of ags with any genes of fgs -it includes normalization by topological properties of the network nodes. due to the presence of different interaction mechanisms in the global network, nea does not expect fgs genes to be altered themselves and therefore is capable of detecting enrichment of e.g. transcriptomics-based ags in a pathway that operates by other mechanisms, such as trans-membrane signaling, phosphorylation etc. nea holds other key advantages, such as exceptionally high power to detect enrichment in a global network, given the latter is sufficiently dense. hence, even smaller gene sets often connect to each other by multiple edges so that even an individual key network node can even appear as an ultimately reduced fgs. this gene-level network analysis, gnea (fig. 1e ) provides a more focused alternative to the default analysis at the pathway level, pwnea (fig. 1d ) and we therefore separately evaluated performance of pwnea and gnea in the present work. table 1 . characteristic features of alternative input data types."
"from the collected training samples, the longitudinal and lateral distances from the radar were considered as the input of the rbfnn, and the corresponding u coordinate of horizontal direction in the image was considered as an output. this network architecture allows for obtaining the coordinate conversion relationship between these two sensors. the network architecture is shown in figure 9 ."
"three kinds of scenario conditions (daytime, nighttime, and rainy-day) were implemented in an urban environment to verify the proposed system's viability. the experiment results can provide the baseline of comparison for future research."
"the position of the object measured by the laser range finder is considered as the ground truth, which is illustrated by the blue line seen in figure 5 . the red line represents the tracking result obtained by the proposed particle filtering algorithm. the result of the internal algorithm of the radar sensor is illustrated by the green line. an offset between the detected and actual positions of the object may be observed owing to the characteristics of the radar sensor. the error and standard deviation of our proposed particle filter tracking algorithm and the internal algorithm of the radar sensor were compared to the ground truth to verify the tracking results. the error is defined as the absolute value of the estimated position from the algorithm and the ground truth. the average error is the sum of the errors divided by the number of times of detections. as shown in table 1, the proposed algorithm had better performance considering the average error, the maximum error, and the standard deviation of error of the longitudinal or lateral direction. in addition, the number of times the proposed algorithm effectively detected objects was also greater than that obtained by the sensor internal algorithm."
"in order to reproduce the actual road conditions, we designed a rainy-day scenario too. as the sensors are mounted on the front bumper, the raindrops often adhered to the camera lens during the rainy day experiment, as shown in figure 14 ."
"where ∆ x and ∆ y are the block sizes, i k is echo strength of the mmw radar, σ is the blurring degree of the sensor, and the weight value of the particle can be obtained by the following equation:"
"a single sensor system can operate independently; however, a parallel architecture was adopted in this study to fuse two different sensors. the main purpose of this is to improve the detection rate that can be achieved by a single sensor. the sensor fusion was divided into three parts. first, the two-dimensional coordinate information of the mmw radar was converted into the coordinate of the image. afterwards, the information obtained by the two sensors was integrated into the same coordinate system. next, the object information needed to be matched to determine whether the same object information had been obtained by both the mmw radar and camera, and to integrate the detection results of the two systems. finally, the trusted sensor was determined based on the confidence index of the sensor."
"gene copy number. ccle, cgp, and ctd all employed affymetrix snp 6.0 microarrays for gene copy number detection. we downloaded the ccle dataset 12 for 994 cell lines. in addition, we downloaded cosmic data 37 independently produced by the same platform and then post-processed in three different ways to provide total, absolute copy number per gene, number of copies of the minor allele, and a binary classification of gene copy number values into \"gain\" vs. \"loss\". all datasets were used as downloaded, without further processing or normalization."
"when a genomic model is used, it is necessary to simulate founder population or read in individual-level genome-wide snp/sequence data either collected from breeding programs under investigation or simulated. this step is skipped when an infinitesimal model is used."
"the radar detection subsystem uses mmw radar to perceive the environment ahead. the proposed multi-object tracking algorithm with a particle filter can effectively track the objects in front and remove non-object noise. the radar subsystem experiments tested three different categories of objects under different conditions. the detection results are shown as green circles in figures 15 and 16. the tests primarily involved a single target in a lane. if there were multiple targets, the alert was reported for closest target to the experimental vehicle. other targets continued to be tracked. a detection rate exceeding 60% was maintained by the radar detection system during daytime, nighttime, and rainy days. the experimental tests performed under different weather conditions verified that the radar detection system is not affected by weather conditions. the experimental results are listed in table 2 ."
"hl and as developed the software. hl performed the analyses and prepared the manuscript. bt, as, jj, fc, and ja helped design the programs, participated in interpretation of results and revision of the manuscript. all authors read and approved the final manuscript."
"in the nighttime experiment, the scenarios included flashing brake lights of front vehicles, headlight reflections, and poor lighting environments, as shown in figure 13 . considering the effect of pavement puddles and shadow environment, the daytime scenarios included direct sunlight, pavement puddles, and shadow environments, as shown in figure 12 ."
"applying this approach to the in vitro drug screen data, we evaluated all features of different types and classes. respectively in tcga data -again using all available features -we measured correlations of features with survival of patients who received one of the 42 frequently used drugs in any of the eight cohorts. we systematically and uniformly compared different feature types, i.e. original data from high-throughput platforms and nea scores as well as classes within the types (e.g. transcriptomics data from affymetrix vs. agilent vs. rna sequencing). each case was tested on both relapse-free and overall survival and three follow-up intervals. we also analyzed the relative performance of different ags classes (suppl. fig. 3 )."
"two types of sensors, an mmw radar and a camera were integrated in this study to develop a frontal object detection system based on sensor fusion using parallel architecture. a particle filter algorithm was employed by the radar detection subsystem to remove noise from non-objects while tracking objects at the same time, and converting the target information into the image coordinates using rbfnn. on the other hand, the image object could be identified as one of three main categories (pedestrians, motorcycles, and cars) by the two-stage vision-based recognition subsystem. the information obtained by the two subsystems was integrated. the sensor with higher credibility was selected as the system output result. three kinds of experiments (daytime, nighttime, and rainy-days) were performed to verify the proposed system. the experiment results show the detection rates and the false alarm rates of proposed system were approximately 90.5% and 0.6%, respectively. these detection rates are better than those obtained by single sensor systems."
"in the other version of our analysis, called gene-wise nea (gnea), [cit] 7 network nodes, regardless of their pathway or go annotation, as a single-gene fgs."
"network enrichment analysis (nea, pwnea, and gnea). network. the network was based on the funcoup method 50 with consecutive merging of five more resources as described and benchmarked previously 36 . the results of that benchmark indicated that funcoup was superior to string (a method similar to funcoup in terms of scale and the size of input data collection 70 ), mostly due to the latter broadly using prokaryotic evidence and therefore less specific in cancer-related analyses. the second conclusion from the benchmark was that adding to the funcoup network edges of curated databases significantly improved its performance. we therefore added the funcoup-based network with functional links from kegg 71, corum 72, and phosphosite 73, msigdb transcription factor-related part 74 ), and an own reverse-engineered network 36 . the resulting network thus combined a wide range of molecular mechanisms, functional relations, and metrics from high-throughput data sets: physical protein-protein interactions, membership in same protein complex, membership in the same pathway, correlation of mrna profiles, correlation of protein abundance values, protein phosphorylation, coherence of go annotations, concordance of upstream regulators (transcription factors and mirnas), co-localization in same sub-cellular compartments, similarity of phylogenetic profiles etc. it contained 974,427 edges (links) [cit] 7 nodes (distinct human gene symbols)."
"the advantages of two-stage vision-based object recognition system are as follows: by using haar-like features, the first-stage classifier can detect efficiently candidate areas. unfortunately, the haar-like algorithm suffers from higher false positive rates (see the purple rectangles in figure 17 ). therefore, the second-stage pca-hog algorithm classifier was utilized to compensate for the higher false positive rates of the first-stage result. the detection results of the vision-based object recognition subsystem are shown as yellow rectangles in figure 18 . the results of the rainy-day experiment are shown as green rectangles in figure 19 . all the experiments performed under different weather conditions involved three classifications of objects: pedestrians, motorcycles, and cars. the detection results of vision-based systems are listed in table 3 . all the experiments performed under different weather conditions involved three classifications of objects: pedestrians, motorcycles, and cars. the detection results of vision-based systems are listed in table 3 ."
"a 24 ghz short-range radar was adopted for front-end environment detection and a multi-object tracking method based on radar was proposed. this method can facilitate tracking multiple object simultaneously and removing noises, which were considered as non-real objects. the flow chart of the proposed radar-based detection subsystem is shown in figure 2 . first, the radar data were divided into different clusters using a clustering algorithm. the particle filter is then used for signal filtering and target tracking. two kinds of probability scores will be evaluated in the particle filter process. the convergence of the particle swarm can reflect the quality of the tracking. for the stable tracking objects, the particles around the object have a higher weighting in the importance sampling step. furthermore, these particles have a higher probability of survival in the resampling step. we define the range probability ( ) as the survival probability of the particles within a radius of 1 m around the object to evaluate the quality of the tracking. on the other hand, the diversity of the particle swarm can cover of all the states of the object. we defined the available probability ( ) as the survival probability of the particles after the resampling step. during the tracking process, in line with the value of, the system adjusts the particle percentage of resampling to ensure the diversity of the particle swarm. in addition, the confidence index of the target object was derived from the range probability and probability of survival. this confidence index determines the credibility of the actual object. the relative velocity and distance between the vehicle and front object were provided by this subsystem."
"in order to reproduce the actual road conditions, we designed a rainy-day scenario too. as the sensors are mounted on the front bumper, the raindrops often adhered to the camera lens during the rainy day experiment, as shown in figure 14 . considering the effect of pavement puddles and shadow environment, the daytime scenarios included direct sunlight, pavement puddles, and shadow environments, as shown in figure 12 ."
"adam-plant also allows users to carry out optimumcontribution selection (ocs), which maximizes long-term genetic gain while constraining inbreeding by constraining the relationship among selected parents [cit] . optimum contribution selection allocates matings to selection candidates at time t by maximizing the function u t :â"
"the mmw radar detection and image recognition systems operate independently, and the two systems obtain information about the detected objects, respectively. to fuse the information of the two systems, the object information must be matched first to determine whether the same object information has been detected by the two sensors. coordinates shown in the same image may correspond to several different radar coordinate information, as illustrated by the green points shown in figure 10 . in addition, the distance estimated from the image coordinates may be inaccurate owing to the bumpy road surfaces that can cause the vehicle to shake; thus, it is difficult to match the object information and effectively determine whether the same object is detected."
"12 analyzed cell line sensitivity to 24 drugs in 504 cell lines. these authors considered a range of numeric sensitivity metrics for their analysis and finally preferred 'normalized activity areas' . these original units were calculated as areas under compound response curves where higher values corresponded to higher sensitivity so that 0 stood for 'insensitive to compound' and 8 corresponded to 'full sensitivity' . further, the activity area values were normalized for unequal luminescence in the assay. we rendered them normally distributed by log-transformation. thus the values in our analysis range from −3.00 meaning 'insensitive to compound' to +2.31 meaning 'maximal sensitivity' ."
"adding omics data to clinical variables has demonstrated the potential for prediction of cancer disease outcome in a dream challenge 19 . one particularly winning strategy was to employ multigenic expression patterns. such 'meta-genes' 20 were, despite the seemingly 'network-free' definition, nothing other than modules in a co-expression network, which allowed dimensionality reduction and a biological generalization. another dream project revealed efficiency of summarizing gene expression in cancer cell lines over pathways 21 . further, identifying patient sub-categories responsive to a treatment is more challenging than one-dimensional drug sensitivity or survival analyses. a practical method should profile individuals across the cohort, so that the profiles can be fit to clinical variables and covariates. therefore, a crucial feature for biomarker discovery would be the ability to assign scores to individual samples rather than to derive feature-pathway associations from the whole data collection. in addition, further sample classification in a flow of new patients should not require re-running the analysis on the whole cohort, i.e. recalculating the data space, as is often the case."
"after the weight of each particle is obtained, the relative position of the object detected by the mmw radar can be estimated. the expected value of the target estimation is expressed as follows:"
"similarly to an earlier presented comparison between ccle and cgp screens 11, we found that the association values between drug sensitivity and original features only weakly agreed between the drug screens."
"a lot of object information was lost while the mmw radar information was processed by internal algorithms. therefore, the original unprocessed data was obtained from the mmw radar in this study. the proposed particle filter algorithm was used to track the front object and address the issue of losing too much information."
"following the same approach, we employed tcga data on somatic point mutations reported in maf files. the column 'variant_classification' contained a number (more than 15) different codes, most frequent being missense_mutation, nonsense_mutation, and silent. the latter constituted around 25% of the total number of somatic mutations reported in the eight cancers, while around half of such cases contained only mutations reported as silent. this fraction would not significantly affect the false positive and true discovery rates in any enrichment analysis. furthermore, we found that in each cohort tens to hundreds of most frequently mutated genes (e.g. top 5% ranked by frequency per base pair length) had a significant rate of purely silent mutations (see supplementary file nmut_vs_frequency.4cohorts.pdf). discarding such would exclude many potential cancer drivers -although of yet unknown mechanisms. we therefore included all mutation records present in the maf files."
"a 24 ghz short-range radar was adopted for front-end environment detection and a multi-object tracking method based on radar was proposed. this method can facilitate tracking multiple object simultaneously and removing noises, which were considered as non-real objects. the flow chart of the proposed radar-based detection subsystem is shown in figure 2 . first, the radar data were divided into different clusters using a clustering algorithm. the particle filter is then used for signal filtering and target tracking. two kinds of probability scores will be evaluated in the particle filter process. the convergence of the particle swarm can reflect the quality of the tracking. for the stable tracking objects, the particles around the object have a higher weighting in the importance sampling step. furthermore, these particles have a higher probability of survival in the resampling step. we define the range probability (p r ) as the survival probability of the particles within a radius of 1 m around the object to evaluate the quality of the tracking. on the other hand, the diversity of the particle swarm can cover of all the states of the object. we defined the available probability (p a ) as the survival probability of the particles after the resampling step. during the tracking process, in line with the value of p a, the system adjusts the particle percentage of resampling to ensure the diversity of the particle swarm. in addition, the confidence index of the target object was derived from the range probability and probability of survival. this confidence index determines the credibility of the actual object. the relative velocity and distance between the vehicle and front object were provided by this subsystem."
"in order to analyze data from the in vitro cancer cell screens and the primary tumor samples in the same manner, we constructed agss by following the same platform-specific approaches. intuitively, having an ags that is too big or too small could deteriorate specificity or sensitivity of nea. therefore, in order to prove that differences are not due to selecting ags genes in a specific way, we tested and compared a number of options for ags compilation. mutation-based agss were created by first listing all point-mutated genes in each given sample (which might include hundreds and even thousands of passenger mutations) and then retaining only those with significant network enrichment against the rest of the set. this approach 36 had been proposed for distinguishing between driver and passenger mutations, so that the filtering should enrich agss in cancer driver genes. next, agss from gene copy number and expression data included genes most deviating from the cohort means. this was achieved by using one of the three alternative algorithms (see methods). again, even such deviant gene sets could still be too large, e.g. due to listing copy number-alterations over extended chromosomal regions. in order to compact these, alternative ags versions were derived by retaining only genes with significant network enrichment for signaling and cancer pathways or for the mutation-based ags of the same sample, which reduced the ags lists 3-10 fold. finally and as an extra option, we tested combined agss, produced by concatenation of the platform-specific agss."
"(1) parental lines p0: for the first seven cycles, 20 parental lines (p0) were randomly sampled from the founder population. for the remaining 18 cycles, the 20 p0 were sampled from the genotypes of selected f5, f6, and f7 in the previous cycles, assuming the germplasm of all the selected individual plants at these three generations were available. the 20 parental lines were allowed to be randomly crossed with each other, and the maximum times of crosses that one individual could be used were four. in total, a smaller subset of 50 out of possible 190 crosses were kept to produce f1. (2) f1: in total, 50 f1 were generated. in reality, a single cross can actually produce a number of f1 individuals that are genetically identical. therefore, for simplicity, only a single f1 individual was simulated for each cross. these 50 f1 were self-pollinated to produce 10 seeds (f2) each. from f2 to f4, the families were derived by selfing from their common ancestors at f1 generation and were denoted as f1-families. (3) f2: within family selection was conducted on f2. within each of the 50 f1-families, out of 10 f2, the five highestranking individuals were selected based on bvp. the selfpollination of each selected f2 produced 30 seeds, resulting in 7500 f3 seeds in total. (4) f3: entire family selection was conducted based on the yp performance of f3 individuals in each f1-families. each f1-family was assumed planted in three plots, and therefore, three replicates were simulated. in total, out of 50 families, 45 families with highest-ranking yp were selected and were self-pollinated to produce 60 seeds (f4) per f1 family (20 f4 per plot). (5) f4: within family selection was conducted for f4. within each of the 45 f1-families, five highest-ranking individuals were selected based on bvp. so 225 f4 were selected in total. from now on, these f4 were used to create lines of single seed descent. each f4 was self-pollinated to generate 20 seeds, resulting in 4500 f5. from f5 to f8, the selection were all based on the f4-lines. (6) f5: line selection was conducted based on the yp performance of all f5 individuals in each f4-line. only one replicate was simulated for each f4-line. in total, 75 out of 225 lines with highest yp were selected and were selfpollinated to produce 900 f6 per line (100 f6 per plot). the germplasm of the 75 selected lines were stored and potentially become parental lines for the next cycles. (7) f6: line selection was conducted based on the total ya performance of all f6 individuals in each f4-line. nine replicates were simulated for each f4-line, which means that each f6 line was grown in 9 plots. in total, 30 out of 75 lines with the highest ya were selected and were selfpollinated to produce 900 f7 per line (100 f7 per plot). the germplasm of the 30 selected lines were stored and they could potentially become parental lines for the next cycles. (8) f7: line selection was conducted based on the total ya performance of all f7 individuals in each f4-line. nine replicates were simulated for each f4-line, which means that each f7 line was grown in 9 plots. in total, 5 out of 30 lines with the highest ya were selected and were self-pollinated to produce 900 f8 per line (100 f8 per plot). the germplasm of the five selected lines were stored and could potentially become parental lines for the next cycles. figure 5 shows the average breeding value of ya across the cycles every year and the average breeding value of ya at f8 every year for different scenarios. the results show that higher correlation (positive) between the traits result in higher genetic gain."
where a rn is the number of times the object tracked by particle filter. η r is a constant. the confidence index of the image subsystem was calculated as follows:
"29 mainly used areas under curve (auc) for their quantitative analysis of 203 drugs in 242 cell lines. we reproduced this approach in our study. in completely insensitive cases, the full area under eight experimental points reached 8, whereas 0 stood for full sensitivity. thus, the scale of this screen was inverted compared to the other screens, which was considered in all calculations."
"a log file is written and updated as the program is running. the log files show the detailed information of simulation process (e.g., the selection and mating strategy, number of selection candidates and the number of seeds produced) at each time step."
"the user decides what genetic variables are saved. not all outputs are required in any simulation. when all the replicates are complete, the mean and the standard deviation of all the genetic variables are also written. plots showing development in means and standard deviations are written to pdf files as well."
"the ensemble of genes set enrichment analyses 28 can combine results from twelve individual algorithms which were previously presented by third parties. egsea then calculates collective gene set scores for unified enrichment estimation. since the framework of our analysis required assigning enrichment scores to individual samples rather than to multi-sample contrasts such as \"experiment vs. control\", we excluded methods that were present in egsea but only applicable to multi-sample contrasts, namely camera, roast, fry, padog, cage, safe, and globaltest. we used the remaining suitable methods ora, zscore, gsva, plage, and ssgsea. the correlation analysis and other models in our work required input in the form of enrichment scores rather than p-values. therefore we could combine scores with two egsea methods which were not based on p-values, namely min rank and average rank."
"in recent years, the development of advanced driving assist systems (adas) has attracted a large amount of research and funds from major car factories and universities. the key issues of adas include on road object detection, anti-collision technology, park assist system, etc. three kinds of sensors (i.e., radar, lidar, and camera) are widely adopted for object detection in front of vehicles [cit] . since there are limitations of single sensors, multi-sensor fusion technology can be used to compensate for the disadvantages of each single sensor [cit] ."
"if a single sensor in the sensor fusion of cascade architecture fails, then the entire system will inevitably fail. meanwhile, the sensor fusion of parallel architecture determines which sensor should be trusted based on the decision mechanism. although one of the sensors might not detect an object or gives a false alarm, if the other sensor correctly detects the object, then the confidence index of each sensor can be calculated via a scoring mechanism, and a credible subsystem can be determined based on the confidence index."
"to verify the feasibility of the algorithm proposed in this study, a laser range finder with high precision was used. the measurement error of the adopted lase finder was ±10 mm to record the center position of the frontal object. the experimental equipment installed to verify the radar tracking system is shown in figure 4 . three verification conditions were set to avoid dark objects and lack of relative speeds, which can lead to losing laser range finder and radar information, as follows: metal and light-colored moving objects, a relative velocity of ±15 km/h or more, and objects moving from far away to nearby."
"another rbfnn is used to match the object information and determine whether the same objects are detected by the two sensors. six factors were entered as the network inputs, which affect the object match, including image coordinate u, object width, object height, object distance estimated from image, object distance measured by the radar, and the u coordinate converted from the radar to the image. either \"match\" or \"non-match\" were obtained as the network output. the camera was installed at an angle parallel to the horizon. when the target object moved from far away to nearby, the position of its center point slightly changed near the center point of the image in the vertical direction. thus, the variation in the image v-direction coordinate was not obvious. therefore, the fusion system primarily enabled the neural network to learn the relationship between the mmw radar coordinate (x, y) and the image coordinate (u, v)."
"the method of estimating according to the weight of each particle is referred to as the sequential importance sampling (sis) particle filter [cit] . however, this method involves particle degradation, leading to insignificant weight values of most particles after several iterative operations. this triggers the system to perform unnecessary calculations on these particles. thus, the real target position may not be covered by the remaining particles. the resampling method was used to address this issue. in each iteration process, the particles with smaller weight values were discarded and replaced by particles with larger weight values. after resampling, the weight values of all particles was set at 1 n, then the next iteration was performed with new particles. the expected value of the target estimation is expressed as follows:"
"assay for cell proliferation used in act screen. cell proliferation was estimated with the wst-1 assay (water soluble tetrazolium). briefly, cells were incubated with each drug for 72 hours in a 96-well plate. at the end of this period, they were incubated with wst-1 reagent (roche) for 2 hours. absorbance at 450 nm was measured following the instructions from the manufacturer. the cell proliferation rate compared to that in the control was calculated."
"overall, the performance of the original profile models on the validation sets appeared comparable to that of pwnea. however importantly, the former had much more freedom in model term selection since the initial feature space was around two orders of magnitude larger than that in pwnea. consequently, despite the rigorous cross-validation and feature selection implemented in the glmnet algorithm, using the original profiles generated more complex models (see the number of terms per model, n) which fit the training sets better. at the validation step however, the performance of the original data models significantly worsened -whereas the pwnea-based models performed at the same level (all results obtained under variable parameters can be found in supplementary files glmnetmodels.basu_vs_new.raw.pdf and glmnetmodels.basu_vs_new.pwnea.pdf). this result essentially corroborated the previous conclusion about higher robustness conferred by nea, compared to the usage of original gene profiles."
"the user needs to specify at which generations and which selection stages the phenotypic record of each trait is realized. the user also needs to specify the number of observations, which represents the number of replicates recorded for the trait(s) of interest and number of plots grown per family in a user-specific selection stage. for instance, if there are three observations for a trait, then there are three plots for each family."
"adam-plant also enables simulation of breeding programs in competing companies, which use a proportion of parental lines from the other companies. (2) it allows simulation of speed breeding with flexibilities in defining generation intervals in order to test the effectiveness of selection in early generations and to quantify genetic progress and genetic variance using different designs for speed breeding. it is also possible to test the potential for integrating speed breeding with gs in accelerating the rate of genetic improvement in simulated crop breeding programs. (3) it allows storing the advanced germplasm in any generation and these germplasm can be used for later cycles. this function makes it more flexible for selection of parental lines. (4) there are more options for phenotyping, genotyping strategies, selection and mating strategies. for instance, optimum contribution selection or minimum co-ancestry mating can be carried out for cross-pollinated crops such as maize in order to constrain inbreeding while ensuring high genetic progress by managing the distribution of genetic contributions to the selected candidates. (5) it allows different units for selection. the selection unit can be population, within family, or entire family, in which the family can be defined with great flexibility e.g., a set of parents or a set of plants used in a poly-cross to create synthetics) in an earlier generation. (6) adam-plant allows great flexibility in mating strategies as it allows crossing of individuals that are in different generations or in different selection units. for instance, adam-plant allows backcrossing by storing the germplasm of one of the parent and crossing this parent to its offspring in the generation where selection is performed, it allows three way cross by crossing an inbred line to an f1, it allows crossing of two single crosses that come from four separate inbred parents and it allows crossing between the individual plants within a pre-defined unit i.e., full-sib cross or half-sib cross. for the three examples presented, breeding plan a represented a traditional breeding program in wheat. breeding plan b represented a modern breeding program, which is a combination of conventional breeding techniques and genomic tools leading to a new genomics-based plant breeding. breeding plan c represented a new technology for rapid generation advance named \"speed breeding, \" which has been successfully deployed in bread wheat [cit] ."
"in order to reproduce the actual road conditions, we designed a rainy-day scenario too. as the sensors are mounted on the front bumper, the raindrops often adhered to the camera lens during the rainy day experiment, as shown in figure 14 . considering the effect of pavement puddles and shadow environment, the daytime scenarios included direct sunlight, pavement puddles, and shadow environments, as shown in figure 12 ."
"for breeding plan c, a 25-year speed-breeding program for wheat using gs was simulated. the breeding program was similar to breeding plan b with the exception that the first 4 generations (f1-f4) were achieved within 1 year instead of 4 years. the comparison of genetic gain from breeding plan c with that obtained from breeding plan a and b is given in figure 8 . the results showed that speed-breeding program could markedly accelerate and increase the genetic gain."
"the method of estimating according to the weight of each particle is referred to as the sequential importance sampling (sis) particle filter [cit] . however, this method involves particle degradation, leading to insignificant weight values of most particles after several iterative operations. this triggers the system to perform unnecessary calculations on these particles. thus, the real target position may not be covered by the remaining particles. the resampling method was used to address this issue. in each iteration process, the particles with smaller weight values were discarded and replaced by particles with larger weight values. after resampling, the weight values of all particles was set at 1, then the next iteration was performed with new particles. the expected value of the target estimation is expressed as follows:"
"to verify the feasibility of the algorithm proposed in this study, a laser range finder with high precision was used. the measurement error of the adopted lase finder was ±10 mm to record the center position of the frontal object. the experimental equipment installed to verify the radar tracking system is shown in figure 4 . three verification conditions were set to avoid dark objects and lack of relative speeds, which can lead to losing laser range finder and radar information, as follows: metal and light-colored moving objects, a relative velocity of ±15 km/h or more, and objects moving from far away to nearby. the position of the object measured by the laser range finder is considered as the ground truth, which is illustrated by the blue line seen in figure 5 . the red line represents the tracking result obtained by the proposed particle filtering algorithm. the result of the internal algorithm of the radar sensor is illustrated by the green line. an offset between the detected and actual positions of the object may be observed owing to the characteristics of the radar sensor."
"(1) it allows users to simulate overlapping breeding cycles with a new cycle starting at each time step. in this paper, each time step represents a reproductive step with different actions such as selection, testing or replication being carried out at specific time steps. the simulation of overlapping breeding cycles allows early generations in one cycle to be used as parents in a new cycle before the parental cycle is finished. this paper describes the simulation method and working process of adam-plant in different plant breeding applications with an emphasis on its main characteristics, component elements and computational performance using a couple of examples of wheat-breeding programs."
"in the first section of results, we provide a detailed explanation of method neamarker and an instructive example of its work, both in comparison with alternative methods. a representative set of such methods was selected by investigating a wide range of earlier proposed algorithms and approaches. in methods (section \"alternative methods of pathway and/or enrichment analysis\"), we discuss their principles, consider both applicability to biomarker discovery and software usability, and motivate our choice of methods presented in fig. 1 and table 1 . thereby performance of our method neamarker is measured in parallel with using original gene profiles and alternative enrichment methods: overrepresentation analysis (ora), gene set enrichment analysis ssgsea 25, 26, signaling pathway impact analysis (spia 27 ) and egsea 28 that integrates multiple enrichment analysis methods. the outline (fig. 1f) and details of the comparative evaluation are reported in results. more specifically, we run the alternative methods in order to: (1) assess content of relevant information in three published experimental in vitro drug screens 12, 13, 29 (dubbed ccle, cgp, and ctd, respectively) as well as in eight tcga clinical cancer cohorts, (2) investigate preservation of this content across drug screens, (3) perform a novel, small scale drug screen and demonstrate that the pathway-level multivariate models withstand the independent validation, and finally 4) validate the identified correlations in clinical treatment profiles from tcga 30 ( table 2) ."
the user needs to specify the economic values for each trait to calculate true aggregate-breeding value. the aggregate-breeding value is calculated by weighting tbv of each trait by its economic value. the user can also specify different economic values on each trait in each selection stage.
"the image captured by the camera can easily be affected by lighting and weather conditions. furthermore, the estimated distance of the front object derived from the camera image has a low precision. a sufficiently large velocity relative to the front object is necessary for the mmw radar to stably detect it. accordingly, these two sensor subsystems were combined in a parallel connection to compensate for the limitations of each sensor and improve the robustness of the detection system. the overall architecture of the proposed detection and recognition system is shown in figure 1 . a clustering algorithm and particle filter were applied to the mmw radar data to achieve noise removing and multi-object tracking. then the object detected by the coordinate system of radar a clustering algorithm and particle filter were applied to the mmw radar data to achieve noise removing and multi-object tracking. then the object detected by the coordinate system of radar sensor was converted into an image coordinate. on the other hand, two-stage classifiers were implemented for the foreground segmentation and object recognition for the image data, respectively, then the object information could be obtained. finally, a radial basis function neural network (rbfnn) was used to fuse the detected object information from the mmw radar and camera."
"we have presented method neamarker for using network enrichment scores in prediction of drug response and demonstrated its advantage compared to the conventional analyses of original gene profiles and alternative, previously presented enrichment methods. in the first place, neamarker allowed combining data from multiple omics platforms. further, the nea scores indicated higher statistical power to detect enrichment and were therefore more prone to manifest correlation with drug sensitivity. the higher robustness, anchored in the network context, enabled better preservation better between independent screens. multivariate models using nea scores were built from a lower-dimensional space, thereby proving more compact and, at the same time, robust when re-tested on novel data. finally, corroborating in vitro phenotypes in corresponding clinical applications was possible by using our method but not by original profiles or alternative methods."
"true additive-genetic value and e i is its residual environmental value. in descendant generations, the alleles at qtl and marker positions were sampled according to principles of mendelian inheritance, and the phenotypes of plants in these generations are also a sum of g and e."
"a lot of object information was lost while the mmw radar information was processed by internal algorithms. therefore, the original unprocessed data was obtained from the mmw radar in this study. the proposed particle filter algorithm was used to track the front object and address the issue of losing too much information."
"another rbfnn is used to match the object information and determine whether the same objects are detected by the two sensors. six factors were entered as the network inputs, which affect the object match, including image coordinate u, object width, object height, object distance estimated from image, object distance measured by the radar, and the u coordinate converted from the radar to the image. either \"match\" or \"non-match\" were obtained as the network output."
"(1) the phased haplotypes and genotypes of the simulated individuals for each chromosome, the position and allele frequencies of the qtls and the markers, and the pedigree can be stored for each generation and each time step. this yields the opportunity to create simulated datasets from full scale and complex breeding programs e.g., to test new evaluation methods or models and to test methods for estimation of population parameters etc., in addition, it enables the user to develop summary statistics currently not included in the standard set of output. (2) estimated breeding values of every single plant can be stored if ebv is predicted for certain generation and time step. (3) the average tbv and phenotypes for each trait can be stored for each trait, for each generation and each time step. (4) realized variance within-and between-families (families are specified by the user) can be calculated for each trait, for each generation and each time step. (5) accuracy of selection can be recorded if selection is performed at certain generation and time step. if the selection unit is population or within family, the accuracy is either the correlation between tbv and ebv under selection on ebv or the correlation between tbv and phenotypes under ps. if selection is on entire families, the accuracy is calculated as the correlation between the sum of tbv of individuals in the family and the sum of ebv/phenotypes in the family."
"to cover all the potential object positions, n pieces of particles were randomly distributed within the radar detection area. each particle represents a potential position of a real object, where the weight of the particle indicates the probability that the object is at this location."
"a visual inspection of the survival curves in fig. 6 sheds light on usefulness of these tentative biomarkers in a clinical setting. as an example, in a 1-year survival perspective, relative risks (rr) would either increase (fig. 6a,c) or decrease (fig. 6b,d) given higher nea scores of the patient samples. by using this fixed follow-up interval and the cohorts of limited size, the confidence intervals at the 95% level would be rather broad: fig. 6a…d, respectively. the fractions of patients who might benefit from using these predictors could be estimated in terms of absolute risk reduction as 0.17, 0.62, 0.08, and 0.25. inversely, the \"number needed to treat\", i.e. how many patients should be treated for one individual to benefit from the new test would have been 6.00, 1.60, 12.91, and 3.94, respectively 49 . however we note that additional responders could be detected by using multiple markers in parallel. as an example, beyond the \"nod-like receptor signaling pathway\" at fig. 6a, the response to topotecan in ovarian cancers similarly correlated with kegg pathways \"one carbon pool by folate\" and \"bacterial invasion of epithelial cells\" as well as with the go term \"cytokine activity\" (not shown). predictions made with these markers would overlap only partially and therefore can complement each other. we presume that such discoveries should ultimately be evaluated by independent validation and careful clinical development. our combined analysis of independent cell screen and clinical results gave a first example of such validation."
the comparison of genetic gain from breeding plan a with that obtained from breeding plan b is given in figure 7 . the results showed that gs can double the genetic gain compared to ps.
"1. in order to solve the shortcomings of each single sensor, by using sensor fusion technology, we integrated the two sensor systems and improved the reliability of the systems. 2. for the fusion architecture of series type, any single sensor failure causes whole system failure."
"the two-stage vision-based object recognition system was similar to in our earlier work [cit] . in the first stage, the haar-like features algorithm was used to identify the candidate regions of object from foreground segmentation. the second stage is responsible for object recognition. three kinds of objects (i.e., pedestrians, motorcycles, and cars) can be identified by svm classifiers. the scheme of the two-stage vision-based object recognition process in shown in figure 6 . the object recognition results are shown in figure 7 ."
"the mmw radar signals are electromagnetic waves. both reflection and refraction will occur when the electromagnetic waves occur on the medium. in addition to the reflected wave from the medium itself, some noise signals of non-real objects are also prone to appear. the relationship between relative distance and echo intensity information was statistically analyzed using a vast amount of data collected during experiments. the statistical results are shown in figure 3 . the statistical results of the signal distribution indicate that both real objects and noise show respective concentrations, and only a small part of the distribution of both overlaps. accordingly, a noise filtering operation was performed. as shown in figure 3a, after the signal on the left side of red curve was filtered, the subsequent target tracking and particle filter algorithm were performed. density-based spatial clustering of applications with noise (dbscan) algorithm [cit] was used to cluster the radar data, and the number of possible front objects was estimated."
"three kinds of scenario conditions (daytime, nighttime, and rainy-day) were implemented to verify the proposed system. all the scenarios were carried out on urban roads. the mmw radar and camera were mounted on the front bumper of the experimental car, as shown in figure 11 ."
"a single sensor system can operate independently; however, a parallel architecture was adopted in this study to fuse two different sensors. the main purpose of this is to improve the detection rate that can be achieved by a single sensor. the sensor fusion was divided into three parts. first, the twodimensional coordinate information of the mmw radar was converted into the coordinate of the image. afterwards, the information obtained by the two sensors was integrated into the same coordinate system. next, the object information needed to be matched to determine whether the same object information had been obtained by both the mmw radar and camera, and to integrate the detection results of the two systems. finally, the trusted sensor was determined based on the confidence index of the sensor."
"another rbfnn is used to match the object information and determine whether the same objects are detected by the two sensors. six factors were entered as the network inputs, which affect the object match, including image coordinate u, object width, object height, object distance estimated from image, object distance measured by the radar, and the u coordinate converted from the radar to the image. either \"match\" or \"non-match\" were obtained as the network output."
"due to the high sensitivity to light sources, the performance of camera sensor depends on the condition of light sources. for example, suffering in an insufficient light source, the vision-based systems cannot extract completely the features of objects at night. on the other hand, in rainy weather experiments, the raindrops adhering to the camera lens block the object in front of the vehicle. thus, the system cannot effectively identify the information of the target, leading to the failure of the image subsystem. therefore, the worst detection rates are achieved at night and on rainy days."
"the user needs to specify the number of seeds generated from seed propagation at each selection stage. this is related to the selection unit. if the selection unit is population or within family, the number of seeds represents the number of seeds produced by each individual plant. if the selection unit is entire family, the number of seeds represents the number of seeds produced by the user-specified family."
"all survival analysis results were obtained using r package survival (https://doi.org/10.1007/978-1-4757-3294-8). in order to estimate significance of the model terms, we used function coxph with continuous feature vectors. however, for visualizing the survival curves (fig. 6 ) each feature was binarized at a cutoff that yielded the lowest p-value for the interaction term. apart from the interaction model, we also checked if the p-value and fdr distributions preserved their properties under a unifactorial model. to this end, sub-cohorts of respective drug-treated patients were included in the survival analysis with the single main factor \"feature\": boxplots. we used the default parameters for function boxplot in r. the boxes contain data points within 25-75th percentile intervals (i.e. between quartiles q1 and q3). the maximal whisker length, mwl, is defined as 1.5 times the q1-q3 interquartile range (i.e. the box length). whiskers can extend to either the mwl or the maximal available data point when the latter is below mwl. markers thus correspond to data points that extend off the box by more than the mwl value."
"next, we validated drug sensitivity profiles of three anti-cancer compounds, tested previously in the ctd screen -rita, prima-1 met /apr-246, and jq1 -in a new in vitro screen, named act (after \"advanced cancer therapies\" centre at karolinska institutet). activity of these compounds was re-tested in a panel of 20 cancer cell lines (the act set) for which gene expression and point mutation profiles data were available from the ccle. similarly to the results in fig. 3, many of the both original gene profiles and nea features showed significant correlation with drug sensitivity, which indicated a potential for creating multivariate prediction models."
"adam-plant is fast in generating simple breeding programs and genotype data. however, the time consumed can become significantly larger in particular when a large number of plants and complex breeding strategies are simulated. the computation time also depends on the utilization of genomic information and on the genetic model used. with polygenic model where no sequence data is generated, the running time is short even with complex selection steps and large number of seeds. when a very precise prediction is required with consideration of full genomic information, however, the time is markedly increased due to sampling of molecular information (qtls, markers and crossovers in the genome) and so on for each single plant. for example, one replicate of a 40 year -commercial gs breeding program with 40 overlapping breeding cycles takes around 23 h for running [cit] ."
"for breeding plan b, a 25-year commercial wheat-breeding program using gs was simulated. the first 10 years was used as a burn-in stage, where ps was used as in breeding plan a without gs. the breeding strategy for this burnin stage was the same as in breeding plan a. in the last 15 years, the selection decisions in f1 to f3 was the same as in breeding plan a. the current study present the breeding strategy of breeding plan b for f4-f7 at year 11-25 where the difference existed between breeding plan a and b (figure 6 ):"
"the supervised learning algorithms was used to learn the relationship between the mmw radar coordinate and image coordinate system. before the coordinate transformation, the radar coordinates (x, y) and image coordinate (u, v) needed to be recorded synchronously to be considered as training samples for offline learning. an mmw radar uses electromagnetic waves as a medium, and it exhibits better reflective property to metal objects. hence, a triangular metal reflector was used as a target object to gather data obtained from the radar and the camera, as shown in figure 8 . a metal reflector was randomly placed in a straight lane at a distance which ranged from 1 m to 12 m in front of the experimental vehicle, and a total of 280 training samples were established."
"considering the effect of pavement puddles and shadow environment, the daytime scenarios included direct sunlight, pavement puddles, and shadow environments, as shown in figure 12 ."
"before considering the type of seed propagation, the user needs to specify reproductive and life cycle characteristics of the population. such characteristics include the reproductive age of the plant, the last generation of the breeding cycle and the generation at which germplasm is stored for later use."
"when the confidence index score is greater than the set threshold th, the reliability of the system is extremely high, and the output result obtained by the system represents the real situation. if the confidence index of each subsystem is greater than the threshold th, then the subsystem with the highest score is responsible for the entire system decision making process."
"this system integrates mmw radar and camera information and improves the scene when one of the detection systems fails by using the sensor fusion of parallel architecture. the system presents complementary characters. for example, as shown in figure 20, the radar did not detect the front vehicle when the relative speed of the radar and object was relatively small; thus, the camera was used to compensate for the radar failure. on the other hand, when the raindrops adhering to the camera lens blocked the scene, leading to image detection failure, the radar compensated for this situation, as shown in figure 21 . all the experiments performed under different weather conditions involved three classifications of objects: pedestrians, motorcycles, and cars. the detection results of vision-based systems are listed in table 3 ."
"where!n means \"complement to n\", i.e. all global network edges that did not belong to n ags-fgs . the number of links expected under true null, i.e. by chance, was determined by:"
"the profiling was performed in ccle study using affymetrix genechip ® human genome u133 plus 2.0 array and in cgp study by affymetrix genechip ® ht human genome u133 array plate. the expression datasets were normalized as described when made public by the authors. expression profiles in the ctd study were from ccle. it has been shown earlier 11 that disagreement between cgp and ccle could be attributed to the usage of different transcriptomics datasets only to a minor extent. we checked both the ccle and cgp expression profiles and concluded that the latter provided poorer statistical power in regard to drug sensitivity as well as lower coverage of both genes (13891 unique mapped gene symbols vs. 18900 in ccle) and cell lines (622 vs. 1034). for these reasons, we used the ccle dataset in all the presented alternative analyses except nea and ora. ags for the latter two were compiled from the both platforms (affymetrix_ccle and affymetrix_cgp). expression values x of the downloaded datasets were transformed to log 2 (x)."
"from the collected training samples, the longitudinal and lateral distances from the radar were considered as the input of the rbfnn, and the corresponding u coordinate of horizontal direction in the image was considered as an output. this network architecture allows for obtaining the coordinate conversion relationship between these two sensors. the network architecture is shown in figure 9 . the camera was installed at an angle parallel to the horizon. when the target object moved from far away to nearby, the position of its center point slightly changed near the center point of the image in the vertical direction. thus, the variation in the image v -direction coordinate was not obvious. therefore, the fusion system primarily enabled the neural network to learn the relationship between the mmw radar coordinate (x, y) and the image coordinate (u, v)."
"this study proposed a sensor fusion technology integrating mmw radar and camera for front object detection. the proposed system consists of three subsystems, including a radar-based detection system, vision-based recognition system, and sensor fusion system."
"this system integrates mmw radar and camera information and improves the scene when one of the detection systems fails by using the sensor fusion of parallel architecture. the system presents complementary characters. for example, as shown in figure 20, the radar did not detect the front vehicle when the relative speed of the radar and object was relatively small; thus, the camera was used to compensate for the radar failure. on the other hand, when the raindrops adhering to the camera lens blocked the scene, leading to image detection failure, the radar compensated for this situation, as shown in figure 21 . due to the high sensitivity to light sources, the performance of camera sensor depends on the condition of light sources. for example, suffering in an insufficient light source, the vision-based systems cannot extract completely the features of objects at night. on the other hand, in rainy weather experiments, the raindrops adhering to the camera lens block the object in front of the vehicle. thus, the system cannot effectively identify the information of the target, leading to the failure of the image subsystem. therefore, the worst detection rates are achieved at night and on rainy days."
"in the nighttime experiment, the scenarios included flashing brake lights of front vehicles, headlight reflections, and poor lighting environments, as shown in figure 13 ."
"adam-plant is a computer software for stochastic simulation of plant breeding programs that utilize diverse genomic and phenotypic information. it was developed with a purpose of guiding breeding decisions in early planning and implementation phases. to maximize its utility, it was developed based on effective models of the genome and the breeding process with great flexibility. it can simulate easy to very complicated breeding programs that are similar to commercial breeding plans (with gs and/or speed breeding technologies), and the best breeding strategy can be identified. based on the results from simulation experiments, breeders have the opportunity to optimize their breeding methodology, and use of resources (number of matings, number of test plots, amount of genotyping etc.) and greatly improve breeding efficiency."
"in order to reproduce the actual road conditions, we designed a rainy-day scenario too. as the sensors are mounted on the front bumper, the raindrops often adhered to the camera lens during the rainy day experiment, as shown in figure 14 . in order to reproduce the actual road conditions, we designed a rainy-day scenario too. as the sensors are mounted on the front bumper, the raindrops often adhered to the camera lens during the rainy day experiment, as shown in figure 14 . considering the effect of pavement puddles and shadow environment, the daytime scenarios included direct sunlight, pavement puddles, and shadow environments, as shown in figure 12 ."
"the mmw radar detection and image recognition systems operate independently, and the two systems obtain information about the detected objects, respectively. to fuse the information of the two systems, the object information must be matched first to determine whether the same object information has been detected by the two sensors. coordinates shown in the same image may correspond to several different radar coordinate information, as illustrated by the green points shown in figure 10 . in addition, the distance estimated from the image coordinates may be inaccurate owing to the bumpy road surfaces that can cause the vehicle to shake; thus, it is difficult to match the object information and effectively determine whether the same object is detected."
"this system integrates mmw radar and camera information and improves the scene when one of the detection systems fails by using the sensor fusion of parallel architecture. the system presents complementary characters. for example, as shown in figure 20, the radar did not detect the front vehicle when the relative speed of the radar and object was relatively small; thus, the camera was used to compensate for the radar failure. on the other hand, when the raindrops adhering to the camera lens blocked the scene, leading to image detection failure, the radar compensated for this situation, as shown in figure 21 . in addition to compensating for single sensors failures, the system integrates the sensors' information when both the radar and camera detect objects simultaneously. the system relies on the coordinate transformation and object matching decision mechanism to determine whether the same objects are detected by the two sensors, as shown in figure 22 . in addition to compensating for single sensors failures, the system integrates the sensors' information when both the radar and camera detect objects simultaneously. the system relies on the coordinate transformation and object matching decision mechanism to determine whether the same objects are detected by the two sensors, as shown in figure 22 . in addition to compensating for single sensors failures, the system integrates the sensors' information when both the radar and camera detect objects simultaneously. the system relies on the coordinate transformation and object matching decision mechanism to determine whether the same objects are detected by the two sensors, as shown in figure 22 . the parallel sensor fusion architecture proposed in this study exhibits the advantages of compensating for the disadvantages of relying on a single sensor. it improves the scene in case of subsystem failure and significantly increases the system detection rate and stability, as listed in table 4 . regardless of the weather conditions, better detection rates were achieved by the sensor fusion system than those obtained when relying on a single subsystem. table 5 lists the detection results of each system for the three object categories under different weather conditions. the sensor fusion system can achieve a detection rate of more than 90%."
the output files are generated and written to a user-specified output directory. different variables for each generation and each time step can be output depending on the interest of the user. the description of the major variables is as follows:
"the mmw radar detection and image recognition systems operate independently, and the two systems obtain information about the detected objects, respectively. to fuse the information of the two systems, the object information must be matched first to determine whether the same object information has been detected by the two sensors. coordinates shown in the same image may correspond to several different radar coordinate information, as illustrated by the green points shown in figure 10 . in addition, the distance estimated from the image coordinates may be inaccurate owing to the bumpy road surfaces that can cause the vehicle to shake; thus, it is difficult to match the object information and effectively determine whether the same object is detected."
"the supervised learning algorithms was used to learn the relationship between the mmw radar coordinate and image coordinate system. before the coordinate transformation, the radar coordinates (x, y) and image coordinate (u, v) needed to be recorded synchronously to be considered as training samples for offline learning. an mmw radar uses electromagnetic waves as a medium, and it exhibits better reflective property to metal objects. hence, a triangular metal reflector was used as a target object to gather data obtained from the radar and the camera, as shown in figure 8 . a metal reflector"
"where denotes the distance from the input data point to the svm hyperplane, is the number of times the object tracked in image subsystem, and and are constants. the confidence index of the sensor fusion system was expressed as follows: ri score score score . (12) when the confidence index is greater than the set threshold ℎ, the reliability of the system is extremely high, and the output result obtained by the system represents the real situation. if the confidence index of each subsystem is greater than the threshold ℎ,, then the subsystem with the highest score is responsible for the entire system decision making process."
"(1) there are a number of computer software packages available for simulating plant breeding programs. however, adamplant has several advantages over the others: it allows users to simulate overlapping breeding cycles with the possibility of a new cycle starting at each time step. the simulation of overlapping breeding cycles allows information and elite genetic material transfer from one breeding cycle to another. it means that early generations in one cycle can be used as parents in a new cycle. this system resembles the procedure of actual commercial breeding programs for crop plants."
"in the next section, the current study showed three examples of commercial wheat-breeding programs using a finite-locus model. the founder population for these programs is established based on stored real dataset containing 988 individual genomes collected from a commercial plant breeding company [cit] . data are from three breeding cycles, each consisting of 330 new f6 lines. approximately 60 parental lines were crossed in the beginning of each breeding cycle, followed by five generations of selfing to produce the f6 lines. the number of markers were 9582 on the entire genome. as no qtl information was available in the data, among all the 9582 markers, 1039 loci that were randomly chosen across the genome were assumed as qtls, and the remaining 8543 loci were assumed as markers for breeding value prediction. the real dataset help to make reasonable commercial breeding plans by considering the population structure and ld structure."
"a single sensor system can operate independently; however, a parallel architecture was adopted in this study to fuse two different sensors. the main purpose of this is to improve the detection rate that can be achieved by a single sensor. the sensor fusion was divided into three parts. first, the twodimensional coordinate information of the mmw radar was converted into the coordinate of the image. afterwards, the information obtained by the two sensors was integrated into the same coordinate system. next, the object information needed to be matched to determine whether the same object information had been obtained by both the mmw radar and camera, and to integrate the detection results of the two systems. finally, the trusted sensor was determined based on the confidence index of the sensor."
"the software was developed with modules, which makes it easy to be extended with new methodologies for example on how to utilize genomic information. although the examples presented were wheat breeding programs, adam-plant can also be used for simulating many other plant species such as outcrossing crop maize and diploid ryegrass. in the future, the breeding program design for tetraploid plants will be integrated in the software. dominance effects that are important for cross-pollinated crops and epistasis effects that are important for self-pollinated crops will also be integrated."
"the radar detection subsystem uses mmw radar to perceive the environment ahead. the proposed multi-object tracking algorithm with a particle filter can effectively track the objects in front and remove non-object noise. the radar subsystem experiments tested three different categories of objects under different conditions. the detection results are shown as green circles in figures 15 and 16. the tests primarily involved a single target in a lane. if there were multiple targets, the alert was reported for closest target to the experimental vehicle. other targets continued to be tracked. a detection rate exceeding 60% was maintained by the radar detection system during daytime, nighttime, and rainy days. the experimental tests performed under different weather conditions verified that the radar detection system is not affected by weather conditions. the experimental results are listed in table 2 . a detection rate exceeding 60% was maintained by the radar detection system during daytime, nighttime, and rainy days. the experimental tests performed under different weather conditions verified that the radar detection system is not affected by weather conditions. the experimental results are listed in table 2 ."
"consistency of the discovered correlates between drug screens. in order to test reproducibility of the drug-feature associations in alternative experimental settings, we used data from three in vitro drug screens: ccle 12, cgp 13, and ctd"
the datasets generated for and analyzed during the current study are available from the corresponding author on reasonable request. the original drug screen data generated for this study (ic50 values) are included as one of the supplementary information files described below.
"(1) f4: the breeding strategy of f4 for gs was the same as for ps with the exception that all the 225 individual f4 were assumed genotyped in each cycle. the genotyped germplasm of f4 with phenotypes for the targeted traits were added yearly to establish the reference population (growing by 225 genotypes per year) for the current cycle where the selection is performed (figure 7) . (2) f5: line selection was conducted based on the yp performance of all f5 individuals in each f4-line. only one replicate was simulated for each f4-line. yp performance of all f5 individuals within each f4-line was recorded, so that there were 225 phenotypes. these 225 phenotypes and 225 f4 genotypes and existing reference population were used to estimate breeding values of each f4 using gblup model [cit], so that 75 highest-ranking f4-lines were identified given gebvs of f4. in total, 75 out of 225 f5 lines with highest gebv were selected and were selfpollinated to produce 900 f6 per f4 family (100 f6 per plot). the information of phenotypes and genotypes in the current cycle were stored and was used for predicting breeding values in the next cycles. the germplasm of the 75 selected lines were stored and potentially become parental lines for the next cycles. (3) f6: line selection was conducted for f6 based on the yp performance of all f5 individuals in each f4-line. nine replicates was simulated, and therefore, each f4-line was grown in 9 plots. similar to f5, total ya performance of all f6 individuals in each f4-line was recorded, so that there were 75 phenotypes. these 75 phenotypes and their corresponding f4 genotype were used for ebv estimation. in total, 30 out of 75 f6 lines with high gebv were selected and were selfpollinated to produce 900 f7 per f4-line (100 f7 per plot). the germplasm of the 30 selected lines were stored and potentially become parental lines for the next cycles. (4) f7: line selection was conducted based on the total ya performance of all f7 individuals in each f4-line. nine replicates were simulated for each f4-line, which means that each f7 line was grown in 9 plots. similar to f5 and f6, five out of 30 lines with the highest gebv were selected and were self-pollinated to produce 900 f8 per line (100 f8 per plot). the germplasm of the five selected lines were stored and potentially become parental lines for the next cycles."
"in conclusion, adam-plant is a flexible and efficient computer software for stochastic simulation of breeding plans for crop plants. adam-plant simulate real commercial breeding program structures with parallel breeding cycles, gs and speed breedingfor self-pollinated and cross-pollinated plant crops. this makes adam-plant an important tool to compare breeding efficiencies and the improvement of performance from a wide range of selection strategies."
"second, an exciting result of this study is that, because of the lasting effects of this intervention (remaining at least 50% effective for 4 months), significant reductions in biting rates can be achieved by clearing vegetation before and/or during the biting season, and that year-round intervention is not required ( table 2 and fig. 2 ). while monthly clearing will very quickly reduce transmission below the model-predicted atp thresholds (potentially within 1 year of interventions), performing \"slash and clear\" just once per year is enough to save up to 18 years of interventions compared to relying on annual mda alone in a hyper-endemic setting like palaure pacunaci ( table 2 and fig. 3 ). choosing to undertake annual or seasonal vegetation clearance if the community does not have the capacity for monthly intervention will still save many years of interventions compared to using annual mda alone even if optimal coverages are achieved (here simulated at 80%). as expected, fewer years of interventions were saved in settings with lower pre-control prevalence. this trend remained true even if seasonal patterns were to shift and the vegetation was cleared annually at a suboptimal time with respect to the biting season ( supplementary table s2 )."
the partitioning approach attribute of the dapf represents the partitioning strategy of the framework. current dapfs implement application partitioning in two different ways; static partitioning and dynamic partitioning. a) in static partitioning the application is partitioned in fixed number of partitions only once [cit] . static application partitioning is a simple and lightweight approach for application offloading; however it lacks the features of addressing the issue of dynamic workload on smds. b) in dynamic partitioning the elastic mobile application is partitioned dynamically at runtime. the dynamic portioning approach is implemented to cope with the issue of dynamic application processing load on smds at runtime. dynamic partitioning of the intensive mobile application at runtime is a robust technique for coping with the dynamic processing loads on smd [cit] . the entire application migration frameworks which do not involve application partitioning are represented as n/a in pa column of table 3 and table 4 .
"the annual gdp of ksa (the recent years) is decreasing due to the high volatile of unreliable crude oil price in the global market as given in fig. 3 . [cit] scheme. due to its high complexity and price volatility, researchers adopted various statistical, mathematical, artificial intelligence, hardware base, computer science knowledge and software based and engineering methods for the prediction of accurate price of crude oil of ksa. the autoregressive integrated moving average (arima) and gene expression programming (gep) techniques used to predict crude oil prices over the period from january 2, 1986 to june 12, 2012 [cit] . the gep model outperforms the arima has the highest explanatory power as measured by the r-squared statistic. however, the gep didn't successes in obtaining high prediction accuracy. the hybrid model integrating wavelet and multiple linear regressions (wmlr) was proposed for crude oil price forecasting of wti obtained higher accuracy than regular lr (learning rate), arima, and generalized autoregressive conditional heteroscedasticity (garch) model [cit], however this method used multiple steps such as particle swarm optimization (pso) for adopting parameters and principal component analysis for processing subseries data. the arima, garch, belief networks, k-means clustering and an empirical mode decomposition (emd) methods have used for crude oil forecasting in the last two decades [cit] . however, due to the volatility, nonlinearity, and irregularity, the classical and econometric model can lead to the decrease of the accuracy. therefore, due to the above mentioned limitations of the classical science, mathematical and statistical approaches, soft-computing models can provide powerful solutions to nonlinear crude oil price prediction [cit] ."
"many experiments found that the computational intelligent algorithms which simulate the way humans' reason by incorporating their rate of efficiency and randomness during decision making. they have broadly divided into techniques based on modelling of human mind and nature inspired algorithms [cit] . these methods often have some advantages over typical mathematical and statistical-based models [cit] . however, these models also have their own drawbacks, such as ann often suffer from local minima and over-fitting problems, while svm and gp, including ann, are sensitive to parameter selection and suitable architectures and dataset behaviors [cit] . also, the standard learning algorithms of the ann models have the same local minima trapping problem and slow convergence speed [cit] ."
"secondly, it turns out that research on preferential reasoning has really only reached maturity in a propositional context, whereas many logics of interest, like the ones mentioned above, have more structure in both syntax and semantics. if one wants to be able to capture the forms of reasoning exemplified above, then one has to move beyond propositional preferential consequence."
"it is very hard to predict the profitability of a program partitioning, due to the combinatorial nature of the problem and due to the very complex and apparently chaotic interaction among transformations resulting from the selection of a given partition. this interplay is machine-specific, and to find a profitable partition for a program we will thus resort to an iterative, feedback-directed search. on the other hand, profitability of optimizations such as tiling or vectorization are easier to assess, typically because they are generally beneficial if they do not destroy some other properties of the code, such as thread-level parallelism or data locality. we will thus rely on performance models to drive these choices."
"the ffnn was trained and tested through the bio inspired learning methods such as abc, gabc, best-so-far and gbabc and it is a computer based mathematical simulation model that predicts high accurate crude oil prices. different ffnn structures of hidden layer nodes with different learning methods, parameters such as cs, d and c values are used as given in table iii . the proposed gbabc algorithm performed well in term of mse, nmse, accuracy and success rate. the convergence speed of gbabc method is faster than typical methods. courtesy of the strong exploration and exploitation process, the proposed method has enough decreased global trapping problem based on guided employed bees, best-so-far onlooker and scout bees' strategies. therefore, the proposed method has a high accuracy, less prediction error and high success rate. thus, the proposed gbabc is a robust and www.ijacsa.thesai.org efficient method for crude oil prices prediction. however, it is difficult to set the parameters of the proposed algorithm and ann model appropriately. based on high accurate predictions, the crude oil investors, analyst or producer can easily predict the volatility of the crude oil prices in different times and the environment as well. the gbabc algorithm can be extended to different application such as clustering, numerical function optimization and other complex problem solver."
"our study has highlighted three major advantages offered by the addition of \"slash and clear\" vc for achieving rapid and sustained onchocerciasis elimination. first, the main result of importance for onchocerciasis elimination programmes is that supplementing mda with vegetation clearing activities can significantly accelerate the average timelines to achieve elimination targets ( table 2 ). the implementation of \"slash and clear\" can potentially save, on average, more than 10 years of interventions compared to relying on annual mda alone if mf thresholds are used as elimination targets, and, notably, the savings could increase to more than 20 years if the corresponding atp thresholds are used ( table 2) . these savings were also apparent in the case of biannual mda, although to a lesser extent. other studies have also shown this value of adding vc to mda, and together with the present results, it is increasingly clear that relying on mda alone may not bring about onchocerciasis elimination in all settings 29, 30 . indeed, field evidence supports this modelling result, demonstrating that the use of annual mda alone in many areas has not led to achievement of targets even after 19 years of applying the intervention 29, 30, while vc alone and vc together with mda have played key roles in onchocerciasis elimination efforts in uganda and elsewhere 11, [cit] 31 . these results strongly indicate that diversifying options, such as including \"slash and clear\" vc into mda programmes, will be required if s. damnosum associated onchocerciasis is to be most effectively eliminated from endemic locations."
"we performed our experiments on two modern multi-core machines: a 4-socket intel hex-core xeon e7450 (dunnington) running at 2.4 ghz with 64 gb of memory (24 cores, 24 hardware threads) and a 4-socket amd quad-core opteron 8380 (shanghai) running at 2.50 ghz (16 cores, 16 hardware threads) with 64 gb of memory. we also experimented on a representative of low-cost computing platforms, an atom 230 processor running at 1.6 ghz with 1 gb of memory (1 core, 2 hardware threads). all systems ran linux 2.6.x. we used icc 11.1 for the xeon and opteron machines, and gcc 4.3.3 for the atom. the compilation flags used for the original code were the same as for the different tested versions; they are reported in figure 7 . figure 6 presents the main characteristics of our benchmark suite. we considered 16 benchmarks from the polybench test suite [cit] . for most programs, the execution time of the original code using the specified problem sizes is below 3 seconds. in figure 6, we report for each benchmark some information on the considered scop: (#loops the number of loops, #stmts the number of statements, #refs the number of array references, #deps the number of dependence polyhedra)."
"abstract-the latest developments in mobile devices technology have made smartphones as the future computing and service access devices. users expect to run computational intensive applications on smart mobile devices (smds) in the same way as powerful stationary computers. however in spite of all the advancements in recent years, smds are still low potential computing devices, which are constrained by cpu potentials, memory capacity and battery life time. mobile cloud computing (mcc) is the latest practical solution for alleviating this incapacitation by extending the services and resources of computational clouds to smds on demand basis. in mcc, application offloading is ascertained as a software level solution for augmenting application processing capabilities of smds. the current offloading algorithms offload computational intensive applications to remote servers by employing different cloud models. a challenging aspect of such algorithms is the establishment of distributed application processing platform at runtime which requires additional computing resources on smds. this paper reviews existing distributed application processing frameworks (dapfs) for smds in mcc domain. the objective is to highlight issues and challenges to existing dapfs in developing, implementing, and executing computational intensive mobile applications within mcc domain. it proposes thematic taxonomy of current dapfs, reviews current offloading frameworks by using thematic taxonomy and analyzes the implications and critical aspects of current offloading frameworks. further, it investigates commonalities and deviations in such frameworks on the basis significant parameters such as offloading scope, migration granularity, partitioning approach, and migration pattern. finally, we put forward open research issues in distributed application processing for mcc that remain to be addressed."
3) application partitioning based application offloading: partitioning of the mobile application at runtime is a prominent approach for outsourcing the intensive components of mobile applications. elastic mobile applications are capable to be partitioned at runtime for coping with the resources constraint on smd. elastic applications are partitioned either statically or dynamically at runtime. the following section classifies and reviews existing approaches on the basis of static or dynamic offloading.
"in current dapfs, resources intensive distributed platform is established at runtime. mobile applications offloading frameworks are developed on the basis of standalone application architecture, whereas the processing of application is performed in the distributed fashion. as a result, current dapfs establish a resources intensive and complex computing environment at runtime. application offloading techniques are primarily based on either entire application/job migration or application partition migration to remote servers. the implementation of distributed architecture for virtual mobile cloud is hindered by the following obstructs. a) local distributed processing models lack in the availability of centralized management; for that reason it is difficult to configure explicitly defined client and server components for the mobile applications. b) virtual clouds necessitate special requirements for the establishment of distributed platform which is challenging to maintain for mobile devices which are participating in adhoc cloud. the special requirements include; smds remain in the close proximity, follow the same movement patterns, voluntariness for service and provision, implementation of specific service architecture [cit] . smds in the virtual cloud exploit additional computing resources for the configuration of distributed platform and management of distributed services provision to the requesting client devices. further, shorter battery life time of smds is major challenge in virtual/adhoc distributed application processing models. therefore, the ad-hoc and virtualized nature of local distributed platform is another obstacle in explicitly defining client and server components of the mobile application. however, the availability of centralized resources and services and centralized management mechanism in cloud datacenters are the motivating factors for incorporating distributed architecture for the intensive mobile applications. the implementation of client/server model can be a potential alternative for the traditional standalone intensive mobile applications for mobile cloud computing. on the other hand, traditional client/server model has the limitations of reliability of client application on server application. applications are configured in such a manner so that client applications remain dependent on the server application. whereas, the wireless access medium is the main inhibiting factor for implementing highly dependent client/server model for intensive mobile applications in mcc. hence, it is challenging for distributed mobile application to incorporate the principles of distributed applications in such a manner so that mobile applications can operate in the situations of inaccessibility of cloud server nodes."
"the artificial bee colony (abc) [cit] by karaboga [cit] to solve the complex nonlinear optimization problem in multivariable functions, is a relatively innovative meta-heuristic optimization algorithm that is based on the social behavior of honey bee colony named: employed bees, onlooker bees and scouts during searching and managing for food sources (fs) [cit] . the first half of the colony consists of the employed bees and the second includes the unemployed. from the different numerical and statistical performance measure demonstrate that the abc algorithm is competitive with other types of meta-heuristic and typical algorithms [cit] . the technical duties of the employed and unemployed artificial bees are given in details."
"using mffn model reached on minimum error and high accuracy through different learning strategies such as supervised, unsupervised and reinforced, such as, backpropagation, genetic algorithm, gradient descent and so on. typical training algorithm, bp has some drawbacks like slow convergence and trapping in local minima [cit] . the bayesian approach used to predict crude oil price through the various independent variable factors such as world oil demand and supply, the financial situation, upstream costs, and geopolitical events. the results show that the crude oil price is estimated to increase to $169.3/ [cit] ."
"in what follows we analyze applications of our constructions to some classes of modal logics commonly used in ai. a further application area, in the context of the description logic alc, was recently explored by the present authors [cit] ."
tiling a permutable loop nest is profitable in particular when there is reuse of data elements within the execution of a tile. another criterion to take into account is to preserve enough iterations at the inner-most loop level to allow for a profitable steadystate for vector operations within the execution of a tile. our extremely simple algorithm to determine the tiling of a loop nest proceeds as follows:
"the time to compute the space, pick a candidate and compute a full transformation is negligible with respect to the compilation and execution time of the tested versions. in our experiments, the full optimization process for the 16 presented benchmarks took less than one hour on the atom, the slowest machine. this time is totally dominated by the execution time of each candidate; had we used a smaller/larger dataset sizes the optimization time would have decreased/increased."
"the trade-off between coarse-grain parallelization, locality and vectorization is very difficult to capture. using our framework, tuning the trade-off between fusion and distribution drives the effectiveness of subsequent well-defined cost models used to transform the code to expose different choices of locality and parallelization. our iterative technique automatically discovers the partitioning with optimal performance, whatever the specifics of the program, compiler and architecture."
"analysis approach, we calculated the tbr and the mf prevalence breakpoints for each of the best-fitting parameter vectors 55, 56, 58, 59 . to calculate the tbr for each parameter vector, we progressively decrease the average number of black flies per human, m, from its original value to a threshold value below which the model always converges to zero mf prevalence. the product of the number of bites per fly per month, β, and this newly found m value is termed as the tbr. we similarly calculate the mf prevalence breakpoints. given a particular biting rate (either the abr or tbr), we estimate the minimum l * below which the model predicts zero mf prevalence. the corresponding mf prevalence at this threshold l * value is termed as the mf breakpoint. the distribution of mf breakpoints at a particular biting rate in a site can be described by an empirical inverse cumulative density function, which we used in conjunction with exceedance calculations 59 to quantify the values of mf breakpoint prevalence thresholds reflecting various elimination probabilities. here, we used the mf breakpoint corresponding to an elimination probability of 95% as the elimination target."
"taking the advantages of high exploration and exploitation process from gabc and best-so-far algorithm, a new hybrid method is proposed called a guided best abc algorithm. the proposed gbabc algorithm will increase the effectiveness of typical abc, gabc and best-so-far abc algorithm [cit] . the gbabc will first use the gbest strategy through employing bee to increase the exploitation process according to the given problem, while the onlookers and scout bees will adapt the best-so-far method for exploration process with balance quantity."
"t he fifth generation of mobile technology (5g) is not only about the development of a new radio interface but also of an end-to-end system. this end-to-end system includes the integration and convergence of all network segments (radio and fixed access, aggregation, metro, and core) with heterogeneous wireless and optical technologies together with massive cloud computing and storage infrastructures [cit] . the 5g architecture shall accommodate a wide range of use cases with different requirements in terms of networking (e.g., security, latency, resiliency, bandwidth) and cloud resources [e.g., distributed nodes with cloud capabilities, edge/core data centers (dcs)]. thus, from an administrative perspective, one of the main challenges for an infrastructure operator will be to provide multiple, highly flexible, end-to-end dedicated network and cloud infrastructure slices, over the same physical infrastructure, to different users or tenants in order to satisfy their application-specific requirements."
the resource manager is responsible for storing and maintaining the up-to-date state of all virtual and physical sources controlled by the vimap. it is also responsible for maintaining the resource allocation relationship between the requested virtual resources and the allocated physical resources.
"this ordering defines that s and t are fused together, and that r is not and is executed before s and t . an equivalent description is:"
"tiling a loop nest is legal if the loops to be tiled can be permuted [cit] . if the loops are indeed permutable, then there would be no dependence path going in and then out of a given tile. this is also known as the forward communication only [cit] property, and can be encoded as an affine scheduling constraint [cit] ."
"in this section, we present the evaluation of the proposed heuristic baseline solution comparing with a random fit based algorithm. the random solution differs on the dc selection strategy but keeps cspf to ensure path feasibility in the virtual link selection stage."
"since ranked models are preferential models, the notion of rational consequence is as in definition 3.4. we can then state the following result: theorem 3.7 a defeasible consequence relation is a rational consequence relation if and only if it is defined by some ranked model."
"schedules are selected such that each dimension is independent with respect to all others: this leads to a oneto-one mapping. rectangular or nearly rectangular blocks are achieved when possible, avoiding complex loop bounds required for arbitrarily shaped tiles."
"rate and the corresponding l * threshold is used in the atp calculations. when vc is not used, the atp threshold is calculated by multiplying the abr and the corresponding l * threshold. note, according to epidemiological theory, crossing below either of these thresholds would lead to the cessation of vector-borne disease transmission in a local setting 55, 61, 62 . however, given the relatively longer life span of adult worms, it will invariably take more time to achieve the mf threshold compared to the atp threshold. furthermore, in an open environment (i.e. where dispersal or migration of flies is significant), the persistence of mf despite stopping local transmission once atp thresholds are crossed would pose a risk of reestablishment of transmission if mda is stopped before mf breakpoints are reached. this risk of recrudescence of infection means, as per the who definitions, that achieving permanent transmission interruption in a local setting may require waiting until the mf thresholds are also met, while meeting the atp thresholds can be considered as conditional interruption or suppression of transmission 27 ."
"however, as in the propositional case [cit], the obvious definition of modal rational closure based on modal rational entailment does not produce an appropriate result. the main consequence of this result is that modal rational entailment does not, in general, produce a consequence relation which is rational."
"p : the rationale of this partial order is as follows: the utility company selling the electricity generated by the power plant tries as far as possible to keep both the pile and the cooling system on, ensuring that the pile can be easily switched off (states s 1 and s 2 ); sometimes the company has to switch the pile off for maintenance but then tries to keep the cooler running, preferably if turning the pile on again does not cause a fault in the cooling system (state s 3 ); more rarely the company needs to switch off both the pile and the cooler, e.g. when the latter needs maintenance (states s 4 and s 5 ); in an exceptional situation, turning the pile on may interfere with the cooler switching it off (state s 6 ); and, finally, only in very exceptional situations would the pile be on while the cooler is off, e.g. during a serious malfunction (states s 7 and s 8 )."
"the typical abc algorithm used the random searching and selecting methods through employed, onlooker and scout bees, unfortunately no global best solution, best-so-far, local, best or mutation method use to improve and balance the exploration and exploitation process successfully. based on random searching ways, abc has global search capability, but poor local search capability. in order to enhance the exploitation capability of the abc algorithm through candidate solutions like employed bee. different improved version of typical abc have been developed for enhancing and balance exploration and exploitation process by introducing different operators, strategies and operators. gbest guided artificial bee colony (gabc) algorithm is one of the attractive modified bio inspired algorithm developed to increase the performance of typical abc [cit] . equation (2) has been modified by the equation (7) to direct the search path towards global optima."
"3) profitability of the transformation: the profitability of the tiling hyperplane method is complex to assess in its general formulation, as it is characterized by the profitability of loop fusion and the impact on subsequent vectorization. our technique has removed the profitability estimate of outer-loop fusion/distribution, since we empirically evaluate all possible choices. in addition, we rely on a second stage dedicated to expose inner loops which are good vectorization candidates. so the problem of the profitability of the tiling hyperplane is reduced to the effectiveness of maximizing data locality in a given class, while outer-parallelism and vectorizable loops are made independent to the problem. technically, one should consider the profitability of multi-level statement interleaving to guarantee that each possible loop structure is evaluated in order to find the best one, trading parallelism and locality at each loop level. however, focusing only on the outer level carries the most important changes in parallelism and communication possibilities. in our optimization algorithm, we chose to systematically apply the tiling hyperplane method on each class of the partition."
"scalability of services is a challenging aspect of distributed application processing in mobile cloud computing. the traditional local dapfs [cit] for remote application processing are deficient in centralized management of the distributed platform. a challenging issue in local dapfs is the unavailability of centralized resources. for example; in the scenario of unavailability of remote service provider, remote services become inaccessible which hinders the objectives of availability of services in distributed computing paradigm. similarly, local resources are accessible to limited number of mobile devices in the local environment. therefore, yes heterogeneity of mobile device architecture and operating system platforms."
"most internal representations used in compilers match the inductive semantics of imperative programs (syntax tree, call tree, control-flow graph, ssa etc.). in such reduced representations of the dynamic execution trace, a statement of a high-level program occurs only once, even if it is executed many times (e.g., when enclosed within a loop). this is not convenient for optimizations that need a granularity of representation reflecting dynamic statement instances. for example, transformations like loop interchange, fusion or tiling operate on the execution order of statement instances [cit] . in addition, a rich algebraic structure is required when building complex compositions of such transformations [cit], enabling efficient heuristics for search space construction and traversal [cit] ."
"it has the highest refining capacity among the opec producers and has plans to add 1.2 million bbl/ [cit] to the current refining capacity of crude oil of 2.9 million bbl/d. [cit], the top three crude oil producing countries were saudi arabia (10.46m b/d), russia (10.29m b/d) and the united states (8.88m b/d) [cit] . it's come in twelfth largest primary energy consumer in the world like around 1 million bbl/d of oil is used for electricity generation during the heat waves season [cit] ."
"given a partitioning of the program statements, the second step is to perform aggressive, model-driven optimizations that respect this partitioning. we first discuss the computation of a sequence of loop transformations that implements the partitioning, and produce tiled parallel code when possible in section v-a. we then present in section v-b our approach for model-driven vectorization in the polyhedral model."
"mf is a poor indicator for other reasons as well like low diagnostic sensitivity, especially in low prevalence settings, and community disapproval of the skin snip procedure, resulting in a general movement away from the use of mf as an indicator and toward the application of vector and serology-based indicators for evaluating onchocerciasis elimination 32, 42, 43 . however, identifying reliable vector and serology thresholds is still a key issue that needs to be resolved 43 . the vector thresholds recommended by who was initially set at a value of 20 atp per site 27 . our results on atp threshold values (table 1), however, have provided three important insights regarding the applicable vector threshold values in a site. first, as in the case of the corresponding mf breakpoint values, vector-based elimination thresholds (here atp) will vary significantly between sites owing to variations in local transmission conditions. second, the relevant atp threshold values may be different from the global value of 20 set by who. third, if vc is not implemented, the atp thresholds at abr may be significantly lower than the globally-set value of 20 but these will increase to 16-91 across sites if vc is used (table 1 ). these are important results, and highlight: (1) that these thresholds are not spatially stationary but are properties of heterogeneous transmission dynamics, and (2) that currently set targets require revaluation."
"our approach to program optimization decouples the search of the program structure from the application of other performance-enhancing transformations, for example, for locality improvement and vectorization. the first step of our optimization process is to compute all valid partitions of the program statements, such that a class of this partition corresponds to a set of fusible statements: those statements that share at least one common loop in the target code. in the second step, for each valid partitioning, we apply model-driven optimizations individually on each class of the partition in a systematic fashion. these optimizations may lead to complex compositions of affine loop transformations (skewing, interchange, multi-level distribution, fusion, peeling and shifting). part of the sequence is computed in the polyhedral abstraction to create outer loop(s) parallel and permutable when possible, optimizing for data locality; details are provided in section v-a. then, it is further modified in order to expose parallel inner-loops with a minimal reuse distance to enable efficient vectorization; this is covered in section v-b."
mcc utilizes cloud storage services [ [cit] for providing online storage and cloud processing services for augmenting processing capabilities of smds [cit] . processing capabilities of smds are augmented by outsourcing computational intensive components of the mobile applications to cloud datacenters. the following section discusses the concept of augmenting smartphones through computational clouds.
"diverse objective functions are considered; saving processing power, efficient bandwidth utilization, saving energy consumption, user preferences, and execution cost. objective of all approaches is to augment the application processing potentials of resources constrained smds. we conclude that current dapfs for mcc are the analogous extensions of traditional cyber foraging frameworks for pervasive computing or local distributed platforms. hence, current dapfs are deficient in the deployment of distributed system standard architectures. as a result, additional complications arise in the development, deployment and management of distributed platform. current frameworks focus on the establishment of runtime distributed platform which results in the resources intensive management overheads on smds for the entire duration of distributed platform. smds exploit computing resources in arbitration with cloud servers for the selection of appropriate remote node, dynamic assessment of smds resources consumption and application execution requirements at runtime, dynamic application profiling, synthesizing and solving for application outsourcing, application migration and reintegration and rigorous synchronization with cloud servers for the entire duration of distributed platform. as a result, additional computing resources of the smds are exploited for the runtime orchestration of distributed platform. therefore, current distributed application deployment algorithms employ heavyweight procedures for distributed application deployment and management."
"the previous study shows that the nn performance can be improved through the selection of suitable structure, activation function, appropriate numbers of input pattern and of course learning algorithms. besides the common applications, ann tools are very effective for financial, stationary and nonstationary, meteorological, natural hazards, stock values and crude oil price time series data prediction. for two decades, ann tools are famous, effective and attractive for prediction time series dataset including crude oil prices, bottom hole pressure in vertical multiphase and stock exchange values [cit] ."
"there has by now been quite a substantial number of attempts to incorporate defeasible reasoning in logics other than propositional logic. after a first tentative exploration of preferential predicate logics by lehmann and magidor [cit], some more recent investigations have attempted to define notions of defeasibility in deontic logics [cit], and of defeasible subsumption for description logics [cit] . nevertheless, a generally accepted semantics for preferential reasoning in modal logics, with a corresponding syntactic characterization, does not yet exist."
"this paper addressed the problem of optimizing and parallelizing programs automatically, focusing on static control loop nests. our approach departs from the traditional besteffort compiler optimizations, aiming for performance portability across a variety of shared-memory multiprocessors. we proposed a combined iterative and model-driven approach, leveraging a state-of-the-art parallelization method based on loop tiling, and combining it with a novel feedback-directed scheme for loop fusion and distribution."
"we report also the number of possible (including invalid) partitions as #part., and the number of semantics-preserving partitions #valid to highlight the pruning factor enabled by our algorithm. we also check the column variability each time we observed a 5% or more difference between the best versions found for a platform and its execution on the other platforms, this to emphasize the requirement for a tuning of the partitioning selection. finally, we also report the dataset size used for the benchmarks (pb. size)."
"maui is a cloud server based dynamic partitioning framework which considers energy saving on smd as the main objective function for offload processing. maui masks the complexity of remote execution from mobile user and gives the notion as the entire application is being executed on smd. the framework is based upon method state migration as a substitute of method code migration. maui copes with the mobility of the mobile user and provides optimized solution periodically to adapt to the changes in network and user location. the critical aspect of maui is the dynamic partitioning of the application at runtime which activates the profiler and solver component dynamically to determine execution point for application partitions. development of the applications on the basis of maui requires additional developmental efforts for annotating the execution pattern of each individual method the application. maui deploys full proxies of the application on both smd and cloud datacenter. maui obliges the overhead of dynamic application profiling, solving, partitioning, migration, and reintegration on smd."
"bio inspired agents have motivated many researchers to develop various mathematical and computational approaches for solving complex optimization problems [cit] . theoretically, it is because of their unique movements of searching, gathering, sharing, dancing, foraging, selection, flying, managing, building, communication, social, emotional, their foraging behavior, their mating and reproduction behavior, their pheromone laying behavior and navigation behavior, self-adapting and self-organizing characteristics [cit] ."
"the significance of this is that the representation result is proved with respect to the same set of properties used to characterize propositional preferential consequence. we therefore argue that our definition of preferential models provide the foundation for a semantics for preferential (and rational) consequence for a whole class of multi-modal logics. we do not claim that this is the appropriate notion of preferential consequence for all modal logics, but rather that it describes the basic framework within which to investigate such notions."
"as mentioned in section 3, casini and straccia [cit] recently proposed an algorithm for determining rational closure in the context of description logics. their algorithm can be adjusted to determine the modal rational closure as defined in definition 3.13. from the description of this algorithm it follows that, for the modal logics under consideration here, determining membership of the modal rational closure is no harder than global entailment checking. furthermore, the algorithm can readily be extended to implement refinements presented by presumptive reasoning [cit], and inheritance networks [cit] . we are currently investigating further optimizations of the algorithm via suitable definitions of modularity for defeasible knowledge bases [cit] ."
"the standard abc algorithm is a unique bio inspired algorithm inspired through the attraction and natural foraging behaviors of honey bees. it has been successfully used for solving different statistical, mathematical, science and engineering problems. the exploration and exploitation are the famous process of abc by employed and unemployed bees [cit] ."
"in this work, we have organized the task of optimization and automatic parallelization of a loop nest as an iterative, feedback-directed search. each iteration of the search is further decomposed into two stages: 1) choosing a partition of the program statements, such that statements inside a given class can share at least one common loop in the generated code; 2) on each class of this partition, applying a series of model-driven affine loop transformations: (a) a tilingbased optimization and parallelization algorithm; (b) a vectorization-based algorithm. our approach differs significantly from previous work using iterative compilation to search for an affine multidimensional schedule [cit] in that we do not require an empirical search of the entire set of sequences of transformations. instead, we limit the search only to the part for which no robust performance models have been derived, but rely on wellunderstood cost models for the other transformations. we aim for a substantial reduction of the search space size while still preserving the ability to explore the most important set of candidate transformations."
"the remainder of this paper is organized as follows: in section 2 we briefly recap the seminal work by lehmann and colleagues on propositional preferential and rational consequence. we then present an account of modal preferential reasoning in section 3. importantly, our representation results are with respect to the corresponding propositional properties or rules, and methods employed in a propositional non-monotonic setting therefore translate seamlessly to a modal context. this includes reasoning tasks such as computing the preferential or rational closure of a defeasible knowledge base [cit] . our modal semantics therefore forms the foundation of preferential consequence for a whole class of modal-based formalisms, which we illustrate with some case studies, namely modal logics of action (section 4) and knowledge (section 5). after discussing some related work (section 6), we conclude in section 7."
"a limitation to our study is that the modelled site-specific breakpoints and the who-proposed thresholds for onchocerciasis have not yet been sufficiently validated. our modeling results, in agreement with others, indicate that breakpoints vary by endemicity and are lower than the operational thresholds currently used by global programmes (table 1) 29, 32, 45 . undoubtedly, this indicates that there is an urgent need for reevaluating and confirming the criteria used for determining whether transmission has been interrupted 43, 45 . however, note that the key conclusion of our study, viz. that adding \"slash and clear\" vc to mda will significantly reduce timelines to eliminate onchocerciasis transmission compared to using mda alone, will not change but the actual timelines predicted would (compare the durations of interventions needed for meeting the model-presicted thresholds versus the who threshold given in table 2 ). future refinements of our model are also needed with respect to modeling seasonality and serological indicators. in this first development, we modelled the mbr as a function of monthly rainfall to capture seasonality and predict the impact of future shifts in seasonal patterns. this model was based on one year of rainfall data and should be updated with future and possibly historical rainfall data. other relevant environmental factors that impact biting rate could also be incorporated, such as temperature 46 . additionally, as the who has adopted serology indicators, it will be important to extend our model to consider this new indicator. finally, we have modelled the efficacy of the vegetation \"slash and clear\" approach based on impact on mbr from only three experimental field sites. as the \"slash and clear\" efficacy parameters may depend on local vegetation and other riverine features, further field studies of the impact of this and indeed other approaches for reducing vector populations and hindering their regrowth based on vegetation clearance should be carried out in relevant settings to provide more reliable estimates of efficacy, and to determine how it may vary between diverse ecologies."
"(we note that specifying a solution to the frame problem is beyond the scope of this paper. instead, we refer the reader to the solutions provided by the abovementioned frameworks which can, in principle, be integrated into the present formalism in a straightforward way.) k basically says that \"a hazardous situation is one in which the pile is on and the cooler off\", \"a hazardous situation may lead to a malfunction\", \"if the pile is on, then flipping switches it off\", and \"one can always flip the pile switch\". we can then conclude"
"the traffic capture shown in fig. 6 validates the use of the cop. first, we can observe the request for virtual machine (vm) creation from vimap toward the cloud controller (which is running on the same server). the creation time for a single vm is 15 s, which includes the necessary time to boot up the vm. second, in fig. 7 we can observe the call request (call id: 1) from the vimap toward the multidomain sdn orchestrator (based on abno). in the call service request, several constraints can be observed, such as the requested end points (aend, zend), several traffic parameters (such as requested bandwidth), the requested transport layer, and the mac addresses of the interconnected vms. the abno computes the necessary domain call requests and sends them toward the as-pce for the optical domain (call id: 00002, 00005), sdn controller 1 (call id: 00001, 00006), and sdn controller 2 (call id: 00003, 00004). the multidomain call service setup delay is of 2.52 s."
"mobility is an important attribute of smds. mobile users enjoy the freedom of computing and communication on move. however, a number of obstacles hinder the goals of seamless connectivity and consistency in the distributed platform of mobile applications; for example handoffs, traveling with high speed, diverse geographical locations and different environmental conditions. as a result, providing seamless connectivity and uninterrupted access to the centralized cloud datacenters in distributed application processing is a serious research issue for mcc. it is important that distributed application model provide versatile access to cloud resources and services on move with ubiquitous attributes and high degree of transparency. however, it is challenging to ensure the transparency of distributed environment. in particular to smd, the issues and limitations in wireless medium hinder the transparency goals of distributed processing of mobile application. the seamless and transparent deployment of distributed platform for computational intensive applications is a challenging aspect for mobile cloud computing. it is mandatory for distributed model to mask the complexities of distributed environment from mobile user and give the notion as the entire application is being processed locally on smd. similarly, it is important to ensure successful execution of remote processing and returning results to smd. sustaining consistency of the offloaded components of the application with lightweight implementation procedures is a challenging aspect of dapfs. consistency is an issue for the components offloaded at runtime [cit], the replicated applications using proxies [cit], and transactions involving related updates to different objects. it is important that the distribution and replication of intensive mobile applications and data should be transparent to the mobile users and application running client device. cloud based distributed processing of mobile application are required to fulfill atomic, concurrency, isolation and durability (acid) properties of the distributed systems. it is challenging to provide location transparency, replica transparency, concurrency transparency, and failure transparency in cloud based application processing of mobile applications."
"artificial neural network (ann) often called a \"neural network\" or simply neural net (nn) inspired by biological system is an interconnected set of artificial neurons that uses a mathematical model or computational model for information www.ijacsa.thesai.org processing based on external or internal information that flows through the network [cit] . the biological neuron structure is given in fig. 4, which have different functions terminologies such as dendrites, which receive activation from other neurons, some processes which converts incoming activations into output activations, axons act as communication route, synapses, neurotransmitters and nucleus. this field goes by many names, such as parallel distributed processing, neurocomputing, natural intelligence systems, machine learning algorithms, bio inspired learning methods, deep learning and multilayer perceptron (mlp), and others soft computing methods [cit] ."
"however, these models used the traditional machine learning algorithms, rely on a fixed set of training data to train a machine learning model and then apply the model to a test set, but may not be effective for non-stationary time series data such as oil price data and other nonlinear complex dataset. from the above-mentioned approaches, that ann model is more sufficient and effective than standard models; the performance can be easily increased rapidly through robust and efficient learning algorithm [cit] . bio-inspired methods are robust and attractive optimization algorithm especially for solving nonlinear complex problems [cit] . the previous histories of softcomputing methods for crude oil prices prediction are summarized in table i ."
"the attribute of migration pattern represents the mechanism of transfer intensive applications to remote server nodes at runtime. current dapfs implement the mechanism of application offloading in a number of ways. the following migration patterns are practiced for traditional application offloading frameworks. a) application transfer is a migration pattern in which the binary code of the application is outsourced to remote server [cit] . b) url download based migration pattern represents a migration pattern in which a url is provided to remote host and application is downloaded from that url as a substitute of transferring the application directly from smd [cit] . c) vm instance represents a migration pattern in which the application is encapsulated in vm instance (partially or entirely) and the vm instance is migrated to remote server. a fresh vm instance is created on the remote server and guest vm instance is copied to the freshly created vm on remote server [cit] . d) umsc is a migration pattern in which mobile agent is employed for the migration of outsourcing application [cit] . umsc serves as a courier for the migration of the application between smd and remote server. e) module/bundle transfer represents a migration pattern in which binary files of the modules of the application are migrated to remote servers [cit] . f) application proxy is a migration pattern in which entire proxies of the application are maintained on the local smd and remote cloud server node [cit] . g) object transfer is a migration pattern in which entire object is outsourced to remote server at application level [cit] . the attribute of security support represents the security provision attribute of the dapfs. security is an important parameter for application offloading frameworks. current dapfs which implement security mechanisms for application offloading are represented with the value yes, whereas the frameworks which lack in the provision of security mechanism for application offloading are represented with the value no. a number of current dapfs required developers support for defining execution scope of the components of application at different granularity level. such approaches restrict application developers to classify and annotate the components of mobile application as local or remote. the attribute of ds shows the requirement of additional support required for the development of the application. therefore, the traditional application offloading models which require developer support [cit] are represented as required, whereas the frameworks which do not require developers support [cit] are represented as not required. the attribute of execution management shows the management policy for the deployment and management of runtime distributed application platform. current dapfs are classified in two categories from the perspective of execution management. decentralized management represents the deficiency of centralized mechanism for the deployment and management of distributed platform [cit] . therefore, smds are responsible for monitoring distributed platform and distributed application execution. therefore such frameworks results in larger overhead of the distributed application deployment at runtime. centralized management represents that a centralized management and monitoring mechanism is provided for the establishment of distributed platform and monitoring of application execution [cit] . the following table shows the comparison of local dapfs for smart mobile devices. localized application offloading frameworks employ decentralized monitoring approach for process offloading which results in the extensive involvement of smds for the management of distributed processing. further, local offloading frameworks are deficient in the centralized management and the availability of resources for the provision of remote services. in the scenario of unavailability of local remote service provider, remote services become inaccessible which hinders the objectives of availability and scalability of services in distributed computing paradigm. to cope with the issues of decentralized dapfs centralized server based solutions are exercised. the server based resources utilization models utilize computing resources and services of centralized servers. the availability of centralized servers lessens the monitoring overhead on smd in the deployment and management of runtime distributed platform. further, centralized servers assist in ensuring the availability of resources and services. the server based dapfs implement application offloading by employing the following three different types of centralized computing resources utilization approaches. a) grid server based distributed platform in which remote services are provided by grid servers [cit] . b) telecommunication service provider (tsp) based distributed platform in which remote services are configured at tsp servers [cit] . c) a cloud datacenter based distributed platform is established by leveraging computing power potentials of cloud datacenters. distributed computing services are provided through service providers by employing the vision of utility computing paradigm [cit] . server based application offloading frameworks accomplish outsourced application processing in diverse modes. several approaches exploit vm cloning; others focus on part(s) of the application to be offloaded. a number of approaches implement dynamic application partitioning whereas other focus on entire job migration. diverse objective functions are considered; saving processing power, efficient bandwidth utilization, saving energy consumption, user preferences, and execution cost. table 3 compares server based offloading frameworks in which centralized resources are available for the provision of remote services."
"each employed bee search around the food source, gathering required information about its quality and position of the onlookers. then, they carry the information about the position of food source back to the hive and share this information with artificial onlooker bees by dancing in the nearby hive. onlooker bees: the onlookers tend to choose the best food sources to further exploit, based on information communicated by the employed bees through their dances. therefore, good food sources attract more onlooker bees compared to the bad ones. the artificial onlooker bees choose the best food source with better quality based on information communicated from those found by employing bees using different ways, such as a probability selection mechanism, greedy selection, fitness function as a proportional of the quality of the food source. the last bees processes are managing by the scout bee group is responsible for the exploration process randomly chose a new good food source to replace the old one. the number of food sources (based on position and quality) which represents a possible solution to the optimization problem and fitness of the associated solution is equal to the number of employed bees and also equal to the number of onlooker bees. the employed, onlooker bees used for exploitation process for a given problem towards best solution space given in equation (2), while scout bees equation (3) for exploitation process."
"in this section, we present the vimap architecture, including the description of its main building blocks, which are shown in fig. 2 . the vimap has been designed to provide coordinated orchestration of network and cloud resources distributed among different cloud providers and locations. the vimap provides per-tenant programmability of its own dedicated resources, it performs the partitioning of the underlying infrastructure, exposing an abstracted view of virtual infrastructure slices to each tenant."
"these bio inspired artificial agents or methods have successfully applied to various applications and linear, nonlinear complex problems such as classification, clustering, time series prediction, numerical function optimization and other combinatorial problems [cit] . the performance of these algorithms depends on exploration, exploitation, balancing, convergence and global optimum position. the typical abc easily trapped in local minima which lead to slow convergence. beside abc, the gabc and best-so-far abc is the modern improved examples in success history as mentioned in the above sections [cit] ."
"condition (i) ensures that the number of iterations that are peeled from the loops is not greater than c; it implies that the remaining iterations of r and s will be fused under a common loop. 1 . since schedule coefficients are restricted to be non-negative, c is simply the difference between the constant parametric parts of the schedules. technically, c is only an estimate of the number of unfused instances, which serves the purpose of this paper. determining the exact number of fused instances requires one to resort to complex techniques such as counting the number of points in parametric polyhedra [cit] . condition (ii) ensures that the schedule row k has non-null values for the coefficients attached to the loop iterators, that is, (ii) ensures that θ r k and θ s k are not constant schedules. this condition is required to guarantee that θ r k and θ s k represent an interleaving of statement instances in the target code, and not simply a case of statement interleaving."
"initially, it was developed for solving linear problems; later on it has been extended to different models such as multilayer feedforward neural network (mffn), recurrent neural network, probabilistic neural network, pi sigma neural network for solving different optimization problems [cit] . mffn is a famous among the various nn structure most commonly used due to its lower complexity and ability to produce satisfactory results for different problem domains as given in fig. 6 ."
"the searching strategy of employed bees given by equation (7) will increase the exploitation around the, current best ever food source through the guided strategy to enhance the real foods source position. these food source positions will be shared with the best so far onlooker and scout bees which will further select the most appropriate position for the given problem. in order to accelerate the convergence speed of the gbabc algorithm, the idea of focusing the search around the current best ever food source gbest, best so far, the best fitness formula was proposed. these bees will repeat their intelligence procedures until a predetermined maximum number of cycles (mcn) or the best food source achieved so far. of course, the enough and balance exploitation and exploration process will successfully escape local optima trapping and slow convergence difficulties. the proposed gbabc method has been used to train the feed-forward neural networks for the crude oil prediction purpose; the details are added in the following section. beside the proposed gbabc approach, the main difference in the implementation phase which is executing based on max cycle numbers instead of max fitness number of evaluations, which are used by the typical abc, ggabc, best-so-far abc and so many others algorithms. table) /*gbest guided employed bees' phase*/ for (each gbest guided employed bee) produce a new food source following the eq. (7); evaluate the fitness of the new food source; calculate the fitness of the explored food source best so far employed bees."
"empirical analysis ascertains the significance of distributing processing loads to remote server nodes [cit] . however, the deployment of distributed application processing platform is obstructed by a number of unresolved challenges for mcc. the latest offloading models [cit] focus on the establishment of dynamic distributed application processing platform at runtime. for the selection of cloud server node, smds arbitrate with cloud server node dynamically at runtime. therefore, the configuration of distributed processing platform at runtime is a resources starving and energy hunting mechanism. dynamic runtime offloading involves the issues of dynamic application profiling and solver on smd, runtime application partitioning, migration of intensive components and continuous synchronization for the entire duration of runtime execution platform. therefore, the development and deployment of intensive mobile applications on the basis of current algorithms is still a challenging research issue."
"vm based cloudlets framework [cit] differs from cyber foraging [cit] by migrating image of the running application to the explicitly designated remote server. a cloudlet is a trusted resource rich computer or cluster of computers that is connected to internet and is accessible for smds. mobile device serve as a thin client providing only user interface whereas actual application processing is performed on the cloudlet in distributed environment. the proposed framework is based upon transient customization of cloudlet infrastructure using hardware vm technology in which vm encapsulates and detaches the temporary guest software environment from the cloudlet infrastructures permanent host software environment. the framework employs variant procedures for vm migration. the critical aspects are that the framework requires additional hardware level support for the implementation of vm technology and is based on cloning mobile device application processing environment to remote host which involves the issues of vm deployment and management on smd, privacy and access control in migrating the entire execution environment and security threats in the transmission of vm."
"elastic mobile applications are attributed with the following features [cit] . a) ad-hoc platform creation is an important attribute of elastic mobile applications. distributed application processing platform is established on ad-hoc basis at runtime in which elastic mobile application is partitioned dynamically and computational intensive components are migrated to remote server nodes. mobile clients dynamically arbitrate with cloud servers or surrogates to determine appropriate server node for remote application processing. b) elastic applications are designed in such a manner so that computational intensive components of the mobile application are separated dynamically at runtime. applications are partitioned at different granularity level depending upon the design and partitioning policy of the offloading algorithm. c) adaptive offloading of the intensive components of the applications is a significant attribute of elastic mobile applications. partitions of the application are offloaded to remote machines for remote execution which augments the computing capabilities of smds. application offloading occurs whereas keeping in view different objective functions; such as energy saving, processing power, memory storage, and fast execution. d) transparency in the distributed execution platform is a significant attribute of elastic applications. transparency assures that elastic mobile application executes transparently on remote surrogates/server nodes. a transparent distributed processing environment gives the notion as entire application is being executed locally on smd. all the complexities of remote execution are concealed from mobile users. researchers determine applications offloading as an appropriate software level solution for alleviating resources limitations in smds."
"the following section describes the generic sequence of operations for vm migration based application offloading. 1) the first step for application offloading is to arbitrate for appropriate surrogate or remote server host. subsequently, the running application is encapsulated in vm on smd which involves the creation of vm instance, vm configuration for running application and encapsulating all the state information of running application in vm instance. 2) vm instance is migrated to the remote server through wireless medium. vm encapsulates either entire application or partition of the application. 3) a new vm instance is created on remote server and the migrated vm is cloned onto the newly created vm instance on remote server. running states of the application are resumed and application is executed on remote server host. finally, results are returned to the smd. 4) remote server ensures complete isolation of guest vm which means that the executing environment of guest vm is prevented from interference. fig.5 shows abstract level flowchart of vm migration based application offloading."
"realizing the high levels of potential performance on current machines is a very difficult task. one of several approaches to addressing this challenge is to develop compiler transformations aimed in particular at loops. this requires a compiler to be able to apply complex sequences of loop transformations and effectively model the effect of hardware resources and the complex ways in which they interact. modeldriven optimization heuristics used in current research and production compilers apply a restricted subset of the possible program transformations, thereby limiting their effectiveness. the problem becomes further complicated when one aims for performance portability over a broad range of architectures."
"the state-of-the-art in tiling and parallelization in the polyhedral model [cit] relies on an analytical approach. unfortunately, a purely analytical approach is not sufficient since it is difficult to adequately account for several high-impact factors such as cache conflicts, memory bandwidth and vectorization. it is important to adapt the optimization strategy to a target architecture; in addition, it is important to achieve portable performance, requiring understanding and management of the interplay between scalability, locality and synchronization overhead on different target machines."
"the vimap includes a dedicated configuration interface for slice provisioning, which is exposed to oss/nms management systems through a restful api. the vimap logic component is responsible for orchestrating the workflows among the different architectural components in order to provision the cloud and network resources for an upcoming request. it is responsible for performing context-aware orchestration, exposing to each tenant only those resources allocated to the tenant by means of virtual representation. it includes an nbi, which exposes the custom set of vimap programmable resources to each tenant."
"program optimization in a polyhedral model is a three stage process. first, the program is analyzed to extract its polyhedral representation, including dependence information and access pattern. for all textual statements in the program, for example r in figure 1, the set of its dynamic instances is captured with a set of affine inequalities. when the statement is enclosed by loop(s), all iterations of the loop(s) are captured in the iteration domain of the statement. considering the 2mm kernel in figure 1, the iteration domain of r is: the sets of statement instances between which there is a producer-consumer relationship are modeled as equalities and inequalities in a dependence polyhedron. this is defined at the granularity of the array cell. if two instances x r and x s refer to the same array cell and one of these references is a write, then they are said to be in dependence. hence, to respect the program semantics, the transformed program must execute x r before x s . given two statements r and s, a dependence polyhedron, written as d r,s contains all pairs of dependent"
"2) static cost model: infinitely many schedules satisfy the criterion (1) for legality of tiling. as a second objective, to achieve good temporal locality we seek a schedule that minimizes the hyperplane distance between dependent iterations [cit] . for code with affine dependences, the distance between dependent iterations can always be bounded by an affine function of the global parameters n."
"the form u. n+w is an upper bound on the distance between all dependent iterations, and thus directly impacts single-thread locality as well as communication volume in the context of parallelization. it is thus desirable to seek transformations that minimize it. the legality and bounding function constraints from (1) and (2)"
application deployment table 4 summarizes challenges to current dapfs and open research issues in cloud datacenters based distributed application processing. issues indicate the unresolved problems in current dapfs whereas challenges indicate the issues of research in distributed application processing for mcc that remain to be addressed. the following section discusses issues in current offloading frameworks and identifies challenges to the cloud based application processing of resources intensive mobile applications.
"for each class of the partition (i.e., each group of fused statements), several goals are achieved through this cost model: maximizing coarse-grained parallelism, minimizing communication and frequency of synchronization, and maximizing locality [cit] . since outer permutable bands are exposed, multidimensional tiling can be applied on them. tiles can be executed in parallel or with a pipeline-parallel schedule. in the generated programs, parallelization is obtained by marking transformed parallel loops with openmp pragmas."
"the framework proposes a security mechanism for ensuring the integrity of communication between smd and cloud server [cit] . upon downloading weblet on smd, the integrity of each weblets is ensured by the installer of the device by recomputing hash value for each weblet and comparing it with the hash value stored in the weblet. the installer registers the application with device elasticity manager (dem). the dem maintains a table of installed applications on the device which need elasticity manager support. the table maintains detailed information about weblets such as signed hashed values and migration settings. several parts of the elastic application are installed on cloud elasticity service (ces). ces maintains installed applications for users. for this purpose users register with ces and authenticate with ces during installation. the cloud based application manager is able to download the application code from an application store instead of uploading from mobile device. the node manager executes the weblet binary provided by application manger. the local weblet can query dem to obtain the list of all active weblets in the same session. the local weblet can broadcast the urls returned by dem to any other weblet that needs to communicate."
"the cop has been employed as a transport api for the orchestration of two sdn opendaylight helium controllers responsible for controlling the ethernet intra-dc domains via openflow 1.3, and the optical transport network via an as-pce with instantiation capabilities as a single interfacing point for the gmpls control plane. in the experimental validation, we have introduced cop agents on top of sdn controllers in order to translate the received cop commands to sdn controller nbis. figure 5 (a) shows a multidomain network scenario where two geographically distributed dcs are interconnected through the wson. figure 5(b) illustrates the integrated it/sdn orchestration workflow for the on-demand deployment of two vms in the cloud (one on each dc location) and the e2e connectivity provisioning across the proposed scenario. the network orchestration is performed using the proposed cop between the sino-mso and, consequently, between the mso and the per-domain controllers. for this experimental validation, a bidirectional call_service is requested by the sino to provide an e2e connectivity to the previously deployed vms. the mso first requests the creation of a virtual link in the upper layer topology (l2), which is translated internally by the vntm mso module into two unidirectional l0 call_services sent to the as-pce through the provisioning manager. they trigger, in the as-pce, the creation of the corresponding gmpls connections (label switched paths [lsps] )."
"the current dapfs for smds employ a number of strategies for the establishment of runtime distributed application execution platform. this section provides thematic taxonomy for current dapfs and reviews the traditional dapfs on the basis of framework nature attributes of the taxonomy. further, it investigates the advantages and critical aspects of current dapfs for smds."
"a key area of mobile computing research focuses on the application layer research for creating new software level solutions. application offloading is an application layer solution for alleviating resources limitations in smds. successful practices of cloud computing for stationary machines are the motivating factors for leveraging cloud resources and services for smds. cloud computing employs different services provision models for the provision of cloud resources and services to smds; such as software as a service, infrastructure as a service, and platform as a service. several online file storage services are available on cloud server for augmenting storage potentials of client devices; such as amazon s 3 [cit], google docs [cit], mobileme [cit], and dropbox [cit] . in the same way, amazon provides cloud computing services in the form of elastic cloud compute. the cloud revolution augments the computing potentials of client devices; such as desktops, laptops, pdas and smartphones. the aim of mcc is to alleviate resources limitations of smds by leveraging computing resources and services of cloud datacenters. mcc is deployed in diverse manners to achieve the aforementioned objective. mcc employs process offloading techniques [cit] for augmenting application processing potentials of smds. in application offloading intensive applications are offloaded to 1553-877x/13/$31.00 [cit] ieee remote server nodes. current offloading procedures employ diverse strategies for the deployment of runtime distributed application processing platform on smds. a challenging issue in current dapfs (distributed application processing frameworks) is the additional computing overhead on smds in the deployment and management of runtime distributed application execution. this paper reviews current dapfs in smds within mcc domain and identifies challenges in the cloud based processing of mobile applications. we classify existing dapfs by thematic taxonomy and investigate commonalities and deviations in such frameworks on the basis of significant parameters such as offloading scope, partitioning approach, migration support, migration granularity, application developer support, migration pattern and execution monitoring. the contribution of the paper lies in the categorization of frameworks on the basis of thematic taxonomy, analysis of current dapfs by discussing the implications and critical aspects, identifying the issues in existing solutions for offload processing and challenges to cloud based application processing of mobile applications. the listing of challenges and open issues guide researchers to select the appropriate domain for future research and obtain ideas for further investigations."
"our objective is to model the search space that contains all possible partitions of a program, such that statements in the same class can be fused under a common outer loop. in addition, we also require the class identifier to reflect the order in which classes are executed, to model code motion. a general framework for this purpose has been developed by pouchet [cit] in the context of multi-level partitions using arbitrary scheduling coefficients. however in the context of the present work we limit ourselves to the modeling of fusible statements at the outer loop level only, a restriction of the general case."
"for the next steps in this research, we expect to extend the work done for the vmg problem definition for the development of more complex algorithms, which exploit cross-resource optimization of cloud and network domains."
"multiple dependence polyhedra may be required to capture all dependent instances, at least one for each pair of array references accessing the same array cell (scalars being a particular case of array). hence it is possible to have several dependence polyhedra per pair of textual statements, as some may contain multiple array references."
"this section compares current dapfs on the basis of taxonomy presented in fig. 4 . we classify application offloading frameworks in two categories: (1) local resources utilization frameworks and (2) table 2 compares local application offloading frameworks, whereas table 3 compares server based application offloading frameworks on the basis of such parameters. the offloading scope a application offloading framework indicates the scope of distributed platform established at runtime. current dapfs implement application offloading by accessing the services of local computing devices or remote cloud server nodes. in the local resources utilization model the services and resources of the local computing nodes are utilized. the traditional local dapfs implement application offloading by employing the following three different types of decentralized local computing resources utilization approaches. a) surrogate based distributed model is composed of remote servers which are accessible in the local environment. the surrogate servers can be stationary or mobile remote computer which is accessible in the local environment of smd. mobile device is enabled to select an appropriate surrogate at runtime for application offloading. in such model a centralized monitors the establishment of distributed platform and provision of computing resources [cit] . b) mobile devices based distributed platform is a virtual or ad-hoc cloud computing model in which distributed smds in the close proximity participate in resources sharing [cit] . the peer smds are enabled to share computing resources and provide remote services. in such an environment, sharing of the computing resources and services is restricted to the computing capabilities mobile devices and services shared by smds involved in the virtual cloud environment. the unavailability of centralized mechanism for the establishment and management of virtual cloud is a challenging aspect of virtual cloud model. c) in centralized server based mobile devices distributed platform, the computing services are provided by mobile worker nodes. however, a centralized server monitors the established and management of distributed application execution platform [cit] . in such a distributed computing model, distributed resources and services provision are restricted to the computing potentials and services of worker nodes (smds). fig. 8 highlights different models employed for the distributed application processing platforms in application offloading."
"using the above bio inspired learning techniques explore the best values of weights of each connection in order to reduce the training error for the crude oil prices prediction task. after repeating this process for a sufficiently large number of learning cycles the network will usually converge to any state, where the error of the calculations is small with high predicted performance. the prediction curves on 75 % dataset during training phase are given in fig. 10, 12 in case of abc, gabc and best-so-far methods, the predicted signal are not close and stable to original; these methods did not success to predict the future crude oil prices as shown in fig. 10, 11 and 12, respectively. however, fig. 13 and 15 are clearly showing that crude oil prices predicted by the gbabc algorithm are very close to actual oil prices. the explored weight values have used to test gbabc performance on ffnn with the rest 25% dataset of crude oil price. the best average crude oil prediction curves out of samples have presented in fig. 14 and 15 . again, the proposed gbabc algorithm predicted prices are very close to the original crude oil prices with different ffnn structures. from the above-mentioned all tables values and figures, the performance of the proposed gbabc algorithm success to reach to minim training and testing, prediction error, fast convergence, high success rate, and high prediction accuracy for crude oil prices. based on the above simulation results and analysis, the proposed gbabc algorithm has the capability to predict the accurate future crude oil prices. based upon on the above-mentioned proposed bio inspired gbabc algorithm outstanding simulation, the saudi arabia crude oil price can be easily predicted based on the past values."
"opteron atom improv. the main performance difference between these program versions is not due to the exploitation of any single hardware resource; rather, it is because of the complex interaction between parallelization, vectorization and data cache utilization. from the point of view of high-level program transformations, it is the loop nest structure derived from program partitioning that has the highest impact on performance."
"the rest of this paper is organized as follows. section ii describes the motivation and presents the problem statement. section iii recalls the fundamental concepts in polyhedral models of compilation. section iv addresses the construction, pruning and traversal of the search space. section v presents the model-based optimization algorithms used in our combined strategy. section vi evaluates this technique experimentally. section vii discusses related work, before the conclusion in section viii."
"the famous upstream and downstream companies operating in ksa are saudi aramco, shell, total, chevron, sinopec, exxonmobil, sumitomo and eni. the country has around 100 oil fields; the five famous in the production are safaniya, khurais, manifa, shaybah and ghawar. the ghawar oil field has the maximum production and estimated proved oil reserves of 70 [cit] was 9.7 million bbl/d, which was 32% and 10.5% of the opec's and the global crude oil production respectively. international energy agency, energy information administration has represented the crude oil production of global [cit], opec and ksa with the following fig. 1 [cit] ."
"predicting oil prices brought a considerable attention by scientific researchers and authors to study it from different aspects and different categories who provide a preemptive knowledge in identifying potential candidate forecasting models for crude oil prices. unfortunately, with inadequate information, uncertain situation, too many variables, the intrinsic complexity of oil market mechanisms, imbalance between production and consumption, weather forecasting, and imprecise elements, the crude oil price system is extremely complex for modeling analyses, and its dynamics are hard to predict its future price [cit] . oil prices are confined between demand and supply framework, oil price volatility analysis and oil price forecasting [cit] ."
"1) the algorithm: our algorithm proceeds level-by-level, from the outer-most loop level to the inner-most. for each loop at that level, candidates for vectorization are computed such that: (1) the loop can be moved to the inner-most position -via a sequence of loop interchanges -while preserving the semantics; and (2) moving this loop to the inner-most position does not remove thread-level parallelism. for each loop in the set of candidates we compute a cost metric based on the maximal distance (in memory) between data elements accessed by two consecutive iterations of this loop [cit], considering all statements enclosed in this loop. the algorithm then moves to the next loop level, until all candidate loops for vectorization have been annotated with the cost metric. loops with the best metric are then sunk inwards to the innermost position, with a sequence of permutations captured within the polyhedral representation. the tiling hyperplane method guarantees that this sinking operation is always legal: this seamless coordination of the two methods is a key benefit of a polyhedral compilation framework. note that because of parametric and possibly non-matching loop bounds, this transformation may result in additional prolog/epilog code surrounding the loops. this is handled seamlessly in the polyhedral representation but would have posed a major challenge to standard transformation frameworks."
"to solve the above-described problem, we propose a first reduction, which consists of (1) finding a vm allocation among the substrate hosting nodes, and (2)"
"where v ij is a new solution in the neighborhood of x ij for the employed bees, k is a solution in the neighborhood of i, φ is a random number in the range [-1,1]."
"in this study, we also consider the impact of \"slash and clear\" on the atp thresholds, as atp is an important entomological indicator considered in intervention programmes against onchocerciasis. the atp is calculated as the product of the abr and the population averaged number of l3 larvae per black fly 27, 60, and the atp threshold thought to indicate that local transmission is no longer sustainable has been fixed at 20 by who 27 . here, we modelled the site-specific atp threshold as the product of the biting rate and l * thresholds calculated for a site as described above (at either abr or tbr). when vc is implemented, the tbr is assumed to be the relevant biting table 3 . ugandan onchocerciasis study sites."
"it is common knowledge that situations in which the pile is on are usually not hazardous. however, in more specific contexts, say when the pile is on but the cooling system is down, one would expect it to be a hazardous situation. one may then also want to draw conclusions like \"if a situation is hazardous, then it is usually the case that the effect of switching the pile off brings about a non-hazardous situation\"; or \"if the pile is on and the cooling system is off, then usually the surveillance agent knows that a malfunction is imminent\"; or \"situations in which the pile is on usually ought to be non-hazardous\"; or even the more complex \"if the agent believes that there is danger, then usually he must perform the action of switching the pile off\"."
"to ensure that two statements are fusible, we can build a parametric integer program [cit] with sufficient constraints for the existence of a semantics-preserving multidimensional schedule [cit], in conjunction with the constraints imposed by definition 1. if this pip has a solution, then the two statements are fusible."
"the results of the simulation for different loads can be seen in fig. 4 . the results show a slightly better performance of the greedy ff approach compared with the random. this result is explained by the fact that the greedy approach minimizes the number of dcs selected in the first stage, minimizing as well the number of connections between dcs and thus decreasing network utilization. the differences obtained are minimal, showing that the dominant factor for the blocking probability in this experiment is not the exhaustion of network resources but the cloud. in this paper, the target is to present the problem of vmg allocation and the baseline solution for the proposed vimap architecture. it is intended for future work in the evaluation of more complex algorithms and its comparison within the vimap."
"aide [cit] establishes distributed platform composed of different computing devices such as laptops, pcs, pdas, and smartphones. the framework is composed of surrogate server and mobile device client. smd searches for suitable surrogate to share application processing load. the partitioning component of the aide partitions the application by following a partitioning policy. the framework exercises class level granularity for partitioning of application. an application profiling component establishes the feasibility of offloading. application profiler reflects on two parameters, the execution history of the application and prediction of the future resources required for the application. profiler aims for offloading the components that could improve the performance of the system. partitions are offloaded to the remote surrogates. aide provides a transparent distributed application deployment framework for mobile applications. the sophistication of application migration and remote execution are masked from mobile users. aide employs a dynamic partitioning and migration approach for offload processing and employs computing services of the remote hosts in the local distributed environment. aide implements distributed execution platform in transparent manner to give the user the notion of application being executed on local device. aide incorporates the option to use multiple surrogates for remote execution. the critical aspects of aide are that the runtime partitioning of the application requires additional computing resources exploitation for the establishment of distributed platform. aide is a decentralized distributed platform for dynamic partitioning and migration; for that reason smds need to monitor the execution environment which imposes heavy monitoring overhead on smd."
"in summary, our data-driven modeling study predicts that clearing vegetation as a form of vc can significantly accelerate the achievement of elimination of s. damnosum-transmitted onchocerciasis, regardless of www.nature.com/scientificreports www.nature.com/scientificreports/ the timing and frequency of implementation. these results also highlight the value of supplementing annual mda with vc, a conclusion that is applicable to many vector-borne ntds. vc, especially implemented via a community-directed approach, such as embodied by the \"slash and clear\" technique studied here, may also overcome the challenges of chemical-based vc delivered using public infrastructures, and thus may constitute a more sustainable approach to deploying long-term vc. further consideration of the optimal indicator and means of measurement to detect the interruption of vector-borne macroparasitic disease transmission is required. our results suggest that measuring transmission status based on vector-related targets is more sensitive than using measures of infection in humans, but the choice of endpoint targets will depend crucially on whether significant dispersal of vectors occurs between intervention sites. finally, this study emphasizes the value of using data-assimilation models for forecasting the effects of parasite intervention strategies even in data-limited situations."
"although typical abc is famous due to its robustness and high efficiency for clustering, classification and numerical function optimization problems, however, due to the same and www.ijacsa.thesai.org the random searching approach of exploration cannot guaranty for finding the best food position, also sometimes it trapped in local minima. the researchers improved typical abc algorithm by different strategies such as, best-so-far, discrete, hybrid, gbest guided and quick within employed, onlookers and scout bees. the typical artificial bee colony model which includes three kinds of bees considering the division of labor: employed bees, onlooker bees and scout bees. each employed bee works on only one food source as given in fig. 6 ."
"this study emphasizes the valuable role that forecasting models can play in programme design and decision-making. while the field data reported from the \"slash and clear\" trials showed great promise for reducing o. volvulus transmission 24, it is clear that such data alone may not provide insight into infection dynamics in humans nor can it be relied upon to anticipate the impact of the intervention in other settings or in the future. this highlights some of the challenges associated with using data alone for decision-making in parasite elimination. thus, while data can provide critical evidence, it is important to note, firstly, that data on one state variable alone cannot predict the behavior of the overall transmission system and, in particular, does not guarantee the detection of change in related but separate parts of the system 25 . combining data with forecasting models may, however, allow us to clearly define what we know, extrapolate this information to other settings, and reliably predict the outcome 25 . furthermore, the use of data-assimilated models provides a framework for explicitly considering uncertainties in data, initial conditions, and parameters 25 . here, we used a bayesian melding framework for expressing initial uncertainties in pre-control abr (a key system driver) and other model parameters, which are updated or localized for a site based on mf infection data. this allows us to discover models with associated uncertainties for a given site which capture the properties of local dynamics better, and thus to make more reliable site-specific future forecasts of intervention outcomes 25 . note that such data-model assimilation is flexible and can enable models to be easily updated as new data are collected, which can be a useful procedure for further reducing forecast uncertainty and thus supporting more reliable decision-making 44 ."
"biting rate reduction due to \"slash and clear\". removing trailing vegetation by the \"slash and clear\" technique was shown in trials to have a strong and rapid impact on black fly biting rates 24 . the biting rate was reduced by 89-99% within one month post-intervention in the intervention sites compared to control sites, and the reduction was long lasting with biting density significantly reduced for up to 4 months 24 . to capture this immediate decline followed by a slow period of population regrowth, we model the effects of \"slash and clear\" as an intervention acting against the mbr whose efficacy declines over time according to an exponential decay function:"
"the cloud infrastructure manager is responsible for distributed cloud orchestration. as opposed to the network manager, it is responsible for the partitioning and aggregation of cloud resources, which might be distributed across different clouds (private, public). once the selected dcs are allocated for a given tenant, it is responsible for creating a tenant session on each child cloud system and mapping all these client sessions to the corresponding vimap tenantid. once this initial abstraction is performed, it is responsible for aggregating all the resources distributed among different clouds into a single unified view accessible by the tenant through the vimap nbi. this is performed by populating the resource manager database with virtual representation of the resources deployed in the underlying infrastructure; these resources are segmented by its corresponding vimap global tenantid."
"cloud computing is the latest distributed computing model that implements the utility computing vision [cit] wherein computing services are provided on demand basis. cloud service models enable with new it business models such as ondemand, pay-as-you-go, and utility computing. the objective of the cloud computing model is to increase the capacity and capabilities of client devices by accessing leased infrastructure and software applications instead of owning them. cloud computing has introduced new kind of information and services and new ways of communication and collaboration. cloud has created online social networks in which scientists share data and analysis tools to build research communities [cit] . in cloud computing, applications are delivered as services over the internet and user access computing resources from centralized cloud servers through service providers [cit] . the examples of public utility computing include amazon web services (aws), google appengine, microsoft azure and aneka. aws offers infrastructure as a service and software as service which enable to utilize the virtualize resources and services in cloud datacenters. it reduces the cost and efforts associated with the administration of computer hardware and software for organizations [cit] . aws are utilized to store personal data through its simple storage service(s3) [cit], and computation is performed using elastic cloud compute. google app engine provides application developmental and deployment platform in googles data centers. it uses powerful infrastructure to provide services worldwide. app engine provides an application development environment which uses well known application developmental tools (such as java and python). it provides a rich set of apis to users whereas sustaining the security and isolation from other applications running the cloud infrastructure [cit] .windows azure is an extensible and open cloud computing platform which provides the services to develop deploy and operate applications and services in cloud datacenters. windows azure offers a simple, widespread, and powerful cloud computing platform for the creation of web applications and services [cit] . aneka offers the platform as a services model for cloud computing. it serves as an application development framework for building customized applications and deploying them on either public or private clouds [cit] .computational clouds implement different types of service models for implementing the on demand computing vision [cit] . service providers provide services in the hardware resources in the cloud datacenters are the physical resources of computational clouds. access to the physical resources is provided in the form of virtual machines. a middleware (hypervisor) masks access to the physical resources and is responsible for the deployment and management of virtual machines. the application hosting platform is composed of cloud programming environments and tools and monitoring tools such as qos negotiation, admission control, pricing and billing. the cloud applications run on the virtual machine instances in complete isolation."
"furthermore, the predicted prices obtained by best so far methods are also close to the original crude oil prices. from table xv, when the results obtained by the proposed gbabc algorithm analysis with original crude oil prices of the first 200 days, where again the original and predicted values are very close. based on the t test, as shown that the p value is greater than 0.05 so the null hypothesis can't be rejected. this shows that the crude oil prices obtained by the proposed method are very close to the actual prices' dataset vi. conclusion"
"crude oil, commonly known as petroleum comes from an oil well, is a liquid or solid found within the explored earth comprised of hydrocarbons (compounds composed mainly of hydrogen and carbon), organic compounds and small amounts of metal [cit] . it is one of the most important energy resources on earth for humans and machine developments. so far, it remains the world's leading fuel, with nearly one-third of global energy consumption."
"the polyhedral representation of programs enables the expression of arbitrarily complex sequences of loop transformations. but the downside to this expressiveness is the extreme difficulty in selecting a good optimization strategy combining the most important loop transformations, including loop tiling, fusion, distribution, interchange, skewing, permutation and shifting [cit] . it is also hard to analytically capture interacting effects of different hardware resources taking into account downstream optimization passes."
"the paper discusses the concept of cloud computing, mobile cloud computing and explains the different techniques to augment smart mobile devices resources based on resources available within the cloud. it analyzes current dapfs by using thematic taxonomy and highlights the commonalties and deviations in such frameworks on the basis of significant parameters. it discusses issues in current dapfs and highlights challenges to optimal and lightweight distributed application frameworks for mcc. current dapfs accomplish process offloading in diverse modes. several approaches exploit entire application migration; others focus on part(s) of the application to be offloaded. a number of approaches employ static partitioning, others exercise dynamic partitioning. variant migration patterns are used; downloading application by providing url to remote host, vm cloning, mobile agent such as usmc, application binary transfer and use of proxies."
"the notion of defeasibility in action theories has been dealt with in nonmodal frameworks for reasoning about actions [cit] . contrary to ours, their work is not concerned with extending preferential reasoning to more expressive logics. however, defeasible statements of the kind we studied here can be used in reasoning about the qualifications of actions: in situations where α holds, the action a is usually executable; but in the more specific context α, a's execution fails. in that sense, our framework also stands as an approach to the qualification problem [cit] and to the more general problem of revising action domain descriptions [cit] ."
"the considered network scenario is composed of multiple wireless radio access and backhaul technologies and multidomain, multilayer, and multivendor transport networks, with heterogeneous control domains, interconnecting distributed cloud infrastructures (both private and public). the use of cop between the sdn network orchestrator and control layers allows the simplification and optimization, in terms of scalability and compatibility, between the different modules, which compose the sdn architecture. cop unifies all the orchestration functionalities into a single protocol paradigm. the cop information model is described in yang modeling language, with rest conf as transport protocol using javascript object notation (json) encoding for data transmission. in brief, cop is composed of three main base functions: 1) topology service: this provides topological information about the network, which includes a common and homogeneous definition of the network topologies included in the te databases of the different control instances. 2) path computation service: this provides an interface to request and return path objects that contain the information about the route between two endpoints. 3) call service: this is based on the concept of call/connection separation and provides a common provisioning model, which defines an end-to-end connectivity provisioning service."
a classification of application offloading frameworks by using their attributes is shown in fig. 4 . this section analyzes current application offloading frameworks and investigates the implications and critical aspects of current dapfs.
"in this paper, we use a stricter definition of fusion in the polyhedral representation. the cases where only a few instances are fused and most of the loop iterations end up in the prolog/epilog parts offer little interest in our framework. in such cases, the core of the computation remains in the distributed loops, and since we aim at exploring all distinct fusion/distribution choices, it is likely that this variant would offer little difference with the case where statements are fully distributed. also, our objective is to directly prune the set of semantics-preserving transformations from the transformations (or, schedules) that do not implement the fusion of statements. note that checking if given a schedule corresponds to fusing the statements is higly impractical: it implies the need to enumerate all possible valid schedules and check this property in each case. we define the added affine constraints the schedules must respect in order to implement fusion using the following fusibility criterion."
all data generated or analyzed during this study are included in this published article and its supplementary information files. model code is available at https://github.com/edwinmichaellab/slashandclear.
"it is unlikely that rainfall patterns will remain constant in the long term future. because the simulations consider time scales over which we cannot rely on stationarity, we performed a sensitivity analysis to evaluate how well the \"slash and clear\" intervention would work given unforeseeable shifts in seasonality. we tested scenarios where the rainfall patterns shift forward by 1, 3, and 6 months while keeping the intervention schedules the same. results from this analysis suggest that there is no significant change in the impact of \"slash and clear\", suggesting that the frequency of the intervention has a stronger effect than the timing even in the presence of uncertain future seasonal changes in rainfall and hence transmission ( supplementary table s2 )."
"the algorithm proceeds by computing the schedule level by level, from the outermost to the innermost. at each level, a set of legal hyperplanes is computed for the statements, according to the cost model defined in section v-a2. dependences satisfied by these hyperplanes are marked, and another set is computed for the next level such that the new set is independent with respect to all previously computed sets, and so on until all dependences have been marked satisfied. at a given loop level, if it is not possible to find legal hyperplanes for all statements, the statements are split [cit], resulting in a loop distribution at the level. we note that from the approach to construction of valid partitions, this cannot occur at the outermost loop level."
the representation theorem for rational consequence relations then states: [cit] ) a defeasible consequence relation is a rational consequence relation if and only if it is defined by some ranked model.
"the mobile nature, compact design, limited computing potential and wireless medium attributes of smds necessitate for optimal, lightweight and rich local services procedures for distributed application deployment in mcc. the incorporation of standardized design and development principles of distributed systems seem to be an optimal solution for coping with the challenges of lightweight distributed application deployment for mcc. the incorporation of distributed client/server architecture of distributed applications with the elastic features of the traditional offloading frameworks appears to be an appropriate optimal solution for addressing the issues of current dapfs for mcc. the development of such lightweight model will result in reducing developmental efforts and enhancement in overall performance of application deployment, management and processing in mobile cloud computing."
"the substrate infrastructure scenario employed for the experiments is an extended version of the nsfnet of 14 nodes and 42 unidirectional links and six dcs (fig. 3) . for simplicity, the dcs are co-located within the same network node locations, and the connectivity between a dc and its corresponding network nodes is modeled to have infinite bandwidth. the substrate infrastructure is initially configured with predefined capacities, which are maintained along all the experiments. the values of the capacities of each dc are uniformly distributed among the values included in each range, as depicted in table i. in the vmg requests, the number of virtual nodes is randomly determined by a uniform distribution between 2 and 20. each pair of nodes is randomly connected with probability of 0.5; in total, we will have nn − 1∕4 links on average. the capacities of the virtual hosts and the virtual links are also selected randomly following a uniform distribution along the values depicted in table i. the vmg requests arrive at the vimap following a poisson process on which the arrival rate is varying. the holding time of the vmg requests in the system follows an exponential distribution with 10 time windows on average. we run all the simulations for 10,000 requests for each instance of the simulation."
"efforts, particularly for onchocerciasis 7 . vc through the application of larvicides was the primary strategy of the onchocerciasis control programme in west africa [cit] . the ugandan experience with vc, when used in conjunction with twice per year treatment with ivermectin (ivm), is also rich and impressive; transmission for example has been interrupted in 15 of 17 endemic foci using this integrated approach [cit] . however, the implementation of larvicide treatments can be labor-intensive and cost-prohibitive 20, 21 . insecticide resistance is also a major concern that threatens the long term success of pesticide-based interventions 1 . innovative and sustainable vc interventions to support endgame elimination activities are thus critically needed."
"the above proposed method is efficient and fast convergence through three modifications for solution update, increase the local search ability of the onlooker bees, maintain the diversity of new food sources by random scout bees, and to resolve round up issues in the computation of the floating point -goodness‖ value. however, the employed bees section used the random way equation (2) of typical abc, which can lead to unbalance of exploration and exploitation process for different complex problems."
"powerful semi-automatic polyhedral frameworks have been designed as building blocks for compiler construction or (autotuned) library generation systems [cit] . they capture partitioning, but neither do they define automatic iteration schemes nor do they integrate a model-based heuristic to construct profitable parallelization and tiling strategies. the polyhedral model creates many more opportunities for the construction of loop nest optimizers and parallelizing compilers. it is currently being integrated in production compilers, including gcc 4.5 and the ibm xl compiler."
achieving a high level of performance for a computational kernel on a modern multicore architecture requires effective mapping of the kernel onto the hardware resources that exploits
"all of these are examples of defeasible inferences. the first two examples are instances of propositional defeasible consequence and are adequately dealt with within the framework for preferential reasoning developed by lehmann and colleagues in the 90's [cit] . for the last four examples, however, it is a different story: firstly, for their specification we need a logical language that is richer than the propositional one: they require, respectively, the ability to express the effects and preconditions of actions; an agent's knowledge and beliefs; regulations or obligations; and combinations thereof. well established formalisms for dealing with these notions in the ai literature are mostly variants of modal logic: the examples above illustrate applications of dynamic logic [cit], epistemic logic [cit], and deontic logic [cit] ."
"step 3: applying optimizations: the last step is to generate a transformed program according to the optimization we have previously computed. syntactically correct transformed code is generated back from the polyhedral representation on which the optimization has been applied. we use the cloog, a state-of-the-art code generator [cit] to perform this task."
"1) compute the order of magnitude of data reuse in the loop nest; 2) compute the depth of the loop nest; 3) if there is o(n) reuse within a loop, and the loop nest depth is greater than 1 then tile the loop nest. to achieve maximal performance it is expected that tuning the tile sizes can provide improvement, however the problem of computing the best tile sizes for a loop nest is beyond the scope of this paper. in our experiments tile sizes are computed such that data accessed by each tile roughly fits in the l1 cache."
"on the other hand, retaining expressiveness is a major objective, as we aim to build a search space of all valid (that is, semantics-preserving) partitions of the program. to achieve this goal, we leverage the expressiveness of the polyhedral representation and its ability to compute arbitrary enabling transformations (e.g., permutation, skewing, etc.) for fusion. we first provide a practical definition for fusion of statements in the polyhedral model in section iv-a, before discussing the construction of the search space of all valid partitions in section iv-b."
"grouping statements such that those in a given class share at least one common enclosing loop is a way to abstract the essence of loop fusion; this general idea also enables the modeling of loop distribution and code motion. if some statements in a given class were not fusible, then this partitioning would be equivalent to the one where the statements are distributed: this is a case of duplication in the search space. our approach to guarantee that we find the most effective partitioning is to exhaustively evaluate all of them. it is important to remove duplicates in the search space to minimize the time for empirical search."
definition 2 ensures that none of the dependences point backward along any of the dimensions to be tiled. this is a stricter condition than simple semantics-preservation.
"the values of rmse obtained by abc, gabc, best-sofar and proposed gbabc algorithms are mentioned in table xiv . in term of rmse values, the typical two methods (gabc and best-so-far) and particularly gbabc algorithm obtained the highest numbers of minimum error values for the prediction of crude oil prices."
"the commonly used (syntactical) approach to loop fusion requires matching bounds for the loops to be fused; otherwise prolog and epilog code need to be explicitly generated for the remaining loop iterations. this problem becomes difficult when considering imperfectly nested loops with parametric bounds. however, using a polyhedral abstraction, the process of generating prolog/epilog code for arbitrary affine loops is handled seamlessly by the code generation step. the only task is to provide a schedule for the program that corresponds to fusing some statement instances inside a common loop. in this representation, loop fusion can be seen as the negation of loop distribution. two statements r and s are distributed if all instances of r are scheduled to execute before the first (or after the last) instance of s. for any other case, there is an overlap and some instances of r and s are interleaved. given a loop level, we distinguish the statement interleaving that describes r being executed fully before or after s (no common loop), from the fine-grain interleaving of statement instances where r and s share a common loop."
"the argument to define and construct a viable notion of rational closure is analogous to that given by lehmann and magidor [cit] in the propositional case. first, we define a preference ordering on consequence relations, with relations further down in the ordering interpreted as more preferred."
"previous research on building a convex search space of legal affine schedules highlighted the benefits of integrating the legality criterion directly into the search space, leading to orders of magnitude smaller search spaces [cit] . this is critical to allow any iterative search method to focus on relevant candidates only."
"where η is the immediate percentage decrease in biting rate due to the removal of vegetation, λ is the decay rate of the efficacy of the intervention at maintaining reduced biting rates, and t s is the time since clearing the vegetation in months. note that, in this formulation, λ controls the rate of return of the black fly population following the intervention. we calibrated the seasonal mbr model with the effects of \"slash and clear\" to monthly biting data from the intervention sites ( supplementary table s3 ) [cit] . the prior parameter ranges for η and λ are given in supplementary table s4 . the pass/fail filter was based on the same acceptance criteria as above."
"in this section, we first describe the general vmg allocation problem. then, we present a reduction of the problem based on constructing the aggregated vmg solution graph, where the objective is to find groups of vms to be allocated together in the same substrate hosting nodes. this reduction is modeled based on a constrained mapping function. finally, a heuristic algorithm solution to this problem is proposed, and simulation results for the algorithm behavior are provided."
"multidimensional polyhedral tiling is applied by modifying the iteration domain of the statements to be tiled, in conjunction with further modifications of θ [cit] ."
"our technique builds an expressive search space of loop transformation sequences, expressed in the polyhedral model as a set of affine scheduling functions. the search space encompasses complex compositions of loop transformations, including loop fusion and distribution, loop tiling for parallelism and locality (caches, registers), loop interchange, and loop shifting (pipelining). we proposed a convex encoding of all legal transformed program versions as the space to search."
"the major result of this study that is immediately relevant for onchocerciasis elimination programmes is that supplementing annual mda programmes with community-directed vc by clearing vegetation can significantly accelerate the achievement of parasite elimination in a sustainable manner. a recent field study in northern uganda indicated that clearing vegetation using a community-driven approach can significantly reduce simulium damnosum s.s. biting rates, but no empirical study has yet been done to test how this will affect infection rates in humans 24 . here, we address this gap explicitly by using a forecasting model to link the observed changes in vector biting rates to the expected infection dynamics in the human population."
"from the nmse results as given in table v, best-so-far outperformed than abc and gabc and proposed gbabc algorithm outperformed than all typical algorithms except best-so-far with 4-7-1 nn topology. the proposed gbabc obtained the best mse out of sample data set for crude oil prices prediction than others bio inspired methods as shown in table vi . here, again the best-so-far method successes in obtaining least testing error when the hidden nodes reached to seven. based on overall result, the gbabc model is reliable and promising for the accurate prediction of crude oil prices."
"the automatic optimization and parallelization framework has been implemented in pocc, the polyhedral compiler collection, 2 a complete source-to-source polyhedral compiler integrating well established free software for polyhedral optimization."
"to remedy the above shortcomings of typical models and learning algorithms, meta-heuristic and their hybrid version methods have been used recently to solve time series problems. these are: artificial bee colony (abc), cuckoo search (cs), bat algorithm (ba), evolutionary algorithms (ea), genetic algorithm (ga), pso, ant colony optimization, hybrid algorithms and so on [cit] . these meta-heuristic learning algorithms are more efficient and famous due fast convergence, high efficiency, easy to understand and implementation and robustness for some problems [cit] . they can easily find the set of best weight values, through suitable parameter selection, activation function and network structure that will cause the output from the mlp to match the actual target values as closely as possible. sections 2 and 3 contain neural networks and crude oil prices forecasting details and honey bees super heuristic optimization methods respectively. the proposed method explained in section 4. the simulation results and conclusion are added in sections 5 and 6."
"t he miniature nature, compact design, high quality graphics, customized user applications support and multimodal connectivity features have made smds a special choice of interest for mobile users. smds incorporate the computing potentials of pdas and voice communication capabilities of ordinary mobile devices by providing support for customized user applications and multimodal connectivity for accessing both cellular and data networks. smds employ wireless network technologies for accessing the internet; such as 3g connectivity, wireless fidelity (wi-fi), wi-max, or long term evaluation (lte). smds are the dominant future computing devices with high user expectations for accessing computational intensive applications analogous to powerful stationary computing machines. examples of such applications include natural language translators [cit], speech recognizers [cit], optical character recognizers, image processors [cit], online games, video processing [cit] and wearable devices for patients such as wearable device with a head-up display in the form of eyeglasses (a camera for scene capture and earphones) is a useful application that helps alzheimer patients in everyday life [cit] . such applications necessitate higher computing power, memory, and battery lifetime on resource constrained smds [cit] . on the other hand, smds are still low potential computing devices having limitations in memory, cpu and battery power. in spite of all the advancements in recent years, smds are constrained by weight, size, and intrinsic limitations in wireless medium and mobility."
"server based dapfs provide centralized management and ensure availability of remote services. however, a number of obstacles obstruct optimization goals of server based remote application processing. in the following section, we discuss issues in current dapfs and identify challenges for leveraging the application processing services of computational clouds."
"we observe that the number of duplicates using such a representation grows exponentially with the size of f . as a major concern is to prevent duplicates in the space, we use an internal encoding for this problem such that the resulting space contains one and exactly one point per distinct partitioning [cit] . the search space is modeled as a convex set of candidate partitions, defined with affine inequalities. there are several motivating factors. the set of possible partitions of a program is extremely large (on the order of 10 12 possibilities for 14 elements [cit], with a super-exponential growth), while the space complexity of our convex set hardly depends on the cardinality of the set. also, removing a subset of unwanted partitions is made tractable as it involves adding affine constraint(s) to the space, in contrast to other representations that would require enumerating all elements for elimination. finally, the issue of efficiently scanning a search space represented as a well-formed polytope has been addressed [cit], and these techniques apply directly."
"virtual machines lead to high cpu utilization. vms share the same cpu/core which increases the cpu scheduling latency for each vm significantly [cit] . vm migration based offloading requires additional computing resources and time for the deployment and management of vm on smd. consequently such approaches increase the execution cost and time of the application. migration of running application along with its data and states is susceptible to security breaches and attacks. further, a number of other research challenges [cit] such as privacy and access control are still addressable which obstruct the goals of optimal vm based migration algorithms for mcc."
"software defined networking (sdn) has emerged as the most promising networking paradigm to realize the integration between cloud and network domains. in sdn, a centralized entity (commonly referred as a sdn controller) is able to program the forwarding behavior of the network elements through different southbound protocols (i.e., openflow). most of the sdn controllers also offer a northbound application programming interface (api) to higher level applications to expose its services (connectivity provisioning, topology discovery…). this abstraction enables network virtualization, that is, to slice the physical infrastructure and create multiple co-existing virtual tenant networks (vtns) independent of the underlying transport technology and network protocols."
"the simulation results of gbabc compared with abc, gbac and best so far abc algorithms. the average mse training and testing, nmse, accuracy and success rate are given from table iv to table viii, respectively. in terms of mse training, the abc, gabc and best-so-far reached to 0.000371, 7.12e-05, 3.93e-06 respectively, while the proposed gbabc algorithm success to obtain the most minimum value than typical algorithms with error 3.13e-11 as given in table iv . in case of 4-8-1, the best-so-far method reached to 1.10e-08 which is the minimum error than obtained by abc, gabc methods."
"the definition of a transport api that abstracts a set of control plane functions used by a sdn controller, allowing the sdn orchestrator to uniformly interact with heterogeneous control domains, will pave the way toward the required transport network interoperability as well as integration with wireless networks and cloud infrastructure. in this paper we have presented the control orchestration protocol and experimentally validate its utility for cloud and network orchestration. moreover, we have defined and experimentally demonstrated the novel vimap component for the resource allocation and planning of vmgs. we have formally modeled the problem and included a baseline heuristic solution to evaluate the proposed architecture."
"the implications of elastic application model [cit] are that the framework accomplishes application level partitioning and migration of applications. the framework employs a comprehensive cost model to dynamically adjust execution configurations and optimizes application performance in terms of a set of objectives and user preferences. the framework provides a security mechanism for the authentication and authorization of weblets migration and reintegration and provides support for synchronization between application on mobile device and weblets running on cloud node. the critical aspect is the establishment of runtime distributed platform for smd which necessitates additional computing resources exploitation for the establishment and management of distribute platform. the framework deploys replication of the application both on the mobile device and application manager of the cloud server. the framework implements a sophisticated mechanism for the migration of weblets between smd and remote cloud nodes. the framework imposes extensive overhead of application profiling, dynamic runtime partitioning, migration, reintegration, and rigorous synchronization on mobile devices for offload processing. fig. 7 shows a generic flowchart for application partitioning based offloading frameworks. the profiling mechanism evaluates computing resources require- ments of mobile application and the availability of resources on smd. profiling mechanism works differently in different frameworks. the critical situation indicates the unavailability of sufficient computing resources on smd. therefore, the computational intensive components of the application are separated at runtime. smd negotiate with cloud servers for the selection of appropriate server node. at that moment partitions of the application are migrated to remote server node for remote processing. upon successful execution of the remote components of the application, result is returned to main application running on smd."
"the attribute of migration granularity represents the level of granularity of offloading intensive components of the mobile application. the finer granularity level results in outsourcing computational load at refined level. however, refined level granularity requires highly intensive monitoring mechanism on smd at runtime [cit] . the refined level granularity requires resources intensive synchronization mechanism between smd and cloud server node. further, finer granularity level has the issue of ensuring consistency in the distributed execution of mobile application. on the other side, the abstract level of granularity results in simple offloading mechanism and requires low monitoring overhead on smd [cit] . however, abstract level of granularity results in increased data transmission overhead and therefore increases fig. 8 . local application offloading models security threats of the outsourcing components of the mobile application. for example, migration of an entire application [cit], module or component [cit] of the mobile application is more vulnerable to network threats as compared to the outsourcing thread [cit] or method [cit] of the mobile application. for the reason that eavesdropping of finer level code is less meaningful to the attackers as compared to the entire module or component of the mobile application. current dapfs implement the following granularity levels for application offloading. a) module level migration represents that entire module or bundle of the application is migrated to remote environment [cit] . b) method level migration represents that partitioning occurs at the method level and intensive methods of the application are migrated to remote server [cit] . c) object level migration represents that entire object is migrated to remote environment for outsourced processing [cit] . d) thread level migration represents thread level partitioning and migration of the application to remote environment [cit] . e) entire application migration in which case entire application is offloaded to remote server [cit] . the attribute of migration support represents the level of support required for the migration of application. the mechanism of application offloading increases resources utilization on smd. specifically, the resources utilization for the operating system increases in application offloading mechanism. currently, a number of application offloading frameworks require additional support from the operating system; such as vm deployment and management [cit] . such offloading frameworks are represented as system level in table 3 and table 4 . on the other hand, the latest application offloading frameworks implement component offloading at application level and do not require additional support for component outsourcing [cit] ]. such frameworks are repressed as application level in table 3 and table 4 ."
"2) performance portability: the optimal partitioning depends on the program, but is also influenced by the target machine. this is shown by the variability column of table 6 . for 9 of the 11 benchmarks with more than one legal partitioning, there exists no partitioning such that when it is executed on all three machines, it performs within 5% of the optimal one found for each machine. increasing the threshold to 10%, this is still the case for 7 of the 11 benchmarks."
"iterative optimization has proved its effectiveness in providing performance improvements over a broad range of architectures and compilation scenarios [cit] . however, none of the previous approaches attempt to construct program transformation sequences as complex and as extensive as the ones presented in this paper while pruning the search space to semantics-preserving candidates only."
"uncertainty analysis. note that while the study sites chosen are located in uganda, the annual mda scenarios do not reflect the ugandan onchocerciasis elimination strategy. in order to be generally applicable, we modelled a standard annual mda intervention strategy to investigate the combined effect of ivm and \"slash and clear\", but in practice the ugandan policy is moving towards enhancing their elimination programmes by implementing biannual mda 12 . to assess the sensitivity of the results to the annual mda programme, we repeated the scenarios described above under a biannual mda programme also at 80% coverage. additionally, it is unlikely that seasonal patterns will remain constant from year to year. therefore, as a sensitivity analysis, we repeated these simulations while shifting the rainfall patterns by 1, 3, and 6 months to assess how sensitive the results are to the timing of the intervention in relation to the biting season."
"where v id is a new achievable solution of a scout bee that is modified from the present position of an abandoned food source (x ij ), the value of w max and w min represent the maximum and minimum percentage of the position adjustment for the scout bee. the last change is finding the minimum objective value, here compare and to select between the old solution and the new solution in each iteration is done by the fitness value of the following equation (6) as,"
"afterward, the provisioning of the e2e service in the upper layer is requested to the sdn controllers, by two new unidirectional call_servicess to each domain."
"loop fusion heuristics were initially designed as localityenhancing optimizations, in isolation from other loop nest transformations [cit] . these non-polyhedral approach are restricted in their ability to find complex partitions, or model the interplay of loop fusion with equally important optimizations such as loop tiling. the lack of a powerful representation for dependences and composition of transformations also restricted the study of enabling loop transformations to enhance the applicability of loop fusion."
"the rest of the paper is organized as follows. section 2 explains the fundamental concepts of mcc. it discusses the concept of cloud computing, mobile cloud computing and explains the different techniques to augment smart mobile devices resources based on resources available within the cloud. section 3 presents thematic taxonomy of current dapfs, reviews current application offloading algorithms on the basis of taxonomy and investigates the implications and critical aspects of current dapfs. section 4 compares current dapfs by comparing the commonalities and deviations by using significant parameters presented in the taxonomy. section 5 highlights the issues in current dapfs and discusses challenges to distributed application processing for mcc which contributes toward the advancement of research and development of optimal distributed applications for mcc. finally, section 6 draws concluding remarks with future directives. table 1 shows the list of acronyms used in the paper."
"to maximize performance, one must carefully explore the trade-off between the different levels of parallelism and the usage of the components of local memory. maximizing data locality may be detrimental to inner-loop-parallelism, and may counter the effects of an efficient, well-aligned vectorized inner loop. on the other hand, focusing the optimization towards the most efficient vectorization may require excessive loop distribution, resulting in poor data reuse and thus may adversely affect performance."
"currently application offloading is implemented in a number of ways. the application offloading frameworks outsource computational load of smd at different granularity levels. the static application partitioning approach is used to separate the intensive components of mobile application only once. the dynamic partitioning approach is implemented to address the issue of dynamic application processing load on smds at runtime. dynamic partitioning of the intensive mobile application at runtime is a robust technique for coping with the dynamic processing loads on smd. in dynamic partitioning application is partitioned dynamically at runtime casually or periodically. in casual partitioning runtime profiling and solving mechanisms are activated in critical conditions to offload intensive components of mobile application. in periodic partitioning the runtime optimization mechanism evaluates computing resources utilization on smd periodically. current dynamic partitioning approaches analyze the resources utilization on smds, computational requirements of the mobile application and search for runtime solving of the problem of resource limitations on smd. the profiling mechanism evaluates computing resources requirements of mobile application and the availability of resources on smd. in critical condition (the unavailability of sufficient resources on smd) elastic mobile application is partitioned and the computational intensive components of the application are offloaded dynamically at runtime. smds negotiate with cloud servers for the selection of appropriate server node. at that moment partitions of the application are migrated to remote server node for remote processing. upon successful execution of the remote components of the application, result is returned to main application running on smd."
"the mcc model is composed of three major components; smds, internet wireless technology and computational cloud. smds use wireless network technology protocols such as 3g, lte, or wi-fi to access the services of computational cloud in mobile environment. however, the connection is less reliable due to mobility requirements such as handoff processes. as smd inherit its nature of mobility, it needs to execute locationaware services which consume resources and turned it to be a low-powered client. fig. 2 shows a generic model of mcc in which the cloud that provides off-device storage, processing, queuing capabilities, and security mechanism is integrated with smd via wireless network technologies."
"recalling our discussion in the introduction, with such a specification one cannot reason with exceptionalities given by statements of the form \"a situation in which the pile is on is usually not hazardous\" and \"a situation in which the pile is on and the cooling system is off is usually hazardous\". thanks to our constructions from the previous section, we can move to a defeasible version of a modal logic of actions in which one can capture defeasible consequences."
"the cop and sdn orchestration are the key enablers on which we build our proposed end-to-end architecture for distributed cloud and network orchestration [see fig. 1(a) ]. on top of the architecture, the global virtualized infrastructure manager and planner (vimap) is responsible for the global orchestration of distributed cloud controllers and the transport network infrastructure. the vimap implements the same functionalities of the vim, as defined by the european telecommunications standards institute in the nfv management and orchestration reference architecture [cit], i.e., it is responsible for the nfv infrastructure resources management, such as computation, storage, and networking. our proposal extends the vim with a new planner component (vimap) for resource optimization and planning, which provides an online platform to run resource allocation algorithms for the arriving infrastructure slice requests from the different tenant applications."
"in order to compare the sizes of the hosts (physical or virtual) in relative terms, we define the function weight, as the weighted sum of the individual computing capacities, we use the constants α, β, and γ to weight up the cpu, memory, and storage capacities, respectively:"
"the contributions of this paper are as follows: (i ) we filled an important gap in the non-monotonic reasoning community by providing a natural and intuitive semantics for preferential reasoning in modal logics; (ii ) we gave to our semantics a corresponding syntactic characterization via our representation results; (iii ) we established the basis with which to 'lift' the propositional notions of defeasible consequence and closure to modal logics in general, and (iv ) we illustrated how our constructions can be applied in two important families of modal logic."
"yes yes the possibility of inaccessibility of the remote services always remains associated in local distributed models. whereas, scalable systems ensure the provision of services irrespective of the number of clients accessing the services. therefore, unavailability of centralized resources and services and scalability of services is a challenging research issue for ad-hoc and virtual distributed models of mcc [cit] . it is challenging to implement peaceful degradation policy on smds in the critical conditions of unavailability of remote services. scalable systems sustain the provision of services and resources for large number of clients whereas availability of services ensures the provision of remote services. it is imperative to ensure the scalability of services in cloud datacenters so that smds are enabled to access centralized services for distributed application deployment with high aim of scalable remote services. in centralized datacenter based computational cloud are resources rich and computational resources and services are provided on demand basis. cloud resources and services are accessible to both stationary computer clients and smds. however, the unique architecture, compact design, operating platforms, low computing potentials, and portable mobile nature of smart mobile devices require special services for ensuring the availability of cloud services. the mobile nature and the intrinsic limitations associated with the wireless access medium of smd necessitate availability of cloud services and resources homogeneously worldwide. it is challenging in cloud based processing of mobile applications to ensure the availability of services and identical access to cloud services over different types of wireless network technologies (wi-fi, 3g and lte). therefore, sustaining uninterrupted provision of cloud services and resources to smds is a challenging research perspective of mobile cloud computing."
"where v ij (or x ij ) is a new solution in the neighborhood of x ij for the guided employed bees and onlooker bee,  ij is a random number in the interval between [0,c], and the term y j www.ijacsa.thesai.org represents jth element of the global best solution in current generation. the equation (7) used to enhance the exploitation process through gbest x j and c values. the proposed gbest guided approach is more effective than typical abc especially for numerical function optimization, time series prediction and for environmental/ economic dispatch considering wind power and classification task [cit] ."
"the polyhedral model is a flexible and expressive representation for loop nests with statically predictable control flow. loop nests amenable to this algebraic representation are called static control parts (scop) [cit], roughly defined as a set of consecutive statements such that loop bounds and conditionals involved are affine functions of the enclosing loop iterators and global variables (constants that are unknown at compile-time). relaxation of these constraints based on affine over-approximations have been proposed recently [cit] . while our optimization scheme is compatible with the recent proposal [cit], we limit the presentation in this paper to describing the representation and optimization of standard scops."
"among the other natural resources, crude oil is the -key‖ treasure for obtaining the stable economic position of the country. it has an important factor affecting the local and global economy of the region as well. for the last two decades, the oil-rich countries have witnessed significant margins and differences in their development rate, economic stability as well as the quality of their organizations. ksa is one of the oilrich, productive hub resources in the middle east region. its cover more than half of the opec's total oil exports, and is a major player in setting the oil price in asia and worldwide [cit] . the country, being the second largest producer of petroleum liquids and the largest exporter of crude, has the ability to have a major impact on the global oil industry and economic stability. ksa has 265.8 billion barrels of crude reserves, the second largest in the world, amounting to 16% [cit], which are predicted to rise to 273 [cit] . ksa is also the second largest producer of crude oil in the world. [cit], the crude oil production in saudi arabia increased 9880 bbl/d/1k to 10070 bbl/d/1k [cit] ."
"in the present paper we aim at filling this gap by providing a generalization of classical preferential consequence to an important family of modal logicswe present the semantic foundation, prove the required representation results, point out computational consequences and benefits, and suggest applications of modal preferential reasoning. the good balance between expressivity and computational properties of modal logics makes them good candidates for the type of extension of preferential reasoning that we have in mind -with (propositional) modal logic one can express more than with classical propositional logic without being hampered by the undecidability of many first-order based languages."
"the first model-driven optimization we consider applies, individually on each class of the partition, a polyhedral transformation which implements the interleaving of statements via a possibly complex composition of multi-dimensional tiling, fusion, skewing, interchange, shifting, and peeling. it is known as the tiling hyperplanes method [cit], and we restrict it to operate locally on each class of the partition."
"we summarize these results in figure 5 where we report the performance improvement over the original code (improv.) for these three architectures, and the performance improvement over the partitioning with the best average on the three machines (variability)."
"step 2: representing optimizations: the second step in polyhedral program optimization is to compute a transformation for the program. such a transformation captures in a single step what may typically correspond to a sequence of several tens of textbook loop transformations [cit] . it takes the form of a carefully crafted affine multidimensional schedule, together with (optional) iteration domain or array subscript transformations. in this work, a given loop nest optimization is defined by a multidimensional affine schedule. given a statement s, we use an affine form on the d enclosing loop iterators x s and p program parameters n. it is written"
"network manager functions are twofold: first they provide the southbound interface toward network infrastructure controllers, including the necessary api or protocol implementations. as we have presented before, the cop is the protocol chosen to unify the network orchestration interface toward different sdn controllers. second, the network manager is responsible for managing the virtual network resources of each tenant. the network manager correlates the vtn representation with the dedicated sdn controller slice, there is a 1∶1 relation between a vtn and a sdn controller slice."
"consider the following scenario depicting a nuclear power station: in a particular power plant there is an atomic pile and a cooling system, both of which can be either on or off. an agent is in charge of detecting hazardous situations and preventing the plant from malfunctioning ( figure 1 )."
"definition 2 (legality of tiling): given two statements r and s. tiling is legal if, for all dimensions d to be tiled and for all dependences d r,s :"
"fig. iii-a shows the thematic taxonomy of dapfs, which are categorized on the basis of framework nature, migration pattern, migration support, partitioning approach and objective functions. the attribute of framework nature indicates the main mechanism employed for application offloading. we categorize current dapfs on the basis of virtual machine migration, entire application migration and application partitioning. virtual machine migration nature of dapfs indicates that smd offload application by encapsulating the running application in vm instance on smd. the vm instance is outsourced to cloud server nodes. a number of current dapfs employ vm migration based approach for offloading intensive components of the application [cit] . the entire application migration nature of dapfs indicates that smds offload entire processing job to remote server nodes. current dapfs offload entire application in two different manners. a) mobile application starts execution of the smart mobile device and in critical condition the running instance of the mobile application is offloaded to remote server node [cit] . b) the client component of the mobile application runs on the smd where the processing load of the application is offloaded to cloud server for remote processing [cit] . application partitioning nature of the framework indicates that computational intensive components of the mobile are separated at runtime. partitions of the application are offloaded to remote server nodes [cit] . the partitioning approach of a framework indicates the mechanism of separating intensive components of the application. current dapfs employ two different strategies for separating computational intensive components of the mobile application. in static application partitioning the intensive components of the application are separated either at compile time or runtime statically. dynamic partitioning mechanism follows dynamic evaluation mechanism for assessing the computational load on smd. the attribute of migration support indicates the level of support required for migrating application or partitions of the application at runtime. currently, migration support is provided either at system level or application level. the attribute of migration granularity indicates the granularity level at which application is migrated. current dapfs offload intensive components of the application at different granularity level. for example thread level granularity indicates that running thread is offloaded for remote processing. in the same way, method level granularity indicates that methods of the application are offloaded for remote processing. migration pattern represents the pattern or way by which application or partition of the application is migrated to remote server. current dapfs employ a number of migration patterns such as vm migration, download using url on remote host, mobile agent serving as courier for application transfer, binary code transfer of the application or copying entire proxy of the application on distributed computing nodes. the objective function attribute indicates the primary objective of a framework for application offloading. current dapfs aim for a number of objective functions such as saving energy on smd, efficient bandwidth utilization, saving processing power on smd, user preferences for fast application processing, or execution cost parameter."
"the mpi communication found in minimd can be split into four components. as atoms move during a simulation, they may move from one node to another -this is known as atom exchange. some atom-pairings may cross node boundaries (i.e. the force acting on atom i may depend on atom j, which exists on another node). following atom exchange, minimd thus builds lists of two kinds of atoms near the borders of a node's subvolume; (1) dependent atoms, which require some force contribution from atoms on another node; and (2) ghost atoms, which exist on another node and may contribute to the force acting on dependent atoms. prior to force compute, each node sends the positions of its dependent atoms to neighbouring nodes (position communication); and, after the force compute, each node receives some force contribution from its neighbouring nodes (force communication)."
"each of the vinsertps instructions in fig. 11 extracts the x, y or z component from one xmm register (in aos), and inserts it into another (in soa). the result is six xmm registers, two for each of x, y and z, which are then combined into three ymm registers using three vinsertf128s. these inserts together account for 21 of the 37 instructions."
"work-groups can be scheduled for execution on any available compute unit and in any order, thus permitting opencl applications to scale to fit different hardware configurations. there is no method of global synchronisation during a kernel, but local synchronisation is possible between all work-items in a given work-group."
"this paper is structured as follows: section 2 provides the nclm and lpv model for turbofan engine. the novel modeling method and its advantages are introduced in section 3. the accuracy improvement of the proposed model is discussed in section 4. the effectiveness of the proposed modeling method is demonstrated by simulations in section 5. in the end, the conclusion of the paper is given in section 6."
"where θ is the scheduling parameter related to the state x p and where a p (θ), b p (θ), c p (θ), and d p (θ) are scheduling functions of a p, b p, c p, and d p, respectively. the key to ensure the accuracy of equation (11) is to obtain the lti models of which the dynamic and steady characteristics are very close to those of the nclm."
"in order to overcome the disadvantages of nclm approach and lpv technique, a novel mechanism modeling approach is proposed in this paper. in this approach, the mechanism flow path calculation of nclm preserves the aerothermodynamic characteristics of nclm. the solution of iteration in equation (6) is calculated by an lpv model and is provided for flow path calculations directly so that only one flow path calculation is required in one-step simulation. the lpv model here is established as follows based on the solutions of iteration in nclm under the sea-level condition."
"scene understanding and prompt reaction to an event is a critical feature for any time critical computer vision system. the deployment scenarios include a range of applications such as mobile robotics, autonomous cars, mobile and wearable devices or public space surveillance (airport / railway station). modern vision systems which play a significant role in such interaction process require higher level scene understanding with ultra-fast processing capabilities operating at extremely low power. currently, such systems rely on traditional computer vision techniques which often follow compute intensive brute-force approaches (slower response time) and prone to fail in environments with limited power, bandwidth and computing resources. the aim of this paper is to review state-of-the-art embedded vision systems available from the literature and in the industry; and therefore to aid researchers for future development."
"table vi compares the performance of our opencl implementation of minimd (using the kernel with explicit vectorisation and 4-way unrolling) and lammps cuda . for the complete application, our implementation is 2x slower than cuda on the c1060, and 1.5x [cit] . a large portion of this difference can be attributed to the poor communication scheme used by our opencl code, which is almost 6x slower than that used by lammps cuda . the cuda code executes all communication routines on the device and, since we are using a single node, is able to avoid all pcie communication; our opencl code, on the other hand, executes all communication routines on the host."
idea. how do we select the set of independent poverty measures? one of the main objectives of using poverty measures is to improve the life of poor people by providing them with immediate financial help. there are two main ways of providing such help:
"corresponding membership function. we need to have a function that describes, based on the person income x, a degree µ(x) to which this person is poor. in deriving this function, we can use the following two commonsense facts:"
"8-way unrolling is more efficient than 4-way unrolling on the amd gpus, again because more parallelism is exposed to the underlying microarchitecture. it does not, however, lead to efficient utilisation of avx on the e3-1240. examination of the assembly generated by the intel sdk (appendix a) reveals that the compiler emits an inefficient instruction sequence for the aos-to-soa transpose; instead of using vector shuffle instructions to permute the gathered aos data, as it does when auto-vectorising the kernel, it uses scalar instructions to build the soa data one element at a time. the a8-3850 also experiences a slow-down, but this is because it does not support avx instructions."
"it should be noticed that the spline interpolation is adopted in nclm to make the parameters of lpv model continuous to improve accuracy around the boundary of former piecewise interpolation in nclm. the former nclm adopting linear interpolation is established based on test data and can be seen as a \"precise\" model, and the nclms mentioned elsewhere all stand for this \"precise\" model."
"counting the proportion of number of poor people: three natural possibilities. it is natural to describe the extent of poverty as the ratio between the number of poor people (or, to be more precise, the cardinality of the set of poor people) and the total population. the first idea, as we have mentioned earlier, is to consider poverty as a crisp property:"
"how poverty is measured now. how do we gauge the extent of poverty in a country or in a region? usually, there is a threshold z (called poverty line or poverty threshold), which is the minimum level of income deemed necessary to achieve an adequate standard of living in a given country, so that:"
"where the sum is over all persons whose income is at or below poverty level. f 1 is called the intensity of poverty, and f 2 is called the severity of poverty. both measures give more weight to people whose income is smaller."
"in this paper, we report on the development of a performance-portable opencl implementation of sandia's minimd benchmark. we show that the performance bottlenecks of the force compute kernel are the same across several architectures, and that the optimisations that we apply to the original scalar code improve performance by more than 2x across a wide range of hardware types from different vendors: cpus and integrated gpus from amd and intel; and discrete gpus from amd and nvidia."
"improving the performance of this kernel on all of the architectures considered in this study thus depends upon achieving better utilisation of simd hardware. as we demonstrate in the remainder of this section, all hardware therefore benefits from the same set of optimisations."
"main idea: using fuzzy logic. the main problem with simply counting the number of poor people is that, like many other things in the world, poverty is a matter of degree: one person can be simply poor, another is somewhat poor, the third person may be very poor. the fact that many properties are not always absolutely true or absolutely false -but often true to a degree -was the main reason why l. zadeh introduced fuzzy logic; see, e.g., [cit] . in fuzzy logic, this degree is usually described by a number from the interval [cit] :"
"the vinsertps instruction has the capability to extract any 32-bit element from an xmm register, and we were therefore surprised to see that each of its uses here extracts the lowest 32 bits. this requires the generation of the 16 remaining instructions, to rearrange the xmm registerseach vpshufd instruction moves an atom's y component to the lowest 32-bits of the register, and each vmovhlps instruction does the same for z."
"a much more efficient way to store the neighbour list is in transposed form (i.e. the 0th neighbour of w atoms, followed by the 1st neighbour, and so on). where these w atoms have a different number of neighbours, we insert \"dummy\" neighbours, which are located at infinity and always fail the cut-off check. this is likely to give better performance than a completely transposed neighbour list (i.e. the 0th neighbour of all atoms, followed by the 1st neighbour, and so on) on architectures with caches. when w is set to the hardware's simd width, all neighbour list accesses become a single load from contiguous memory locations (fig. 3) . we therefore set w to 1 for the amd cpu; 8 for the intel cpu and gpu; 32 for nvidia gpus; and 64 for amd gpus."
"the remaining components of the simulation are uninteresting in a performance portability study. one kernel zeroes the forces of each atom (prior to the force compute); another updates atom positions, based on their current velocities; and the final kernel updates atom velocities based on the forces acting upon them. representing these streaming operations in opencl such that their performance scales with memory bandwidth (as expected) is trivial."
"the graph in fig. 8 presents the total performance improvement of our optimisations, relative to the original scalar baseline code. this performance improvement is cumulative, since each optimisation is built upon the previous version; the only exception to this is 8-way unrolling, which replaces 4-way unrolling. the speed-up of our best kernel (i.e. explicit vectorisation with unrolling) is consistently greater than 2x across all architectures."
"our current implementation does not implement any of these components in opencl. instead, we make use of the default implementation provided by the original minimd, with device-to-host and host-to-device transfers either side. this is likely to be a performance bottleneck for discrete gpus, due to the latency and bandwidth of the pcie bus. the performance of the code executing on cpus and integrated gpus will also be decreased, although to a lesser extent; our code does not make any distinction between devices, and each transfer results in a memcpy on these platforms. there is a clear need to port these components to opencl in future work -in addition to decreasing the amount of data transferred between host and device, the operations themselves would benefit from executing in parallel."
"where f η,hpt and f w,hpt stand for the linear interpolation functions of performance maps and γ 4 is the ratio of specific heat. the temperature t 43, pressure p 43, mass flow w 43, and power n hpt are the required outputs."
"for serial code, where the lists for each atom are built one after the other, each index can be written to memory sequentially; further, if the amount of memory allocated for the neighbour list is too small, it can be reallocated. opencl does not support the allocation of device memory within kernels, and assigning one work-item per atom leads to all neighbour lists being built in parallel; the starting offset of each neighbour list must be known ahead of time. in our code, we allocate enough memory for each atom to store some maximum number of neighbours (derived from simulation parameters), and calculate the offsets accordingly."
"in all experiments, we round the number of work-items up to a multiple of the hardware's \"preferred work-group size multiple\", as reported by the device, and allow the opencl runtime to choose the number of work-groups."
"due to the positive effect of the verification, further researches are encouraged: (a) the proposed model could be applied to other aeroengines that require iteration of balance equation, such as turbojet engine and turboshaft engine, and (b) the accuracy and real-time performance could be verified on hardware in loop."
"optimising codes for each new hardware offering is clearly desirable, ensuring fair comparisons between them [cit], and enabling hpc sites to switch between vendors. however, creating and maintaining highly optimised implementations for each platform is often not feasiblelegacy production codes are too large to re-write using multiple programming languages/toolkits, and maximising performance on a new platform requires significant programmer effort and expertise."
"the lpv model consists of a set of lti models and equilibrium points (u p,e, x p,e, y p,e ) in equation (9) . if the parameters of the lti models vary strongly around some equilibrium points, a quasi-lpv model is required. here \"quasi\" means the selected scheduling parameter should be related to the state. it is assumed that the scheduling parameters vary slowly enough so that the scheduled lti models can be seen as approximate linearized models near equilibrium points."
"many legacy scientific and engineering applications are not performance-optimal, achieving a very low percentage of peak flop/s even on a single node (i.e. before performance degradations due to less-than-perfect node scaling). traditionally, application scientists have been reluctant to optimise codes for two reasons: (1) tuning for a specific platform reduces an hpc site's ability to move between vendors; and (2) the ability to maintain code is seen as more important than achieving the best possible performance."
"to obtain a real-time model, a common way is to linearize the nclm at plenty of operating points and, then, to map the established lti models together into an lpv model."
"following this change, the intel sdk no longer performs auto-vectorisation; rather than packing contiguous workitems into simd execution units, each of the explicit vector operations in the kernel is mapped directly to an sse instruction. since each work-item now runs independently, the neighbour list access is no longer a gather, and we disable the neighbour list transposition (by setting w to 1, as we did on the amd cpu) to compensate."
"one possibility is to allocate a certain fixed amount of money (or goods) a to each poor person. in the us, an example of such allocation is distributing food stamps -approximately the same amount goes to every person below a certain poverty threshold. in this case, the original incomes change from x i to x"
"possibility. since we have shown that the fgt measures naturally come from fuzzy techniques, and fuzzy techniques have a precise mathematical foundation, it makes sense to look for precise mathematical justifications of fgt measures."
"meanwhile, although the traditional lpv technique is suitable to obtain a real-time model, the straightforward linearization of the traditional lpv model will neglect some aerothermodynamic details and, therefore, affect the modeling accuracy. moreover, if the lpv model is established using the similarity criteria, it is hard to deal with the outputs that do not meet the similarity criteria, such as thrust and power. in such cases, massive linerizations of the traditional lpv model or divisions of flight envelope are required throughout the large-scale flight envelope, which will restrict the application of lpv technique."
"it is assumed that the current state x and input u are approximately close to a nearby equilibrium point (u e, x e ), where x e,1 ≈ x 1, u ≈ u e, and the scheduling parameter θ can be obtained from the current state. the relationship between the coefficients and the scheduling parameter θ can be seen as a scalar function; thus, the number of the required parameters can be reduced by fitting curves:"
"the use of fpgas in application areas like communication, image processing and control engineering has increased significantly over the past decade [cit] . computer vision and image processing algorithms often perform a large number of inherently parallel operations, and are not good candidates for implementation on machines designed around the von neumann architecture. some image processing algorithms have successfully been implemented on embedded system architectures running in real-time on portable devices [cit], and relatively small literature has been dedicated to the development of high-level algorithms for embedded hardware [cit] . the demand for real-time processing in the design of any practical imaging system has led to the development of the intel open source computer vision library (opencv) for the acceleration of various image processing tasks on gpps [cit] . many imaging systems rely heavily on the increasing processing speed of today's gpps to run in real-time."
the remainder of this paper is structured as follows: section ii presents a survey of related work; section iii introduces the opencl programming model; section iv details the development and optimisation of our performanceportable md code; section v compares the performance of our code with implementations highly optimised for a specific platform; section vi discusses the implications of our results; and section vii concludes the work.
"this loop makes use of n3 -the force i exerts on j has the same magnitude as the force j exerts on i, but in the opposite direction. a given atom-pair (i, j) appears in the neighbour list for either i or j, and the force between them is calculated only once. this avoids redundant computation of forces, but introduces a potential for update conflicts (lines 15-17), since several atoms may share a neighbour -if we attempt to update the same neighbour multiple times simultaneously, only one update will take effect. since opencl does not support global synchronisation between work-items, it is difficult to prevent such conflicts. although many devices support atomic memory operations, on current-generation hardware they are too expensive to be used in an inner-loop such as this one [cit] . we instead opt to not use n3, modifying the neighbour list build to store (i, j) in the lists of both i and j, and removing lines 15-17 from the kernel. this removes the potential for conflicts, at the expense of twice as much computation."
"although recently hardware manufacturers launched new heterogeneous products, e.g., xilinx zynq ultrascale+ mpsoc 1 (cpu, gpu and fpga) and altera soc products 2 (cpu and fpga), these are not fully exploited in computer vision domain (except handful of recent work, e.g., [cit] ) as majority of the existing algorithms are not designed to target heterogeneous platforms. consideration of target hardware during the algorithmic development cycle is not always necessary and the domain experts often prototype new algorithms using library-rich languages such as matlab. however, efficient deployment of these prototypes on a heterogeneous hardware is challenging. asynchronous data process network [cit] may provide a plausible solution to this problem, however requires further research."
"although the intel i7-3770 (ivy bridge) cpu attached to the hd4000 can also be used as an opencl device, we do not report on its performance. we instead report performance on an intel xeon e3-1240, a server part built on a closely related microarchitecture (sandy bridge)."
"the lj simulation performed by minimd can be broken down into four components. the calculation of short-range forces is responsible for more than 80% of its runtime, and evaluates the force between all atoms separated by less than some \"cut-off\" distance (r c ). to avoid evaluating the distance between all pairs of atoms at each time step, the force compute makes use of a pre-computed \"neighbour list\" [cit] for each atom. building this list is the secondmost expensive step, accounting for approximately 10% of execution time. the remaining time is split between: internode communication, which sees nodes exchange position and force information for atoms near the boundary of their sub-volume; and the update of atom velocities and positions."
"our complete opencl implementation is 1.7x faster than the original minimd code running on the same hardware, and at most 2x slower than \"native\" implementations highly optimised for particular platforms. whether this performance compromise is an acceptable penalty for increased code portability is open to debate, but the results in this paper suggest that opencl is a suitable development tool to bridge the gap between architectures -enabling hpc sites to develop codes that exhibit good levels of performance, across a wide variety of hardware, without sacrificing maintainability. appendix assembly generated for gather operations figures 10 and 11 list the assembly generated by the intel sdk for two of our kernels: the auto-vectorised vector gather kernel, and the kernel with explicit vectorisation and 8-way unrolling, respectively. the instructions have been re-ordered and grouped to improve readability, and any instructions not directly related to the gather of positions have been removed."
"in order to obtain a real-time model based on lpv model for both control and health monitoring, a novel integrated mechanism model is proposed. the iteration in nclm is replaced by an lpv model to improve the real-time performance, and the flow-path calculation is reserved to prevent the loss of the aerothermodynamic characteristics, and therefore, the dynamic performance of the integrated model will be more similar to the nclm than the traditional and straightforward lpv model does. the modeling process is as follows: firstly, the small perturbation method is used to get local linear models at ground operating points. secondly, the equilibrium points are combined into the coefficient matrices of the lpv model to simplify the structure. thirdly, the polynomial regression method is used to fit parameters of the lpv models to reduce the required parameters. then, the similarity rule and polytope theory are used througout full flight envelope operation. finally, the lpv model is integrated with flow path calculation, which takes account of the characteristic of aerothermodynamics of the turbofan engine. the main contribution of this paper is to integrate the lpv model with flow path calculation to avoid iteration. the continuity of parameters in lti models is discussed, and smooth coefficients of ltis are obtained by using the spline interpolation method to improve the modeling accuracy. to verify the efficiency of this approach, the accuracy and real-time performance of the proposed model, the traditional lpv model, and the nclm are compared in the simulation at the end of this paper."
"in order to obtain satisfactory real-time performance, it is a common way to simplify or neglect some nonsignificant processes. [cit] s, attention has been paid on this problem. a simplified model of an f100-pw-100 turbofan engine is constructed to operate on a hybrid computer [cit] . some simplification in the flow path of the nclm is used for a highly compact model which is able to be operated on an engine mounted computer [cit] . despite the sacrificed accuracy of the simplified model, the real-time performance is not significantly improved to meet the demands of ecu. accelerating the convergence speed of the iteration process is an effective way to ease the computational burden to some extent [cit] . however, these models are still calculated in an iterative way and the iteration will be hard to converge if the current state is far from the steady state. besides, the inputs are calculated by the controller contain noises inevitably, which always makes the model keep away from the steady state and will worsen the real-time performance of the nclm. data-driven methods provide a choice to construct a real-time model without iteration [cit] . the model with low complexity is built to capture the behavior of the system [cit] . a data-based takkgi-sugeno fuzzy model is studied in reference [cit] throughout the flight envelope. however, these approaches rely on plenty of flight data and the modeling process will become more complex if the division of flight envelope is carried out. another way to avoid the iteration in nclm is to transformed the balance equations into differential and algebraic equations, as shown in reference [cit] . however, this transformation is complicated because it requires a lot of considerations on the accumulation effect of a cavity. establishing linear models without iteration is an ideal way to deal with these problems."
funding: this research was founded by the funding of jiangsu innovation program for graduate education (no. kylx15_0255) and national science and technology major project (2017-v-0004-0054).
"all performance figures are given as execution times in seconds, and all implementations make use of single precision floating-point arithmetic. we configure minimd to simulate 256,000 atoms with a uniform density of 0.8442, using the default cut-off distance of 2.5, for 100 timesteps. except where noted, all experiments pair hardware with the opencl sdk provided by the vendor."
"the equation (15) reveals that a p and c p can be calculated at every discrete time k and taht the partial-derivative terms have a very close connection with the dynamic property of the system, and it is obvious that the square matrix ∆x 1 p,k, ∆x 2 p,k, · · ·, ∆x nx p,k need to be non-singular. if ∆x 1 p,k, ∆x 2 p,k, · · ·, ∆x nx p,k is close to a singular matrix, less upper bounds r ub of iterations of nclm are required and the given perturbations should be adjusted or the order of the linear system should simply be reduced."
"a performance-portable code is clearly more desirable if its performance is competitive with that of native implementations. we therefore compare our opencl implementation to (a) the original minimd, (b) a hand-vectorised implementation using avx instructions on the intel xeon, and (c) lammps cuda on the c1060 [cit] ."
"research into computer vision has made steady and significant progress in the past two decades. the tremendous progress, coupled with cheap computational power has enabled many portable and embedded devices to operate with vision capabilities. digital signal processing and for that matter digital image processing (dip) is an exciting area to be involved in today. having been around for over two decades, it is typically used in application areas where cost and performance are key [cit], including the entertainment industry, security surveillance systems, medical systems, automotive industry and defence. dip systems are often implemented using the ubiquitous general purpose processors (gpps). the increasing demand for high-speed has resulted in the use of dedicated digital signal processors (dsps) and general purpose graphics processing units (gpgpu); special types of gpp optimised for signal processing algorithms. however, power dissipation is important in almost all dsp-based consumer electronic devices; hence the high-speed, power-hungry gpps become unattractive. battery-powered products are highly sensitive to energy consumption, and even line-powered products are often sensitive to power consumption [cit] . for hardware acceleration and low power consumption, dip designers have opted for alternatives like the field programmable gate array (fpga) and application specific integrated circuits (asic)."
"after converting the position array to an array of float4 vectors, our next step is to convert the arithmetic to vector form (fig. 6 ). this exposes intra-loop parallelism to the compiler, and leads to a more succinct (and arguably more readable) version of the kernel."
"for the intel sdk, using fission improves force compute performance by 1.1x, and neighbour list build performance by 2x, increasing the speed-up over the original minimd to 1.7x. for the amd sdk, using fission results in worse performance than the original minimd; although the improvements to the force compute and neighbour list kernels are similar, communication costs become 9x more expen- sive, possibly due to scheduling conflicts between the threads spawned by mpi and amd's opencl runtime."
". it makes no sense to add such \"combined\" property measures to the original list: indeed, to compute the poverty value corresponding to this measure, we do not need to painstakingly add the values of f (x i ), it is enough to apply the corresponding combination function to the poverty values corresponding to the original poverty measures."
"expanding equation (7) around each equilibrium point (u e, x e, y e ) and neglecting high-order items yields the following lti model:"
"current computer vision algorithms are highly complex and consist of different functional blocks that are suitable for a variety of targets i.e., cpus, gpus or fpgas. therefore, designing computer vision systems for single target hardware platform is inefficient and does not necessarily meet performance and power budgets especially for embedded and remote operations. a heterogeneous architecture is a natural alternative but manifests new challenges:"
"generally speaking, the gpus see more benefit from the memory optimisations while the cpus see more benefit from explicit vectorisation. this reflects both the simpler cores of gpus (which are more sensitive to memory latencies, and are thus more impacted by the original memory access pattern) and the immaturity of opencl compilers for cpus. amd gpus are the only architecture to benefit significantly from both types of optimisation (with a speed-up of ≈ 11x)."
"-design choices to dissect the algorithm according to their suitability for the target hardware, -interoperability and data flow synchronisations between functional units as different blocks may have different timing constraints. -programmability and coordination between different hardware platforms. there is a need for unified programming environment."
"how the concepts of devices, compute units, processing elements, work-groups and work-items map to different architectures is decided by the opencl runtime, a library that is often provided by the hardware vendor (but can also be supplied by others [cit] . a typical mapping between cpu/gpu hardware and opencl is shown in fig. 1 ."
"the flow path calculation is the process to calculate the parameters of the outlet of each component in figure 1 according to the parameters of the inlet. the calculation of three main components of the engine core, the nozzle, and the balance equation of the nclm are discussed below."
"although this is not the first paper to address the issue of performance portability for opencl codes [cit], it is the first (to our knowledge) to do so for molecular dynamics (md). the use of co-processors in this domain has been shown in previous research to deliver significant speedups [cit] . however, it remains to be seen if a single set of optimisations allows for efficient execution across cpus, discrete gpus, and integrated gpus. the contributions of this paper are as follows:"
"exchange and borders pose a similar challenge to opencl as the neighbour list build, since the number of atoms each work-item appends to the list (of atoms being exchanged or at the borders) is not known ahead of time. the packing and unpacking of messages for position and force communication is much simpler, and the transfer of the packed messages can be hidden behind useful work; however, the current communication scheme used by minimd imposes an ordering on the sending and receiving of these messages, which complicates matters."
"in order to obtain a mechanism model of turbofan engine with satisfactory real-time performance and accuracy, an integrated model is proposed. the iteration in nclm is substituted by an lpv model to improve the real-time performance, and the flow-path calculation is integrated with the lpv model to preserve the nonlinearity and aerothermodynamics of the model. the influence of the residual and the continuity of the nclm are discussed to improve the accuracy of the integrated model, and it can be concluded that lower residual and spline-fit interpolation of nclm make the integrated model more accurate. simplification and curve fitting are used to reduce the number of the restored parameters. in the simulation, only 8% parameters are required after the simplification."
"the amd sdk provides the fastest runtime for opencl, beating the intel sdk by approximately 10%. this is also 1.5x faster than the original c++ version of minimd, demonstrating that opencl is a suitable development tool for utilising the simd architectures of modern cpus. however, our opencl implementation is 2x slower than the heavily-optimised version of minimd which we developed in previous work [cit] . this code employs a number of novel simd and threading optimisations, which give it a significant advantage over opencl; its force compute and neighbour list build are 2x and 6x faster respectively."
"an aeroengine, with the requirements of long engine life, great operational flexibility, and control performance, is a complicated aerothermodynamic system [cit] . the engine model is formulated in a set of mathematical expressions for controller design and health monitoring [cit] . in the modeling process, the goal is to obtain an accurate and real-time model with a simple structure. the nonlinear component level model (nclm) is a common model established on the basis of mechanical and aerothermodynamic theories. though the accuracy of the nclm is very high, it can hardly deal with the trade-off between the accuracy and real-time performance [cit] . there are increasing demands needed to be considered due to the development of the modern aircraft, and therefore, the nclm has become increasingly complex, which makes the trade-off much more difficult to deal with. meanwhile, the balance equations in the nclm need to be iteratively solved within limited time while the computational and memory resources of the electronic control unit (ecu) in aeroengines are limited. because the real-time performance of nclm on hardware of aeroengine is not satisfactory, the nclm is not suitable for controller design or health monitoring. hence, a real-time model with acceptable modeling accuracy and a few required parameters is necessary for practical applications, which is the motivation of this paper."
"the aeroengine studied in this paper is a low-bypass twin spool turbofan engine, consisting of an intake, a fan, a compressor, a combustor, a bypass, a high-pressure turbine, a low-pressure turbine, a mixing chamber, an afterburner, and a nozzle. the structural diagram and cross-sectional number of this twin-spool engine are shown in figure 1 . the air extraction for cooling the turbine and bypass duct is not detailed in this paper. the nclm is a set of mathematical formulas for aeroengines based on the principles of aerothermodynamic, rotor dynamic, and other principles followed by various engine components. it adopts a lot of physics-based empirical formulas under some basic assumptions to calculate the flow path."
"2) neighbour list gather: if the neighbour list of each atom is stored in contiguous memory, any access to this list from work-items operating in simd requires a gather; with n neighbours per atom, w contiguous work-items read from memory with a stride of n."
"where ∆x i p,k stands for the state derivative at the kth time step under the ith perturbation. then, the partial derivative terms in equation (9) can be calculated in the form below:"
"in opencl, a host cpu running c/c++ code uses library calls to communicate with and control one or more devices. each device is made up of a number of compute units, which can be further sub-divided into processing elements. device functions are known as kernels and can be compiled just-in-time (jit) from source, or loaded from a cached binary if one exists for the current target platform. these kernels are executed in a single-program-multiple-data (spmd) fashion by a one-, two-or three-dimensional set of work-items, which are grouped together into work-groups."
"in this section, the accuracy and computational complexity of the integrated model will be discussed. comparisons among the integrated mechanism model, the nclm, and the traditional lpv model are also carried out. the outputs of these models are p 3, p 4, p 43, p 6, t 3, t 4, t 43, t 6, and f. all the simulation experiments are conducted on a personal computer with an intel core i7-6500u cpu and 8 gb memory."
"if contiguous work-items are mapped to simd execution units (as is the case on gpus, and for auto-vectorised code on cpus), then lines 3-6 have the biggest performance impact. each of the memory accesses on these lines requires a gather operation (i.e. they are \"uncoalesced memory accesses\", in cuda parlance). if work-items instead/also expose parallelism through the explicit use of vector types (as is the case on amd gpus and for non-vectorised code on cpus), then the use of only scalar types results in significant simd inefficiency."
"on nvidia gpus, the accesses to x, y and z become coalesced and the intel compiler is able to emit a more efficient sequence of instructions -an array-of-structs (aos) to struct-of-arrays (soa) transpose, in place of a series of scalar loads -on both cpu and integrated gpu hardware. vector gather of positions figure 5 . effect of using vector types on memory access pattern. as shown in table iii, most architectures benefit from this change, with only amd hardware seeing little (or no) improvement. performance is not significantly impacted on the hd6550d or v7800, suggesting that they are already able to hide the latency of these memory accesses. the a8-3850 experiences a slow-down of 1.2x, again because it does not execute work-items in simd. although the sdk uses a vector load to gather x, y and z as expected, the remainder of the kernel continues to use scalar arithmetic; therefore, each component of the loaded vector must be extracted into a scalar register, resulting in worse performance."
"the core behaviour of the neighbour list build is very similar to that of force compute; each work-item computes the distance between an atom and a set of potential neighbours, and compares that distance to the cut-off. we optimise this using float4 vector types, as we did for force compute. however, instead of updating forces, each work-item appends the indices of those atoms that pass the cut-off check to a list -an operation that does not map well to opencl."
"from the simulation results, the following can be concluded: (a) compared to nclm, the proposed model saves over 75% of simulation time if the input noises are not considered and 89% in the presence of the input noises, which means that the real-time performance of the proposed model is insensitive to input noises. (b) when the input noise is neglected, although the integrated mechanism model works slower, it gives better mae and rmse values for t 3, t 4, and t 43 than the traditional lpv model. meanwhile, the average mae and rmse of the proposed model decrease by 0.15% and 0.08%, respectively. (c) the real-time performance of the proposed model is insensitive to the input noises, and therefore, the simulation time is nearly the same as the simulation time when ignoring noises. although the input noises will slightly deprave the accuracy of the two models, the average mae and rmse of the proposed model decrease by 0.15% and 0.12%, respectively."
"author contributions: q.c. and f.l. conceived the main idea, j.h. and q.c. designed the model structure, q.c. carried out the numberic experiments and analyzed the data, and m.p. and q.c. wrote the paper."
"this transposition also enables the intel sdk to autovectorise our kernel, which is only attempted when the compiler expects it to improve performance (based on some heuristic). we believe that the presence of a large number of gather operations caused the original kernel to fail this check, thus preventing auto-vectorisation. table ii gives the execution times for the kernel with and without a transposed neighbour list. the a8-3850 is the only architecture not to see a speed-up, and this is due to it executing work-items in scalar; when w is set to 1, the neighbour list layout is not changed. the cost of a gather operation increases with simd width, and we therefore see higher speed-ups on architectures with wider simd."
"in this paper we made a modest effort to review embedded computer vision systems that satisfy application specific constraints e.g., performance or power. the literature is scattered and covers a range of application areas, vision algorithms and target hardware. this paper made an effort to categorize them in an orderly fashion. we identified two emerging trends (described below) in this domain namely, heterogeneous computing and bi-inspired computing for efficient vision systems."
"however, where a code change does not improve performance, it does not significantly degrade it. these results thus show that it is possible to accelerate opencl code without focusing on any particular microarchitecture, and demonstrates the performance portability of our optimisations."
"unrolling the loop over neighbours in our explicit vectorisation kernel, such that it computes the force between i and several neighbours simultaneously, allows us to regain lost simd efficiency. theoretically, the compilers could employ such an optimisation themselves, but we found that this is not yet the case. table iv lists the speed-ups for two alternative unrolling factors (4 and 8), relative to the previous best approach (i.e. vector gather, with scalar arithmetic). for 8-way unrolling, we use float8 types, to explore the potential performance benefits of avx instructions on cpus; for 4-way unrolling, we use float4 types. to handle the case where the number of loop iterations does not divide exactly by 4 or 8, we pad an atom's neighbour list with dummy atoms. both cpus benefit from 4-way unrolling, due to running all simd instructions at 100% efficiency. unrolling also exposes more parallelism to the very-long-instruction-word (vliw) architecture of amd gpus, which see a similar performance increase. the performance impact of unrolling on other architectures is inconsistent, and further investigation into this matter is necessary."
"where f w,comp and f η,comp stand for the linear interpolation functions of performance maps and γ 2 is the ratio of specific heat. the efficiency η comp, temperature t 3, pressure p 3, mass flow w 3, and power n comp are the required outputs. note that w bp and h bp are calculated in bypass and not detailed in this paper. the meaning of the parameters and subscriptions in the equation are listed in the nomenclature section."
"similarly to the previous possibility, an efficient set of poverty measures should enable us to predict how these measures change when we provide this help. in other words, once we know the poverty values"
"the biggest remaining causes of poor performance in our opencl implementation are: firstly, that the avx code generated by current opencl compilers is inefficient; and secondly, that both the original minimd and our optimised avx code make use of n3, and thus do half as much work during the force compute and neighbour list build. as discussed in section iv, making use of n3 in opencl is difficult, due to restrictions in the programming model regarding work-item synchronisation."
"advantages and limitations of the existing poverty measures. empirically, the poverty measures f 0, f 1, and f 2 work well, they are used by economists and governments throughout the world to gauge poverty -and thus, to gauge the success of different measures aimed at reducing poverty. for example, in mexico, the measure f 2 is officially used as a poverty measure; in egypt, all three measures are used; see, e.g., [cit] . the main problem with these measures is that they are semi-heuristic. there is no precise justification and therefore, there is no guarantee that a slightly modified version of one of these measures would not provide a better description of the extent of poverty. it is therefore desirable to look for poverty measures which can be theoretically justified -based on some reasonable assumptions."
"however, studies performed by both academia and industry have shown that sections of these codes are amenable to gpu acceleration -and that programmers are willing to spend time porting kernels to cuda (or selecting appropriate openacc [cit] pragmas) in exchange for significant speed-ups. the results in this paper (specifically, that our figure 9 . opencl code-snippet for gathering x, y and z positions in the kernel with 8-way unrolling."
"conclusion of this section. all three fgt measures f 0, f 1, and f 2 naturally appear in the fuzzy interpretation, as the ratio of the number of poor people to the population as a whole:"
"another possibility is to provide tax deductions to all the poor people; this is also done in the us. since taxes are usually proportional to the income, tax deduction means, in effect, that the resulting income x i of all poor people increases by the same factor, i.e., goes from x i to x"
"the laval nozzle throat is adjustable, and the area of the nozzle outlet is a specified multiple of the area of the nozzle throat [cit] . equation (4) shows the calculating process of the nozzle:"
"the open computing language (opencl [cit] ) is a crossvendor standard which ensures the functional portability of codes across hardware, thereby eliminating the need for applications to be re-coded on a per-device basis. however, it makes no guarantees of performance portability -an opencl application may be highly tuned for a particular architecture, but exhibit very different performance on others. this paper investigates the development of a \"performance-portable\" opencl implementation of sandia's minimd benchmark [cit], which seeks to exhibit good levels of performance across multiple architectures. minimd is a simplified version of the popular lammps package [cit], intended for use in optimisation studies such as this one; due to its simplicity, potential optimisations can be explored much more rapidly than in the context of a production application."
"in this section, we discuss the challenges of achieving high performance portability for all four components of the simulation. however, our optimisation efforts focus exclusively on short-range force calculation, since it accounts for such a high fraction of execution time. as we demonstrate in section v, the other components of the simulation may become a significant bottleneck on some hardware; there is a clear need to investigate these issues further in future work. fig. 2 shows pseudo-code for the scalar short-range force calculation loop in minimd. most of the work in this loop could potentially be executed in parallel; the positions of atoms are fixed during this operation, and thus the interatomic distance (and force) between all pairs of atoms in the system can be computed concurrently. a natural way to map this work to the opencl programming model is to assign one work-item to each atom."
"we believe that these behaviours are caused by a combination of compiler immaturity and the way in which we have expressed the gather operation in opencl (fig.9) . future compiler releases, or an alternative representation of the gather in code, may address these issues, leading to considerably better performance for this kernel on hardware supporting avx instructions. figure 11 . assembly from our kernel with 8-way unrolling."
"a detailed description of the hardware and software setup used in this paper is given in table i . the only exceptions to this setup are: the hd4000, which can only be used as an opencl device under windows; and the a8-3850 cpu (plus its integrated hd6550d gpu), which is hosted elsewhere. [cit] c++ compiler and gcc respectively."
"in recent years, cpu alternatives have gained significant traction within the high-performance computing (hpc) industry, and many of the world's fastest supercomputers [cit] currently employ hybrid system designs that pair traditional cpus with discrete co-processors (e.g. cell, fpgas, gpus, intel xeon phi). some vendors are already placing cpu and gpu cores on the same chip (amd fusion, intel hd graphics), while others are exploring this option (nvidia project denver)."
"on one hand, different from the mass flow calculation of other components, the outlet mass flow calculation of the rotor components and nozzle are independent of the inlet mass flow (see equations (1), (3), and (4)). therefore, there are mass flow conflicts between inlet and outlet of the rotor components and the nozzle (except the fan because there is no inlet mass flow calculation of the fan). on the other hand, due to the physical connection of the two shafts, the power between the high-and low-pressure rotors needs to be balanced (see the computational flow of mass flow in figure 2 ). therefore, there are four mass flow balance equations and two euler equations due to the rotational motion, and the calculation of the nclm is to solve this differential equation [cit], namely the balance equation:"
"while the first approach has implications in performance improvement, the latter ones are more suitable for low power applications. popular higher level complex image processing algorithms that are used in embedded computer vision literature includes stereo vision, feature extraction and tracking, motion estimation, object detection, scene segmentation and more recent convolutional neural network (cnn). these categories and corresponding literature are captured in table 2 ."
"some simplified methods, such as performance maps and the neglect of vane-tip clearance, are used in the calculation of the rotor component to obtain approximate results. the performance map is widely used to calculate the mass flow and efficiency of the rotating part of the engine [cit] . the calculations of the compressor are given as follows:"
"because nclm is established based on test data, it is a very important model for control and health monitoring, but this model is not a real-time model for two reasons: (1) the calculation of ∇ v k g iter in equation (6) needs repeated flow path calculations and worsens the real-time performance of nclm; (2) if the noises and disturbances of the inputs need to be considered, more flow path calculations will be required because the uncertainty continuously moves the states of the system away from equilibrium point, and therefore, the real-time performance of nclm is not satisfactory. therefore, the application of nclm approach is usually limited due to its poor real-time performance."
"table v compares the performance of our opencl implementation of minimd with the two native c++ implementations, executing on the intel e3-1240. in keeping with the rest of the paper, we present opencl results from the intel sdk; we also present results from pairing the amd sdk with intel hardware, as a means of comparing the two compilers. for both sdks, we use the kernel with explicit vectorisation and 4-way unrolling."
"an efficient set of poverty measures should enable us to predict how these measures change when we provide this help. in other words, once we know the poverty values"
"in previous work [cit], we investigated the performance portability of opencl kernels in the context of wavefront applications, demonstrating that the performance of opencl on cpus can match that of a scalar, mpi-based implementation. this work extends our approach to a new class of application (molecular dynamics), with a significantly different memory access pattern (gather-scatter), and focuses on the efficient utilisation of modern single-instructionmultiple-data (simd) hardware."
"the ability to detect moving objects in a scene is a fundamental problem in computer vision. this is a baseline problem that requires detection accuracy as well as computational efficiency to guarantee a successful high level processing in behavioural or event analysis [cit] . various background subtraction methods [cit] have been proposed and proven to be successful for detecting moving objects with the use of stationary cameras. these methods build statistical background models and extract moving objects by finding regions which do not have similar characteristics to the background model. human visual systems processes a very high volume of data and hence it is often selective and activity driven (responsive to the scene event). the high volume data problem is also faced by many modern technical systems like computer vision systems which need to deal with a multitude of image pixels at any point in time. physiological research has illustrated that biological vision systems use neuronal circuits to extract movement in the visual scenes [cit] . biological visual systems are intrinsically complex hierarchical processing systems with diverse specialised neurons, displaying very powerful specific biological processing functionalities that traditional computer vision techniques have not yet fully emulated [cit] . another important finding during the last decades, that most neuromorphic designers may overlook is the fact that processing of the visual information is not serial but rather highly parallel [cit] and hence such implementations should target parallel architectures."
"others have examined the performance portability of opencl kernels from different domains [cit] . in all cases, the authors conclude that \"auto-tuning\" (i.e. writing kernels in a parametrised fashion and determining optimal parameter values at runtime) is an appropriate method of achieving portability across hardware. this methodology is well-suited to opencl; library calls are provided for obtaining detailed hardware information, and kernels can be compiled with different parameter values at run time."
"gpus from different vendors are divided into compute units and processing elements in different ways: on intel gpus, each execution unit is a compute unit of 8 processing elements; on nvidia gpus, each stream multiprocessor (compute unit) consists of 8, 32 or 192 \"cuda cores\" (processing elements) on the tesla, fermi and kepler architectures respectively; and on amd gpus, a compute unit since gpus rely on a cpu host, communication with the host can impact application performance. integrated gpus can communicate with the host through shared memory, but all data to be used by discrete gpus must be transferred explicitly to the device via pci-express (pcie)."
"combustor is regarded as a chamber to convert chemical energy into heat, and the combustion time delay is assumed to be ignored. some empirical formulas are used to calculate the burning process and the pressure loss [cit] . equation (2) shows the calculating process of the combustor:"
"all of the cpu cores in a node are grouped into a single device. for cpus with hyperthreading support, each hyperthread appears as a compute unit (e.g. the e3-1240 used in this work has 4 cores, but 2-way hyperthreading, and thus 8 compute units). the amd and intel runtimes expose the simd architectures of modern cpus differently; the amd sdk generates sse/avx code only if opencl's vector types (e.g. float4, float8) are used explicitly, whereas the intel sdk attempts to auto-vectorise kernels by packing contiguous work-items into simd execution units."
"similar to the calculation of compressors, performance maps including the mass flow to pressure ratio map f w,hpt and efficiency to pressure ratio map f η,hpt are used to calculate the parameters of the turbine outlet [cit] . equation (3) shows the calculating process of the high-pressure turbine:"
"the most similar md codes to the implementation presented here are the lammps gpu package [cit] and the md benchmark from the scalable heterogeneous computing (shoc) suite, both of which can be compiled to use opencl. they also both carry out a simple lennard-jones (lj) simulation, although with different parameters and initial conditions. shoc is intended to be run across multiple architectures, but its input is sufficiently different from the default lammps and minimd simulations that its performance is not necessarily representative. our opencl implementation does not build on either of these existing implementations -a serial, scalar baseline is more representative of a legacy application and will better enable us to identify the performance benefits of each individual optimisation."
"3) position gather: the loads on lines 4-6 become three gather operations; one for each of x, y and z. for w contiguous work-items, this equates to 3w scalar loads, from up to w cache lines. defining the position and force arrays using vector types (fig. 4) serves as a hint to the compiler that x, y and z are stored contiguously; for all of the hardware we consider, the full position of an atom can be loaded in a single memory access, and this hint permits the compiler to employ a more efficient gather (fig. 5) ."
"for all j from 1 to k. under this requirement, the value v ′ j should also not change. the corresponding change in v ′ j is equal to"
"a relatively new addition to the opencl standard is the ability to fission (i.e. split) a device into multiple subdevices, and issue work to them separately. we can leverage the existing mpi parallelism in minimd, combined with fission, to permit the use of n3 on cpus; specifically, we can start 8 mpi ranks and assign one opencl sub-device to each. we consider this optimisation only because it does not impact performance portability -although we re-introduce the neighbour force updates (lines 15-17, from fig. 2), we guard them with a pragma (i.e. #ifdef n3), and the code changes necessary to support fission are minimal, affecting only the initial opencl setup code. fission is not (currently) supported by gpus, but kepler's ability to support multiple mpi tasks feeding work to the same gpu could be used to the same effect."
"the inputs are given in figure 8a,b, and the states and outputs are shown in figure 9 . the integrated model and the traditional lpv model share the same states and matrices a and b according to equation (17), and thus, the responses of the states are the same in figure 9a . compared with the maximum absolute error (mae) of the nclm, the mae of the states (n l and n h ) is less than 1.32% and the root mean square error (rmse) is 0.77% in table 1 . the mae of the outputs of the integrated model is 0.45%, which is less than that of the traditional model. in table 1, the bold data shows that the integrated model significantly improves the accuracy of t 3, t 4, t 43, p 6 and, therefore, improves the average mae and rmse. in fact, the inputs inevitably contain noises caused by sensors or disturbances, and therefore, perturbations on the inputs should be considered when verifying the accuracy and real-time performance of the considered models. a normally distributed pseudorandom noise ω ∼ n(0, 0.004 2 ) is added to the fuel flow in figure 8a while the other three inputs remain the same. table 2 shows the time consumption of each model over a thirty-minute simulation. t c1 and n flowpath1 stand for the time consumption and the number of flow path calculations, respectively, with smooth inputs, while t c2 and n flowpath2 stand for the time consumption and the number of flow path calculations taking account of input noises, respectively. it can be concluded that the time consumption mainly depends on the number of flow path calculations. therefore, when the input noise is considered, the time consumption and the number of flow path calculations of the nclm increase evidently, while those of the integrated model and traditional lpv model are almost not affected. in table 3, the noise has a slight side effect on the accuracy of all the outputs of the two models. the bold data shows that, similar to table 2, the integrated model can also improve the accuracy of t 3, t 4, t 43, and p 6 and the average mae and rmse when the noises are considered."
