text
"while we have developed a new method to evaluate both spatial and temporal dynamics of connectivity and shown that this method provides more disease-specific information compared to other fc analyses,"
"the proposed system continuously tracks the movement of the user's eyeball and finds the position where the user is currently gazing on the screen. however, it is a waste of resources to perform such a function continuously even if the user does not need any operation. however, if the system is paused when no operation is needed, the system will lose the position information of the user's current eyeball since the system does not know where to start tracking once the system resumes. therefore, the performance evaluation program was made by first giving a position to start eye movement tracking and tracking the movement of eye movement started from this position as shown in figure 12 . in this figure, the red circle is the target position to which the eyeball should gaze. in the signal acquisition phase for training, the target circles on the screen were designed to reciprocate with a maximum of nine positions. in the performance evaluation stage, ten positions were selected in the same range to confirm the performance of how the svr can track eye movements when the eye moves to an arbitrary position that has not been trained."
"with (45) and (42), which are independent of m, we can obtain an approximated value of (39) as function of the statistics of the channel."
"in this class, rather than computing fc between pairs of regions that are spatially fixed based on a predefined atlas, we used a seed-based correlation approach to define a distribution of fc that varied in space."
"in this paper, we designed two kinds of experiments for performance evaluation and real application to menu selection using the proposed system. for performance evaluation, the target circle located at the center of the screen randomly changes its horizontal position and we measured the eog during that procedure. from this measurement, we calculated the difference between real horizontal position of target circle and expected gazing position from the developed system. in addition, this test also recognized the bite action that replaces the click action of the mouse interface. the second experiment is a real application of the proposed hci. for this, we developed an eog and emg-based pointing interface system that can be used as a mouse. through this experiment, we verified if the proposed eog and emg-based interface can easily applicable to menu selection command on a computer system as a new interface method."
"the replication dataset comprised 15 healthy volunteers (mean age 33.3 years, r 5 9.2 years, 14 male) and 12 patients with chronic schizophrenia (mean age 32.8 years, r 5 9.2 years, 10 male). the two groups were matched for age, pre-onset iq and years of education. the patients were diagnosed as per the standard operational criteria in the diagnostic and statistical manual of mental disorders iv (the official manual of american psychiatric association). all patients were treated with antipsychotic medication; in addition, four patients were receiving psychotropic drugs. to reduce the acute effects of antipsychotic medication on the day of scanning, patients were asked to abstain from their usual medication regime. the study protocol was approved by the addenbrooke's nhs trust local research ethics committee and all subjects provided informed consent in writing before participation."
"the rest of the paper is organized as follows. section 2 describes a fbmc/oqam-based communication system where channel impairments are combated either at the transmission or at the reception side. next, section 3 addresses the design of equalizers and precoders, which may have multiple taps. the effects of performing multitap filtering are studied in section 4. to that end, we provide analytical expressions to determine if multi-tap precoding boosts the average transmit power and if multitap equalization enhances the average noise power. in order to validate the closed-form expressions derived in section 4, some numerical results are presented in section 5. finally, section 6 draws the conclusions."
"to improve the generalization ability of the system, we used the support vector regression (svr) as a curve-fitting model. the curve-fitting model can represent a set of discrete data as a continuous function. thus, by using the curve-fitting model to the proposed system, the proposed system is capable of coping with any arbitrary eye movements not included in the training data set. svr is a kind of curve-fitting model used to solve the regression problem based on the existing support vector machine (svm) that has excellent modeling and prediction capabilities. in this paper, we used the chang and lin's model that can be found on \"libsvm' library [cit] . since the performance of svr depends on the type of kernel functions, we tested three types of kernel functions including polynomial kernel function, sigmoid kernel function and gaussian kernel function. for the performance test, we set 90% of data for training the svr and 10% of data for test and repeated 10 times without duplication of the test data using a 10-fold cross validation method. figure 10 shows the effects of the types of kernel functions on svr performance. in these plots, the horizontal axis shows the actual moving distance of the gazing position and the vertical axis shows the prediction results of svr. as shown in these plots, result plot of svr with gaussian kernel type showed the best results with 0.99317 of r value (multiple correlation coefficient value)."
"thirdly, the possibility of intra-scan sleep cannot be excluded. as shown previously, sleep can influence resting state dynamics. in the present study, participants were given detailed instructions to stay awake; beyond that, no measures had been employed to identify or control any potential effects of sleep."
"the signal passed through the preprocessing filter is fed to the instrumentation amplifier for differential amplification. the instrumentation amplifier requires high common mode rejection ratio since the amplitude of the common mode signal to be removed is several ten times bigger than the eog signal to be obtained from this system. therefore, we applied 'ina128′, which is a widely used ic chip for amplification of bio-signals and the designed circuit has 120 db of common mode rejection ratio. in addition, the gain of the instrumentation amplifier is set to 56 v/v to obtain a magnitude of ±50~±150 mv which is sufficiently larger than the white noise that can be added in the subsequent based on the time point t 1, the left side shows the signal from slow eye movements and the right side shows the signal from fast eye movements. when using a high-pass filter with 3.0 hz cut-off frequency, it is advantageous to detect the time when the eyeball moves since a waveform having large amplitude appears only in a section in which the eyeball moves, similar to the case of taking a derivative. however, it can be seen that the filtered signal varies greatly according to the velocity of the eyeball movement, and since the filter attenuates a large part of the frequency band of the signal, the size of the waveform itself is reduced and it is vulnerable to noise. on the other hand, when using a high-pass filter with 0.008 hz cut-off frequency, when the eyeball does not move, it shows a typical eog signal pattern showing a flat waveform, so that the change in the eyeball motion speed does not greatly affect the signal shape changes. therefore, it is suitable to analyze the moving distance of the eyeball using the degree of the rise or fall of the signals. also, since the signal strength of the desired portion of the signal is big enough, it is more robust against noise. however, since it requires a relatively long time to stabilize the signal close to '0, there is a disadvantage that it is difficult to recover the waveform if a large noise is added during the measurement such as when an impact is applied to an electrode or a cable. when a high-pass filter with 0.1 hz cut-off frequency is used, the characteristics of the two cases mentioned above are shown together and can be selected as a compromise. based on this, to consider the fact that the system requires precise analysis of eye movement distance, high pass filter with 0.008 hz cut-off frequency was selected and applied."
"to determine if other techniques behave similarly to the frequency sampling approach, p t 1 has been computed when the taps of the precoders, i.e.,"
"to provide summary statistics of temporal variations in the spatially varying fc map for each rsn. the total possible number of features in practice, a feature selection heuristic would typically be employed to identify a single combination of rsns, rather than undertaking an exhaustive search. however, the objective of this analysis was to comprehensively characterize average performance across all possible combinations, without any dependence on a particular feature selection strategy."
"the human-computer interface (hci) is one of the key technologies used to improve the interaction between human and computer. in terms of usability of hci, it is very important to study interfaces that can give commands to computers in a more human-friendly way. there are various interface approaches for hci including voice, gesture, electroencephalogram (eeg) from brain, eye movement, etc., and the characteristics of each type are clear [cit] . first, the voice-based hci is a technology that receives the voice of a user by using a microphone and recognizes the user's intention through language processing. this technology has been steadily evolving since recognizing certain commands and has evolved into technologies such as voice typing and natural language processing [cit] . this method is very attractive since it is very intuitive and can directly transmit the user's command. however, the performance of this system is very dependent on the type of users, ambient noise, and performance of natural language processing. gesture-based hci can division. to reduce the signal complexity for real time eyeball tracking, modified sliding window algorithms are applied for piecewise linear approximation (pla). the width and height of the line segment and slope and length of the line segment are selected as a feature set for eyeball tracking. to find the eyeball position, support vector regression (svr) is applied as a curve-fitting model. a developed system is applied to an eyeball mouse interface to select horizontally listed buttons on the computer screen and this can be a practical alternative interface particularly for a disabled person with quadriplegia to access computer or mobile devices. through the pla and parameter optimization, proposed system minimized the consumption of hardware resources and it can be easily implemented on open-source platforms including arduino and raspberry-compatible modules."
"studies of dynamic fc have invariably focussed on characterizing the temporal dynamics of fc, with little consideration given to any possible dynamics in the spatial layout of rsns. the spatial extent of functional networks is typically defined using a parcellation atlas or with ica, both of which enforce anatomical boundaries that are fixed in space over all time. here, we aim to establish whether resting-state fc exhibits meaningful spatial dynamics and whether spatial dynamics can improve single-subject prediction of schizophrenia diagnosis. while it is known that functional brain networks exhibit spatial dynamics during task performance [cit], little is known about whether these spatial dynamics persist in rest or whether they are altered in disease. in addition, neuropsychiatric disorders such as schizophrenia are typically characterized by reductions in gray matter volume and these reductions can remain even after registration to a standard template. allowing the spatial extent of functional network boundaries to vary between individuals and over time can in principle account for reductions in gray matter volume, since the spatial extent of a functional network is inherently reduced to match the extent of atrophy. while some studies have sought to characterize the spatio-temporal dynamics of rsns in healthy subjects [cit] and in schizophrenia patients (ma, calhoun, phlypo, & adalı, 2014), these studies assume spatial independence among networks; further, they have not evaluated the extent to which dynamic fc can improve single-subject prediction of diagnostic status."
"recent evidence suggests that neuropsychiatric disorders are associated with marked abnormalities in dynamic fc and that these dynamic therefore, the dynamic properties of functional brain networks merit further study in the context of candidate biomarkers for clinically useful predictive models in psychiatric disorders such as schizophrenia."
"analogously to the previous section, the average power of the equalized noise within one subband at the receiver side will be analyzed in this part. according to figure 1, the received signal before the detection process can be described aŝ"
"alternatively, in the second approach, the system exclusively relies on the equalizers to combat the negative effects of the channel. thus, (3) is simplified tô"
"rather than employing carefully constructed null models to address the vexed question of whether fc is dynamic and non-stationary [cit], we have explicitly demonstrated that spatio-temporal dynamics in the resting state can effectively characterize disease pathology in a serious psychiatric disorder. our work establishes the utility of studying spatiotemporal dynamics in resting-state fmri, at least in psychiatric disorders, irrespective of whether these dynamics satisfy statistical tests of non-stationarity."
"(a) based on the time point t1, the left side shows the signal from slow eye movements and the right side shows the signal from fast eye movements. when using a high-pass filter with 3.0 hz cut-off frequency, it is advantageous to detect the time when the eyeball moves since a waveform having large amplitude appears only in a section in which the eyeball moves, similar to the case of taking a derivative. however, it can be seen that the filtered signal varies greatly according to the velocity of the eyeball movement, and since the filter attenuates a large part of the frequency band of the signal, the size of the waveform itself is reduced and it is vulnerable to noise. on the other hand, when using a high-pass filter with 0.008 hz cut-off frequency, when the eyeball does not move, it shows a typical eog signal pattern showing a flat waveform, so that the change in the eyeball motion speed does not greatly affect the signal shape changes. therefore, it is suitable to analyze the moving distance of the eyeball using the degree of the rise or fall of the signals. also, since the signal strength of the desired portion of the signal is big enough, it is more robust against noise. however, since it requires a relatively long time to stabilize the signal close to '0′, there is a disadvantage that it is difficult to recover the waveform if a large noise is added during the measurement such as when an impact is applied to an electrode or a cable. when a high-pass filter with 0.1 hz cut-off frequency is used, the characteristics of the two cases mentioned above are shown together and can be selected as a compromise. based on this, to consider the fact that the system requires precise analysis of eye movement distance, high pass filter with 0.008 hz cut-off frequency was selected and applied."
"performance evaluation was performed on 4 out of 7 subjects participating in signal acquisition for training the system. each subject performed 300 eye movements for testing. the spending time for this experiment was dependent on the subject. we did not give specific time guidelines for repetition of this experiment since each subject showed different levels of eye fatigue. so, we suggested that the subjects re-participate in this experiment after they had fully rested when they felt eye fatigue. in general, they continued the experiments for approximately 30 s to 1 min and then took rest. all subjects were male and aged between 20 and 30 years old. for the performance evaluation, two performance evaluation indices were applied. the first performance evaluation index was obtained by measuring the difference in units of cm on a 51 cm monitor between the position of the target circle and the tracking position of eyeball using the proposed system as shown in figure 13 . as shown in this figure, the significant difference by subject is not significant. in this figure, minimum error was measured at positions −3 and 3 which is the median position of horizontal range. in addition, it can be seen that there is a similar error value at the symmetric position with respect to reference 0 point. this infers that the cause of error may be coming from the approximation of the movement of the eyeball to the linear motion, whereas the human eyeball movement is a rotational motion with varying angle. therefore, it infers that the error increases as the difference between linear motion and rotational motion becomes larger as the position deviates from the central position where the eyeball motion starts. target circle and the tracking position of eyeball using the proposed system as shown in figure 13 . as shown in this figure, the significant difference by subject is not significant. in this figure, minimum error was measured at positions −3 and 3 which is the median position of horizontal range. in addition, it can be seen that there is a similar error value at the symmetric position with respect to reference 0 point. this infers that the cause of error may be coming from the approximation of the movement of the eyeball to the linear motion, whereas the human eyeball movement is a rotational motion with varying angle. therefore, it infers that the error increases as the difference between linear motion and rotational motion becomes larger as the position deviates from the central position where the eyeball motion starts. the second performance index is the time (in units of seconds) required to complete the dragand-drop operation after the center circle has moved to an arbitrary position as shown in figure 14 . although the required time varies by subject, it can be confirmed that general trend for each target positions are similar."
"methodologically, we employed a sliding window of fixed duration to assess temporal fluctuations in connection strengths. interactions among different brain regions can have different durations at different times; therefore the approach of a fixed window length may not be optimal in capturing these variations. despite the criticism leveled at the sliding window method [cit], in line with many other studies [cit], we have shown that this method can be effective in delineating disease states. for analysis 2, feature selection was performed by ranking features according to t statistic magnitude. this could have resulted in the selection of correlated and thus redundant features. multivariate feature selection heuristics such as minimum-redundancy-maximum-relevance (mrmr) can alleviate this problem [cit] and potentially improve classification performance."
"fc was measured between pairs of regions comprising several canonical rsns. we considered 14 previously delineated rsns [cit] . each rsn comprised multiple spatially contiguous cortical and/or subcortical regions, resulting in a total of 90 regions across the 14 rsns. while mutual exclusivity among regions was not explicitly enforced, most regions did not share common voxels."
"ten-fold cv: individuals (patients and controls) were randomly divided into 10 equally-sized samples. the svm was then trained using 9 of the 10 samples and the remaining held-out sample was used to evaluate classification accuracy. this was repeated 10 times, each time holding out a different sample to evaluate accuracy. classification accuracy, specificity, sensitivity and auc were averaged across the 10 folds."
", ∀q, then we can state that the increase/decrease of the transmit power and the increase/decrease of the noise power for using multi-tap filtering coincide because this relation is satisfied"
"firstly, we define rsns based on templates from a previous study [cit] and have not investigated other modes of network definition. secondly, the influence of medication on classifier performance is unknown. all the patients were diagnosed with schizophrenia and medicated for several years, which is a potential confound. physiological confounds or the effects of antipsychotic medication can potentially impact resting-state fc dynamics, and thereby impact classifier performance."
the residuals from this regression were spatially smoothed using a gaussian kernel of full-width at half-maximum (fwhm) of 4mm. any linear trend was removed from each voxel time course and temporal band-pass filtering (0.01-0.1 hz) was performed to reduce the effects of low frequency drifts and high frequency physiological noise [cit] . the resulting time courses were used for further analyses.
we have demonstrated that characterizing the dynamics of restingstate fc in both time and space can provide substantially improved single-subject prediction of schizophrenia diagnosis compared to conventional static characterizations of fc. our novel methodology involves jointly mapping temporal and spatial dynamics in fc and combines sliding-window and seed-based correlation analyses. our findings provide complementary evidence that suggests dynamic fluctuations in resting-state connectivity are of clinical utility and cannot be trivially ascribed to sampling variability and/or intra-scan head movement.
"since the signal after pla appears as a set of line segments, the width in the horizontal direction and the height, slope, or length in the vertical direction can be considered as feature parameters to represent the eog signal. these features can be obtained by simple computation from the shape of the line segment and they can express the information of the line segment intuitively [cit] . in this paper, we define the two-feature set f hv that represents width and height of line segment and f θl that represents slope and length of the line segment."
"all scans were acquired using a 1.5 tesla ge signa scanner (general electric, milwaukee, wi) located at the bupa lea hospital, cambridge, uk. resting-state functional images were acquired using t2*-weighted epi sequence, as participants laid quietly in the scanner with eyes closed. imaging parameters were: tr 5 2 s, te 5 40 ms, flip angle 5 708, voxel size 5 3.05 3 3.05 3 7 mm, slice gap 5 0.7 mm and number of volumes 5 512. further details on the demographics and acquisition of this dataset can be found in a previous study [cit] (friston 24-parameter model; [cit] ) and signals from the white matter and the ventricles were regressed from each voxel time course, to account for head motion and physiological noise."
"from the closed-form expressions derived in this paper, we may conclude that if the transmit power increases/ decreases, then the noise variance increases/decreases as well with the same magnitude, as long as the same filters are used as precoders or equalizers. this reveals that if the transmitted symbols are properly scaled when the transmit processing boosts the power, then there is no degradation due to equalizing the demodulated data instead of precoding the symbols to be transmitted. in view of this discussion, it is of paramount importance to determine if the power is boosted due to transmit processing operations and to state if some normalization is required. the analysis that has been conducted allows us to approximately formulate the power as the function of the statistical expectation of precoders, which depends on the statistical channel information. thus, a priori knowledge of channel statistics is required. it is worth mentioning that many algorithms, such as the mmse channel estimation, also make use of channel statistics information [cit] . alternatively, we may compute the instantaneous transmit power given the precoders. however, the power has to be recalculated if precoders are updated to adapt http://asp.eurasipjournals.com/content/2014/1/84 to the new channel conditions. simulation-based results confirm that the method based on the statistical channel information characterizes the transmit power with reasonable accuracy. hence, the proposed method may be used to determine if the transmitted symbols should be scaled or not without computing the instantaneous power, thus reducing the complexity."
"electromyogram is a measure of activity potential produced by the activity of the skeletal muscles and it usually has an amplitude ranging from 50 µv to 30 mv. the skeletal muscles under the control of one motor neuron contract when the particular motor neuron is excited, and the muscles are excited or contracted almost simultaneously through connected motor neurons. since the excitation of shock repeats at a frequency of 7 to 20 hz, emg is also measured in this frequency band. the temporal muscle or temporalis is the muscle located from the eyebrows to the cheekbone on both sides of the head. it is classified as a muscle of mastication and serves to move the lower jaw to chew food. therefore, when the emg is measured in the temporal muscle, a signal with large amplitude is generated in the case of bending on the muscle for bite, and a signal with small amplitude is generated when the bending force is released."
"in this paper, we developed a single channel bio-signal-based hci system using pla and svr. with only two electrodes, a developed bio-signal sensing system can measure and use eog and emg signals independently at the same time by frequency division and signal processing. for real-time processing, a modified sliding window pla algorithm is developed to reduce the signal complexity. to enhance the eye tracking performance, svr algorithm is applied and it can enhance the generalization capability. a developed hci system can perform operations similar to dragging and dropping used in a mouse interface in less than 5 s with only eyeball movement and bite action. compared to conventional eog-based hcis that detects the position of eyeball only in 0 and 1 levels, a developed system can continuously track the eyeball position less than 0.2 s. in addition, compared to conventional eog-based hcis, the reduced number of electrodes can enhance interface usability."
". nevertheless, it is difficult to formulate p t1 in a closed-form expression, since the expectation of a fraction cannot be computed straightforwardly. in this sense, appendices 1 to 4 give details on how to obtain an approximate value of (21) . one alternative to compensate the possible boost of power due to multi-tap precoding is to evaluate (21) given the precoders, which is equivalent to dropping the expectation, and then using this value to compute β. while this ensures that the power is not increased, it entails the recalculation of β whenever the precoder is modified. if we are able to characterize the expected value of p t 1, then β does not have to be updated since its value is based on the statistical knowledge of the channel. the reduction of the complexity burden justifies the attempt to derive an analytical expression of (21)."
"to provide summary statistics of temporal fc dynamics for each pair of regions. it is important to note that l ij is not necessarily equal to the static fc between regional pair i; j ð þ computed under class i. standard deviation provides a simple characterization of dynamics that has been extensively used as a test statistic for dynamic behavior [cit] . in class ii, the total possible number of features used for classification was 2m, namely, the mean and standard deviation of q ij t ð þ for m pairs of regions."
". (20) it is important to remark that the discrete orthogonality conditions restrict r f m − bearing in mind the above discussion and assuming that the orthogonality properties are satisfied, p t 1 can be compactly formulated as"
"in the formula for calculating power(i), it can be seen that the temporal emg signal is squared and then the time average is obtained between the two vertices p x i and p x i+1 where the i-th line segment appears. as shown in figure 11b, since the dc component is removed by the high pass filter and the temporal muscle emg signal is concentrated around 0, the amplitude of the squared emg signal becomes more prominent when the bite action occurs as shown in figure 11c . afterwards, the power(i) is compared with the threshold value th, and if power(i) is greater than th, we conclude the measurer to be in a state of bite and vice versa. figure 11d conceptually shows that temporal muscle emg is classified according to bite action by applying threshold value th."
"as part of analysis 1, the predictive power of every possible network combination (16,383) was evaluated for each of the four classes of fc. for each class, the combination which provided maximum classification accuracy is listed. the abbreviations are as per table 2 ."
"the dynamics of fc are classically construed and analyzed as statistical dependencies that unfold in time [cit] ) . a novel contribution of this study is to provide a methodology to enable mapping fc dynamics that unfold in space and time, such that the spatial extent of each region is permitted to shrink/grow within a local neighborhood."
"in this paper, we designed two kinds of experiments for performance evaluation and real application to menu selection using the proposed system. for performance evaluation, the target circle located at the center of the screen randomly changes its horizontal position and we measured the eog during that procedure. from this measurement, we calculated the difference between real horizontal position of target circle and expected gazing position from the developed system. in addition, this test also recognized the bite action that replaces the click action of the mouse interface. the second experiment is a real application of the proposed hci. for this, we developed an eog and emg-based pointing interface system that can be used as a mouse. through this experiment, we verified if the proposed eog and emg-based interface can easily applicable to menu selection command on a computer system as a new interface method."
"in this analysis, we firstly implemented a feature selection step to identify the most distinguishing features and these features were this analysis assessed classifier generalizability and performance on new, unseen data."
"for each chosen combination of rsns, ten-fold cv was performed 30 times, each time with a different (random) partition of the dataset into 10 folds. between two separate runs of ten-fold cv, none of the folds contained exactly the same subjects, avoiding potential sampling bias. this cv procedure was repeated independently for each of the four classes of fc, yielding a distribution of classification accuracy, sensitivity and specificity across combinations of rsns for each class. for a given class of fc and a chosen network combination, the mean and standard deviation of accuracy, sensitivity and specificity were calculated over 30 runs of tenfold cv."
"in order to determine if the addressed subband processing leads to an increase of the transmit power, we shall compare (12) with (14) . to simplify the analysis, we first split (12) into two terms as follows:"
"it is worth mentioning that by definition, classes i and ii consider both intra and inter-network dynamics whereas classes iii and iv incorporate only intra-network dynamics. previously, many studies on temporal dynamic fc have analyzed between-network interactions in schizophrenia. [cit] reported hyperconnectivity between thalamic and sensory networks in patients; [cit] observed hypoconnectivity among default mode occipital and cinguloopercular networks; [cit] while the relative differences in classification performance between the four fc classes were largely preserved in an independent dataset, performance was overall reduced (figure 9 ). this reduction in classifier performance might be due to one or more differences between the two datasets: dataset 1 comprised more subjects (n 5 82), whereas dataset 2 had only 27 subjects; this may induce truncation errors in performance measures. also, dataset 1 was of a higher spatial resolution than dataset 2 and was acquired at a higher field strength, both of which impact on the signal-to-noise ratio of measured time courses. another consideration is that the datasets comprise schizophrenia patients with distinct clinical characteristics;"
"to illustrate the good spectral containment exhibited by the subcarrier signals, figure 2 shows the power spectral density (psd) of the signal transmitted on the tenth subchannel when the air interface is based on ofdm and fbmc/oqam. the psd is normalized so that the gain in the passband region is 0 db. finally, the real-valued pam symbols are estimated after compensating the phase term and extracting the real part,"
"the measured eog signal using the proposed system showed potential difference only at the time of eyeball movement. even if the speed of eyeball is very slow, still it showed very stiff potential difference like a step function. in this case, only the time of eyeball movement and the position of the eyeball are important data and we do not need to consider small signal change information during eyeball fixation. therefore, pla would be a good way to reduce signal complexity. there are various pla algorithms including sliding window [cit], scan along [cit], top down [cit], and bottom up [cit] . several hybrid algorithms are also developed to take advantage of both algorithms at the same time. in this paper, we designed a modified sliding window-based pla algorithm that is best for real-time signal processing. top-down-and bottom-up-based algorithms showed good approximation performance but were not appropriate for real-time signal processing applications since they require batch processing. some research results showed modified top down or bottom up-based algorithms for real-time applications but still required some time delay to get the responses. compared to these methods, sliding window and scan along-based algorithms showed quick processing responses. another advantage of the sliding window-based method is that it can change the signals shapes to fit to our applications during calculation of error between the original signals and approximated signals. figure 4 shows the pseudo code of the standard sliding window algorithm. as shown in this code, when we increase the count number of p 2, we need calculate the error at every point between p 1 and p 2 which causes calculation burden. in addition, error, and threshold (th)-based creation of vertices at every point sometimes showed poor approximation performances."
"as shown in previous experimental results including performance evaluation test and application test for menu-selection command interface for computer, the proposed system showed acceptable performance in terms of accuracy, speed, and usability. compared to conventional eog-based hci, the proposed interface system has a different way in tracking gaze positions. in general, the eog-based hci or eog mouse detects the up, down, left, and right movements or blinking of eyes using pattern recognition algorithms and uses them as commands. in other words, in conventional ways, only a series of discrete commands can be executed, and it cannot recognize variance of gazing positions since it cannot determine how much the eyeball has moved. in general, the performance of a eog-based mouse system is measured using accuracy, sensitivity, specificity, or precision since it is in the form of pattern recognition for up, down, left, or right movements. however, we cannot use same performance measures since we continue to track the movement of the eyeball, so we used two kinds of performance measures, traction error and required time to operate as shown in the experimental results. the average difference between position of target circle and position from prediction of the proposed system showed only 1.4 cm difference on 51 cm screen. since we set the diameter of target circle to 3 cm on a 51 cm screen, 1.4 cm is small enough to use for mouse control. the maximum variation of tracking error according to the subjects is less than 0.3 cm, so we can say that the proposed system is not user-specific. in terms of required time to drag-and-drop command, the system showed 1.97 s of average time, indicating an acceptable level as alternative interface method without using hands. the maximum variation of required time according to the subjects is 0.9 s. compared to the average of required time to command, 0.9 s is not small, but it cannot be considered as a difficult problem since this difference was based on the familiarity of the subject."
"since the frequency spectrum of p[n] is confined within the interval − 2π m 2π m, the subcarrier signals only overlap with the most immediate neighbors. as a consequence, the roll-off factor is equal to 1, which implies that the summation zone in (3) solely encompasses the values q − 1, q, q + 1 . it is important to mention that the effect of the channel does not widen the bandwidth occupied by the subcarrier signals. therefore, the limitation done in formula (3), in terms of number of included summation elements, is accurate and does not have any influence on the performance of the receiver. the assumption that inter-carrier interference comes from the adjacent subcarriers would be wrong only in the presence of very high carrier frequency offsets. however, we assume that there is perfect synchronization between the transmitter and receiver, since we do not want to focus on the influence of the synchronization errors, rather our goal is to concentrate on the impact of multi-tap filtering on the transmit/noise power boost."
"it is worth mentioning that the variable s in (10) cannot be omitted because the equality [cit] } are function of the channel frequency response (cfr). since it is customary to model the taps of the channel impulse response as random variables, the cfr evaluated at any given frequency is also a random variable, and therefore, the taps of the precoders are random variables that are uncorrelated with the symbols. incorporating (10) and (11) into (9) results in"
"features selected according to the scheme described in analysis 2 (section 2.5). classifier performance was then plotted as a function of window length for both the dynamic-in-time classes (class ii and iv). the same process was used to evaluate the impact of variation in neighborhood size, where size varied from 0 mm (no neighboring voxels) to 10 mm. variation in neighborhood size was only relevant to classes iii and iv, where the spatial extent of each region was permitted to shrink/grow within its neighborhood."
"four different classes of static and dynamic fc were inferred from resting-state fmri data acquired in schizophrenia patients and healthy controls. we independently trained a svm for each class of fc to perform single-subject prediction of diagnostic status. as detailed below, classifiers incorporating both the temporal and spatial dynamics of resting-state fc consistently achieved substantially higher classification performance than classifiers based on static connectivity characterizations. svms were trained using three distinct schemes: analysis 1 exhaustive evaluation of classification performance across all possible feature combinations; analysis 2 selection of a single set of discriminatory features, followed by training and evaluation using crossvalidation; and, analysis 3 nested cross-validation. analysis 1 enabled evaluation of average classification performance, independent of a feature selection heuristic, whereas analyses 2 and 3 quantified classifier generalizability. high and low connected states, compared to that in control subject (video s1). 2007; [cit], this network only features in two of the four classes, with its spatial dynamics not used in class iii."
"electrooculogram is a signal that measures and records the resting potential of the retina. the human eye works as a dipole in which the front cornea is an anode and the rear part is a cathode. therefore, when the eyeball moves and the cornea approaches one electrode, the potential of the corresponding electrode rises, and the potential of the opposite electrode falls. the most commonly used electrode attachment position for eog is near the outer angle of both eyes as shown in figure 1 and the position information of horizontal direction can be obtained by the pair of electrodes. eog and emg signal based on frequency division. to reduce the signal complexity for real time eyeball tracking, modified sliding window algorithms are applied for piecewise linear approximation (pla). the width and height of the line segment and slope and length of the line segment are selected as a feature set for eyeball tracking. to find the eyeball position, support vector regression (svr) is applied as a curve-fitting model. a developed system is applied to an eyeball mouse interface to select horizontally listed buttons on the computer screen and this can be a practical alternative interface particularly for a disabled person with quadriplegia to access computer or mobile devices. through the pla and parameter optimization, proposed system minimized the consumption of hardware resources and it can be easily implemented on open-source platforms including arduino and raspberry-compatible modules."
"here, we focussed on developing reliable and accurate machine classifiers to predict the diagnosis of schizophrenia in individuals based on their resting-state fmri scan. we demonstrated that the prediction of diagnostic status can be substantially improved by modeling the dynamic properties of fc within key resting-state networks. in particular, we were able to reliably predict diagnostic status with accuracy exceeding 90% when both temporal and spatial dynamics of fc were taken into account by the machine classifier (class iv). in contrast, when only static measures of fc were utilized by the classifier, accuracy plummeted to below 80% (class i)."
"when scrubbing was performed to correct for head motion, classification accuracies remained largely unchanged for all fc classes (supporting information figure s5, analysis 2). scrubbing resulted in an overall slight improvement in classification accuracy, while the relative differences between the four classes were preserved. this suggests that intra-scan head micro-movements are unlikely to account for the substantially improved performance achieved with dynamic classes of fc. repeating analysis 1 for an alternative set of seed regions indicated that classifier performance was insensitive to seed region choice (supporting information figures s1 and s6 ). classification performance evaluated using analysis 3. 60% of data was used to build classifiers, which was then used to classify the remaining data potential has yet to be realized in clinical practice, primarily due to the lack of accurate and reliable predictive biomarkers. the development of reliable neuroimaging biomarkers that provide high predictive value at the single-subject level is therefore crucial to enable the field of psychiatry to progress to an era of precision medicine."
"it is common practice in fc studies to define regions of interest either using atlases, functional parcellations or in a meta-analytical manner in which a spherical roi is defined around the peak activation coordinates, followed by averaging the time courses from all voxels in the roi to obtain a representative time course for the roi. both this approach and alternative ica based methods impart a fixed spatial layout on the rsns. in contrast, we developed a simple methodology to accommodate spatial variability in fc."
"after all analog signal processing steps have been performed, the adc processed signals are sent to the pc. the adc process is performed using a microcontroller with a 10-bit resolution and a sampling frequency of 300 hz. then, the converted digital data is transferred to usart to usb conversion ic chip through usart communication method and data can be transmitted through a usb cable connected to the computer. the usb cable also supports the 5 v power to the measurement system. to prevent electric shock from unexpected damage or leakage, the micro controller and the usart to usb conversion ic chip are electrically isolated. the atmega8 microcontroller is used in this hardware and input voltage of this system is 5 v. this system consumes 1.8 ma at 5 v, so the power consumption of this system is around 9 mw."
"the choice of window length, seed region and neighborhood size are rather subjective, and thus we evaluated the sensitivity of classification performance to variations in these key parameters. for reasons of computational tractability, these sensitivity analyses were performed with respect to analysis 2. the window length was varied from two tr points (4 s) to 234 tr points (7.8 min, the static case for dataset 1)."
". (31) the denominator on the left-hand side corresponds to the transmit power, when precoders have a single tap. the denominator on the right-hand side accounts for the summation of the average noise power in all subcarriers, when the length of the equalizers is one. in light of condition (31), we may conclude that if symbols are properly scaled when the transmit power increases, then there is no degradation for counteracting the channel at reception rather than at transmission, as long as the same filters are used as precoders or equalizers."
"single-subject prediction of diagnosis, illness outcome and treatment response offers significant potential to influence clinical decision making in neuropsychiatry [cit] . to date, single-subject predictions inferred from fmri have largely focussed on static fc and other static properties of the bold response. in particular, multivariate pattern-recognition techniques (machine learning) applied to static fc measures have been trained to distinguish psychiatric patients from healthy controls with accuracies ranging between 60% and 80%, depending on the disorder and illness severity [cit] . to be useful for clinical practice, biomarkers with improved accuracy, reliability and predictive value are essential [cit] ."
"given that the regression of motion parameters is not sufficient to eliminate variance related to head motion [cit], further motion correction was performed by censoring highmotion volumes in each individual [cit] . specifically, volumes with a frame-wise displacement (fd) exceeding 0.5 mm were censored, where fd measures the extent of head movement from one volume to the next, and is calculated as the sum of the absolute values of the differentiated realignment estimates [cit] ."
"eog and emg have different signal characteristics in terms of signal amplitude and frequency ranges. therefore, the emg of the temporalis muscle can be obtained by frequency division of the measured signal which is shown in figure 11a . in this figure, we cannot recognize the emg since the amplitude of emg is much smaller than eog. in general, emg has a relatively bigger amplitude than eog. however, in this case, the amplitude of eog is bigger than the amplitude of emg since the electrodes are attached to the temple on both sides which is relatively far from temporalis. the frequency division is performed using a butterworth 4 pole 30 hz high-pass filter designed by the iir method, and the obtained temporal emg signal by iir filter is shown in figure 11b . since the frequency range of emg signal does not overlap with eog signals, we can use both signals independently at the same time. for this, the proposed system analyzes these temporal muscle emg signals and determines if the line segment information obtained through the pla process is generated in bite action or not. for this, when the pla outputs new segment information, only the temporal muscle emg of the same interval as the time segment of the corresponding line segment is separated and the signal amplitude within that interval is determined. when the temporal emg according to the time is emg(t), the intra-segment power power(i) for the i-th segment can be expressed by the following equation."
"the proposed system can be applied in various ways for hci or hmi. as one example, we developed eog and emg-based pointing interface system that can be used as a mouse. the developed system is similar to the performance evaluation program which is shown in section 3.1., but it is more user friendly for convenience of use. first, when you run the program, you will only the second performance index is the time (in units of seconds) required to complete the drag-and-drop operation after the center circle has moved to an arbitrary position as shown in figure 14 . although the required time varies by subject, it can be confirmed that general trend for each target positions are similar. target circle and the tracking position of eyeball using the proposed system as shown in figure 13 . as shown in this figure, the significant difference by subject is not significant. in this figure, minimum error was measured at positions −3 and 3 which is the median position of horizontal range. in addition, it can be seen that there is a similar error value at the symmetric position with respect to reference 0 point. this infers that the cause of error may be coming from the approximation of the movement of the eyeball to the linear motion, whereas the human eyeball movement is a rotational motion with varying angle. therefore, it infers that the error increases as the difference between linear motion and rotational motion becomes larger as the position deviates from the central position where the eyeball motion starts. the second performance index is the time (in units of seconds) required to complete the dragand-drop operation after the center circle has moved to an arbitrary position as shown in figure 14 . although the required time varies by subject, it can be confirmed that general trend for each target positions are similar."
"while the yellow border encapsulating each region represents neighboring voxels. neighboring voxels include all voxels within a �6 mm distance outer to every network region, in all directions. this neighborhood delineates a space in which regions can dynamically shrink/grow as a function of time. each rsn is associated with a single seed region (green) that was used for conventional seed-based connectivity analyses [cit] . here, we evaluated a variety of window lengths, and a length of w520 s (10 trs) was chosen. the effect of different window lengths was also assessed (see section 2.6). for a window length of w and an acquisition comprising t time points, the total number of windows was given by j5t2w11."
", without resorting to neither precoders nor equalizers. however, p[n] has to be designed to satisfy the perfect reconstruction property [cit] . based on this, it is clear that the objective of precoding and equalizing the symbols is to counteract the channel. in other words, the equivalent responses given by"
"over the past two decades, blood oxygenation-level dependent (bold) functional magnetic resonance imaging (fmri) acquired during rest has emerged as a promising approach to understand complex brain function."
"namely, dataset 1 comprised only trs patients, while patients in dataset 2 were responsive to antipsychotic medication and showed milder positive and negative symptoms. out of the differences listed above, the only one that can be addressed post-acquisition is the difference in resolutions of the two datasets. to test the influence of resolution, we down-sampled images from dataset 1 to 4 mm isotropic resolution and performed analysis 1. we found that down-sampling voxel resolution did not worsen classification performance by more than 4%."
"the proposed system continuously tracks the movement of the user's eyeball and finds the position where the user is currently gazing on the screen. however, it is a waste of resources to perform such a function continuously even if the user does not need any operation. however, if the system is paused when no operation is needed, the system will lose the position information of the user's current eyeball since the system does not know where to start tracking once the system resumes. therefore, the performance evaluation program was made by first giving a position to start eye movement tracking and tracking the movement of eye movement started from this position as shown in figure 12 . in this figure, the red circle is the target position to which the eyeball should gaze."
"networks are evolving in such a way that different systems with specific characteristics co-exist in the same area. cognitive radio networks, where secondary users share the same spectrum as the primary licensed users, are a good example. in these systems, secondary users have to detect the spectrum conditions to transmit on those bands where the primary user is inactive [cit] . in this regard, there are initiatives to use spectral slots in the professional mobile radio (pmr) band and white spaces freed up by the current digital television system. in these scenarios, the devices that transmit in the unoccupied bands have to guarantee that no interference will be induced in legacy primary networks. therefore, future wireless communications should be able to transmit in a fragmented spectrum where different spectral components are unlikely to be *correspondence: marius.caus@upc.edu 1 department of signal theory and communications, universitat politecnica de catalunya (upc), barcelona 08034, spain full list of author information is available at the end of the article tightly synchronized. this observation highlights that it is deemed necessary to utilize spectrally agile waveforms, and thus, further research beyond the established orthogonal frequency division multiplexing (ofdm) technique is required [cit] . it is worth mentioning that due to the aforementioned features, other modulation formats are also considered for the next generation systems [cit] ."
"the aim of this study is to evaluate the extent to which biomarkers characterizing both the spatial and temporal dynamics of key rsns can improve the accuracy of machine-based single-subject prediction of schizophrenia diagnosis. we hypothesize that spatio-temporal dynamics in resting-state fc is altered in schizophrenia patients and that these dynamics distinguish patients from healthy controls with greater precision compared to static characterizations of rsns. to address this hypothesis, we developed a novel sliding-window based method to map both spatial and temporal fluctuations in rsns defined relative to a seed region. unlike complementary ica-based methods, our method does not enforce temporal or spatial independence between rsns, meaning that rsns can potentially overlap and share common regions at any time. using two independent datasets, we evaluated the accuracy with which schizophrenia patients can be distinguished from participants included 41 patients with treatment-resistant schizophrenia (trs; mean age 40.9 years, r 5 10.0 years, 28 males) and 41 age-matched healthy controls (mean age 38.3 years, r 5 10.5 years, 24 males). trs patients did not respond to at least two different antipsychotics in at least two trials [cit] and were taking clozapine [cit] . clinical and demographic characteristics are shown in table 1 and resolution 5 0.98 3 0.98 3 1.0 mm. resting-state fmri data was acquired for 8 min (234 volumes) using a t2*-weighted echo-planar imaging (epi) sequence with tr 5 2 s, te 5 40 ms, voxel dimensions 5 3.3 3 3.3 3 3 mm and matrix size 64 3 64."
"without the loss of generality, precoders are expressed as the function of a finite impulse response filter that is multiplied by a scalar that controls the power allocated to each subband. in consequence,"
"while the difference in correlation distribution of rsns is a new observation, it is in line with the theory of \"dysconnectivity\" among brain regions in schizophrenia [cit], which has consistently been supported by evidence from studies using multiple imaging modalities [cit] . our observation of more voxels remaining negatively connected in patient group would present as a reduction in fc in the case of fixed region analysis (equivalent to taking average correlation among voxels), as reported in classes i and ii, and in previous studies [cit] . however, this averaging reduces classification accuracy from 91.1% (class iv) to 84.5% (class ii, dynamic in time case) and further to 79.5% (class i, static in time case), as listed in table 4 . in addition, we have provided new insights into aberrant spatio-temporal fc dynamics in schizophrenia, which build on previous studies that exclusively investigate static fc in the disorder [cit] ."
"the patent of korea, kr-a-101539923, \"bio-signal-based eye tracking system using dual machine learning structure and eye tracking method using same\" invented by gyeong woo gang and tae seon kim is partially resulting from the work reported in this manuscript."
". the operation (.) ↓x performs a decimation by a factor of x. to combat the channel impairments at the receive side, demodulated signals can be processed with multi-tap equalizers a q [k] . at the output of the qth equalizer, we receive an estimation of the transmitted data that is written as"
"for class iv, both means and standard deviations of histogram bins were compared. these comparisons were performed for every network; figure 8 demonstrates the specific case of ddmn, where group level averages of each feature are shown. figure 8a stands for class iii; figure 8b shows the group averages of means of bins and figure 8c shows the averages of standard deviations of each bin. for the control group, there were significantly more voxels in many of the positive correlation bins, whereas in patients, negative correlations were more dominant. again, the dynamic case (class iv) had a greater number of significantly different bins compared to the static case (class iii). while only the case of ddmn is presented here, a similar trend was observed in the case of other rsns as well."
"the euclidean error method also can overcome the overshooting phenomenon commonly found in eog waveforms. the overshooting phenomenon occurs when the user attempts to move the eyeballs to the target position. if eyeballs move to the beyond the target position-in other words if eyeballs fail to move to the correct position at a time-then they return to the original target position quickly, creating an overshooting phenomenon. as shown in figure 9, the euclidean error method can detect vertices effectively by ignoring overshoots in eog waveforms."
"according to the measures of sensitivity, specificity and auc (figure 4), class iv consistently outperformed the other three classes, while class i yielded the poorest performance."
"electrooculogram is a signal that measures and records the resting potential of the retina. the human eye works as a dipole in which the front cornea is an anode and the rear part is a cathode. therefore, when the eyeball moves and the cornea approaches one electrode, the potential of the corresponding electrode rises, and the potential of the opposite electrode falls. the most commonly used electrode attachment position for eog is near the outer angle of both eyes as shown in figure 1 and the position information of horizontal direction can be obtained by the pair of electrodes."
"the 14 rsns and their constituent regions are listed in table 2 and shown in figure 1 . network regions are shown in green and orange, the number of network nodes involved and the seed region chosen to define the network are listed. ba -broadmann area. note that nodes can span multiple bas and regions; not all voxels comprising a ba or region are necessarily included as part of a node. the fourteen rsns are shown in figure 1 ."
"the euclidean error method also can overcome the overshooting phenomenon commonly found in eog waveforms. the overshooting phenomenon occurs when the user attempts to move the eyeballs to the target position. if eyeballs move to the beyond the target position-in other words if eyeballs fail to move to the correct position at a time-then they return to the original target position in the case of euclidean error, there is little change of power of error (p err ) and efficiency according to the change of k, so p err and efficiency gently rise as th increases. in contrast to euclidian error, the rmse shows an irregular efficiency change according to k and euclidean distance also shows irregular p err changes according to k. in addition, in contrast to others, the efficiency is more sensitive to k than th in euclidean distance. therefore, we can increase the value of k without significant loss of approximation performance and we set k to 30 which is half of the buffer length k. theoretically, it can be considered that th is best optimized when the efficiency is 1, but in some practical cases, it is considered that there are cases where the extra vertices must be generate for better approximation. for this reason, 20% of the margin is considered despite lowered efficiency."
"at the other end of the link, the transmitted signal is affected by multipath propagation and contaminated by noise. as a consequence, the received signal is given"
"to enhance traction accuracy, we need to consider that human eyeball movement is a rotational motion not linear motion. to find the gazing position on the screen, the proposed system was designed based on an approximation that the eyeball movement is a linear motion. in this case, the error will increase as the difference between linear motion and rotational motion becomes larger as the position deviates from the position where the eyeball motion starts. in addition, for wider application to practical system, it is required to solve the dependency problems of screen size and user position. for successful implementation of a developed system on hci, it is good interface method for a disabled person with quadriplegia to access computer or mobile device including head mount display (hmd). also, various applications can be considered for the proposed method. for a direct application, as we show the experimental results, it can be used as a hands-free menu selection interface for systems including tv, smart phone, or computer. by combining eyeball position and emg signal information, various commands set for mobile robot also can be considered. also, this can be an alternative way to control the wheelchair for disabled person with quadriplegia. as an indirect application of this method, eyeball position data can be stored as life log and it can be applicable in various ways such as monitoring in healthcare, attention assessment tests and test of human attention. since the proposed system is designed considering the computational complexity for real-time processing, it can be easily implemented on open source electronics platforms including arduino and raspberry-compatible modules. also, proposed system can be merged into various open-source electronics platforms-based applications including game, iot, automation and robotics and it enhance the ranges of application for proposed system."
"the structure of the designed hardware for signal acquisition is shown in figure 2 . it amplifies the biological signals measured through the electrodes attached to both temples and transmits the signals to the pc. measured signals are differential signals obtained by differential amplification of signals received from both electrodes. in this case, the common mode signal is treated as noise. in figure 1 . potential changes according to the eyeball movement for eog measurement."
"since the signal after pla appears as a set of line segments, the width in the horizontal direction and the height, slope, or length in the vertical direction can be considered as feature parameters to represent the eog signal. these features can be obtained by simple computation from the shape of the line segment and they can express the information of the line segment intuitively [cit] . in this paper, we define the two-feature set f hv that represents width and height of line segment and f θl that represents slope and length of the line segment."
"the structure of the designed hardware for signal acquisition is shown in figure 2 . it amplifies the biological signals measured through the electrodes attached to both temples and transmits the signals to the pc. measured signals are differential signals obtained by differential amplification of signals received from both electrodes. in this case, the common mode signal is treated as noise. [cit], 7, 38 4 of 18 the common mode signal using signal feedback. however, in this case, the number of electrodes would need to be increased, which is not desirable for usability. in this paper, a pre-processing filter is applied as a method for measuring bio-signals without adding an additional ground electrode. the 1-pole active high-pass filter with a cut-off frequency of 0.008 hz is used as a pre-processing filter. here, the pre-processing filter is a kind of high-pass filter that removes low-frequency components before the signal is applied to the differential amplifier prior to passing the instrumentation amplifier. this pre-filter has two roles, the first is to ensure that the input signal is within the common mode input voltage range of the instrumentation amplifier of the measurement circuit. since the designed system and the human body do not have a common ground, for the measurement circuit, it is not possible to predict the level of common mode potential of the human body signal. therefore, in some cases, there is a risk of exceeding the common mode input voltage range of the instrumentation amplifier and it may lead to severe signal distortion and must be avoided. the second role of the pre-filter is to eliminate the dc offset. during the differential amplification of signals between the two electrodes, if a dc offset voltage exists between the two signals, they are not removed from differential amplifier. in the case of the bio-signal measuring circuit, since the amplification gain is usually several hundred times higher, the amplifier is easily saturated due to the dc component. at this time, the high-pass filter is equally applied to the signals transmitted from both electrodes, so that the dc voltages of the two signals are equalized and it can solve the two problems described above."
"to improve the generalization ability of the system, we used the support vector regression (svr) as a curve-fitting model. the curve-fitting model can represent a set of discrete data as a continuous function. thus, by using the curve-fitting model to the proposed system, the proposed system is capable of coping with any arbitrary eye movements not included in the training data set. svr is a kind of curve-fitting model used to solve the regression problem based on the existing support vector machine (svm) that has excellent modeling and prediction capabilities. in this paper, we used the chang and lin's model that can be found on \"libsvm' library [cit] . since the performance of svr depends on the type of kernel functions, we tested three types of kernel functions including polynomial kernel function, sigmoid kernel function and gaussian kernel function. for the performance test, we set 90% of data for training the svr and 10% of data for test and repeated 10 times without duplication of the test data using a 10-fold cross validation method. figure 10 shows the effects of the types of kernel functions on svr performance. in these plots, the horizontal axis shows the actual moving distance of the gazing position and the vertical axis shows the prediction results of svr. as shown in these plots, result plot of svr with gaussian kernel type showed the best results with 0.99317 of r value (multiple correlation coefficient value)."
"electromyogram is a measure of activity potential produced by the activity of the skeletal muscles and it usually has an amplitude ranging from 50 μv to 30 mv. the skeletal muscles under the control of one motor neuron contract when the particular motor neuron is excited, and the muscles are excited or contracted almost simultaneously through connected motor neurons. since the excitation of shock repeats at a frequency of 7 to 20 hz, emg is also measured in this frequency band. the temporal muscle or temporalis is the muscle located from the eyebrows to the cheekbone on both sides of the head. it is classified as a muscle of mastication and serves to move the lower jaw to chew food. therefore, when the emg is measured in the temporal muscle, a signal with large amplitude is generated in the case of bending on the muscle for bite, and a signal with small amplitude is generated when the bending force is released."
"performance evaluation was performed on 4 out of 7 subjects participating in signal acquisition for training the system. each subject performed 300 eye movements for testing. the spending time for this experiment was dependent on the subject. we did not give specific time guidelines for repetition of this experiment since each subject showed different levels of eye fatigue. so, we suggested that the subjects re-participate in this experiment after they had fully rested when they felt eye fatigue. in general, they continued the experiments for approximately 30 s to 1 min and then took rest. all subjects were male and aged between 20 and 30 years old. for the performance evaluation, two performance evaluation indices were applied. the first performance evaluation index was obtained by measuring the difference in units of cm on a 51 cm monitor between the position of the in the signal acquisition phase for training, the target circles on the screen were designed to reciprocate with a maximum of nine positions. in the performance evaluation stage, ten positions were selected in the same range to confirm the performance of how the svr can track eye movements when the eye moves to an arbitrary position that has not been trained."
"another advantage of the sliding window-based method is that it can change the signals shapes to fit to our applications during calculation of error between the original signals and approximated signals. figure 4 shows the pseudo code of the standard sliding window algorithm. as shown in this code, when we increase the count number of p2, we need calculate the error at every point between p1 and p2 which causes calculation burden. in addition, error, and threshold (th)-based creation of vertices at every point sometimes showed poor approximation performances. to enhance the calculation efficiency and approximation performance, we developed a modified sliding window algorithm as shown in figure 5 . first, as koski suggested [cit], we used k increment instead of an increment 1. by increasing the step point from 1 to k, we can reduce the calculation complexity but need to find the optimal k value for acceptable approximation performance. also, we added a vertex position correction algorithm after adding a new vertex point. this correction algorithm performed if k more data are added right after new vertex point p x [cnt] was generated. at this point, if we generate temporal vertex point p 2, then we can make two line segments p x [cnt − 1]p x [cnt] and p x [cnt]p 2, and we can find the new position of p x [cnt] that minimizes the sum of error. at this time, to consider computational time, we set the number of searching points to be less than p x [cnt] − k. in this work, to set the maximum response delay to be less than 0.2 s, we set that k to 55. to enhance the calculation efficiency and approximation performance, we developed a modified sliding window algorithm as shown in figure 5 . first, as koski suggested [cit], we used k increment instead of an increment 1. by increasing the step point from 1 to k, we can reduce the calculation complexity but need to find the optimal k value for acceptable approximation performance. also, we added a vertex position correction algorithm after adding a new vertex point. this correction algorithm performed if k more data are added right after new vertex point px[cnt] was generated. at this point, if we generate temporal vertex point p2, then we can make two line segments 1 and, and we can find the new position of that minimizes the sum of error. at this time, to consider computational time, we set the number of searching points to be less than . in this work, to set the maximum response delay to be less than 0.2 s, we set that k to 55. although we used the same approximation algorithm, the performance varies on the selection of function errorfnc. there are various error functions and we considered three kinds of error function in this paper, root mean squared error (rmse), euclidean error and euclidian distance as shown in figure 6 . figure 7 shows the approximation performance dependency on error function type. for this, we defined the efficiency as the ratio of optimized number of line segment (mopt) to pla-based number of the line segment (m). in this case, mopt is calculated based on the original number of eye movement trials. in other words, if efficiency is bigger than 1, then it infers that pla missed the vertex point that needs to be created. therefore, the smaller the efficiency, the more the part that can be represented by one line segment is often represented by two or more line segments. although we used the same approximation algorithm, the performance varies on the selection of function errorfnc. [cit], 7, 38 8 of 18 in this paper, root mean squared error (rmse), euclidean error and euclidian distance as shown in figure 6 . figure 7 shows the approximation performance dependency on error function type. for this, we defined the efficiency as the ratio of optimized number of line segment (m opt ) to pla-based number of the line segment (m). in this case, m opt is calculated based on the original number of eye movement trials. in other words, if efficiency is bigger than 1, then it infers that pla missed the vertex point that needs to be created. therefore, the smaller the efficiency, the more the part that can be represented by one line segment is often represented by two or more line segments. in figure 7, the graph of euclidean error shows gentle rises, while the graph of rmse and euclidean distance shows irregular vibrations. the reason behind this difference can be found in figure 8 . in figure 7, the graph of euclidean error shows gentle rises, while the graph of rmse and euclidean distance shows irregular vibrations. the reason behind this difference can be found in figure 8 . in figure 7, the graph of euclidean error shows gentle rises, while the graph of rmse and euclidean distance shows irregular vibrations. the reason behind this difference can be found in figure 8 . in the case of euclidean error, there is little change of power of error (perr) and efficiency according to the change of k, so perr and efficiency gently rise as th increases. in contrast to euclidian error, the rmse shows an irregular efficiency change according to k and euclidean distance also shows irregular perr changes according to k. in addition, in contrast to others, the efficiency is more sensitive to k than th in euclidean distance. therefore, we can increase the value of k without significant loss of approximation performance and we set k to 30 which is half of the buffer length k. theoretically, it can be considered that th is best optimized when the efficiency is 1, but in some practical cases, it is considered that there are cases where the extra vertices must be generate for better approximation. for this reason, 20% of the margin is considered despite lowered efficiency."
"the signal passed through the preprocessing filter is fed to the instrumentation amplifier for differential amplification. the instrumentation amplifier requires high common mode rejection ratio since the amplitude of the common mode signal to be removed is several ten times bigger than the eog signal to be obtained from this system. therefore, we applied 'ina128, which is a widely used ic chip for amplification of bio-signals and the designed circuit has 120 db of common mode rejection ratio. in addition, the gain of the instrumentation amplifier is set to 56 v/v to obtain a magnitude of ±50~±150 [cit], 7, 38 6 of 18 signal processing. the differential amplified bio-signal is filtered through the post-processing filter to remove residual noise and amplified once more. the noise elimination of the post-processing filter starts with the low-pass filtering process to only pass the band less than 40 hz which is enough to observe the eog and temporalis emg signals. in addition, since the low-pass filter limits the bandwidth of the signal, the sampling frequency in the analog-to-digital conversion (adc) can be set low and it can reduce the computation complexity during digital signal processing. the second stage of noise cancellation is the elimination of 60 hz noise using a band rejection filter. often, the 60 hz noise generated from various factors during the measurement environment cannot be removed solely by the low pass filter, so band elimination filter is added so that the system can be utilized even in a harsh measurement environment. in the final stage of the post-processing filter, signal amplification of 10 v/v level is performed so that the size of the bio-signal is increased to ±0.5~±1.50 v to be suitable for analog-to-digital conversion."
"the proposed system can be applied in various ways for hci or hmi. as one example, we developed eog and emg-based pointing interface system that can be used as a mouse. the developed system is similar to the performance evaluation program which is shown in section 3.1., but it is more user friendly for convenience of use. first, when you run the program, you will only see the tray icon on the windows operating system without any visible pointing menu on the screen. right clicking on this tray icon allows you to select your name among the list. if one has finished some set-up procedure in advance, the user can set personalized sf u value by selecting his or her own name. in the case of a first-time user, a simple set up procedure is required to obtain customized sf u value for each user. after the user selection is completed, the same tray icon menu can be used to connect the bio-signal acquisition hardware to the pc. after the connection is established, the user's bio-signal is transmitted to the pc and the developed interface system is executed in real time. figure 15 shows an example of a real demonstration of the developed system. the system initially waits until there is a bite action. when temporal muscle movements are detected, the finger-shaped pointer slowly appeared in the center of the screen and then six selection buttons appeared near the pointer as shown in figure 15a . after gazing at the central pointer, if the color of the pointer changes to blue, the user can move the pointer to the position of the selection buttons by moving the gaze as shown in figure 15b . one can click on the button when the eyeball moves over the desired button as shown in figure 15c and release the teeth biting force as shown in figure 15d . after all operations are completed by the button click, the pointer and the button that appeared on the screen disappear and the system enters the waiting state until user bites again. this series of operations can run a completed command to the pc or any type of machine including robot or computer through a simple operation. it is similar to the drag-and-drop operation that is similarly used with an existing mouse interface, so a user can adapt this method intuitively. compare to camera-based hcis that suffer from high false-positive rate in the idle state because of the unintended ocular movement, proposed system can make start and stop point of our commands and we do not have to worry about error from idle state by adding the emg signal to eog additionally, during the standby state, there is no obstacle to the visual field displayed on the screen and the computational burden is very limited. therefore, it is possible to execute a desired command through a simple operation for about five seconds without using a conventional mouse. see the tray icon on the windows operating system without any visible pointing menu on the screen. right clicking on this tray icon allows you to select your name among the list. if one has finished some set-up procedure in advance, the user can set personalized sfu value by selecting his or her own name. in the case of a first-time user, a simple set up procedure is required to obtain customized sfu value for each user. after the user selection is completed, the same tray icon menu can be used to connect the bio-signal acquisition hardware to the pc. after the connection is established, the user's bio-signal is transmitted to the pc and the developed interface system is executed in real time. figure 15 shows an example of a real demonstration of the developed system. the system initially waits until there is a bite action. when temporal muscle movements are detected, the finger-shaped pointer slowly appeared in the center of the screen and then six selection buttons appeared near the pointer as shown in figure 15a . after gazing at the central pointer, if the color of the pointer changes to blue, the user can move the pointer to the position of the selection buttons by moving the gaze as shown in figure 15b . one can click on the button when the eyeball moves over the desired button as shown in figure 15c and release the teeth biting force as shown in figure 15d . after all operations are completed by the button click, the pointer and the button that appeared on the screen disappear and the system enters the waiting state until user bites again. this series of operations can run a completed command to the pc or any type of machine including robot or computer through a simple operation. it is similar to the drag-and-drop operation that is similarly used with an existing mouse interface, so a user can adapt this method intuitively. compare to camera-based hcis that suffer from high falsepositive rate in the idle state because of the unintended ocular movement, proposed system can make start and stop point of our commands and we do not have to worry about error from idle state by adding the emg signal to eog additionally, during the standby state, there is no obstacle to the visual field displayed on the screen and the computational burden is very limited. therefore, it is possible to execute a desired command through a simple operation for about five seconds without using a conventional mouse."
"the proposed system can be applied in various ways for hci or hmi. as one example, we developed eog and emg-based pointing interface system that can be used as a mouse. the developed system is similar to the performance evaluation program which is shown in section 3.1., but it is more user friendly for convenience of use. first, when you run the program, you will only"
"the effect of varying window length for the cases with dynamic temporal fc (classes ii and iv) in analysis 2 is depicted in supporting information figure s3 . it is observed that for a wide range of window lengths from 10 to 70 trs (20-140 s), the performance of the classifier is robust, whereas the accuracy decreases for window lengths above and below this range. supporting information figure s4 delineates the effect of varying neighborhood levels on the classifier performance under analysis 2. incorporating a neighborhood of 2-6 mm is shown to increase (by 6%) the classification accuracy compared to not considering any neighborhood voxels. varying the extent of the neighborhood to 10 mm did not significantly affect classifier performance."
", where h[n] is the channel impulse response (cir), and v[n] is the sequence that contains the noise samples. the noise at the receiver input is circularly symmetric and gaussian distributed, i.e., v[n] ∼ cn (0, n 0 ). to recover the information conveyed on each subband, the received samples are fed into a bank of filters, which are matched to the transmit filters, and then the outputs are downsampled. the output of the qth filter is given by"
"fbmc/oqam has the key ingredients to deal with the restrictions that will be introduced by future wireless systems, such as the transmission in a fragmented spectrum. it is well known that the channel has to be counteracted to some extent in the fbmc/oqam context to guarantee a certain quality of service. this translates into precoding the symbols at the transmit side or equalizing the demodulated data at the receive side. both precoders and equalizers should perform multi-tap filtering when the channel is highly frequency-selective. the work presented here characterizes the average transmit power and the noise power when multi-tap precoders and equalizers are used. the closed-form expressions reveal that if the same filter is used as a precoder or equalizer, then the transmit power and the noise power increase or decrease with the same magnitude. therefore, we can conclude that there is no degradation due to combating the channel at reception rather than at transmission, as long as the transmitted symbols are properly scaled if transmit processing boosts the power. to determine whether the symbols should be scaled or not, we have formulated the transmit power as the function of the statistical knowledge of precoders when the criterion of design is based on the frequency sampling approach. the main reason to focus on the frequency sampling method is because it offers a good analytical tractability, which paves the way to get closed-form expressions, while it has a performance comparable to the optimum mmse. the numerical results show that the analytical expressions derived in this paper are reasonably accurate, and thus, they can be used to address the issue related to the power boost. the alternative to using the precoder statistics consists in computing the instantaneous power. however, this solution is very demanding in terms of complexity, since the power has to be recalculated whenever precoders are updated. this highlights that the characterization of the transmit power derived in this paper, which relies on precoder statistics, is useful since it provides reliable information with reduced complexity. both the numerical results and the closed-form expressions allow us to conclude that in the simulated scenarios, multi-tap precoding and equalization based on the frequency sampling approach do not boost the power. it is left for future work to investigate which precoding designs have a negative impact on the transmit power. although the design of new equalization techniques is out of the scope of this paper, the numerical results have revealed that there is still space to improve the trade-off between complexity and performance of the state-of-the-art solutions."
"temporal dynamics were mapped with sliding-window correlation, while spatial dynamics were characterized by enabling network regions to vary in size (shrink/grow) over time according to the functional connectivity profile of their constituent voxels. these temporal and spatial dynamics were evaluated as biomarkers to distinguish schizophrenia patients from controls, and compared to current biomarkers based on static measures of resting-state functional connectivity. support vector machine classifiers were trained using: (a) static, (b) dynamic in time, (c) dynamic in space, and (d) dynamic in time and space characterizations of functional connectivity within canonical resting-state brain networks. classifiers trained on functional connectivity dynamics mapped over both space and time predicted diagnostic status with accuracy exceeding 91%, whereas utilizing only spatial or temporal dynamics alone yielded lower classification accuracies. static measures of functional connectivity yielded the lowest accuracy (79.5%). compared to healthy comparison individuals, schizophrenia patients generally exhibited functional connectivity that was reduced in strength and more variable. robustness was established with replication in an independent dataset. the utility of biomarkers based on temporal and spatial functional connectivity dynamics suggests that resting-state dynamics are not trivially attributable to sampling variability and head motion."
"results from analysis 1 applied to the validation dataset (dataset 2) are shown in figure 9 . while classification accuracies for all classes are generally reduced in the validation dataset, it is evident that class iv once again provides the greatest classification accuracy."
"eog and emg have different signal characteristics in terms of signal amplitude and frequency ranges. therefore, the emg of the temporalis muscle can be obtained by frequency division of the"
"we conducted follow-up interviews with 11 survey respondents to gain a more in-depth understanding of their perceptions and experiences. we recruited participants with different roles (wellness program administrators vs. employees), experiences (participation vs. nonparticipation), and attitudes toward the programs (positive, ambivalent, or unenthusiastic). we also strove for diversity in program type and features. we conducted all follow-up interviews via phone or video conferencing. interviews lasted from 30 to 45 minutes. each interviewee was compensated with a usd$10 gift card. all interviews were audio recorded and transcribed for analysis."
"in this paper, the term 'reference system' is used to refer to a dynamic baseline scenario that excludes the studied 'bioenergy system.' 'land reference' and 'energy reference' are discussed separately. 'dynamic' is used to emphasize that during a specified time period, aspects of the reference which affect climate forcing, such as vegetation and carbon stocks, and energy source, are likely to change. the term 'relative climate effect' is used in this paper to describe the net climate forcing attributed to the bioenergy system when compared to the reference system. assessment of net climate effects requires consideration of the climate effects associated with all resources used, including land, energy and material inputs."
"life cycle assessment (lca) is a common method for quantifying the environmental impacts of a product system (product, process or service) or decision [cit] serving the function of interest (e.g., energy services). the results of lca studies are expressed on a functional-unit basis (e.g., impact per mj of delivered energy). the intended application and the reasons for carrying out an lca study determine the goal and scope of an lca, as well as the functional unit, system boundary and other methodological choices. lca standards [cit] provide guidance on these aspects, but do not explicitly consider the choice of reference systems. iso 13065 [cit] provides two options for the reference system against which to compare bioenergy: business as usual (bau) and \"projected future\" which modifies bau to reflect expected changes in baseline trends over time (e.g., population, technology, consumption, etc.)."
"when bioenergy is produced as a co-product or from biomass which is a by-product, total effects are shared between the bioenergy and other relevant products. for example, when using biomass residues such as forest residues or corn stover for bioenergy, their share of land use impacts can be allocated on the basis of energy content, mass, or economic value. the allocation basis most appropriate for the goal and scope of the study should be chosen, and it is recommended to assess the sensitivity of the results to the choice of allocation method (iso 14044). for some secondary residues such as manure, sewage sludge or bio-waste, while it is theoretically possible to estimate their share of total land use effects, it may be difficult in practice."
"the selected input parameters were the following. the neural network had 5 hidden layers with 15 neurons per layer. the learning rate and the momentum factor were set to be 0.1 and 0.01, respectively. the entire reference grid was given to the network to be used for training. only one termination condition was set: the supervised training should be terminated after the tenth iteration."
"prior to the experiment, the specimen is first partially surrounded by plasticine. two-hour epoxy (bostik, milwaukee, wi, usa) (work time of 2 h and maximum full cure after 16 h) is applied between the aluminium surrogate implant and bone, then immediately the whole specimen is fully covered by plasticine and the time output signal starts recording, refer to figure 5 . cure time, t, is the time measurement of the epoxy curing. the total duration of 1000 min cure time is recorded: the first 30 min at 1-min increment and afterwards at 5-min increments. each specimen mass is measured and tabulated in table 1 . it is evident that the test specimens are severely mass-loaded."
"in previous sections, we noted that participants felt tracking technologies did not support or fit well with their preferred physical activities. responses from non-participants show that this deterred them from starting using health tracking in the health and wellness program in the first place."
"procedure. participants walked the outdoor route twice and the indoor route three times for training purposes. in both studies directions were only provided during the first walk to help the vip familiarize with the route. they were instructed to avoid unnecessary head movements and hand gestures as well as talking to their o&m instructor unless there was an emergency. video and audio were registered by means of a smartphone camera to facilitate data annotation (observing behaviors across the different environments and situations) and synchronization (start/end of walk, environments, and obstacles). in the outdoor study, gps coordinates were additionally logged using a garmin gpsmap-64s unit at a rate of 1 registration per second. upon completing the last walk, participants were asked to describe stressful moments they experienced along the route."
"as found in our previous study [cit], providing the system equations real-time estimates of ρ in and ρ out should improve the estimation accuracy. in this study, a single-loop detector was installed at the entrance of the tested link to produce real-time estimates of ρ in . in contrast, in the next section, an nn model is developed to obtain real-time estimates for the ρ out values."
"in our previous fe study on stress wave-based monitoring on novel endoprosthesis design [cit], a 130 mm long hollow aluminium cylinder of 25 mm outer diameter and 3 mm thickness was it should be noted that the circumferential guided wave modes for a traction-free cylinder are dispersive and its dispersion curve is very similar to the lamb wave mode (guided waves in plates) when the radius to thickness ratio is large and cylindrical thickness is small [40, [cit] . the excited signal mainly consists of fundamental wave modes at this frequency-thickness excitation. in the preliminary study, the pz26 actuator was bonded on the oval surrogate bone and in order to verify the excitation modes, out-of-plane laser vibrometry (polytec inc., irvine, ca, usa) was used to perform longitudinal and circumferential line scans at time sampling of 0.1µs and scan length of 35 mm, starting 10 mm away from the actuator, refer to figure 6 . retroreflective films were attached to the line scans to enhance the signal to noise ratio. for the longitudinal scan, 128 spatial points were recorded and displaced automatically using the xy positioning system. for the circumferential scan, 35 points were recorded and displaced manually by rotating the surrogate bone. however, the curvature of the oval surrogate was not entirely perpendicular to the laser head. as a result, the recorded signals are not entirely in the out-of-plane direction (normal to the curvature), and consequentially, the fundamental symmetric mode is more apparent in the circumferential direction refer to figure 7 . nevertheless, this technique serves to identify the propagating guided wave modes."
"the most common outcome variable measured by wellness program administrators we surveyed and interviewed is engagement (i.e., participation rate) in the health tracking program. as health tracking is relatively new, most wellness program administrators are preoccupied with increasing employee interest in tracking their health and in motivating them to shift from a sedentary lifestyle to a more active level."
"16% of responses from company x (0 from other companies) mentioned either technical problems or uncertainties that hindered participation. for example, the app or device offered by their companies worked only with certain types of smartphones. others explained that they were uncertain of the sign-up process. for some programs, participants had to purchase a tracker without reimbursement. this deterred them from participation: \"you had to purchase the product and it was not something i thought i would use after the program.\" (e67, company x, survey). as such, financial and technical problems did play a role when employees decided not to use health tracking. however, other themes also emerged and will be discussed more in-depth in the following sections."
"for others, concerns revolved more around data disclosure at large: \"i don't want a 3rd party to have any more data about me than necessary so i choose against any wearables. i don't like the idea of being monitored\" (e444, company x, survey). current systems do not give the user control of their own data, and people are usually confused about how their data are used by these systems [cit] . finally, while we do not have data to support this, it is not unreasonable to think that employees might have fewer privacy concerns because they trust their companies to choose tracking devices that protect their data."
"two 3 mm thick aluminium 6060 t5 mill-finished cylinders with different diameters are used for this investigation. a small cylinder of outer diameter 25 mm approximately 130 mm in length is used as the shape-like bone structure. since the cross-sectional bone structure is different for each amputated patient, refer to figure 1, oval and triangular-like aluminium surrogates are produced. a large cylinder of outer diameter 32 mm is used to make two 4-extramedullary strut implant based on the novel osseointegration implant design [cit], refer to figure 2 . both cylinders are pressed to shape to produce the aluminium surrogate bone and snug-fitted surrogate implant with the oval and triangular cross-sections (refer to figure 4 ). in this study, the aluminium surrogate implant has four extramedullary struts, which extend approximately 50 mm in length and the case depth of approximately 15 mm. the distance between the extramedullary struts is approximately 22 mm (at the tip). a rubber back cover is adhesively bonded to constraint translational movement along the cylinder. the curing of the adhesive epoxy, which has been used to model the healing of a fractured bone, is used to simulate the osseointegration process in this study [23,76,79−81] . in a physiological perspective, the osseointegration process begins from a blood clot to callus mineralisation and ossification which is primarily similar to the curing process of viscous elastic epoxy from liquid to solid state [cit] . the use of epoxy for curing is not an exact representation for osseointegration. furthermore, the solidified epoxy properties (young's modulus and poisson's ratio) are not equivalent to the parent material properties. however, the curing process is similar to an extent to fundamentally demonstrate the concept of changes in material properties."
"consistent with prior research on why people adopt fitness trackers [cit] . in this way, the incentive becomes an \"added motivator\". however, the financial incentive was the only reason e376 tracked, and so here the incentive is more coercive, matching concerns by researchers and media [cit] . financial incentives can be that final nudge to employees because they appreciate their efforts are rewarded. however, the financial incentives could also force participants more than just a gentle nudge, in a manner we suspect is unlikely to be sustainable. some participants could socialize with other people with whom they did or did not have direct working relationships and develop \"workout buddy\" relationships with them: \"sharing daily and weekly goals and providing a buddy system which creates a support system and also accountability system which worked really well for me\" (e169, company x, survey). the social nature of the workplace health and wellness programs provided externally driven accountability to help employees make progress toward their goals."
"we designed two ad hoc orientation and mobility tasks gathering a wide range of behavioral and biophysical signals from 10 vip with various categories of sight loss (see table 1 ), who volunteered to participate in our study. collecting electroencephalogram (eeg) signals, we assess the cognitive load and task engagement in the performed task. eeg signals are shown to be stable indicators of the cognitive load in a 2 wireless communications and mobile computing vision is less than 10% and more than 5% 1 (f) 2 (f, m) vi-3"
"company x is a us-based technology company with 5,000-10,000 employees located in more than ten north american cities. its employees have mostly desk jobs but with flexible working hours. the health and wellness program is a pointbased system, where employees can earn virtual points from various health and wellness activities. this includes, for example, filling out wellness surveys or participating in educational seminars. employees can then use these virtual points to purchase company swag or fitness products from an internal online wellness store. the company provides a wide range of options to encourage healthy behavior and relevant tracking. these include:"
"the state equation produces accurate results if the scaled traffic flows (q in /ρ in and q out /ρ out ) are accurate [cit], as shown in section 4.3. the total counts can be extracted from traditional loop detectors or video detection systems. we should note here that the ρ value in equation (2) plays a major role in delivering accurate outcomes. ρ is defined as the ratio of the number of probe vehicles (n probe ) to the total number of vehicles (n total ), as shown in equation (3). for instance, if ρ is equal 0.1, and the number of probe vehicles is 5, then the expected total number of vehicles is 50."
"prior to the experiment, the specimen is first partially surrounded by plasticine. two-hour epoxy (bostik, milwaukee, wi, usa) (work time of 2 h and maximum full cure after 16 h) is applied between the aluminium surrogate implant and bone, then immediately the whole specimen is fully covered the curing of the adhesive epoxy, which has been used to model the healing of a fractured bone, is used to simulate the osseointegration process in this study [23, 76, [cit] . in a physiological perspective, the osseointegration process begins from a blood clot to callus mineralisation and ossification which is primarily similar to the curing process of viscous elastic epoxy from liquid to solid state [cit] . the use of epoxy for curing is not an exact representation for osseointegration. furthermore, the solidified epoxy properties (young's modulus and poisson's ratio) are not equivalent to the parent material properties. however, the curing process is similar to an extent to fundamentally demonstrate the concept of changes in material properties."
"the ρ out values vary between 0 and 1, the 0 value means that no probe vehicles were observed at the exit of the link, while the value of 1 means that the d p value is the same as the d t . the selected inputs must be relevant to the model output ρ out to allow the nn model to build a strong relationship between the model inputs and outputs, and therefore produce high estimation accuracy. for instance, in our case, the ρ out value decreases as a t and a p increase. for instance, a high value of a t means that the link is more congested and thus the number of departures (d t ) is expected to be high. the ρ out value also decreases with increasing speed (s 1, s 2, and u s ). the speed is an indicator of the congestion level of the link; for instance, if the speed is low, then more vehicles are expected to be on the link, leading to higher values of d t . a single hidden layer with one neuron, with a transfer function of hyperbolic tansgent sigmoid, was used to build the nn model as shown in figure 2 . the levenberg-marquardt (lm) optimization has been proven in the literature to outperform the gradient decent and conjugate gradient methods for medium-sized problems [cit] . furthermore, the lm is considered the fastest back-propagation algorithm and thus was implemented in the proposed approach. the weights and biases of the developed nn model are described below. w 1 depicts the weights between the input layer and the hidden layer, while w 2 represents the weight between the hidden layer and the output layer. b1 and b2 represent the biases at the hidden and output layers, respectively. figure 2 describes the proposed akfnn approach, combining the akf model with the nn model."
"nissenbaum and patterson have applied the lens of contextual integrity to the area of health tracking in the workplace [cit] . they suggest that health tracking technologies challenge informational norms due to the close monitoring and gathering of personal information, and the ability of these data to be used by third parties without the knowledge of the user. nissenbaum and patterson argue how this area is \"sorely lacking fundamental factual details\" [cit] . responding to this call for research, we focus on whether and how these potential challenges are experienced in practice."
"a probe vehicle is defined as a vehicle that provides real-time information, such as its instantaneous position and speed. several benefits of using probe vehicle data have been recognized; for example, the high quality of data compared with existing data sources (e.g., cameras and loop detectors), and data can be collected at any location inside the network, thus offering a clear picture about traffic behavior at any time. therefore, transportation agencies are putting effort into facilitating the use of probe vehicle data."
"after the experimental investigation, the plasticine was removed to allow visual inspection of the adhesive layer between the implant and bone cylinder. whilst the extramedullary struts of the oval section specimen was fully and successfully bonded to the aluminium surrogate bone (i.e., fully osseointegrated), the near strut of the triangular specimen did not integrate adequately (i.e., absence of osseointegration), leaving an approximately 10 mm depth gap, due to lack of epoxy adhesive between the specimen interfaces, refer to figure 18 . this inadequate bonding can be interpreted as a lack of or absence of osseointegration. furthermore, this serves as an explanation to the triangular specimen near-case result: absence of inflection point and noisier compared to the unified cases. in comparison, it is worth noting that successfully bonded cases showed significant changes in the low frequencies between 100~300 khz. in this regard, it is evident that the proposed assessment methodology described above can be used to assess the degree and the lack of osseointegration. one would expect the first arriving wave mode is the most predominant feature as shown in the triangular specimen far-case, however, the oval specimens have shown a significant increase of a later our future work will include detailed fe investigation including modelling and representative substitution for soft tissue, composite bone model and implant material to further validate this acousto-ultrasonic method for potential continuous monitoring and assessment of osseointegration. a quantitative measurand will potentially assist in identifying and predicting common implant failures and complications, such as construct failure, aseptic loosening, and skin-implant infection, consequently prompting early rehabilitation and body functionality."
"where d is the link length and tt is the average vehicle travel time. since probe vehicles can share their instantaneous locations every ∆t, the travel time of each probe vehicle can be computed for any road section. thus, the probe vehicle travel time is used in the measurement equation, using equations (4) and (5) . the measurement equation can be written as shown in equation (8):"
". since the core of the specimen is filled with silicone rubber, the wave propagation characteristics across this material were also determined. blocks of silicone rubber with different dimensions were constructed, and the average longitudinal bulk wave velocity of 1098.9 m/s was determined using pulse-echo method and v539 transducers (panametrics, waltham, ma, usa)."
"the program provides an ai method for cem in an esri arcgis 1 10.0 environment. the program was implemented in python and is freely accessible through the esri tool center [cit] . based on the distribution (which serves as the base of the presence/absence calculations) and the grids of the climatic, edaphic, topographic, and other data of the reference and future periods (which provide the predictors, or explanatory variables, for the learning and projection phase), the program learns the climatic patterns found within the distribution of the studied species and then predicts the future potential distribution (makes projection). the program implements a multilayer, feed-forward ann to learn the climate envelope of species. sigmoid, tangent hyperbolic activation function is used. the multilayer topology includes (1) one input layer with the same number of neurons as the number of the given input predictor variables; (2) several hidden layers (the number of the hidden layers and the neurons of the hidden layers can be set); (3) and one output layer with one neuron that is able to estimate the presence/absence in a certain geological point (a point of the grid). the supervised learning is achieved by a backpropagation algorithm with adjustable learning rate and momentum factor. multiple predictions can be made in one procedure. the trained network can be saved to and loaded from a file, therefore, training and prediction can be separated."
"the projecting and processing phases are done if the program was started with the parameter \"should projection be done?\" being checked. during the projection phase the program iterates through the points of the projection grid(s) and the trained neural network makes a projection. the projection values, typically within the (0;1) interval, are discretized to binary presence/absence data by a manually specified threshold. they can be preserved in a new column of the projection grid. the processing phase is responsible for drawing the potential distribution(s) based on the projection(s) of presence/absence. it is achieved by creating and aggregating thiessen polygons (voronoi cells). detailed structure of the program can be seen in figure 2 ."
"after the experimental investigation, the plasticine was removed to allow visual inspection of the adhesive layer between the implant and bone cylinder. whilst the extramedullary struts of the oval section specimen was fully and successfully bonded to the aluminium surrogate bone (i.e., fully osseointegrated), the near strut of the triangular specimen did not integrate adequately (i.e., absence of osseointegration), leaving an approximately 10 mm depth gap, due to lack of epoxy adhesive between the specimen interfaces, refer to figure 18 . this inadequate bonding can be interpreted as a lack of or absence of osseointegration. furthermore, this serves as an explanation to the triangular specimen near-case result: absence of inflection point and noisier compared to the unified cases. in comparison, it is worth noting that successfully bonded cases showed significant changes in the low frequencies between 100~300 khz. in this regard, it is evident that the proposed assessment methodology described above can be used to assess the degree and the lack of osseointegration."
"in this paper, a fusion of probe and single-loop detector data is utilized to produce the model features. the single-loop detector was installed at the entrance of the link and thus ρ in can be computed directly using equation (3). the ρ out variable is calculated from the nn (the nn output). seven possible inputs (features) were considered in the nn model, as defined in table 1 . conducting a feature selection technique to validate the importance of each feature for the nn model, the number of the model features was dropped to five features. it should be noted that the selected model inputs can be easily extracted when probe vehicles are on the link. ρ out can be expressed as a function of the selected inputs, as presented in equation (23)."
"the objective of this paper is to enhance understanding of the significance of the choice of reference system, and to provide guidance on the appropriate choice of reference system for quantifying climate effects of bioenergy in various contexts. this paper therefore provides a framework for choosing a suitable reference system for different research questions and discusses the benefits and challenges of each approach. the main focus of this paper is the choice of the land reference. while this paper concentrates on climate effects, the approach is relevant to the evaluation of broader impacts of bioenergy systems (e.g. biodiversity) and to analysis of other land-based production systems. this paper is the first of a series of papers on quantifying the climate effects of bioenergy developed by iea bioenergy task 38 research network."
"overall, employees who participated in our survey had many positive things to say about health tracking in workplace health and wellness programs. yet they also expressed a diversity of concerns. the sources of these concerns ranged from the design limitations of current activity tracking technologies to significant individual differences in attitudes towards personal health, the need for separation between work and personal life and financial limitations. in the space below we first discuss the issues that emerged most prominently in our data. this is what ackerman has called \"the socio-technical gap\" [cit] between the diversity of what people conceptualize as health and wellness, the more holistic goals of the administrators of health and wellness programs and the limits of technologies to support health tracking. we then consider why, despite much criticism about privacy issues, so few of our participants expressed this concern."
"the social-technical gap overall, the program administrators of company x and the other companies expressed in the survey that the most important goal of the health and wellness programs was to help employees maintain a healthy lifestyle and learn about healthy lifestyle choices, and to support a better working environment. for example, in company x, the health and wellness program was designed based on a particular serious + educational + [cit], may 6-11, 2017, denver, co, usa conception of wellbeing, which focused on nutrition, hydration, breathing, movement, thoughts, and rest."
"the study tests the developed akfnn approach by using a fusion of probe and single-loop detector data. a comparison between the traditional kf, akf, and akfnn models is presented."
"to avoid potential misinterpretations of short-term cycles, it is useful in prospective studies to consider historical trends over a time interval similar to that which the study intends to project into the future. this provides an idea of the magnitude of changes that can occur and potential cyclical behaviour in systems that could otherwise bias bau projections [cit] . a us example illustrates challenges of prospective studies: based on an analysis of periodic projections compared to actual forest dynamics in the us, [cit] concluded that a fixed baseline, assuming no changes in annual growth and removals from a given point in time, would have been closer to observed data than the bau projections which simulated anticipated rates of growth and removals."
"the following graphs in figures 14-17 for the specimens with oval and triangular cross-sections, respectively, were produced by the o-index formula. it is shown that the o-index steadily increases over cure time and later plateaus; gradient approaching zero, as it fully bonds with the surrogate implant. the curing of the adhesive resulted in an initial high gradient as seen in figures 14, 15 and 17 . in figures 14 and 15, the maxima gradients are located approximately at 400 min, whereas, in figure 17, the triangular specimen far-case the maxima gradient is located approximately 187 min."
"limited studies have used only information from probe vehicle data (e.g., global positioning systems [gpss]) to estimate the state of on-road traditional vehicles [cit], such as traffic travel time, traffic density, traffic speed, and traffic volume. the real-time estimation of traffic density is important to achieving better traffic operations management in urban areas. this paper aims to estimate the total number of vehicles on signalized link approaches using only probe vehicle data. the estimate outcomes can be provided to traffic signal controllers to optimally determine the allocation of green time for each traffic signal phase [cit], leading to better intersection performance measures such as intersection delays and vehicle crashes [cit] . one concern with using probe vehicles is measuring their level of market penetration (lmp). the lmp is defined as the ratio of the total number of probe vehicles to the total number of vehicles. providing accurate lmp estimates improves the estimation accuracy of the vehicle counts [cit] . therefore, in this paper, a machine-learning technique is developed to provide reliable lmp estimates."
"whereq is the average traffic flow entering and exiting the link, and h(t) is a transition vector that converts the vehicle counts to travel times, and is the inverse of the average flow (i.e., the first term of equation (7)), as shown in equation (9)."
"in analysis of our findings we have noted which company participants were from for clarity, and we recognize that most quotes we provide are from company x due to the majority of responses. however, this dataset of both company x and all other responses provides a broad variety of experiences on which we base our findings. in this study, we also interviewed and surveyed people who played a role in implementing and/or maintaining workplace health and wellness programs, and refer to these as \"wellness program administrators.\" some of these administrators had development, implementation, and maintenance of workplace wellness programs as their main job responsibilities. these employees were usually in human resources departments. others had management and development of the programs as an added role in their job."
"limitations of approach 2a include the uncertainty of the natural regeneration reference and that it is not necessarily realistic in practice. carbon dynamics and climate forcing associated with current vegetation, potential natural vegetation, and the regeneration pathway between current and natural states, are all uncertain. trajectory of carbon stocks on a land area in the absence of human management depends on assumptions applied to define the prevailing global and regional conditions, as well as future conditions that involve disturbance from fire and pests, growth and decomposition rates, changing climate, and interactions with other species. it is argued that potential natural vegetation \"is impossible to model\" because of uncertainties surrounding ecosystem dynamics [cit] . the exclusion of human activities limits the ability to compare a bioenergy system with a realistic, alternative scenario. market-mediated effects are omitted although they could reduce or enhance climate impacts of bioenergy compared to realistic alternative scenarios [cit] ."
"the spatial scope of bioenergy studies can vary from global to field scale depending on the goal of the study. for example, bioenergy can be studied at a forest stand level (e.g. when defining a carbon footprint for a specific production chain) or at a national level (e.g. when studying the impacts of a national bioenergy policy aimed at intensifying forest harvest or expanding the cultivation of energy crops). the goal of the study defines if the spatial scope excludes (approach 2a) or includes (approach 2b) the indirect impacts on land use. the same spatial scope must be maintained over the temporal scale of the study for both bioenergy and reference system. similar to temporal scope, the spatial extent for assessing climate effects should be defined, and it is generally not the same as the spatial scope for the bioenergy and reference land use. climate effects are usually studied at global scale. for example, one can study the global climate effects of land management in europe. one could also be interested in local climate effects such as the impacts on local precipitation patterns associated with forest clearing in the amazon or local temperature impacts due to changes in albedo [cit] ."
"the most common components of workplace health and wellness programs include screening activities, preventive interventions of health risks, and health promotion activities for healthy lifestyles [cit] . with increasing healthcare costs, organizations explore options that might motivate their employees to pursue healthier lifestyles, such as being more physically active. wellness programs are often coupled with incentives for participation or competition, and encourage employees to use fitness centers [cit], maintain physical activities during winter [cit], and increase physical activity levels toward a pre-set goal [cit] . with the advent of digital health tracking, integration of these technologies into wellness programs is an obvious step for many companies."
"3.1. eeg. the eeg data was first time-domain interpolated using the fast fourier transform (fft) to account for missing samples due to connectivity issues. subsequently, all signals were baseline-normalized by subtracting for each participant and for each channel the mean of resting state registrations. these were obtained during a series of laboratory studies with the same participants [cit] ."
"under approach 2b, indirect effects of the bioenergy system should be captured by the analysis, as the goal is to understand all net climate effects occurring due to the differences among forcers when the bioenergy system is compared to the alternative anthropogenic reference system. economic models can reflect indirect market-mediated effects such as substitution, market restructuring, rebound effects, and impacts on land use elsewhere (iluc) [e.g., 74, 75] . indirect impacts may also be estimated based on empirical data from similar situations or effects documented in \"natural experiments\" [cit] . economic simulations allow for system expansion to handle market-mediated effects of by-products, in contrast to the allocation approach in 2a above. in principle, allocation is avoided through system expansion and inclusion of market-mediated effects such as product substitution, but in practice some allocation may be needed due to setting of system boundaries. for example, to calculate an avoided emission from substituting diesel oil by biodiesel the ghg emission intensity for diesel oil is often defined by allocating emissions between different crude-oil-based products processed in oil refineries."
"these concerns are echoed and elaborated in research taking a more theoretical approach to the development. the increasing quantification of employees is worrisome because all other things that make up well-being, but that are not easily measured, can be undermined [cit] . researchers highlight how incentivized health tracking, when connected to the workplace, risks disciplining employees with implications for those who cannot afford to say no, that there is a risk of reducing health to numbers, and voice concerns that data can be de-anonymized and used for other ends than expected by the employee [cit] ."
"after the experimental investigation, the plasticine was removed to allow visual inspection of the adhesive layer between the implant and bone cylinder. whilst the extramedullary struts of the oval section specimen was fully and successfully bonded to the aluminium surrogate bone (i.e., fully osseointegrated), the near strut of the triangular specimen did not integrate adequately (i.e., absence of osseointegration), leaving an approximately 10 mm depth gap, due to lack of epoxy adhesive between the specimen interfaces, refer to figure 18 . this inadequate bonding can be interpreted as a lack of or absence of osseointegration. furthermore, this serves as an explanation to the triangular specimen near-case result: absence of inflection point and noisier compared to the unified cases. in comparison, it is worth noting that successfully bonded cases showed significant changes in the low frequencies between 100~300 khz. in this regard, it is evident that the proposed assessment methodology described above can be used to assess the degree and the lack of osseointegration. one would expect the first arriving wave mode is the most predominant feature as shown in the triangular specimen far-case, however, the oval specimens have shown a significant increase of a later the o-index magnitude of the oval specimen cases is relatively similar however the triangular specimen far-case is an order of magnitude greater than the oval specimen. the increase and decrease in energy for the oval specimen at approximately 400~450 khz; label b (refer to figure 9 ), has influenced the o-index in the early stages as seen from the time derivative. overall, the o-indices are primarily driven by the change in frequency between 100 khz to 300 khz, refer to figures 14b, 15b and 17b . the results for both cases for the oval specimen and far-case for the triangular specimen have shown a clear increasing asymptotic trend in the later stages. however, triangular specimen near-case did not show similar o-index and has different magnitude, trend, and oscillating gradient (refer to figure 16 )."
"the reference system should be chosen so that the comparison between the bioenergy system and the reference system responds to the question studied. otherwise, the results and conclusions of the study may be misleading. the question itself depends on the goal of the study. bioenergy may be derived from land dedicated to bioenergy feedstock cultivation, or from biomass that is a by-product of other land use(s) such as forestry and agriculture, or from processing or postconsumer residues and waste streams. however, the selection of the reference system depends on the goal of the study, rather than on the type of feedstock."
"when following approach 2b, the question becomes, \"what are the effects of a change in the bioenergy system over a given time horizon?\" the energy reference is the most likely energy system in the absence of the bioenergy system studied. one common method is to assume that bioenergy replaces another energy source serving the same functional unit in terms of energy. however, substitution of fossil fuels by bioenergy may lead to indirect market-mediated effects which can include both positive and negative feedback effects [cit] . interactions among policies, social preferences, relative prices and other market and non-market forces influence energy choices in both the bioenergy system and the reference system [cit] . hertwich [cit] notes that emission reductions are not brought by the technologies per se, but by interactions of policies and society that drive behavioural change (i.e. the policies that result in fossil fuel displacement by bioenergy). these issues illustrate the complexity and uncertainties related to the energy reference. thus, it is recommended that the uncertainties are reflected appropriately by considering several possible scenarios, as for the land reference."
"since the core of the specimen is filled with silicone rubber, the wave propagation characteristics across this material were also determined. blocks of silicone rubber with different dimensions were constructed, and the average longitudinal bulk wave velocity of 1098.9 m/s was determined using pulse-echo method and v539 transducers (panametrics, waltham, ma, usa)."
"the integration of health tracking technologies in workplace health and wellness programs has not gone unnoticed by media, who has largely reported critically on the subject. concerns of a blurring of work and private life spheres have been put forward, and questions of whether employees actually have a choice to participate or a coerced into participation have been raised [cit] ."
"in order to evaluate the degree of osseointegration, the acousto-ultrasonic methods and frequency analysis techniques, which have been used for shm to describe the material mechanical properties [35, [cit], are considered. the acousto-ultrasonic evaluation has been studied in bone healing assessments, such as fractured bone healing, structural form, and osteoporosis, by utilising the wave propagation velocity and time-of-flight method [23, [cit] . over the past decade, there is particular interest in stress wave propagation for quantitative analysis, and furthermore, guided waves propagation in bone has also been considered both computationally and in vivo experiments [cit] ."
"the actuator excites a 50v 1 mhz triangular pulse signal which is generated by the ni pci5412 function generator (national instruments, austin, tx, usa). the input signal is filtered in a model 3944 multichannel filter (krohn-hite, brockton, ma, usa) and amplified by a krohn-hite model 7602 wideband amplifier. the raw output signals are recorded using picoscope 6402d and its software (pico technology,cambridgeshire, united kingdom) with a sample interval setting of 2 ns for a total of 50,000 samples with 128 averages."
"figures 8a,b show the spectra development as a function of cure time (i.e., simulated osseointegration). it is evident that response measured in the frequency range from 50 khz to 500 khz increases throughout cure time. the sensitivity of the early curing is noticeable after approximately 300 min, where the response in the frequency band approximately 130~250 khz increased significantly. this is consistent with the 2-h work time of the adhesive used. the spectrograms of the time-series at five different stages (0, 120, 240, 480 and 1000 min) of cure time are shown in figures 9 and 10 for the 'near' and 'far' sensors, respectively (see also figure 4 ). in the oval specimen near-case, two waves at low and high frequencies propagate around the specimen circumference from both directions. the circumferential flexural waves propagating from the shortest distance arrives at 9 µs for 99.2 khz (wave a) and 6.4 µs for 465.4 khz (wave b), refer to figure 9 and table 2 . waves c and d arrive at 21.1 µs for 167.8 khz and 22.1 µs for 434.9 khz, respectively. it should be noted that the longitudinal flexural waves reflected at the ends of the specimens arrive slightly after the circumferential flexural wave propagating from the longest distance, which serves the explanation of the larger spread of waves c and d in time domain (refer to figure 9 ). the frequency of the ultrasonic guided wave wave a increases, however, the others decrease as cure time increases. the spectrograms of the time-series at five different stages (0, 120, 240, 480 and 1000 min) of cure time are shown in figures 9 and 10 for the 'near' and 'far' sensors, respectively (see also figure 4 ). in the oval specimen near-case, two waves at low and high frequencies propagate around the specimen circumference from both directions. the circumferential flexural waves propagating from the shortest distance arrives at 9 µs for 99.2 khz (wave a) and 6.4 µs for 465.4 khz (wave b), refer to in the oval specimen far-case (refer to figure 10 and table 3 ), the flexural waves propagating in both directions arrive at the same time since the sensor is placed an almost equal distance from the actuator. the first arriving circumferential flexural waves, waves a and b, are observed to be developing from 0 to 240 min cure time. afterwards, waves c and d, identified as the returning longitudinal flexural waves reflected at the ends of the specimens, are more apparent. similar to the near-case, the frequency of the ultrasonic guided wave wave a increases while the others decrease in frequency as cure time increases, see figure 10 and table 3 . wave c in the far-case also significantly increases in psd magnitude of 27.6 db/hz as cure time increases. wave b increases in psd magnitude of 11.5 db/hz at cure time 240 min then decreases to 9.8 db/hz at cure time 1000 min. figure 11a,b show the spectra development as a function of cure time. as in the results presented in the previous section, it is evident that the frequency range from 50 khz to 500 khz increases throughout cure time. the sensitivity of the early curing is noticeable after 300 min, where the frequency of approximately 130~250 khz increased significantly. however, the triangular specimen near-case response, shown in figure 11a, is noticeably different from the others, and there is no evidence of the appearance of any significant frequency peak or prominent change that is similar to the others."
the study tests the proposed akf model using only probe vehicle data. the approach was evaluated considering different probe vehicle lmps ranging from 10% to 90% at increments of 10%.
"the advantage of approach 2a is that it provides a reference without human intervention, thus requires no assumptions about human behaviour. in approach 2a the market-mediated impacts (responses of any other anthropogenic function to the production of the studied bioenergy) are not relevant, thus omitted. this is because all the functions are considered to take place as they occur, and in the reference system there are no anthropogenic functions. thus approach 2a does not require modelling of the market-mediated effects, which can be very challenging and is subject to significant uncertainties and sensitivities [cit] ."
"when the goal of the study is to determine the absolute climate effects from the studied bioenergy system (aligned with approach 1a in fig. 1 ) an energy reference system is not relevant. when following approach 2a (see fig. 1 ), the energy reference is \"no human intervention\" (no energy supplied). thus, energy reference is not applied in approaches 1a and 2a. in life cycle assessment, the climate effects associated with the use of different energy products, including bioenergy products, are studied per functional unit (e.g., 1 mj heat delivered). this facilitates comparisons of absolute climate effects (1a) or relative climate effects (2a) of different energy products providing a functionally equivalent energy service. typically, bioenergy is compared to fossil fuels, which currently dominate the global energy supply, representing of 81% [cit] ."
"nn is a machine learning technique that aims to recognize relationships between vast amounts of data by employing a certain number of neurons in every single hidden layer to achieve better accuracy [cit] . the network consists of three main layers: the input layer, the hidden layer, and the output layer. this section takes into account the recommendation of using two market penetration rates (at the entrance and exit of the link) rather than one market penetration rate along the tested link in the kf equations [cit] . accordingly, the state equation and the h vector in the measurement equation are revised as presented in equations (21) and (22) . ρ in and ρ out are the probe lmp at the entrance and the exit of the link, respectively."
"similar to the land reference, the energy reference can be defined for current, historical or future situations, depending on the goal and scope of the study. the energy reference may reflect effects associated with industrial investments, political developments or more sustainable technologies which can be far-reaching in breadth of markets impacted, as well as long-term [cit] . one of the main aims to use bioenergy, like other renewable energy sources, is to reduce consumption of fossil fuels and greenhouse gas emissions. as renewable energy becomes increasingly prevalent, the ghg intensity of the overall energy supply will decrease, and this could be significant for prospective analyses over long periods. the temporal scope may also impact on the assumptions on auxiliary energy inputs used in the bioenergy production chains."
"assessing the relative climate effects of a bioenergy system involves uncertainties in both the bioenergy and the reference system, especially in prospective studies since such systems cannot be verified [cit] . scenario analysis based on empirical data can inform assumptions about future projections. the spatial scale of the study influences the data requirements [cit] and for regional or global analysis, average or aggregate data from fao are often considered [cit] . energy scenarios can be informed by the official projections of iea and ipcc."
"personal health tracking devices such as fitbit, jawbone up, or the apple watch, are rapidly becoming common fixtures in workplace health and wellness programs [cit] . one industry report estimates that more than 27.5 [cit], compared to just 166,000 [cit] . in some cases, companies are incentivizing the use of these devices by offering financial benefits to employees who are physically active and share their personal health data, such as steps, heart rate, and sleep patterns with workplace or insurance health and wellness programs. employees may be rewarded for their activities with virtual points that can be exchanged for company swag or gifts, or companies may offer a discount on health insurance premiums or deductibles. the popularity of such programs that capitalize on the easily available health tracking data often hinges on an argument that a healthy workforce is a more productive workforce with the resultant declines in healthcare expenditures."
"this section demonstrates the impact of using two ρ values rather than using one predefined ρ value. the average predefined ρ value is defined as the value for the entire tested link. the average ρ value remains constant for the entire simulation for each lmp scenario. for instance, if the scenario of 10% lmp is tested, the ρ value in both the state and measurement is treated as a value of 0.1. in this study, the authors proposed the use of two ρ values; one at the entrance and one at the exit of the link to reflect the total number of arrivals and departures from the given total number of probe arrivals and departures, respectively. ρ in is measured directly using the installed loop detector at the entrance of the link. the developed nn model is used to predict the ρ out values (section 5.2). then, the ρ in and ρ out values are utilized in the akf equations. recall that the akf model relies only on probe vehicle data, while the akfnn model uses a fusion of probe vehicle and single-loop detector data."
"another critical initial parameter in the akf model is m i . this parameter represents the mean value of the noise in the state equation. this paper tests 16 different m i values (i.e., 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, and 15) . figure 6b presents the vehicle count estimation rmse values for different m i values. the rmse value is equal to 4.7 veh when the simulation starts with a 0 value of m i . in contrast, the rmse value is 3.9 veh when the value of m i is equal to 11."
"an advantage of approach 1a is its simplicity: reference scenarios are not required and results are verifiable and easily reproduced. assessing absolute climate effects of a bioenergy system is relevant when the goal is to follow or monitor the development, in particular ghg effects, of a bioenergy system over time or verifying the ghg effects in comparison to a predetermined threshold level (e.g. within a sustainability scheme)."
"in order to support informed decisions, the climate effects of bioenergy systems need to be analysed and communicated. this study provides a framework for choosing suitable reference systems for different types of research questions. for most research questions, an analysis of relative climate effects is required and a comparison with a reference system appropriate for the particular question asked is needed. we provide a process to choose and describe a reference system that is appropriate to the goal and scope of an assessment. if the goal is to study the climate effects of bioenergy as a part of total anthropogenic activity, the appropriate land reference is regeneration toward natural potential vegetation, and energy and material reference systems are not relevant. if the goal is to assess the effect of increasing or decreasing bioenergy use, the appropriate reference system incorporates human activities, and represents the expected alternative use of the land, energy, and materials in the absence of the bioenergy system being studied. because large uncertainties surround reference systems, and these counterfactual scenarios have a decisive influence on the calculation of climate effects of a bioenergy system, several alternative reference scenarios may be considered. the assumptions made for reference systems underpinning the results of each study should always be clearly presented, and the interpretation and communication of the results should be commensurate with the constraints of the methods applied."
"in this study, we define health tracking as using a means to keep track of any aspect of health. that is, in addition to automatic sensing, we also include health and wellness programs that implement manual tracking or self-report of health data as part of the investigation. as organizations currently implement health tracking in various ways, this allows us to explore employee perceptions and experiences broadly."
"in responses about why employees had chosen not to track as part of their workplace health and wellness program, 18% from company x (0 from other companies) mentioned that health tracking and wearable devices were just not interesting to them. for example, one respondent noted: \"generally not interested in wearable technology\" (e173, company x, survey). we were not, however, able to follow up with survey respondents who had noted disinterest as a reason for non-participation. we also recognize that non-use is complex and fluid, and not necessarily a problem to be solved [cit] . although prior research has informed why people abandon their trackers after use [cit], future research should look into refusing to use from the start to understand non-use in general."
"vision is less than 5% and more than being able to count fingers less than one meter away 3 (f, f, m) 4 (f, f, m, f) vi-4"
"despite being promising, reported findings should be considered with caution due to the limited number of participants, which did not allow for an in-depth analysis of specific stressors in each category of vision impairment. a larger group study would need to be carried out to confirm and quantify the trends obtained here. furthermore, the well-established emotiv epoc+ eeg headset has certain limitations with respect to the quality of the recorded signal during experiments involving physical activity \"in the wild\" such as those presented here. future steps of the present study include refining the predictive model through exploring novel multimodal biosignal features for cognitive load assessment and comparing different classifiers. such findings hopefully pave the way to emotionally intelligent mobile technologies that take the concept of navigation one step further, accounting not only for the shortest path but also for the most effortless, least stressful, and safest one."
"to motivate employees, company x offered a variety of health and wellness activities, and the incentives were designed to include more than just step counting or distance tracking. for example, employees could also participate in meditation classes, join an ironman race, or go to the gym more than three times a week. however, many of these activities required manual self-report whereas step counting was integrated directly into the tracking systems, and thus more easily reported. this led to most attention being paid to step-counting and leaderboards favored walking and running more than other activities, even though the website and app allowed employees to manually log and convert some of their other activities into miles. because of this, some employees interpreted step counting as a companywide goal (one million miles). in addition, simply presenting the number of times an employee engaged in an activity does not provide enough information to help employees pursue or maintain more sophisticated health goals. for example, number of gym visits may not be helpful to support goals such as improving strength level."
"one would expect the first arriving wave mode is the most predominant feature as shown in the triangular specimen far-case, however, the oval specimens have shown a significant increase of a later arriving wave and an earlier increase in unification prior to the inflection. this can be attributed to the fact that the reaction of adhesive epoxy begins immediately upon mixing the two components and the portions are not controlled. this gives rise to variation in fully cure duration and hardening. the adhesive layer may not have been uniform and consequentially, uneven curing at different areas of the specimen. the preparation time taken to assemble the test specimen upon the application of the adhesive and the installation of the plasticine to simulate soft tissue damping is also different in each specimen. nevertheless, these features indicated that the o-index is sensitivity to the bonding quality between the implant and bone structure. it is also evident that the inclusion of the damping effects by silicone and the highly damped plasticine did not affect the ability of the proposed osseointegration assessment methodology."
"three interviews were conducted in person, and all other interviews were conducted via phone or video conferencing and lasted from 30 to 90 minutes. each interview participant was compensated with a usd$10 gift card. all interviews were audio recorded and transcribed for analysis. two researchers read through the transcripts and discussed emergent themes iteratively. we then used these themes as a basis for designing survey questions intended to reach out to a broader set of participants."
"the traditional kf technique is utilized with predefined error values of the state and measurement noise; these error values remain constant for the entire simulation. however, these values are hard to obtain in the field and they are always changing with time. hence, an akf is developed to overcome this issue and to dynamically estimate the error values in the state and measurement estimates. the akf is comprised of two equations: (a) state equation and (b) measurement equation. the state equation is derived from the traffic flow continuity equation as defined in equation (2) . the state equation computes the number of vehicles by continuously adding the difference in the number of vehicles entering and exiting the section to the previously computed cumulative number of vehicles traveling along the section. this integral results in an accumulation error which requires fixing, and thus the measurement equation is needed. in equation (2), the ρ value can be observed from historical data."
"in this section, we present three types of health tracking implementation and their incentive models derived from the literature as well as our interview and survey data. these implementations are not exclusive -one company might implement multiple options simultaneously. we also do not claim this to be an exhaustive list. our goal is to provide an overview of the health and wellness programs we studied to help situate our findings."
"to understand the design and practice of health tracking in workplace health and wellness programs, we conducted exploratory interviews, a survey, and follow-up interviews with survey respondents. [cit] ."
"most employees who completed our survey and interviews perceived their incentivized workplace health and wellness program as a sign that their employer cares about employee health and happiness. this is consistent with findings from another recent workplace survey where the implementation of a workplace program helped to increase employee identification with their company [cit] . however, the potential discrepancy between health tracking technologies and the overall goals of the wellness program might affect employee relationship with the company. future research should consider implementations of health tracking and the influence on employee-employer relationship in detail. the disconnect between the limits imposed by technologies on what can qualify as health and wellness in workplace programs and employee health goals and health practices could result in employees feeling coerced to use health tracking technologies especially in long-term programs where financial incentives are involved. furthermore, where many appreciated employer interest in employee health and wellness activities outside the workplace, some quite legitimately felt this sort of attention was intrusive."
"the concept of ann is inspired by the structure and operation of the nervous system. ann is a machine learning system that has computational units, called neurons, simplification of the human neurons. in general, neurons are organized to lie in layers and are densely connected to each other. ann is able to learn and recognize patterns such as climatic patterns that can be found within the distribution of a species. [cit] ."
"timeframe for assessing climate effects the timeframe for assessing climate effects is not necessarily the same as the temporal scope for the study. for example, one could evaluate cumulative radiative forcing over 100 years (consistent with the common climate metric gwp 100 ) for land management that took place within a ten-year study period. the change in temperature reached at some future point in time (e.g. 2100), estimated by the global temperature change potential gtp [cit] that can be attributed to the bioenergy project may also be of interest. regardless of which climate metrics are chosen to assess impacts [cit], the assessment of climate effects starts from the same point of time as the temporal scope for the study."
"the last parameter tested in this study is the initial prior estimate of error covariance p i . the error covariance parameter describes the accuracy of the state system. for instance, if the covariance value is low, then the state outcome is accurate and close to the actual value. as stated in the literature, the initial parameters should always be tuned to achieve accurate estimation accuracy. thirteen different p i values were tested (i.e., 5, 10, 15, 20, 25, 50, 75, 100, 120, 150, 200, and 250) . figure 6c another critical initial parameter in the akf model is m i . this parameter represents the mean value of the noise in the state equation. this paper tests 16 different m i values (i.e., 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, and 15) . figure 6b presents the vehicle count estimation rmse values for different m i values. the rmse value is equal to 4.7 veh when the simulation starts with a 0 value of m i . in contrast, the rmse value is 3.9 veh when the value of m i is equal to 11."
"the uncertainty associated with defining reference systems is unavoidable because the reference scenario determines the path that was not, or will not be, followed (i.e. the counterfactual) and thus, by definition, its characteristics cannot be verified [cit] . retrospective analyses build on a higher degree of knowledge on the studied system compared to prospective analyses which attempt to project future behaviour. because the reference system plays a fundamental role in quantifying climate effects of bioenergy, it is important to understand the sensitivity of an analysis to reference system assumptions. impacts of some sources of uncertainty can be quantified by means of statistical approaches such as monte carlo simulation [e.g.,100]."
"based on findings in the neuroscientific literature we extracted a series of features descriptive of the cognitive and the physiological state of the participants in each time instance. the brain activity is characterized by rhythmic patterns across distinct frequency bands, the definition of which can vary somewhat among studies. here we analyzed eeg in six bands, namely, delta (0.5-4 hz), theta (4-7 hz), alpha 1 (7-10 hz), alpha 2 (10-13 hz), beta (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30), and gamma . beta activity is associated with psychological and physical stress, whereas theta and alpha 1 (i.e., lower alpha) frequencies reflect response inhibition and attentional demands such as phasic alertness [cit] . alpha 2 (i.e., higher alpha) is related to task performance in terms of speed, relevance, and difficulty [cit] . gamma waves are involved in more complex cognitive functions such as multimodal processing or object representation [cit] . features related to signal power and complexity were extracted using the pyeeg open source python module [cit] . for each of the 14 eeg channels, we computed the relative intensity ratio as an indicator of relative spectral power in each of the six frequency bands [cit] ."
3.3. bvp and hr. the photoplethysmography sensor of the e4 device measures the blood volume pulse (bvp) from which it derives on board the heart rate (hr). we min-max normalized both data streams to account for interindividual differences [cit] .
"one concern about the kf model is the use of predefined fixed values of the statistical parameters, mean and variance, of the kf state and measurement errors. applying the kf model in real-world problems is limited since the statistical parameters are assumed to be known [cit] . the mean and variance entities are known as variable rather than fixed values. to produce a flexible model, this study employs the akf model to provide real-time estimates of the statistical parameters of the kf state and measurement errors as described in the following section."
"the system state and measurement equations can be written as in equations (10) and (11), considering the errors (noise). the term u(t) is the given inputs for the system. the vector h(t) is used to convert the vehicle counts to travel times. the vector w(t − ∆t) is the state noise and is assumed to be gaussian noise with the mean of m(t) and variance of m(t). the measurement noise v(t) is assumed to be gaussian noise with the mean of r(t) and variance of r(t)."
this section evaluates the performance of the proposed models. the first subsection evaluates the performance of the akf model and then compares the akf with the kf model (section 5.1). the second subsection presents the performance of the nn model used for estimating the lmp of probe vehicles at the exit of the link (ρ out ) (section 5.2). the third subsection compares the performance of akf with the akfnn approach (section 5.3). the fourth subsection investigates the sensitivity of the akf estimation model to the initial conditions (section 5.4). the accuracy of the proposed models was evaluated based on the root mean square error (rmse) as shown in equation (24) . the rmse has been frequently used in the literature to measure the difference between the model estimates and the actual values.
"modelled as the surrogate bone. the fe dispersion curve of the cylinder circumference indicated a dominant fundamental circumferential antisymmetric (flexural) guided-wave for a 1 mhz triangular pulse excitation. the dispersion curves measured (as shown in figure 7 ) substantiated the fe study [cit] . two-dimensional fast fourier transformation is performed on the points along the longitudinal and circumferential lines to create the dispersion curves in order to identify the dominant guided wave modes from disperse [cit] . the experimental results show that the circumferential wave propagating on the triangular and oval cross-section would be similar to those travelling around the cylinder circumference as shown in the dispersion curves in figure 7 . the dominant wave mode is the flexural circumferential wave modes in both directions, and a weak symmetrical wave mode is also present [cit] . since the core of the specimen is filled with silicone rubber, the wave propagation characteristics across this material were also determined. blocks of silicone rubber with different dimensions were constructed, and the average longitudinal bulk wave velocity of 1098.9 m/s was determined using pulse-echo method and v539 transducers (panametrics, waltham, ma, usa)."
"to get an overall view of opinions on health tracking in the workplace, this study also included participants who had not taken up this opportunity. out of our 536 responses, 107 respondents from company x (20%) and 4 from other companies (13%) replied that they had chosen not to use health tracking in their workplace health and wellness program. the lower percentage of non-participation in other companies might result from self-selection bias for our study. these participants were then asked to comment on their choice in a follow-up, open-ended question (\"in the previous question you said you chose not to use health tracking in the health or wellness program offered in your workplace. why is that?\"). participants often mentioned more than one concern in these responses."
"therefore, employers should think about how to better design and promote a diversity of components for their wellness programs that can help support individual health goals without overstepping boundaries."
"many companies provide their employees with discounts for fitness trackers or reimbursement for gym membership to encourage employees to exercise on their own [cit] . one recent us nation-wide survey reported that around 18% of companies also provide onsite exercise facilities [cit] . these programs usually do not require employees to report their use or participation but also do not provide incentives for maintaining or improving healthy behavior. some research shows that free gym membership supplemented with educational resources, coaching or incentives for participation time can better improve employee physical activities than when these measures are implemented alone [cit] ."
"the most likely alternative land reference varies between feedstocks and cases. when the bioenergy feedstock is the primary product from land use, as in the case of dedicated energy crops, the land reference is the most likely alternate way to use the particular parcel of land in the absence of bioenergy. if the bioenergy land use replaces other functions, the indirect effects due to the displacement must be considered. when the bioenergy feedstock is not the primary product from land use, such as for agricultural or forestry residues, the appropriate reference would describe the default land management and fate of the residues, which could be in situ decomposition [e.g., 36] or burning [cit] ."
"most of the companies in our study were very transparent about their data collection process and access policies. these companies usually adopted third-party platforms to implement health tracking programs. as part of the system design, employers often do not have individual health tracking data but receive aggregated, anonymous reports at the end of each program period. wellness program administrators also strove to openly address the potential privacy concerns by explaining the data use policy in newsletters (p7, p10) and by having an open, standard faq in response to employee concerns (p9). there remains, however, an issue in the fact that a third-party is introduced into the process of intimate disclosure of heath data between the employees and the workplace."
"the importance of applying an appropriate reference system for evaluating the climate effects of bioenergy is widely recognized [1, 4, 7, [cit] as is the need to describe and justify the chosen reference system [cit] . nevertheless, carbon dynamics in the land reference are typically ignored in bioenergy studies [cit], and some studies explicitly focus on the climate effects of biogenic carbon fluxes in a bioenergy system while excluding consideration of the reference systems [cit] . however, carbon stocks in biomass and soil are explicitly included in several studies of forest bioenergy [cit] ."
"the major challenge of approach 2b is constructing the counterfactual scenario describing how things \"would have been\" (retrospective) or \"would be\" (prospective) in the absence of the bioenergy system. consequently, the definition of appropriate system boundaries and identification of what functions are influenced by the studied change become critical issues. economic models used to assess land system effects of bioenergy have been criticised for their uncertain results, reliance on false assumptions, and omission of critical drivers of change in land reference [cit] . the most likely alternative reference system is influenced by economics, population, technology, and policy, and multiple interactions with the environment over time. reference systems are thus always uncertain and reflect the information available to, and values of, whoever defines them. because more than one \"likely alternative\" reference system is plausible, it may be informative to analyse several plausible reference systems [cit] ."
"the parameterization of the tool at launching. the number of the projection grids. the program bounds the first grid to the first distribution, and so on. nonexistent projection distributions are created, while the existing ones are overwritten. the output of the program is the list of the projection distributions that can be handled by model builder or by other scripts."
"to illustrate the range of experiences employees have with integration of tracking into workplace wellness programs, we present findings from an empirical study conducted in three phases. first, we interviewed employees and wellness program administrators across seven us companies to understand their experiences with health tracking in a workplace wellness program. analysis of these interviews informed a survey of over 500 employees and 45 wellness program administrators. finally, we conducted follow-up interviews with selected survey respondents."
"previous studies have discussed the healing characteristics of a fractured bone, which is associated with the increase of whole-bone stiffness [cit] . it is anticipated that the osseointegration process trend is similar to fractured bone healing. the o-index is categorised into three stages (refer to figure 3 previous studies have discussed the healing characteristics of a fractured bone, which is associated with the increase of whole-bone stiffness [cit] . it is anticipated that the osseointegration process trend is similar to fractured bone healing. the o-index is categorised into three stages (refer to figure 3 this study presents acousto-ultrasonic stress wave interrogation technique to measure osseointegration levels of a bone and implant for health monitoring and assessment. the frequency analysis is conducted to investigate and discuss the wave propagation in the construct as the boneimplant osseointegrates. furthermore, o-index is calculated for each specimen to assess with the definition of a numerical quantity that can be used to describe the state of osseointegration."
"the goal and scope of bioenergy studies vary. for example, the goal may be to analyse the historical effects of a specific bioenergy chain or policy (retrospective analysis) or the potential effects of a proposed policy or a planned change in a biomass production system (future, prospective analysis). the scope of a study may range from options to manage a specified plot of land over a short time period, to national contexts over long periods [cit] . uncertainties of the bioenergy and reference systems increase as spatial and temporal scales increase, in particular for prospective analyses [cit] ."
"workplace health programs are not a new development. the first workplace health programs emerged in response to government oversight of workplace safety conditions in factories and mills going back as far as the industrial revolution. these programs primarily consisted of physicians investigating safety conditions through in-person visits [cit] . workplace safety is now regulated in most developed countries and health programs now encompass not only safety but also health more broadly. today, employee health promotion in the workplace is an \"international trend\" [cit], and investments in these programs are rising [cit] ."
"the effect of soft tissues on the dynamic response of a human tibia has been reported that the mass-loading, predominantly due to the muscle, decreases the resonant frequency and increase damping [84−88] . in order to simulate the soft tissues effects, plasticine; modelling clay, is used to surround the whole specimen to simulate the soft tissue mass-loading and damping. plasticine is chosen because of its high damping quality, and it can be moulded to ensure maximal contact, easily removed and re-used. the inside of surrogate bone sections are filled with silicone and cured for 48 h to simulate the soft cancellous bone in the medullary cavity."
"the effect of soft tissues on the dynamic response of a human tibia has been reported that the mass-loading, predominantly due to the muscle, decreases the resonant frequency and increase damping [cit] . in order to simulate the soft tissues effects, plasticine; modelling clay, is used to surround the whole specimen to simulate the soft tissue mass-loading and damping. plasticine is chosen because of its high damping quality, and it can be moulded to ensure maximal contact, easily removed and re-used. the inside of surrogate bone sections are filled with silicone and cured for 48 h to simulate the soft cancellous bone in the medullary cavity."
"building on our previous work [cit], in this study, we place the focus entirely on the visually impaired, assessing biomarkers that can predict in real time the mental effort of the visually impaired while navigating in unfamiliar indoor and outdoor urban environments. the challenges vip experience during orientation and mobility tasks can be framed according to the cognitive load theory [cit], since, during orientation and navigation tasks, a specific amount of space is consumed by the working memory for the exact cognitive demands necessary."
"to get insight into various options and implementations of health tracking in the workplace, we initially conducted semi-structured interviews with three wellness program administrators and seven employee participants from seven different companies. we recruited participants using snowball sampling through mailing lists and authors' social networks. interviews with wellness program administrators focused on their goals, design process, and the implementation details of their programs, as well as their roles in implementing and maintaining the program. interviews with employees explored prior experiences with health tracking in general as well as experiences with the health and wellness program offered by the workplaces."
"some participants noted that they already used a tracker outside of their workplace health and wellness program. [cit] showing that 69% american adults already track some health indicators, and one in five do so supported by technology [cit] . previous experience with activity tracking devices plays a role in the reactions towards workplace health and wellness programs. for example, employees who already tracked some aspects of health, but in a looser or paper based manner, appreciated how the health tracking system setup in their workplace made it easier for them: \"as a reminder and single source tracker (as opposed to losing journals, etc.)\" (e113, company x, survey). another respondent explained: \"it was able to save me some manual work\" (e498, company x, survey). these employees had some experience with tracking prior to entering the health and wellness program, and were positively tuned towards health tracking."
"overall, we found that many participants did appreciate feeling accountable, whether internally or externally. this is in conflict with prior research that has shown how being held accountable for physical activity at work can be straining [cit] . we recognize that in a survey and interview study such as ours, the negative aspects of accountability were less likely to surface as clearly as in long-term ethnographic studies. however, our findings suggest that there may be instances when accountabilities can work positively in workplace health and wellness program."
the study develops an nn model to estimate the lmp of probe vehicles at the exit of the link to reflect the total vehicle departures.
"where q is the traffic flow (vehicles per unit time), k is the traffic stream density (vehicles per unit distance), and u s is the space-mean speed (distance per unit time). the u s can be represented as shown in equation (5),"
"the current [cit] continuous distribution map of european larch (larix decidua mill.) was derived from the euforgen digital area database [cit] ), whereas the discrete (fragmented) observations were ignored. [cit] was bound to the reference period of the climate data, because the studied species has a long life cycle and can slowly adapt to the changing climate [cit] . larix decidua is one of the most climate-sensitive tree species of the alps [cit] ."
"in line with the current literature [cit], the emerging cross-validated results suggest that physiological features related to skin conductance are accurately and robustly predicting the amount of cognitive load in real time. taking into consideration these findings, the design of assistive aids adapts in real time to the requirements and personal needs of the user."
"presented similar to the oval specimen, figures 12 and 13 show the spectrogram at the five different stages of cure time. in the triangular specimen near-case, only one change in frequency of 389.1 khz at 36 µs which shifts to 381.5 khz at 38.1 µs and decreases in psd magnitude of 6.1 db/hz as cure time increases, refer to table 4 and figure 12 . wave d is identified as the returning longitudinal flexural wave mode reflected at the end of the specimen. the results are distinctly different from the other cases presented in this paper. the significance of this set of results will be discussed later. figure 13 shows the spectrogram of the triangular specimen far-case. as in previous oval specimen results, the development of the spectral response as a function of cure time is pronounced. wave e and f are the returning circumferential flexural waves of wave a and b, respectively. waves c and d are the returning longitudinal flexural waves reflected at the ends of the specimens. wave a increases in frequency whereas wave b-f decrease and wave z remains the same, refer to table 5 and figure 13 . wave a and b change in psd magnitude increase significantly of 29.5 db/hz and 26.9 db/hz as cure time increases, respectively. furthermore, wave z is one of the higher-order modes and unfortunately it is difficult to identify in this frequency range. this result will be discussed in the following section which shows that the changes in the energy of the signal are a good indication of the degree of osseointegration."
"pzt pz26 (meggitt plc, bournemouth airpot, dorset, uk) of 5 mm diameter and 2 mm thickness is used as the actuator and pz27 of 5 mm diameter and 1 mm thickness is used as the receiver. the electronic transducers are bonded onto the tip of the extramedullary struts as a shown in figure 4 . the two receivers are bonded on the strut: one closest to the actuator approximately the actuator excites a 50 v 1 mhz triangular pulse signal which is generated by the ni pci5412 function generator (national instruments, austin, tx, usa). the input signal is filtered in a model 3944 multichannel filter (krohn-hite, brockton, ma, usa) and amplified by a krohn-hite model 7602 wideband amplifier. the raw output signals are recorded using picoscope 6402d and its software (pico technology, cambridgeshire, uk) with a sample interval setting of 2 ns for a total of 50,000 samples with 128 averages."
"a single-loop detector was installed at the entrance of the link to measure ρ in and also to use as an input to the nn model. accordingly, this study develops an nn model to estimate ρ out . the tested link is shown in figure 1 . the next section describes the selected inputs (features) and the output variables of the nn model."
"other survey respondents were more negative. for example, one respondent explained: \"i have had previous negative experiences with tracking (obsessive/disordered behavior) so i choose not to track anymore\" (e288, company x, survey). this respondent, and others who voiced similar concerns, feared that health tracking could end up being an obsession, and potentially lead to lower health levels: \"tracking using apps ends up causing me stress\" (e361, company x. survey). concerns of overuse and obsession with tracking technologies have also been voiced by participants in studies of short-term activity tracking workplace wellness campaigns [cit] . on the one hand, many employees are positive about health tracking technologies in workplace health and wellness programs and consider it useful for increasing physical activity levels. however, even though program administrators tried to encourage a range of activities related to health and wellness, both employees and program administrators recognized that activities that can be easily tracked shape the programs."
"companies increasingly include health tracking as an element in their workplace health and wellness programs. this may include manual tracking or self-report, but also increasingly includes the use of tracking technologies such as the fitbit, the applewatch and other devices that can automatically sync data with the health and wellness program. companies incentivize sharing of health data from these devices in various ways. for example, some employees may receive virtual points in a gift shop, while other employees receive discounts on insurance premiums. among researchers and media, this has caused privacy concerns, discussions of datafication of health and disciplining of the employees."
"in a recent study, the kf model was proposed to estimate the number of vehicles on signalized link approaches using only probe vehicle data [cit] . the kf state equation was based on the traffic flow continuity equation and thus one value of probe vehicle lmp (ρ), for the entire link, is used to scale up the probe measurements to reflect the total flow in the second term of the flow continuity equation as presented in equation (1) . it was found that using two lmp values (at the entrance and the exit of the link) produce more accurate vehicle count estimates, especially when dealing with low lmps, as described later in section 4.3. in equation (1), n(t) is the number of vehicles traversing the link at time (t), ∆t is the variable duration of the updating time interval, n(t − ∆t) is the number of vehicles traversing the link in the previous interval, q in and q out are the probe flows entering and exiting the link between (t − ∆t) and (t), respectively, and ρ is the lmp of probe vehicles."
"where r(t) is the observation noise at time t. the first term of equation (20) is the covariance of v at time t, and n is the number of measurement noise samples. as a summary, the kf and akf models use the same equations except for the fact that the akf model estimates the statistical parameters of the noise for every estimation step using equations (17) to (20) ."
"finally, reference systems need to account for co-products of bioenergy, such as wood products in the case of forest industry [cit], animal feed in the case of crop-based ethanol or biodiesel, lignin in the case of lignocellulosic ethanol, or heat in the case of integrated ft diesel production [e.g., 88, 89] . this is a complex task because there are many possibilities for substitution in markets and for most co-products considered at national or larger spatial scales, so ripple effects extend through countless additional products, each with different ghg intensities associated with its production and use [cit], and their functional equivalency may be unclear [cit] . thus, co-product allocation methods (discussed above) are eventually necessary."
"the lack of consensus on the appropriate land reference systems has contributed to misunderstanding and disagreements about the climate effects of bioenergy [3, [cit] . different methodological choices can result in totally different conclusions, which may confuse the audience of bioenergy studies, including decision-makers. consequently, recent publications emphasize the importance of coherent reference system selection, and that corresponding assumptions need to be justified and communicated [cit] ."
"although we stressed the benefits of the tool, we should not forget to mention the challenges. ann is a black-box method, which is not able to help the ecologists to understand the underlying processes and factors that drive the distribution of species; the method can be applied specifically for modeling. this version of the tool lacks automatic parameter setting and regularization scheme, which could prevent the model from becoming overfitted (no statistical measures are calculated during the training phase and, therefore, no automatic calibration can be achieved)."
"two 3 mm thick aluminium 6060 t5 mill-finished cylinders with different diameters are used for this investigation. a small cylinder of outer diameter 25 mm approximately 130 mm in length is used as the shape-like bone structure. since the cross-sectional bone structure is different for each amputated patient, refer to figure 1, oval and triangular-like aluminium surrogates are produced. a large cylinder of outer diameter 32 mm is used to make two 4-extramedullary strut implant based on the novel osseointegration implant design [cit], refer to figure 2 . both cylinders are pressed to shape to produce the aluminium surrogate bone and snug-fitted surrogate implant with the oval and triangular cross-sections (refer to figure 4) . in this study, the aluminium surrogate implant has four extramedullary struts, which extend approximately 50 mm in length and the case depth of this study presents acousto-ultrasonic stress wave interrogation technique to measure osseointegration levels of a bone and implant for health monitoring and assessment. the frequency analysis is conducted to investigate and discuss the wave propagation in the construct as the bone-implant osseointegrates. furthermore, o-index is calculated for each specimen to assess with the definition of a numerical quantity that can be used to describe the state of osseointegration."
"where ibp stands for interval band power. the baseline ibp refers to a prestimulus time period without any task demands, in our case the resting state, whereas the activation interval (test ibp) refers to the time period while working on the experimental task. we slightly modified the estimation of the erd/ers index, defining test ibp as the time interval of one second of our recorded data. in this way, we result with one time point of erd/ers per second, where every time point expresses the synchronization or desynchronization according to the same baseline."
"lca approaches have been classified as attributional lca [cit] or consequential lca [cit] . an attributional approach typically deals with byproducts through allocation, whereas a consequential approach commonly applies system expansion [cit] . both approaches can be applied for retrospective and prospective purposes [cit] . both approaches have been applied in diverse ways [cit], using different reference systems [cit], resulting in variable results and controversies [3, [cit] . we contend that it is more important to clearly define the goal of the study and to choose the reference system that is suitable and appropriate for the goal than to categorise the lca modelling technique."
"visual impairment affects approximately 285 million individuals worldwide according to the who [cit] . assistive navigation aids are essential to the visually impaired (vip) for improving their quality of life and increase their independence. traditionally, vip relied exclusively on the white cane due to its simplicity; despite its reliability in obstacle detection, it does not provide any information regarding important aspects of navigation such as the distance, the speed, or the shortest path to the destination [cit] . new technologies came to fill this gap, enhancing the traditional assistive aids, aiming to improve the route planning [cit], navigating long distances [cit], discovering landmarks [cit], and detecting obstacles [cit] . ranging from smartphone applications to wearable devices, assistive navigation aids promote greater independence and enable vip to perform tasks formerly impossible or difficult to accomplish [cit] . yet, the focus of these aids is often on optimizing way-finding or localization tasks without taking into consideration the individual's needs [cit] ."
"this paper makes two primary contributions. first, it uses empirical data to provide an understanding of employee experiences and attitudes towards health tracking in workplace health and wellness programs. we found that program fit turned out to be a more immediate concern than privacy. second, we found that program administrators in our study were already aware of the gap between a holistic view of health and what is incentivized because it is easy to reliably track (often this is steps). we encourage the program administrators to continue their work to develop and promote programs with holistic views of health that incentivize and support more than what is easily tracked. we call for research to help administrators in this quest, which will lead to a better fit between what employees want from their workplace wellness program and what they get."
"the akf and the nn models were combined to develop the novel akfnn approach. results demonstrate that the akfnn approach significantly improves the vehicle count estimation accuracy since the ρ in and ρ out values are estimated better. subsequently, the paper compared the akf with the akfnn models, showing that the akfnn model outperforms the akf model, enhancing the estimation accuracy by up to 26%."
"bioenergy is expected to contribute to climate-change mitigation by providing energy services that displace fossil fuels while generating fewer greenhouse gas (ghg) emissions than the displaced fuels [cit] . assessing bioenergy effects on climate requires comparison of scenarios with and without bioenergy to determine the net difference in emissions and other climate-forcing factors [2, chapter 11, p.88 ]. the reference system comprises the \"without bioenergy\" scenario. the use of land, energy, and materials in the reference system are important for determining net effects of bioenergy on climate."
"while carbon stock changes can be estimated over time using natural vegetation models [e.g.,95], the natural regeneration land reference involves large uncertainties. biomass productivity and carbon accumulation depend on contextual factors including slope, orientation, soils, prior land management, climate, and the frequency and intensity of disturbances [cit] . most parks and protected areas are actively managed to reduce impacts from disturbances so these areas may not provide an accurate reference for unmanaged lands. the increasing frequency of extreme weather events, fire, invasive species, pests and other disturbances can impact significant portions of forest in some locations; up to 25% over a decade per one recent study from the southeast us [cit] . it is impossible to accurately predict future natural disturbances and the impacts of climate change on growth and decomposition rates in natural areas. also projections based on recent historical trends can be misleading [e.g.,68] because disturbance regimes may be large and infrequent or vary cyclically [cit] . global climate change can impact forest productivity due to raised atmospheric co 2 concentrations, increased atmospheric nitrogen deposition, longer growing seasons, changes in rainfall patterns, higher temperatures, and the incidence and severity of disturbances due to forest fires and insect pests [cit] . one should also specify whether anthropogenic influences on the carbon stock and growth rates, such as human-assisted regeneration or influence on fire suppression, are taken into account."
"in the following sections, we first present and discuss participant perception of health tracking in health and wellness programs. most (x: 81%, o: 66%) survey participants perceived health tracking programs useful to support their health goals. variations in different programs could cause people to have different experiences. also, company x, as described in the previous section, has a particularly comprehensive program, which may contribute to the higher perceived usefulness. other participants expressed some reservations. second, we found that time commitment was a main concern for those employees who had chosen not to participate in health tracking in the workplace health and wellness program. however, aspects such as different personal health goals, a disinterest in tracking overall, and technological challenges also surfaced. a small group of non-participating employees voiced privacy concerns, to which we pay special attention."
"amputations generally occur in different anatomic locations for each patient and no bone geometry is exactly same (refer to figure 1 ). the current osseointegrated implant devices mainly rely on further bone alternation or removal of amputated bone for a good fit with the device. russ, fitzgerald and chiu recently developed a new customisable osseointegration implant for long bones (australian patent no. 2017902308) [cit] . this customised implant design will be installed without the need for reaming or bone removal. [cit] reported on a novel osseointegration implant design which combines the extramedullary struts and intramedullary stem to ensure both initial and long-term stability, refer to figure 2. this customisable implant involves using ct imaging for a perfect fit, which bone substance is preserved since reaming is not required in the procedure, and finite element analysis to optimise shape and stress distribution for each individual patient to minimise stress-shielding and bone loss. lastly, this complex implant is then 3d-printed using biocompatible materials as one unitised structure for fast and accurate production without the need and complication of multicomponent assembly. in addition, this implant design can incorporate potential sensing technique by embedding sensors on the extramedullary struts and/or intramedullary stem. this paper is the first study to investigate the incorporation of acousto-ultrasonic methods on this particular novel implant design for continuous osseointegration monitoring. [cit] . this customised implant design will be installed without the need for reaming or bone removal. [cit] reported on a novel osseointegration implant design which combines the extramedullary struts and intramedullary stem to ensure both initial and long-term stability, refer to figure 2 . this customisable implant involves using ct imaging for a perfect fit, which bone substance is preserved since reaming is not required in the procedure, and finite element analysis to optimise shape and stress distribution for each individual patient to minimise stress-shielding and bone loss. lastly, this complex implant is then 3d-printed using biocompatible materials as one unitised structure for fast and accurate production without the need and complication of multicomponent assembly. in addition, this implant design can incorporate potential sensing technique by embedding sensors on the extramedullary struts and/or intramedullary stem. this paper is the first study to investigate the incorporation of acousto-ultrasonic methods on this particular novel implant design for continuous osseointegration monitoring."
this paper presents a framework for real-time automatic assessment of cognitive load when visually impaired people move and navigate in unfamiliar outdoor and indoor environments. the objective is to demonstrate the feasibility of real-time tracking of mentally demanding tasks which can be used as on the fly feedback to assistive devices. mobility aids for visually impaired people should be capable of implicitly adapting not only to changing environments but also to shifts in the cognitive load of the user in relation to different environmental and situational factors.
"we sent out the survey through wellness program administrators who agreed to help distribute it during the exploratory interviews. we also recruited participants using authors' social networks. we received 606 complete responses. after excluding 25 respondents who were unaware of any health and wellness programs offered in their workplace, we had 581 valid responses with 45 from wellness program administrators and 536 from employees. we sought to reach a broad audience, but one particular wellness administrator shared the survey particularly broadly and enthusiastically. as a result, most survey responses (539 out of 581 responses) came from one particular company (hereafter 'company x') since our survey was distributed through their internal mailing list. this provides us an opportunity to gain an in-depth understanding of perception and experience of a large (over 5000 employees) company (table 1) while also analyzing responses from at least 13 other companies (table 2) employees were asked what type of health tracking programs were offered in their workplace health and wellness program, and what factors they have considered when deciding whether to participate. if they had participated or were participating in such programs, we asked about their experience of participation, whether they considered that the program supported their health goal, and their overall perception of the program. wellness program administrators were asked about their goals, experience, and challenges of developing and deploying health tracking in the health and wellness program. survey respondents were enrolled into a raffle with one of six gift cards (one usd$50 and five usd$20) if they provided an email address at the end of the survey."
"reference systems are often poorly or inconsistently defined, or not specified [cit] . in the literature, a reference system may have different names: baseline; business-as-usual; counterfactual; reference case, scenario or situation; or shadow scenario [4, [cit] . the variable use of terminology can be confusing. the reference system is analogous to the baseline scenario used in multi-functional scenario analysis as the reference for modelling changes under various economic, environmental and social constraints [cit], or the baseline applied in carbon offset projects to quantify the credits earned by the project [cit] . a baseline scenario is common in economic analyses but may represent one of several hypothetical simulations where none is meant to represent the most likely description of land, energy and material use in the absence of bioenergy. the baseline scenario should not be confused with the base year applied in some environmental accounting and reporting schemes, such as the unfccc and the kyoto protocol: the base year is a historical benchmark against which emissions and removals during a specific timeframe are compared."
an acousto-ultrasonic stress wave technique to analyse the frequency response over cure time has been demonstrated to assess and monitor integrating of a bone-like and implant surrogate. the findings indicated that o-index provides a plausible approach for continuous monitoring of the degree of osseointegration. the spectrogram indicated changes in the low-frequency response at different arrival times as the specimen cures. it is shown that the development of low-frequency throughout cure time indicates successfully bonding between the aluminium surrogate implant and bone. the o-index trend and its derivative can be used to identify the different stages and absence of osseointegration. future work is currently underway to investigate this acousto-ultrasonic method in clinical/animal trials and to optimise on novel implant design.
"2b provides a reference system that incorporates human activities (i.e. anthropogenic functions are included in the reference system). the net climate consequences of increasing or decreasing bioenergy use can be studied by comparing the bioenergy system to the reference system describing the most likely alternative systems for land ( fig. 2), energy and materials. this reference system might be called 'business as usual' (bau) if continuation of documented trends is (or was, for retrospective analysis) the most likely scenario in the absence of the bioenergy system. in prospective studies, there may be options proposed or changes expected that are different from past trends and distinct from the bioenergy option. these could be considered in alternative reference systems. models can be employed to estimate differences in climate forcers under scenarios with and without bioenergy."
"our findings reveal many positive and negative attitudes among employees toward workplace activity tracking and frustrations among program administrators in the limitations of current technologies. on the positive side, many employees felt that the wellness programs demonstrated that their employers care about their health and working conditions beyond mere measures of productivity. some also acknowledged that their use of health tracking improved their own awareness of activity levels. at the same time, many criticized the health tracking programs for failing to support individual health goals in favor of easily defined and measurable one-size-fits-all metrics. some found health tracking stressful and a few were concerned about personal data disclosure, privacy and an enforced blurring between work and personal life. many program administrators hoped to support more holistic views of health, yet current programs so far tended to incentivize mostly what is easily measured and tracked: steps. in the following sections, we review current literature about workplace health and wellness programs and the recent movement of bringing health tracking into these programs. we then provide an in-depth discussion of employee challenges, goals and expectations when participating in such program, as well as reasons for deciding not to participate. we consider the tension between the design and the practice of these programs as well as implications for implementation."
"when secondary waste and residues (e.g. sawdust) are used for bioenergy, land management does not change and the appropriate reference system would describe the alternate fate of the residues in energy and material systems. the most likely alternate fate of feedstock should be considered in the reference system, which may include for example sawdust for animal bedding, manure for soil amendment, or tall oil for chemical products. sometimes the alternative fate may be a different energy product, in which case the climate effect may involve market-mediated impacts. if the likely alternative disposal of the feedstock was in landfill, it could represent a carbon reservoir and a methane source [cit], both of which must be taken into consideration when calculating the effects of diverting the waste to bioenergy."
"assessments involving long rotation forest systems are especially sensitive to choice of temporal scope. the estimated change in carbon stocks depends on which phases of forest growth, harvest and regrowth are included in the study. emission profiles and carbon stocks in land reference projections vary depending on assumptions and choice of a starting point. for example, [cit] emphasize that \"estimates of effects always depend on the reference case and many alternative future scenarios are possible.\" they go on to illustrate how different reasonable reference scenarios would generate completely divergent climate-forcing results when compared to a single bioenergy scenario [cit] documented how the bau estimated by the us forest service at different points in the past reflected expectations of net growth or net loss of carbon stocks, depending on when the bau started. similarly, if forest management intensifies in the bioenergy system, relative effects depend on how long management investments continue in the future. what was considered best management at t 0 fig. 3 . temporal scopes for studying the land system. the shaded area between t 0 and t 1 represents the change in carbon stocks quantified in the study. the natural regeneration following management for bioenergy is included (a,c) or excluded (b,d,f) in the temporal scope and corresponding differences in carbon stock quantification are illustrated. case 3e illustrates a bioenergy system with increasing c stock due to improved management. case f illustrates a business as usual (bau) reference system. may become sub-optimal or counter-productive in the future, for example, due to improved forest management, genetic material, new competing species and pests, or changing climate or policy instruments that influence the relative value of different ecosystem services. at the end of a future forest rotation, we cannot state with certainty whether a forest will be abandoned, razed for urban development, cleared for agriculture, or continue to be managed for biomass. as the future is uncertain it is essential to clearly document assumptions and their implications on results and interpretation."
"the research proposed a novel akf model for estimating the number of vehicles on signalized approaches using only probe vehicle data. an akf model was developed to provide real-time estimates of the statistical properties (mean and variance) for the state and measurement errors. the state equation is derived from the traffic flow continuity equation, while the measurement equation is constructed using the traffic hydrodynamic equation. results show that the proposed akf model outperforms the traditional kf model (improves the estimation accuracy by up to 29%), demonstrating the need to use real-time values of the statistical noise parameters in the kf model."
"two estimation models were presented, namely (a) the akf and (b) the akfnn. the akf model uses only probe vehicle data assuming a fixed lmp value that is obtained from historical data, while the akfnn uses a fusion of probe and single-loop detector data with real-time estimates of the lmp values (ρ in and ρ out ). in this paper, a robust nn model was developed to provide accurate real-time estimates of the ρ out values. the selected features of the nn model are a t (observed from the single-loop detector), a p, u s, s 1, and s 2 (observed from probe vehicles)."
"this transparent and minimal use of health tracking data might have eased current employee concerns. on the other hand, some employees felt their data were underused and wished their programs would provide in-depth analysis of their data, creating more opportunities to help achieve their health goals. as more and more employees have experience with health tracking data, employers might need to put thought into how to provide personalized information to support individual health goals while assuaging privacy concerns. building on the citizen science movement, individual-lead collective health data analysis has drawn attention from research [cit] . as employees see the need to derive better value from the tracked data shared with wellness programs, there is potential in employees participating in the decision of how these data can be used. future research should investigate employee expectations and concerns with regard to employee-driven data analysis."
"whether an analysis is retrospective or prospective, two distinct timeframes must be specified. first, the temporal scope for the study is defined as the period during which ghg emissions and removals and other climate forcers will be measured. this is generally the period during which the studied systems are expected to impact land resulting in climate forcing effects. second, the timeframe over which the climate effects are quantified has to be specified (timeframe for assessing climate effects). these methodological choices influence the results and their interpretation, and should correspond with the goal and scope of the study. in the following, the importance of these choices is discussed through hypothetical examples. the temporal scope for the study and timeframe for assessing climate effects must be the same for the reference system and the bioenergy system."
"finally, the study investigated the impact of the initial conditions (n i, m i, and p i ) on the akf performance. results show that the akf model is very sensitive to the initial conditions. for instance, starting the simulation with an n i value of 8 instead of 0 improves the estimation accuracy by 10%. in addition, starting the simulation with an m i value of 11 instead of 2 enhances the estimation accuracy by up to 10%. for the p i parameter, an improvement of 7% could occur if the simulation starts with an initial value of 150 instead of 75 veh 2 . the study also tested the accuracy of the akfnn estimation by allowing the p i parameter to be tuned (tuned akfnn approach), showing that more improvement could be achieved. specifically, the tuned akfnn improves the accuracy by up to 27%."
"real-time traffic state estimates have been increasingly recognized following the introduction of recent advanced technologies such as connected vehicle (cv) technologies. cvs aim to improve road safety by potentially reducing human errors, mitigating traffic congestion levels by offering alternative routes, and reducing on-road emissions and fuel consumption [cit] . nowadays, conducting research with limited probe vehicle data (e.g., cvs) is a challenge, especially when no additional data sources are provided. hence, past research has utilized probe data in conjunction with existing detection systems to enhance proposed traffic models, despite the limitation that fixed detection techniques (e.g., loop detectors) always have some noise in their data [cit] ."
"there is currently no information about the long-term outcomes with the current developed implant systems and the early evaluation of the implants fundamentally provides important understandings to further optimise future designs for the later stage of osseointegration. the clinically approved osseointegrated leg prostheses (oilp) are: the osseointegrated prostheses for the rehabilitation of amputees (opra) system [cit], the integrated leg prosthesis (ilp) system [cit] and the compress ® device [cit] . the opra system comprises of an intramedullary titanium screw while the ilp system involves a press-fit intramedullary implant. these large endoprosthesis implants are commonly used in arthroplasty as it provides good primary stability [cit] . however, despite the advantages of oilp, their process requires a large amount of cancellous bone to be removed by intramedullary reaming. this indirect heating from reaming and bone removal process will negatively impact the osteogenic factor and, thus, resulting in loss of the essential bone substances. clinical and numerical assessments have reported on bone resorption and stress-shielding in the near-prosthetic location, which potentially causes aseptic loosening and construct failure [cit] 17] . the compress ® device, a complaint pre-stress fixation system, has been designed to tackle the issue of stress-shielding, which is accomplished using an implant system that applies constant compressive force across the bone-implant interface and with a smaller intramedullary component [cit] . this small implant design greatly prompts osteogenic factor which is desirable for secondary stability in later stages of osseointegration. however, in some cases, it does not provide adequate stability to support physiologic loading in the early stages of osseointegration due to torsional overload [cit] ."
"3.2. eda. the skin conductance data was decomposed into two continuous components, namely, phasic and tonic component [cit] . this decomposition and subsequent extraction of tonic and phasic electrodermal activity (eda) features were performed using the ledalab toolbox (http://www.ledalab.de/). overall, we extracted six features: mean tonic eda (tm) and the number of \"spontaneous\" scrs (i.e., phasic changes not traceable to specific stimulation), which are known to be particularly suitable for longitudinal monitoring of emotional stress-elicited eda (i.e., tonic arousal); sum of amplitudes of registered scrs (as) and average, maximum, and cumulative phasic eda (pm), which provide varying indicators of instantaneous phasic arousal [cit] ."
"in order to evaluate the degree of osseointegration, the acousto-ultrasonic methods and frequency analysis techniques, which have been used for shm to describe the material mechanical properties [35,37−43], are considered. the acousto-ultrasonic evaluation has been studied in bone healing assessments, such as fractured bone healing, structural form, and osteoporosis, by utilising the wave propagation velocity and time-of-flight method [23,61−63] . over the past decade, there is particular interest in stress wave propagation for quantitative analysis, and furthermore, guided waves propagation in bone has also been considered both computationally and in vivo experiments [cit] ."
"to some respondents this concern related to the boundary between personal life and workplace: \"i feel my stats are personal. i don't need work involved in my personal wellness tracking because it goes beyond my work day. i don't want to feel like my every move is being monitored by work. it just feels uncomfortable\" (e534, company x, survey). people have different tolerances for how they prefer work and private life to merge, and this diversity is likely to always be present to some degree. however, if financial incentives become great enough, some people might choose to utilize health tracking, even if it overrules their personal preferences. while we did not see many participants express these concerns, it is still important to acknowledge them, and to realize that this can lead to feelings of coercion regardless of financial compensation."
"in our study, we strove to understand overall concerns about health tracking in health and wellness programs. [cit] . most of our survey and interview participants did not voice concerns about privacy in regards to sharing their health tracking data with their employers. this stands in contrast with recent concerns of both researchers and the media [cit] . however, this does not mean that privacy is not an issue. in this section, we discuss several possible explanations. we also note that as other concerns are addressed, privacy concerns may become more important."
"the program has a linear run in a temporal term with five distinguished phases: verifying, data preprocessing, training, projecting, and processing phases; see figure 1 . the verifying phase verifies the input data and the parameters formally and in terms of the content. in case of any problem, the program shows an informative error message and terminates. the data preprocessing and training phases are done if the program was started with the parameter \"should training be done?\" being checked. during the data preprocessing, the climatic data are studentized (standardized) for faster training; presence/absence is calculated for every geographic point, and the training pattern is created. either the entire grid of the reference period can be used for training or a part of it can be selected randomly. in the training phase, the core of the program (the neural network) learns until one of the previously set three termination conditions is satisfied (see them in the next section)."
"the reference use of materials should be defined for significant inputs required in the bioenergy system in addition to land, biomass and energy [cit] . such inputs may include resources used for infrastructure (e.g., steel, concrete), fertilizers, pesticides, process chemicals, and services (e.g., repair and maintenance). typically, a material reference system assumes that the resources would not be used or produced in the absence of bioenergy. however, similar to land and energy, market responses to changes in consumption complicate the characterization of the most likely alternative system. the impacts of the materials reference are typically minor compared to those of land and energy references."
"serious + educational + [cit], may 6-11, 2017, denver, co, usa moreover, not everyone had goals that can be easily tracked using fitness trackers and apps and therefore are not easily awarded points or credit in the health and wellness programs. for example, although it might be possible to manually journal weight training, employees often cannot input these data into the system. similarly, some participants had healthy eating goals, but these data were also not part of the incentivized activities: \"it's not so much about getting in shape anymore, but more about paying attention to how i treat my body. what i eat and working out go hand in hand\" (e96, company x, survey). others just did not see how tracking would help overall health levels: \"while i completely agree that health is important, i don't see how tracking these features day-to-day is important\" (e399, company x, survey)."
"an extract of modeling results can be seen in figure 4 . the modeled potential distributions include parts of norway and sweden, which are not displayed. the modeled potential distribution for the reference period shows great similarity to the observed distribution. although more similarity could be reached in the case of a longer training phase, that could result in an overfitted model. the cohen's kappa [cit] ) value of the model result for the reference period was 0.4905."
the concept and aim of the program are complex issues and might include many potential developing targets. the main effort for the future version of this program (1) would handle probabilities rather than (or in addition to) binary presence/absences; (2) would continuously model to the reference period to calculate roc/auc or cohen's kappa values and apply them for early stopping regularization (calibration); (3) would dynamically change the discretization boundary; (4) and would optimize the projecting and processing phases to multicore processors.
"the study examines the impact of the initial conditions on the akf estimation model. three initial condition parameters are tested: the initial vehicle count estimate, the initial mean estimate of the state noise errors, and the a priori initial covariance of the state system. this paper is organized as follows. the first section describes the development of the simulation data. the second section describes the estimation models and the problem formulation for the kf, akf, and akfnn models. the third section discusses the results of the new proposed models. the fourth section provides the conclusions of the study and recommended future work."
"overall, outdoor and indoor environments that were more dynamic with respect to complexity and unexpected obstacles, such as crossing a major road, strolling through an open urban space, walking through a narrow alley with coffee tables and advertisement boards, using an elevator, and going through automatic doors, resulted in substantially higher erd values (i.e., lower relative power) across the two alpha table 2 ) and for each category of vision impairment (severely impaired: visual acuity less than 10% but greater than 2%; almost blind: visual acuity less than 2%). (a) lower alpha band erd/ers. (b) upper alpha band erd/ers. in each subplot, outdoor scenes are depicted in the left panel and indoor environments are drawn in the right panel. bands, which implies increased task difficulty. these cognitive load \"hotspots\" are in full agreement with the scenes reported as stressful by the participants themselves at the end of the study."
"as participants in health and wellness programs begin to share health tracking data with their employers and insurance providers, there is a need to understand perceived benefits and concerns about the sharing, and how these experiences might evolve over time [cit] . knowing how employees value (or do not value) the use of health trackers can inform the design and development of wellness programs. well-designed programs can lead to greater uptake and sustained engagement with healthy behaviors."
"in the case of training, the user should set the parameters of the ann, in other words, the number of hidden layers, the number of neurons per hidden layer, the learning rate, and the momentum factor. a point-type esri shapefile (grid) containing the climatic parameters in columns should be loaded as input of the climatic data of the reference period (reference grid). the grid should contain only the climatic parameters and the fid/oid/shape fields. the user should previously select the appropriate column to avoid high collinearity [cit] ). another input is the distribution of the species formatted as esri polygon-type shapefile (reference distribution). the program bounds the reference distribution to the reference grid. also, the number of training points and the termination conditions of the training can be set. if no training point number is given, the program uses the entire reference grid as a training pattern. the optional termination conditions are (1) the number of iterations; (2) the error value to be reached; (3) the training duration in milliseconds."
a structured approach for choosing a suitable land reference in bioenergy studies is illustrated in fig. 1 . each subsection (q1-q4) presents a question one needs to ask when defining the goal and scope of the study. assumptions need to be clearly stated and disclosed and care is required to ensure valid interpretation consistent with the study's goal.
"28% of responses from company x and 2 responses from other companies noted that they were already active, and did not feel they needed to use a tracking device to stay active: \"i have my own personal tracking system.\" (e528, company x, survey). previously we discussed how participants who already kept track of one or several health indicators in their minds or by pen and paper were generally positive towards switching to technology supported tracking. however, for participants who already used technology supported tracking, switching to the technology used in the health and wellness program was a hurdle."
"the discrepancy between individual health goals and what fitness trackers and apps can track created tensions. while some respondents were potentially interested in switching from their own tracking routine to using a tracking device in connection to their wellness program, they felt it did not fit their overall wellness goals, or did not feel tracking could support them in reaching their goals."
"as a result, many employees considered health tracking technologies more useful for people who were new to tracking and exercise. people who already were active often did not see the value of counting steps and therefore did not participate in the program or only participated to get the financial incentives. program designers put effort into encouraging a diversity of activities, supporting individual variation in health goals. however, the ease and prevalence of fitness tracking technologies, combined with leaderboards that clearly acknowledged only steps and distances, caused employees to focus on activities that could be tracked with wearables."
"interview transcripts and open-ended survey responses were coded using open and iterative coding. two authors coded separate sections, and then discussed these codes and emergent themes, identifying tensions or overlaps. all authors then engaged in an iterative process of identifying main themes in the analysis and writing."
"to automatically identify the cognitive load of urban indoor and outdoor spaces experienced by vip while walking through it based only on their biosignals, we postulated the study as a supervised classification process. a widely used ensemble learning method for classification was employed, namely, random forest (rf) classifier [cit], selected due to its ability to deal with possibly correlated predictor variables and because it provides a straightforward assessment of the variable importances."
"climate effects of bioenergy result from feedstock production, transportation, processing and use of bioenergy [cit] . in addition, indirect climate effects can occur if the bioenergy system causes a change in other activities, for example, through influence on land, or energy and food markets [cit] . climate effects have generally been quantified as net effects of ghg emissions and removals, but recent studies [cit] have shown that non-ghg climate forcers can significantly influence the climate impacts of a bioenergy system, for example when changes in land management alter reflectance (albedo) and particulate emissions. thus, total net climate effects are a product of interactions among all climate forcers that differ when the bioenergy system is compared to a reference system. climate forcers include greenhouse gas (ghg) emissions, albedo, latent heat, aerosols, black carbon and other particulates across the life cycle stages. the choice of which climate forcers are considered and how they are determined is critical [cit] . the assumptions that define the reference system are important for all climate forcers."
"having extracted the power band features from the eeg signals, we estimated the event-related (de)synchronization (erd/ers) index, a well-established measure of band power change in eeg originally proposed by pfurtscheller and aranibar [cit] . it is defined as"
"in our data set, we have most responses from employees from company x. due to the anonymization process of the survey we could not always determine which company respondents were affiliated with. however, some provided links to the health and wellness program used in their company, and we were thus able to determine that we have responses from at least 13 companies besides company x. as we analyzed interviews and survey responses we paid special attention to potential discrepancies between attitudes and experiences expressed by employees of company x and all other companies. we found, however, the determining factor of experience and attitudes was in the nuance of what was offered at their local office or company, not whether they were employees in company x. across all survey questions, most responses between employees from company x and other companies are within 5% of each other. we hereafter present the results of responses as split percentages (x: x%, o: o%) for company x and other companies, respectively. we discuss nuance in differences. in the following we first present some background information to show the various tracking options available to employees in company x."
"the major drawback of approach 1a is that it does not capture the effects of the bioenergy system compared to what would have occurred in the absence of the bioenergy system. for example, in the absence of the bioenergy system, vegetation may have continued to grow, been used for other purposes, or been burned in-situ, each of which has a different effect on ghg emissions. due to this drawback, approach 1a has limited applicability in assessing climate effects of bioenergy systems."
"a modeling process, including the input data types, the selected parameters, and the modeling result, based on the distribution of european larch (larix decidua mill.) is presented as a case study. although using a more sophisticated cem and more adequate predictor variables (e.g., soil type, exposure, potential evapotranspiration) could reflect more on the demand of the species, the only aim of the case study was to show how easy the application of the tool is."
"the proposed framework is based on multimodal fusion of brain and peripheral biosignal features. using stressrelated features of the eda signal and an eeg index of cognitive load based on event-related (de)synchronization in the alpha band (erd/ers), we identified the most important cognitively demanding \"hotspots\" for the generic vip population and for the specific categories of sight loss, pointing out the particular needs/difficulties faced by each vip category. the high prediction rates in the multimodal classification experiments (83-97% auroc weighted, table 4 ) are very encouraging of the proposed approach. even if the chosen urban and building sites did not represent all possible different outdoor and indoor environments and situations in terms of complexity and difficulty, the charted routes were designed so as to combine most of the mobility challenges faced by vip."
"absolute emissions can include the reduction in biomass and soil carbon stock from clearing native vegetation and preparing a site for agriculture or biomass production. inclusion of the carbon loss in establishment of the bioenergy crop is equivalent to assuming a static historical baseline corresponding to the situation before the bioenergy system was established. under approach 1a, market-mediated impacts are excluded as they require comparison to a reference scenario. where bioenergy is one output of a multi-product system, the emissions are attributed between bioenergy and by-products using allocation."
"the experimental work involves three piezoelectric elements (i.e., an actuator and two receiver sensors) bonded onto the different struts of the implant to measure the frequency responses as the specimen cures (refer to figure 4 ). the actuator excites; transmit energy, in the form of stress waves, and as the specimen unifies, the energy transmission changes due to the change in stiffness of the overall construct. our future investigation on continuous osseointegration monitoring will consider the intramedullary stem and other possible locations for sensors placement."
"some companies organize events or challenges to promote awareness of healthy behavior and to encourage employees to increase levels of physical activity [cit] . these events or challenges usually range from one to three months in length with incentives such as gift cards or cash rewards. in some cases, companies offer employees free tracking devices, while in others employees must purchase their own devices to participate. sometimes such events are designed as competitions, during which employees might compete as individuals or in teams for prizes. others have predefined goals, such as average steps/miles per day or number of days biking to work, and any employee who reaches the goal can receive incentives."
"health tracking programs, in their current forms, impose other more concerning challenges on employees than their privacy expectations. for example, some employees struggled with the balance between busy work schedule and fitting workout into their routine. although employees appreciated employer interest in physical activities, longterm, everyday health tracking can become burdensome for some. positive health and wellness promotion rhetoric in the workplace influences employee choices to participate, and they expect positive outcomes. however, such efforts can also force an unwelcome renegotiation of boundaries between work and private life for some [cit] ."
"similar to the oval specimen, figures 12 and 13 show the spectrogram at the five different stages of cure time. in the triangular specimen near-case, only one change in frequency of 389.1 khz at 36 µs which shifts to 381.5 khz at 38.1 µs and decreases in psd magnitude of 6.1 db/hz as cure time increases, refer to table 4 and figure 12 . wave d is identified as the returning longitudinal flexural wave mode reflected at the end of the specimen. the results are distinctly different from the other cases presented in this paper. the significance of this set of results will be discussed later."
"the advantage of approach 2b is that it provides the most complete assessment of the climate effects occurring due to a decision about bioenergy. information comparing a \"most likely\" alternative scenario to the bioenergy system is often required by policy makers, decision makers in the energy sector and land managers."
"the erd/ers index of cognitive load was averaged over all electrodes per frequency band per second. the resulting averaged index was binned in three chunks, namely, \"low,\" \"medium,\" and \"high\" load. we trained a rf model to predict the aforementioned labels of cognitive load index per each band, inferring on the features extracted from the skin conductance and blood volume pulse sensor. the adjustment of the two most important parameters of rf was performed by means of grid search parameter estimation with 5-fold cross-validation. we exploited the effect of the number of estimators [cit] and the effect of the maximum number of features [.5, 1, 2] * √ number of features. overall, the optimum number of estimators was 300 and the maximum number of features was set equal to the total number of features for each experiment. table 4 reports the classification results in terms of auroc weighted metric. hereafter, we will refer to auroc weighted metric with the term \"accuracy.\" for each frequency band, the average accuracy over 5-fold is reported, along with the respective standard deviation. we note that for all frequency bands the performance of the models is quite accurate and robust. as mentioned, the erd/ers index employed for the definition of the classes was averaged over all electrodes; in literature, there are many studies associating specific electrodes to brain functions, for instance, to memory recall tasks; however, the emotiv epoc+ used for the experiments does not provide a full coverage of the cranial surface so as to focus on specific electrodes. following the exact same scheme for the classification of the cognitive load states (\"low,\" \"medium,\" and \"high\") from the separate electrodes per band we obtained accuracy values identical to the averaged results per band. figure 2 depicts the most predictive features of eda and heart rate of the cognitive load. note that the order of importance and the relative amplitude of the \"gini\" importance value are comparable for all the frequency bands showing the stability of the approach. these findings are in line with the studies in the literature, where the skin resistance is stated to be an important indicator of the cognitive load [cit] ."
"the experimental work involves three piezoelectric elements (i.e., an actuator and two receiver sensors) bonded onto the different struts of the implant to measure the frequency responses as the specimen cures (refer to figure 4 ). the actuator excites; transmit energy, in the form of stress waves, and as the specimen unifies, the energy transmission changes due to the change in stiffness of the overall construct. our future investigation on continuous osseointegration monitoring will consider the intramedullary stem and other possible locations for sensors placement."
"the nn model was employed to predict the (ρ out ) value, which is used to reflect the total number of vehicle departures from the given number of probe vehicle departures. the data set was divided into 70% for training, 15% for validation, and 15% for testing. the validation data set is used to measure network generalization and to avoid any over fitting problems [cit] . the developed nn performance is shown in table 3 . the mean square error (mse) is 0.01 and the r value is close to 1.0. the r value measures the correlation between model outputs and desired outputs. a value close to 1.0 means that the model outputs are very close to desired outputs. figure 3 shows the error histogram for the training, validation, and testing data and their deviations from the zero error bar. most of the errors lie around the zero error bar, which means that the developed nn model appropriately addressed the research goal (i.e., estimating ρ out ). figure 4 presents the predicted and actual values for the ρ out at different lmps."
"three participants (3%) from company x and two participants from other companies voiced privacy concerns. while many respondents in our study felt that their employers' interest in their health was a good thing, some felt it was invasive and thus chose not to participate."
"wheren − is the a priori estimate of the vehicle counts calculated using the measurement prior to instant t, andp − is the a priori estimate of the covariance error at instant t. the kalman gain (g) is demonstrated in equation (14). the posterior state estimate (n + ) and the posterior error covariance estimate (p + ) are updated as shown in equations (15) and (16), considering the probe vehicle travel time measurements."
"on-going, continuous health tracking program can be useful in some cases and employees in our study disagreed with each other. where some were in agreement with prior research [cit] and appreciated the limited length of health promotion efforts, others wanted more. some survey participants and interviewees whose companies offered short-term health tracking events expressed a preference for continuous health tracking options: \"health is not for three months\". however, we urge companies to consider the goals and appropriate use cases for each option and to explicitly communicate these goals to their employees when implementing health tracking programs. while continuous health tracking might help people who want to develop consistent, regular routines of working out, it may not be useful for everyone. in this case, the incentives should be designed to encourage consistency rather than fixed, step-count goals [cit] . short-term events are effective to increase employee attention and to boost activity level in a short period of time [cit] . people also might be able to continue the habit and apply the knowledge obtained from participating in short-term events and use it independently serious + educational + [cit], may 6-11, 2017, denver, co, usa [cit] . however, companies should also consider how to help sustain the behavior if employees wish to. these short-term events should also account for a diversity of health goals, activities, and work routines. in this way companies need to understand which challenges the individual employees are facing, and find the right fit to respond to that need, rather than forcing one-size-fits-all wellness programs."
"the application, applied methods, and example model results of the newly developed ann distribution arcgis tool are reported to introduce this tool to the community of ecologists. the application of the program is simple because no data transformation, presence/absence calculation, and data migration to statistical software are needed. the program was optimized to the typical data formats of cem. as far as the authors know, the presented program is the first ann-based simple cem tool written to arcgis."
"where m(t) is the state noise at time t, the first term of equation (18) is the covariance of w at time t, n is the number of state noise samples. the mean (r) and variance (r) of the measurement noise are shown in equations (19) and (20), respectively."
"consistent with prior work [cit], 51% of the responses from company x and 2 responses from other companies mentioned concerns about time commitment. these concerns pertained to time spent uploading and interpreting data as well as to long-term use. some participants felt that they just did not have time, and that wearing a tracking device and understanding its data would be \"one more thing to do in a busy day\" (e34, company x, survey). thus, some felt that health tracking would be one more workplace demand difficult to accomplish in an already hectic schedule. one participant explained: \"requires too much work / effort on my end and i don't necessarily have the time to follow through on a daily basis\" (e246, company x, survey). these participants saw the potential of tracking to offer insights, but objected specifically to the long-term, daily use the program encouraged: \"why do i have to track all the time? it's sickening to having to do that. i only need maybe once a quarter at most\" (e73, company x, survey). previously, we highlighted how some participants appreciated that physical activity was connected to the workplace, however, here we see others interpreted this same intervention as yet another workplace responsibility."
"models, such as forest growth or energy system models, are often employed for prospective analyses [cit] . to define the energy reference system, information on the current energy system can be used as a starting point. the large-scale capital investments in current energy infrastructure make it difficult to change energy systems and changes take long time. when modelling reference land use for forests, historical data, such as the age class distribution of the stands and possible changes in future growth rates and carbon stocks, e.g. due to climate change, should be taken into account. uncertainties for land system modelling can be high due to simplifications required to represent complex systems, and low quality input data [cit] . when determining and trying to minimize the uncertainty in defining the reference system, it is important to concentrate on those aspects to which the reference system is the most sensitive [cit] ."
"the program can be run (1) as a tool of the arctoolbox either manually or by model builder; (2) or as a script from the python window or from other scripts. the program needs several inputs to be given and starting parameters to be set. all the inputs and parameters can be set in the starting window of the tool (figure 3 ) or as parameters of the function. after the program has been started, the user cannot affect the running of the program. in the tool window, the user specifies whether both training and projection should be done or only one or the other. in the case of the training-only or the training-and-prediction mode, the trained network can be saved to a given file. in the case of prediction-only mode, the network previously saved can be loaded from the given file."
"for the clustered samples, the weights of fuzzy attributes are very important to the accuracy of trustworthiness evaluation for monitoring point. ccsd method [cit] for determining the weights of attributes in multiple attribute decision making considers attributes with big standard deviations should be given more important weights than those attributes with small standard deviations. however, this property is not suitable for the weight distribution of fuzzy attributes of monitoring points. we solve the problem and give an improved ccsd method for monitoring points."
"let's take class setosa and class versicolor for example. for each class, we take the first forty-five samples as training samples and the last five samples as test samples. fig. 1 and fig. 2 show the weights of each fuzzy attributes computed with four weight distribution methods (equal weight, ccsd [cit], information entropy [cit] and improved ccsd) for class setosa and class the weights of attributes for class versicolor."
"we have performed experiments on a pc with intel (r) core (tm)2 duo e7500 2.93 ghz and 2 gb of main memory running linux kernel 2.4.20. the grain of monitoring point is set to system call. each system call is intercepted by loadable kernel module (lkm) and modified to capture the attributes' values of the system call. because the trustworthiness of deterministic attributes determines the trustworthiness of monitoring point directly, we mainly discuss the effect of fuzzy attributes on the trustworthiness of monitoring point."
"the expected behavior trace of software is usually composed of a sequence of monitoring points and events or actions causing monitoring points' transition. the common monitoring points are system call, function module, component, etc. the fine-grained monitoring points result in a high degree of trustworthiness, but with low software running efficiency. therefore, the monitoring points should be set by comprehensively considering the needs of the degree of trustworthiness and software running efficiency in actual application. for each monitoring point, there are a group of attributes describing the expected running situation when the software runs to it. it is very important to study the multidimensional attributes of monitoring points for the trustworthiness evaluation of software behavior."
"the paper proposes an approach of trustworthiness evaluation of software behavior based on multidimensional fuzzy attributes. it clusters the samples of the same monitoring point based on multidimensional fuzzy attributes to construct a more accurate expected trace of software. for a better trustworthiness evaluation effect, the paper proposes an improved weight distribution method of multidimensional fuzzy attributes based on ccsd method for weight distribution of attributes in multiple attribute decision making. the improved weight distribution method is suitable not only for the multidimensional fuzzy attributes of monitoring points, but also for any weight distribution according to one-class samples, so it is of widespread usage. our future work is to consider the selection of multidimensional fuzzy attributes for monitoring points to achieve a better effect for trustworthiness evaluation of software."
"in the training phase, the sample values of a fuzzy attribute for the same monitoring point can vary obviously because the path from last monitoring point to the current monitoring point can vary. therefore, the expected behavior trace constructed is inaccurate if all samples of the same monitoring point are taken as one training set. we solve the problem by clustering these samples based on multidimensional fuzzy attributes."
⑤ compute the between-class entropy of any two classes according to (2) and merge two classes with smallest between-class entropy. the number of classes and samples for each class must be also modified. (2) is as follows:
"our experiment uses machine learning databases (mldbs) from uci repository [cit] including lots of different databases. we select iris database which has three classes (setosa, versicolor and virginica). each class has 50 samples and each sample has four attributes (sepal length, sepal width, petal length and petal width) which are all continuous real variables. there is a great difference between the attributes' values of class setosa and other two classes; while, there is little difference between the attributes' values of class versicolor and class virginica."
"versicolor respectively. for class setosa, there is little difference among the correlation coefficients of four attributes in the improved ccsd, so the weights of attributes mainly depend on the standard deviation. table iv . for the test samples of class versicolor, the test results of four weight distribution methods are same. for samples of class setosa, the evaluation results of four weight distribution methods are same and have no misjudgment samples; for the average of trustworthy degree of four weight distribution methods, improved ccsd has the smallest average of trustworthy degree which means the largest degree of deviation, so it can determine the untrustworthy sample most easily. for samples of class virginica, the evaluation results of improved ccsd have the smallest misjudgment samples and the smallest average of trustworthy degree; the second is information entropy. ccsd for determining the weights of attributes in multiple attribute decision making is the worst one. therefore, our improved ccsd has better effects for trustworthiness evaluation."
"with continuous deepening of the application of software in the sensitive fields such as finance, military affairs and economy, the requirement of software trustworthiness becomes more urgent. how to ensure high confidence of software during software development and running has become an important research direction of software theory and technology [cit] . if the software behavior is always accordant with the expected behavior, we call the software is trustworthy [cit] . for trustworthy software, the behavior and results can be expected and the behavior states can be monitored when it runs."
"software vi6.1 is the editor in red hat 9 linux. in clean environment, we capture 15 samples along three different paths in a monitoring point of software vi6.1 where three paths converge. three fuzzy attributes time interval, memory variation and cpu variation are involved and they are the absolute value of difference of time, memory occupancy rate and cpu occupancy rate respectively between the current monitoring point and last monitoring point, as shown in table i. table ii shows the fuzzy attributes' values of two normal traces and two abnormal traces in the monitoring point for test."
"on the basis of the above problems, this paper presents an approach of trustworthiness evaluation of software behavior based on multidimensional fuzzy attributes. first, the expected behavior of software monitoring points is constructed for each class of samples according to the clustering results based on multidimensional fuzzy attributes. second, an improved weight distribution method of multidimensional fuzzy attributes is proposed based on ccsd method for determining the weights of attributes in multiple attribute decision making. both the dispersion of attribute value and the influence among these attributes are considered, which ensures the better effect of trustworthiness evaluation."
"the above attributes of software monitoring point can be divided into two categories: deterministic attributes and fuzzy attributes. for deterministic attributes, once any of them deviates from the normal value, the monitoring point is determined to be untrustworthy directly. these attributes include function, arguments policies, context, etc. fuzzy attributes cannot be expressed as accurate numbers. they are fuzzy and granted the prescribed error bounds. these attributes include cpu occupancy rate, ip transmission efficiency, memory occupancy rate, time interval, etc. the above references construct the expected behavior of software monitoring points by running the software many times in the training phase, and take all samples of the same monitoring point as one training set. the sample values of a fuzzy attribute for the same monitoring point can vary obviously because the path from last monitoring point to the current monitoring point can vary. therefore, the expected behavior trace trained by the above approaches is inaccurate."
"the smaller standard deviation means that the sample data is more centralized, and the ability to describe normal behavior is stronger, so the weight of fuzzy attribute j a is larger, and vice versa. we integrate the standard deviation and correlation coefficient and define the weights of fuzzy attributes as follows:"
"simulation technology provides a very powerful way to move from an \"as-is\" system to an ultimate \"tobe\" system. an \"as-is\" system accurately captures the behavior of the original system and then hypothetically changes the system until the best scenario is identified. by conducting a \"what-if\" analysis, the simulation analysis should be able to discover a future determined without implementing the technology [cit] . in the study, the coal output using excavation is 16 tons/min. as they improve productivity by enhancing the longwall shearer at the working face and adopting improved equipment, the following matters must be investigated in terms of materials handling:"
"the simulation model consists of several parameters. table 1 shows the tasks performed by the operators of a group. the 33 operators are classified into 12 categories. in addition, table 2 summarizes the list of parameters to be used for executing simulation. based on our investigation into the underground coal mine and interviews with the managers of the mining company, the equipment capabilities were identified. the machine failure rates and operator travel times were estimated based on investigation and the drift map under the ground."
"problem have been proposed [cit] ). in addition, the use of operation process simulation for six sigma projects has been introduced to illustrate the process to define, measure, analyze and improve the current process [cit] ."
"technological advancements have made coal mining more productive than it has ever been. to keep up with technology and to extract coal as efficiently as possible, modern mining personnel must be highly skilled and well-trained in the use of complex, state-of-the-art instruments and equipment. coal is mined by two methods: surface, or open pit, mining and underground, or deep, mining. there are two main methods of underground mining: room-and-pillar and longwall mining. self-advancing, hydraulicallypowered supports temporarily hold up the roof while coal is extracted [cit] . in this study, underground coal mining with the longwall shearer is examined. an overview of the coal mine is shown in figure 1 . the longwall shearer is a sophisticated machine with a rotating drum that moves mechanically back and forth across a wide coal seam. the loosened coal falls on to a pan line that takes the coal to the conveyor belt for removal from the work area. then, the coal is transferred to the storage bin by the conveyor. finally, the coal is transported up to the ground by the main-shaft skip. a survey shows that the longwall-utilization rate is 68 percent out of the scheduled availability, the shift-off is 7 percent, and the down time is 25 percent, as shown in figure 2 (a). furthermore, 61 percent of the longwall-utilization involves the time needed for cutting operations, and 39 percent is operational delay, as shown in figure 2 ("
"at the end of the work shift, they finished their work and returned to ground. each work shift was 6 hours. in mining logic, however, coal was identified using the available data. coal was excavated based on the production capacity of the installed longwall shearers whenever all operators of the group were at their posts and the machines were available. after performing excavation, the coal was transferred by the scraper chain conveyor to the storage bin area underground to await the main-shaft skip. sixteen tons of coal were loaded into one of the main-shaft skips and then transferred to ground. finally, the conveyed coals were unloaded from the main-shaft skip."
"these simulation programs were written in arena [cit] . figure 3 shows the major flow of the operations in the simulation model. the proposed model is composed of two major logical subsystems: worker logic and mining logic. in worker logic, the operators were assigned to specific tasks. at the beginning of the designated work shift, the operators moved in a group to the assigned positions in the underground mining site, and they performed operations for the predetermined working time."
"a simulation model of an operations and materials handling system of an underground coal mine was constructed, and a series of experiments were executed. the relationship between the coal output and the materials handling systems was clarified by performing a series of experiments with a simulation model. the amount of coal output was restricted by the conveyance capacity of the main-shaft skips. the required buffer space of the storage bin was determined by the specifications of the materials handling systems and the operating conditions of the longwall shearers at the working face underground. in addition, the required load weight of the conveyors along the dip and the main entry was identified through the simulation experiments. final decisions should be made by taking safety management into consideration. although it can be quite difficult to describe model operations and conveyance systems precisely for underground coal mines, it was possible to find the bottleneck of the conveyance system and to determine more efficient mining and conveyance methods by performing simulation."
"the conveyance capacity of the main-shaft skips was identified as the bottleneck in the materials handling system. the expected capacity of the conveyed coals by the main-shaft skip in one day (ec) was given by, this study employs a discrete event simulation technique. the primary performance measures considered in this study are as follows:"
"in this study, a simulation model was constructed and used to examine the performance of an underground coal mine. the conveyance system from the longwall mining site to ground facilities comprises scraper chain conveyors, a storage bin, and the main-shaft skips. the relationship between mining speed and the velocity of the main-shaft skips was examined in conjunction with the inventory of the coal storage bin underground."
"the proposed modeling algorithm is applied to training dataset. the implementation of this algorithm is preformed in two stages: 1) identifying the fuzzy model structure by means of fuzzy c-mean clustering, 2) identifying the parameters of fuzzy consequents using least-squares estimation method."
"where h, s, and ν are statistically independent. however, physical radio-frequency (rf) transceivers suffer from impairments that are not accurately captured in this way. informally speaking, such impairments 1) create a mismatch between the intended signal s and what is actually generated and emitted; and 2) distort the received signal during the reception processing. this calls for the inclusion of additional distortion noise sources that are statistically dependent on the signal power and channel gain. detailed distortion models are available for different sources of impairments (e.g., i/q imbalance, hpa non-linearities, and phase-noise); see [cit] for a detailed description of hardware impairments in ofdm systems and related compensation algorithms. however, the combined influence at a given flatfading subcarrier is often well-modeled by a generalized channel model [cit], where the received signal becomes"
"in this section, fcm is employed to define the structure of fuzzy models. this consists of estimating the location of cluster centers and defining the corresponding fuzzy rules of each cluster. the fcm algorithm partitions the dataset into c predefined subsets through optimizing an objective function, which indicates the desirability of each c-partition. the data partitioning into clusters depends on similarity/dissimilarity of each cluster member, which is generally defined by the distance of data points from cluster centers [cit] ."
"for any non-zero realization ofρ 1,ρ 2 . since this happens with probability one, the op in (39) is obtained in this case. the proofs for the cases of fixed gain af relaying and df relaying follow a similar line of reasoning."
"the inherent non-linear and unpredictable behavior of biotechnological processes makes them extremely complex. the conventional methods that are based on mathematical optimization techniques (such as response surface methodology, rsm) consider only local optimization and have some problems like screening for principal parameters and assuming a uni-model objective function [cit] . in recent years, applications of artificial intelligence (ai) to biotechnological processes are growing rapidly. knowledge-based approaches including artificial neural networks (anns), n. chaibakhsh, institute of bioscience, universiti putra malaysia, 43400 upm serdang, selangor, malaysia (e-mail: nchaibakhsh@gmail.com)."
"in addition, the optimal condition for achieving minimum required time (130 min) and minimum amount of enzyme (20 mg) are obtained, where q is defined as follows, the result is presented in table v . the actual yield obtained from experiment is 97.3%. the corresponding error at this condition is less than 3% vi. conclusions neuro-fuzzy modeling of immobilized candida antarctica lipase b-catalyzed synthesis of di-isobutyl adipate ester was successfully performed. the structure and parameters of the model was defined based on the gathered information from experiments using clustering methods. in this approach, the fuzzy c-mean (fcm) clustering technique was employed to define the location of cluster centers and the corresponding fuzzy rules of each cluster, where the parameters of consequent were adjusted by least-squares estimation methods. this helps to reduce the number of necessary fuzzy rules. the optimal number of the cluster centers is captured by performing a trial-and-error procedure. the simulation results show a small deviation between the models predicted values and the experimental data. a genetic algorithm was applied to the developed model for solving the optimization problem. the results obtained from simulation experiments and actual data indicates that the proposed approach can be suitably employed to estimate the optimal conditions in enzymatic esterification processes. the method can be used to deal with the difficulties of developing detailed models for biotechnological processes optimization."
"the high cost of preparing training data is a great problem, where many different experiments are required to be performed. however, employing small datasets for developing fuzzy models may cause over-fitting difficulties. clustering approaches have been proposed to define the structure of fuzzy systems and to reduce the tunable parameters of fuzzy models with minimum losses in the accuracy [cit] ."
"t he use of relay nodes for improving coverage, reliability, and quality-of-service in wireless systems has been a hot research topic over the past decade, both in academia [cit] and in industry [cit] . this is due to the fact that, unlike macro base stations, relays are low-cost nodes that can be easily deployed and, hence, enhance the network agility. the vast majority of works in the context of relaying systems make the standard assumption of ideal transceiver hardware."
"this paper analyzes the current problems in the wisdom medical, expounds the essence of association rules, and proposes an improved fp-growth algorithm, so as to serve the growing volume of medical data. devoted to its application of disease complications, experiment results compare the performance difference between the classical fp-growth algorithm and the improved fp-growth algorithm, and get the support and confidence of disease complications with the improved fp-growth algorithm. this will provide a new decision support for medical diagnosis."
"to obtain an accurate and reliable model, it is necessary that an appropriate dataset for the input variables be determined. it is suggested that all possible combinations of the low-dimensional fuzzy model be considered and evaluated to find the dominant input variables [cit] . here, a set of variables including temperature, time, enzyme amount and substrate (isobutyl alcohol: adipic acid) molar ratio (smr) was chosen as the input parameters based on the previous studies [cit] . the input/output vectors for the model can be presented as follows,"
"in this section, the theoretical results are validated by a set of monte-carlo simulations. furthermore, the concepts of sndr and capacity ceilings and the practical design guidelines of section v are numerically illustrated."
"this subsection derives general expressions for the op that hold true for any distributions of the channel gains ρ 1, ρ 2 . these offer useful tools, which later will allow us to derive closed-form expressions for the cases of nakagami-m and rayleigh fading. note that ρ 1, ρ 2 appear in both numerators and denominators of the sndrs in (13)- (14) and (17) . the following lemma enable us to characterize this structure."
"in the df relaying protocol, the transmitted signal s 2 at the relay should equal the original intended signal s 1 . this is only possible if the relay is able to decode the signal (otherwise the relayed signal is useless); thus, the effective sndr is the minimum of the sndrs between 1) the source and relay; and 2) the relay and destination. we assume that the relay knows h 1 and the destination knows h 2, along with the statistics of the receiver and distortion noises."
"data mining is a type of decision support process, a process of searching for available information hidden in a large amount of data. [cit] . specifically, it is to mining information and data hidden from a large number of random data, but the information and data through mining must be useful [cit] . analyze the information and data after mining and provide intuitive information demonstrations by using visual methods or other friendly ways for decision makers to help decision makers grasp the internal rules and patterns, reduce the probability of error appearance and provide the correct decision support."
"association rules are a very important field in data mining. association rules mining is to find the relationship between a large number of items in the database and find out the interrelation between all the subsets of frequent items or attributes [cit] . this paper proposes to use association rules in the application of disease complications. by association rules mining, the aim is to describe the relationship between healthy data and disease and the relationship between the disease and its potential complications, find out the frequent relationship between different complications of the disease, and provide a more comprehensive decision support for wisdom medical."
"for the last evaluation step, the performance of the model is evaluated with respect to testing data (table ii) . the experimental and predicted values of the esterification yield with respect to testing data are plotted in figure 4 (c), which shows the degree of accuracy of the developed model for predicting the original output at different conditions."
"next, we consider the ergodic capacity of the df relaying channel which is more complicated to analyze than the af relaying channel; the decoding and re-encoding at the relay gives additional constraints and degrees-of-freedom to take into account [cit] . for example, an information symbol must be correctly decoded at the relay before re-encoding, and different symbol lengths and transmit powers can then be allocated to the two hops to account for asymmetric fading/hardware conditions."
"this thesis is one of the achievements of the construction of the innovation base for the employment of college graduates funded by the shanghai municipal education commission, the extra-curricular"
"this function is employed for the global and unconstrained searching. in table iv, three of the best optimum conditions obtained after performing various optimizations through global searching are presented. as can be seen in the table iv, a high amount of enzyme is still required and the reaction time is long."
this appendix contains some useful lemmas. the first lemma derives the cdf of sndr-like expressions and is used to obtain the ops under nakagami-m fading.
"in the next part of the study, the effects of reaction parameters on the degree of esterification were evaluated and the best operating conditions were obtained by conducting a model-based optimization using ga technique. the optimization was performed for global and constrained solutions by searching through the parameter space. the results obtained were compared with actual data from experiments to validate the accuracy of optimized parameters. it should be noted that so far there are few studies on using a combination of individual modeling approaches for enzymatic esterification."
"in the present work, first a neuro-fuzzy model was developed for enzyme-catalyzed esterification process based on experimental data. in order to reduce the number of rules, fuzzy c-means (fcm) clustering algorithm was employed to define the structure of the fuzzy model, where the parameters of fuzzy rules were adjusted by least-squares methods."
"the distortion noise from hardware impairments (after conventional compensation algorithms have been applied) acts as an unknown noise-like interfering signal η i that goes through the same channel h i as the intended signal, thus making (6) fundamentally different from a conventional multiple-access channel, where each user signal experiences independent channel fading."
"the intuition behind this expression is that the information that can be sent from the source to the destination is upper bounded by the minimum of the capacities of the individual channels. a closed-form upper bound, which holds for any channel fading distributions, is derived in the new theorem. theorem 4: the ergodic capacity c df ni (in bits/channel use) with df relaying and non-ideal hardware is upper bounded as"
"this paper revisits classical dual-hop relaying where a source communicates with a destination through a relay; see fig. 1(a) . there is no direct link between the source and the destination (e.g., due to heavy shadowing), but the results herein can be extended to that scenario as well. contrary to most prior works, we consider a generalized system model that accounts for transceiver hardware impairments. this model is described in the following subsections and the block model is shown in fig. 1(b) ."
"lemma 2: let c 1, c 2, c 3 be strictly positive constants and let ρ be a non-negative random variable with cdf f ρ (·). then,"
"a comparison between the developed neuro-fuzzy model and a quadratic response surface (rsm) model, which used for modeling of the enzymatic synthesis of di-isobutyl adipate [cit], indicates superior data fitting and prediction capability of the nf for the testing dataset. the r 2 and aad values for the rsm testing dataset were 0.8928 and 2.5619, respectively. generally, in many simplified models such as rsm, the effects of some parameters on the response are not considered, that leads to an increase in the error value"
"the optimum number of cluster centers can be captured through an iterative procedure. here, some modifications are considered in implementing strategy in order to increase its performance."
"in recent years, the use of enzymes, as ''green'' alternatives to chemical catalysts, in organic synthesis has increased extensively due to several advantages such as mild reaction conditions, high selectivity and specificity, low energy requirement, ease of product isolation, and biocatalyst reusability [cit] . in this study, di-isobutyl adipate has been synthesized through lipase-catalyzed esterification of isobutyl alcohol with adipic acid. adipic acid esters are used in a variety of applications, as solvent, plasticizer, lubricant and also in paint strippers, adhesives, perfumes, cosmetics, coatings, and gear and transmission oils [cit] ."
"in figure 4 (a), the experimental and predicted values of the esterification yield with respect to training data are presented. the obtained results indicate that the responses of the developed model are very close to the actual data."
"note that the op expressions in propositions 1 and 2 allow the straightforward computation of the op for any channel fading distribution, either directly (for df) or by a simple numerical integration (for af). in section iii-b, we particularize these expressions to the cases of nakagami-m and rayleigh fading to obtain closed-form results."
"consider the dual-hop relaying scenario in fig. 1 . let the transmission parameters between the source and the relay have subscript 1 and between relay and destination have subscript 2. using the generalized system model in lemma 1, the received signals at the relay and destination are"
"corollary 2: suppose snr 1, snr 2 grow large with a finite non-zero ratio and consider any independent fading distributions on ρ 1, ρ 2 that are strictly positive (with probability one)."
"where the parameters b 1, b 2, c were defined in proposition 1 for fixed and variable gain relaying. despite the approximative nature of this result, we show numerically in section vi that (36) is an upper bound that is almost as tight as the one in theorem 3. in addition, both expressions are asymptotically exact in the high-snr regime."
optimization is one of the important stages of the engineering design process. it increases the efficiency of the process without increasing the cost and material consumption thus improving the benefit-cost ratio both economically and environmentally [cit] . chemometric methods are very helpful in increasing the performance and reliability of process optimization. they are used to study various parameters using a small number of experiments [cit] .
"physical transceiver hardware introduces impairments that distort the emitted and received signals in any communication system. while the impact of individual hardware impairments (e.g., phase noise, i/q imbalance, and hpa non-linearities) have been well investigated in the corresponding literature, it is the aggregate impact of all hardware impairments and the respective compensation algorithms that determine the practical system performance. motivated by this, we considered a generalized impairment model that has been validated in prior works for single-hop communications and applied it on flat-fading dual-hop relaying, considering both af and df protocols. our analytical and numerical results manifested that the performance of dual-hop relaying is notably affected by these hardware impairments, particularly when high achievable rates are required. closed-form expressions for the exact and asymptotic ops were derived under nakagami-m fading, along with tractable upper bounds and approximations for the ergodic capacities. these expressions effectively characterize the impact of impairments and demonstrate the existence of fundamental sndr and capacity ceilings that cannot be crossed by increasing the signal powers or changing the fading conditions. note that even very small hardware impairments will ultimately limit the performance. these observations also hold true for every individual subcarrier in dual-hop ofdm systems."
"the idea of the fp-growth algorithm. the basic idea of the fp-growth algorithm is to use a tree structure to compress transactions and preserve the relationship between attributes in transactions [cit] . to create the fp-tree, the fp-growth algorithm only needs to complete two scans of the transaction set. this greatly reduces the number of scans and improves operational efficiency."
"proof: for brevity, the proof is given in appendix b. this theorem shows clearly the impact of hardware impairments on the channel capacity: the distortion noise shows up as an interference term that is proportional to the snr. the upper bound will therefore not grow unboundedly with the snr, as would be the case for ideal hardware [cit] . the next section elaborates further on the high-snr regime."
"the combination of anns and fl techniques makes neuro-fuzzy (nf) systems a very flexible option for optimization applications [cit] . the essential behavior of biological processes can be characterized by using these systems based on gathered information from experiments. fuzzy logic-based models have the advantages of high approximation ability and interpretability. they can be developed easier than mathematical models and are able to handle nonlinear systems [cit] . fuzzy classification techniques have already been applied to several biochemical processes such as modeling of the kinetics in enzymatic hydrolysis of penicillin-g [cit], modeling and optimization of fed-batch fermentation processes [cit], fuzzy classification of microbial biomass and enzyme activities for evaluating soil quality [cit], and modeling changes in biomass composition during bioethanol production from lignocellulosic materials [cit] ."
"the parameters of membership functions and fuzzy consequents can be adjusted by anfis method with respect to given input-output training data patterns. the main concern in this regard is that by increasing the number of membership functions, the number of parameters that have to be tuned would increase. in this case, employing a small database for adjusting entire parameters may cause that the model becomes over-fitted. to deal with this problem, it is possible to use clustering techniques in order to reduce the number of fuzzy rules. one of the most commonly used clustering approaches is fuzzy c-means (fcm) [cit] . this technique was originally proposed by dunn [cit] and later extended by bezdek [cit] . in this approach, the system's operating space is partitioned into several operating regions, where each rule would represent a local linear model at the corresponding regime [cit] . in this case, the tsk fuzzy models are able to approximate the nonlinear systems by performing an interpolation of local linear models via their inference mechanism."
"corollary 1: suppose snr 1, snr 2 grow large with a finite non-zero ratio and consider any independent fading distributions on ρ 1, ρ 2 that are strictly positive (with probability one)."
"proof: for a set of independent random variables ξ i with marginal cdfs f ξi (x), the random variable min i (ξ i ) has cdf 1 − i (1 − f ξi (x)). the proof follows by combining this standard property with lemma 2 and (17)- (18) ."
"while η t, η r are distortion noises from impairments in the transmitter and receiver, respectively [cit] . the distortion noises are defined as"
"the fuzzy surfaces for the developed model are presented in figure 5 that shows changes of the model output (ester yield) with respect to the inputs variations. thus, finding the best possible conditions through optimization would be easy. as illustrated in figure 5 (a), at low substrate molar ratio, the yield increases with increase in incubation time up to a certain amount and thereafter decreases due to the hydrolysis of produced ester by accumulated water (by-product of esterification reaction). by increasing the amount of substrates, the reaction yield increases and maximum yield is obtained at maximum time and substrate molar ratio. figure 5 (b) shows the effect of time, enzyme amount and their mutual interaction on the ester synthesis, while temperature and substrate molar ratio are constant at their centre points (0.769 and 0.563 (normalized values), respectively). the yield increases with increase in enzyme amount. in fact, greater amounts of enzyme enhance the formation of the acyl-enzyme complex to produce the ester. this result can also be seen in figure 5 (c) that shows the effect of varying enzyme amount and temperature on the synthesis of adipate ester at constant substrate molar ratio and time (0.563 and 0.536, respectively). at low amount of enzyme, the yield increases with increase in temperature that promotes acceleration in the rate of reaction [cit] . higher reaction temperatures cause enzyme inactivation due to denaturation process that can be compensated by using higher concentrations of enzyme. figure 5(d) shows that the effect of substrate molar ratio on the reaction yield is more significant than temperature. in fact, the presence of larger amounts of substrate increases the probability of substrate and enzyme collision [cit] . increase in the yield with increasing alcohol:acid molar ratio can also be assigned to better solubility of the solid acid in higher amounts of alcohol, and reduction of viscosity of the reaction mixture."
"while the capacity of the af relaying channel with ideal hardware has been well investigated in prior works (see e.g., [cit] and references therein), the case of af relaying with hardware impairments has been scarcely addressed. in the latter case, the channel capacity can be expressed as (32) where the factor 1/2 accounts for the fact that the entire communication occupies two time slots. the ergodic capacity can be computed by numerical integration, using the fact that the pdf of γ af ni can be deduced by differentiating the cdf in theorem 1. however, an exact evaluation of (32) is tedious, if not impossible, to obtain in closed-form."
", where ζ(·) is a continuously decreasing, twice differentiable, and convex function. the convexity is motivated by diminishing returns; that is, highquality hardware is more expensive to improve than lowquality hardware. the following corollary provides insights for hardware design. for df. the alternative solution decreases cost and increases (42), thus the evms must be equal at the optimum."
"observe that the guidelines in corollary 4 are necessary, while the sufficiency only holds asymptotically in the high-snr regime. thus, practical systems should be more conservatively designed to cope with finite snrs and different channel fading conditions."
"in recent years, evolutionary computation approaches, such as gas, has been investigated as an effective approach in optimization of bioprocess engineering problems. genetic algorithms are non-model based optimization methods with the ability to find globally the optimum solutions in complex multidimensional search spaces [cit] . they work based on the concepts of natural selection and evolution of biological species."
"the iteration would be stopped when no further improvement is observed in j(u,v). in general, it is expected that by increasing the number of cluster centers, the accuracy of model would increase. however, in order to avoid model over-fitting and the excessive computational costs, it is recommended that the number of clusters be defined automatically [cit] . many different validity indices are suggested for this regard. a validity function that performed well in practice was proposed by xie and beni [cit] . this function depends on the total variance of geometric distance measure and the separation of the cluster centers, which is defined by,"
"(16) for fixed and variable gain relaying, respectively. comparing the sndrs in (13)- (14) with the ideal hardware case in (16), the mathematical form of the former is more complicated, since the product ρ 1 ρ 2 appears in the denominator. it is, therefore, non-trivial to generalize prior works on af relaying with nakagami-m fading (e.g., [cit] to the general case of non-ideal hardware. this generalization is done in section iii and is a main contribution of this paper."
"just as for af relaying, the sndr expression with df relaying is more complicated in the general case with hardware impairments. this is manifested in (17) by the statistical dependence between numerators and denominators, which is different from the ideal case in (18)."
"for the second scanning, fp-tree is built, marking its root node as \"null\" and creating a header table. the frequent items are sequentially inserted into the fp-tree, and the support count of each tree node is updated. figure 1 shows the flowchart of the classic fp-growth algorithm. idea of improved fp-growth algorithm. due to the huge amount of medical data, the efficiency of classical fp-growth algorithm in the application of the disease complications is low, an improved fp-growth algorithm is used to eliminate the redundancy in the process of mining data, reduce the amount of medical data mining and retain the necessary and useful information to improve the efficiency of operation."
"the first scan obtains the frequency of the current item, removes the items that do not meet the support requirements, and sorts the remaining items. [cit] ."
"where κ i,t, κ i,r are the levels of impairments (in terms of evm) in the transmitter and receiver hardware, respectively. the hardware cost is a decreasing function of the evms, because low-cost hardware has lower quality and thus higher evms. hence, it is of practical interest to find the evm combination that maximizes the performance for a fixed cost."
"in order to increase the performance of the optimization process, the parameters such as population size, number of generation, crossover and mutation rates have to be chosen appropriately. the proposed parameters for optimization are presented in table iii . it is possible to arrange different selections to achieve maximum yield. however, the required enzyme amount should be minimized in order to make the process more economical. the fitness function can simply be defined as the absolute difference between the target output equal to 100% yield and the model output at each sequence or generation, as follows,"
"one is to analyze the support count extracted by fp-growth algorithm and get the final support and confidence, the other is to compare the difference in operational efficiency between the classic fp-growth algorithm and the improved fp-growth algorithm."
"in which, c is the number of membership functions for each input. the consequence of each rule is calculated by multiplying the corresponding rule in its relative degree of fulfillment in layer 4 as follows,"
different molar ratios of adipic acid and isobutyl alcohol were mixed corresponding to the different substrate molar ratios generated by a four-factor-five-level central composite design (ccd). ccd is an efficient statistical design with a hypercube geometry region which is generally used for decreasing the number of experiments while maintaining statistical significance [cit] . five milliliter of hexane was added as solvent. different amounts of lipase were subsequently added. the reaction was performed at different temperatures and for different time periods. the esterification reaction is represented by scheme 1
"it is noted that each cluster would lead to one fuzzy rule; therefore, adopting a limited number of clusters can help to reduce the number of necessary fuzzy rules. however, the accuracy of the developed model is dependent on the number of clusters. as it is illustrated in figure 2, the validation index is evaluated in an iterative process when c changes from two to m -0.5, where fcm is run at each step. the estimated values of validity function, j xb, are 0.1533, 0.1465, 0.1077, 0.1780, 0.1512 and 0.1527 for 2, 3, 4, 5, 6 and 7 clusters, respectively. the obtained results indicate that considering four cluster centers are adequate in order to cover the entire range of the variable changes. in figure 3, the changes of validation function during clustering process, the fuzzy partition for four clusters and the corresponding centroids of each cluster are shown. by obtaining the optimal number of clusters, the positions of cluster centers and corresponding membership function, the structure of fuzzy model can be defined. in table i, the characteristics of the neuro-fuzzy model are presented. fig. 3 . the values of validity index and optimal cluster centers then, it is possible to adjust the parameters of consequent by minimizing the squared error with respect to experimental data. the parameters of the input membership functions and the consequences are presented in appendix a. the developed model is validating by performing a comparison between the responses of the model and experimental data."
"which is inversely proportional to the squares of κ 1, κ 2 . this validates that transceiver hardware impairments dramatically affect the performance of relaying channels and should be taken into account when evaluating relaying systems. the ceiling is, roughly speaking, twice as large for df relaying as for af relaying; 8 this implies that the df protocol can handle practical applications with twice as large sndr constraints without running into a definitive outage state. apart from this, the impact of κ 1 and κ 2 on the sndr ceiling is similar for both relaying protocols, since γ * is a symmetric function of κ 1, κ 2 ."
"corollary 3 shows that it is better to have the same level of impairments at every 9 transceiver chain, than mixing highquality and low-quality transceiver chains. in particular, this tells us that the relay hardware should ideally be of the same quality as the source and destination hardware."
"the parameters a, b 1, b 2 depend on the choice of the af protocol and are given in proposition 1, while d κ"
the model training process was performed via fcm and lse in matlab ® (ver. 7.1) programming environment and simulated by matlab simulink ® (ver. 7.5).
"in recent years, traditional medical models such as lack of medical resources and uneven distribution of medical resources have greatly affected the development of medical services in china. these problems have led to the urgent need for the transformation and reform of the traditional medical mode in china. the arrival of the information age has promoted the development of the medical industry and brought the traditional medical industry into a new era [cit] ."
"we finally derived some useful design guidelines for optimizing the performance of hardware-constrained relaying systems: 1) use the same hardware quality on all transceivers; 2) follow the necessary conditions in corollary 4 to find hardware qualities that can achieve the required system performance; and 3) more sophisticated relaying protocols (e.g., df) are also more robust to hardware impairments."
"this simulation experiment is based on the disease that can lead to complications. the improved fp-growth algorithm is applied to the research of disease complications. it helps to strengthen the attention of complications in disease diagnosis in medical decision making, and be clear of possible disease complications. the simulation experiments in this paper will be divided into two aspects."
"the emergence and application of the concept of \"smart medicine\" has greatly changed the traditional chinese medical model [cit], and alleviated the situation of many patients with fewer medical resources. however, with the rapid increase on the amount of medical data, the problems of data redundancy and inefficient operation efficiency are urgently needed to solve. how to improve the retrieval and processing efficiency of medical data through information technology, how to provide medical staff with effective, accurate and reliable disease data, how to get the probability data of the occurrence of disease complications, these have become the core issues in the process of the development of wisdom medical ."
"in layer 3, the firing strength of each rule is calculated by normalizing corresponding degree of fulfillment with respect to the summation of fulfillment degrees of all rules."
"), as proved by corollary 2. as the capacity ceiling is determined by the level of impairments, it increases when κ 1, κ 2 are decreased. fig. 6 also shows the upper capacity bound from theorem 3 and the simplified capacity approximation from (36) . the former gives a somewhat tighter result, but both are asymptotically exact in the high-snr regime. although the expression (36) was derived in an approximative manner, we observe that it can indeed be considered as an upper bound on the ergodic capacity and, more importantly, is far easier to evaluate."
"by defining the fuzzy membership functions and corresponding fuzzy rules, the main requirement is that the parameters of fuzzy rules be adjusted. for this aim, least-squares estimation (lse) technique is employed for adjusting the parameters of consequent based on experimental data. for each input-output pattern, eq. (6) can be written as,"
"the best operating conditions of the esterification process can be captured by conducting a model-based optimization approach. for this purpose, the main requirement is a model that describes the process behavior, an optimization methodology, and a goal defined by a fitness function. in this regard, the fuzzy model developed in the previous section is used to predict the process output, where genetic algorithm (ga) was chosen for optimization. gas work based on the evolutionary principles, which have many advantages over the conventional optimization methods. in the cases that there are no guidelines to optimize a process with different parameters, gas would be an appropriate tool to find an optimal solution or at least an answer close to the best one. gas are capable to search globally for the optimal solution by checking a vast collection of answers. the optimization was performed by using matlab ® optimization toolbox (ver. 5.0)."
"in this section, a nonlinear model based on neuro-fuzzy modeling techniques is developed for the esterification process. neuro-fuzzy systems have a flexible mathematical structure that combines the learning capability of neural networks and the reasoning ability of a fuzzy rule-based system. a well-known architecture extensively used for nonlinear system identification is adaptive neuro-fuzzy inference system (anfis). the sugeno fuzzy models (also known as tsk fuzzy models) developed by takagi, sugeno and kang are particularly employed as the core of the anfis, due to their capability to describe nonlinear systems behavior by rather small number of parameters [cit] . in the first order tsk model, the consequent is an affine linear function of the input variables, which can be expressed by a set of typical if-then rules as follows [cit],"
"optimization was also performed to find the best possible conditions where the amount of enzyme is at its lowest level, 20 mg (table v) . in this case, q is defined as follows, in order to validate the accuracy of the optimization result, an experiment with the same parameter values was carried out in the laboratory. obtained result shows a good agreement between the predicted and actual data. the actual yield obtained is 97.8% where the error is about 2.2%."
"candida antarctica lipase b immobilized on a macroporous acrylic resin (specific activity 10000 plu/g; water content 1.4%), novozym ® 435, was purchased from novo nordisk a/s (bagsvaerd, denmark)."
"from the performance comparison results shown in figure 3, it can be clearly seen that, in terms of performance, the improved fp-growth algorithm is better than the traditional fp-growth algorithm. when the support is reduced, the classic one has a more significant reduction in runtime, while the improved one has a more stable growth."
"performance analysis. in order to make the experimental data more intuitionistic, the comparison results are represented by a line chart, as shown in figure 3 ."
"the proof follows from lemma 2 and lemma 3 in appendix a, by noting that the end-to-end sndrs for nonideal hardware in (13)- (14) and ideal hardware in (16), are of the form in (46) for different values of a, b 1, b 2, c, d."
"isobutyl alcohol (2-methylpropan-1-ol) and adipic acid were purchased from merck co. (darmstadt, germany). all other chemicals and solvents used in this study were of analytical grade."
"where the amplification factor g ni is selected at the relay to satisfy its power constraint. the source needs no channel knowledge. if the relay has instantaneous knowledge of the fading channel, h 1, it can apply variable gain relaying with"
"in this study we found differences with statistical significance in queries and sessions employed to retrieve information for children and general-purpose content. given that most of our findings are in-line with previous studies of children's information-seeking behavior [cit], we consider that this work represents a valuable and well-suited methodology to study the search behavior of users retrieving information for children. although these case-studies are highly valuable for the understanding of user's behavior, the characterization of users on a large-scale using query logs has several advantages: it is unobtrusive (which makes possible to capture the actual user behavior), safer, repeatable, non-disruptive, non-reactive, inexpensive, a resource of longitudinal data (accessibility over long periods of time) and it allows the design and evaluation of solutions in realistic scenarios. although the main disadvantage of query logs is the presence of noise, this problem can be overcome by using accumulated results from samples of reasonable size."
"similar query reformulations types have been used in previous query log analysis [cit] . although m.r is not a formal query reformulation (since no change is performed on the previous query), we included it in this analysis because this action is commonly use in the search process. table 7 shows the percentages of the query reformulation types found in our data. these percentages were calculated on sessions containing more than one query which correspond to the 83%, 87%, 90% and 55% of the kids, teens, mature teens and whole data sessions, respectively. a salient difference is the average drop of 22.5% of new queries issued in the children sessions compare to the general-purpose sessions. most of this drop is reflected in the greater use of the same queries to explore further results, which accounts on average for 90% of the new query reformulation type drop. small gains in all the other query reformulation types were also found in the teens and mature teens sessions. nonetheless, kids sessions only showed increase in the word removing, word changing and reusing of previous queries. although, it has been shown that children commit spelling mistakes more frequently than older users, we didn't find an increase of the spelling correction reformulation type in the kids sessions. this may be indicate the need to employ more robust methods to detect spelling variations. however, considering that the percentage of spelling corrections is very low in all the session types, this result may be also due to the lack of appropriate spelling corrections tools in the search engine."
"ocean data. the ocean velocity fields used in this work were obtained from the copernicus marine environment monitoring service (cmems) available at http://marine.copernicus.eu/. in particular, we have used the datasets provided by the high resolution global ocean model 34 for most of the mission (the global analysis and forecast product global_analysis_forecast_phy_001_024). the system contains daily 3d global ocean current field data. the horizontal resolution of the model is 1/12° (approximately 8 km) with regular longitude/latitude equirrectangular projection and 50 vertical geopotential levels ranging from 0 to 5500 meters. in particular, to perform the lagrangian path planning simulations, the daily operational velocity fields have been derived from the dataset by averaging the currents over the water column that extends from 0 to 902 meters depth (glider diving depth)."
"deep learning-based approaches have recently emerged in the field of medical image analysis [cit] . this was initiated by the great success of an image recognition competition [cit] . numerous novel technologies [cit] have been reported. for example, u-net-type fully convolutional networks [cit] are some of the most successful networks for medical image segmentation, which might be useful for skeleton segmentation and extraction of hot spots of bone metastatic lesion."
"where v(x, t) is the velocity field of the ocean in the region of interest. in our analysis we will assume that the motion of particles is mainly horizontal. many lcs studies have been performed in a two-dimensiolnal scenario in which is assumed that fluid parcels remain on surfaces of constant density (isopycnals), which are quasi-horizontal [cit] . we will discuss deviations from horizontal motion afterwards."
"sessions allow us to understand the way users accomplish information needs and how they interact with the search engines. we collected three type of metrics to compare the session characteristics of our datasets: session length, duration and query reformulation metrics. session length is the number of query entries issued in the session. query entries can refer to new queries issued by the user or to further results clicked using the same query. this metric is an indicator of search efficiency since a greater amount of query entries suggests that more changes to the queries and document visits are needed to fulfill the search task. session duration is defined as the time in minutes between the last and first query issued in the session and it is an indicator of the complexity of the underlying information need. query reformulation metrics can be use to understand the way users change their queries to reach their information need. additionally, goal length and duration were also analyzed as an attempt to understand how efficiently atomic information needs are accomplished and how these goals are issued in our data sets across sessions. table 6 summarizes the metrics obtained for our data sets. all the results obtained were found statistically significant by using the wilcoxon test in the following paragraphs these results are analyzed. figure 4 shows that general-use sessions are mostly short given that 80% contain less than 5 query entries. on the other hand the length of sessions use to retrieve information for children tend to be longer and its distribution is more table 6 ) suggests that the users of these sessions weren't certain of the relevance of the information found since users have to perform more queries and explore more documents. this result is consistent with bilal's findings [cit] in which children showed nonlinear navigation style when solving research tasks. this search style is characterized by the exploration of several choices before a final relevance judgment is made [cit] . this result can also indicate that the documents retrieved by the search engine are not sufficient to satisfy the user's information need."
"btrflynets for skeleton segmentation and hot spot extraction are different networks but are nonetheless similar. major differences exist in terms of the sizes of input and output images as well as the number of output layers. skeleton segmentation input was a pair of anterior and posterior images of a whole body, and hot spot extraction input was a pair of anterior and posterior patch images. output of anterior skeleton consisted of 13 layers corresponding to 12 bones (skull, cervical vertebrae, thoracic vertebrae, lumbar vertebrae, sacrum, pelvis, ribs, scapula, humerus, femur, sternum and clavicle) and background. outputs of posterior skeleton were 12 layers for ten bones (skull, cervical vertebrae, thoracic vertebrae, lumbar vertebrae, sacrum, pelvis, rib, scapula, humerus and femur) and background. note that one output layer in the posterior was for overlapped regions of the rib and scapula. output for hot spot extraction was consisted of three layers each of which corresponded to a hot spot of bone metastatic lesion, hot spot of non-malignant lesion (e.g., fracture, infection) and others (e.g., physiological renal uptake, radioactive isotope distribution of bladder and background). in addition, sizes of feature maps of the btrflynets were different because of the size differences of input images. in fig. 4, numbers of output layers and the sizes of feature maps are shown in blue for skeleton segmentation and in red for hot spot extraction. furthermore, the btrflynet for hot spot extraction had an additional layer following the input layer enclosed by dotted red squares. this additional layer derives from improvement by residual blocks [cit] which is described later."
"(2) where α and β are set to 1. this equation groups a pair of queries if the edit distance is below a threshold α, the queries contain only β different words (word distance), or the queries are used to click in the same domain.the time restriction specifies that the queries grouped keep the order of submission. the goals are constructed by merging all the pairs of related queries in a transitive fashion. we employed the word and edit distance to find related queries because these features have been shown to be highly effective in the identification of goal boundaries [cit] . the sessions and goals used to satisfy children's information needs are those that visit at least one dmoz domain . three sets of sessions and goals were built using these criteria. table 1 summarizes the characteristics of the set of sessions and goals collected."
"an important future work will involve increasing the size of the training dataset to improve the misclassification of the osteoarthritis case. the effect of dataset size on performance would be an interesting topic. optimising the hyperparameters of deep networks, e.g., number of layers, number of channels (feature maps) and weights in loss functions, is also essential to boost the performance in terms of segmentation and extraction accuracy as well as computational cost. it would be interesting to perform a leave-one-out examination for further performance analysis. developing an anatomically constrained network is also necessary to avoid anatomically the wrong results and to enhance the reliability of the system."
"the major findings of this work are summarized in the movie s1, which runs from the 15th [cit] until the 1st [cit] . this animation overlaps the lagrangian pattern provided by function m (obtained from velocities averaged in the 0-902 m range), with silbo's speed at different points along the glider path. additionally, the movie displays instantaneous averaged velocity fields and the waypoint positions at different times."
"another important characteristic of dmoz is the use of age tags which can be used to distinguish content suitable for kids up to 12 (kids), 15 (teens) and 18 years old (mature teens)"
"long-time, long-distance transoceanic glider path planning is now possible using dynamical systems methodologies and techniques that have been used before in astronautics (e.g. the mariner 10, voyager 1, and rosetta missions [cit] ) to support the flight of low cost space missions based on gravity assisted trajectories. however, the implementation of path planning based on dynamical systems ideas in the oceanic context, presents new challenges. the described dynamical analysis relies on the quality of the velocity fields (geometrical objects such as hyperbolic trajectories and their invariant manifolds depend upon knowledge of the flow field). ocean motions are turbulent in nature, thus obtaining trusted ocean current forecast and analysis remains a challenge. the success of the application of the dynamical systems methodology to the silbo transoceanic mission confirms the high reliability of copernicus global data to accurately represent the ocean state across the north atlantic, since the identified hyperbolic trajectories and their stable and unstable manifolds are indeed present in the ocean and visible to the glider, providing effective navigation routes on which the glider has reached exceptionally high speeds which have no precedent in this context. we expect that the described methodology and tools will contribute to the discovery of new underwater clean-transport pathways for crossing oceans. effective path planning in transoceanic glider missions will open new possibilities for improving the quality and increasing the density of measurements in under-sampled open-ocean deep regions (0-1 km depth), which could be assimilated and incorporated into global operational marine forecasting systems. this, in turn, will positively impact the diagnostic of deep sea observed changes due to global climate change."
"conclusions in the same line have recently been drawn in ofcom's studies [cit] . they reported that gaming is the preferred internet activity for children aged 5-7 and second preferred for children aged 8-11. in particular 37% of the children aged 5-7 (52% for children aged 8-11) were found to use the internet at least once per week for gaming while 19% use it for information purposes (46% for children aged [cit] . on the other hand for users aged 12 -15 informational and social activities are more popular than gaming. in this case 66% use the internet at least once in the week for informational purposes and 48% for gaming. the lower use of informational search in the kids queries compared to the other query sets can be caused by the current lack of specialized ir applications to satisfy children's information needs or to the unsuitability of most of the content in the web for these users. given that these type of users are more familiar with the interaction of multimedia and on-line applications, the design of more interactive tools can highly improved the motivation and success of users searching for children-friendly information."
"the mean time change measures the average time in seconds that users wait to perform a query reformulation. it is measured by calculating the submission time difference in seconds of the reformulated and original query. the wait time to perform query reformulations is longer for most types in the children sessions. this results was expected since children's physical and cognitive skills are less developed than in grown ups. (e.g. type speed and reading skills are lower), moreover children need more time to concrete their information needs and they are less focus during the search [cit] . surprisingly the only reformulation type that doesn't follow this trend is the m.r type, which is the most frequently used in the children sessions. however, we found in the query log cases in which consecutive identical queries are submitted with the same time-stamp, which influences the accuracy of this measure. we found that this occurs in 10% of the m.r actions in the sessions of the entire query log and 30% in m.r actions in the kids sessions."
"transoceanic slocum glider missions are relatively recent and until now there has been only a few of them. the first successful north east transatlantic mission was achieved by the scarlet knight ru27 [cit] 1 . [cit] by the ru17 glider attempt, which was unfortunately lost just off the coast of the azores. other subsequent missions have been performed by cook, drake, silbo and ru29 gliders 2 . missions have been an adventurous path to learning about a completely unexplored terrain and to gain information about many different aspects of the missions, ranging from glider flight dynamics, battery consumption, resets, bathymetry risks, aborts, piloting error, physical and biological impediments (such as barnacles adhesion and fouling) and their effects on long term navigation, etc. in the silbo mission described in this article, navigation has been in the 0-1000 [cit] scarlet knight ru 27 flew between 0-200 meters depth range and this allowed taking a maximum advantage of the gulf stream speed, aligned with the direction of the voyage. in other missions, with gliders drake and cook, deep flying has been shown to be an effective way to fight unfavorable currents since at large depths currents are weaker. since the first missions, in order to gain insights into the ocean landscape, different approaches have been considered. the first missions were flown using sea surface temperature (sst) fields as a primary reference for mesoscale flow features, and waypoints were programmed according to the displayed structures. sst was chosen for its global availability, its relatively fast update cycle from avhrr satellite data, and its ability to resolve many surface flow features. alternatives to this product have been currents derived from the sea level anomaly, 3d current fields from models, etc. in this letter, we demonstrate the success and promise of a new approach to path planning for future auv crossing missions that was implemented for silbo. this is the dynamical systems approach to transport that involves using the space-time structure of the ocean current field in a way that optimizes the propulsion of the glider in a manner that promotes sustainable missions. more specifically, the methodology proposed in this mission for supporting the waypoint selection uses lagrangian coherent structures (lcs). this is not disconnected from velocity fields, but based on them since lcs provide a time dependent lagrangian pattern (i.e. based on fluid particle trajectories) which at each day encompasses information from the velocity field in past and future days, and therefore is suitable for advising about lagrangian paths, such as those followed by gliders. eulerian velocity fields or instantaneous temperature fields used in previous missions are more rudimentary in this regard. the idea of exploiting natural dynamics for vehicle transport has been previously used in space mission design. the work is similar in spirit to our work in the ocean in the sense that the gravitational field of the planetary system is used to determine a desired mission trajectory for a spacecraft with low thrust capabilities 14, 15 . these ideas have also been previously proposed in oceanic setting, for planning glider routes through ocean currents 16, but they have not been applied to transoceanic missions in the way that we have done for the silbo mission."
"the goal length is an indicator of the efficiency to express atomic information needs. on the other hand, since goals were grouped without considering the session limits (without time window restrictions), long goal durations indicate the presence of informations needs that involve long term planning, as planning a trip, or information needs that are recurrent, for example checking the news. figure 6 shows the length distribution of the goals extracted from the data sets. contrary to the session length behavior, the difference in the length between the datasets is small (4.8 in the whole data set vs 5.8 in the kids set on average).this result indicates that children sessions contain a greater formulation of atomic needs. figure 7 shows that the goal duration of general-purpose goals are significantly longer than in the children goals. this results is interesting because it suggests that children information needs are less frequently conformed by atomic informations needs split in long periods of times. this behavior is consistent with previous experiments in which children are found to be less focus during the search process [cit], which diminishes their ability to formulate information needs in longer time periods."
"we found that informational queries are preferred in the whole, teens and mature teens data set over transactional and navigational queries. previous studies have also found this behavior on large query logs. for instance broder [cit] reported on a random sample of 1000 queries from the altavista log that 48% of the queries were informational, 30% transactional and 20% navigational, which are comparable to the percentages obtained in our sample."
"multi-atlas figure 5 presents the typical results of skeleton segmentation, and fig. 6 shows dice scores for all test cases. note that the multi-atlas-based approach [cit] employed b-splinebased non-rigid registration of 164 atlases from the training dataset and only anterior images were segmented because of high computational cost. figure 7 shows the typical extraction results of hot spots of bone metastatic lesions when the sensitivity per hot spot of bone metastatic lesion was 0.9. table 1 presents the number of false positive pixels, false positive regions and misclassified pixels by u-net, btrflynet and resbtrflynet. figure 8 compares automatically measured bsi with true bsi, which was computed using true regions of bones and hot spots of bone metastatic lesions."
"besides corroborating previous case-studies results on a large scale, we were also able to characterize the tasks children prefer in the web (by the classification of query intent), the preferred topics/interest of these users (by the cue words analysis) and their query reformulation behavior."
"patch images that contained one or more pixels in the human mask were forwarded to the trained btrflynet for hot spot extraction. finally, patch images with the extracted hot spots were integrated into an output image whose size was equal to that of the input image."
"this study proposed a deep learning-based image interpretation system for automated bsi measurements from a whole-body bone scintigram, in which btrflynets were used to segment the skeleton and extract hot spots of bone metastatic lesions. we conducted threefold cross-validation using 246 bone scintigrams of prostate cancer to evaluate the performance of the system. the experimental results revealed that the best performance was achieved by a combination of btrflynet with dsv for skeleton segmentation and btrflynet with residual blocks, and the number of misclassified pixels for which was minimum. the computational time of both processes for a case was 112.0 s., and automatically measured bsi showed high correlation (0.9337) with the true bsi, both of which is deemed clinically acceptable and reliable."
"given these observations the queries were classified into question and phrasal queries. question queries (e.g. what is the leprechaun) are defined as the queries that contain at least one of the following tokens: what, where, why, who, when, whose, will, am, are, is, have, has, whether, which and whom [cit] . none of the queries in the query log contained the question mark character (possible due to query normalization before the release of this query log). phrasal queries (e.g. words that start with the n letter ) refers to the presence of noun and/or verb phrases in the queries. the frequency of these type of queries are summarized in table 2. in this table is shown that question queries are more frequent in the children queries (queries use to retrieve information for children) than in the general purpose query set, which is in line with druin's observations [cit] .these findings suggest that query reduction and query segmentation techniques can be particularly beneficial for children content queries since it has been shown that longer queries are less efficient [cit] . similarly query reformulation techniques based on morphological and syntactical features of the queries can improve the search process by mapping phrases to concepts and keywords [cit] ."
"query logs represent valuable sources of information to understand the search process and to improve search engine systems. for instance, query logs have been widely exploited in the literature to study user's behavior/interaction with ir systems, to classify queries [cit], to infer search intent [cit], to generate user profiles [cit], to produce query suggestions [cit], among others."
the better understanding of users retrieving information for children on a large-scale achieved in this paper allow us to discuss several ways to improve the search experience of these users. we discuss improvements on two ir dimensions: query assistance and aggregated search.
"silbo's control mechanisms allow the glider to control its heading so as to pass through manually defined waypoints (wps) with or without compensating for local depth average current. our goal is to extract information from the oceanic currents, in particular, about the natural dynamics of particle trajectories advected by ocean currents, since we expect that this knowledge will inform the choice of wps. in the ocean, particles follow trajectories x(t) that evolve according to the dynamical system:"
"the proposed btrflynet-based skeleton segmentation took 16 s. for the case using 24 threads. by contrast, the cost of the multi-atlas-based method for an anterior image was over 300 times greater than that of btrflynet. the most timeconsuming step was non-rigid registration, which took 3420 s. on average, even when ten registration processes ran in parallel."
2 . this feature allow us to compare the characteristics of the queries and sessions for these three target groups. in section 2 we introduce the most relevant related work on query log analysis and children search behavior. in section 3 we describe the data employed in this study and the methodology followed for its analysis. section 4 present the results obtained for the children queries and sessions. in section 5 we discuss some directions and ideas of how to improve information retrieval for children based on our findings and children search behavior. finally in section 6 conclusions are drawn and directions for future work are stated.
"note that although it is not possible to establish if these queries were performed by children, we are still able to study the characteristics of the queries and sessions when the underlying information need is related to content for children. moreover, it has recently been reported (from a survey of 2131 families in uk) [cit] that children aged 5-15 are frequently trusted to search the web on their own (68% and 84% for children aged 5-7 and 8-15, respectively), which suggests that there is a high chance that queries retrieving information for children are actually submitted by children."
"the experiment was approved by the ethics committee at osaka city university (approval no. 3831) and tokyo university of agriculture and technology (approval no. [cit] . the total number of bone scintigrams was 246, derived from japanese males with prostate cancer whose ages were from 52 to 95 (average: 72.8, standard deviation: 6.96). the dataset was divided into three groups to conduct threefold cross-validation. we also prepared a validation dataset to determine an optimal training iteration to avoid overtraining. in summary, 164 scans were for training, 41 for validation and 41 for testing. because the validation and testing datasets were switched in onefold, we obtained test results from 246 total scans. the number of anterior and"
sessions are constructed by grouping contiguous queries submitted with a time difference smaller than t θ and that are from the same user. a formal definition of session is shown in equation 1.
"this study presents a system consisting of skeleton segmentation and extraction of hot spots of bone metastatic lesion followed by bsi measurement. we employed a deep learning-based approach to achieve high accuracy in skeleton segmentation and hot spot extraction. one of the reasons for the low accuracy of skeleton segmentation and hot spot extraction in existing studies [6, 8, 14, [cit] may be that anterior and posterior images have been independently processed, thus resulting in the inconsistent results. we used a butterfly-type network (btrflynet) [cit] which fuses two u-nets into a single network which can process anterior and posterior images simultaneously. because a deep and complicated network might be problematic for the training process, we introduced deep supervision (dsv) [cit] and residual learning [cit], both of which are effective at avoiding gradients vanishing or exploding during the training of a deep network. we conducted the experiment using 246 cases of prostate cancer and demonstrated the effectiveness of the proposed system by comparing it with conventional approaches, namely multi-atlas-based skeleton segmentation and u-net-based hot spot extraction."
"the analysis of the movie s1 confirms that the exploitation of natural dynamics efficiently optimizes glider transport. alternatively, if the glider is forced to fly against this natural dynamic, speeds of the glider are notoriously small. we describe two events in the movie supporting the first assertion, and two events supporting the second one. table 1 summarizes these findings. between the 14th-17th of june and the 18th-23rd [cit] two successive events (events 1 and 2 in table 1 ) take place which demonstrate the enhancement of glider speed due to the presence, in an appropriate configuration, of geometrical dynamical objects described as hyperbolic trajectories and their stable and unstable invariant manifolds. in these two events the glider shows high performance (high velocities) while it approaches to a hyperbolic trajectory (ht) through its stable manifold (sm) and when it leaves its neighborhood through the unstable direction (um). in the vicinity of the ht the glider reduces its speed. figure 5 supports this description by specifically selecting areas of the movie s1 at days 19th, 20th and 23rd [cit] which encompass the glider and the hyperbolic point. in particular, fig. 5a shows the glider position and its speed while approaching a hyperbolic point along its stable manifold on the 19th [cit] . figure 5b confirms the speed reduction at the closest position to the hyperbolic point on the 20th [cit] . figure 5c shows the glider moving away from the hyperbolic point through the unstable manifold on the 23rd [cit] . remarkably, this day the glider speed achieves a record velocity (1.04 m/s), which is unprecedented for this type of missions, since typical operational velocities for this type of gliders are below 0.5 m/s. these findings confirm that stable manifolds (sm) are optimal paths towards the ht, i.e. the glider approaches to the ht very efficiently along this direction. on the other hand, unstable manifolds (um) are the optimal path for moving away from the ht. in the neighborhood of the ht the glider slows down. consequently, an optimal path to navigate is to follow the dynamical sequence sm-ht-um. to avoid excessive slow down near the ht, it is appropriate that before approaching it to closely, the wp placed there is moved to a new ht. this ht must be selected in such as a way that its sm is aligned with the um of the previous ht, so that the new wps force the glider to leave the neighborhood of the previous ht along the direction of the um. an appropriate navigation sequence thus would concatenate: sm-ht-um/sm-ht-um. this results in a wave-shape path with the glider moving alternatively from stable to unstable manifolds, as visible from fig. 3 . the video also shows that silbo described this waving-path when it speeded up to 1 m/s and flied out the ne american waters heading to the open north atlantic waters."
"additionally, cue words are well-suited to be used in the information integration phase of the aggregated search paradigm. in this phase content from different verticals is processed, reorganized and presented to the user. current ir systems only provide very simple methods to aggregate results from the verticals. ir systems could parse the content of the web results and aggregate only the content type suggested by the cue words of the query, this would highly reduce the cognitive load of users to find relevant information."
cue words provide information about the characteristics of the content searched by users and their identification have been proved useful to aid search through query expansion techniques [cit] . our motivation to identify cue words is to identify the most common characteristics and content type of the information searched in the query sets. the identification of cue words is based on a contextual model [cit] which is a defined in equation 3 .
"the internet today is widely used by children for information, communication and entertainment purposes. [cit] the internet access of children aged 2-17 increased from 46% to 78% in the united states [cit] . similarly, the london school of economics reported that 75% of the 9-19 years old people in the uk have access to the internet at home and 98% [cit] . undoubtedly, the access and use of the internet by children will keep increasing in these and other regions of the world in the coming years. unfortunately, most of the current information permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. retrieval (ir) systems are designed for adults and previous studies have shown that children's information needs [cit], search approaches [cit] and cognitive skills [cit] differ from those of adults. thus, there is an increasing need for research aimed at understanding children's information needs and to provide ir systems that suit the characteristics of content for children."
"the cost of the best combination of networks including pre-and post-processes (e.g., spatial standardisation) was 112.0 s. per case, which seems acceptable for clinical use."
"in this section we explore the characteristics of the query entries. we considered the query length (which is measured by the number of words per query), the type of queries (question queries, phrasal queries, query intent), cue words, the rank position of the domains clicked, the query frequency distribution and the distribution of users across the datasets."
"queries were also analyzed using broder [cit] classification which captures three types of user intent: informational, navigational and transactional queries. informational queries are used to address an information need by locating content relevant to the topic of interest (e.g. areas in africa giraffes live in). navigational queries are used to locate a specific website, which can be the main website of an organization or a hub site (i.e. bobthebuilder.com). transactional queries are used to locate a website with the aim of obtaining a product. the product may refer to an item to be purchase, an application to be executed on-line (i.e. alphabet coloring pages) or a multimedia resource to be downloaded."
"interestingly, this trend was not observed for the kids queries in which transactional queries are preferred (increase of 20% in respect to the average user of the query log). we found that transactional queries are mainly used in the kids and teens queries to interact with web applications (e.g. flash/java games, academic quizzes) or to obtain free on-line resources (e.g. poems, songs lyrics, coloring pages)."
users constantly modify their queries in an attempt to get better results from the search engine. the analysis of these query refinements allow us to have a better understanding of the way user's interact with the search engine and the search strategies employed to satisfy their information needs. in this paper we analyze the following types of query reformulations:
"we also remark that during the mission, typically, silbo navigated with the current correction mode off. thus it was sensitive to strong currents as it was not forced to approach the wps following a straight line. there exist days, visible from the movie, in which currents deviate the glider from a rectilinear path, at stages in which the glider is still far from the wp, however these deviations are not an obstacle to approaching the wps."
"the limitations of the proposed system must be mentioned. figure 11 shows the case with the maximum error with the bsi measurement (red arrow in fig. 8 ). although the skeleton was recognised correctly, hot spots by osteoarthritis in thoracic and lumbar vertebrae were misclassified as hot spots of bone metastatic lesions. one possible reason for this failure is the limited amount of training data for osteoarthritis. training using a large dataset with osteoarthritis cases remains an important future study."
"query length is an indicator of the complexity of the query and the difficulty of the user to express information needs using keywords. the average query length found for the kids, teens and mature teens datasets were 3.8, 3.4 and 3.2, respectively. these values differ from the mean of the entire query log, which is 2.5 words per query, with statistical significance using the wilcoxon signed-rank test at the 95% confidence level. the average query length of the entire data is also in line with the average length reported for other large scale query logs [cit] ."
"first, the posterior image was flipped horizontally and aligned to the anterior image for simultaneous segmentation. second, spatial standardisation consisting of rotation, scaling and translation was applied to both the anterior and posterior images to ensure the body axis was parallel to a vertical axis of the image. [cit] mm. third, grey-scale normalisation was performed for both images independently using the following equation."
"the red circles in fig. 5 show typical errors in segmentation by the multi-atlas-based method because of atypical shapes or directions of the skull, right humerus and sternum. figure 6 suggests that the multi-atlas-based method was inferior to btrflynet-based approaches for all organs, and the differences were statistically significant. an example of the improvement gained by dsv is indicated by the yellow circle in fig. 5 . figure 6 indicates that btrflynet with dsv was superior to the naïve btrflynet for nine out of 12 bones in an anterior image and three out of ten bones in a posterior image. statistical differences were fig. 8 relationship between automatically measured bsi and true bsi observed for three bones each in anterior and posterior images. by contrast, the rib in a posterior image was the only bone for which the naïve btrflynet was statistically superior. therefore, we concluded that btrflynet with dsv was the best in our experiment. the main reason may have been lower loss during training. figure 9 shows transitions of training losses at the output layers, where the red line of the btrflynet with dsv was lower than the blue line of the naïve btrflynet, which suggests that the dsv was effective at reducing loss in the training data."
"in this paper we explore the aol query log [cit] to compare the queries and sessions use to retrieve information for children, and queries and sessions use to retrieve general purpose information. the aim of this analysis is twofold: (i) to identify differences in the query space, user sessions and user behavior of these two types of queries. (ii) to enhance this query log by identifying children queries, sessions and actions. this resource will be used to study other important problems in ir for children as query assistance and query classification."
"the same reformulation types were found to have the lowest ratio values in the children sessions which shows that these reformulation types are also preferred when users are not satisfied with their current queries. nonetheless, a different behavior was found in the children sessions since all the ratio values are higher than in the general purpose sessions which suggests that query reformulations are more frequently used as follow-up of current results obtained. it is important to mention that the difference in the ratios are mostly caused by the significant higher use of the pattern click − click in the children queries. this finding is in-line with [cit] studies on children search behavior stating that children need to explore further options before making a final relevance decision. this result also indicates that it is harder for these users to identify the relevancy of the results from the snippet and title presented in the result list, making necessary the exploration of the results in greater detail. this result is also aligned with our previous findings on the children queries rank data, and the children session characteristics since a greater amount of results clicked explains the longer length and duration of children sessions."
"we analyzed the proportion of users that submit queries in more than one of the data sets. the results are summarized in table 5 . this table shows that users that retrieve information for children rarely submit queries to extract information from the teens and mature teens data sets. analogous results were found for the users of the teens and mature teens datasets. nonetheless, this result should be taken with caution since the user identification in the aol query log was performed automatically, which implies that there is no guarantee that users of the query log correspond to actual users. 3 variations of the same domain were removed from this list (e.g nickjr and nickjr.dom)."
"the aol query log employed contains approximately 36 [cit] . currently, this is the largest and most up-to-date freely accessible query log in the web each entry contains an anonymous user id, time of submission, rank position and domain of the url clicked. it is important to mention that there is controversy about the usage of this query log in the research community given the privacy issues that arose by the identification of actual users in the press [cit] . nonetheless, all the results presented in this paper are obtained from accumulated counts and no actual user identification is performed. the identification of the queries employed to retrieve content for children was performed by matching the entries listed in the dmoz kids & teens directory, which contained 45.635 entries, with the domains clicked in the entries of the query log. given that the query log does not include the entire url visited, matches are restricted to the cases in which only the domain is listed as dmoz entry. three data sets of query log entries were constructed by employing the matching procedure described above on the dmoz entries tagged for kids, teens and mature teens."
"typical conventional glider path planning methodologies for determining optimal paths have demonstrated their effectiveness in regional environments. among these methodologies are linear programming, probabilistic sampling, potential fields or genetic algorithms and artificial intelligence methods such as a * (see [cit] ). some of these methods require the use of regional ocean models (roms) forecast datasets with high space-time resolution (1/32 o, hourly). nowadays in the open ocean only low space-time resolution models are available (1/12 o, daily) and therefore these techniques are not implementable in transoceanic glider crossing missions. in this regard if global models in the future would increase their resolution and accuracy it could be that regional methods provide efficient solutions also in these missions, but this is not the case for the current state of the art. some attempts in this direction are references 12,13 that describe the path planning a* technique used at the end of the first atlantic glider crossing, once it approached the iberian coast. in particular the method was implemented with the rom eseoo iberian domain data (1/32°, hourly, +72 h). additional regional path planning requirements, as for instance the demand of environmental obstacles by the theta algorithm, are not available at global scale, or the need of a rather stable environment just subjected to small perturbations, a must for incremental methods such as d* and phi*, does not work in a highly dynamic and changing environment like that found in the open ocean. long-term long-distance path planning missions require guiding techniques that are useful for highly dynamic open-sea areas, and thus they must be based on robust and fundamental ocean features."
"interestingly, these results confirmed previous studies in the field of human-computer interaction (hci). [cit] stated that kids of age 8 to 12 tend to formulate longer queries as a consequence of their preference to write queries using natural language constructions instead of keywords, specially when the information need requires multiple phases to be solved. it has also been found that children tend to express complex information needs by directly typing the question they have [cit] ."
"in the hot spot extraction experiments with multiple threads, the naïve btrflynet was the fastest because it shared the deepest layers for anterior and posterior images as compared with u-net. resbtrflynet was 1.5 times longer than the naïve btrflynet because of the high computational cost of residual blocks. however, the difference was not considerable."
"in order to make a correct interpretation of the stable and unstable directions of an ht the instantaneous depth-averaged current field must be superimposed onto the m field so that the direction of the manifolds is revealed. it is not possible to distinguish these directions just from the function m template. if a glider were to approach a ht along an unstable manifold, it would slow down since it would be navigating in a counter-current flow. two events of this kind are described next. table 1 . detailed description of five events with special configurations that propel or slow down glider motion. each event is described by the day, the glider speed and its position with respect to the dynamical objects: hyperbolic trajectories (ht) and their stable (sm) and unstable (um) manifolds. sequences sm-ht-um provide high speed along manifolds and reductions in the vicinity of ht. configurations such as ht-sm or um-ht force the glider to move against the natural dynamics resulting in a slowing down of the motion along manifolds. events 3 and 4 in table 1 correspond to a period in which the glider was flown against the current to test its propulsion mechanism. in these events the glider navigates towards the ht along its unstable manifold or leaves the ht along its stable manifold. therefore the glider follows inverse paths to those described above as it moves along manifolds that do not support its displacement. in this case the glider shows extremely low speeds when it is at positions along the manifolds, and speeds slightly increases in the neighborhood of the ht."
residual blocks [cit] are used instead of convolutions and deconvolutions in the btrflynet for hot spot extraction. the improved btrflynet is called resbtrflynet in this study.
"where i in is an input grey value, i x% is the upper x th percentile and is the golden ratio. central regions of the images (fig. 2) were then forwarded to the trained btrflynet. inverse transformation of the spatial standardisation and the alignment of the posterior image were performed to transfer the segmentation labels to the input images."
"is an indicator of the effectiveness of the query reformulations. the most effective query reformulations were w.r, w.c and m.r and the least effective were wa, pq and sc in the children and general-purpose sessions. however the ratio values in the children sessions were always higher given the greater number of clicks reported in these sessions. the higher rate value of the p.q query reformulation in the children sessions can be explained by the fact that children tend to repeat the same search even if it does not return new results [cit] . this occurs given their perception of the web as a source containing all the information they need. this behavior is also in-line with their tendency to loop searches and hyperlinks, as it is reported by bilal [cit] ."
"the use of longer queries to retrieve children-friendly content can be one of the causes of the lower retrieval performance found for theses queries since it has been shown that longer queries have poorer performance in current search engines [cit] . for this reason we consider that refining long queries is highly beneficial for these users. [cit] present a method to rewrite shorter queries by segmenting the original query and ranking the segments. their method obtained 8% map improvement on two trec collections (trec123 [cit] ). similarly, [cit] employ a method to extract key concepts from the queries based on a classifier trained with query-dependant and corpus features. these keywords are used to rewrite shorter queries which are also proven to have better retrieval performance on a trec collection."
"movie s2 and s3 represent, respectively, lagrangian structures for velocities averaged in the range 0-453 m and at the 453 m depth layer. these movies support similar conclusions to the ones obtained from s1, thus confirming assumptions about the robustness of the lagrangian structures and their ability to provide a fundamental ocean landscape for navigation in spite of uncertainties."
"in the training process, he's initialisation [cit] was used to initialise all weights of the networks. the loss functions were minimised using adaptive moment estimation (adam) [cit] . the detailed parameters are given as follows."
"the mean rank change represents the rank position difference of the clicked domains in the reformulated and original query. higher rank position values correspond to the results located in the bottom of the result list and lower values to the results on the top of the list. thus, higher mean rank change indicates that the query reformulation is less successful since users click in average on results below the original queries. we found that all the query reformulations are successful in children and general-purpose sessions, except for m.r and p.q. this result is logical since these actions do not involve query changes and users generally click on top results first. it is interesting to note that although query reformulation is significantly less used in the children sessions (given the high used of the action m.r ), these can be highly helpful since the mean rank changed is lower for the children sessions."
"for the analysis we rely on the kids and teens section of the dmoz directory 1 to identify the queries employed to retrieve content for children. the aim of this dmoz section is to provide child friendly and safe content to cover the specific needs of people under the age of 18. we consider that using this directory to identify children-content queries is reasonable and realistic enough given that the content of the directory is frequently regulated and maintained by senior editorial staff, which guarantees that websites with harmful or unsuitable content for children are excluded."
in the next section we perform experiments on the proposed heuristics to determine which one is the most effective in producing an encoder function approaching max-equivocation.
"the incrow algorithm scans c's columns in increasing order of probability, and each column from top to bottom. when it finds a conflict c, it considers the best swap c[c ↔ c ] of c with elements on the same row and the best transformation of c with a different symbol"
"thus we shall consider max-equivocation where the message distribution is non-uniform. we assume the key distribution is uniform for simplicity, although no results implicitly rely upon this assumption. from corollary 1 and lemma 3 we have that the max-equivocation is ensured when the entropy of the ciphertext is minimized. unfortunately max-equivocation (and thus perfect secrecy) cannot be achieved in all scenarios."
"in summary, our multi-modal deep representation learning framework harvest features that are highly predictive of protein function. it captures both sequential protein raw information with the topological structure to improve the ppi prediction accuracy and multi-class classification accuracy given the complex, non-linear interaction networks ppi network. we apply our methods on both dip and hprd datasets. after applying the cbow model based on generated metapaths, our model is able to take into account the graph topological information into account. we use various mainstream metrics to assess the performance over the new released dip_20170205 full dataset including eight species and hprd datasets. through extensive comparisons with both traditional machine learning methods and state of the art deep learning methods, we prove that our method outperforms most of them over the same datasets."
"to obtain the deep features from the 468 dimensional vectors, we utilize the stacked auto encoder(sae) framework as shown in fig. 1 and in eq. 6."
"a encodes the message m with key k using an appropriate encoder function enc, obtaining the ciphertext c. a sends c to b via a public channel, where c is also read by the eavesdropper e. knowing the key k, b decodes the message m using the inverse of the encoding function, dec. not knowing the key k, e tries to infer m from their knowledge of enc and c and using unlimited computational power."
"the extensive algorithm considers all conflicts in c and for each one the best swap and the best transformation, and then it performs the swap or transformation operation that is the best among all possible conflict resolutions (ties in favor of swaps). it repeats this step until there are no conflicts left. the pseudocode of extensive is described in algorithm 2."
"the first formal results on unconditional security were published by shannon [cit] using results from the formal theory of communication that he had just invented [cit] . shannon considers the transmission of an encrypted message on a public channel between two agents a and b that share a cryptographic key. shannon investigates how long the key has to be to transmit the message with perfect secrecy, meaning that a third agent intercepting the encrypted message obtains no information about the original message. shannon proves that to achieve perfect secrecy the key must be as long as the message and can be used only once, and that the one-time pad algorithm achieves this under uniform message distribution. this means that, to transmit each n-bit long message, the sender and receiver have to have previously agreed on a fresh n-bit long key, which is often impractical. perfect secrecy is proven to be unachievable with a shared key shorter than the message to be transmitted."
"we have compared four heuristics to efficiently derive an effective encoder function. we established that the decrow algorithm works better than the others under our experimental parameters, consistently providing encoder functions that are only a few percentage points worst than the theoretical optimum and in less time than the other heuristics. finally, we have shown that the effectiveness of the encoder functions constructed by our heuristics improves with the size of the message space, suggesting that they could be used on real instances of the problem."
"consider the information available to agent e. agent e knows the encoder function enc, and ciphertext c, but not the message m or key k. we will use random variables to model e's knowledge about the communication before and after e intercepts the ciphertext c. let m (resp. k, c) be a random variable on the support set m (resp. k, c) representing the a priori value of the message m (resp. key k, ciphertext c) according to e, i.e. the value before the ciphertext is intercepted."
"when the distribution on the larger side of the rectangle is not uniform, it is possible that the optimal encoder function is not a latin rectangle. due to the inherent complexity of finding an encoder function that minimizes the entropy of the ciphertext, computing an optimal encoder function for a given key and message distribution is not trivial. in the next section we experimentally compare different heuristics."
"preserving privacy of private communication is a fundamental concern of computer science. modern efforts can be divided into two categories: computational and unconditional security. computational security of the privacy of a message depends on the assumed superpolynomial lower bound on complexity of some particular functions, e.g. factorization. such lower-bound results are currently unproven, and may be weakened by technological progress in engineering, algorithmic theory, and quantum computing. unconditional security is based on information-theoretic reasoning and proven independently of computational hardness. for this reason, unconditional security results are more general and solid than computational security results. however, the strict requirements to obtain unconditionally-secure cryptographic algorithms can make them generally impractical."
even languages that have a fixed or bounded message length (such that redundancy does not ensure a unique message from a given ciphertext) do not in general have uniform distributions over the messages [cit] .
the code to perform these test is written in java 1.8 and run on linux 3.13 64-bit kernel on an intel core i7-3720qm 2.60ghz cpu with 8gb of ram. the code used to run these experiments is available upon request.
"duce large amounts of data for constructing primary protein databases. these databases provide primary and rich sources for developing molecular and functional networks. nevertheless, these genome-based techniques demand expensive wet-lab investment and exhaustive lab work. also, because of the equipment biases in the experimental environment, the results generated by these genome-based methods are subjected to inevitable inaccuracy. moreover, compared with the significant amount of protein sequence data, the functional units that have been discovered are comparatively restricted. previously, traditional machine learning algorithms such as decision trees (dt), naive bayes (nb) and nearest neighbor (nn) [cit] have been utilized efficiently in lots of data mining tasks. yet, these traditional machine learning techniques lack the capacity of discovering hidden associations and extracting discriminant features from the input complex data. lately, accompanied with the advancement of ai techniques, deep learning methodologies [cit] extracting non-linear and high dimensional features from the protein sequences [cit] have emerged as a new tendency. these deep learning techniques and frameworks have been recently applied in tremendous biomedical research fields, biological network analysis, and medical image examination. however, since natural and real-world data distributions are highly complex and multimodal, it is essential to incorporate different modalities and patterns from the data to attain satisfactory performance. additionally, discovering biological pattern from the graph topology of these protein networks is fundamental in comprehending the functions of the cells and their constitutional proteins. when applying deep learning techniques to biological network analysis, these modalities include topological similarities such as 1st-order similarity, 2nd-order similarity, and homology features extracted from protein sequences. additionally, next-generation sequencing technologies also generate large amounts of dna/rna sequences which are then translated into protein peptides in the form of stacked amino acid residues. these protein sequences consist of fundamental molecules which perform biological functions for various species [cit] . thus, the functionality of a protein is encoded in the amino acid residues. to recognize the protein functionalities, researchers categorize proteins into various families such that proteins within the same family share similar functions or become the parts on the same pathway. in this paper, we propose a advanced multi-modal deep representation learning framework preserving different modalities to harvest both protein sequence similarity and topological proximity. this framework leverages both relational and physicochemical information from proteins and successfully integrates them using a late feature fusion technique. these concatenated features are provided to the interaction identifier and protein family classifier for the training and testing tasks."
"since perfect secrecy cannot be achieved in this scenario, we define the maximum achievable security and how to attain it. secrecy, as defined by shannon, is a measure based on mutual information, and measures how much information is leaked by the communication, but not how hard it is for the attacker to decrypt the message given this leaked information. instead of measuring secrecy, we measure the message equivocation of the encryption, measured as the conditional entropy of the message given the ciphertext. equivocation, introduced by shannon, measures how difficult it is for the attacker to decrypt the message after intercepting the ciphertext. intuitively, equivocation measures the average number of message/key pairs that could have produced a given ciphertext."
corollary 1: a necessary and sufficient condition for maxequivocation is that the entropy of the ciphertext is equivalent to the maximum between the entropy of the key and the entropy of the message:
"the metapath approach presumes that within a network, the nodes co-occur along a short path tend to have intrinsic relationships. therefore, based on random walk statistics [cit], metapath-based methods optimize the node embeddings such that nodes have similar representations if they co-occur on short random walks over the graph [cit] . the basic idea of this set of approaches is to learn the encoding matrix such that the following equation is satisfied."
we have considered the problem of finding an encoder function when the distribution over the message space is nonuniform and the distribution over the key space is uniform.
"the algorithms proceed by reducing the number of conflicts in c, and returning c when there are no conflicts left and consequently c represents an encoder function. a conflict can be replaced in two different ways: by swapping it with a symbol in a different position in c or by transforming it to another symbol."
"shannon also proved that for a cryptosystem to be perfectly secure it is necessary that the key space is at least as large as the message space, i.e."
"in this paper, we compare our multi-modal deep learning framework with representative traditional machine learning methods and state of the art methods. these state of the art methods include deep learning methods. then, we verify our method across all eight species provided by dip_full and hprd datasets. for instance, for the s.cerevisiae dataset in dip, our accuracy achieved 99.79% while the performance of the other four methods was 95.28%, 95.15%, 99.40% and 74.56% respectively. as for the recall score for s.cerevisiae species, our recall scores achieved 98.13% while the values of the other four methods are 97.07%, 93.92%, 94.14% and 37.25% respectively. for the hprd dataset, we can see from the results that the mean roc score achieves 0.9942, which is consistent with other species in the dip dataset. additionally, we predict protein families based on the deep protein representation features with our models on the dip dataset. our method achieves up to 33.9% improvements in terms of micro-f1 score and achieves up to 74.4% improvements in terms of macro-f1 score over the best performance among random forest, svc and gaussiannb classifiers. different from other protein family classification methods [cit] which require at least 200 instances for each family, our method does not heavily rely on large dataset."
"for the grouped amino acid composition, the 20 types of amino acids are classified into five categories according to their physicochemical properties [cit] . these five categories include the aliphatic group (g1: gavlmi), aromatic group (g2: fyw), positive charge group (g3: krh), negative charged group (g4: de) and uncharged group (g5: stcpnq) [cit] . gaac computes the frequency of each group of amino acids as follows:"
"after the unsupervised learning phase, deep protein features and protein topological representations are learned. we then employ the feature fusion for those extracted features before feeding them as the inputs to the supervised learning model. figures 1 and 3 present the integrated structure of our deep multi-modal representation learning framework, including the two phases of the leaning process. first, we fusion features learned from the cbow model and sae model. by doing this, various modalities as the outputs from the previous unsupervised learning phase are integrated."
"informally, the unicity of a cryptosystem is the average length of the ciphertext such that only one pair of message and key could have produced the ciphertext. this is only an issue when not all bit strings are messages. it is possible to compute the ciphertext length for which the number of messages per ciphertext reduces to 1. this is called the unicity point."
"the tests generate random message spaces, message distributions, and key sizes; determining the domain for an encryption function. the different algorithms for generating encryption functions are then all run on the same random examples and their outputs compared, in particular:"
"this section is dedicated to studying upper and lower bounds for equivocation, and to find necessary and sufficient conditions for a cryptosystem to achieve max-equivocation."
"the following results consider the decrow and randrow algorithms run while comparing message sizes. table iii shows the results for 100,000 tests (equally split across proba and probb) with fixed message size (and random key space size such that"
"in addition to interaction prediction, we performed downstream multi-family protein classification tasks as well using the same features from the unsupervised learning phase. in our experiments, family annotations are obtained from the uniprot database https://www.uniprot. org/. we use all the proteins in the dip dataset and acquire the families they belong to from the database. amongst the protein families in the dataset, we only choose those families with more than 15 samples. this results in the top frequent 99 protein families to verify our results. we present the training accuracy and validation accuracy in fig. 9 to show our model is not subjected to overfitting. then, we evaluate using 5-cv and compare our prediction accuracy with traditional methods including random forest, svc and gaussiannb classifiers. since classifying proteins according to their family annotations is a multiclass classification task, we therefore use f1 score to assess the models' performance defined as the following eq. 20:"
"we conclude that irrespective of the relative sizes of the entropies of the message space and key space, the maximum equivocation is always achieved when it corresponds to their minimum. we say that a cryptosystem respects maxequivocation when it achieves this upper bound."
"we have presented max-equivocation, a generalization of shannon's perfect secrecy condition that can also be established when the entropy of the key is smaller of the entropy of the message. message equivocation measures the number of different messages that can be decrypted from the ciphertext, and max-equivocation holds when this number is maximized. max-equivocation is achieved when the encoder function minimizes the entropy of the ciphertext."
"redundancy of a language can typically be significantly reduced by using compression methods upon the language and then using the compressed result as the message. while this reduces redundancy, it still creates some structure within the language and does not achieve uniform message distribution."
win% the percentage of times each algorithm generates the lowest entropy (may sum to over 100% since more than one algorithm can generate the same lowest entropy).
"during the experiments, we used area under the receiver operating characteristic curve(auc_roc), specificity(spc), accuracy(acc), precision, and recall (or sensitivity) to measure the prediction accuracy and data divergence using our method. the metric formulas are described as the following equations:"
"recently, thanks to the scalability and adaptability of random walk technique, lots of research methods utilize random walk based methods to learn node representations over graph structured data [cit] . among these methods, metapath is the most recent one. during metapaths generation process, we set the length parameter as length to indicate the walking distance starting from each"
"the remainder of the paper is organized as follows. we present the data preprocessing strategies, theoretical background and algorithms of our methods in the \"methods\" section. the \"results\" section describes the framework parameter settings, dataset statistics, and experimental results. finally, we conclude the paper and envision the future work in the conclusion part."
"end for 26 : end for protein node in the network. also, for each protein node, we set the neighborsize as the contextual sampling parameters indicating how many neighbors we take into account as shown in fig. 2 . after that, we apply cbow model trying to learn the distributed node representations within the network structured data and maximizes the likelihood of preserving the topological similarities between nodes. we can regard metapath-based methods as a graph representation model that estimates the occurrence likelihood of observing v i given all the preceding vertices along the short path as shown in eq. 10."
"after preprocessing the raw sequential data, we transform various lengths of protein sequences into 468 equal length vectors using ifeature apis [cit] . in the unsupervised learning phase, we first extract the deep features from previously generated equal length vectors, which will be fed into supervised prediction model."
"recently it has been shown that shannon entropy measures the effort for decrypting the ciphertext by asking arbitrary binary questions, which is not a general security scenario [cit] . other measures based on different entropies have been introduced. smith's min-entropy [cit] has been shown to measure the resistance of the cryptosystem against one-time attacks in which the attacker has a single chance to guess the message correctly. other entropy measures include g-leakage, that parametrizes min-entropy with a given gain function [cit], and guessing entropy, that quantifies the effort required to decrypt the ciphertext with equality tests [cit] ."
"the proba algorithm splits the probability available x (initially 1) randomly into two parts x 1 and x 2, and then recursively calls itself with these parts of the two halves of the message (sub-)space. this continues until the subspace is a single message that is assigned the whole probability."
"a shared-key cryptosystem induces the set of its possible ciphertexts and a decoder function. the existence and uniqueness of such a decoder function is ensured by the requirement that enc(·, k) is injective. note that injectivity of enc(m, ·) is not strictly necessary, but is chosen here to preserve symmetry of the results."
"the results for comparing incrow, decrow, randrow, and extensive over 450,000 tests (equally split across the two probability generation algorithms) are shown in table ii . the results indicate that the decrow algorithm performs the best in all categories, followed by randrow, then incrow and finally extensive. in particular the results show that extensive performs by far we now consider a more refined analysis of the decrow and randrow algorithms to contrast their results and study the impacts of other factors."
"the results presented in this paper can be re-derived using any other entropy measure, generating alternate definitions of max-equivocation that consider different types of attackers. modification of the heuristics to use a different type of entropy is trivial. while in general we do not expect that the optimal encoder function for a type of entropy to be optimal also for other measures that are not refinements of the first, we leave this as an open question."
"we then consider encryption functions under this new perspective and show that in general the theoretical best is not achievable. further, we show that popular approaches to encryption functions, such as latin squares or quasigroups [cit], are also not practically optimal."
"in this section, we illustrate our proposed framework which can be divided into three phases including a protein sequence preprocessing phase, an unsupervised learning phase, and a supervised learning phase. comprehensive illustrations of each phase associated with their inputs and outputs are examined in the following sections."
"for computational intelligent machine learning and data mining methods, it is demanded that the lengths of the feature dimensions are the same. consequently, encoding protein sequences with various length amino acids into equivalent length feature vectors are necessary for the following machine learning tasks. therefore, in this phase, we extract physicochemical information from the protein residues consisting of stacked amino acids and transform them into equal length numerical vectors. in this procedure, we maintain the constitutional protein residue information as much as possible by obtaining the inherent information in the protein peptides. we use the following four methods for converting various lengths protein sequences into fixed length numerical vectors [cit] ."
"however, this requires a and b to have exchanged such a key k before sending the message m, and k can be used only once."
"we not only tested our model within the same species, but also used the s.cerevisiae training dataset as the overall training dataset and assess the prediction performance on the rest seven species using various metrics. in the experiment, the s.cerevisiae training dataset includes 36,006 negative and positive samples. the prediction performance of the rest seven species is presented in norvegicus species, which have not been explored by other methods yet and also achieves promising prediction accuracy."
"once the message space size, message distribution, and key space size have been chosen, each algorithm is run in turn on the same initial state, with the data recorded after each encoder function has been created."
"the design of encryption functions, or their components such as s-boxes, has been widely studied [cit], particularly when designing optimal systems. the results here suggest that when the message space (or inputs to an s-box) are non-uniformly distributed then the result cannot ensure maxequivocation. further, finding optimal functions should not be limited merely to latin squares/rectangles or quasigroups, since higher max-equivocation can be achieved without such tight restrictions."
"to the best of our knowledge, this is the first multimodal deep representation learning framework for analyzing protein-protein interaction networks. specifically, the contributions of our method are listed as follows:"
"we derive upper and lower bounds to equivocation. we show that max-equivocation is achieved when the entropy of the ciphertext is minimized, contrarily to intuition. that is, the best theoretical encryption scheme is one that minimizes the entropy of the ciphertext."
the randrow algorithm is equivalent to incrow except that the columns are scanned in a random order. the pseudocode of randrow is the same as the one described in algorithm 1 except that the order of the columns of c is randomized after line 1.
"the probb algorithm splits the probability available into two parts, assigns the first part to the current message (initially the first), and then continues with the remaining probability and remaining messages. this tends to generate more skewed probability distributions than proba."
the amino acid composition(aac) statistics is the proportion of each amino acid type inside a protein sequence. the aac computes the ratio of each type of amino acid and convert the peptides into equal length numerical vectors. the aac can be computed as follows:
"abstract-preserving the privacy of private communication is a fundamental concern of computing addressed by encryption. information-theoretic reasoning models unconditional security where the strength of the results is not moderated by computational hardness or unproven results. perfect secrecy is often considered the ideal result for a cryptosystem, where knowledge of the ciphertext reveals no information about the message or key, however often this is impossible to achieve in practice. an alternative measure is the equivocation, intuitively the average number of message/key pairs that could have produced a given ciphertext. we show a theoretical bound on equivocation called max-equivocation and show that this generalizes perfect secrecy when achievable, and provides an alternative measure when perfect secrecy is not. we derive bounds for max-equivocation, and show that max-equivocation is achieved when the entropy of the ciphertext is minimized. we consider encryption functions under this new perspective, and show that in general the theoretical best is unachievable, and that some popular approaches such as latin squares or quasigroups are also not optimal. we present some algorithms for generating encryption functions that are practical and achieve 90 − 95% of the theoretical best, improving with larger message spaces."
the structure of the paper is as follows. section ii recalls key concepts. section iii introduces max-equivocation and theoretic bounds. section iv presents results on when the theoretic bounds can and cannot be achieved. section v introduces some algorithms for finding encryption functions. section vi analyzes the experimental results of the algorithms. section vii briefly discusses alternative measures of entropy. section viii draws conclusions and discusses related work.
"in this work we consider the same scenario in which a key with a different size to the message is shared between a and b, and we ask how many bits of message can be sent while respecting a suitable definition of unconditional security. however, we want to define a security condition that is also attainable also when the key is smaller than the message, since this is the most common case in practice."
"we show a theoretical upper bound on equivocation, and we call this condition max-equivocation. we show that maxequivocation generalizes perfect secrecy, corresponding to perfect secrecy when it can be achieved and providing a characterization of maximum possible security when perfect secrecy cannot be achieved."
"we present some algorithms for generating encryption functions from the message and key information that are practical and achieve reasonably good results. in general these give solutions with a quality of 90 − 95% of the theoretical best when the message space is less than 2 11, and the quality improves as the message space increases in size."
"we have shown that having a ciphertext space larger than the message space can increase the effectiveness of encoder functions, proving the general non-optimality of encoder functions that correspond to latin rectangles. also, we have shown that in general it is impossible to give an encoder function that achieves max-equivocation."
we want to provide a measure of the security of a sharedkey cryptosystem that is meaningful whether or not the the key used is smaller than the message to be sent.
max-equivocation can be achieved for any distribution over the message space when the key space is of equal size and the key distribution is uniform. the solution for max-equivocation in this case is any encryption function that can be represented as a latin square.
"here, v j is the next neighboring node of v i . enc p (z i, z j ) represents the statistical probability of v j given its neighboring node v i along the path p. since we only have one type of relationship or edge in the ppi networks, our metapaths generation process was defined as the homogeneous metapaths generation. in our paper, the ppi networks are undirected graphs with vertices v representing proteins and edges e representing interactions. accordingly, we generate nodeoriented metapaths for the protein nodes in the ppi network first and then apply cbow model to learn the distributed topological representations for each protein node. the details for the unsupervised learning phase can be found in algorithm 1."
let m n (resp. c n ) be a random variable on messages (resp. ciphertexts) of length n. then the unicity point u is defined as the least positive value of n such that
"as shown in eq. 12, given a protein p, its topological feature is u cont p and its deep protein sequence feature is u seq p respectively. we concatenate these two features together as u p to represent p."
"as can be seen in figs. 10 and 11, our multi-modal deep representation learning framework outperforms the fig. 7 comparison of the auc-roc score between our method and traditional machine learning methods over eight species"
"the entropy of the ciphertext has: lower bound of the greater of the entropy of the message or the key; and upper bound of the entropy of the message plus the entropy of the key. that is,"
"we will now introduce semi-injectivity, the core property of an encoder function. we will show that as a consequence of the encoder function being semi-injective on both message and key, message equivocation and key equivocation actually coincide, so we can just call them equivocation."
"lemma 5: the entropy of the ciphertext when the encoder function can be represented as a latin rectangle has: lower bound of the greater of the entropy of the message and the entropy of the key; and upper bound of the logarithm of the size of the maximum between the sizes of the message space and the key space. that is,"
"here, n g is the number of amino acids in group g, n(t) is the number of amino acid for type t, and n is the total length of the peptide sequence."
"shannon defined perfect secrecy as the highest possible security condition attainable on a cryptosystem [cit] . perfect secrecy is attained when the mutual information between the message and the ciphertext is zero; when knowledge of the ciphertext gives no information about the message. that is, the prior and posterior entropies coincide, meaning that observing the ciphertext did not reduce the uncertainty of e on the message in any way."
"cbow ( cont, mp u, win) 6: end for 7: ← concatenate ( cont, seq ) 1: deepfeaextr seq (v, d) 2: for i ← 1 to epochs do 3: for h ← 1 to h do 4: sample a batch size of number n as x 5 :"
"we explore here the potential of metadata enrichment and the promising avenue of planning technology as a step forward in developing reusable, interoperable los and pushing forward the agenda for innovative instructional engineering methods and joint tools."
"after the initialisation of the clusters where all clusters are well-defined and labelled, the system is ready to run in normal operation with knn as the equipment identification algorithm. during normal operation, the data acquisition will be running constantly. the computer will process 300 data points, similar to the number of data points used in the initialisation stage, to extract the 3 selected features, namely iμ, ivar, and iavar. these features will form the attributes of the test element. the test element will be placed among the trained elements in the testing space. once again, using knn algorithm, the test element is classified according to the 5 nearest neighbours by euclidean distance. it is very resource-intensive to do the data processing, feature extraction and classification for every subsequent data step; therefore, the experiment did the classification after every 50 steps to reduce the workload of computer. this step size is adjustable, depending on the needs."
"from an ai planning perspective, we can exploit the previous general adaptation requirements even further. the idea is to adapt profiles to losand thus generate extremely flexible learning designs-by extending the modeling, metadata extraction, and planning techniques (steps 2, 3, and 4 of figure 2 )."
we can evaluate our approach from a quantitative perspective that measures system response and a qualitative perspective that measures the benefits and effort for course designers and students.
"metadata labeling is specified by the lo's creators, usually in an xml standard format, such as the ieee learning technology standards committee's (ltsc) standard for learning object metadata (lom). 4 the purpose is to offer a unified way to label los for their eventual use or reuse as interoperable units (see figure 3 ). there are many useful entries for pedagogical theories, including the general descriptors, but only three aspects address personalization:"
"for effective interoperability, a lo must be a stand-alone, modular entity that incorporates its learning context, or semantic relationships, in its own metadata. metadata labeling is a key issue for los' semantic annotation, encoding, exchange, and reuse, as it offers a successful way to catalogue and navigate content, context, usage, and structure. [cit] this is valid for both adaptive courseware generation, where the goal is to ensure a student completes the required activities, and dynamic courseware generation, where the goal is to assist students in navigating a complex hypermedia space. metadata labeling is also crucial for dealing with correct task adaptation in terms of educational aspects, such as the difficulty of using the lo and how it affects the learning process. [cit] intuitively, the ultimate goal is to create a personalized course, a studentcentered solution in which the gathering of activities and their sequencing is tailored to each student's specific needs, objectives, and background."
"the presentation mapping step is another translation process to transform the calculated learning design into a standard language (manifest) for the lms's use. although current lmss support different languages, such as ims-ld for dotlrn or moodle templates, the compilation algorithm is quite general. for each student, the process generates one or more documents that include the learning goals, the prerequisites and previous knowledge, the roles, the activity structure (which represents the los and their orderings), and the resources required by the los. after uploading these documents on an lms, the student progressively navigates through the contents, avoiding exposure to all the course's los."
"a. acquisition of steady state electric current waveforms from equipment a 12v dc office grid is set up for this experiment. it includes five commonly-used office equipment. in addition to the 12v powered equipment of led desk light, table top fan and 22\" led tv, the office grid also includes a 12v to 5v step down dc-dc converter for mobile phone charging and a 12v to 19v step up dc-dc converter for laptop. a current transducer was installed along the live line of the dc circuit to acquire the signal of the electric current waveform. this acquired electric current signature is sent to a computer for processing. fifty readings of steady state electric current waveforms were acquired without labelling. each waveform reading was made up of 300 data points acquired at a frequency of 500hz. fig. 2 shows all the 50 sample steady state waveforms. this experiment is interested in clustering and identifying individual office equipment in the office grid, thus only one load is active at any point of time without any overlapping signal from other equipment."
"the mapping for course generation is a translation process that extracts lo metadata information and automatically builds the course structure through the causal dependencies between los and their adaptation aspects. note that there isn't a unique mapping, as it would depend on the techniques used in the solving stage-for example, a set of formulas for mathematical models, an action-based formulation for planning, or a constraint satisfaction problem for constraint programming. the underlying idea in any mapping is to process the course's los and include student information (background, profile, learning goals, and temporal or resource constraints) to create a course structure that a particular solver can use as an input."
"personalization approaches are challenging, and the horizon is still unclear. 3 adapting los to learning styles is a big shift from the conventional way of teaching: some lecturers were reluctant to redesign los and courses to fit different profiles. moreover, assembling los for fully adapted instructional courses requires experience and training, which sometimes handicaps it-illiterate lecturers. the demand for self-learning and it approaches continues to grow, but the application of planning technology can ease the incremental construction of tailored courses. 13 medium-long course p e r s o n a l i z e d l e a r n i n g"
"these electric current waveforms will form the signature of the equipment where machine learning techniques can be used to decipher, analyse, learn, and identify as required [cit] . machine learning has been used a lot in the smart energy management system, and k-nearest neighbours (knn) is one of the more commonly used classification machine learning techniques in identifying and classifying the test element [cit] . however, knn requires the elements in the test space to be known and labelled before the identification process. this will be troublesome and tedious if the number of elements are large."
"as the top of figure 1 shows, we have different routes to achieve both learners' goals. if we focus on the los of the learning the java language module (figure 1, bottom), the number of routes is even higher. john will require most of the los here, whereas rebecca will need just a few to learn the main features of java, excluding the oop-related los. this is content adaptation; students receive different sequences of los according to their profile, knowledge, and interests. let's also imagine that the an example lo requires the use of a computer. rebecca has a laptop, but john has no computer, so he needs to go to a lab. picture now that the generics lo is a lecture that requires in-person attendance on tuesdays, from 1:00 to 3:00 p.m., and the classroom's maximum capacity is 20 people. this is context adaptation: considering the real-world constraints (time, resource consumption, and group activity synchronization) to schedule a route or perhaps avoiding some los if another route is feasible. thus, adaptation implies provision for dynamic learning content, an adaptive behavior to promote the quality of learning, and a flexible process that lets students adjust their schedules to the course's resources."
"creating personalized courses is a hard task because los in themselves are insufficient for significant instruction. hence, offering an incremental and friendly way to link los using pedagogical and instructional design theories is highly appreciated. 13 a course initially designed with the same los for all students (see figure 1 ) can later be enriched by including collections of tailored los suitable to particular profiles. for instance, according to richard felder and linda silverman's learning style classification, 7 a lecture can be very recommendable for verbal students but not for visual ones, and just the opposite holds true for a diagram. this incremental process lets designers extend lo metadata records, thus improving their adaptation capabilities."
"ultimately, scheduling reduced scalability by approximately one order of magnitude. however, the performance was still reasonable and let us manage scheduling constraints in groups of 10 to 20 students, even for long courses."
"as for students, the experience was highly positive. they found the provided los to be a helpful resource for catching up on the background required for the course and grasping key ideas as well as an ideal mechanism for self-assessment. 1 student motivation also stems from the ease of signing up for the course. no previous training was required; they just had to classify themselves in one or more learning styles (www.engr. ncsu.edu/learningstyles/ilsweb.html) and define their background, preferences, learning outcomes, and, optionally, their temporal or resource constraints. the most outstanding result among students was the degree of satisfaction with the course's self-organizing activities. this approach let each student have a personalized learning route that was especially designed for their profile, which makes it easier to fit the student's personal and temporal restrictions. w e can affirm that students generally seem more enthusiastic about e-learning than lecturers. from our experience, the use of planning technology proved to be very successful in promoting adaptation. but it's important to note that the success of this approach can't be directly assessed through student grades: following a fully adapted path doesn't necessarily mean a better score. however, it does require students to study effectively and quickly, which provides a higher motivation for using los that fit their preferences and learning styles."
"where m is the data set size (1) the next useful feature will be the variance of the electric current during steady state. the variance value, ivar, is the squared of the standard deviation value, i, of the data set. the standard deviation is a measure of how spread out the data points' values are from its mean (2); and this is used to quantify the amount of variation or dispersion of a data set."
"adaptation involves several technological issues, as depicted in figure 2 : use of common lo repositories and modeling tools, algorithms for students' information acquisition, application of solving techniques, and visualization of learning designs on learning management systems (lmss). the role of course designers is to model a course by reusing or defining new los. the relationship of students with the system is established when setting their profiles and preferences as well as during navigation through their personalized learning designs. one route per student is generated (in an offline mode) before using the lms, so the student's learning behavior doesn't require any particular change-technological aspects are transparent to students. this personalized learning encompasses five essential requirements."
"silhouette analysis is a method to validate the consistency and reliability of clusters after the unsupervised k-means clustering process. it studies the separation distance between the resulting clusters and plots out a measure that shows how close an element of a cluster is to other elements in neighbouring clusters. the range of this measure is between -1 and +1. elements with silhouette value near to +1 denote that they are far away from other clusters. elements with silhouette value near to 0 indicate that they are very close to the decision boundary between 2 neighbouring clusters. elements that might have been assigned incorrectly will have negative values. the k-means clustering process is considered successful when all the elements have high value. if there are elements with low or negative values, then it is necessary to restart the clustering process. if the occurrence of negative values persists after many iterations, then it implies that there are not enough features or there is an incorrect number of clusters. the silhouette plot in fig. 4 implies that not all elements are well-grouped in the clusters, which implies that 2 features are not enough to clearly distinguish the clusters. an additional feature, iavar, was added into the clustering process to form 3-dimensional clusters. fig. 5 shows the result of a successful k-means clustering process after using 3 features. this section shows that the unsupervised k-means clustering method was able to group the 50 raw steady state electric current waveforms into 5 distinct clusters by using the extracted features of iμ, ivar, and iavar. however, the clusters are only labelled with cluster 1, 2, 3, 4, and 5, instead of the actual equipment names and, therefore, will not be useful in the identification process. in the next section, the supervised machine learning method, k-nearest neighbours, will be used to label the clusters."
"the objective of the initialisation process is to establish a database for the identification process that will be performed during the normal operation. fig. 1 shows the steps involved. in this initialisation stage, both unsupervised and supervised machine learning techniques will be applied. however, the machine will still require the inputs from the users to input the number of clusters and to indicate the appliance names. this will be done via a simple text-based human machine interface."
"given that dc power supplies are getting more popular in the form of photovoltaic panels and batteries, there is a possibility of having dc office grid in the near future. energy management systems in the future might have to deal with dcpowered loads without the intervention of ac-dc rectification process. one way to manage loads is to connect the loads with the main computer via communication means such as ethernet lines, or wirelessly, through wi-fi. however, these will signify extra cost and additional modification. these add-on components can be installed in bigger equipment, such as a printer or television, but will not be cost-effective and feasible for smaller equipment such as lights and mobile phone chargers. another alternative, as proposed in this paper, is to use a current transducer installed along the dc grid to sense the current consumption."
"opted for developing their own nonstandard solutions. the resulting incompatibility of lo formats, along with the tendency of regarding them in isolation, has contributed to a general lack of reusability and interoperability."
"the metadata information extraction stage analyzes student information and iterates over the los to generate one standard planning domain definition language (pddl) action per lo and student, as detailed in figure 5 's mapping. this compilation is highly efficient, as each action comprises four entries automatically extracted from the values of the lo metadata: name, with the lo name; duration, with the lo learning time; conditions, based on the profile's dependencies plus the relations defined in its metadata; and effects, based on the learning outcomes. this process also validates and matches preconditions and effects, thus detecting incorrect lo metadata and discarding unfeasible actions."
"in detection of states and identification of loads, it is much faster and resource-efficient to use extracted features instead of using every data point from the data set. extracted features are derived values from a data set that are informative and non-redundant. good features should be descriptive of the data set and should help to reduce the dimensions of the data set significantly. mean operating current value is obviously one of the most distinctive features. the mean or average value, iμ, is the expected value of the data set which can be calculated by summing up the values of all data points divided by the size of the data set (1) ."
"as the trained elements in the testing space are formed by the features that describe the steady state of individual office equipment, this method can also be used to detect steady state and transient state of the equipment. the plotting of the test element in the testing space showed the state change processes from off state to transient state to steady state."
"this personalization perspective has been addressed by using different techniques, such as adjacency matrices, integer programming models, neural networks, and intelligent planning techniques. [cit] in particular, researchers have successfully applied ai planning techniques to the construction of adapted courses as a means to bring the right content to the right person. 10, 12 however, designing a course usually requires dealing with aspects such as group interaction, collaboration, and sharing of specific (and perhaps costly) resources. thus, it isn't only about bringing the right content to the t he internet offers many opportunities for promoting student learning: you can easily find terabytes of learning object (lo) repositories for course assembling, customization, and content packaging. [cit] but despite this amount of data, institutions engaged in educational processes have traditionally right person but also at the right time and with the right resources, a missing aspect in traditional e-learning."
john is in his first year of bs in computing science and is interested in programming. rebecca is a selftaught programmer with experience in object-oriented programming (oop) and c++; she wants the java certificate.
"although the two extracted features, iμ and ivar, in general, are able to describe most of the electric current waveforms of the office equipment, they fall short in the unsupervised clustering process. therefore, on top of the above two familiar features, the paper uses an alternative variance, iavar, as the third feature. instead of the above mentioned variance which is the mean-square deviation from average, iavar is defined as the mean of squared difference of successive data (3)."
"in this paper, a 12v dc office grid with common office equipment is set up. the proposed method was applied to the dc office grid to initialized the training set which will be used to identify the equipment and to determine when they are in steady state. the office grid comprises three 12v equipment, namely led desk light, table top fan, and a 22\" led tv. the experiment was made more challenging by including a 12v to 19v step up dc-dc converter for laptop and a 12v to 5v step down dc-dc converter for mobile phone charging. the following section will discuss the experimental set up, the initialisation and training stage. this will be followed by the identification process in the normal operation stage and the conclusion."
"although a steady state of the equipment can be assumed to be established when there are more than a certain number of subsequent test elements falling into the same cluster, the detection can be further enhanced by setting a boundary around the centroid of the cluster which was determined earlier by kmeans clustering. the steady state of the equipment can thus be defined, by having a certain number of subsequent test elements being identified by the same cluster, and within the boundary set around the centroid. this experiment assumed that a steady state has been reached when there are at least 5 subsequent test elements fulfilling the 2 criteria. this number can be adjusted according to needs. fig. 7 shows the electric current path of the test element from the start of a laptop to its steady state."
"additionally, the lo3 outcome is higher when the student's profile is high and smaller if it's low. this means that when using lo3, a more oop-experienced student such as rebecca will require less effort than john. after doing this lo, a student with more oop experience will become more competent in java classes than a novel student who has just started with oop. analogously, we can model temporal and resource constraints for context adaptation, such as, for instance, that lo1 requires a computer or that lo3 must be done in a collaboration group, which entails synchronization constraints among the students (and other constraints to arrange/attend a seminar)."
"one of the common research interests in household and office energy management systems is load monitoring. load monitoring involves knowing which equipment is turned on or off and whether the equipment has reached its steady state in normal operation [cit] . currently most of the research in this area revolves around ac power loads, this is due to the fact that most of the major electrical load are ac powered [cit] . however, it is noticeable that there is an increasing number of dc power equipment in the office. equipment that are traditionally powered by ac are now converting to dc; for example, the influx of led lights replacing the traditional fluorescent lights and the dc powered fan replacing the conventional ac fan. many modern office equipment are also coming in dc powered and require an ac-dc rectification adaptor, for example, laptops, mobile phones and led tv."
"so although planning technology is highly appreciated by students and less popular among lecturers, who are somewhat reluctant to give up their traditional role of course planners, reality shows that in the context of web libraries, it's difficult for a lecturer to create fully personalized plans for students that meet their personal constraints, too. both students and lecturers can agree that the flexible application of planning techniques can provide the right content to the right person at the right time."
"the result of the solving stage, regardless of the technique used, is a tailored learning design: a sequence of los that suits each student's preferences and necessities. from a pedagogical perspective, a tailored design is simply a collection of los, but the temporal and resource constraints must be contemplated in a real scenario. thus, some los might be associated with a time stamp and to a particular resource-for example, an lo that requires the use of a shared microscope is only available when the lab is open."
"we tested our approach on four ai courses (short, short-medium, medium-long, and long, with approximately 10, 20, 40, and 80 los, respectively) in a repository of 172 los for different learning styles. we targeted plan quality and system scalability, considering only planning and planning+scheduling (synchronization and resource consumption constraints, randomly generated, on los). we conducted two experiments in our solver to minimize makespan (shortest plan) and maximize the students' learning rewards based on the los that best fit them. 10 we defined problems with 1, 2, 4, 8, ... 256 fictitious students with different profiles and ran the experiments on a 2.33-ghz intel core 2 duo cpu with 3.23 gbytes of ram. figure 6 shows the results. obviously, the maximization problem involves longer plans, which degrades performance. including the scheduling constraints (+sc) increases the complexity of the course and makes the problem more difficult to solve, which means an expectable negative impact on performance. in this situation, we can only solve problems with fewer students (mainly in longer courses). this general issue was due to csp solver complexity-our embedded scheduler handled up to 2,000 variables for the los in a reasonable time, but anything extra exhausted the allocated time."
"the proposed method in this paper uses unsupervised kmeans clustering and supervised knn in the initialisation stage, to remove the tedious process of knowing all elements beforehand. both methods require features to be extracted from the signature obtained from the electric current waveforms of the equipment. these features become the attributes of the training and testing elements. the unsupervised k-means clustering, groups the collection of unknown elements into their respective clusters, based on similarity in their features [cit] . the clusters are then labelled using the knn technique, by injecting a known test element into the training space. in the normal operation stage, new unknown test elements will be created from the acquired signal of the electric current waveform. these new unknown test elements will be identified to a labelled cluster using knn algorithm by the majority votes from its nearest neighbours. the steady state of the equipment can be determined when the test element is within a distance from the centroids obtained in the k-means clustering during initialisation stage."
"the extracted features, iμ, ivar, and iavar, contribute to the attribute values of the training elements. in order for the user to label a cluster, the user will need to start the equipment in the dc office grid. after the equipment goes into steady state, the signal of the electric current waveform can be acquired by the current transducer and sent to the computer. the features of the waveform will be extracted in the same way as the training elements, in order to create the new test element. this new test element will then be thrown into the training set, and the similarities between all training elements and it will be calculated, using euclidean distance. it will be classified to a cluster, based on majority voting of its nearest neighbours using the knn algorithm. the user will then be prompted to (a) elements described by 2 features (b) elements described by 3 features enter a label name for the test element, which is also the label name for that particular cluster. all the training elements of that cluster will be classified and labelled with the same label name. this process is repeated for all the 5 clusters in the training set. fig. 6 shows the labelled clusters. using this method, the user will only need to keep track of 5 different test element labels instead of all the 50 training elements. this will be significant when the training set is very large. the training space will now become the testing space for the identification of equipment while detecting the state of the dc office grid during normal operation of the office grid. this will be discussed in the next section."
"moving beyond, we can model a higher level of adaptation, as shown in lo3 (classes and objects) in the top part of figure 4 . given two levels for previous oop knowledgenamely, high and low (rebecca's and john's, respectively)-lo3 is valid for any type of student, but if the student's previous oop knowledge is high, the prerequisite for this lo is lower than if it were low."
"to assess lecturers' opinions on course contents and adaptation to student profiles, we designed a questionnaire for a long ai course; see table 1 . ten lecturers who regularly teach ai to graduate students answered our survey. we also gathered the opinions of 10 students for this course. generally speaking, the lecturers agreed with the flow automatically generated (by the planner) of the learning designs. however, lecturers missed the particular, pedagogical organization of los that they manually include in their way of teaching in the form of advice or recommendations to students. although they generally agreed with the course composition and causal dependencies among los, they felt that the course's overall structure was too open to students."
"mapping the e-learning problem to a standard pddl+csp model facilitates the use of independent solvers and abstracts out the e-learning features from the planning and scheduling details. a whole description of planning and scheduling technology is out of scope here, but more information about our solver appears elsewhere. 10 in short, the planner determines the best los, and the scheduler handles students' sharing temporal and resource constraints, thus deciding when and how to use such los."
"ideally, the dc current consumption of electrical load should be a constant value; therefore, the simplest way to assume the operation of any equipment is by comparing the current consumption from the power source to the mean operating current of the office equipment. however, it is observed from the experiment that the dc current consumption of equipment is not always a nice straight line. on top of the gaussian white noise in the dc line, there are other noises caused by external disturbance or due to internal components."
"is crucial: \"lo1 requires lo2\" and \"lo2 requires lo1\" would entail a contradiction, although it's automatically detected and avoided during modeling. third, the course designer needs some experience with los in a given domain to accomplish a high degree of personalization."
"knn is a supervised machine learning algorithm that is based on the computation of the k nearest training elements in the overall training set and on the election of the class through majority voting on the labels of the nearest elements. it is a distance-based algorithm; this paper uses euclidean distance to determine the closeness of the training elements to the test element. it is usually used to classify a new test element; however, in this case, it is used to label the clusters instead."
"we have examined the effect of the cost of reputation building on the evolution of cooperation through indirect reciprocity. by analytical investigation and individual-based computer simulations, we have shown that the slight cost of reputation building completely destroys indirect reciprocal cooperation regardless of the cost-to-benefit ratio of cooperation or moral assessment rules."
"it is important to note that the indirect reciprocity account relies on individuals' substantial cognitive abilities for reputation processing (e.g., communication skills, capacity for judgment based on social norms). high-level intelligence has been thought to be very costly in terms of biological fitness 12, 13 . in other words, natural selection does not favor the evolution of intelligence unless excessive benefit is present. although the concept of indirect reciprocity is based on humans' high-level intelligence, which potentially causes a loss of biological fitness, to our knowledge no study has investigated the effect of the cost of reputation processing on the evolution of indirect reciprocity."
"in a broad sense, our results are closely related to the second-order free rider problem 24, 25, which asks \"who incurs a cost to preserve social systems for the maintenance of cooperation?\". a typical example is the evolution of altruistic punishment, which asks [cit] \"who bears a cost of the punishment of defectors?\". imagine that there are three types of individuals: free riders (defectors), altruistic punishers (cooperators who punish free riders) and cooperators who do not punish, and that there is a cost to administering punishment. in this situation, although altruistic punishers punish free riders with a cost, cooperators never bear the cost. in other words, cooperators can \"take a free ride\" on punishers' punishment of free riders, the socalled second-order free ride. natural selection therefore favors cooperators and leads to the extinction of altruistic punishers, and then, after the extinction of the punishers, defectors finally dominate the population of cooperators."
"let g t (p, q) be the fraction of individuals among (p, q) strategists whose reputation is good at round t, and let x(p, q) be the probability density of (p, q) strategists over the population. then, the average payoff of (p*, q*) strategists at round t is:"
"reputation dynamics in the basic analytical model. here we show that the dynamics of the fraction of individuals among (p, q) strategists who have a good reputation, g t (p, q), depends on p, but not on q."
"let's consider a population comprising an infinite number of individuals. each individual in the population has a reputation, either good or bad. for each round t 5 1, 2… in a generation, each individual randomly finds an opponent and plays a one-shot prisoner's dilemma game. in this game, two individuals in a pair simultaneously choose to either ''cooperate'' or ''defect''. cooperation confers a benefit, b, to the recipient while accruing a cost, c, to the donor (b . c . 0). in contrast, defection yields nothing to either person. moreover, each individual's behavior in the game is witnessed by an observer who decides whether to build and spread the donor's reputation at a cost c r (. 0). finally, at the end of the generation, each individual leaves offspring depending on his/her fitness defined as the total payoff during the generation (i.e., natural selection). higher fitness implies a higher probability that the individual can leave more offspring."
"to further explore the effect of the cost of reputation building in more complicated situations, we constructed an individual-based computer simulation model. let us consider a population of n individuals. as in the seminal work on indirect reciprocity 7, each player has a reputation score, s, that ranges from 25 to 5. at the beginning of each generation, which comprises m x n consecutive rounds, scores are reset to zero."
"where g t~ð ð g t p,q ð þx p,q ð þdpdq denotes the fraction of individuals whose reputation is good in the whole population. the first term of equation (1) represents the benefit of cooperation multiplied by the probability that the focal (p*, q*) strategist is cooperated with by other individuals; the second term denotes the cost of cooperation multiplied by the probability that s/he cooperates, and the third term is the cost of reputation building multiplied by the probability that s/ he builds the donor's reputation when s/he is in a role of an observer. since we can show that the reputation dynamics, g t (p*, q*), depends on p*, but not on q*, for any moral assessment rules 11, 14 (see methods), g t (p*, q*) in equation (1) can be replaced by g t (p* ), that is:"
"in conclusion, the fraction of individuals among (p, q) strategists who has a good reputation at round t is determined by p but not by q."
"we can consider various ways to update reputations, called moral assessment rules 7, 11, 14, 18 . one way is that cooperation (defection) is simply regarded as good (bad) irrespective of the donor's or the opponent's reputation. another way is to introduce the concept of justified defection: defection against a bad opponent is judged as good. moreover, one might judge cooperation with a bad opponent as bad, which is the concept of unjustified cooperation. taking these possibilities together, we assume that individuals in a community share the same moral assessment rule and they judge a donor's goodness based on the donor's own reputation and behavior and the opponent's reputation. a moral assessment rule is then represented by an eight-dimensional vector, r 5 (r bdb, r bdg, r bcb, r bcg, r gdb, r gdg, r gcb, r gcg ). the subscript letters indicate, from the left to the right, the donor's reputation, the donor's behavior and the opponent's reputation, respectively, and r [ [cit] denotes the probability that the donor's behavior is judged as good in each of the eight situations. for example, (0,0,1,1,0,0,1,1) indicates image scoring 43 in which cooperation (defection) is simply regarded as good (bad). further, r *db 5 1 is consistent with the concept of justified defection, and r *cb 5 0 reflects the concept of unjustified cooperation (* is a wild card)."
"given equation (4) and g 1 (p, q) 5 1 for any (p, q), we can see that g t11 (p*, q*) depends on p* but not q* for any moral assessment rules."
"recall that the reputation of an individual is updated only when the observer decides to build that individual's reputation, the probability of which is e q ð þ~ð ð qx p,q ð þdpdq. hence, the fraction of individuals among (p*, q*) strategists whose reputation is good at round t11 can be written as:"
"in the present study, as in the previous studies 9, 15, 17, we assume that each individual makes decisions in the prisoner's dilemma game based on the opponent's reputation. such a decision-making rule, called a behavioral strategy, is denoted by a two-dimensional vector, p 5 (p b, p g ), where p b and p g [ [cit] indicate the probability that the individual cooperates when his/her opponent's reputation is bad and good, respectively. in addition, each individual in the role of the observer decides whether to build and spread the reputation of the donor. by q [ [cit], we denote the probability that the individual builds the donor's reputation. that is, each individual's strategy is defined as a pair of p and q, indicated by (p, q)."
"how does an observer build the donor's reputation? in other words, what behavior of the donor is interpreted as good or bad? we assume that an observer build the donor's reputation based on the donor's own reputation and behavior and the opponent's reputation 14, and that all individuals in the population share the same moral assessment rule 9, [cit] (see methods) . furthermore, each individual's reputation at the initial round is assumed to be good."
"one caveat to our conclusion is that other mechanisms may support the evolution of cognitive abilities to build reputations and thus indirect reciprocity. one candidate is multilevel (group) selection [cit] . suppose that individuals of a population are subdivided into groups and interact within these groups, and that individuals who build reputations are concentrated together within the same groups. then, groups of individuals who build reputations can achieve high-level indirectly reciprocal cooperation and thus, despite the cost of reputation building, outperform the other groups of individuals who do not build reputations. consistent with this conjecture, it has been demonstrated that, in the absence of costs of reputation building, multilevel selection supports the evolution of a moral assessment rule under which indirectly reciprocal cooperation is evolutionarily stable 21 . another candidate is network reciprocity 5, 32, 33 . in a typical setting considered in studies of network reciprocity, individuals of a population occupy the vertices of a graph and the edges determine who interacts with whom. that is, each individual plays a game only with her neighbors and mimics the most successful neighbor's strategy. in this setting, individuals who build reputations can form a cluster in which indirect reciprocity is established, and thus might be able to prevail in the population. moreover, it might be possible that building reputations is beneficial enough to overcome the cost in situations that differ from those considered in our study. indeed it has been mathematically shown that, when individuals engage in several games, an evolutionary outcome of a single game cannot be predicted without assessing the structures of the other games 34 . how these mechanisms complement indirect reciprocity in the presence of the cost of reputation building will require further investigation."
"these results together demonstrate that, as predicted by the analytical models, the evolution of cooperation based on indirect reciprocity is extremely vulnerable to the cost of reputation building in a complicated situation (e.g., a reputation score is 11-scale but not binary 23 )."
"ooperative interactions among genetically unrelated individuals are a fundamental aspect of human society. however, cooperation accrues a cost, c, to the donor of the cooperation while conferring a benefit, b, to another individual (b . c . 0). what mechanism enables the evolution of this costly behavior, i.e., cooperation? this issue has been of considerable concern in both social and biological sciences [cit] . one of the proposed mechanisms for the evolution of cooperation is \"indirect reciprocity\" working through reputation [cit] . that is, cooperative behavior can prevail because the behavior builds the donor's good reputation and then he or she receives some reciprocal benefits from someone else in the community. for example, if an individual a helps b, another individual c observes the cooperation; c builds and spreads a's good reputation, and then another individual d helps a by referring to a's reputation. this account is quite powerful because this does not require kinship, a spatial (group)-structure, or repeated interactions among individuals to explain the emergence of cooperation."
"where b t 5 1 2 g t denotes the fraction of individuals who have a bad reputation. in the same way as that in the basic model, we can show that the reputation dynamics g t (p*, q*) depend on p* but not on q*. hence, we can show that, given a value of p, individuals with q 5 0 always get the highest payoff in each round, regardless of the cost-to-benefit ratio of cooperation (c/b), the amount of the cost of reputation building (c r . 0), or moral assessment rules. in other words, natural selection always favors individuals who never build and spread a reputation, and thus indirect reciprocity never works."
"in this study, we examine the evolution of indirect reciprocity in light of the cost of reputation building (spreading). by mathematical analyses and individual-based computer simulations, we demonstrate that the slight cost of reputation building completely destroys indirect reciprocal cooperation regardless of the cost-tobenefit ratio of cooperation or moral assessment rules (social norms)."
"in a context of indirect reciprocity, it has been demonstrated that cooperation can evolve through indirect reciprocity given an appro- www.nature.com/scientificreports priate reputation system 7, 11 . however, little attention has been paid to the issue of who covers the costs of maintaining the reputation system. as shown in the present study, in the presence of the cost, the reputation system is no longer sustainable and thus indirect reciprocal cooperation vanishes. the present results shed light on the possibility that the second-order free rider problem is inherent in various contexts other than altruistic punishment."
"consistent with the previous studies 7, 8, we find that, in the absence of cost of reputation building, the frequency of cooperation increases with the decrease in the cost-to-benefit ratio of cooperation and the increase in the number of rounds in a generation (see the black points in fig. 1 ). this tendency is common to all the three moral assessment rules (compare the panels a vs. b vs. c in fig. 1 ). this indicates that the evolution of indirect reciprocal cooperation becomes easy as the net benefit of cooperation and the number of interactions among individuals increase."
"further, it is worth noting that indirectly reciprocal cooperation does not necessarily rely only on reputation building. for example, if the population is small enough, each individual can judge others' goodness by direct observation, instead of the indirect observation via reputation. moreover, experimental studies on humans and rats demonstrated another type of indirect reciprocity: an individual is more likely to help anonymous others when s/he received help in previous interactions, compared with when s/he did not [cit] . this type of reciprocity, called upstream indirect reciprocity, seems to be driven by a feeling of gratitude rather than reputation building. however, to date, few studies have provided theoretical explanations of the evolutionary origin of upstream indirect reciprocity [cit] . humans' sociality is supported by their sophisticated cognitive abilities, e.g., language skills, which are biologically costly 12, 13 . despite the importance of the cost of intelligence in human evolution, little is known about the effects of this cost on the evolution of cooperation (but see references 41, 42 ). this study is, to our knowledge, the first to assess the relationship between the cost of high intelligence and the evolution of indirect reciprocal cooperation, highlighting the importance of considering the costs of high-level cognitive abilities in studies of the evolution of humans' and animals' social behavior."
"extended analytical model. here we assume that each individual makes decisions based not only on the opponent's reputation but also on his/her own reputation. in this case, a behavioral strategy is denoted by a four-dimensional vector, p 5 (p bb, p bg, p gb, p gg ), where the left subscript letter denotes the focal individual's own reputation, the right denotes the opponent's reputation, and p represents the probability that s/he cooperates given his/her own reputation and that of the opponent."
"equation (3) indicates that, given a value of p, individuals with q 5 0 always get the highest payoff in each round, regardless of the costto-benefit ratio of cooperation (c/b), the amount of the cost of reputation building (c r . 0), or the moral assessment rules (update rule of g t (p, q)). this means individuals with q 5 0 always get the highest fitness, sum of the payoffs, for any p. in other words, natural selection always favors individuals who never build and spread a reputation (i.e., q 5 0) for any behavioral strategies, p. note that this statement is also true when we assume more general behavioral strategies, that is, when individuals make decisions based not only on the opponent's reputation but also on their own reputation (see methods) 14, 18 . in summary, we show that, in the presence of the cost of reputation building, natural selection results in a society where no individual builds a reputation and thus indirect reciprocity never works."
"as in the analytical model, we assume all individuals in the population share the same moral assessment rule (social norm). we consider the following three rules. one is the simple, called scoring 7, in which the reputation score increases by one unit if a potential donor cooperates; it decreases by one unit if s/he defects. that is, cooperation and defection are judged as good and bad, respectively. another rule is mild (or sometimes called standing) 11, 16, 20 : the score increases if a donor defects against a bad individual (s, 0) or cooperates; it decreases otherwise. this rule incorporates the concept of justified defection 7, 11 and has been known to stabilize the indirect reciprocal cooperation 9, 16, 18, 20 . the third rule is sterm (or sometimes called kandori) 9, 11, 21 : the score increases if a donor cooperates with a good individual (s .5 0) or defects against a bad individual; it decreases otherwise. this includes not only the concept of justified defection but also unjustified cooperation (i.e., cooperation with a bad individual is regarded as bad) 11 . a donor's behavioral strategy is given by a number, k (k 5 25 to 6): an individual with this strategy, k, cooperates if the score of the recipient is at least k. in other words, a high reputation score of an individual often implies a higher probability that others will cooperate with the individual. an observer's strategy is depicted by a number, q [ 0,1 f g: an individual with q 5 1 (0) does (does not) update and spread the donor's reputation score based on the moral assessment rule. that is, each individual's heritable traits are k and q."
"where p 0 t p 1 ð þ indicates the payoff in the prisoner's dilemma game, which depends on p* but not q*, and c r q* denotes the cost of reputation building, which depends on q* but not p*."
"however, the presence of the cost of reputation building completely destroys indirect reciprocal cooperation regardless of the costto-benefit ratio of cooperation or moral assessment rules (see the red points in fig. 1) . even when the amount of cost of reputation building is only 1% of the benefit of cooperation, the evolution of cooperation is impossible (red squares in fig. 1 ). we also examined another case in which the reputation of a donor is built by the recipient instead of a third person/observer. that is, the recipient decides to or not to incur a cost of building the reputation of the donor. we confirm that, consistent with the original case, introducing the cost of reputation building makes it the evolution of indirect reciprocal cooperation impossible (see suppl. fig. s1 )."
"at the end of each generation, each individual leaves offspring depending on his/her fitness (i.e., natural selection). the fitness value is defined as the total payoff received during the generation (m 3 n rounds). higher fitness implies a higher probability that the individual can leave offspring. we use the \"binary tournament selection\" procedure, a genetic algorithm, to select individuals 22 . in addition, mutation is introduced: with the small probability m, each individual's strategy k and q changes to another value randomly."
"we also investigated a situation in which an observer who does not build the donor's reputation can lose his/her own good reputation. in other words, individuals who do not incur a cost of reputation building can be judged as bad. we assume that, at each round, in addition to a donor, a recipient and an observer, one individual is selected as an observer of the observer, who we call a ''second-order observer.'' s/he decides whether or not to update and spread the observer's reputation score with a cost c r' . 0. a second-order observer's strategy is depicted by a number, r [ 0,1 f g: an individual with r 5 1 (0) does (does not) build the observer's reputation. when a secondorder observer's strategy is r 5 1, the (first-order) observer's reputation score increases by one unit if s/he builds the donor's reputation; it decreases by one unit otherwise. the results of the computer simulation show that, consistent with the original case, natural selection never leads to indirectly reciprocal cooperation regardless of the cost-to-benefit ratio of cooperation or moral assessment rules (see the red points in suppl. fig. s2 ), except for the unrealistic case without the cost of reputation building for second-order observers (i.e., c r9 5 0; see the black points in suppl. fig. s2 ). this is because individuals building a reputation as a second-order observer (i.e., r 5 1) are exploited by individuals with r 5 0 for the cost, c r9 (see suppl. fig. s3) ."
"to perform our numerical evaluation, we developed a c++ event-driven simulator, where we randomly generate the arrival of 55,000 demands originating from the rus. arrivals are generated according to a truncated poisson distribution, which is used to capture the fact that css support limited backhaul traffic, and are uniformly distributed among rus in the network."
"due to the lack of reference hdris, the performance of the proposed btmiqa method was only compared with those of other blind state-of-the-art methods on espl-live hdr database, as listed in table 7 . likewise, btmqi, higrade [cit] and the proposed btmiqa method designed for tmis performed better than bliinds-ii and brisque designed for ldris. furthermore, higrade and the proposed btmiqa method can be observed to perform better than btmqi, which confirms the importance of the chrominance distortion in the tmi quality assessment."
"the main difference between a high-quality tone-mapped image and a traditional low-dynamic-range image or poor tone-mapped image is that the former preserves more detail information in its brightest and darkest regions. a highquality tone-mapped image appears more natural and beautiful, which is similar with the characteristics of a high-quality traditional image. in this study, the detail features in the brightest, darkest, and global regions of a tone-mapped image are extracted, as well as the natural scene statistics features in the luminance and yellow channels and the aesthetic colorfulness. by the application of a simple pooling strategy to the extracted features, a new blind tone-mapped image quality assessment (btmiqa) method has been developed and its high performance has been experimentally verified. there are, however, some aspects of the proposed btmiqa requiring further study. for example, there is the need to determine an appropriate luminance threshold for extracting the brightest and darkest regions from tone-mapped images with different scene contents. six different luminance thresholds are used to extract three types of the brightest and darkest regions for comprehensive distortion detection. an appropriate method for the initial extraction of the brightest and darkest regions using a single luminance threshold is required, and this can be followed by consideration of the high-level aesthetic features for the evaluation of the colorfulness of the image. additional general features can be extracted to further enhance the tone-mapped image quality assessment and its usability in high-dynamic-range-related applications."
"in this section, we evaluate the performance of the proposed protocol. we performed simulations using the ns-2 simulator for three protocols: mag-and lma-based subscription, and the proposed protocol. we performed our simulation study for the two topologies shown in fig. 8, where each wired link has a 10 mbps bandwidth and a 10 ms link delay. we used ieee 802.11k as the wireless mac-layer protocol in our simulation. we modified the ieee 802.11 model in the ns-2 simulator to support the l2 trigger operation defined in ieee 802.11k. we set the data rate as 11 mbps and the multicast source generated cbr packets with a size of 1,000 bytes and a bit-rate of 512 kbps. the pim-sm multicast protocol was used to construct a multicast tree. the difference between the two topologies was that there were direct paths from the multicast source to mags without the intervention of the lma in topology 1, while every multicast packet was forwarded to mags through the lma in topology 2. the metrics for performance comparison were the end-to-end delay of multicast data, the service disruption period, and the number of lost packets during handovers. we defined the service disruption period as the difference between the reception time of the last packet from the pmag and that of the first packet from the nmag. we ran the simulations with five different random seeds for each topology and calculated the average end-to-end delays of multicast data, the service disruption periods, and the number of lost packets. the mn's handover occurred once in each scenario. table 2 shows the average service disruption periods and the average numbers of lost packets during a handover by the three protocols. mag-based subscription shows the longest service disruption period, while the proposed protocol shows the shortest one, as we noted in section iii. since a longer disruption period increases the number of lost packets, mag-based subscription shows the largest number of lost packets, while the proposed protocol shows the smallest number of lost packets. an interesting result is that only one packet is lost with the proposed protocol in both topologies. this is a result of the tunneling mechanism between the pmag and nmag in the proposed protocol, where in-flight packets to the pmag are forwarded to the nmag (described as \"tunneled packets\" in figs. 9c and 10c) . fig. 9 depicts the end-to-end delay of the three protocols in topology 1. the figure shows that mag-based subscription and the proposed protocol have a much lower end-to-end delay than the lma-based subscription in topology 1. this is because in topology 1, there is a direct path from the multicast source to mags, whereas there is no such path between the source and an lma. the figure also shows that the proposed protocol outperforms mag-and lma-based subscription in terms of lost packets, because it tunnels packets between mags during handover."
"during tone mapping, the luminance values in the middle range of the hdri are slightly compressed to better preserve the details in the middle luminance region. however, the luminance values in the highest or lowest range are significantly compressed, resulting in significant loss of details in the highest or lowest luminance region. in other words, the detail distortion mainly occurs in the brightest or darkest region of tmi [cit] . the detail information entropy could thus be extracted from the brightest and darkest regions to evaluate the local detail distortion."
"one example of ran functional split specifications is the enhanced cpri (ecpri) [cit], where a number of solutions have been defined, which, compared with cpri, reduce fronthaul capacity requirements between the cus and the rus while still enabling limited complexity and footprint of traditional base stations and providing sharing of both processing hardware and housing facilities."
"due to the high capacity required by fronthaul traffic, traffic grooming can be beneficial, i.e., different fronthaul flows originating from various rus at the cell sites can be aggregated into one (or a few) lightpaths and transported toward their cus. this can be convenient, especially if the aggregated fronthaul flows are destined toward the same cu pool. however, fronthaul traffic grooming is performed at the cost of introducing additional latency due to the switching of multiple traffic flows in the grooming node and inserting them into a single lightpath at the output of the node."
"as suggested by the video quality expert group (vqeg) [cit], the pearson linear correlation coefficient (plcc), spearman rank-order correlation coefficient (srocc) and kendall's rank-order correlation coefficient (krocc) are used to validate the performance of the proposed iqa method. the plcc is an indication of the prediction accuracy, while the srocc and krocc are measures of the prediction monotonicity. for a good objective iqa method, the plcc, srocc, and krocc should all be close to unity."
"the first evolution of ran is represented by c-ran, where digital processing is performed in cu pools, which are located in common sites (e.g., access cos, main cos, or even the core cos) and shared by several rus. although cu centralization in c-ran enables capex/opex savings compared with traditional distributed ran (d-ran), it introduces new challenges due to the high-capacity (up to tens of gbit/s per cell site) and low-latency (i.e., below a few milliseconds 4 ) fronthaul traffic, exchanged between a cu and its corresponding ru and transported via a cpri interface. 5 for this reason, despite the success of cpri, many network operators have started to question its suitability, especially in view of the massive small cell deployment and traffic increase envisioned for 5g [cit] . as a matter of fact, 5g small/micro/pico-cells \"densification\" will induce serious scalability issues in the fronthaul traffic transport, mainly due to the fact that fronthaul traffic is typically transported at a fixed line rate, which is independent of the end-users' transported traffic. thus, alternative solutions for the ran functional separation are now under analysis in various consortia [cit] and standardization bodies, e.g., [cit] working group [cit], and they are often referred to as ran functional splits."
"we focus on optical access-aggregation networks used for the backhauling of mobile traffic. as shown in fig. 2, rans fig. 1 . enb functional separation in 5g networks. in this paper, we assume that the du is co-located with its corresponding cu. 1 minimizing the number of active pools is an indirect minimization target to enable reduction of network opex, as the energy consumed at cu pools. 2 note that operators deploying otn for fronthaul/midhaul transport are already working on optimizing today's otn technology to fit with 5g service requirements, e.g., to reduce mapping latency from 10 μs to around 1 μs or less through the so-called mobile-optimized otn [cit] . include several cell sites (css), i.e., enodebs, and a set of central offices (cos) of different hierarchical levels, which are organized in \"ring-and-spur\" topologies and consist of access cos, main cos, and one core co, which represents the ran point of presence (pop) and the interface toward the core network."
"the main contributions of this paper are as follows: (1) after providing a schematic overview of different ran split solutions, we model the impact of fronthaul transport solutions, with particular focus on the impact of traffic grooming, on the tolerated fronthaul latency; (2) we define the dynamic cu placement/handover (dcph) problem in otn-over-wdm access-aggregation networks and propose an adaptive algorithm for this problem, namely, the maxc-h algorithm, which minimizes the number of active pools while achieving low network blocking; (3) through a simulative study, we analyze the impact of (i) cu handover, (ii) traffic grooming, and (iii) traffic bifurcation on the c-ran performance, evaluated in terms of cu consolidation, latency, and number of lightpaths."
"pmipv6 is designed to provide network-based mobility management support to mns in a topologically localized domain. in pmipv6, an mn is not required to participate in any mobilityrelated signaling, and proxy entities in pmipv6 (i.e., lmas and mags) perform all mobility-related signaling on behalf of mns. fig. 1 shows the system architecture of pmipv6. the role of the lma is similar to that of the home agent (ha) in mipv6. a mag detects an mn's movement and processes mobility-related signaling with the mn's lma on behalf of the mn. the mag establishes a bi-directional tunnel with the lma for each mn, and emulates the mn's home network on the access network."
"the espl-live hdr database was recently created by the researchers from the university of texas at austin, usa. a total number of 1811 ldris contained in the database were generated by three distinct types of hdr processing algorithm including tmo, multi-exposure fusion, and post processed. the subjective scores were obtained via a large-scale crowdsourcing online subjective study, where over 5000 raw opinion scores on the total images. the 747 tmis and its subjective quality scores extracted from the database are used in the experiments."
"t elecommunication networks are experiencing a rapid evolution to support emerging bandwidth-intensive and/ or low-latency internet services, such as video streaming, online gaming, augmented reality, internet of things, autonomous driving, etc., and to sustain the huge growth in the number of devices (e.g., smartphones, tablets, sensors, industrial machinery, etc.) connected to the network. the deployment and management of future, i.e., fifth-generation (5g), telecommunication networks are challenged by the extremely high performance required by 5g services, in terms of latency, availability, bit-rate, data loss, etc. such challenges not only have an impact on the radio interface between enodebs and endusers in the mobile long-term evolution (lte) network but also affect the deployment of the underlying radio access network (ran), which supports traffic aggregation from enodebs and its transport toward the core network infrastructure."
"in contrast to lma-based subscription, the proposed protocol does not need to exchange pbu and pback messages after l2 handover is completed, since multicast data is directly delivered via a mag. therefore, the proposed protocol reduces additional bandwidth usage while achieving seamless multicast service during handover. fig. 5 does not indicate exactly when the l2 trigger occurs or how the pmag obtains the ip address of the nmag after receiving the l2 trigger. in order to clarify these points, we apply the proposed protocol to the ieee 802.16e [cit] and ieee 802.11k [cit] networks. the pbs notifies the pmag of the mn's new base-station id (bsid) and the pmag resolves the nmag's address using the new bsid. to this end, mags should be able to match the bsid of a bs with the ip address of the corresponding mag. the mapping can be configured manually by the network administrator or by using an access router information resolution protocol such as a candidate access router discovery (card) protocol [cit] . after the l2 handover, the initiation period is completed, and the nbs triggers the nmag to make it forward buffered packets to the mn by sending a fast neighbor advertisement (fna) to the nmag."
"in the overlay case [see fig. 4(b) ], fronthaul traffic is not groomed, and each fronthaul flow between an ru and its corresponding cu is routed over a dedicated lightpath. therefore, no switching latency is required in this case, and, with reference to the example in the figure, the overall latency in the overlay case corresponds to"
"underexposure or overexposure of a tmi may also affect its naturalness. a high-quality tmi appears natural and satisfies a number of characteristics of natural images. the nss features are thus used to evaluate the distortion of the naturalness of tmi. as illustrated in fig.6, the tmid database [cit] contains three types of tmis generated by three different tmos. the tmi in fig.6(a) is normally exposed and of higher subjective quality, while those in figs.6(b) and 6(c) are respectively overexposed and underexposed and of lower subjective qualities."
"in comparison with backhaul, fronthaul traffic has more stringent requirements in terms of both capacity and latency. moreover, according to the selected ran split, i.e., the ecpri interface as in fig. 3, it can be either packetbased or circuit-based; hence, it can be proportional to or independent from the actual amount of user traffic (i.e., the backhaul), respectively."
"considering the scale and distortion types of tmid and espl-live hdr databases, the two databases were combined, named as the tmid-espl database which consists of 867 tmis. the training set contains tmis from tmid as well as espl-live hdr databases, but to be fairer, the training set does not contain images having same image content with that in the testing set, that is, the training set and testing set are not overlapped thoroughly. the performance of the proposed btmiqa method was also compared with those of other state-of-the-art methods on the tmid-espl database, as listed in table 8 . noted that the pooling strategy of the methods in this table are all the machine learning methods, such as support vector machine (svm), rf. here, rf is used as the common pooling strategy of these methods to train for a prediction model. it is also seen that the proposed btmiqa still performed better than the other methods on the tmid-espl database."
"we consider a multilayer otn over wdm network as underlying transport technology, so our algorithm must perform a grooming, routing, and wavelength assignment (grwa) in an otn over a wdm aggregation network and explore the interaction of grwa with cu placement to reach the objective of minimizing the average number of active pools, 1 i.e., nodes hosting cus, while achieving a satisfactory blocking probability. adopting a multilayer otn over wdm transport architecture to perform fronthaul traffic grooming has an impact on the latency between cus and rus, 2 which plays a key role in the cu placement. in turn, location of the cus influences the amount of fronthaul traffic inserted in the network. therefore, latency has a direct impact on network resource utilization and cu consolidation. in our previous work [cit], we investigated dynamic cu placement for cu consolidation, 3 but the location of a cu could not be modified during operation (e.g., if it is receiving traffic from an ru). in this paper, we consider also the case in which cus can be moved during their activity, i.e., we allow cu handover."
"average number of lightpaths: finally, fig. 7(c) shows the average number of active lightpaths in the four cases. as expected, for increasing b s, λ av increases for all four algorithms and saturates to a maximum value. however, the motivation for this increase is different in the various cases. specifically, in the overlay case, grooming can be performed only for backhaul traffic, as dedicated lightpaths are provisioned for fronthaul transport between rus and their cus. therefore, when less cu pools are activated [e.g., around five active pools for lower loads, as shown in fig. 7(a) ], typically in medium-higher network stages (i.e., main cos or the core co), grooming backhaul demands is less frequent. then, as b s increases, there is more opportunity for backhaul traffic grooming, as cus are placed in lower network stages. on the other hand, when fronthaul traffic grooming is allowed (i.e., in maxc-h, nonbifurcated, and t-constrained cases), a higher number of shorter lightpaths are typically needed to efficiently exploit network capacity and obtain cu consolidation at higher network stages at the same time. this behavior is more evident for the t-constrained and especially for the maxc-h cases, as the opportunity for traffic bifurcation provides higher flexibility in performing traffic grooming."
"where x pools,d is a binary variable, equal to 1 if a new pool (i.e., a node hosting only the cu for demand d) is activated, whereas variable n lightpaths,d represents the number of new lightpaths established to accommodate demand d. the parameters c p and c l represent the cost, expressed in relative cost units, of one cu pool and one lightpath, respectively. as the relative values of these two parameters drive the trade-off between cu centralization and demand blocking, and, due to the fact that, in this paper our main focus is on the minimization of cu pools, we set c p ≫ c l (e.g., c p 100 · c l ) so as to privilege cu centralization."
"provision the first task of the grwa step is to perform grwa for fronthaul traffic, due to the fact that fronthaul has more stringent requirements in terms of latency and required network capacity. the k shortest (i.e., best-cost) paths between c d and the candidate cu node are calculated using a yen algorithm, and these k grwa solutions are inserted in a list k (lines [cit] . the main cost metric used in our algorithm is the hop count. however, to favor the utilization of the residual capacity in already provisioned lightpaths, costs are assigned to a given lightpath edge considering the number of physical links it traverses, divided by 2. 12 moreover, to discourage unnecessary grooming, we assign to grooming edges a cost equal to 0.6. the value 0.6 allows us to break the tie if, when applying the yen algorithm, equalcost paths are obtained between a short route where a new lightpath must be established and a longer route reusing existing lightpaths. furthermore, note that, when fronthaul traffic for a new demand is routed, and there are already existing demands from the same cu, the different fronthaul flows can be transported along parallel lightpaths between the ru-cu pair. in general, these lightpaths can be routed along distinct physical paths; therefore, in the first version of the maxc-h algorithm, we assume fronthaul traffic can be physically bifurcated. however, we also consider a variation of the maxc-h algorithm, where fronthaul traffic bifurcation is not allowed. in the case that one or more additional lightpaths are needed between an ru-cu, which already exchange fronthaul for other existing demands, the new lightpaths must be routed along the same physical path of the existing ones, although they will use distinct wavelengths."
"the nmag that receives the hi message joins the multicast group described in the hi message. after that and after the mn connects to the nmag, the mn receives tunneled multicast data and then new multicast data is delivered via the nmag."
"the proposed protocol has the shortest disruption period needed for l2 handover. it does not need the binding update delay, in contrast to the mag-and lma-based subscriptions. in addition, the proposed protocol reduces the number of lost packets by forwarding and buffering at the new mag while the mn performs a handover. although it needs additional signals for hi and hack messages, it does not increase the service disruption period. we can redefine the service disruption period in each protocol shown in fig. 7 as follows: table 1 shows the important multicast performance metrics of mag-and lma-based subscription, and the proposed protocols. since mag-based subscription requires both a multicast tree join delay and a binding update delay, it has the longest service disruption and as a result of this, its packet loss rate is also high. lma-based subscription achieves both a shorter service disruption period and a lower packet loss rate than mag-based subscription, since it does not require a multicast tree join delay. however, it still needs a binding update delay. the proposed protocol has the shortest service disruption period, because it does not need both a multicast tree join delay and a binding update delay. moreover, its packet tunneling mechanism further reduces the packet loss rate. as shown in the table, the proposed protocol provides both a short service disruption period and a low packet loss rate while providing an optimal path length."
"in the example of fig. 4(a), two grooming nodes are traversed by fronthaul flow a, where fronthaul traffic 6 note that, in the otn case, we assume that fronthaul flows can also be groomed with backhaul traffic."
"moreover, for the various flavors of the maxc-h algorithm, we consider k 10 as the number of shortest-path grwa solutions to be evaluated in algorithm 1, as higher values of k do not provide relevant gains while negatively impacting complexity. for the t-constrained maxc-h algorithm, we set t 0.5 s to impose that, on average, each demand undergoes at most one cu handover (note that the mean demand duration is 1 s)."
"in this paper, to evaluate the impact of traffic grooming on fronthaul latency and, in turn, on cu centralization, we consider two different solutions for the fronthaul traffic transport [cit], i.e., (1) otn, where fronthaul flows between any ru and its corresponding cu can be groomed with other traffic into shared lightpaths, 6 which can be initiated/terminated also in intermediate nodes along the ru-cu path (namely, we consider multihop grooming for fronthaul traffic, assuming an otn-over-wdm network architecture), and (2) overlay, where each fronthaul flow is transported over a dedicated lightpath between the ru and the corresponding cu (i.e., we only allow single-hop grooming for the fronthaul traffic between an ru-cu pair)."
"all the experimental results on the tmid, espl-live hdr and their combination tmid-espl databases showed that the proposed btmiqa method achieved the highest performance. the proposed method is thus highly consistent with the hvs, enabling efficient extraction of features for high-performance hdri quality assessment. considering its additional benefit of low complexity, the proposed btmiqa method is more practicable than other tmi quality assessment methods."
"in this section, we propose an efficient handover protocol for multicast subscribers in the pmipv6 domain. the design goals of the proposed protocol are as follows. first, protocols should show low end-to-end delay, as with mag-based subscription, which uses the optimal path. to achieve this goal, the mag joins a multicast group in advance so that multicast packets can be delivered through the mag. this reduces the end-to-end delay of multicast data and thus the proposed protocol has lower end-to-end delay than lma-based subscription. second, with mag-based subscription, multicast packets are delivered through the optimal path from the source to an mn. however, the mag needs to join a multicast group whenever a new mn connects to it, which causes a long handover disruption period and high packet loss rate. lma-based subscription has a shorter service disruption period, but it cannot completely eliminate multicast data losses during handovers. the proposed protocol aims to achieve a shorter service disruption period than lma-based subscription and to minimize packet losses during handovers."
"c-ran provides significant capex/opex savings, mainly enabled by simplified antenna architecture and sharing of processing resources and housing facilities among different bbus, and can effectively support advanced coordination techniques, such as coordinated multipoint (comp). however, c-ran requires a large amount of fronthaul traffic between bbus and rrhs, which is carried through cpri interfaces [cit] . moreover, this traffic must be transported under low latency constraints, e.g., of the order of a few ms. due to these high-capacity and low-latency requirements, multilayer optical networks based on otn over wavelength division multiplexing (wdm) are being deployed for the realization of c-rans [cit] ."
"in the otn case [see fig. 4(a) ] grooming of fronthaul traffic is allowed, but every time a grooming node is traversed, a fixed latency contribution equal to t sw must be considered. for the example in fig. 4(a), we also show the overall set of latency contributions for fronthaul flow a (i.e., between \"ru a\" and \"cu a\") in the otn case, corresponding to"
"note that, although only backhaul traffic demands are randomly generated and taken as input of the dcph problem, in general, once a cu location is selected for the ru source of the backhaul demand, one fronthaul traffic demand also has to be routed from the ru to the cu together with the backhaul demand between the ru and the core co. in this context, for a given backhaul demand originating from a cs c, two special cases may arise according to note that the switching latency contribution shall be accounted for also in the case that fronthaul traffic is groomed with backhaul traffic only. 8 the term \"demand\" is used in this paper to identify how we model traffic generation. in other words, in our model, two or more demands can originate from the same ru, but they represent the variation of the overall mobile end-user traffic, which is aggregated at the cs. 9 note that, in this paper, we only consider uplink traffic, though similar considerations can also be drawn for downlink or bidirectional traffic. the location selected for the cu; i.e., (1) in the case that the cu is co-located with the ru, only the backhaul demand needs to be routed, and (2) if the cu is located at the core co, only the fronthaul demand is routed."
"it is worth noting that backhaul blocking probability (not shown as a figure) is kept below a satisfactory value of 1% for all values of b s, especially for the t-constrained and maxc-h cases. however, due to the inefficient utilization of lightpath capacity, in the overlay case blocking probability is below the 1% threshold only for higher loads. though counterintuitive, this behavior is motivated by the fact that the primary objective of the algorithms is to consolidate cus, which is easier for lower loads. instead, for higher loads, cus are typically placed at lower network stages, leading to lower capacity requirements for fronthaul traffic transport."
"the basic mechanism of pmipv6 is as follows. when an mn is connected to a new mag, access authentication is performed using mn-identification (mn-id). after the mag obtains the mn's profile, it sends a proxy binding update (pbu) message including the mn-id to the mn's lma on behalf of the mn. when the lma receives the pbu message, it sends a request to the authentication, authorization and accounting (aaa) server to check whether or not the sender of the pbu message is an authorized host. if the sender is an authorized mag, the lma accepts the pbu message and responds to it with a proxy binding acknowledgment (pback) message including the mn's home network prefix option. finally, the lma establishes a route for the mn to the mag over the bi-directional tunnel."
"the rest of the paper is organized as follows. section ii overviews rans and describes the technological/architectural solutions adopted to implement c-ran. in section iii, we provide details on the impact of latency on c-rans and how latency is affected by traffic grooming. in section iv, we introduce the dcph problem in otn-over-wdm accessaggregation networks and describe the heuristic algorithm designed to address the problem in section v. illustrative numerical results are presented in section vi, whereas section vii draws the conclusion."
"between \"ru a\" and \"cu a\" is groomed with fronthaul flows b and c in grooming nodes 1 and 2, respectively. 7 moreover, three propagation latency contributions are required and accounted for the propagation over the physical routes connecting ru a and grooming node 1 (τ 1 ), grooming nodes 1 and 2 (τ 2 ), and grooming node 2 and the cu pool (τ 3 )."
"two-layer separation of enbs, assuming co-location of the du and cu, and we refer to this element as the cu (this co-location is commonly assumed in various architectures [cit] ). therefore, only fronthaul and backhaul traffic are considered in this paper. note that, according to the adopted ran split, various interfaces have been defined for fronthaul transport, such as the enhanced cpri (ecpri) [cit], described in detail in section ii.a. as a future work, we will target the study of du/cu placement by considering the more flexible three-layer separation of enbs."
"as it is difficult to characterize a cost function capturing the combined impact of cu centralization and network capacity requirements, to compare different solutions of the dcph problem for a new incoming traffic demand d, in this paper, we define a generic cost function, which takes into account the activation of a new pool (i.e., in a node without other active cus) to host the cu for demand d and the establishment of new lightpaths to provision the demand, i.e.,"
"in this paper, we propose a new multicast protocol for proxy mobile ipv6. in the proposed protocol, mag joins a multicast group in advance by detecting the l2 trigger and exchanging hi and hack messages so that multicast packets can be delivered through mag. the proposed protocol has a shorter service disruption period and a lower packet loss rate than mag-and lma-based subscription. it also decreases end-to-end delay, since it provides an optimal delivery path. according to our performance study, we found that the proposed protocol shows improved performance in terms of end-to-end delay, service disruption period, and the number of lost packets during a handover period."
"to the best of our knowledge, no existing work has evaluated the interplay between fronthaul latency and traffic grooming on the cu placement in a dynamic otn-over-wdm access-aggregation network. besides this, in our work, we also consider how the flexibility brought by cu handover has an impact on c-ran resource utilization."
"the dynamic range that human visual system (hvs) is capable of perceiving in a scene (approximately 10 6 :1) is substantially greater than that of traditional low dynamic range (ldr) imaging technology, which can only capture about eight f -stops (256:1) [cit] . high dynamic range (hdr) imaging, which is capable of capturing the full range of light in a scene, has thus attracted increasing attention in recent years [cit] . however, the current high-cost of hdr display devices has limited their applications. tone-mapping technologies have consequently been developed to enable the visualization of high dynamic range images (hdris) on ldr display devices while maintaining the visual attractiveness of the images. this involves the conversion of hdris into corresponding low-dynamic-range images (ldris), referred to as tone-mapped images (tmis) [cit] . however, the utilized tonemapping operators (tmos) inevitably cause information loss and quality degradation due to the reduction of the dynamic range of the converted images. despite this inferiority of a tmi to the original hdri, tmi generated by a suitable tmo is still more visually pleasing than a traditional ldri [cit] . therefore, it is necessary to develop appropriate tmi quality assessment methods to improve the processing of hdris and expand their applications."
"in the experiments, we compared the performance of the proposed method with the existing state-of-the-art iqa methods on the tmid database [cit], espl-live hdr database [cit] and their combination. the tmid database was developed by the researchers from university of waterloo, canada. fifteen sets of images are contained in the database, with each set comprising a hdri and its eight tmis generated by eight different tmos. twenty subjects were requested to rank the eight tmis of each set from the best to the worst, using scores between 1 and 8. the twenty scores of each tmi was then averaged and adopted as its final score."
"several multicast protocols based on pmipv6 have been proposed [cit] . as mentioned in section i, they are categorized as lma-and mag-based subscription protocols. figs. 2 and 3 show the respective flows of mag-and lma-based subscription."
"in this paper, we focused on the dynamic placement of cus in optical access-aggregation networks, with the objective of minimizing the number of active cu pools. to this end, we defined the dynamic cu placement/handover (dcph) problem in wdm access-aggregation networks and provided a latency-aware heuristic algorithm, namely, maxc-h, for the cu placement/handover and grwa of mobile traffic demands. we also evaluated how c-ran performance is influenced by maxc-h algorithm features, i.e., (i) cu handover, (ii) traffic grooming, and (iii) traffic bifurcation. we found that, especially for higher loads, fronthaul latency plays a critical role in reducing the number of active cu pools. advanced sharing of baseband processing resources can be obtained if the c-ran is capable of performing cu handover and, especially, if multihop grooming capabilities are enabled for fronthaul transport, e.g., by adopting an otn-over-wdm network architecture. as a future work, we plan to extend our study also considering the three-layer separation of enbs into ru, du, and cu."
"we define a node in the network (either a cs or a co) as an active pool if it hosts at least one active cu, which can be associated with an ru in another node or to the co-located ru, in the case that the active pool is itself a cs. 10 as we assume a cu is always hosted at the core co and is associated with a co-located ru, by definition, the core co is one active pool."
"average number of active pools: figure 7(a) shows, for the four cases, the average number of active pools (p av ) as a function of the served backhaul traffic (b s ). note that two benchmark cu placement solutions, i.e., fully distributed and fully centralized (not shown in the figures), corresponding to the case of cus co-located with their rus at all mcs and to the case with only one cu pool at the core co, would produce a normalized p av of 80 and 1, respectively."
"with these improvements, which support mobility in the ip layer, the demand for multimedia group communications such as mobile internet protocol television (iptv) and video conferencing has also increased. to support efficient multimedia group communications in mobile networking environments, many studies have sought to combine ip multicast with mobility management protocols. in particular, mipv4 and mipv6 provide two basic approaches for supporting multicast services to mobile nodes: foreign agent-based multicast (remote-subscription) and home agent-based multicast (bi-directional tunneling) [cit] . moreover, based on these two approaches, several mobile multicast protocols such as mobile multicast (mom) [cit], multicast by multicast agent (mma) [cit], range-based mobile multicast (rbmom) [cit], and timer-based mobile multicast (tbmom) [cit] have been proposed. however, since these mobile multicast protocols are designed for mipv4 or mipv6, and thus require mobile nodes to participate in message signaling, they cannot be applied directly to pmipv6."
"average fronthaul latency: the difference among the four algorithms in performing cu consolidation can be observed from another point of view in fig. 7(b), which shows the average latency between an ru and its corresponding cu pool, i.e., l av . in all cases, l av tends to decrease with increasing loads, due to the larger amount of fronthaul traffic inserted, which limits the opportunity for cu consolidation at the core co or, in general, at nodes in higher layers of the network. as is evident from the figure, the lowest values of l av are obtained independently from b s, with the overlay algorithm, when distributed placement of cus (i.e., closer to rus) is necessary to face network congestion at higher stages of the network. moreover, at lower loads, the other algorithms provide comparable values of l av, although, in the nonbifurcated case, latency is slightly higher, mainly due to the fact that lightpaths are typically routed over longer paths to maintain nonbifurcated traffic. interestingly, at a certain value of b s (i.e., around 1000 gbit/s), l av becomes lower for the nonbifurcated case, in comparison with t-constrained and maxc-h algorithms, showing that the impact of traffic bifurcation on ru-cu latency is more relevant than the limit in the number of cu handovers."
"before we describe the operation of the proposed protocol in detail, we first define a new mobility option called the multicast option. the multicast option is included in the mobility option field of handover initiation (hi) messages. fig. 4 illustrates the format of the multicast option. as shown in the figure, the multicast options include an 8-bit type field, an 8-bit length field, a 16-bit reserved field, and a field for multicast group addresses. the type field represents the type of mobility option. the length field contains an 8-bit unsigned integer indicating the length of the option in octets, excluding the type and length fields. this field must be set to 16n+2, where n is number of addresses in the multicast addresses field. the multicast addresses field is filled with the multicast group addresses to which an mn currently subscribes."
"considering the expected explosion of 5g traffic and massive deployment of small cells [cit], the aggressive rrh-bbu separation in the original c-ran architecture is expected to face serious scalability issues due to fronthaul requirements. therefore, more flexible functional separations are under study [cit], which are referred to as ran functional splits. such flexible solutions are envisioned as outstanding candidates to help in supporting high-bandwidth/low-latency fronthaul traffic and enable effective network reconfiguration and re-adaptation."
"moreover, as we will explain in detail in section v, upon the arrival of a new traffic demand, in this paper we reconsider the cu placement to find a better location for that cu also in the case that one or more ongoing demands exists toward that cu; i.e., we allow cu handover, which is a main novelty of this paper. note that this requires the live migration of \"stateful\" virtual machines. supported by the recent advances in network function virtualization (nfv), we speculate that such cu handover can be performed in the form of live virtual machine migration, in line with [cit] ."
"for lower values of b s, the average number of active pools approximates the lower bound of one pool for all the algorithms, i.e., only the pool at the core co is sufficient for the whole set of rus. the overlay algorithm is an exception to this, i.e., a few more pools are activated in this case, due to the fact that using dedicated wavelengths for fronthaul transport corresponds to a higher network capacity requirement and consequently to lower opportunities for cu consolidation. on the other hand, for increasing b s, the values of p av increase in all cases. as expected, the lowest p av is obtained, in general, for the maxc-h case, which allows the highest flexibility in performing cu handover and grwa of the backhaul and fronthaul traffic. on the other hand, when adopting the overlay fronthaul transport, the highest average number of active pools is obtained, due to the fact that using dedicated wavelengths for fronthaul transport leads to underutilization of network capacity. indeed, to pursue cu consolidation, in the overlay case, direct lightpaths are typically deployed on longer physical routes between the rus and higher stages of the accessaggregation network. consequently, this quickly leads to network congestion, especially in higher hierarchical levels of the network (i.e., in links interconnecting the main cos and the core co) and thus forces new cus to be placed closer to the corresponding rus, so as not to introduce further fronthaul traffic in the network. the difference between the two fronthaul transport solutions is more evident for increasing load, when the importance of traffic grooming is more relevant. as expected, also in the case of a nonbifurcated algorithm, p av is higher compared with maxc-h, due to the fact that multiple demands originating from a given ru must be routed along the same physical route. this is not always possible, especially for increasing load; therefore, in order to be able to accommodate new demands, cus are often placed at lower stages of the network or even co-located with the rus. moreover, considering the t-constrained algorithm, the number of active pools is comparable with the one in the maxc-h case, except for very high traffic, when the limit of the number of cu handovers per ru plays a role."
"note that, in general, the propagation delay required in the overlay case is different from the sum of propagation delay contributions needed in the otn case, mainly for two reasons: (1) in the otn case, aiming at efficiently exploiting network capacity may lead fronthaul traffic to be transported over longer end-to-end routes between the ru and the cu, due to the presence of grooming nodes, which are not necessarily in the shortest physical path between the ru and the cu; (2) in the overlay case, using dedicated lightpaths for each fronthaul flow may lead to congestion of some network links; hence, direct lightpaths between rus and cus might be routed over longer routes compared with the shortest path."
"in this paper, we propose an efficient multicast solution for pmipv6. the proposed protocol aims to provide an optimal routing path for multicast data and to reduce packet loss and the service disruption period during handovers. to this end, in the proposed protocol, a mag joins a multicast group and initiates a handover in advance. to evaluate the performance of the proposed protocol, we performed simulations using the network simulator (ns)-2. according to the performance study results, compared to lma-and mag-based subscriptions, the proposed protocol provides a shorter service disruption period and lower end-to-end delay by using an optimal routing path and it reduces the number of lost packets during the mn's handover."
"the legacy ieee 802.11 standard does not define the l2 trigger operation. however, one of the extensions of the ieee 802.11k standard extends the ieee 802.11 operation to support the l2 trigger. we apply the proposed protocol over ieee 802.11k networks, as shown in fig. 6b . in ieee 802.11k networks, the l2 handover consists of the three steps of the l2 trigger as well as an association. except for the l2 trigger, the remaining operations are similar to those of ieee 802.16e. the previous ap (pap) lets a mn prepare to move to another ap (hi). the mn sends a neighbor list request to the pap, which responds with a site report that contains the list of neighbor aps. the pmag, which manages the pap, sends an hi message to the mags administrating the aps listed in the site report. then, the mn chooses a new ap from the list and makes an association with it. after the connection between the mn and ). in addition, in the case of magbased subscription, if the nmag has not joined a multicast group to which a mn subscribes, an additional join delay (d j o i n ) is needed. since the joining delay depends on the current state of the multicast tree, it can be a crucial factor for the whole handover disruption period of the mn."
"a. detail features in brightest, darkest and global regions fig.2 (a) and 2(b) show two tmis generated by two different tmos, selected from the tmid database [cit] . the tmi in fig.2 (b) clearly appears overexposed, with the resultant loss of much detail. in contrast, the tmi in fig.2 (a) preserves practically the complete information. the subjective score of the tmi in fig.2 (a) is thus higher than that of the tmi in fig.2(b) . in many iqa methods, information entropy is used to measure the amount of information contained in the image [cit] . table 1 compares the global entropies of the two tmis in fig.2 . as can be observed, the entropy of the tmi in fig.2 (b) is higher than that of the tmi in fig.2 (a), contrary to subjective perception. this illustrates the inaccuracy of measuring the subjective quality of an image based on only the global entropy."
"the dynamic cu placement/handover (dcph) problem in the wdm access-aggregation networks can be stated as follows. given (1) a hierarchical multistage access-aggregation network topology, represented by a graph gn, e, where n is the set of nodes (including cos and css) and e is the set of optical fiber links, and (2) random dynamically generated backhaul traffic demands 8 originating from css and directed to the core co 9 decide the placement/handover of cus and the grwa of backhaul and fronthaul traffic, minimizing the average number of active pools in the network, constrained by (i) the network link capacity (i.e., wavelength capacity and number of wavelengths per fiber) and (ii) the maximum fronthaul latency."
"the main objective of the dcph problem consists of minimizing the average number of active cu pools, weighted by the amount of time when each of them is actually serving a demand. this objective captures the benefits of resource sharing provided with the c-ran approach; i.e., it gives a measure of the required opex. for example, assume two cus are co-located in the same node (i.e., the same cu pool) for a given amount of time. in this case, the average number of active nodes is halved with respect to the case where the two cus were located in two different locations for the same period of time, as two different nodes (i.e., two different cu pools, each hosting only one cu) would be activated. however, pursuing cu centralization (e.g., concentrating as many cus as possible at the core co) leads to a huge increase in network capacity requirements, as a high amount of fronthaul traffic is inserted in the network, thus possibly causing higher demand blocking."
"similarly, fig. 10 shows the end-to-end delay in the case of topology 2. as shown in the figure, all the protocols have comparable end-to-end delays, since every multicast packet is forwarded to mags through the lma in topology 2. however, the proposed protocol causes a smaller number of lost packets than the number of lost packets during handover. fig. 9 . end-to-end delay of each protocol in topology 1. mag: mobile access gateway, lma: local mobility anchor. mag-and lma-based subscription during handover, due to its packet tunneling. in both figs. 9 and 10, we can see that tunneled packets in the proposed protocol suffer from a long endto-end delay, since they are buffered at the nmag until the target mn completes an l2 handover. fig. 11 illustrates the impact of the access delay between the lma and mag in the service disruption period measured in topology 1 and 2. we measure the change of the service disruption period with the increase of the link delay between the lma and mag. fig. 11 shows that in each topology, the service disruption periods in both mag-and lma-based subscription increase linearly as the access delay between lma and mag is increased. in contrast, the proposed approach is not affected by the increase of the access delay."
"the objective of the dcph problem is to minimize the average number of active pools in the network, while limiting demand blocking probability. to this end, the heuristic algorithm developed in this paper aims at maximum cu centralization and, if it is convenient to provide higher centralization, allows cu handover. for this reason, it is called maximum centralization with cu handover (maxc-h). an incoming demand d is characterized by a series of parameters, i.e., (1) its source ru located at cs c d, (2) the required backhaul traffic b d, and (3) the demand duration t d . upon arrival of demand d fc d, b d, t d g, the maxc-h algorithm also takes as input the current network state, consisting of the set of all the deployed cus along with their location, the installed lightpaths, and their residual capacity as well as the residual capacity in all the optical fiber links in the network. then, the following main steps are executed, which are also detailed in algorithm 1. variables used in the procedure are summarized in table i. (1) identify optimal cu location. a list of candidate nodes is created to search for the optimal cu location for demand d; the different solutions, i.e., the candidate nodes in the list, are sorted considering their cost as in eq. (3) (lines 1-8). note that trivial solutions, i.e., locating the cu at the cell site or at the core co, are also included in a list z. (2) cu placement/handover. after computing the amount of required fronthaul traffic f d, which depends on the backhaul traffic b d (line 9), the list of candidate cu locations is scanned, starting from the first node in the list (lines 10-44). first, the algorithm checks if a cu is already present in the network for the ru at cs c d (line 12). if such a cu is present, and it is already located at the optimum location (i.e., the first node in list z), the available capacity in the lightpaths already used between the ru and the cu (for fronthaul traffic) and between the cu and the core co (for backhaul traffic) is decremented by f d and b d, respectively (lines 12-15). in such a case, a trivial grwa is performed for demand d, and the corresponding bandwidth values (f d and b d ) will be deallocated from the lightpaths after t d . note that, if the available capacity in one or more of these lightpaths is not sufficient to provision f d or b d, the demand is blocked, and the maxc-h algorithm is considered for a subsequent demand (lines [cit] . on the other hand, if a cu is already present for the ru at cs c d, but its location does not coincide with the optimum location, cu handover needs to be performed, 10 note that we assume cos also have a co-located cs; i.e., cos can also originate backhaul traffic demands directed to the core co. 11 note that in our numerical analysis, we do not explicitly simulate migration, as migration bandwidth for cu handover is negligible with respect to the amount of backhaul and fronthaul traffic."
"as shown in fig. 6, maxc-h always provides a lower number of active pools per demand, mainly due to the possibility of performing cu handover if it is convenient to improve cu consolidation. maxc-h and adaptive have comparable performance in terms of normalized p av, only for lower served traffic, confirming that maxc-h is able to better adapt to the dynamic changes of network traffic behavior. in other words, this demonstrates that the maxch algorithm is able not only to reduce the number of active pools but also support more user traffic, thanks to the opportunity of moving cus and consequently reducing the amount of fronthaul traffic, which might lead to network congestion. as a matter of fact, for the considered arrival rates, no demands are blocked in the maxc-h case. conversely, the adaptive algorithm provides higher blocking, i.e., of the order of 20%, even for medium traffic (e.g., 20 gbit/s per ru). 15 now we provide in fig. 7 the results for the different flavors of the maxc-h algorithm, as described in section v.a. this comparison allows us to quantify the impact of the various features of the maxc-h algorithm on network performance."
"in this paper, we focus on the development of adaptive algorithms for the dynamic placement of cus to enhance the utilization of processing and transport resources. for example, following the spatio-temporal dynamics of 5g tidal traffic, in low-traffic conditions several virtualized cus can be centralized at so-called cu pools located in higher layers of the metro-access network, so as to promote power savings and enhanced coordination; on the other hand, when traffic increases, cu pools can be located at lower layers, i.e., closer to antenna sites, to avoid excessive fronthaul traffic insertion. hence, the ability to dynamically reconfigure the cu location allows network operators to achieve the desired balance between baseband resource consolidation and network capacity utilization."
"therefore, a trade-off between capacity utilization and allowed fronthaul latency arises when performing traffic grooming and routing in c-rans, which, in turn, has an impact on the overall network blocking probability and cu consolidation."
"to clarify the impact of grooming on fronthaul latency contribution, we show an illustrative example in fig. 4 for the otn and overlay cases, considering the transport of fronthaul flows originating from three different rus, i.e., \"ru a,\" \"ru b,\" and \"ru c.\" in the example, we focus on the latency contributions considered for the fronthaul traffic between \"ru a\" and the corresponding cu, i.e., \"cu a,\" though similar observations can be drawn for the latency contributions for fronthaul flows originating from rus b and c."
"recently, there have been several attempts to support multicast services in pmipv6 [cit] . these are based on two alternative approaches, referred to as mobile access gateway (mag)-based subscription and local mobility anchor (lma)-based subscription. in mag-based subscription, when a mobile node (mn) connects to a new mag that has not yet joined the multicast group, the mag performs multicast tree joining. magbased subscription can provide an optimal multicast routing path to the mn, but it causes a large overhead for multicast joining. on the other hand, lma-based subscription has a smaller joining overhead, since the lma, which manages multiple mags, joins multicast groups instead of mags. however, this approach results in non-optimal multicast routing paths, since multicast data is always delivered through the lma even though there is a shorter path."
"although tone mapping affects the colorfulness of hdri, a tmi with good colorfulness affords visual perception. hvs is sensitive to luminance and saturation, which are thus important attributes for assessing the aesthetic quality of an image [cit] . fig.9 shows a tmi and its luminance and saturation maps, which are divided into nine patches by the rule of thirds grid. the luminance features, denoted by f 13 -f 21, and the saturation features, denoted by f 22 -f 30, can be combined to evaluate the aesthetics of the tmi [cit] . the following expressions will be applied"
"as can be observed from the additional experimental results presented in table 5, each of the three types of features performs better when combined with another feature, and the best performance is achieved when all three types of features are combined. this confirms that all three types of features are important to the quality of tmi. the simple pooling strategy applied to the features in this study also contributed to the higher performance."
"centralized-radio access network (c-ran) is a promising architecture to mitigate the aforementioned issues in 5g networks. in c-rans, the cell-site (cs) equipment is functionally separated into two elements, i.e., a remote radio head (rrh), also known as a remote unit (ru), which remains located at the antenna premises and is responsible for wireless signal transmission and reception, and a baseband unit (bbu), which performs baseband processing and which can be located remotely and centralized into common sites."
"in this section, we will first show the implementation details of our system, including the hardware and the software. then, we will describe the experiments to verify the operations of our system. finally, we will discuss some implementation issues when deploying our system. table 1 lists the hardware and software used in our prototype system. our mobile app is developed to run on apple ios. mesh beacons and mesh routers are implemented using the redbear development board [cit] because of its small size and the co-existence of ble and wi-fi interfaces. the application server is an ubuntu pc with a node.js web server. the cloud database is implemented using google firebase [cit], which also provides the user registration and authentication functionalities for our mobile app. our ble beacon network is deployed on the 8th floor of the engineering building in our campus. figure 7 shows the floor plan where we deployed four mesh beacons (the blue ones labeled a through d) and one mesh router (the red one). first, we describe the user interface of the mobile app because we use this to measure and display the response time. three screenshots of our mobile app are shown in figure 8 . after the user logs in and then manually presses the \"locate\" button, the central \"locationing\" circle with breathing light effect will appear as in figure 8a . during this locating period, the mobile app scans for the beacon messages sent by mesh beacons. as soon as a beacon message is received and the physical location is determined, the location name will show up as in figure 8b, which is \"tku building e location a\" in this case. meanwhile, the mobile app will broadcast the user's presence state into the bluetooth beacon network. in response to the presence message from the mobile app, the mesh router will send an acknowledgement message back to the mobile app. when the acknowledgement message finally arrives at the mobile app, a welcome message shows up and the check mark appears as in figure 8c, meaning the whole process has now completed. regarding the response time, the time (0.842 s) shown in figure 8b refers to the period starting from the user pressed the \"locate\" button, to the time when the physical location is determined at the mobile app. note that the screenshot in figure 8c was captured in another test. the received time (1.983 s) shown in figure 8c refers to the period starting from the completion of locating, to the instant that the mobile app received the acknowledgement message from the mesh router. first, we describe the user interface of the mobile app because we use this to measure and display the response time. three screenshots of our mobile app are shown in figure 8 . after the user logs in and then manually presses the \"locate\" button, the central \"locationing\" circle with breathing light effect will appear as in figure 8a . during this locating period, the mobile app scans for the beacon messages sent by mesh beacons. as soon as a beacon message is received and the physical location is determined, the location name will show up as in figure 8b, which is \"tku building e location a\" in this case. meanwhile, the mobile app will broadcast the user's presence state into the bluetooth beacon network. in response to the presence message from the mobile app, the mesh router will send an acknowledgement message back to the mobile app. when the acknowledgement message finally arrives at the mobile app, a welcome message shows up and the check mark appears as in figure 8c, meaning the whole process has now completed. regarding the response time, the time (0.842 s) shown in figure 8b refers to the period starting from the user pressed the \"locate\" button, to the time when the physical location is determined at the mobile app. note that the screenshot in figure 8c was captured in another test. the received time (1.983 s) shown in figure 8c refers to the period starting from the completion of locating, to the instant that the mobile app received the acknowledgement message from the mesh router."
"when the field of deployment is large, some of the mesh beacons will be many hops away from the mesh router. when the users are located around those remote mesh beacons, they may experience a longer response time owing to greater physical distance and higher packet loss rate. a practicable solution to this problem is to divide the large deployment area into several smaller subareas, and equip each subarea with its own mesh router. mesh routers, on the other hand, can connect to the application server through wi-fi or ethernet interfaces. the idea is depicted in figure 12 . therefore, the number of hops from any mesh beacons to its closest mesh router can be well-controlled. besides, mesh beacons can be configured not to rebroadcast the messages from different subareas they belong to, so as to reduce the total number of broadcast messages in the deployment field."
"in our view, only data resources within the permissive category facilitate reuse without negotiation, license alignment, or other burdensome tasks. all other categories have issues that hinder reuse."
"the remainder of this paper is organized as follows. in section 2 we will discuss related work. in section 3 we will describe the system architecture and explain the flow of operations. our implementation details, experimental results, and some implementation issues are described in section 4. finally, section 5 concludes our work."
"where iod is the abbreviation for integral optical density and iod t and iod c denote the iod of the test and control lines, respectively. g t and g c describe the gray intensity of pixels on the test and control lines, respectively. g 0 represents the mean gray intensity of the reading window area."
"where is the learning rate, hái data means the expectation of distribution defined by the training data and hái recon represents the expectation of distribution defined by the reconstruction model. similarly, the energy function of gaussian-bernoulli rbm is:"
"as for the training sample, we use 18 images with different levels of analyte concentrations (from low to high). the window size winsize is 13, and the dimension of input data is thus 171. the extracted region of interest (roi) is selected as 50 â 90 in each image, and accordingly, the number of training sample is 18 â 50 â 90."
"in the distributed computing framework based on hadoop or spark, the data query mainly includes two stages: ''before reading'' and ''after reading''. the '' before reading'' stage refers to the process in which the system reads data from the storage layer and loads the partitions into the memory of the compute node in parallel. in this process, the system needs to logically determine the data that needs to be loaded into the memory according to the spatiotemporal attributes of the data. in the ''read after'' phase, the system further refines the in-memory data according to the query conditions. the purpose is to determine whether the data meets the query conditions and retain the data that satisfies the conditions. when requesting data with index, the system first accesses and loads the index file, and logically determines the data partition to be loaded into memory [cit] . it then reads the data into memory through the custom ''filesplitter'' and ''filerecordreader'' classes. they define the form in which data is read and the strategy of data filtering. this avoids traversing the entire data set, which greatly reduces data retrieval time."
"according to the logical processing flow of data, a forestry big data management platform can be abstracted into five layers, which are data acquisition, storage, query, analysis and application, respectively. the data acquisition layer mainly refers to a process in which various data are acquired through different platforms and technologies and initially processed at the data collection end. at the storage layer, the main task is to design and improve the method that data is stored and indexed. the query layer mainly includes requesting and filtering data. the main task of the analysis layer is to analyze and mine forestry data to assist forestry decision-making. the application layer is to transfer traditional forestry information technology to the big data platform for high-performance computing based on the analysis of forestry big data. as shown fig. 4, it is a schematic diagram of a five-layer platform for forestry big data management. the forestry big data management framework can be deployed directly into a single computer cluster, or constructed based on cloud computing and related technologies."
"the query layer is the bridge between data and computing in the forestry big data framework. the system can reasonably allocate i/o resources and computing resources based on the query layer. and by scheduling and controlling the query request, it ensures the load balancing of the cluster, thereby improving the performance of the entire framework."
"of the 56 data resources we evaluated, 22 (39%) received between 4 and 5 stars, indicating that they met our broadest requirements for being reusable, which allowed for some caveats, only 10 (18%) received 5 stars meeting all parts (a-e) of the criteria. 23 (41%) of the resources received less than 3 stars, which is notable because even data resources for which the provider has reserved all copyrights can receive a score of 3 stars if the data covered by the license is easily accessible. 32 (57%) of the resources had 3 stars or less, meaning that a majority of resources had significant issues with even basic reusability. overall, average scores by licensing category were: permissive 4.5, restrictive 2.6, copyright 1.4, unknown 0.7, copyleft 3.0, and private pool 1.0."
"at present, the main storage method of the data is based on the relational database, the non-relational database and the distributed file system. taking full advantage of the different storage methods for data storage can reduce the pressure on the storage system and improve the efficiency of data requests. after the user makes a request to the system, at the query layer, the system needs to request data from the storage layer according to the storage structure of the data and the organization of the storage system, and store the required data in the memory, which can reduce unnecessary disk i/o. the query layer bridges that the system loads data from disk into memory, and is a configurable, pluggable component in the big data model."
"for data calculation and analysis, in big data systems based on hadoop or spark, it can ensure locality of data by partitioning data based on spatial attributes. this strategy can greatly reduce i/o consumption among nodes during data request and calculation. however, in the hadoop and spark kernels, there is no mechanism for controlling the location of the node where the data partition is located. therefore, it is impossible to ensure that the partitions are evenly distributed among the nodes in the cluster. when calculating the data, the system cannot fully utilize the computing resources in the cluster, and guarantee the load balance. this results in a low degree of parallelism in the calculations and affects the performance of the cluster. therefore, the system can select the optimal storage location for the data by considering the load pressure of each node for storing. it is also possible to ensure the reliability of computing resources by providing an ''available cpu'' aware strategy to solve the problem of load balancing and the problem of unstable computing parallelism."
"when the mobile app discovers a beacon message from a certain mesh beacon, it locates itself based on the major/minor values embedded in the message. in our design, the mobile app directly looks up a built-in location table to resolve the major/minor values, so the name of the detected location can be shown on the user interface immediately."
"at present, the visualization technology for spatial data is widely used, such as googleearth [cit] . the ability to visualize forestry data is an important function of the forestry big data framework. the main purpose of visualization is to present the results of big data system calculations and analysis to users. by combining with the forestry real-time monitoring system and the forestry decision support system, it provides great convenience for forestry data analysis and forestry management."
"in our current experiment, the uav exhibited horizontal drifting due to the influence of the steel cable and other external disturbances. this restricted the duration of the flight experiment. horizontal position control should be added to the uav control system to limit its horizontal movement. a high-accuracy positioning system is a prerequisite for horizontal position control. this will be considered in our future work. eventually, a free-flight experiment will need to be conducted to fully test the applicability of our uav."
"criteria a: is the license or terms of use in an easy-to-find location? is there one, unambiguous license, as opposed to multiple, conflicting versions? is the license standard?"
"(criteria a) we found that 24 (43%) resources used an explicit standard license and 22 (39%) used custom terms, 5 (9%) had inconsistent licensing information, and 5 (9%) had no licensing information. table 1 illustrates the count of resources by license type and the associated licensing category. resources with custom licensing language fell into several licensing categories: 12 were restrictive, 9 permissive, and 1 private pool."
"in this work, we investigate the altitude control design for a tail-sitter vtol uav equipped with turbine engines. in contrast to the electric motors used in previous uavs, the propulsion system of our uav is more powerful and can enable a wider range of applications for this type of uav. however, the dynamic system of the turbine engines is more complicated than the above-mentioned electric motors, and the system has an apparent time delay. therefore, the engine dynamics cannot be neglected in the altitude control design. we therefore design an integrated controller that considers the engine dynamics. in this control design, the rotor speed command is the control input. this is different from previous control designs in which the thrust force is the control input. the proposed controller consists of a pd control term and an acceleration feedback term. a stability analysis is performed and can be used to guide the parameter selection for the uav experiment. in addition, the controller is also designed to achieve specific gain and phase margins. we conduct uav experiments to demonstrate the effectiveness of our integrated controller. to the best of our knowledge, the altitude control and the stability analysis for this type of uav have not been considered before."
"because the type and structure of forestry data are complex, storing the data in database or local file system cannot meet the performance requirements. in practical applications, volume 7, 2019 it is necessary to apply big data technology to organize and store data reasonably. when requesting data, the data is read into memory at the minimum cost according to the storage structure. next, in this chapter, we study the technology of each layer."
"gold immunochromatographic strip, which is labeled with the colloidal gold nanoparticle, is on the basis of an immunochromatographic process that utilizes the high specificity of antigen-antibody reaction and provides rapid determination of target analyte. the gics, as shown in fig. 1, is formed by a variety of constituents including a sample pad, a conjugate pad, an absorbent pad and a nitrocellulose membrane on which the reaction occurs. there are generally two formats of gics, namely sandwich and competitive formats. here, we only discuss the sandwich format which uses two antibodies to bind the analyte in between. with the presence of an antigen in the sample, a sandwich-type compound is formed between the labeled antibody and the antibody immobilized on the membrane. after that, the red or purple red color caused by the accumulation of gold nanoparticle at the test and the control lines would appear on the membrane. particularly, the signal intensity of the test line is directly related to the concentration of the target analyte in the samples. therefore, the concentration of the target analyte can be assessed visually or by a reader system for quantitative analysis by monitoring the signal of the sandwich-type compound on the test line [cit] ."
"our previous work has shown that the overall engine model can be divided into two components: a linear dynamic model (rotor speed model), which characterizes the linear relationship between the rotor speed command and the rotor speed response, and a nonlinear static model (thrust model), which characterizes the nonlinear relationships between the rotor speed response and the engine thrust response."
"most vtol uavs are equipped with electric motors as discussed in the introduction section. the structures of those uavs are relatively simple, and they are typically lightweight. therefore, the uav experiments are relatively easy to perform. however, our uav is equipped with two turbine engines and weights over 20 kg. a control failure during the experiment may result in severe consequences to the operating staff and to the uav itself. for safety considerations, the uav experiment has to be performed in an open space outdoors. in addition, the uav does not take off from or land onto the ground directly because that requires a launcher and other auxiliary equipment which can be very complicated to build and very expensive. instead, our uav is suspended under a gantry crane using a steel cable (as shown in fig. 12), and the uav takes off from the suspended location. the staff and the uav are protected by the steel cable during the flight experiments. the height of the gantry is 7.2 m, and the width is 6.8 m. when the uav is suspended, the tail of the uav is at a height of approximately 1.5 m above the ground to minimize ground effects influencing the engine performance. the configuration of the uav flight experiment system is shown in fig. 13 . the altitude, vertical acceleration and engine rotor speed were measured by the laser range finder, the imu and the hall sensor, respectively. the noise in the acceleration measurement was suppressed by the data filtration method developed in our previous work [cit] . a data fusion method was used to estimate the vertical velocity of the uav by fusing the acceleration measurement with the altitude measurement."
"it is, therefore, the main objective of this paper to overcome the difficulties identified above by launching a quantitative analysis on the gics via accurately segmenting the test and control lines from the acquired gics images."
"in the training stage, three features are utilized as the input of the visible layer of the first rbm. after training layer by layer, the correlation of input data in time and space is mapped to hidden layers successively. particularly, the batch method is exploited to update the weights so as to speed up the training rate. in the prediction process, the classification results can be calculated by forward propagation of the trained dbn."
"last but not least, when the deployment area is large, it takes more mesh beacons to cover the area. with a greater number of mesh beacons in the network, the overhead incurred by the broadcast behavior of message relays is also increased. specifically, although broadcasting avoids the need to create and maintain routes, it may be less efficient from the perspective of the total number of consumed messages to fulfill an end-to-end communication between two devices [cit] . from our point of view, the overhead of the broadcast can be controlled if the bluetooth beacon networks are well-designed. just like what we have mentioned in the previous paragraph, if the rebroadcasts can be limited inside smaller subareas, the scalability of the bluetooth beacon networks will not be an issue. in addition, the official bluetooth mesh protocol also utilizes the managed-flood approach to implement the mesh network. this approach is based on message relays to rebroadcast messages, which is exactly what our mesh beacons do. as a matter of fact, our mesh beacons do more than acting as message relays-they play the role of reference points for micro-location services at the same time. acting as message relays-they play the role of reference points for micro-location services at the same time."
"when any mesh beacon hears the acknowledgement message, it rebroadcasts the message by using the managed-flood-based approach again. the purpose of rebroadcasting the acknowledgement message is to relay it to the mobile app. when the acknowledgement message from the mesh router has been received at the mobile app, the mobile app displays a welcome message indicating that the backend system is aware of the user's presence."
"as shown in fig. 7, a straight line is fitted for describing the relationship between the concentration and the riod via the least square approach; especially, the horizontal axis stands for the hcg concentrations, while the y-coordinate denotes the corresponding value of riod which is calculated according to eq. (17) ."
"as an area developed on the basis of forestry applications and big data technology, the theoretical basis of forestry big data can be abstracted and interpreted from three levels, including mathematical foundation, system structure and application value. they are the theoretical structure of the forestry big data system. the current research on forestry and big data is also based on this theoretical structure for optimization and development. as shown in fig. 3, it is the theoretical structure of forestry big data."
"timely detection of disease is a major challenge in the forest management process. for pest and disease monitoring, machine vision technologies have a wide range of applications [cit] . at present, to improve monitoring efficiency, machine vision technology is mainly combined with big data technology. for example, [cit] analyze and summarize forest pest and disease data in recent decades and establish a cloud-based forest pest monitoring platform. the platform stores data in hdfs and hbase, and processes data based on mapreduce. it can effectively improve the storage capacity, computing speed and information sharing and transmission efficiency of the existing data center."
"note that at approximately 10 s, the uav reached its limit for horizontal drifting. the steel cable became tensed again, and it applied both a vertical force (pointing upward) and a horizontal force (pointing toward the camera) to the uav. therefore, the uav moved closer to the camera at 12.5 s, as shown in fig. 18d . in addition, we can see a sudden increase in the uav acceleration in fig. 15 . we also observe an increase in the uav vertical velocity and uav altitude in figs. 16 and 17, respectively. these were all caused by the interference of the steel cable. however, the uav altitude controller quickly responded to this sudden interference and reduced the rotor speed slightly to offset the increase in altitude. therefore, there was a slight decrease in the rotor speed response after the disturbance, and the uav altitude quickly returned to 1.7 m. the fluctuation in the uav altitude was less than ±3 cm after the hover flight condition was reached. this small fluctuation demonstrated the effectiveness of our integrated controller."
"the remainder of this paper is organized as follows. in sect. 2, the gold immunochromatographic strip assay and the problem formulation are presented. section 3 provides a detailed introduction on the restricted boltzmann machine, the deep belief networks, as well as the applications in the segmentation of gics images. section 4 mainly discusses the performance of image segmentation via the deep belief network and also evaluates its overall performance in terms of some well-defined criteria. finally, conclusions are drawn in sect. 5."
"in figure 1 we can see two bluetooth beacon networks, in which all communications take place over the transport of the ble technology. as for the communications outside the bluetooth beacon networks, one may choose whatever technologies are available such as ethernet or wi-fi. in our system, the mesh router connects to the application server by using its built-in wi-fi interface."
"we observed that the considered resolvent of composed operators (13) is closely related to newton's method to non-smooth sparse optimization problems. however, how to choose the symmetric positive definite matrix in finite dimensional spaces for implementing the proposed algorithm more efficiently is not clear now. we will discuss it in future work."
"the first three parts of the criteria (labeled a, b, c) refer to mechanical aspects of license discovery and resource access. here, standard licenses (i.e., licenses that are invoked referentially or by template, like creative commons licenses, open database license (odbl), etc.) are preferred since custom language and terms may require negotiations and possible involvement of institutional counsel to clarify and confirm the rights and permissions. the two latter parts of the criteria (d and e) evaluate the reuse aspects of the licensing terms. part d considers any restrictions on the kind of reuse and part e considers any restrictions on who can reuse the data. one star is awarded for each part when all types of reuse are permitted and all audiences can reuse the data without negotiation; however, the rubric does make allowances for some restrictive terms if \"research\" or \"non-commercial\" reuse contexts are frictionlessly facilitated. each part of the criteria can be summarized with the following questions:"
"deep belief network (dbn), proposed by hinton and salakhutdinov [cit], is an extensively studied and widely used deep learning model. it is remarkable that the deep learning model is a biologically inspired model which mimics the layered structure of the cortex [cit] . essentially, dbn is a greedy and multilayer-formed learning model combined by a stack of restricted boltzmann machines (rbms). unlike other multilayer and nonlinear models, the distinct merit of dbn is its capability of obtaining the states of hidden layers units by one forward pass. in the last few years, dbn has drawn increasing research attention in many application fields such as recognition, signal and information processing, image processing and classification [cit] . therefore, we propose to use the dbn approach for quantitative analysis of a gold immunochromatographic strip."
"theoretically the transmission range of bluetooth 4 can be up to 100 m [cit] . in our experimental deployment the maximum distance between neighboring mesh beacons is about 8 to 10 m. the reason is that after several field tests, we found that if the distance is greater than 10 m, packet losses would become significant. this is due to the limited transmission range of the development boards we used, as well as the non-line-of-sight deployment of the devices. since we need to deploy the mesh router in our own lab for a stable network connection to the application server (a pc in the same lab), we have no choice but to deploy part of the system non-line-of-sight. in real deployments such as airports or exhibition centers, one can choose bluetooth devices with stronger output power or external antennas to increase the radio coverage, along with proper line-of-sight placement of the bluetooth devices to improve the transmission quality. doing so can also reduce the total number of devices needed to cover the whole field of deployment."
"with the advancement of microelectromechanical and wireless communication technologies, the industry of internet of things (iot) is flourishing at the world-wide scale. according to a recent forecast report by gartner [cit], the number of installed iot devices will be exceeding 25 [cit] . popular application scenarios of iot include smart homes, smart hospitals, smart factories, and smart cities, to name a few. in these application scenarios, a huge number of iot devices must be deployed as the infrastructure. to ease the interconnection among iot devices and the backend systems, a number of low-power wireless communication protocols have been standardized for iot, such as zigbee [cit], bluetooth low energy (ble) [cit], wi-fi halow [cit], lora [cit], etc. both zigbee and ble work in the 2.4 ghz ism band for short-range communication, while wi-fi halow and lora aim at providing longer reach for iot devices. although differences exist, these protocols share a common welcome message and sends it to the smartphone of the vip customer over the bluetooth beacon network, letting him know that the airline is aware of his presence and may send a staff to assist him. through the bluetooth beacon network, the airline can also provide other information to the vip customer such as his flight status, or offer him some shopping coupons when he is about to pass through the shop having sales promotion. the vip customer can also ask for help by pressing a button in the app. since the real-time location of the customer is tracked, even if he has left the location where he asked for help, the airline staff can still reach him at his real-time location."
"mobile app apple ios version 10.2 as we stated earlier, the mesh beacons are configured to broadcast beacon messages (i.e., type 1 messages) periodically. when the mobile app discovers a beacon message, it first checks whether the carried mesh uuid matches our own specific value. if it is a match, the mobile app uses the discovered major/minor values to locate itself immediately. once the locating process is successful, the mobile app will send the mobile user's presence state by broadcasting a presence message (i.e., a type 2 message). in the presence message, the mobile app specifies the encrypted user id and the location (i.e., the received major/minor values), meaning that the user is nearby the location associated with the major/minor values. once a type 2 message is detected by any mesh beacon, the mesh beacon uses the embedded user id and location values to construct a type 3 message, and then broadcasts the type 3 message. for all other mesh beacons that discover the type 3 message, they use the managed-flood-based approach to relay it. when the type 3 message finally reaches the mesh router, the mesh router sends the encrypted user id and the major/minor values to the application server over http. at the same time, the mesh router uses the received type 3 message to compose a type 4 message by simply changing the service id to 0xd124, and then broadcasts the message. with the help of mesh beacons, the type 4 message is forwarded over the bluetooth beacon network to the mobile app. on receiving the type 4 message with its own user id, the mobile app will show a message box indicating that the application server has been notified of the presence of the user. at the application server, the major/minor values are translated into the name of the associated location, and then the user's presence state is sent to the cloud database. eventually, with the presence state of all the mobile users in the database, we are able to analyze the locating history and to create our own innovative applications."
"in this subsection we describe the flow of operations of our system. when the mesh beacons are powered on, they periodically broadcast beacon messages over the advertising channel, just like ordinary beacon devices do. moreover, our mesh beacons also listen to the messages carrying users' presence state from the mobile apps at the same time. in the following, we illustrate the flow of operations when a smartphone user with our mobile app enters the communication range of our bluetooth beacon network."
"since the invention of ibeacon and given the extreme popularity of smartphones, the ble-based beacon technology has been deployed extensively as a basis to provide various locating services for smartphone users. in this research, our main contribution is that we combined the bluetooth broadcast and mesh topologies to extend the applicability of beacon solutions. specifically, apart from broadcasting beacon messages, our beacon devices also serve as beacon readers that can discover the presence of specific users, and then forward the presence state hop-by-hop to the backend server over the bluetooth beacon network. with the knowledge of a specific customer's presence, the backend server can respond to the customer with a personalized message, again via the relay of the bluetooth beacon network. in some use cases, such as welcoming a vip customer at the airport, our interactive locating system can give the customer a much improved user experience, since all the communications rely on a single network technology-ble. neither costly mobile internet connections nor the troublesome wi-fi connections are needed for the customers."
"forestry data is involved in a wide range of fields and applications. the forestry management is often analyzed in the multi-dimensional scenarios, including time and space. therefore, spatial attribute is an important attribute of forestry data. according to the characteristics and sources of the data, the spatial data is mainly divided into five categories, known as remote sensing data, surveying data, location-based data, internet of things data and social network data [cit] . the data commonly used in forestry are the first four types. the forestry data model is abstracted into two categories based on volume 7, 2019 figure 2. three-band raster data case."
"the rdp's star rubric (http://reusabledata.org/criteria.html) is a five-part criteria that addresses: the findability and type of licensing terms, the scope and completeness of the licensing, the ability to access the data in a reasonable way, restrictions on how the data may be reused, and restrictions on who may reuse the data. each of the five criteria (labeled a-e) are quantified by up to a 1.0 star value, so data resource evaluations (e.g., scores) can range from 0 to 5 stars. the rubric is quite extensive, with a branching evaluation workflow, multiple rules and decision points within most parts of the criteria, and bypasses for cases where a particular rule may not apply or make sense."
"with remote sensing technology, continuous, dynamic, large-scale and low-cost forestry data sources can be obtained, which promotes the research and development of forestry technologies. remote sensing mainly includes satellite remote sensing, uav remote sensing and wireless sensor networks [cit] . among them, wireless sensor network (wsn), also known as ground remote sensing [cit], is an extension of remote sensing, and they complement each other. the data needed for different forestry applications should be obtained from different remote sensing platforms."
"as in figure 8c, meaning the whole process has now completed. regarding the response time, the time (0.842 s) shown in figure 8b refers to the period starting from the user pressed the \"locate\" button, to the time when the physical location is determined at the mobile app. note that the screenshot in figure 8c was captured in another test. the received time (1.983 s) shown in figure 8c refers to the period starting from the completion of locating, to the instant that the mobile app received the acknowledgement message from the mesh router."
"it is important to acknowledge that the rdp's rubric emphasizes the reuse and redistribution needs and activities of u.s. based, non-commercial, research use cases. this perspective reflects our own experience as data resource aggregators and primary data producers, and our frustrations navigating terms of use that limit certain communities' (e.g., clinical researchers) and kinds of reuse (e.g., new tools) [cit] . we also found that this specific and practical point of view was helpful in keeping the rubric and its application logically manageable. when this perspective has limited our evaluations, we have captured the fact that other entities may have different results."
"if the system needs to request the remote sensing image in the specified area, firstly, according to the range of the mbr (minimum outer border) of the area, the image data block overlapping with the mbr and its metadata such as spatial coordinate information are read into the memory. then, the spatial vector data of the query area and the image data block in memory are mapped to the same scale, and the vector data is transformed into binary raster data with the value of 1 in the region and 0 outside the region. finally, the raster data and image data are masked, and the returned data is the requested target data."
"data acquisition: data is the basis and prerequisite for the development of forestry big data. with the development of big data, the current forestry big data system can well support the real-time calculation of massive data, but in the long run, the scale of data will be larger and larger, and the computing resources and storage resources are limited. therefore, in the future, there are cases that existing computing resources cannot cope with the massive data. one method to solve this problem is to reduce the data size during the data collection phase. with the development of artificial intelligence (ai), related technologies have created enormous value for various fields. applying ai technology to the remote sensing platform can initially process the data at the data collection end. in recent years, ai-based satellite remote sensing platforms have attracted the attention of researchers and have been further developed. running ai technology on satellites increases the ability of satellites to acquire and process information, which improves data quality and reduces redundant or worthless data. for other remote sensing platforms such as drones, ai technology can also be applied to improve the ability to intelligently process information when collecting data. by using ai technology for the data collection side, the pressure on data storage and processing can be reduced, thereby improving the performance of forestry big data."
"forestry big data brings great opportunities for the development of forestry. with the continuous improvement of the theory, forestry big data has become an independent discipline."
"because the range of areas that need to be presented is different, it is required that the data can be adapted to different scaling ratios when displayed. in response to this demand, the system could use an image pyramid model to organize the data. there are usually three important steps in building an image pyramid model as show in algorithm 1, including data segmentation and data sampling, and data storage. the system first needs to segment the data by a fixed size to form data tiles. then the pixels in each data tile are sampled according to the specified sampling ratio. the adjacent sample results are merged into one tile and serve as higher layer tiles. the system iterates through the process until it is finally merged into one tile size. finally, all tiles are indexed and stored in a distributed storage system. when the user initiates a visual request, the system locates the level of the tiles to be loaded according to the size of the request scope, and loads the tiles within the request range into the memory. this data visualization strategy avoids the pressure of memory and bandwidth caused by loading the entire map. as shown in fig. 8, it is a schematic diagram of a remote sensing image pyramid model. when grading and tile partitioning, the number of levels depends on the scale of the map, and the number of tiles is determined by the size of the image [cit] . after the user submits a visualization request to the system, the system searches for the corresponding pyramid level according to the scope of the request, and the tile data contained in the layer within the request scope, and then the system performs the mosaic [cit] operation on the data tile data and shows the results on the client side. [cit] optimize the remote sensing image parallel mosaic algorithm based on spark. the method can effectively reduce the frequency of i/o and effectively improve the mosaic efficiency. and it is suitable for the mosaic of massive image data."
"experiment 2 basically follows the design of experiment 1. however, the difference is that we focus more on the multi-hop relay function of the mesh beacons and measure the response time. specifically, in this experiment, if we stand at location d, then only mesh beacon d is configured to receive the presence message from the mobile app (i.e., type 2 message), while mesh beacons a, b, and c are forced to ignore type 2 messages. this ensures that when the user is physically located at location d, its presence message is only received by mesh beacon d rather than by the nearby mesh beacon c, which has less number of hops to the mesh router. the same idea applies to all other cases, in which only the mesh beacon closest to the user is able to receive the presence message from the mobile app. again, at each of the four locations, we measure the response time for five times and show the result in figure 10 ."
"based on the classification technology, the change detection of remote sensing images can obtain timely and consistent forest interference information, so that forest resources can be fully utilized and effectively protected. based on hadoop, dan hammer and others implement a high-confidence forest disturbance warning system, which establishes appropriate geolocation classification rules and enforces local forest conservation efforts."
"with the added acceleration feedback term, we can adjust the k a coefficient to let k a fall within the k a stability range. this means that the new controller developed in this work is able to achieve stable altitude control with much greater time delay in the engines."
"in an rbm, the joint probability distribution of visible and hidden units pðv; hjhþ is defined by the energy function. for a given state (v, h), the energy function of bernoullibernoulli rbm is [cit] :"
"1. when the mobile app discovers a beacon message from a certain mesh beacon, it locates itself based on the major/minor values embedded in the message. in our design, the mobile app directly looks up a built-in location table to resolve the major/minor values, so the name of the detected location can be shown on the user interface immediately. 2. once the locating process has completed, the mobile app composes the presence message containing the encrypted user id and the detected major/minor values, and then broadcasts the presence message. the objective of broadcasting the presence message is to inform the backend system that this specific user has shown up at a particular location. 3. as soon as any mesh beacon hears the presence message, it uses the managed-flood-based approach to rebroadcast the presence message. the purpose of rebroadcasting the presence message is to relay it to the mesh router. the managed-flood-based approach can make sure that once a specific presence message has been broadcasted by a mesh beacon, the mesh beacon will not rebroadcast it again. 4. when the mesh router hears the presence message originated from a certain mobile app, it uploads the content in the presence message to the application server. meanwhile, it sends an acknowledgement message back to the mobile app by broadcast. 5. when any mesh beacon hears the acknowledgement message, it rebroadcasts the message by using the managed-flood-based approach again. the purpose of rebroadcasting the acknowledgement message is to relay it to the mobile app. 6. (continue from item 4) when the application server receives the presence state from the mesh router, it forwards the presence state to the cloud database. in the cloud database we are able to see the locating history for all the mobile users."
"in the formula (21), t −1 needs to be calculated. however, it is sometimes difficult to evaluate it. inspired by the method introduced by fukushima [cit], we provide an alternative way to compute the resolvent of composed operators, which avoids computing the inverse of operator t."
"in the last few decades, the scale of global data has grown at an unprecedented rate. the overall created and copied data volume in the world increased by nearly nine times within five years, and the figure is doubling at least every other two years [cit] . while the amount of data is exploding, the concept of ''big data'' came into being. although currently there is no authoritative, unified standard definition for big data in the scientific, academic, and government sectors [cit], big data has become a leading term in all fields [cit] ."
"since the invention of ibeacon and given the extreme popularity of smartphones, the ble-based beacon technology has been deployed extensively as a basis to provide various locating services for smartphone users. in this research, our main contribution is that we combined the bluetooth broadcast and mesh topologies to extend the applicability of beacon solutions. specifically, apart from broadcasting beacon messages, our beacon devices also serve as beacon readers that can discover the presence of specific users, and then forward the presence state hop-by-hop to the backend server over the bluetooth beacon network. with the knowledge of a specific customer's presence, the backend server can respond to the customer with a personalized message, again via the relay of the bluetooth beacon network. in some use cases, such as welcoming a vip customer at the airport, our interactive locating system can give the customer a much improved user experience, since all the communications rely on a single network technology-ble. neither costly mobile internet connections nor the troublesome wi-fi connections are needed for the customers."
"our goal with the (re)usable data project is to draw attention to the licensing issues that are challenging the reuse of valuable biomedical data, not to criticize any specific organization or data resource in the community. rather, we hope rdp will encourage the community to work together to improve licensing practices in order to facilitate reusable resources for all. reusing data en masse comes with numerous challenges and can be better enabled via the practices articulated in initiatives like the fair data principles and fair-tlc evaluation framework [cit] . rdp's focus on licensing issues is meant to draw attention to the pervasiveness of current practice failures and their effects."
"as part of our evaluation process, we often contacted data resources with clarifying questions about their licensing information and tracked these conversations on the rdp github repository. these exchanges led to more accurate evaluations and sparked dialogue about how resource curators could improve the clarity of their licenses. additionally, in response to our outreach on social media, we received requests to evaluate eight additional data resources. we believe this early engagement demonstrates a community interest in enabling reuse, and a desire to contribute to open discussions about how to fix our licensing problems. moreover, while rdp has been focused on biological and biomedical data resources, we believe the goals and problems we have raised are domain agnostic, and want to collaborate with other data communities to ensure that our rubric is relevant and applicable across disciplines."
"in this experiment, we check the overall operations of the prototype system. first, we let a user with the mobile app walk from location a to location b, and then return to location a. as expected, the smartphone user was able to see the correct locating results in the mobile app which switched from location a to location b, and then switched back to location a. then we checked the moving pattern recorded in the cloud database. figure 11 is a screenshot from the google firebase, in which the top-level key refers to the encrypted id of the user and the second-level keys refer to the locating records for this user. in each record there are values of major and minor, the associated location name, and the timestamp of the locating record. from figure 11 we can see three consecutive records with the determined location a, b, and then a. the order of the determined locations in the cloud database apparently matches the route taken by the user, and the timestamps correctly indicate the time that the user was physically located at those locations. next, we test the use case of two simultaneous users. specifically, to ease the cross check of the locating results, in this test two smartphones were held by a single person to emulate two users taking the same route simultaneously. the route starts from location b to location d via location c, then takes the reverse direction back to location b. table 2 summarizes the observed locating results recorded in the cloud database, from which we can compare the timestamps of the two emulated users at these locations. since the test was carried out by a single person, ideally the locating results of the two emulated users should be exactly the same. however, we can see that there exist time differences in most of the locating results. again, we believe this is due to the natural uncertainty of multi-hop wireless transmissions and possibly the contention of wireless resource. fortunately the time differences are limited to only a few seconds and would not be a problem. next, we test the use case of two simultaneous users. specifically, to ease the cross check of the locating results, in this test two smartphones were held by a single person to emulate two users taking the same route simultaneously. the route starts from location b to location d via location c, then takes the reverse direction back to location b. table 2 summarizes the observed locating results recorded in the cloud database, from which we can compare the timestamps of the two emulated users at these locations. since the test was carried out by a single person, ideally the locating results of the two emulated users should be exactly the same. however, we can see that there exist time differences in most of the locating results. again, we believe this is due to the natural uncertainty of multi-hop wireless transmissions and possibly the contention of wireless resource. fortunately the time differences are limited to only a few seconds and would not be a problem."
"forestry information systems need to provide different services for different needs. therefore, in contrast to the characteristics of data in other fields, forestry data is updated quickly and with no uniform format. these data have multidimensional spatial features, which make forestry data processing more difficult. with the development of technologies such as sensors, satellites and drones, the resolution of remote sensing observation data significantly improves, the number of bands of remote sensing data gradually increases, and the period of data acquisition decreases. therefore, the data volume of remote sensing data has exploded. currently, the scale of data in forestry applications far exceeds the size of previous data, and the demand for service response is higher. if the traditional data storage and computation methods are introduced, it is difficult to guarantee the desired results in an acceptable time. specially for real-time applications such as disaster monitoring, in the case of massive data, it is meaningless to monitor the forest if the data processing efficiency is still as low as before. for forestry applications that need to process massive amounts of data, big data technologies can be introduced to solve the current bottlenecks [cit] ."
"to create an index, we first need to design an index format based on the storage system. for instance, to build an index in hdfs, it is necessary to determine the fields stored in the index node, the structure of the index, and so on. to build an index in spark, the first thing to do is to customize an rdd for index. then, we build the index based on the index data structure. as shown in figure 7, it is a schematic diagram of using spark to build an index, which shows the main transform and action operations during building an index. the specific process is described as follows. spark first reads the data from disk into memory and then partitions it according to the logical space of the data. the k value in the figure is the logical space partition id of the data (such as the hilbert partition id, etc.), and the data is repartitioned according to the partition id. this can ensure the locality of data and alleviate the network i/o burden caused by frequent data communication among different nodes. moreover, reasonable data partitioning operation can ensure load balance of distributed clusters, thereby reducing node storage and computing pressure. after that, we need to build a local index for each partition. this is done by calling the ''rdd.mappartitionwithindex()'' operator and passing a custom function for index construction as the parameter. then, after each partition generates an index, the ''rdd.collect'' operator is used to submit the local index to the master node, and the master node builds a global index based on them. this method is not limited to building an index in memory, it can also persist the index to the storage system in the format of the index file."
"once the locating process has completed, the mobile app composes the presence message containing the encrypted user id and the detected major/minor values, and then broadcasts the presence message. the objective of broadcasting the presence message is to inform the backend system that this specific user has shown up at a particular location."
"it is obvious that the riod and the hcg concentrations have good corresponding relationships by the presented dbn approach and the adaptive cellular neural network (cnn) [cit] from fig. 7 . the correlation coefficient of the presented dbn approach is 0.97681, while the correlation coefficient of the adaptive cnn is 0.9689. therefore, the presented dbn approach opens up a new way of imagebased method for quantitative analysis of gics system."
"each data resource received a score from 0 to 5 stars according to the rubric (http://reusabledata.org/criteria.html). sources were curated directly into the rdp's github repository (https://github.com/reusabledata/reusabledata/tree/master/datasources) as yaml files from a template to help ensure the provenance of statements. the template includes metadata such as source name, description, source type, license type, data access url, the license issues uncovered during the evaluation, and any commentary about how the license was evaluated. the evaluations were checked by at least two evaluators, and comments on the evaluations were made on github pull requests to allow for transparency and continued conversation. evaluations then went through a battery of syntactic and consistency checks. when necessary, the evaluated resource was contacted for clarification."
"vector data is a data structure that uses points, lines, and polygons to describe things in space. it is an object-oriented data model. a vector object describes a spatial object through a combination of geographic information and object properties. moreover, vector data has unique advantages in representing the positional relationship of spatial objects. vector data files can be converted to binary or text formats for further processing."
"to fully understand the development of forestry big data (the application of big data in forestry), in this paper, we review the whole development process of forestry big data, including the historical background, architecture, key technology development and application, and future development trends of forestry big data. as shown in fig. 1, it is the whole framework of this survey. the rest of this article is organized as follows. in the section ii, we review the origin of forestry big data and the main technologies, and summarize the types of forestry data. in the section iii, we outline forestry big data, mainly to illustrate the opportunities that forestry big data technology brings. in section iv, we summarize forestry big data applications and technologies. in section v, we briefly outline some of the challenges of forestry big data technology based on open source big data systems. in section vi, we point out several of the current problems in the development of forestry big data and future development trends, and the vii section summarizes the full text."
"the concept of smart forestry has not been clearly defined so far. it is based on digital forestry, using cloud computing, internet of things, mobile internet, big data and other new generation information technology. as a subsystem of the smarter planet, through intelligent forestry, it can promote the coordinated development of forestry resource management and ecosystem construction. smart forestry can make forest management digital, perceptual, interconnected, and intelligent. the forestry data is the basis for the operation of the intelligent forestry system, which can assist forestry decision-making through the analysis and mining of forestry data. therefore, big data technology is the basis for the development of smart forestry. the application of big data technology to forestry is still in its infancy. therefore, the relevant technology of forestry big data still needs to be further studied."
"the international forest survey (monitoring) [cit] s, indicates that the integration of remote sensing (rs), geographic information system (gis) and forest sampling technology (fs) is the future direction of forest surveys. they are deeply integrated and organically integrated. the global positioning system (gps) is only used as an auxiliary technology for forest sample plot location. this technology system can perform forestry tasks faster and more efficiently, thus reducing the loss of manpower and material resources [cit] ."
"the user experience of bluetooth beacon-based applications is enhanced because our system can work without smartphone users' internet connections. in other words, smartphone users only need to turn on the bluetooth interface to enjoy the interactive locating services without paying a dime on their phone bills."
"in recent years, newton-type methods have been combined with the forward-backward splitting (fbs) algorithm to accelerate the speed of the original fbs algorithm. see, for example, [cit] considered the following convex optimization problem:"
"optimizing the storage, query and analysis of forestry data based on big data technology can improve the response performance and robustness of current forestry applications. by real-time calculation and analysis for forestry data, we can capture and predict changes in the state of the forest to make quick decisions, which can reduce losses due to delays [cit] . in this chapter, we mainly summarize the application of big data in forestry from the aspects of forest vegetation classification and change detection, forestry parameter inversion and forest growth and disaster monitoring."
"the associate editor coordinating the review of this manuscript and approving it for publication was rongbo zhu. processing methods. traditional computing and statistical methods are not effective enough to handle the big data. the main characteristics of big data are high volume, variety and velocity (referred to as ''3vs'') [cit] . because of the increasement of the requirements for the quality and security of data in various applications and services, ''veracity'' is regarded as one of big data's characteristics at present. therefore, the characteristics of big data gradually become ''4vs'' [cit] . in addition to the problems caused by the data, there are security and privacy issues arising from the development of big data technology. however, the advancement of big data still brings unprecedented opportunities to various industries [cit] . currently, big data is widely used in the fields of urban infrastructure construction [cit], smart grid [cit], intelligent transportation [cit], emergency communication network [cit], social network [cit] and agriculture and forestry monitoring [cit] ."
"a major basis for forest vegetation classification based on remote sensing images is ''same objects with same spectrums''. however, the problems of ''same object with different spectrums'' and ''different objects with same spectrums'' caused by the increasing in resolution have a great influence on the classification accuracy. for this problem, we can combine spectral information with features such as texture and terrain to accurately classify vegetation cover. the forestry vegetation classification uses data collected by multiple sensors with multiple temporal characteristics, and the classification methods are gradually diversified. for instance, combining landsat-8 images with the topographic map data, sample plot data, and the forest manager survey data to classify tree species based on textures [cit], and using modis data and dem data to classify phenological attributes of tree species. it is also possible to extract forest vegetation types based on environmental satellite imagery (hj-1 ccd) and modis time series data [cit], or to classify tree species fine by fusing sar or lidar data with hyperspectral remote sensing data [cit] ."
"we know that bluetooth beacons are broadcast on the three advertising channels. since the message relays in the bluetooth mesh networks are also based on broadcasting, when the number of mesh beacons is large, the contention of using the advertising channels may become an important issue. a good solution to this problem is the latest bluetooth 5.0 [cit] . in bluetooth 5.0, a new physical layer named le 2m is introduced (note that the physical layer used in bluetooth 4 is called le 1m). with le 2m, its theoretical data rate of 2 mbit/s can significantly reduce the air time needed to transmit a given amount of data, leading to better spectral efficiency. moreover, in bluetooth 5.0 the payload of the advertising packets can be offloaded to the available data channels, leaving only the header data transmitted on the advertising channels. on the problem of possible interference from wi-fi communications, bluetooth 5.0 features an improved channel sequencing algorithm that improves the pseudo randomness of the next hop channel sequencing. this algorithm also improves the co-existence performance of bluetooth devices in the presence of wi-fi devices. when the commercial bluetooth 5.0 chips are widely available and have been used in beacon systems, we believe that the locating performance of such systems can be greatly enhanced."
"the main task of forestry big data is to reasonably organize and store the forestry data according to the characteristics of forestry data with the help of the existing big data software and high-performance hardware, to achieve the efficient query and analysis of the forestry big data. in recent years, big data technology related to the forestry data, including the research of remote sensing big data and spatial big data, has developed rapidly, such as hadoop, spark, storm [cit], and flink [cit] . there are some typical computing frameworks such as spatialhadoop [cit] based on hadoop platform and geospark [cit] based on apache spark. these studies can effectively manage massive amounts of data, and we can apply these technologies to forestry, which greatly promotes the development of forestry."
"if the data are stored in a distributed file system or a nonrelational database, the system still need to traverse all the data during searching for data. for forestry big data systems that process massive amounts of data, although each node reads data in parallel, it still generates a lot of i/o and memory consumption. moreover, the process requires a large amount of cpu and wastes hardware resources. therefore, to efficiently and accurately filter unnecessary data in the data query process, it is necessary to index the data in the storage system. forestry services are often based on spatial attributes, such as resource inventory for a forest farm or a specific area of the forest farm, so the index in forestry data is mainly based on spatial attributes. the two most important concepts in spatial indexing are data partition and index structure. in a distributed environment, the partition here refers not only to the data partition in the storage system, but also to the partition of the compute node after the data is loaded into the compute node's memory (such as the logical partition of rdd or the split partition of hadoop). the index structure includes r-tree, kd-tree, quadtree, grid index, and so on."
"in forestry big data, the main role of the analysis layer is to provide tools for forestry data analysis, such as commonly used data mining algorithms. therefore, the system can quickly analyze and calculate forestry data for different forestry services."
"open data: forestry big data is still in the infancy, and forestry data is characterized by dispersion, irregularity and low quality. from the perspective of technological development, to make a breakthrough, we first need to obtain accurate and valuable data. although it is difficult to solve now, with the development of smart forestry, people's demand for new technologies is constantly increasing. therefore, forestry data will gradually become open."
"based on the numerical experiment, we can see that the crb curve strongly impacts the stability region. we now derive the theoretical upper limit of k a by analyzing the crb curve. according to (22), we can compute the following:"
"where in this section, we will analyze the stability performance of the integrated altitude control system. the controller is also designed to achieve specific gain and phase margins to satisfy the requirement of robustness."
"where r stands for the maximum range of the image data type. for example, r equals to 1 when the data type of input image is double-precision floating point, and r equals to 255 when the data type is an 8-bit unsigned integer, etc. mse denotes the cumulative squared error between the binary mask i 1 and the normalized original image i 2, which can be obtained by:"
"in this subsection we describe the flow of operations of our system. when the mesh beacons are powered on, they periodically broadcast beacon messages over the advertising channel, just like ordinary beacon devices do. moreover, our mesh beacons also listen to the messages carrying users' presence state from the mobile apps at the same time. in the following, we illustrate the flow of operations when a smartphone user with our mobile app enters the communication range of our bluetooth beacon network."
"as mentioned before, the uav is equipped with two turbine engines with the same specification. because they receive the same rotor speed command, the two engines operate at the same rotor speed and generate the same thrust force. therefore, we have the following:"
"in this experiment, we would like to measure the response time from the mobile app users' perspective. specifically, we put the smartphone beside the mesh beacons and then open the mobile app. at each of the four locations (a through d), we measure the response time five times, and show the results in figure 9 . note that since the locating process is based on an internal table lookup in the mobile app, the time needed to finish the locating process does not depend on the real locations. therefore, in figure 9 we only show the time taken for the mobile app to receive the acknowledgement message."
"for data storage, scidb is ideal for storing remote sensing data because it is an array-oriented storage model. however, since scidb is still in the development and improvement phase, currently it only provides a c language interface. therefore, for distributed computing frameworks written in non-c languages such as hadoop, it is difficult to call the scidb interface. the system maintenance costs will increase. therefore, scidb-based forestry data storage solutions are not currently widely used. nosql databases such as hbase have certain advantages for storing forestry data in vector format, which supports the structure of vector data well. however, these nosql databases are not designed for image data itself, it is necessary to reorganize the data format during storing, such as converting data into a byte stream."
"big data in a broad sense includes not only the structured, semi-structured and unstructured data that are growing, but also the processing techniques and processing tools for these data. with the development of technology and the increasing demand for computer response speed, big data technology has been widely used and developed in various data processing fields. however, the data format and processing flow of big data are quite different from the traditional data"
"currently, forestry big data technology based on open source big data computing framework can effectively manage and analyze data. however, because the performance depends on the underlying storage system and the computing engine, there are still several problems that need to be solved in some forestry big data systems."
(criteria d) we found that 20 (35%) of the data resources included clear and unambiguous licensing language that provided for unfettered reuse for all purposes (criteria e) 17 (30%) included clear and unambiguous language that provided for unfettered reuse for all user groups.
"it is noteworthy that that the largest single type of licenses were custom licenses, suggesting that resource providers either felt that a standard license did not meet their needs or that they were not knowledgeable about standard licenses. moreover, while the majority of custom licenses were restrictive, 9 were permissive, which leads us wonder if some needs and intentions are not being met by the existing set of standard permissive licenses. although it is encouraging that the largest single license category is permissive, the total body of non-permissive license types is larger."
"forest management is a long-term process, which requires dynamic information on the distribution, composition, structure, and disturbance of forests over time. therefore, monitoring forests can protect forest ecosystems effectively and promote the sustainable development of forest resources. forestry monitoring relies primarily on remote sensing and location technology to capture a variety of data, including remote sensing platforms, such as ground-based systems, aircraft-based systems (manned aircraft or uavs) and satellite-based systems [cit] and gps positioning systems, etc. ground systems include sensors that monitor a fixed location on the ground and the surrounding environment, as well as movable monitoring instruments. the ground system is mainly targeted to obtain real-time monitoring data of specific attributes within a specified small range, and satellite remote sensing and aerial remote sensing provide large-scale global observation data."
"the main tasks of forest disaster monitoring include forest fire prevention and pest and disease detection. forest hazard monitoring has high requirements for time and sometimes it even requires staff to analyze and deal with the situation in a few seconds [cit] . in applications such as forest fire monitoring, the system usually uses remote sensing data with high temporal resolution and low spatial resolution, such as noaa, modis, and fengyun [cit] . it is also possible to combine satellite remote sensing with ground-based sensor data for fire prediction and monitoring [cit] . by monitoring pests and diseases in forests, operators can obtain information on changes in the spectral characteristics of vegetation and take appropriate measures in a timely manner if necessary. the data used for pest and disease monitoring are mainly high-spectrum and high spatial resolution data such as modis and spot [cit] ."
"in this paper, the human chorionic gonadotropin (hcg) is selected as the target analyte in experiments. it is of great significance to quantitatively determine the concentration of hcg that can be used as indicators of a number of diseases such as ectopic pregnancy differentiation and fetal down syndrome screening. in particular, ten different concentrations of hcg have been added into gics strips in the experiments, and therefore, we can capture gics images at the fixed time points as shown in fig. 2 ."
"because there are no connections in any two visible units and also any two hidden units, the conditional probabilities pðhjv; hþ and pðvjh; hþ are factorial and can be calculated by:"
"by representing data as vector data and raster data, it is possible to simplify the data processing complexity for large amounts of data. therefore, when processing forestry data, it is very simple to ensure the consistency of the data in various attributes, thereby improving the efficiency of data processing and accuracy of the result."
"first of all, the gray intensity of pixels in the neighborhood should be considered since the intensity of test and control lines are generally larger than the surrounding areas. if the square window size is set as winsize, a vector with size winsize â winsize can be obtained. for pixels near the image border, we use the mirroring method [cit] to obtain intensity values of regions inside the window but beyond the image border. however, as is well known, with the analyte concentration increasing, the color in test and control lines will deepen, while the corresponding intensity values of pixels will decrease. based on this observation, we introduce two input features: one is the distance feature that represents the distance to the center, and the other is the difference feature that represents the difference of intensity values between two lines and the background. by doing so, the dbn approach can perform well even when analyte concentration is at a low level. it should be noted here that all input features of dbn should be normalized."
"in this section, we introduce the restricted boltzmann machine and the deep belief networks (see, e.g., [cit] for more details), which will be applied to the gics image segmentation when learning features to distinguish the test and control lines."
"when a resource provided inconsistent or no licensing information, only parts a and c of the rubric were used in the evaluation. while one could assume that some of these resources wished to reserve all of their copyrights when no information was found, the ambiguity and lack of clear intent would require clarification and possibly legal counsel. 11 data resources had such contradictory or missing information; therefore, the summary statistics for parts b, d, and e of the rubric do not include data for these resources (see figure 1) . we have qualified all of the numbers given below to prevent ambiguity."
"massive forestry data includes both spatial and non-spatial data [cit] . by analyzing and processing these data and extracting hidden information and knowledge from the data, more valuable data can be provided for forestry development and planning. knowledge acquisition determines the knowledge representation and reasoning methods, establishing knowledge base, writing inference procedures, and debugging and modification [cit] . at present, data mining technology is considered as an important tool for analyzing massive data and extracting knowledge from data. the application of data mining and visualization technology to forestry data analysis has greatly promoted the development of forestry."
"in our system, we use ble non-connectable advertising packets to implement all types of messages so there would be no need for the devices to pair with each other before sending any data. the breakdown of a ble air interface packet is shown in figure 3 . according to bluetooth specification v4. 1 [cit], the maximum size of a ble advertising packet data unit (pdu) is 39 bytes. excluding the 2-byte header field and the 6-byte advertiser's address field, there are up to 31 bytes to carry the advertising data. the advertising data is further divided into a sequence of advertising data (ad) structures. the ibeacon protocol uses ad structure 1 and ad structure 2 combined to carry its advertisement information, with the message format shown in figure 4 ."
"because of this practical limitation, the uav may drift horizontally during the hover flight. the distance of the uav horizontal drifting was constrained by the length of the cable and the height increase of the uav. the maximum radius, r, for uav horizontal drifting can be computed as follows:"
"gold immunochromatographic strip (gics), labeled with the colloidal gold nanoparticle, is on the basis of an immunochromatographic process that utilizes the high specificity of antigen-antibody reaction and provides rapid determination of target analyte. in the past decade, the gics has been extensively studied and widely applied to the biomedical and related areas for determination of analytes in specimens due to its fascinating advantages including short assay time, ease of use, good specificity and satisfactory stability [cit] . up to now, a variety of material-selection-based approaches have been introduced by biochemical researchers to enhance the quantification performance of gics, see, e.g., [cit] . on the other hand, it should be highlighted that the research on modeling the biochemical reactions of gics with aim to optimize assay performance has stirred considerable research interest, see, e.g., [22, 23, 34-36, 38, 39, 41] . a focus of research in this area has been on how to exploit the quantitative instruments of gics for more sensitive and quantitative performance, see [cit] and the references therein."
"from (22), it can be seen that k d and k p are functions of k a and w. by fixing k a, we are able to obtain the crb curve"
"where k au is the upper limit of k a . an easy way of solving this equation is to compute w d t d at first by combining (24) and (25), which gives us the following:"
"in this subsection, we discuss some implementation-related issues of the system, including the communication range of bluetooth beacons, the contention on the ble advertising channels, the interference from wi-fi devices, and how to deal with a large deployment area."
"remark 1 as discussed in the introduction, there are essential difficulties in processing the gics image because of (1) an additional step of preprocessing introduced to acquire the region of interest through commonly used image segmentation operators; (2) the blurriness of the test and control lines resulting from their production process; and (3) the interference noises existing on the detection window of strip."
"with the increasing popularity of the concept of precision forestry [cit], integration of technologies such as rs, gps and gis has enabled the remote sensing technology to be comprehensively upgraded and applied in business. the speed of acquisition of remote sensing data is constantly increasing. for the remote sensing images, currently, the features are high resolution and hyperspectral, which provides more information for forestry applications and increases the data processing burden. the remote sensing data from various satellites consists of many large images, which are very complex in structure, spectrum, and text features [cit], bringing several difficulties to data management."
"here, we only plot a small portion of the dk d /dw and dk p /dw curves. all curves will extend to below the w-axis (negative"
"copyleft: copyleft licenses allow for reuse, transformation, and redistribution. however, new contributions derived from the original data resource must be distributed under the same license. examples include cc by sa 4.0 and the gnu gpl 3.0."
"experiment 2 basically follows the design of experiment 1. however, the difference is that we focus more on the multi-hop relay function of the mesh beacons and measure the response time. specifically, in this experiment, if we stand at location d, then only mesh beacon d is configured to receive the presence message from the mobile app (i.e., type 2 message), while mesh beacons a, b, and c are forced to ignore type 2 messages. this ensures that when the user is physically located at location d, its presence message is only received by mesh beacon d rather than by the nearby mesh beacon c, which has less number of hops to the mesh router. the same idea applies to all other cases, in which only the mesh beacon closest to the user is able to receive the presence message from the mobile app. again, at each of the four locations, we measure the response time for five times and show the result in figure 10 . as expected, the above experimental results are almost identical with that of experiment 1, with the exception of the measurement results at location d. specifically, here the majority of the response times at location d are around 2.7 s, while in experiment 1 only two measurements exceed 2.7 s. this reveals the fact that although the user is physically closer to mesh beacon d, in experiment 1 some of the presence message from the mobile app must have been received and then forwarded by the nearby mesh beacon c so a better performance (fewer packet losses) can be observed. moreover, if we compare the measurement results at location c of the two experiments, we can find that the average response time in experiment 2 is slightly higher than that in experiment 1. we speculate that this is because in experiment 1, when the user is standing beside mesh beacon c, some of the presence messages may have been received and then forwarded by either mesh beacon a or mesh beacon b. therefore, some of the measurement results are on the 2-hop distance (round-trip). however, in experiment 2 the round-trip distance from mesh beacon c to the mesh router is 4 hops in every measurement. this is the reason why the average response time becomes higher than that in experiment 1. as expected, the above experimental results are almost identical with that of experiment 1, with the exception of the measurement results at location d. specifically, here the majority of the response times at location d are around 2.7 s, while in experiment 1 only two measurements exceed 2.7 s. this reveals the fact that although the user is physically closer to mesh beacon d, in experiment 1 some of the presence message from the mobile app must have been received and then forwarded by the nearby mesh beacon c so a better performance (fewer packet losses) can be observed. moreover, if we compare the measurement results at location c of the two experiments, we can find that the average response time in experiment 2 is slightly higher than that in experiment 1. we speculate that this is because in experiment 1, when the user is standing beside mesh beacon c, some of the presence messages may have been received and then forwarded by either mesh beacon a or mesh beacon b. therefore, some of the measurement results are on the 2-hop distance (round-trip). however, in experiment 2 the round-trip distance from mesh beacon c to the mesh router is 4 hops in every measurement. this is the reason why the average response time becomes higher than that in experiment 1."
"in this subsection, we apply the fixed-point approach method that was proposed in section 3.2 to solve the resolvent of the sum of a finite number of maximally monotone operators."
"data query (or data request) is an operation closely related to the storage strategy and storage structure of the data, the main purpose of which is to load the required data into memory quickly and accurately for calculation. an effective data query strategy can avoid unnecessary network i/o and disk i/o, as well as reduce the load pressure of cpu and memory. in the forestry information system, there are different query methods for different services, such as range query, k-nn query, and join query."
"based on gis, remote sensing and gps, data mining and statistics can effectively analyze spatial data features by finding implicit connections in data, which provide important analytical tools for forestry big data in data analysis. this will deepen people's understanding and mastery of spatial data features and laws, and help researchers make correct predictions about future trends. statistics are often used in data analysis with rule attributes. data mining can be used to obtain potential information and make decision support."
"the closed-loop system is bounded-input boundedoutput (bibo) stable if the characteristic equation, δ(s), has no roots in the closed right-half of the s-plane [cit] . we now analyze the roots of δ(s). to do that, we define δ * (s) as follows:"
"in this section we review the literature in the field of ble-based locating systems and applications, as well as some existing implementations of bluetooth mesh networks. we also give a brief introduction to the bluetooth beacon technology."
"in a distributed file system-based storage strategy, the index is usually stored in the file system with the data files. in the process of data query, the system needs to access the local disk of the master node and load the global index. the system then accesses the local disks of slaves and loads the local index based on the global index. therefore, the disks are accessed twice before the data is loaded, which has great effect on the efficiency of the data query. for this problem, because the amount of index file data is relatively small, the system can choose to persist the index in memory to improve the efficiency of data query or use other strategies to reduce disk access times."
"in recent years, vtol uavs become very attractive due to their ability to perform quasi-stationary flight (hover or near hover flight), which enables their application to special missions such as monitoring and inspection for both military and civil uses [cit] . an accurate and robust altitude controller is critical for vtol uavs to achieve a quasi-stationary flight [cit] ."
"where m is the mass of the uav, z(t) is the altitude as a function of time t, f z (t) is the non-gravitational resultant force in the vertical direction, and g is the gravitational acceleration. under quasi-stationary flight conditions, the thrust force, which points toward the nose of the uav body, predominates over the aerodynamic forces [cit] . by neglecting the aerodynamic forces, the engine thrust, designated f t (t), can be regarded as the only non-gravitational force exerted on the uav. therefore, f z (t) is the vertical component of f t (t). because the attitude of the uav body can be effectively controlled to be vertical by a high-gain attitude controller under quasi-stationary flight conditions [cit], the engine thrust, f t (t), can be approximated as in the vertical direction. this results in the following:"
"among many available detection schemes for quantitative instruments developed throughout the years, the most frequently used methods rely on the reflectance photometers to obtain the gics signals, see, e.g., [cit] . at the same time, there has been a growing research interest in the development of image-based instruments for gics, see, e.g., [cit] . the critical design specification for image-based system is the image processing technology whose significance has now been well recognized in the bioinformatics community. in particular, it is of vital importance to look for an efficient image segmentation method to accurately distinguish the test and control lines from the gics image. in recent years, a number of methods have been introduced to achieve the goal of segmenting the test and control lines of gics images. some typical methods include the otsu threshold segmentation approach, the fuzzy c-means (fcm) clustering method and the cellular neural network (cnn) [cit] . a common limitation yet a major challenge for these methods, however, is that the acquired gics image usually involves unavoidable noises caused by various factors including temperature, humidity, colloidal gold and nonuniform permeation of specimens. also, it is often difficult to ensure the accuracy for images of low concentration where the noises take a great proportion and the signal-tonoise ratio is therefore small (i.e., the intensities of some background noises are much higher than the signals) [cit] . to address these issues, we propose to use the recently developed deep belief network (dbn) algorithm, a stateof-the-art machine learning technique, for the gics image segmentation in this paper."
"application server: a shared http server for the mesh routers from several bluetooth beacon networks. it buffers the presence states of the mobile users, and forwards the presence states to the cloud database."
"from fig. 6, we can see that the dbn approach provides a robust method for accurately extracting both lines from the gics images with different concentrations of the target analyte; especially, the concentration of specimens in the gics images can be low, middle or high. it can also be verified from table 1 that the dbn method in this paper has a satisfactory performance in image segmentation with high classification accuracy. therefore, the dbn has proven to be a novel approach for quantitative analysis of gics systems."
"our current implementation of messaging over the bluetooth beacon network is based on rebroadcasting in the advertising channels. it is easy to implement, and it precludes the need to pair bluetooth devices before sending messages to each other. furthermore, without having to run a routing protocol and maintain a routing table, the complexity and memory consumption of the bluetooth devices can be minimized. although sending messages by broadcast is less efficient, the overhead can be well-controlled if we design the rebroadcast mechanism carefully as well as dividing a large network"
"in this paper, we have developed a novel approach to quantitative analysis of a gold immunochromatographic strip (gics) using deep belief network (dbn) in order to enhance the robustness when accurately segmenting the test and control lines from the gics images. three features (including intensity, distance and difference) have been proposed for the dbn method to learn in order to successfully distinguish the test and control lines from the region of interest that is obtained by preprocessing the gics images. experiments have been carried out on different concentrations of hcg images. furthermore, several indices have also been proposed to verify the presented dbn method and demonstrate that the dbn approach fig. 7 the fitted line between the concentration hcg specimens and the riod via the least square approach fig. 8 the psnr comparison of segmenting images indeed gives high accuracy. future research directions would include the modification of dbns via adopting adaptively control strategies (e.g., [2-4, 10, 17, 18, 31, 32] ) so as to further improve the performance of dbns, and also developing more advanced image segmentation methods (e.g., [cit] ) for quantitative analysis of a gics."
"the ecological parameters of forest vegetation include tree height, dbh (diameter at breast height), and leaf area index. the data that the vegetation parameter inversion relies mainly on is different radar data [cit] . when inverting parameters such as tree height, laser radar and polarization interference synthetic aperture radar can be used to obtain measurement data of vertical structure [cit], such as using satellite largefootprint lidar icesat/glas data to obtain tree height information [cit] . for small-scale forests, uav remote sensing data can be used to obtain tree height information [cit] . for parameters such as canopy closure, they are generally calculated based on multi-spectral data sets, such as landsat tm and modis images [cit] . forest parameter inversion based on hyperspectral data can overcome the shortcomings that multispectral data has fewer bands and lower resolution, but it requires additional data preprocessing, which will occupy part of the computing resources [cit] ."
"based on (13), we can obtain the integrated single-input single-output altitude transfer function model, which characterizes the relationship between the engine rotor speed command and the uav altitude as follows:"
"as shown in figure 1, the overall system consists of five roles: mobile app, mesh beacon, mesh router, application server, and cloud database. a number of mesh beacons and a mesh router form a bluetooth beacon network. the five roles are elaborated as follows:"
"forest is a complex and widely distributed ecosystem. monitoring forest resources is a costly project. in forestry monitoring, forestry resource survey is an important process. forestry data obtained through forestry resource surveys is the basis for the forestry management. in this section, we first trace the key methods and techniques of forestry surveys in various periods and explain the development process of forestry big data, and then abstract the data types of forestry data."
"while a great number and variety of publicly-funded biomedical data are ostensibly \"open\", and some are accessible via aggregated databases, complex licensing issues hinder them from being put to their best use [cit] . a lack of licensing rigor and standardization forces data users to manually seek, often repeatedly and from multiple data providers, essential reuse and redistributions permissions. issues include missing licenses, non-standard licenses, and license provisions that are restrictive or incompatible. the legal interpretation of, and compliance with, database license and reuse agreements has become a significant burden and expense for many fields in the scientific community [cit] . a complex and lengthy set of legal negotiations is required for a data integration project to legally and freely redistribute all of its relevant data. ironically, few data resources have the capacity to pursue policy violations and, in our experience, most researchers who restrictively license their data do so because they want to be credited for their work and are unaware of the downstream reuse implications. thus, it is not uncommon for researchers to ignore license restrictions. this landscape does not benefit data providers, users, or scientific progress."
"common distributed file systems include hdfs [cit], ceph [cit], gridfs [cit], and glusterfs. as shown in fig. 5, the general pattern of forest data in stored procedures is described. in this figure, the data is mainly stored in two categories, vector data and raster data. vector data is stored in a non-relational database or distributed file system in the form of key-value pairs. for the storage strategy of raster data, metadata and pixel data are usually treated as two units for storage. when storing, they choose a way to store according to their own attribute characteristics and query structure. in the forestry data storage model based on distributed file system, the data needs to be formatted. in the case of raster data, the pixel data is split into rows, columns, or blocks and then stored in the file system. metadata can be stored with pixel data in a file system or stored separately in a relational database. forestry data can also be converted into objects with spatial attributes based on object-oriented thinking. after the data is divided, the data locality needs to be considered when it is to be stored. that is, if data objects are adjacent in space, they should be stored on the same node or rack to reduce the querying cost among nodes. we can assign a unique id to each data slice using a space fill curve (such as hilbert curve [cit] ) and organize and store the data according to the ids."
"we performed the stability analysis for the controller and determined the stability region in the parameter space. in addition, a uav hover flight experiment was conducted using the new altitude controller. due to the practical constraints, the uav had to be suspended under a gantry crane using a steel cable, which limited its moving space and produced interference with the uav during flight. the results of the engine rotor speed, uav vertical acceleration, uav vertical velocity and uav altitude showed that the uav altitude was effectively controlled at the target altitude, with fluctuations of less than ±3 cm, even with the interference of the steel cable. these results demonstrated the effectiveness of the new altitude controller."
"in order for biomedical discoveries to be translated into human health improvements, the underlying data must be thoroughly reusable: one should be able to access and recombine data in new ways and make these recombinations available to others. significant resources and influence have been invested and leveraged to make biomedical data publicly available and scientifically useful [cit] . projects such as the nih ncats translator, data commons, illuminating the druggable genome, bgee, and the monarch initiative demonstrate that efforts to aggregate and integrate data are seen as a worthwhile undertaking. however, despite these efforts, technical, logistical, descriptive, and legal barriers continue to impede data interoperability and reusability. we are specifically concerned with the ways in which data licensing practices have created widespread legal and financial barriers across the biomedical domain."
"it is well known that mathematics is the foundation of natural science. big data, as an emerging frontier discipline, has not yet formed its own independent mathematical theory system and modeling method. the main purpose of big data is to quickly learn and acquire knowledge from the data. when conducting the analysis and application of big data, it mainly relies on the traditional statistical basis and the theory and method related to artificial intelligence. therefore, they are also the main mathematical theoretical basis of current big data research. as one of the most important branches in the computer science, big data cannot meet the current demands for speed and precision in applications. it is necessary to optimize the storage and query of data from the perspective of the structure of the computer system. in this process, optimization is often made from the efficient use of memory, load balancing of storage and computation, and scheduling of computing resources. the motivation for proposing the concept of big data is to solve the problem that the data cannot be reasonably calculated in time and space in practical applications. until now, the main value of big data still lies in the application, and the real value of big data can only be reflected in practical applications. in forestry applications, along with big data plays an increasingly important role in forest monitoring and forestry decision-making, smart forestry has ushered in new development opportunities."
"forestry data analysis based on stand-alone gis tools cannot meet the requirements of speed and accuracy in massive heterogeneous forestry data analysis. applying big data technology to forestry can simplify the computational complexity, which not only facilitates the mining and utilization of data, but also improves the processing efficiency of forestry data. therefore, the development of forestry big data brings new opportunities for the forestry management."
"comparing with general big data, forestry big data is not just a huge amount of data. the greater challenge is its complex data organization and its diverse data structure. in forestry information systems, many services often use more than one type of data for calculation and analysis. a system that can provide common services needs to deal with vast amounts of heterogeneous, irregular forestry data. moreover, forestry services tend to have strict time requirements. whether or not to respond to requests submitted by different forestry applications in real time is an important task in the field of forestry big data."
"relational database can only store data with fixed structure, but for forestry big data, data is irregular and many data can't be stored according to fixed pattern. as the amount of data increases, the load pressure of the database also increases, which has high requirements for the system's response speed and pressure resistance. therefore, the traditional relational database cannot guarantee the stability and availability of the system, and it is not suitable for storing and querying massive forestry data. commonly, relational databases are used to assist non-relational databases or distributed file systems for data storage. for some data with a small data size and a regular pattern, one can choose a relational database for storage. this makes the structure of the storage system simple and clear and reduces its load pressure."
"the restricted boltzmann machine (rbm) [cit], as shown in fig. 3, is a bipartite graph in which visible units v are linked to hidden units h through undirected weighted connections. in general, visible units represent observations, and hidden units tend to represent features. a special characteristic of rbm is that there are no connections in any two visible units and also any two hidden units. due to different situations, there are two types of rbms, namely bernoulli-bernoulli rbm with binary visible and hidden units, and gaussian-bernoulli rbm where hidden units are binary but the visible units are linear with gaussian noise."
"after the engines were switched to automatic control at 3.36 s, there was a sudden increase in the rotor speed command. the rotor speed response also started to increase but only gradually. this resulted in a clear discrepancy between the rotor speed command and the rotor speed response. this discrepancy is why an integrated controller is needed for uavs equipped with this type of engine. at approximately 4.4 s, the rotor speed response exceeded the equilibrium speed. the uav started to take off at this time. thereafter, an apparent increase began to build in the uav acceleration, uav velocity and uav altitude, as shown in figs. 15-17. note that a filtration method developed in our previous work was applied to suppress the noise in the acceleration measurements after the uav took off, and it took approximately half a second to converge [cit] . therefore, the filtration result of the uav vertical acceleration (red line in fig. 15) started at approximately 4.9 s. the filtration result was the acceleration actually used by the controller."
"in our system, we use ble non-connectable advertising packets to implement all types of messages so there would be no need for the devices to pair with each other before sending any data. the breakdown of a ble air interface packet is shown in figure 3 . according to bluetooth specification v4. 1 [cit], the maximum size of a ble advertising packet data unit (pdu) is 39 bytes. excluding the 2-byte header field and the 6-byte advertiser's address field, there are up to 31 bytes to carry the advertising data. the advertising data is further divided into a sequence of advertising data (ad) structures. the ibeacon protocol uses ad structure 1 and ad structure 2 combined to carry its advertisement information, with the message format shown in figure 4 ."
"in order to facilitate several points of evaluation in the rdp rubric and illustrate shared qualities among related licenses, the rdp uses an internal categorization of licenses and licensing information, organizing them into six reuse-oriented types. while we acknowledge that the licensing landscape is much more complicated than these categories communicate, classifying licenses via these basic terms was conceptually helpful and provided needed efficiency and simplicity during the evaluation process. the six license types are described and examples are provided below."
"deep belief network (dbn) was proposed by hinton [cit], and since then, dbn has been extensively investigated and widely employed in both theory and applications of various deep learning tasks. as shown in fig. 4, the dbn is a neural network constructed from multilayer rbm and one-layer backpropagation (bp). dbn is actually a greedy and multilayer-formed learning model combined with a stack of rbms. a distinguishing feature of the dbn is its capability of obtaining states of hidden layers units by one forward pass. in the course of training a dbn, the first step is the so-called pre-training that stacks a lot of rbms layer by layer in a bottom-up manner. once the parameters of the lower-layer rbm are determined by learning, the vectors of hidden feature activations can be utilized as the input of visible units for the higher-layer rbm. then, in the fine-tuning stage, the error back propagation approach is utilized to adjust the weights of whole network. for the ith node of the output layer, we suppose that the actual output is o i and the expected output is d i . the sensitivity d i can be calculated by:"
"our current implementation of messaging over the bluetooth beacon network is based on rebroadcasting in the advertising channels. it is easy to implement, and it precludes the need to pair bluetooth devices before sending messages to each other. furthermore, without having to run a routing protocol and maintain a routing table, the complexity and memory consumption of the bluetooth devices can be minimized. although sending messages by broadcast is less efficient, the overhead can be well-controlled if we design the rebroadcast mechanism carefully as well as dividing a large network into smaller subareas properly. in the future, we plan to replace the bluetooth interfaces with version 5.0 and incorporate the official bluetooth mesh protocol into our system, to achieve better locating performance and the interoperability between ble-based mesh networks."
"unknown: this category captures licensing statements that have conflicting terms, incompatible license references, or are so non-standard or unclear that a data resource's reuse terms cannot not be reasonably understood or confirmed."
"(criteria b) 32 (57%) of the resources included a license that was explicit, comprehensive, and unambiguous in scope over the data. c se he el. he se nt 22 ) pe ell it, (criteria c) we found that 48 (86%) of the resources passed criteria c by making all of their data reasonably accessible at an api endpoint or structured download site."
"this paper is organized as follows. in section ii, the configuration of the tail-sitter uav will be introduced. in section iii, the integrated altitude model for uav quasistationary flight will be discussed. in section iv, the altitude control scheme is proposed and analyzed. the experimental results will be presented in section v. finally, we conclude the paper in section vi."
"for vector data visualization, firstly, the spatial objects need to be rasterized according to different scales, and then the raster images generated by these objects need to be aggregated, superimposed and rendered according to the spatial position [cit] . vector data visualization is often used to create thematic maps, or some location-based services. at present, in many web-based applications, the system superimposes vector data and raster data to display, so that forestry scenes and detailed forestry features can be combined. combining with 3d technology, forestry data can be visualized in threedimensional space [cit] . then we can understand the impact of spatial attributes such as terrain on forestry through spatial analysis, and visualize the results, which greatly facilitates forestry decision-making."
"the main novelty of our work is primarily twofold. (1) a new framework of automatic image inspection is established to solve the problems in the quantitative evaluation of gold immunochromatographic strip, where the dbn algorithm is applied to accurately extract the test and control lines. (2) by learning three features including intensity, distance and difference, the presented dbn can distinguish the test and control lines from the region of interest that is obtained by preprocessing the gics images. it is shown from the experiment results that the proposed method provides high accuracy in terms of the performance of the segmentation, the feature parameter, the fitting line and the peak signal-to-noise ratio."
"the sequence diagram in figure 2 shows an example of the flow of operations, where the labeled numbers correspond to the 7 items described above. in this example, there are three mesh beacons on the transmission path between the mobile app and the mesh router. from the diagram, we can see how the interactivity between the mobile user and the backend system is achieved. from the users' perspective, they are able to receive more than the basic locating service. benefiting from interacting with the backend system over a single bluetooth beacon network, they can enjoy personalized services without having to connect to the internet. from the system operators' perspective, they are able to detect the presence of specific users at particular locations, and then offer personalized services to them. with the presence state stored in the cloud database, the system operators can also track and analyze user behaviors using the users' real-time and past locations."
"as mentioned before, the uav was suspended under the gantry crane with a 4 mm stainless steel cable. initially, the cable was in a tensed condition because it needed to carry the weight of the uav. after the uav took off, the cable became loosened because of the elevation change of the uav. the loosened cable would exert a force on the uav body due to the weight of the cable. the force in the vertical direction may be neglected compared with the engine thrust force. in addition, our altitude controller is robust enough that the altitude of the uav was barely impacted. however, the horizontal component of the cable force, in addition to other external disturbances (such as wind), may influence the uav position in the horizontal plane. in this work, we did not impose horizontal control for our uav because altitude control is the focus here."
"restrictive: restrictive licenses provide more permissions compared to data resources wherein all copyrights have been reserved by the provider, but still include terms that may hinder data integration and reuse. examples include cc by nd 4.0."
"with the development of the technology, the speed and accuracy of forestry data acquisition have been greatly improved. to cope with the growth of massive data, we need to improve the ability to calculate and analyze data. based on the current development of big data and forestry technologies, in this paper we summarize the data types and data formats of forestry big data as well as the theoretical structure of forestry big data. and the five-layer model structure of forestry big data is proposed, including data acquisition, storage, query, analysis and visualization and application. based on the current development status, the key technologies in each layer are analyzed in detail. then we summarize the challenges faced by forestry big data, and predict the problems of forestry big data development process and future development trends finally."
"where (t) represents the rotor speed response, δ (t) denotes the rotor speed command, t d is the time delay of the rotor speed model, k is the proportional coefficient, a 3, a 2, a 1 and a 0 are the polynomial coefficients of the nonlinear thrust model, and f e (t) is the thrust force of a single turbine engine."
"the (re)usable data project (rdp) originated from discussions on the ncats biomedical data translator project (https://ncats.nih.gov/translator), which aims to integrate and leverage biomedical information across a vast diversity of sources. while licensing issues influenced many of the reuse barriers we discussed, participants could not agree on licensing standards, illustrating the complexity and confusing state of the data licensing landscape. the rdp was created to systematically describe the current data licensing landscape from the perspective of data aggregation, reuse, and redistribution of publicly funded biological and biomedical data resources. the rdp's rubric for evaluating data reusability and re-distributability includes a set of criteria and a scoring system that categorizes and weighs licensing and database characteristics, for example the findability and type of licensing terms, negotiation requirements, scope, accessibility, as well as use case and user type restrictions. the rdp aimed to develop a scoring system that is intuitive and comprehensive, but also defensible and agnostic to domain and scientific task. it is important to note that we are not lawyers and the rdp does not provide legal advice. we are a group of scientists, engineers, librarians, and specialists that are concerned about the use and reuse of increasingly interconnected, derived, and reprocessed data. we want to make sure that data-driven scientific endeavors can work with one another in meaningful ways without undue legal concerns. we hope the licensing evaluation rubric will help others navigate the legal synthesis and redistribution of public data and enable data providers to choose licensing terms that make it easier for others to use and redistribute their data."
"at the beginning of the experiment, the engine rotor speed was controlled manually. the rotor speed command was increased gradually until it reached a value slightly lower than the equilibrium rotor speed ( 0 ). then, the uav was switched to automatic control. this process was used to prevent the engine overshoot that could occur if automatic control were to be applied immediately after the engines started. we now present the experimental results. fig. 14 shows the engine rotor speed command and the actual engine rotor speed response during the experiment. figs. 15-17 show the uav vertical acceleration, uav vertical velocity (estimated based on acceleration and altitude measurements) and uav altitude. fig. 18 shows the pictures of the uav status at different times during the experiment. as mentioned before, the engines were manually controlled at the beginning until they reached a rotor speed close to the equilibrium speed. this corresponds to the blue line from 0 s to 3.36 s, as shown in fig. 14 . because the rotor speed was increased gradually, the rotor speed response (shown in red line in fig. 14) was close to the command. during this manual control period, the uav remained stationary because the thrust force was not sufficient to lift the uav. therefore, the imu measured the gravitational acceleration, which equals 9.8 m/s 2, as shown in fig. 15 . the noise in the acceleration measurement was the result of many factors such as the vibration of the turbine engines and the external disturbances. consequently, the uav vertical velocity presented slight fluctuations about zero (fig. 16), and the uav altitude remained at approximately 1.5 m. this period corresponds to the flight status shown in fig. 18a ."
"the majority of resources failed to receive a full star for parts a, d, or e of the rubric (figure 3) . as data users and stewards, we have encountered and been frustrated by the ways in which licensing issues hinder data reuse, integration, and redistribution. while 48 (86%) of the resources we evaluated provided easy and actionable data access, only 10 (18%) received a full 5-star rating and 32 (57%) of the resources received 3 stars or less, indicating that there were serious barriers to reuse. these findings support our experience, in that the data we need is often accessible, but cannot be reused or redistributed. missing licensing information and the variability and potential incompatibility of license types are primary areas needing improvement. for large data integration projects that ingest data from multiple resources to derive new knowledge and provide new tools, this landscape requires costly interactions with individual organizations and institutions."
"the rdp's main efforts have been the creation and application of a rubric that defines the licensing characteristics of aggregated data resources and measures how these licensing behaviors impact reuse. this includes the capture of structured metadata that provide a high-level description of a resource, a working view of its licensing, information to reconstruct the decisions behind our evaluations, and additional notes of interest to others wanting to understand the reusability of a resource's data. it is important to note that the rubric was constructed for the evaluation of and only applied to public resources databases, not to individual's datasets or contributions."
"at present, the data is mainly processed through various big data and machine learning technologies, including deep learning, support vector machines, and so on. for example, deep learning can be used to extract and classify tree species from lidar data [cit] . facing the huge challenge of computing performance due to the increasing data volume, [cit] parallelize the isodata clustering algorithm based on mapreduce, which is easy to use. [cit] implement a satellite remote sensing classification method based on hadoop, which includes two parts: data storage and processing. in the storage part, the system stores heterogeneous data through hdfs. in the processing part it uses the mapreduce programming paradigm to implement the k-means algorithm. the system can improve the classification accuracy, and also make full use of the integrated configuration resources."
criteria b: does the license clearly define the terms of continuing reuse without need for negotiation with the data creators or resource curators? does the license have a complete scope that covers all of the data and not just a portion?
"to date, we have fully evaluated 56 data resources with the rdp's star rubric. as the idea for the rdp emerged from an ncats biomedical data translator meeting, we originally evaluated data resources used by the translator and the monarch initiative, wherein the reuse and free redistribution of publicly available data for disease discovery has been particularly burdensome. we then expanded our scope to evaluate model organism databases (mods) and data resources that the newly funded nih data commons pilot phase will address [cit] . we also evaluated several resources that were brought to our attention by the community."
"we do not envision the rdp star rubric and evaluation data as only a tool for analyzing the past. rather, the rubric could be used to test and guide future licensing choices. for example, it could be used by groups considering how to plan for the long-term sustainability of data resources, which may include a variety of monetization options. the rdp rubric could be applied to understand the implications of different strategies, including the potential interoperability between resources and as check on continued data reusability."
"where m and n represent the numbers of rows and columns in the input images, respectively. as an important performance indicator, the psnr describes the ratio of the signal's peak value over the magnitude of the background noise. generally, we prefer a larger psnr value since the binary spot mask in this way fits better with the raw image surface [cit] . figure 8 shows the psnr values of dbn method utilized for segmenting gics images of eight different concentrations of hcg. it is obvious that the dbn method possesses good performance of segmentation, which provides higher accuracy than the other existing methods for segmenting the gics images shown in the [cit] ."
the kransnoselskii-mann algorithm is a popular iterative algorithm for finding fixed points of non-expansive operators. the convergence of it is summarized in the following theorem.
"in propositions 1 and 2, the resolvent of composed operators u −1 a * ta is computed either with t −1 or requiring au −1 a * to satisfy additional conditions. in practice, it is still difficult to evaluate it without these conditions. to overcome this difficulty, in this subsection, we propose a fixed point algorithm to compute the resolvent of u −1 a * ta. our method discards these conditions on t −1 and au −1 a * ."
"considering the distinguished features of the gics images, we divide the extracted roi into two parts for segmenting in order to reduce the complexity of algorithm and avoid unnecessary calculations. one part consists of the control line and corresponding background, while the other part consists of the test line and its background. based on the dbn approach mentioned above, all parameters of the dbn have been fixed for all experiments. firstly, we find that the performance of segmentation is best when the network has two rbm layers and each layer has 100 hidden nodes. next, we set the learning rate as 1 in the pretraining stage as well as the fine-tuning state, and the minibatch sizes for both stages are set as 100 and 50, respectively. finally, the number of iteration is set as 20 because the classification accuracy tends to stable after that."
"inspired and motivated by the work of moudafi [cit], in this paper, we discussed the resolvent of composed operators u −1 a * ta. under some additional conditions, we obtained explicit solutions of the resolvent of composed operators. the obtained results generalized and extended the classical results of fukushima [cit] and bauschke and combettes [cit] . on the other hand, we presented a fixed point algorithm approach for computing the resolvent of composed operators. by virtue of the krasnoselskii-mann algorithm for finding fixed points of non-expansive operators, we proved that the strong convergence of the proposed fixed-point iterative algorithm. as applications, we employed the proposed algorithm to solve the scaled proximity operator of a convex function composed of a linear operator (48), and the proximity operator of a finite sum of proper, lower semi-continuous convex functions (50)."
"raster data is composed of a series of row and column grids, in term of matrix form. for the remote sensing image, these grids are represented as pixels, and each pixel represents an attribute value. raster data can be represented by a matrix or a set of matrices, and different raster data have different bands. as shown in fig. 2, the 24-bit rgb image can be represented as a dataset with three bands. for hyperspectral remote sensing images, the number of bands can reach several tens. in principle, the logical structures are similar, but the data processing efficiencies differ greatly."
"when the mesh router hears the presence message originated from a certain mobile app, it uploads the content in the presence message to the application server. meanwhile, it sends an acknowledgement message back to the mobile app by broadcast."
"it should be mentioned that it is a challenging task to process the gics images due to their inherent features outlined as follows. first, in order to enhance the efficiency, we just concentrate on the region of interest with the test and control lines immobilized on the strip. therefore, the obtained gics images should be preprocessed to acquire the region of interest via some commonly used image segmentation operators. second, both lines in the reading window might become blurred, uncertain and mixed with the background since they are generally made/ smeared via a roller in a non-uniform manner. in addition, when the sample to be detected (such as urine, blood and serum) flows over the membrane, some interference noises are inevitable on the detection window of strip [cit] . in order to overcome the challenges mentioned above, we intend to establish a dbn-based framework for quantitative analysis of the gics by accurately extracting the test and control lines from the acquired gics images."
"in this experiment, we would like to measure the response time from the mobile app users' perspective. specifically, we put the smartphone beside the mesh beacons and then open the mobile app. at each of the four locations (a through d), we measure the response time five times, and show the results in figure 9 . note that since the locating process is based on an internal table lookup in the mobile app, the time needed to finish the locating process does not depend on the real locations. therefore, in figure 9 we only show the time taken for the mobile app to receive the acknowledgement message. from figure 9 we can see that at both locations a and b the observed response times are relatively stable, but at locations c and d some of the response times are more than 2.7 s. the reason is that at locations a and b, they are only one hop away from the mesh router; while at locations c and d, they are 2 and 3 hops away, respectively. it is straightforward that with a greater number of hops on the path, the probability of a lost presence message or acknowledgement message is also higher. furthermore, we need to describe the behavior of the mobile app in order to justify the result. in our implementation, the mobile app takes about 0.7 s to initialize the advertisement process, and then proceed to advertise the presence message for 1 s. after that, the mobile app will start listening to the acknowledgement message from the mesh router. this explains why the observed minimum response time is around 1.7 s. if no acknowledgement message is received within 1 s, the mobile app will advertise the presence message again for another 1 s. knowing the behavior of the mobile app, from figure 9 we can see that at both locations a and b the observed response times are relatively stable, but at locations c and d some of the response times are more than 2.7 s. the reason is that at locations a and b, they are only one hop away from the mesh router; while at locations c and d, they are 2 and 3 hops away, respectively. it is straightforward that with a greater number of hops on the path, the probability of a lost presence message or acknowledgement message is also higher. furthermore, we need to describe the behavior of the mobile app in order to justify the result. in our implementation, the mobile app takes about 0.7 s to initialize the advertisement process, and then proceed to advertise the presence message for 1 s. after that, the mobile app will start listening to the acknowledgement message from the mesh router. this explains why the observed minimum response time is around 1.7 s. if no acknowledgement message is received within 1 s, the mobile app will advertise the presence message again for another 1 s. knowing the behavior of the mobile app, we can conclude that at locations a and b, the mobile app successfully received the acknowledgement message after the first advertisement period. however, from the measurement results at locations c and d, we can find that some of the acknowledgement messages arrived after the second advertising period. nevertheless, we think that the response time can be well-controlled if we place any two neighboring mesh beacons within a reasonable distance to keep the packet loss rate low. furthermore, it is also possible to reduce the response time by shortening the advertising period and the waiting period because this is simply an implementation issue. since the 0.7-s initialization process is inevitable, we expect that in the best case the minimum response time can be within 1 s."
"note that k and k g are the parameters of the uav engine model, and they are constant for a particular uav. because there are no finite zeros of e st d, the roots of δ * (s) are identical to those of δ(s) [cit] . therefore, we study the roots of δ * (s) instead of doing that directly on δ(s)."
"a \"private pool\" license is one where the resource requires data users to add their own data to the pool, or limit the accessibility of derivative data to others that have also joined the pool. conceptually, this is similar to some copyleft licenses, but without the public \"open\" component."
"gps is a highly accurate satellite-based radio navigation system, which provides information of location, speed and time. in forestry monitoring, gps [cit] can accurately measure the location of forestry objects including points, lines, and polygons. based on monitoring data from gps, researchers can perform a range of forest management tasks such as forest mapping, forest compartment boundary surveys, forest road surveys, ground live events (remote sensing), and resource inventory."
"for forestry data, before building a computational model, we need to design a data structure that can be used for distributed computing based on the data model of the storage layer. this is a process of abstracting data [cit] . then, after the query layer loads the data into memory, the data is converted (such as the rdd.map() operator in spark) to obtain the custom data object, and the data mining algorithm is applied to each partition of the object in memory. after the query layer loads the data into memory, the data is converted (such as the rdd.map() operator in spark) to get the custom data object, and then the data mining algorithm is applied to each partition of the memory object. in a distributed environment, it is necessary to consider the data communication cost amongst nodes and the caching and serialization of data before parallelizing the algorithm. by considering data locality, the system can reduce the data communication cost between nodes, and it can deal with boundary problems well by loading data redundantly [cit] . when data is computed in memory, it needs to be cached, compressed and serialized according to the requirements. by properly optimizing the memory processing mechanism in the calculation process, the load pressure of the system memory decreases significantly, thereby improving the performance of data analysis. based on the above analysis, in the forestry big data framework, the system can encapsulate the algorithms and strategies of forestry data analysis and provide interfaces that can be called by other programs. we can also further optimize forestry big data systems based on existing research. for instance, for the analytical calculation of remote sensing data, it can be abstracted into a large matrix. then, we can optimize the performance of the system based on existing research related to matrix calculations [cit] ."
"the sequence diagram in figure 2 shows an example of the flow of operations, where the labeled numbers correspond to the 7 items described above. in this example, there are three mesh beacons on the transmission path between the mobile app and the mesh router. from the diagram, we can see how the interactivity between the mobile user and the backend system is achieved. from the users' perspective, they are able to receive more than the basic locating service. benefiting from interacting with the backend system over a single bluetooth beacon network, they can enjoy personalized services without having to connect to the internet. from the system operators' perspective, they are able to detect the presence of specific users at particular locations, and then offer personalized services to them. with the presence state stored in the cloud database, the system operators can also track and analyze user behaviors using the users' real-time and past locations. 7. when the acknowledgement message from the mesh router has been received at the mobile app, the mobile app displays a welcome message indicating that the backend system is aware of the user's presence."
"criteria c: does the resource provide its data in a reasonable good-faith location, and is there a reasonable and transparent method of accessing that data in bulk?"
"to solve the resolvent operator (43), we formally reformulate it as a special case of the resolvent operator (13), which was studied in the previous section. more precisely, we obtain the following convergence theorem. be generated by the following:"
"the behavior of forest resources survey occurred when humans began forest management activities [cit] . early forestry surveys were totally based on visual method. in the 18th century, the development of mathematics promoted the forest mensuration. [cit] that the forest mensuration system was formed. [cit] s, the mathematical statistics and the theory of aerial photogrammetry and photo interpretation were introduced into forest survey, forest survey technology has been greatly improved and rapidly developed. however, at that time, there was still no unified technical system amongst the countries for the investigation of forest resources [cit] . the defects of these technologies mainly include the following points: (1) due to aerial photo is only applicable in a few applications, a large amount of information is not mined and utilized; (2) since the aerial photo is obtained through the center projection, the workload of transfer is huge and a large error will occur when calculating area [cit] ."
"data analysis: for forestry data processing, the system is not only required to respond to the requests from users without delay, but also to process the collected data (such as the monitoring data collected by the ground sensor in real time) in real time. however, there is relatively few researches on this need in current forestry information systems and forestry data-based applications. therefore, the related technology of flow calculation can be applied to the processing of forestry data, such as storm, so that the data can be calculated in real time and automatically. for data visualization, the technology currently applied in forestry information systems is only to show the results, which is not conducive to interactive analysis and immersive visualization. having combined virtual reality technology and geographic information system together, vrgis (virtual reality geographic information system) plays an important role in many frontier fields such as ''smart city'' [cit] . therefore, during forestry data visualization, vr technology can be introduced to improve the interaction capability of the system and the user experience."
mobile app: a locating application running on mobile users' smartphones. it listens to the beacon messages sent by the mesh beacons to figure out the current location of the mobile user. it responds to the received beacon message by broadcasting the mobile user's encrypted presence message.
"currently, the forestry survey is based on ''3s'' technology. more advanced data acquisition and processing technologies are introduced, which make the survey more precise, intelligent and refined. in the field of forestry data collection, the methods are gradually diversified and the accuracy is getting higher. for instance, we can use ground laser scanners [cit] to generate point cloud data to build 3d data models for forest surveys, or use radio frequency technology [cit] to collect real-time data on trees and the environment. by combining big data technology with traditional forestry data analysis methods, the data processing efficiency is improved, which meets the need for real-time and accuracy. therefore, big data has become an important technical support for the forestry information system. big data has changed the process and structure of forestry data processing. the data can be effectively managed by abstracting the data model."
"the uav is also equipped with a micro-computer, an inertial measurement unit (imu), a hall sensor and a laser range finder. the micro-computer controls the uav by processing all the measured data and providing commands to the engines. the imu (analog devices adis16488a) measures the attitude and acceleration of the uav and provides the data to the micro-computer at a frequency of 25 hz. the imu is installed at the center of gravity of the uav to prevent the rotation (due to external disturbance) of the uav body from influencing the acceleration measurement. the hall sensor measures the engine rotor speed with high accuracy (error range of ±10 rpm for a rotor speed of over 80,000 rpm). the laser range finder (dimetix dlsb-15) measures the altitude (height above the ground) of the uav at a frequency of 5 hz, and it has a measurement error of less than ±1.5 mm."
"data is the foundation of science, and there is an increasing focus on how data can be reused and enhanced to drive scientific discoveries. however, most seemingly \"open data\" do not provide legal permissions for reuse and redistribution. not being able to integrate and redistribute our collective data resources blocks innovation, and stymies the creation of life-improving diagnostic and drug selection tools. to help the biomedical research and research support communities (e.g. libraries, funders, repositories, etc.) understand and navigate the data licensing landscape, the (re)usable data project (rdp) (http://reusabledata.org) assesses the licensing characteristics of data resources and how licensing behaviors impact reuse. we have created a ruleset to determine the reusability of data resources and have applied it to 56 scientific data resources (i.e. databases) to date. the results show significant reuse and interoperability barriers. inspired by game-changing projects like creative commons, the wikipedia foundation, and the free software movement, we hope to engage the scientific community in the discussion regarding the legal use and reuse of scientific data, including the balance of openness and how to create sustainable data resources in an increasingly competitive environment."
"forestry big data is mainly developed from the three aspects of mathematical foundation, system structure and application value. in this paper, we focus on the systematic structure with extensive research value and development prospects, and briefly introduce the relevant theories and developments of the other two aspects. the entire system of forestry big data was analyzed in depth. in the future, big data will bring greater opportunities for forestry development, and there will be more challenges waiting for researchers to overcome"
"the logical structure of the data. one is the raster data model that divides geographic information into finite cells which is assigned a specific value according to its attributes [cit] and the other is the vector data model represented by a sequence of finite points and line segments. both of the data models can be used to represent any spatial conceptual model. however, based on the characteristics of these models, vector data models are often used for object-based models, such as a building, a river, or a type of land. raster data models are commonly used for models with ''field'' properties, such as elevation models, temperature models, etc. the remote sensing image is a typical raster data model and one of the most important forestry data."
"when data is stored based on a distributed file system, the data structure can be flexibly designed according to requirements. the distributed file system can be used to improve the throughput and reliability of the system, so that the data can be stored stably and efficiently."
"the rest of the paper is organized as follows. in section 2, we review some backgrounds on monotone operator theory. in section 3, we first investigate the solution of the resolvent of composed operators u −1 a * ta. second, we propose a fixed point approach for solving the resolvent of u −1 a * ta. finally, we employ the proposed fixed point algorithm to compute the resolvent of the sum of a finite number of maximally monotone operators with u. in section 4, we apply the obtained results to solve the problem of computing scaled proximity operators of a convex function composed by a linear operator and a finite sum of proper, lower semi-continuous convex functions, respectively. we give some conclusions and future work in the last section."
"with δ r (w) and δ i (w) represent the real and imaginary parts of δ * (jw), respectively, and they are expressed as follows:"
"as soon as any mesh beacon hears the presence message, it uses the managed-flood-based approach to rebroadcast the presence message. the purpose of rebroadcasting the presence message is to relay it to the mesh router. the managed-flood-based approach can make sure that once a specific presence message has been broadcasted by a mesh beacon, the mesh beacon will not rebroadcast it again. 4."
"in traditional forestry research, forestry data analysis is based on gis and spatial data mining techniques [cit] . the biggest problem with this strategy is that the complexity of the forestry data makes data mining and data analysis more difficult. and data mining algorithms often require a lot of iterative calculations. therefore, forestry data mining is a task that requires a lot of time and computing resources. in the forestry big data system, the data mining algorithm can be parallelized by the underlying distributed computing framework (such as spark or hadoop), which can improve the calculation efficiency and decrease the running time. developers can use the mahout [cit] machine learning toolkit or the spark mllib [cit] toolkit for forestry data mining and analysis, or parallelize traditional machine learning algorithms as needed. giachetta [cit] implement a large-scale geospatial and remote sensing data processing framework based on hadoop. the framework is highly scalable and adaptable, allowing previously algorithms and toolkits to be easily parallelized with fewer changes. [cit] parallelize the k-means algorithm based on spark, and they prove that the scheme is with high performance and scalability."
"through visualization technology, the system can intuitively present information such as relationships between data objects and real-time state changes in multi-dimensional space to the user. moreover, users can also analyze large amounts of complex data simply and efficiently by interacting with the system in real time and visualizing the results. therefore, visualization technology plays an important role in forest management."
"for the purpose of showing the performance of segmentation comprehensively, we choose different levels of analyte concentrations as testing sample. three typical simulation results by using dbn approach for segmenting the gics images are shown in fig. 6 when the concentrations of the target analyte are, respectively, 75, 200 and 500 ml. furthermore, the classification accuracy of all testing images is listed in table 1 ."
"currently, indexes in distributed systems based on forestry big data are mainly based on hadoop or spark. due to memory limitations, for most big data systems, such as spatialhadoop [cit], the secondary index model is commonly used, that is, data is indexed by global index and local index. as shown in fig. 6, it is a secondary index structure. typically, in the hadoop architecture, local indexes are used to index spatial data inside each partition, which is stored in the datanode node. the global index is used to index each partition and is stored in the namenode node."
"the current technology and strategy of forestry big data can effectively deal with massive forestry data and meet the requirements of real-time query and calculation for forestry applications. and with the development of 5g technology [cit] and the widespread application of internet of things [cit] technology, the forestry big data technology will be further developed and more widely used. in this section, we mainly introduce the future development trend of forestry big data from three aspects: data acquisition, data analysis and data opening."
"a new prototype tail-sitter vtol uav has been developed at tsinghua university as shown in figure 1 . the uav is equipped with two jetcat p200 micro turbine engines. each engine is able to provide a maximum thrust of 230 n at a maximum rotor speed of 112,000 revolutions per minute (rpm). with two engines, the uav has a maximum thrust of 460 n, which corresponds to a takeoff mass of approximately 46 kg. the length of the uav (from nose to tail) is close to 2 m, and the wingspan is approximately 1.6 m. the overall weight (including the engines) of the uav is approximately 23 kg, depending on the amount of fuel carried."
"in this section, we consider the problem of computing the resolvent of composed operators (13) . the obtained results extend and generalize the corresponding results of fukushima [cit] and bauschke and combettes [cit], respectively. second, we develop a fixed point approach for computing the resolvent of u −1 a * ta. we also propose a simple and efficient iterative algorithm to approximate the fixed point. the convergence of this algorithm is established in general hilbert spaces. finally, we apply the fixed point method to solve the resolvent of the sum of a finite family of maximally monotone operators."
"in response to forest fires, [cit] establish a forest fire forecasting and early warning system based on hadoop and other open source large data technologies."
"mesh beacon: a beacon device that can send, receive, and relay messages in the bluetooth beacon network. specifically, it not only broadcasts beacon messages, but also receives presence message from mobile apps, and relays the presence message to the application server. 3."
"in traditional forestry monitoring, data is stored and calculated in a single machine. when remote sensing technology is in its infancy, the amount of data is small and the quality of data is poor, so that data obtained by techniques such as remote sensing cannot be effectively applied to forestry decision-making. with the development of remote sensing technology, the resolution of remote sensing data is improved significantly, and the accuracy of its positioning function is higher, but the scale of remote sensing data greatly increases. the traditional computing and storage methods are no longer suitable for the current large-scale forestry data and performance requirements in forestry services."
"in reality, the uav control performance is inevitably influenced by external disturbances and the control model uncertainties. the control system needs to be sufficiently robust to ensure a successful flight. the gain margin and the phase margin are two important measures for quantifying the robustness of a control design [cit] . we need to design our controller to achieve certain gain and phase margins. the altitude control system with the gain-phase margin tester, t (a, ϕ), is shown in fig. 9 . the tester is typically expressed by the following:"
"in this section, we introduce the process of gics image via the deep belief network. the flowchart of the dbn-based gics image segmentation is shown in fig. 5, and our aim is to learn features to distinguish the test and control lines from the acquired strip images. the main objective of segmenting the gics image is to determine whether pixels in the image belong to the test line or the control line, and therefore, it can be regarded as a classification problem. in general, the size and pixel numbers of the images acquired from strip are slightly different, and this is not conducive to the image processing. therefore, the acquired images should be preprocessed at first so as to extract region of interest (roi) with the fixed size. the selection of the input feature is particularly important for the dbn as it plays a key role in the performance of classification. in this paper, each pixel in the image is treated as a sample, and three factors have been taken into account for selecting the input feature of dbn."
"mesh router: a gateway device between the bluetooth beacon network and the rest of the network. it is responsible for collecting the presence messages from the mobile apps within the range of the bluetooth beacon network. once the mesh router receives a presence message, it will upload the presence state to the application server through hypertext transfer protocol (http), and send an acknowledgement message back to the mobile app in response to receiving the presence message from a specific user."
"copyright: this category is used both for licensing statements that positively assert a resource provider's exclusive copyrights, often referred to as \"all rights reserved\", and for when a resource makes no statement about the disposition of its data. under current us copyright law, creators do not have to explicitly register or copymark their creations to claim their exclusive rights [cit] ."
"in this section, we will first show the implementation details of our system, including the hardware and the software. then, we will describe the experiments to verify the operations of our system. finally, we will discuss some implementation issues when deploying our system. table 1 lists the hardware and software used in our prototype system. our mobile app is developed to run on apple ios. mesh beacons and mesh routers are implemented using the redbear development board [cit] because of its small size and the co-existence of ble and wi-fi interfaces. the application server is an ubuntu pc with a node.js web server. the cloud database is implemented using google firebase [cit], which also provides the user registration and authentication functionalities for our mobile app. our ble beacon network is deployed on the 8th floor of the engineering building in our campus. figure 7 shows the floor plan where we deployed four mesh beacons (the blue ones labeled a through d) and one mesh router (the red one). table 1 . hardware and software used in our prototype system."
"while the rdp's star rubric and evaluation results provide a general view of the data resource licensing landscape, we believe a more in-depth approach would be valuable. we are interested, for example, in developing criteria that would define and assess more complicated interactions and compatibility characteristics between data resources. exploring the license interaction space more deeply would require the creation of a richer internal model for our data, possibly using ontologies and leveraging the use of reasoners to aid in the task. finally, we would like to capture and add to our analyses data resource size, connectivity, and structured funder information. these improvements could provide a more complete and holistic picture of the reusability and impact of publicly funded research data."
"the bluetooth beacon technology has gained significant attention in providing proximity-aware services for consumers, businesses, and industrial environments. there are mainly three \"pseudo-standards\" for bluetooth beacons: ibeacon [cit], eddystone [cit], and altbeacon [cit], proposed by apple, google, and radius networks, respectively. all three pseudo-standards are based on broadcasting ble advertising packets, which carry specific information in the payload area. when a bluetooth scanner receives an advertising packet, it decodes the content and takes corresponding actions. the message format of ibeacon consist of 4 fields: uuid, major, minor, and tx power. by comparing the received signal strength of the ibeacon message and the value of tx power in the message, the receiver can determine the approximate proximity to the ibeacon transmitter. for eddystone, it has four message formats served for different purposes: eddystone-uid, eddystone-eid, eddystone-tlm, and eddystone-url. take eddystone-url as an example, this type of message is used to broadcast an url that redirects the receiver to a website, which realizes the so-called physical web [cit] . altbeacon was designed for an open and interoperable specification. the message format of altbeacon consists of manufacturer id, beacon code, beacon id, and reference rssi. compared with ibeacon, an altbeacon message has more user data bytes, which delivers more data per message."
"for remote sensing data sets, the image data files can be divided into image pixel data (i.e. pixel data of remote sensing image) and image metadata. metadata is used to describe the attributes of remote sensing data [cit], including image information, geographic information and satellite sensor information. to process the data, the two structures can be stored separately, or integrated into one structure."
"the integrated altitude control system is shown in fig. 2, where z r is the altitude command, z is the uav altitude response, c(s) is the controller, and g(s) is the altitude transfer function defined in (15) . in this paper, we propose a new altitude controller that consists of a pd control term and an acceleration feedback term. the controller can be expressed as follows:"
"for the k a values considered in fig. 4, we can plot the dk d /dw and dk p /dw curves in fig. 5 . because dk d /dw is independent of k a, all the dk d /dw curves collapse to one line (red line)."
"each workstation is dedicated to a sequence of tasks and to a range of product variants. the range of products, the tasks performed on the workstations and the allocated resources are fixed before the launch of the line. if the system manages changes, it handles of flexibility, as presented in fig. 2 . the uml class diagram is centred on the workstation, building the node of the model."
"the actuation of the control channels of the microfluidic device is achieved via custom control software where each of the control channels can be individually actuated. the execution of prolonged ivtt reactions cannot be achieved via this manual process and requires the use of automated protocols incorporated within the control software. when preparing a microfluidic device for experiments, similar automated protocols can be utilized to execute a number of useful processes: the flushing of the device dead volume with a new reagent, the mixing of the reagents within the ring reactor, and the loading of a new reagent into the reactor whilst displacing an equal volume of the current solution. in addition, two complex process are available: the conduction of a device calibration, and the execution of a prolonged cell-free protein expression. all of the aforementioned processes can be easily executed from the main interface, alongside the ability to configure multiple parameters to vary specific process settings such as the inflow channel, inflow volume, and mixing duration."
"for the following work, the adopted definition is: a reconfigurable system is composed of maximum standardized sub-assemblies, enabling rapid volume or product change through production structure modification."
"to demonstrate the effectiveness of the multilayer microfluidic platform for the conduction of ivtt experiments, the described setup was used to express the degfp protein. the experiment was conducted in a commercially available 30 ivtt reaction mixture -comprising all the necessary transcription and translation componentry -supplemented with reaction substrates and dna templates. experiments were conducted at a temperature of 29 °c; a temperature found to be optimal for the ivtt expression of proteins."
"many articles use uml class diagram for production system representation, in order to represent the link between product, operation resources and information system [cit] ."
"where y is the n â k gene expression data matrix, a is the n â m control strength or connectivity matrix and s is the m â k matrix denoting the tfas. the uncertainties in the observation data are assumed to be gaussian [cit], and are represented by the entries of the noise matrix !. genes and tfs are known to interact in a dynamic and non-linear manner; however, a log-linear relationship provides a good approximation. because a particular tf regulates only a few other genes, the connectivity matrix a is expected to be sparse. the problem then boils down to estimating s and *to whom correspondence should be addressed."
"6. post-exposure bake using two hot plates (70 °c and 105 °c) in the following manner: allow the wafer to sit at 70 °c for 20 s before transferring the wafer to the 105 °c hotplate and leaving it here for 40 s. finally return the wafer to the 70 °c hotplate for a further 20 s to complete the post-exposure bake. 7. allow the wafer to cool to room temperature on a stack of microfiber tissues. develop the wafer by transferring it to a petri dish filled with 726 mif photoresist developer to start the development process. development is accelerated when performed on a benchtop shaker and the entire wafer is submerged in the developer. 8. rinse the wafer with demineralized water and use a stereo microscope to check the wafer surface for any photoresist residue. if photoresist residue can be seen, then return the wafer to the developer. 9. reflow the positive photoresist by placing the wafer on a hot plate set at 110 °c for 25 min. this process will result in rounded features as well as annealing any cracks that may have appeared during the fabrication process. proceed to silanize the wafer as described in step 1.17. 10. remove the second silicon wafer from the oven, allowing it to cool to room temperature before proceeding with the spin coating. apply 5 ml of su8 3050 photoresist to the center of the wafer. 11. to obtain a feature height of 30 μm apply the following spin protocol: spin for 20 s at 500 rpm (110 rpm/s), increase the spin speed to 4,000 rpm (330 rpm/s) and hold here for 42 s, and decelerate the wafer to 0 rpm with a deceleration of 200 rpm/s. using a microfiber tissue carefully remove any edge beading which may have occurred during the spin coating. 12. soft bake using two separate hot plates (65 °c and 95 °c) in the following manner: allow the wafer to sit at 65 °c for 30 s. then transfer the wafer to the 95 °c hotplate and allow it to rest here for 14 min before returning the wafer to the 65 °c hotplate for another 30 s. remove the wafer from the hotplate and allow it to cool to room temperature. 13. measure the intensity of the uv lamp before exposure and use this to determine the exposure duration required to achieve a total exposure dosage of 260 mj/cm 2 . place the photomask (emulsion side down) onto the photoresist film and place the wafer underneath the uv light source. expose the wafer using a uv lamp until a total exposure of 260 mj/cm 2 is achieved."
"1. run the analysis script 'calibrationscript.m' and once prompted select the desired '.nd2' file. a single reactor image will be shown with which the correct image intensity can be determined. use the slider to optimize the image intensity such that the edges of the microfluidic channel are clearly visible. 2. an image of the reactor will be shown. within this image, select an area inside the reactor channel of which the fluorescence intensity should be determined. note: the fluorescence intensity of each reactor, for each recorded image will be determined with the results displayed in a simple plot, allowing the visualisation of the results."
"cells are able to sense and respond to their environment using complex dynamic regulatory networks 1, 2 . the field of synthetic biology utilizes our knowledge of the naturally occurring components comprising these networks to engineer biological systems that can expand the functionality of cells 3, 4 . conversely, it is also possible to further our understanding of the natural networks governing life by designing simplified, synthetic analogues of existing circuits or by forward-engineering biological systems which exhibit naturally occurring behaviors. the de novo engineering of such biological systems is performed in a bottom-up fashion where novel genetic circuits or signalling pathways are engineered in a rational manner, using well-defined parts 5, 6 . combining the rational design of networks with the design of biologically relevant systems enables the indepth characterization and study of biological regulatory systems with various levels of abstraction 7 ."
"among the presented layouts, the best ones for the use case will be selected. a qualitative analysis will be carried out, based on the experience of decision-makers. on the experimental side, layouts will be implemented and scenarios will be modeled and simulated using discrete event simulation, enabling assembly system assessment regarding performance indicators for reconfigurability. in addition, operation research algorithms will serve the choice of the best appropriate production schedule and resource allocation."
"in order to assess several solutions for ras design, a modelization able to support the manufacturing systems design elements: layout, machine, material handling and services, is needed [cit] . this paper proposes a formalism and a modelization seeking analysis of assembly layouts regarding reconfigurability. for this study, performance indicator for reconfigurability is restricted to the time needed to change from one assembly system configuration to another with a different product mix ratio or volume."
"the industrial use case is engine assembly in automotive industry. this work is realized after crankcase and cylinder head milling. both parts are first assembled with their components on separated lines, which merge to join the engine crankcase and cylinder head together. then, on a final manual assembly line, the last components are added to the engine. while main components, such as crankcase, cylinder head, crankshaft, camshaft, are manufactured in the plant, the majority of the small components are manufactured by suppliers."
"the most complex pre-programmed process executes a long-duration ivtt experiment, allowing users to initiate the experiment and subsequently allow it to operate unattended until completion. throughout the experiment, reactors 1 and 5 were used as blanks, with only water being added to the reactors during dilutions. reactors 2 and 6 were utilized as negative controls and contained only ivtt reaction solution and ultrapure water. the remaining reactors (3, 4, 7, and 8) contained the ivtt reaction solutions and 2.5 nm of linear dna coding for the degfp gene. initialisation of the reactors is achieved by fully filling all the reactors (excluding 1 and 5) with the ivtt reaction solution, before 25% of the reactor volume was displaced with ultrapure water. hereafter, the periodic injection of reagents into the reactors was initiated. the experiment was conducted such that new reagents were injected into the reactors every 14.7 minutes, with 30% of the reactor volume being displaced during each dilution cycle. the composition of each injection was such that 75% of the injected fluid comprised fresh ivtt solution, whilst the remaining 25% consisted of either dna or ultrapure water. following each injection of new reagents the reactors were continuously mixed, after which a fluorescence image of each reactor was recorded using the microscope. the reaction was subsequently allowed to run continuously for 68 cycles, resulting in an experimental duration of 16.5 h. the results of this experiment are given in figure 9 ."
"one of the key biological processes is transcriptional regulation, which controls the gene expression and amount of rna produced. this process is regulated by transcription factors (tfs), which are specialized proteins causing the genes to express by binding onto the gene promoters. a thorough understanding of this complex transcriptional regulation and tf-gene interaction will potentially aid in predicting the biological processes and designing control strategies to cure and/or avoid the diseased conditions (la¨hdesma¨ [cit] ) . microarray technologies are able to measure the level of gene expressions and quantify them in the form of gene expression data. such data are widely used in the inference of gene-gene interactions. transcription factor activity (tfa), which is defined as the concentration of its subpopulation with dna binding ability, controls the transcriptional regulation [cit] . the correlation between tfas and tf expression level is modified at the posttranscriptional and post-translational stage. it is, therefore, much harder to measure tfa profiles experimentally, and scientists have resorted to computational methods for their estimation [cit] ."
"a, where y is available and some a-priori information about the matrix a is known. network component analysis (nca), [cit], provides a more accurate model for tf-gene regulation and makes use of the related prior information available. it was shown that provided certain conditions are met, the nca algorithm produces a unique solution of the aforementioned estimation problem in the absence of noise. the nca criteria require that: (i) the matrix a is full column-rank; (ii) if a row is removed from s as well as the output elements connected to it, the updated control strength matrix should still be of full columnrank; (iii) the tfa matrix s should have a full row-rank. these criteria guarantee that the solution obtained is unique up to a scale ambiguity [cit] . when the nca criteria are satisfied, the optimization problem reduces to:"
"the industrial use case is engine assembly in automotive industry. this work is realized after crankcase and cylinder head milling. both parts are first assembled with their components on separated lines, which merge to join the engine crankcase and cylinder head together. then, on a final manual assembly line, the last components are added to the engine. while main components, such as crankcase, cylinder head, crankshaft, camshaft, are manufactured in the plant, the majority of the small components are manufactured by suppliers."
"(1) a novel algorithm, robust network component analysis (robnca), is proposed which has the inherent ability to counteract the presence of outliers in the data y by explicitly modelling the outliers as an additional sparse matrix. the iterative algorithm estimates each of the parameters efficiently at each iteration, and delivers superior consistency and greater accuracy for tfa estimation."
"if a handling robot is allowed to be mobile, by coupling it to a platform movable by an operator, it is possible to relocate the resource up to several times a day, as represented by fig. 5 . this reconfiguration may be planned each week and can be adjusted the day before. adding a movable robot on the assembly line enables resource adjustment, which is not possible with the current layout, where robots are fixed on workstations and moving them would cost in average between one and three weeks of time. this time interval is explained by the laborious transport of the hardware, calibration, security checks and ramp up."
where a n 2 r ðmàlnþâ1 is a vector consisting of the non-zero entries in e a n . construct an l n â m matrix u n such that
"with production rates in the range of 200.000 to 640.000 products per year on a line, and a target variety of 16 variants for two product families, the use case is classified as high volume with medium variability according to the classification found in the literature (fig. 1) [cit] . considering the three factory levels: strategic, tactical, and operational, the considered reconfiguration rate is positioned on tactical and operational levels, the strategic level being related to agility [cit] . indeed, with a time horizon of months, weeks, days or even minutes, reconfigurable changes aim to resize or reschedule the system."
"currently, production lines in the use case factory are composed of a succession of workstations, building a flow shop layout. production volume is high, transported between workstations by means of a conveyor."
"(2) a particularly attractive feature of the robnca algorithm is the derivation of a closed form solution for the estimation of the connectivity matrix a, a major source of high computational complexity in contemporary algorithms. to further lower the computational burden, a still faster closed form solution is derived that requires matrix inversion of much smaller size. the resulting algorithm is comparable with fastnca in terms of computational complexity, and is hundreds of times faster than ni-nca."
"this layout is a solution, which answers partially to the scalability paradigm. indeed, by adding more resources on a workstation, takt time is reduced. if production demand is reducing on the line, the movable robot can be placed on another line. there are mainly two solutions for the control and planning of the system. movements of the agv can either be planned for a production period, as a week or a day. other strategy is to recalculate regularly the best allocation for the robot. this layout necessitates high-level data connection, and an energyautonomous robot able to reload on each workstation to avoid time losses on dedicated battery loading stations."
"1. ensure that the water cooler and peltier element have been turned on, with the surface temperature of the peltier set to 4 °c. mount the cooling setup as close to the microfluidic device as possible, minimising the uncooled volume between the peltier and the device inlet."
"crucially, the device outlined within this manuscript allows reactions to be sustained for prolonged durations resulting in steady-state transcription and translation rates. by periodically injecting new reagents into the reactors -and removing reaction (by)products -the reactions are sustained and complex dynamic behaviors can be monitored. in this way, a platform has been created that -to some extent -mimics the cellular environment. furthermore, this platform enables the exploration of the system dynamics, by adapting the period between injections and the specific composition of the injections. as a result, these multilayer microfluidic devices are a powerful tool for the characterisation and optimisation of novel synthetic networks which display complex dynamic behavior."
"the simulations for standard deviation for tfas are presented in the supplementary material for robnca, ni-nca and fastnca. it is noted from supplementary figures s5-s7 in the supplementary section that robnca yields the lowest variation whereas fastnca shows much higher variation in the tfa estimates than both the other algorithms. it can therefore be concluded that robnca outperforms ni-nca both in terms of estimating the tfas as well as in terms of consistency for yeast cell cycle data."
"the industrial use case is engine assembly in automotive industry. this work is realized after crankcase and cylinder head milling. both parts are first assembled with their components on separated lines, which merge to join the engine crankcase and cylinder head together. then, on a final manual assembly line, the last components are added to the engine. while main components, such as crankcase, cylinder head, crankshaft, camshaft, are manufactured in the plant, the majority of the small components are manufactured by suppliers."
"workstations are characterized by resources, related operating tasks and localization (fig. 4) . within resources, workers, tools, robots and special machines are distinguished and are inheritances of class \"resource\". a robot uses an assembly tool. besides, the operator working on the station fulfils a task only using his hands, or a tool like for example a screwdriver, or a special machine, which can be manually or automatically actuated. a single workstation can include between 0, 1 or more of each resource type. the main point is that, in the current system, resources are fixed."
"where the matrix o denotes the outliers. the outlier matrix o is a column sparse matrix, as there are typically a few outliers. the joint optimization problem for the estimation of the three parameters, which also allows for controlling outlier sparsity, can be formulated as"
"this section investigates the observed performance of robnca, in comparison with the state-of-the-art algorithms including fastnca, ni-nca and als in terms of mse using both synthetic and real data. the efficiency and consistency of robnca in estimating the tfas under various scenarios is also illustrated. the datasets for all of the experiments as well as the matlab implementation of fastnca and ni-nca are downloaded from http://www.seas.ucla.edu/liaoj/download.htm and http:// www.ece.ucdavis.edu/jacklin/nca, respectively."
"an important feature of all gene network reconstruction algorithms is the computational complexity incurred in their implementation. the computational complexity of estimating a in (29) at a particular iteration is approximately where ðm à l n þ is the number of non-zero unknowns in the n th column, which is usually small. we now compare the computational complexity of the four algorithms using the subnetwork data from yeast and e.coli. average runtime calculated in seconds is summarized for four subnetworks of each data in table 1 . these experiments were performed on a windows 7 system with a 1.90 ghz intel core i7 processor on a matlab 7.10.0. it is noted that the run time of robnca is comparable with that of fastnca and is hundreds of times faster than ni-nca algorithms for both of its implementations, i.e. involving linear programming and quadratic programming. moreover, the run time for robnca is far superior to that of the als, a direct consequence of the closed form solution derived for estimating the connectivity matrix. it can also be observed that the faster closed form solution for estimating a (29) provides additional savings over its predecessor (11). therefore, it can be inferred from these experiments on synthetic and real datasets that robnca renders superior performance than the contemporary algorithms not only on the yardsticks of accuracy and reliability, but also in terms of computational complexity. the high computational complexity of ni-nca far outweighs the benefits it offers in terms of consistency. fastnca has the smallest run time out of all the algorithms but has poor reliability and is the least robust to the presence of outliers in the data."
"the last step in the iterative algorithm pertains to the estimation of the outlier matrix o by using the values sðjþ and aðjþ obtained in the preceding steps. it is straightforward to notice that the optimization problem (3) w.r.t o decouples across the columns and results in k subproblems, each of which being expressed as follows:"
"this modelization is a base for a comparison of the several layouts and may help to develop further ones and justify reconfigurability concepts. during the project, the proposed layouts and configurations will have to be justified, using operational research."
"1. pneumatic control system (see figure 2 ) 1. using the manufacturers' protocol, establish a tcp connection between the fieldbus controller and the user workstation. use control software (provided as supplementary files) to compose modbus commands which are sent via the tcp connection to controller, and actuate the solenoid valves. 2. expand the fieldbus controller with eight 4-channel digital output modules, one for each solenoid valve being used. each solenoid valve presides over a connecting pin. connect the positive wire to one of the four positive outputs on one of the digital output modules whilst connecting the negative wire to one of the ground ports of the digital output modules. note: to get the system to work correctly, the solenoids should be connected systematically, with the first solenoid being connected to the first output port, the second solenoid to the second output port and so on. in our system, two valve arrays are used with 8 and 22 solenoid valves respectively. output ports 1-8 connect to the 8-valve array, and output ports 9-30 connect to the 22-valve array. 3. connect both valve arrays to a compressed air source using 1/4\" tubing. use pressure regulators to set the pressure of the 22-valve array to 3 bar, and the 8-valve array to 1 bar. figure 3 ) note: to flow fluids through the flow layer of the microfluidic device, a commercially available 4-port pressure regulator is used. the output pressure of each port is regulated via software provided with the pressure controller. connect the pressure regulator to a computer using the supplied usb-connector. 1. connect the pressure regulator to a compressed air source, ensuring that the supplied pressure does not exceed the maximum pressure permitted by the regulator. 2. connect a male luer to 3/32\" barb connector to each of the four female luer lock output ports of the pressure regulator. connect a length of soft tubing (od: 3 mm, id: 1 mm, l: 10 cm) to the barb. 3. connect a second male luer to 3/32\" barb connector to the open end of the soft tubing and attach this to the fluid reservoir connector ports. 4. use the provided software to set the desired pressure of each outlet of the flow regulator, pressurising the reagents stored within the reservoirs to result in the flow of the reagents into the microfluidic device. the connection of the reservoirs to the microfluidic device will be discussed in the section 4.2."
"unlike the conveyor, which path is fixed, the agv carrying a product can adapt its path to the product type and assembly operations needed. work content which is common to several product families can be achieved on a conveyor section, and specific tasks on single isolated workstations, between which the agv flow is free."
"the system representation of a rms/ras should include manufacturing facilities, all kinds of resources and production data. links between instances need to be underlined. fig. 3 . [cit] bergamo, italy, june 11-13, 2018 fig. 4. uml class diagram of the current production system uml class diagram has been chosen for the representation of the static production system with its hardware components, from production cell to production plant scale. this view also enables to outline information content, in form of tasks and production plan. uml representation allows highlighting associations and dependencies with generalisations, compositions, and to precise the multiplicity between elements, in order to have a modelization as close as possible to the real system."
"the industrial use case is engine assembly in automotive industry. this work is realized after crankcase and cylinder head milling. both parts are first assembled with their components on separated lines, which merge to join the engine crankcase and cylinder head together. then, on a final manual assembly line, the last components are added to the engine. while main components, such as crankcase, cylinder head, crankshaft, camshaft, are manufactured in the plant, the majority of the small components are manufactured by suppliers."
"the industrial use case is engine assembly in automotive industry. this work is realized after crankcase and cylinder head milling. both parts are first assembled with their components on separated lines, which merge to join the engine crankcase and cylinder head together. then, on a final manual assembly line, the last components are added to the engine. while main components, such as crankcase, cylinder head, crankshaft, camshaft, are manufactured in the plant, the majority of the small components are manufactured by suppliers."
"1. pdms preparation 1. prepare two pdms precursor solutions by combining the base and curing agents in a plastic beaker and using a mixing rod to stir the two components until fully mixed. the control layer requires 20 g of base agent and 1 g of curing agent (20:1 ratio). the flow layer requires 40 g of the base agent and 8 g of the curing agent (5:1 ratio). degas the solutions in a desiccator. 2. place the flow layer wafer in a petri dish and pour the 5:1 ratio pdms mixture over the wafer. degas the pdms for 30 min to remove air bubbles. 3. spin coat the control layer wafer (prepared using the negative su8 3050photoresist) with 20:1 ratio pdms. pour 5-10 ml of the pdms onto the center of the wafer and run the following spin protocol (retain the left over pdms for later use): spin at 500 rpm for 15 s (100rpm/s), increase the spin speed to 1450 rpm (300 rpm/s) for 45 s, and then decelerate the wafer to 0 rpm (200 rpm/s). 4. to ensure a homogeneous pdms film thickness, place the pdms coated wafer on a level surface in a closed petri dish (to avoid dust contamination). let the wafer sit for 30 min. 5. remove the flow layer from the desiccator and place both the flow and control layers in an oven (80 °c). cure both layers for [cit] min and remove when the pdms is malleable enough to manipulate, whilst remaining slightly sticky. immediately proceed with the alignment process."
"the assembly sequence faces a strong diversity, due to the different cylinder volumes, european norms which have impact on components, engine types (diesel, gasoline, with or without turbocharger…). this diversity is partially managed through logistics. indeed, in order to reduce the size of the line-side delivery areas and to relieve worker's mental load, kitting areas have been installed in the production plant. on dedicated logistic areas, kits of manufactured items are composed, each corresponding to a future engine. kits are then taken to the assembly lines and put on the conveyor besides the engine they belong to. during assembly tasks,"
"several statistical techniques including principal component analysis (pca) [cit] and independent component analysis (ica) [cit] have been used to deduce useful information from sets of biological data. however, the successful application of these algorithms hinges on the assumptions of orthogonality and independence between the signals, which do not hold for biological signals in practice [cit] . in fact, some prior information is usually available for many systems, and it should be incorporated in the system model, e.g. chip-chip data indicates which tfs and genes are known to interact. the gene regulatory network can be modelled linearly as follows [cit]"
"upon substituting (19) in (18), the solution e a n in theorem 1 readily follows. therefore, using theorem 1, an estimate of e aðjþ can be efficiently obtained and this approach results in substantial reduction in computation complexity compared with the als algorithm. remark 2. while the aforementioned closed form solution provides a significant advantage in terms of computational complexity over the als algorithm, we note that the solution requires inverting the matrix q. for large networks, this can potentially be a large matrix, whose inverse incurs computational load, and may lead to inaccuracies as well. in the following discussion, we derive a still faster algorithm, robnca 2, that takes advantage of the special structure of the column vector e a n and provides added savings over the closed form solution derived in theorem 1."
"most of the contemporary algorithms have studied the gene network construction problem using nca with gaussian noise models. however, inaccuracies in measurement procedures and abnormal gene responses often render heavier tails to the gene expression data, and gaussian noise models may no longer be a natural fit in these cases. the decomposition techniques used in the available algorithms are highly sensitive to the presence of outliers i.e. the samples that do not conform to the gaussian noise model, and their estimation capabilities are extremely susceptible to outliers. as a consequence, the gene network inference becomes unreliable for practical purposes. therefore, we focus on deriving computationally efficient nca algorithms that are robust to the presence of outliers."
"based on this modelization, table 1 compares the six proposed layouts. [cit] bergamo, italy, june 11-13, 2018 fig. 9. uml class diagram of the modular blocks layout thanks to the proposed modelization, it was possible to identify limits and opportunities regarding flexibility, reconfigurability and scalability of a production system, considering the use case of diesel and gasoline engine assembly. this modelization aims to be a tool for the identification of the modular mesh in order to design a reconfigurable system. several proposals for a reconfigurable system considering product and volume variety have been modeled, based on the current layout of the case study factory."
"where the matrix v t n qðjþv n is invertible, as v n has full column rank. the symmetric invertible matrix qðjþ can be partitioned as"
"in order to assess several solutions for ras design, a modelization able to support the manufacturing systems design elements: layout, machine, material handling and services, is needed [cit] ). this paper proposes a formalism and a modelization seeking analysis of assembly layouts regarding reconfigurability. for this study, performance indicator for reconfigurability is restricted to the time needed to change from one assembly system configuration to another with a different product mix ratio or volume."
"2. alignment and bonding 1. using a scalpel, remove each of the four devices from the pdms on the flow layer wafer. upon removing the pdms layer from the silicon wafer, immediately cover the feature side with scotch tape to avoid dust particle contamination. 2. roughly align the flow layer blocks on the control layer by eye, placing the feature side of the device into contact with the control layer pdms. subsequently, make fine adjustments to the position of each of the flow layer blocks to align the channels of the flow layer with the control layer channels, using a stereo microscope to aid in the visualization of the adjustments. 3. apply pressure to remove air pockets between the two pdms layers. pour the remaining 20:1 ratio pdms saved earlier around the aligned flow layer blocks. place 100 g weights on each of the devices to ensure sufficient contact during the bonding process. 4. return the aligned devices (including the weights) to the 80 °c oven and leave them to bond for at least 1.5 h and no longer than 6 h. remove the wafer from the oven and extract each of the individual devices from the control layer wafer, covering the feature side of each device with scotch tape. 2. iteratively, punch a single hole for each of the 9 flow layer inlets, 24 control layer channel inlets, and the single flow layer outlet of each device. punch the device with the feature side facing up, using a camera to ensure holes are punched within the feature boundaries. 3. for each device, clean a single microscope slide with isopropanol and acetone and dry the slides under a stream of n 2 . subsequently, place the microscope slides on a hot plate set to 150 °c for 15 min. 4. use oxygen plasma ashing to bond the pdms devices to the glass slides, applying an ashing power of 50 w for 45 s. ensure that the feature side of the device is facing upwards when ashing. once complete, place the device feature side down on the glass slide, applying pressure to remove trapped gas between the surfaces. 5. place the bonded devices onto a hot plate set to 110 °c for 1 h. weights can be placed on top of the devices to improve adhesion of the device."
"one step further is the job shop ( fig. 8), having products transferring only on agvs. each workstation disposes of fixed resources. the production field has a matrix structure, where each node is a workstation. those are independent and can be readjust during production. supply is completely done through kitting in order to avoid logistic containers on the production line."
"2. preparing the microscope 1. using the microscope, locate any points of interest (a single point within each reactor is sufficient) within the microfluidic device, and store the coordinates thereof. these points will be imaged during the calibration and experimental processes. note: during the calibration and experimental procedures included within the control software, the microscope is instructed to periodically capture images at the previously stored coordinates. to achieve this, the control software communicates with the microscope software, informing it to record new images. this communication is unique to each microscope setup, and as such this functionality has been modified within the provided software interface. provided is a dummy executable, which can be modified by the end-user for compatibility with their own microscope system."
"it is important to point out that robnca is significantly different from ni-nca algorithm. ni-nca, as the name suggests, is a non-iterative algorithm that uses a subspace-based method for the estimation of the connectivity matrix a using eigen-decomposition and relies on solving a constrained quadratic optimization problem, which has high computational cost. on the other hand, in robnca, we propose two closed form solutions for the estimation of the connectivity matrix a, which result in considerable reduction in computational complexity."
"proof. since the optimization problem (15) contains linear equality constraints, the kkt conditions are necessary for optimality [cit] . let any e a ã n be a local minimum. then, since the kkt conditions are necessary, there exists a lagrange multiplier ã such that e a ã n, ã à á is the solution to the system of equations in (16) and (17). now since the objective function is convex, it follows that e a ã n is also a global minimum [cit] . this implies that the kkt conditions are also sufficient for optimality."
"note: scripts have been provided for the analysis of the images (see supplementary files or the table of materials), making use of the 'bfopen' analysis package, which is required for the reviewing of '.nd2' files (provided by our microscope setup)."
"the assembly sequence faces a strong diversity, due to the different cylinder volumes, european norms which have impact on components, engine types (diesel, gasoline, with or without turbocharger…). this diversity is partially managed through logistics. indeed, in order to reduce the size of the line-side delivery areas and to relieve worker's mental load, kitting areas have been installed in the production plant. on dedicated logistic areas, kits of manufactured items are composed, each corresponding to a future engine. kits are then taken to the assembly lines and put on the conveyor besides the engine they belong to. during assembly tasks,"
"post-exposure bake using two hot plates (65 °c and 95 °c) in the following manner: allow the wafer to sit at 65 °c for 60 s before transferring the wafer to the 95 °c hotplate and leaving it here for 4.5 min. return the wafer to the 65 °c hotplate for a further 30 s to complete the postexposure bake. 15. allow the wafer to cool to room temperature on a stack of microfiber tissues. develop the wafer by transferring it to a petri dish filled with mrdev-600 photodeveloper to start the development process. development is accelerated when performed on a benchtop shaker and the entire wafer is submerged in the developer. 16 . rinse the wafer with isopropanol and use a stereo microscope to check the wafer surface for any photoresist residue. if photoresist residue can be seen, then return the wafer to the developer. once fully developed, hard bake the photoresist by placing the wafer on a hot plate set at 150 °c for 1 h. 17. silanize both wafers to prevent the adhesion of pdms during the soft-lithography processes. to perform the silanization, pipet 2-3 droplets (per wafer) of the silane into a small glass vial. place this vial, along with the wafers into a desiccator and pull vacuum for 5-10 min. seal the desiccator and leave the wafers under vacuum for a period of 12 -16 h. caution: the silane is toxic and should not be inhaled. take care to work in a fume hood and to wear nitrile gloves when handling the silane. this includes placing the vacuum pump in the fume hood when pulling vacuum on the desiccator. 18. release the vacuum from the desiccator and remove the silanized wafers. rinse with water and use a steam of n 2 to dry the wafers. at this point the wafers can be placed in storage until required."
"we now look to investigate the consistency of the algorithms. the disagreement between the tfa estimates of the four subnetworks is calculated and the results are shown in figure 4a . out of the three algorithms considered, robnca incurs the smallest disagreement. the performance of ni-nca is somewhat comparable; however, fastnca shows a high degree of inconsistency."
"in place of the conveyor, agvs are used to transport products, instead of being only used to move robotic resources. in this layout proposition, the line can be organized as a flow shop, including sections with conveyors and sections with agvs for pallet transport."
"only extracts of uml diagrams are represented fig. 5 to 8. the rest of the diagrams, not represented, are similar to the current system displayed fig. 4 . it is also possible to combine the presented solutions."
"in order to assess several solutions for ras design, a modelization able to support the manufacturing systems design elements: layout, machine, material handling and services, is needed [cit] . this paper proposes a formalism and a modelization seeking analysis of assembly layouts regarding reconfigurability. for this study, performance indicator for reconfigurability is restricted to the time needed to change from one assembly system configuration to another with a different product mix ratio or volume."
"the estimate sðjþ, so obtained, is used in the upcoming steps to determine a and o. (8) [cit] . however, a closed form solution was not provided and the proposed algorithm relied on costly optimization techniques to update the matrix a. because this minimization needs to be performed at each iteration until convergence, the als algorithm is known to be extremely slow for large networks, and computational resources required may be prohibitive [cit] . hence, it is imperative that a closed form solution is obtained for the optimization problem in (8), so that the algorithm is faster and efficient."
"the mode of operation of the system is the following one: when the pallet with the product and its kit enters a workstation, the cell reads the product's id. this gives information about the related production plan and the tasks which have to be performed on the cell, with the necessary resources. this procedure is presented fig. 3 ."
"a viable alternative to the use of cellular hosts for the characterization of synthetic networks is the application of in vitro transcription and translation (ivtt) technologies. acting as a testbed for synthetic networks, reactions are performed in solutions comprising all the components required to enable gene expression 19, 20, 21 . in this manner, a biologically relevant, albeit artificial, environment is created within which synthetic networks can be tested . the design cycle can be further accelerated by utilizing cell-free cloning techniques such as the gibson assembly to rapidly engineer novel networks, and by constructing networks from linear dna templates which -unlike the plasmids required for in vivo testing -can be amplified via polymerase chain reactions (pcr) 33, 34 . batch reactions are the simplest method by which ivtt reactions can be performed, requiring a single reaction vessel wherein all of the reaction components are combined 35 . such reactions are sufficient for protein expression and basic circuit testing yet prove insufficient when attempting to study the long-term dynamic behavior of a network. over the course of a batch reaction, reagents are either depleted or undergo degradation resulting in a continuous decrease of the transcription and translation rates. furthermore, as reactions progress by-products accumulate that can interfere with -or completely inhibit -the correct functioning of the network. ultimately, the use of batch reactors limits the dynamic behavior which can be observed, with negative regulation being particularly challenging to implement 5, 36 ."
"we now turn our attention to the comparison of these algorithms on real data. two datasets are considered for this purpose, which are the saccharomyces cerevisiae cell cycle data [cit] and e.coli data [cit] . the transcription factor activities are estimated for the tfs of interest in each experiment, and the results are compared for different algorithms. in addition, the variability of the estimates is evaluated using the subnetwork analysis [cit] which will be explained in the following subsections."
"flexibility and reconfigurability are represented fig. 2, as a need in changing capacity function of the economic environment [cit] . flexibility is relevant in a well-forecasted economic environment, while reconfigurability is used in a turbulent market."
the estimation of o matrix is shown in the supplementary material where figure 1 depicts the outliers present in the synthetic data and their estimates using robnca algorithm. it is noted that robnca is able to identify the outliers very well. figure 2 shows the recovered signal as after subtracting the outlier matrix o from the data matrix x. it can be observed that the recovered signal is a good match with the original signal.
"the assembly sequence faces a strong diversity, due to the different cylinder volumes, european norms which have impact on components, engine types (diesel, gasoline, with or without turbocharger…). this diversity is partially managed through logistics. indeed, in order to reduce the size of the line-side delivery areas and to relieve worker's mental load, kitting areas have been installed in the production plant. on dedicated logistic areas, kits of manufactured items are composed, each corresponding to a future engine. kits are then taken to the assembly lines and put on the conveyor besides the engine they belong to. during assembly tasks,"
"this solution has to be assessed regarding costs indicators because of the high price of autonomous mobile robots. with a throughput of several hundreds of thousands of products per year, it is not feasible to have all products on agvs on fulltime."
"because the connectivity matrix aðj à 1þ has full column rank (by virtue of nca criterion 1), the matrix a t ðj à 1þaðj à 1þ is invertible. therefore, an estimate of the tfa matrix s at the j th iteration can be readily obtained as"
"despite the relatively straightforward fabrication process of pdms based devices, the use thereof requires an extensive hardware setup. comprising valve arrays, pressure regulators, pressure pumps, incubators, and cooling units, the transition from fabrication to use is not elementary, and requires a significant initial investment. in addition, the ability to consistently set-up and perform successful experiments with these devices requires a significant time-investment; a point which this manuscript aims to address. however, once in place, the entire setup can be modified for a range of purposes. furthermore, the hardware setup comprises numerous modular elements, each of which can be expanded to allow more complex microfluidic device designs to be employed. additionally, the modular design enables the replacement of hardware components by similarly functioning alternatives, such that users are not limited to the specific setup described here 48, 49 ."
"in this vision, optimization consists in attribution of the tasks to the workstations. depending on the assembly sequence, the layout minimizing displacements is chosen. reconfigurability potential consists in two aspects: reaffectation of cells and adding of workstations, as long as the area enables matrix growth. reaffectation of cells supports changes in product types, or in production volume for a specific product, while another is decreasing. fig. 8 . job shop layout with agv transport"
"these experiments indicate that robnca solves the estimation problem with much more accuracy than ni-nca and fastnca. it is important to emphasize here that the mse for ni-nca is always higher than robnca and its computational complexity is many times greater than the latter, which can prove to be a bottle-neck in case of large datasets."
"due to fluctuations in pressure and imperfections during microfluidic device fabrication, the volume of fluid displaced during a single injection cycle can vary between devices. as such, prior to performing ivtt experiments, the displaced reactor volume per injection cycle (refresh fraction) was determined. this calibration requires the filling of all eight reactors with a fluorescent reference solution. in this case, a purified fitc-dextran solution (25 μm) was used. subsequently, the reactors are diluted 10 times with ultrapure water. by measuring the decrease in fluorescence per dilution cycle for each reactor, the volume of fluid displaced during a single injection cycle was determined. within the control software, this value (the refresh ratio) was recorded for use during the ivtt experiment. crucially, to account for variations in the flow rate across the device, as well as discrepancies in the individual reactor volumes, the refresh ratio is determined and stored for each individual reactor. the sequence of filling and diluting the reactors was conducted automatically using the perform calibration program which forms part of the control software. the results of the calibration experiment are shown in figure 8 ."
"when performing prolonged ivtt experiments, there are two main causes for the failure of a reaction; the introduction of air into the microfluidic device or the degradation of the ivtt reaction solution. the occurrence of air within the microfluidic device is most often the direct result of small air bubbles existing in the inflow solutions, which are subsequently injected into the microfluidic device. upon entering the device, the presence of air inhibits the proper flow of fluids, whereby the reactions are no longer periodically refreshed leading to the formation of batch reactions within the reactor rings. in some cases, the air is slowly removed from the device by the repeated flushing of reagents, after which the reaction continues as intended (as shown in figure 9 ). in other cases the air remains trapped, and can only be removed by aborting the experiment and subsequently applying continuous (high) pressure to the flow layer of the microfluidic device, analogous to the filling process described in section 5.1 of the protocols. during our experiments the cell lysate is stored in ptfe tubing on a peltier element cooled to 4 °c. both measures aid in limiting the degradation of the ivtt reaction solution over time, with the inert ptfe tubing ensuring limited interaction between the tubing and the reaction solution and the cold temperatures preserving the functional (bio)molecular componentry required to perform ivtt. should degradation of the reaction solution occur -as the result of insufficient cooling or undesired interactions between the reaction solution and the storage environment -then this will exhibit itself experimentally as a gradual reduction of protein expression over time. once degraded, the ivtt reaction solution cannot be recovered and a new experiment should be prepared. a prolonged ivtt reaction was initiated such that 30% of the reactor volume is displaced every 14.6 minutes. the reaction was allowed to run for over 16 hours before being terminated. two reactors of the microfluidic device were used as blanks, with only ultrapure water being flown through the reactors throughout the experiment (reactors 1 and 5). all the other reactors comprised 75% ivtt reaction solution and 25% of either ultrapure water (reactors 2 and 6) or 2.5 nm linear dna templates coding for the expression of degfp (reactors 3, 4, 7, and 8). in all four reactors where dna was added, there is clear degfp expression. three of the four reactors provide similar fluorescence intensity, with one reactor displaying lower fluorescence signal. this could be caused by a disparity in flow resulting in less dna entering the reactor, or due to variations in the reactor dimensions. after 14 hours, a sudden increase is seen in the signal of the reactors containing dna. this is caused by an air bubble entering the flow layer of the microfluidic device, presumably originating from one of the inflow solutions. the trapping of air in the microfluidic device significantly limits the flow of fluids through the channels, whereby no fresh reagents can be added to or removed from the reactors until the air has passed. upon resumption of flow, the experiment returns to its previous fluorescence intensity. please click here to view a larger version of this figure."
"our goal is to estimate the three parameters a, s and o by solving the optimization problem (5). however, it can be noticed that the optimization problem is not jointly convex with respect to (w.r.t) a, s, o f g . therefore, we resort to an iterative algorithm that alternately optimizes (3) w.r.t one parameter at a time."
"the assembly sequence faces a strong diversity, due to the different cylinder volumes, european norms which have impact on components, engine types (diesel, gasoline, with or without turbocharger…). this diversity is partially managed through logistics. indeed, in order to reduce the size of the line-side delivery areas and to relieve worker's mental load, kitting areas have been installed in the production plant. on dedicated logistic areas, kits of manufactured items are composed, each corresponding to a future engine. kits are then taken to the assembly lines and put on the conveyor besides the engine they belong to. during assembly tasks, proceedings,16th ifac symposium on information control problems in manufacturing bergamo, italy, june 11-13, 2018"
"copyright © 2018 ifac elements are directly taken from the kit and joined on the engine. this enables to manage a part of product diversity, when the diversity only consists in variety among components. however, for strong variety management in a same production line, as changes in the number of cylinders, kitting is not sufficient. this is why automotive industry faces a need in reconfigurable systems."
"robnca is seen to outperform the state-of-the-art algorithms for estimating both a and s in terms of mean square error (mse). in addition, robnca is applied to yeast cell cycle data [cit] and escherichia coli data [cit], and by plotting the standard deviation of estimates, it is observed that robnca offers better consistency than fastnca and ni-nca."
"in this work, we present robnca, an algorithm for robust network component analysis for estimating the tfas. the robnca algorithm accounts for the presence of outliers by modelling them as an additional sparse matrix. a closed form solution available at each step of the iterative robnca algorithm ensures faster and reliable performance. the performance of the proposed robnca algorithm is compared with ni-nca and fastnca for synthetic as well real datasets by varying snr, degrees of correlation and outlier concentration. it is observed that while fastnca is computationally simpler, yet the tfa recovery is inaccurate and unreliable, a direct consequence of the sensitivity of its decomposition approach to the presence of outliers. the ni-nca algorithm offers performance somewhat comparable with the robnca algorithm; however, the robnca algorithm is much more computationally efficient and does not require solving costly optimization problems. therefore, the cumulative benefits of robustness to the presence of outliers, higher consistency and accuracy compared with the existing state-of-the-art algorithms, and much lower computational complexity make robnca well-suited to the analysis of gene regulatory networks, which invariably requires working with large datasets."
"note: the soft-lithography process used to fabricate pdms based multilayer microfluidic devices can be separated into three distinct steps: 1) the pdms preparation of both the flow and control layers, 2) the alignment and bonding of the two pdms layers, 3) the completion of the device."
"3.1.1 impact of correlation the algorithms are first tested for low and highly correlated source signals by varying the signal-tonoise ratio (snr). the noise is modelled as gaussian in all the experiments. the results are averaged over 100 iterations and are depicted in figure 1 . it is observed that the presence of a small amount of outliers makes the estimation using fastnca unreliable and inconsistent for both low and highly correlated signals. on the other hand, ni-nca is able to estimate s better than fastnca, and the estimation of a is quite accurate and consistent as well. it can be observed that the overall estimation performance for a is much better and more consistent than that of s. the reason for this could be attributed to the availability of some prior information for the former. since robnca takes into account the presence of outliers in the observed data, it outperforms the other two algorithms for estimating both a and s and its consistent performance should be contrasted with the unboundedness and unpredictability exhibited by the other two algorithms. in general, the performance of all the algorithms improves with the increase in snr and degrades with the increase in correlation of the source signals. 3.1.2 impact of outliers as noted earlier, the presence of outliers can severely affect the performance of algorithms. it is therefore, important to investigate the impact of the presence of outlying points in the observation matrix y. comparison performed for low and high concentration of outliers is depicted in figure 2 . it is observed from figure 2a that in the case of low concentration of outliers, ni-nca provides good accuracy for a and estimates it quite consistently. the estimation of s gives a small mse as well and generally performs consistently. fastnca, however, is not able to estimate both the matrices even for high snrs. this indicates its high vulnerability to the presence of even a small number of outliers. in case of a higher concentration of outliers, the performance of ni-nca degrades a little bit as depicted in figure 2b . it is observed that robnca is able to estimate the two matrices for both low and high outliers, and outperforms the other two algorithms."
"3. off-chip cooling setup (see figure 4) 1. use pvc tubing (od: 10 mm, id: 6 mm) to connect the water-cooling system to the cold-plate water block using compression fittings. fill the fluid reservoir of the water-cooling system with a coolant and gently tilt the unit to displace any trapped air, continuously adding coolant to the reservoir to ensure it remains full. when all the gas is removed from the system, fill the reservoir to approximately 90-95% of its maximal volume. ensure that the tubing, peltier element, and cooling block are in direct contact with one another at all times. 4. connect the peltier element to the temperature controller (via a serial bus connector), such that the voltage supplied to the peltier can be regulated. securely place a thermistor on the peltier surface, connecting the output to the temperature controller. after turning on the water cooler, adapt the voltage supplied to the peltier until the temperature is stable at 4 °c. note: with this setup, the peltier temperature is controlled manually by adapting the supplied voltage, while the thermistor serves only to monitor the temperature. placing the end of the tubing into the desired reagent solution, fill the syringe with the reagent. 4. insert the stainless-steel connector pin into the polyurethane tubing connected to the syringe and fill the tubing with the reagent. when using small reaction volumes, the reagent will not enter the reservoir, and the tubing itself will act as the reservoir. disconnect the syringe and insert the connector pin into one of the flow layer inlet holes of the microfluidic device. 5. apply pressure to each of the reservoirs using the pressure regulator software to force the reagents into the microfluidic device."
"so far, in factories comparable to the use case, flexible manufacturing systems are implemented, which means, that the changeability of the market is forecasted. this strategy manages systems with high volume and some variety and the production system, by means of automatic tool change within cnc-machines or robotic systems and flexible programming, is fully able to manufacture the planned product variants. the aim of this paper is being able to consider turbulent variations too (cf. fig. 2 )."
"remark 3. by comparing the closed form solution derived in (11) with (29), it is clear that the latter only requires inverting a submatrix q 11 ðjþ of qðjþ. since the connectivity matrix is usually sparse and the number of nonzero entries ðm à l n þ in the n th column is usually very small, inverting the ðm à l n þ â ðm à l n þ matrix q 11 ðjþ results in a considerable reduction in computational complexity and ensures a much faster implementation of the iterative algorithm."
"the mobile robot is efficient and viable only if it is not permanently moving. indeed, it is better to have a fixed or only a movable robot as described in paragraph 4.1 upon a limit. the threshold value depends mainly on the cost of the agv and of the control system software."
"a pdms-based multilayer microfluidic device has been presented, and its capability to sustain ivtt reactions for prolonged periods of time has been demonstrated. although well-suited for this specific example, this technology can conceivably be used for numerous other applications. the additional control over fluid flow -paired with the ability to continuously replenish reaction reagents whilst removing (by)products -is ideal for continuous synthesis reactions, the investigation of various dynamic behaviors, and the simultaneous conduction of multiple variations of a single reaction."
"variability between individual devices, and in the external conditions (such as pressure fluctuations) can result in inaccuracies when performing experiments using these devices. to address this issue, a calibration of the system should be performed prior to each experiment, providing a unique refresh ratio for each of the reactors. whilst the calibration addresses the device-to-device and experiment-to-experiment variations, it is a time consuming process and not flawless. fluids with differing viscosities will not flow with the same rate when exposed to identical pressure, and as such performing the calibration with multiple reagents may not yield identical refresh ratios. this effect is attenuated by utilising three control channels to peristaltically pump the reagents into the microfluidic device, as opposed to regulating the flow by varying the supplied pressure only. as a last resort in cases where the disparity in viscosity is very large, a unique refresh ratio can be implemented for each individual reagent by performing multiple calibration experiments."
"in order to assess several solutions for ras design, a modelization able to support the manufacturing systems design elements: layout, machine, material handling and services, is needed [cit] ). this paper proposes a formalism and a modelization seeking analysis of assembly layouts regarding reconfigurability. for this study, performance indicator for reconfigurability is restricted to the time needed to change from one assembly system configuration to another with a different product mix ratio or volume."
"considering a lightweight collaborative movable robot, this time-lapse may be reduced up to less than one hour. the flexibility of the end-tool has consequences on the easiness of the reconfiguration and the number of workstations on which it can be used. in the other case, tool change can be considered. with a shorter relocation time of the robot, the reconfigurability rate of the assembly system is improved."
"where the matrix qðjþ is indeed invertible by virtue of the linear independence of the rows of s (nca criterion 3). substituting (18) in (17), we have"
"the proposed algorithm is tested against varying noise for two important scenarios: (i) when the source signals are correlated, and (ii) the observed data are corrupted with outliers. using the same connectivity matrix, source signals were generated which had low, moderate and high correlation [cit] . the outliers are artificially added to the data by modelling them as a bernoulli process. the success probability indicates the concentration of outliers present and is assumed to be the same for all the genes. since only a few points are expected to be corrupted in the real data, the outliers are assumed to be sparse and therefore the success probability of presence of outliers is kept small. the performance of robnca, fastnca and ni-nca is evaluated in the aforementioned scenarios. robnca algorithm is implemented in matlab. since the observed data matrix y is expected to contain outlying points, the algorithms are assessed by computing the mse incurred in estimating the matrices a and s, instead of fitting error for y. the comparison with als is omitted here because it takes much longer to run as will be shown in the next subsection."
"the assembly sequence faces a strong diversity, due to the different cylinder volumes, european norms which have impact on components, engine types (diesel, gasoline, with or without turbocharger…). this diversity is partially managed through logistics. indeed, in order to reduce the size of the line-side delivery areas and to relieve worker's mental load, kitting areas have been installed in the production plant. on dedicated logistic areas, kits of manufactured items are composed, each corresponding to a future engine. kits are then taken to the assembly lines and put on the conveyor besides the engine they belong to. during assembly tasks,"
"recent advances in technology have enabled monitoring of cellular activities using more sophisticated techniques, and have provided a deluge of biological data. using these data to unravel the underlying phenomena that regulate various activities in a living organism offers the potential to reap numerous benefits."
"towards that end, we take the approach of explicitly modelling the outliers as an additional matrix that corrupts the data points. from (1), it follows that the complete system model that accounts for the presence of outliers as well as noise can be expressed as"
"the goal of this manuscript is to present a complete protocol for the fabrication of multilayer microfluidic devices capable of performing longterm ivtt reactions. in addition, this manuscript will describe all of the hardware and software required to perform prolonged ivtt reactions. the actuation of the microfluidic device -necessary to control the flow of fluids therein -is achieved using a series of pneumatic valves which connect directly to the microfluidic devices via lengths of tubing. in turn, the pneumatic valves are controlled via a custom-built virtual control interface. fluid flow within the microfluidic devices is achieved using continuous pressure which is provided by a commercially available pressure regulation system. ivtt reactions are typically performed between 29 °c and 37 °c and a microscope incubator is used to regulate the temperature during reactions. however, the functionality of the ivtt mixture gradually degrades when stored above 4 °c. as such, this manuscript will expand on the off-chip cooling system used to cool the ivtt mixture prior to injection into the microfluidic device. in conclusion, this manuscript provides a comprehensive overview of the procedures required to successfully perform prolonged ivtt reactions using a microfluidic flow reactor such that other researchers will be able to replicate this technology with relative ease."
"the consistency of the algorithms is assessed for this experiment as well and the respective disagreement for each of the four algorithms is shown in figure 4b . fastnca is again seen to incur the maximum disagreement. ni-nca and als perform better than fastnca; however, robnca gives the least disagreement for the four estimates of tfas and performs the most consistently out of all the algorithms."
"one or more tasks from a production sequence, linked to a final product type, are performed on the workstation. the product itself is composed by the main part, machined by the car manufacturer -engine crankcase or cylinder bloc in the presented use case, and of items, manufactured by the company itself or by a supplier. as seen in paragraph 2.1, the logistic related to the manufactured items for assembly is divided into two techniques: kitting and line-side supply. supply containers are related to a workstation and its localisation in the production plant."
"the microfluidic device possesses nine unique inlets, of which four were utilized during this experiment. the first contained the commercially obtained ivtt reaction mixture. the ivtt reaction mixture accommodates all the components required to successfully express proteins however, purified gams was added to the reaction mixture -at a final concentration of 1.3 µm -prior to loading into the microfluidic device. the addition of the gams protein serves to minimize the degradation of linear dna species when performing the experiments. crucially, the ivtt mixture was injected into polytetrafluoroethylene (ptfe) tubing coiled onto a peltier element with a surface temperature of 4 °c to cool the solution prior to the injection thereof into the microfluidic device; preventing the degradation of the reaction solution prior to its use. microbore polyether ether ketone (peek) tubing was used to connect the ptfe tubing leaving the peltier element surface with the microfluidic device, reducing the volume of the ivtt reaction mixture not being cooled. the second solution inserted into the device contained the linear dna template coding for the degfp -dissolved in ultrapure water -at a concentration of 10 nm. the third solution, ultrapure water, served multiple purposes during the experimental procedures. primarily, the ultrapure water was used to ensure that the displaced volume per dilution was equal for all reactors, acting as a replacement for dna in the control reactions. additionally, ultrapure water was also used to dilute the fluorophore during the device calibration and to flush the dead volume of the device when switching between reagents. the final solution inserted into the device was a purified fitc-dextran solution (25 μm) required to perform the initial device calibration. the dna, water, and fluorophore solutions were injected into tubing (0.02\" id, 0.06\" od) which could subsequently be inserted into one of the inflow channels of the microfluidic device as per section 4.2 of the protocols. as such, these solutions were stored at 29 °c for the entirety of the experiments."
"in order to assess several solutions for ras design, a modelization able to support the manufacturing systems design elements: layout, machine, material handling and services, is needed [cit] . this paper proposes a formalism and a modelization seeking analysis of assembly layouts regarding reconfigurability. for this study, performance indicator for reconfigurability is restricted to the time needed to change from one assembly system configuration to another with a different product mix ratio or volume."
"a production system composed of modular blocks is proposed fig. 9 . the rms is divided into modules of small size, easy to move. the production line is a succession of blocks, including resources, and a section of conveyor ( fig. 9 ). several blocks are assembled the one after the other, building a continuous conveyor, transporting workpieces between stations. reconfiguration of the system consists in the rearrangement of the blocks or/and of the tools within the blocks."
"advanced techniques like the drill-and-join algorithm presented in section 2.3 follow a similar approach in an optimized way to find bases of the target function and efficiently synthesize a functional program with the same behavior as the target function. still, the worst-case complexity for these techniques remains exponential in n."
"opaque predicates are operations that have been rewritten in an equivalent but more complex form to hinder reverse engineering. opaque predicates are used in obfuscation techniques to add unfeasible paths, thus increasing the complexity of deobfuscation and recognition. many off-the-shelf compilation chains line epona [cit] and llvm obfuscator [cit] currently implement opaque predicates. this method, used in conjunction with other techniques, such as control flow flattening and virtualization-based obfuscation, make the task harder for reverse-engineers."
"z3 4.1. z3 [cit] handles statements in the smt-lib language. it uses a dpllbased sat solver, a core theory solver handling equalities and uninterpreted functions, and integrates its functionalities with satellite theory solvers for linear arithmetics, bit-vectors, arrays and other theories. the models maintained by each theory solver are combined incrementally. quantifier are handled by an abstract machine for matching."
"we have implemented the algebraic simplification method in c and applied it to the deobfuscation of the comparison between an input and a constant described in section 2.2.2, obfuscated with mba obfuscation using polynomial of different degrees. the time required for the algebraic simplification is presented in figure 4 . in the figure we show the time required for running the algebraic simplification (blue line with xs on the left) and for deciding the satisfiability of the resulting simplified expression (red line with dots on the right)."
"we will show that the drill-and-join synthesis method is very effective, always outperforming the smt solvers presented in section 4 while solving a more complex problem (program synthesis instead of satisfiability). in fact, as the degree of the obfuscating polynomial increases, drill-and-join becomes able to synthesize the target function in less than the time required to obfuscate it in the first place. finally, the method has the advantage of producing compact synthesized functions, which is particularly important if the function is to be used in some subsequent computation, as it is the case with concolic execution."
"when a conditional statement is found by the analyzer, it can be evaluated in a random point of the input space. if it evaluates to true (resp. false) then the then (resp. else) branch is reachable and can be explored by the analyzer, while the reachability of the else (resp. then) branch may require much more time to determine."
"s, left shift, logical right shift, arithmetic right shift s, conjunction ∧, disjunction ∨, exclusive disjunction ⊕, and negation ¬. then we define a boolean arithmetic algebra (ba-algebra) as follows:"
"d, where one coefficient at least is odd. we assume now that this invertible coefficient α i0 is 1. this is justified since we can divide all coefficients (α i ) and the constant f d (k) by α i0, and multiply all coefficients of f"
"the red line with dots on the left in figure 2 represents the size of the obfuscated binary file corresponding to the obfuscated constant comparison statement from section 2.2.2. the graph shows that the size of the compiled binary grows exponentially with the degree of the polynomial used. the trend shows that using high-degree polynomials has a considerable impact on the size of the obfuscated binary. for instance, using a degree 5 polynomial means that every single obfuscated conditional statement in the source code occupies ∼ 370 kb, making the total size of the binary program impractically large."
"this study summarizes and extends our preliminary work on classifier selection [cit] and sparse fusion [cit] . we expand the theory part in three respects. first, section ii is expanded as a tutorial-like material for readers less familiar with state-of-the-art fusion. second, we provide mathematical evidence that, under reasonable assumptions, baseline (unregularized) logistic regression is unlikely to produce sparse solutions. thirdly, we give a detailed account into setting of the regularization parameters. this involves arguing that sparse regularization is able to zero out unreliable classifiers and that, under ideal conditions (no observation noise in the scores), the solution converges to unregularized solution. the experimentation is further expanded by (1) providing comparison of different score pre-warping variants for calibration purposes, (2) [cit] core task and (3) providing analysis of correlation coefficients across the selected classifiers. to sum up, even though regularization and sparsification have been studied for both linear and logistic regression schemes, their integration into fusion schemes in speaker verification is novel. our study is the first large-scale comparison of regularized, in particular sparse, fusion schemes in speaker verification."
"locating opaque predicates in a program is not obvious, even for invariant opaque predicates like obfuscated constants. this is the goal of tools like the loop tool [cit], that detects opaque predicates in a program and tries to break them using smt solvers."
"algebraic normal form (anf), also known as positive polarity reed-muller form, is a compact form for the representation of boolean functions. a formula in anf is an exclusive disjunction of clauses, where each clause is a conjunction of variables. the negation operator is unnecessary, as all variables appear in positive form. more formally, the anf of a boolean function f :"
"finally, we tested the obfuscated predicate presented in section 2.2.2 with mba obfuscation of degree 4 on a 32-bit variable. we embedded the mbaobfuscated conditional in a small c file, compiled it with gcc and examined the binary with s2e."
"in this section we use the drill-and-join synthesis method presented in section 2.3 to directly reconstruct the unobfuscated function from the obfuscated conditional. the synthesis method considers the target obfuscated function as a black-box oracle, and reconstructs it by interrogating it and learning about its behavior."
"explore a explore b figure 1 . symbolic execution. when a conditional statement is found the guard c and its negation are tested for satisfiability, and the two branches are explored accordingly."
in this section we briefly discuss how the synthesis approach would be used in a practical deobfuscation case and we provide some initial results on real-world obfuscated programs.
"researchers have been trying to automatically counteract control flow flattening techniques for years [cit], but not many effective tools are available, one notable exception being johannes kinder's jakstab [cit] . however, at the current state of the art no automated approach is effective in reverse-engineering code obfuscated by state-of-the-art control flow flattening [cit], severely crippling the capabilities of binary obfuscated binary reverseengineering."
"mba obfuscation is used to both obfuscate and diversify a conditional statement. we observe that its robustness has never been assessed in the literature, so one of the contributions of this paper is to evaluate it."
"opaque predicates. opaque predicates [cit] have been introduced as a cheap control flow obfuscation technique, consisting of building predicates whose value is hard to determine statically. this adds unreachable paths to the control flow graph build by reverse-engineering the obfuscated code. opaque predicates are static if they have the same value on each execution, and dynamic otherwise."
"we introduce a simple conditional statement that will be used frequently in the rest of the paper: a comparison between a variable and a constant, proposed as an example in [15, appendix a]. we will obfuscate this conditional statement with the mba obfuscation method described above."
"logistic regression is a probabilistic model of the decision boundary between two classes and its parameters (weights) are usually found as the maximum likelihood (ml) estimate on a training set [cit] . however, ml solution easily overfits with limited number of training scores (trials) which manifests itself as fusion weights with large magnitude [cit] . consequently, even a small change in the base classifier outputs causes large change in the fusion score leading to unreliable decisions."
"the first three rows show best individual base classifiers in terms of actdcf, mindcf and eer. as these scores are not pre-calibrated, calibration error is quite large. as expected, fusion improves accuracy over the best single classifier systematically. regarding score pre-warping, z-cal and s-cal yield similar fig. 3 . effect of the shrinkage factor (devset, female trials, itv-itv condition). the relative shrinkage factor in -axis is in eq. (11) normalized by the unregularized weight norm. results. they produce less errors compared to both the unwarped and the non-clipped score pre-warping variants. fusion training with grad."
"in sparse regularized fusion training, all weights are constrained by regularizer, some are pushed to zero, but even those that are retained are regularized. thus, when mismatch between devset and trainset is small it is expected that even subset of classifiers, which weights are regularized, cannot improve on the unregularized fusion. elastic net with marginally improves actdcf. elastic net with and, respectively, has similar actdcf trends as ridge and lasso, as one may expect. a general trend is that aggressive shrinking (small ) increases both mindcf and actdcf. the equal error rate (eer), however, does not follow the same trend; this might be because weight optimization target is the dcf rather than the eer region. [cit] core task as listed in table v . we compare five fusion strategies:"
"following [cit], we test synthesis on an obfuscated predicate obtained from a real life drm obfuscated binary by mougey and gabriel [cit] and built for windows xp sp3."
"when optimization is based on (12), the correspondence between and the shrinkage threshold can be found by a binary search on the possible values of . in each iteration, we select fig. 1 . regularized classifier fusion. we display the contours of for fusion of two classifiers. the global minima of are indicated by red crosses. for constrained optimization, we search for the minimum inside the region (here, the cases and are displayed). the case finds a sparse solution because classifier 2 gets zeroed. this solution hits closest to the true minimum on the unseen test data. even regularization outperforms the unconstrained case. thus, regularization and sparsification might be particularly useful under unpredictable corpus mismatches [cit] ."
"elastic-net, on the other hand, is based on the idea that we can combine both and regularizers into one constrained optimization problem, (13) as can be seen, (13) is a generalized variant of both lasso and ridge regression. one can always find such a where, in terms of performance, elastic-net will at least as good as lasso or ridge regression. however, whereas lasso and ridge regression had to select only one regression parameter, now we need to cross-validate over a 2-d space. in this work, the parameter is first fixed and then shrinkage factor is cross-validated as in lasso and ridge. in practice, will also be cross-validated in so that the best and shrinkage factor will be selected based on cross-validation set to be applied on the evaluation set."
"we have tested the drill-and-join method to synthesize the unobfuscated function within an obfuscated statement, using the obfuscated statement as a black box for the synthesis process. we tested the method on the comparison between an input and a constant described in section 2.2.2, considering obfuscating polynomials of degree from 2 to 6 and adding spurious variables do obtain input sizes from 32 to 96 bits. the results are presented in figure 5 . figure 5 (a) presents the time necessary to synthesize the function. comparing with the time required by the smt solvers presented in figure 3 (a) it shows that the synthesis approach is faster than the smt approach and scales better with the degree of the polynomial used in the obfuscation. figure 5 (b) presents the number of calls to the oracle performed by the algorithm during the synthesis procedure. the number of calls does not significantly increase with the degree of the polynomials, confirming that the increase in computation time is mostly due to the increased cost of each call to the oracle. the computational time cost of the drill-and-join algorithm is exceeded by the time required by the oracle to answer the calls, showing the efficiency of the algorithm. we also note that we did not implement any of the optimizations to drill-and-join proposed by the author, like parallel implementation or subspace caching [cit] ."
"boolector 1.5.118. boolector [cit] supports queries in its own language btor and in the smt-lib language. it uses strong rewriting algorithms to simplify the statements just after parsing, which we expect to be effective for the deobfuscation of obfuscated conditionals. it starts by checking satisfiability of an overapproximation of the formula of interest, and uses a counterexamplebased iterated refinement approach to verify its satisfiability. it also supports bit-blasting for bit-vectors and lazy handling of the theory of arrays. the data shows that z3 is the slowest smt solver for handling obfuscated conditionals, and that boolector is slightly faster than stp on higher degree cases. the two different implementations of stp, integrated in klee and via metasmt, do not differ significantly in solution time, showing that metasmt does not add a significant overhead to the solvers."
recently yadegari and debray considered the problem of symbolically analyzing obfuscated conditionals [cit] . they also focus on manipulating conditionals to hide them or to hide their relation with the inputs. they mainly consider the class of obfuscated conditionals that can be handled by taint-based analysis. we complement this approach by considering a class of obfuscation mechanisms that are inherently difficult to analyze with taint analysis. [cit] also measured the impact of different taint analysis techniques on the quality of the deobfuscation for several off-the-shelf virtualization-based obfuscators.
"for evaluation of the methods, we consider the detection cost function in (1), where the cost parameters are, and . we measure both the minimum dcf (mindcf) and the actual dcf (actdcf). we also consider calibration error, defined as the difference of actdcf and mindcf, and the well-known equal error rate (eer), corresponding to the case of equal miss and false alarm rates 5 . table iii shows our twelve base classifiers based on different cepstral features and four different speaker modeling techniques. when a base classifier shares the same model and features, it means that the base classifiers are independent implementations. for speaker modeling, we use the generative gmm-ubm-jfa [cit] and the discriminative gmm-svm approaches with kl-divergence kernel [cit] and bhattacharyya kernel (bhat) [cit] . we also include feature transformation (ft) method [cit] as an alternative supervector for svm. all of the methods are grounded on the universal background model (ubm) paradigm [cit] and share similar form of subspace channel compensation, though the training methods differ. we the ubm and the session-variability subspaces, and additional data from the switchboard corpus to train the speaker-variability subspace for the jfa systems. each base classifier has its own score normalization prior to score pre-warping and fusion. to this end, we use tz-norm [cit] data as the background and cohort training data."
"dynamic opaque predicates [cit] are opaque predicates whose value can change between executions. commonly family of predicates are correlated so that even if they change value, the execution trace remains the same. hence, we expect dynamic synthesis to be effective on deobfuscating traces produced by dynamic opaque predicates, as long as the whole family of predicates is examined by the synthesis algorithm. if some of the predicates are outside the fragment of code to be synthesized, we expect synthesis to be misled and add unreachable traces to the control flow graph."
"we have started by evaluating the practical feasibility of mba obfuscation when using polynomials of increasing degree. we have found that both the time required to produce the obfuscated statements and their size grow very rapidly, de facto preventing the use of polynomials of degree above 5 or 6."
"assume that an attacker suspects that some compiled code contains an obfuscated constant, and wants to determine if the constant has been obfuscated using the mba obfuscation method described above. then the attacker will scan the code looking for evidence that the code has been produced by the obfuscation method."
"stp git-12/02/2015. the stp solver [cit] supports queries in the cvc and smt-lib languages. it uses word-level preprocessing with several heuristics including array abstraction refinement and a bit-vector linear arithmetics equation solver, then it translates the words to sat for satisfiability checking. stp is the default smt solver used internally by klee, but it is also supported by the metasmt framework. we used both versions of stp to verify whether metasmt has an impact on the resolution time."
"based on the output of viterbi process, a convex optimization equation is derived to estimate epipolar line distortion. we summarize the properties of the epipolar line distortion caused by normal factors in intelligent vehicle applications. based on these properties and inspired by the famous optical flow problem, we convert this distortion estimation problem to an optimization problem and employ the convex optimization theory to solve it. the viterbi process and convex optimization are integrated into an online framework (as shown in fig. 2) and two parts benefit each other without losing speed in this framework. it can automatically keep the epipolar line constraint to avoid the degradation of stereo matching results, which usually happens when other stereo matching methods being applied for driving vehicles."
"we make several interesting observations from table vi . firstly, comparing the best individual classifier to the other strategies, fusion of multiple base classifiers outperforms individual classifier in nearly all the cases. in a few cases (most notable, itv-tel), the single classifier has good calibration though. second, comparing the unregularized baseline to the regularized variants, one of the latter variants wins in most conditions. the exception is the tel-tel condition where the unregularized baseline outperforms all the regularized variants. in fact, tel-tel condition is the easiest case, possibly due to larger development set and longer experience of the team in processing telephony data. comparing ridge, lasso and elastic net, none is a clear winner but the relative performance depends on the condition and metric. regarding the primary metric, actdcf, all of them are useful for reducing the number of false alarms compared to the unregularized baseline by a statistically significant margin. for instance, with only a slight increase of target speaker misses, ridge and elastic net reduce the number of false alarms to nearly half of that of the unregularized baseline on the mic-mic condition. generalization bounds show that sparse solutions that give low error rates have a good chance of generalizing to an unseen dataset [cit] . however, as such bounds are loose on non-sparse solutions, depending on the data set, dense weight vector can generalize well also as we have seen here."
"we have presented a sparse regularized logistic regression score fusion for speaker verification. [cit] corpus (i.e. evalset). we find that sparse regularization brings improvement over unregularized variant in all other sub-conditions and measures (eer, mindcf, actdcf) except in tel-tel condition."
"several white box implementations of well-known block ciphers have been cryptoanalyzed using black-box methods [cit] . inspired by this, we want to test the effectiveness of black-box dynamic synthesis to break or simplify opaque predicates. dynamic synthesis interrogates a program by considering it as a black box and inductively synthesizes it by learning from its input/output behavior. we show how dynamic synthesis can improve concolic analysis by simplifying constraints. we used dynamic synthesis to implement a cryptanalysis method that produces an equivalent, concise form in algebraic normal form (anf) of the obfuscated conditionals. this form simplifies the constraints handled by the constraint solver."
"while many symbolic execution techniques take advantage of concrete tests to improve efficiency, to the best of our knowledge dynamic synthesis has not been investigated against obfuscated conditionals and never used to drive concolic execution."
"motivated by this observation, we consider regularized [cit] logistic regression whereby weight vectors with large norm are penalized. regularization defines a constrained optimization problem where one finds a compromise between training data accuracy while avoiding weights with large magnitude. regularized solution can also be viewed as maximum a posteriori (map) estimate of the fusion weights, over which one imposes a prior distribution [cit] . as in any practical bayesian learning method, two additional design concerns are now introduced: (1) choosing the regularizer (functional form of the weight prior) and (2) training its parameters that act as hyperparameters. to exemplify, ridge regression [cit] or squared euclidean norm regularization corresponds to choosing an isotropic gaussian prior with zero mean where the variance parameter determines the degree of regularization applied. in this study, we train the regularization parameters using a held-out validation dataset and focus on the first design question, the choice of the regularizer."
"as a future work, it would be interesting to pursue methods that optimize ensemble diversity and ensemble classification error simultaneously as a way to obtain an ensemble with a good generalization property. alternatively, run-time classifier ensemble selection for each speech utterance, similar to adaptable fusion using auxiliary quality measures would be an interesting direction."
let f n 2 be the set of all n-tuples of elements in the galois field f 2 . any vectorial boolean function f : f
"we note that the coefficient of α i in the expression of β i,1 does not depend on i, and is invertible in z 2 n . we can thus compute all (α i ) from these coefficients (β i,1 ):"
"byte-code representation that is executed by an embedded virtual machine, possibly with a randomly generated instruction set. such randomized obfuscation makes it possible to diversify the binary generation process. virtualization is implemented in off-the-shelf obfuscation programs like themida [cit], code virtualizer [cit], vmprotect [cit], and tigress [cit] . state of the art in deobfuscation shows that control flow flattening not based on opaque predicates can be broken by using static path deobfuscation [cit] . recent work [cit] focuses on the use of symbolic analysis together with taint analysis to deobfuscate virtualized binaries and allow exploration of their execution path. symbolic analysis maintains sets of constraints on the execution paths to determine which inputs cause each branch of a conditional statement to be explored. concolic analysis combines concrete execution of program traces with symbolic analysis to increase code coverage and trigger hidden behavior. this depends on the ability to symbolically determine the satisfiability of conditional statements, since the analyzer has to decide which branches of the conditional to analyze, as shown in fig. 1 ."
"extensive experiments were conducted to compare proposed algorithm with other practical state-of-the-art methods for intelligent vehicle applications. according to evaluation results at the kitti [cit] training dataset which includes total 194 images, our method has 7.38% average error rate compared to sgbm's 12.88% and elas's 11.99%. we also test the proposed algorithm in our experimental autonomous vehicle at real driving environments. for any 640x480 images with maximum 40 disparities, the running time is about 196ms with gtx titan gpu and xeon e5-2620 cpu. real driving videos including featured cases and typical failure cases can be found in the supplementary material."
"klee expresses the smt constraints in quantifier-free fix-size bit-vector logic with array, arbitrary solvers and function symbols (qf aufbv), thus we experiment with smt solvers that are able to handle such logic."
this simplification provides a way to decide the satisfiability of an mbaobfuscated conditional of any degree with the same complexity of deciding the satisfiability of an mba-obfuscated conditional of degree 1.
"we have evaluated the effectiveness of several smt solvers in deciding the satisfiability of mba obfuscated conditionals. we have observed a concolic execution engine like klee is significantly slowed down in determining the satisfiability of obfuscated conditionals, since the analysis time appears to increase exponentially with the degree of the polynomial used for the obfuscation."
"our goal is to find optimal weight vector (say, ) so that classification errors are minimized on the development data, thus hopefully on the unseen evaluation data. here we adopt the detection cost function (dcf) commonly used in the nist speaker recognition evaluations 1 to assess the accuracy of any speaker verification system:"
we have considered mba obfuscation as an example of a class of obfuscation techniques that are resistant to taint analysis and most static analysis methods used in code optimization.
"several symbolic analysis techniques use smt solvers to determine whether a given conditional statement can be satisfied, considering the constraints that the analyzer has accumulated up to that point."
"the z-cal pre-warping function is defined similarly to s-cal, only difference being that instead of smooth sigmoidal shape, z-cal defines a piece-wise linear function with hard thresholding (clipping). z-cal is defined as: . z-cal parameters are optimized in a same way as s-cal parameters. we also experimented with the unclipped variant of z-cal, optimization was performed in a same way except that the clipping step was not used. it is expected that unclipped z-cal will provided similar results as optimizing fusion scores without pre-warping. score pre-warping methods selected for this study have been summarized in table i ."
"the above argument assumed diagonal covariance matrices and a particular special case of logistic regression. even though the same analysis no longer holds for full covariance matrices, it does illustrate that there are cases when unregularized solution cannot find a sparse solution. with sparsity promoting regularizers, on the other hand, we can force sparse weights regardless of whether the classifiers are correlated or not."
"white box cryptography. the goal of white box cryptography is to prevent an attacker from identifying and extracting the key in a block cipher encryption, even with a full control over the execution platform. to achieve this goal, whitebox cryptography consists in implementing a specialized version of the algorithm that embeds the key k, and which is able to do only one of the two operations encrypt or decrypt [cit] . this implementation is resilient in a white box context because it is difficult to extract the key k by observing the operations carried out by the program and because it is difficult to forge the decryption function starting from the implementation of the encryption function, and vice versa."
"static opaque predicates are invariant [cit] if they always have the same truth value (i.e., correspond to true or false) and contextual [cit] if their truth value depends on other variables (e.g., the comparison between a variable and an mba-obfuscated constant in section 2.2.2). we expect the synthesis approach proposed here to be effective against any kind of static predicate, as long as the predicate works a sufficiently small input space."
"each element in the expansion is a function belonging to a vector space of a lower dimension than the vector space of f . the expansion can be applied again to obtain more elements of an even lower dimension until they become all bit constants, and oracle-based synthesis uses calls to the black-box oracle to understand the values of such constants for the function under analysis and synthesize it. in general this requires a number of expansions linear in n, creating an exponential number of elements."
the drill map δ is used to produce the bases of a functional program simulating a given target function by dividing the function's space in subspaces and recursively calling itself on them:
"we assume that, during the development phase, one has access to a development set containing score vectors from base classifiers, . here, indicates whether the corresponding speech sample originates from a target speaker or from a non-target . though it is not always the case during the nist sre campaigns, here we assume that these labels contain no errors. we consider linear score fusion of the form, where contains the classifier weights (discrimination component) and the bias (calibration component). the augmented score vector contains constant 1 and the base classifier output scores."
"since the raw base classifier scores may have different interpretations (e.g. log-likelihood ratios, svm scores or i-vector cosine distances) with considerable variation in their scales, it is important to properly align the score distributions [cit] . note that the base classifiers typically include their internal score normalization such as t-norm [cit], used for normalizing the classifier outputs across varying test segments and speakers with the help of external cohort models. here the concern is to make global score alignment at the classifier level. to avoid confusion with speaker score normalization techniques, we refer to global classifier-level score pre-processing as score pre-warping."
we test different smt solvers on deciding satisfiability of an opaque predicate generated by applying mba obfuscation with polynomial of different degrees to the comparison between an input and a constant described in section 2.2.2.
"finally, figures 5(c) and 5(d) present the number of applications of the drill and join functions during the execution of the algorithm. they provide additional insight on how the synthesis method scales with the input size and degree of the obfuscation polynomial."
"the data show that the synthesis time increases exponentially with the input size, following a similar increase in the number of required calls to the oracle. this suggests that the obfuscator can effectively counteract this attack by increasing the number of variables used by the obfuscation polynomial, and consequently the input size. we will consider applying bitwise taint analysis to determine which bits of the input are in fact affecting the result and which are spurious additions, allowing the synthesis algorithm to focus only on the important bits."
"comparing the fusion training methods, gradient systematically outperforms the other two methods in all three costs. the det plot in fig. 2 confirms this. we find the direct optimization of mindcf produces generally higher error rates than logistic regression which does only indirect minimization. this suggests that logistic regression offers better generalization performance. for the rest of the experiments, we choose gradient with s-cal."
"consequently, we can quickly construct an underapproximation of the control flow graph by following only the branches we are certain about. subsequently, the undecided branches can be re-examined to decide whether they lead to dead code or should be added to the graph."
"the polynomial g d has degree d, with coefficients in z 2 n . its coefficient of degree 1 is odd, while all other coefficients are even (since b 1 is odd, while all other b j are even). we can then identify the number of divisions by 2, and apply our method with g d, instead of f"
"control flow flattening replaces the control flow logic with a dispatch-execute loop, forcing an adversary to perform global analysis to understand local control flow transfers and obstructing both forward and backward analysis. virtualizationbased obfuscation translates parts of the source code to be obfuscated into a is c sat?"
"for computational reasons the constraint is typically approximated using the constraint, which is also known as lasso [cit] . the vector norm can also be constrained by, which corresponds to ridge regression. however, unlike lasso, ridge is not a sparsity promoting constraint [cit] ."
"even though joint classifier ensemble selection and training the fusion weights is a combinatorial optimization problem, it can be mathematically formulated as -regularization [cit] where the regularizer (zeroth norm) counts the number of non-zero weights, corresponding to the selected classifier ensemble. since its time complexity is still exponential with respect to the number of base classifiers, the usual workaround is to use -regularization instead, a method known as lasso (least absolute shrinkage and selection operator) [cit] . in the logistic regression model, -regularization has also been applied [cit] . lasso shrinks all the coefficients, with some of them forced to be exactly zero. by regularizing logistic regression with the lasso constraint, we can simultaneously optimize fusion weights and perform classifier selection."
"1. we perform an experimental feasibility analysis of mba obfuscation, in terms of obfuscation time and size of the obfuscated code. we show how the time required for obfuscation and the obfuscated file size grow exponentially with the degree of the polynomial used for the obfuscation. 2. we test the effectiveness of smt-based techniques for determining the truth values of mba-obfuscated conditional statements. we find that smt solvers can solve mba-obfuscated predicates in a time in the order of tens of seconds. 3. we present an algebraic simplification approach that reduces the complexity of polynomial mba deobfuscation to the deobfuscation of linear mba obfuscation. we show that the algebraic simplification is orders of magnitude faster than smt solvers. however, it only works if the mba follows a specific construction that is easy to change, thus is not general enough to be considered as an efficient technique in practice. 4. we test a dynamic synthesis method for determining the truth values of mba-obfuscated conditional statements, and show its higher effectiveness compared to smt-based techniques. since synthesis is also more general than algebraic simplification, we conclude that synthesis is the most effective method among the ones we have evaluated."
"we have investigated the direct use of the drill-and-join dynamic blackbox synthesis method, to simplify the representation of obfuscated conditionals. more generally this approach would be used by an attacker using concolic execution, along with an adapted strategy to drive the symbolic execution and employing synthesis as a more efficient rewriting strategy to boost the efficiency of an underlying smt solver. we have found that drill-and-join can efficiently synthesize the obfuscated function, thus counteracting mba obfuscation. since dynamic synthesis does not depend on any property of mba obfuscation, this result extends to other obfuscation methods."
"in this study, we consider weighted linear combinations of the base classifier scores as the fusion. with a small number of adjustable parameters, linear fusion scheme often shows good generalization performance. but it is crucial for the weights to be optimized using robust method which tolerates reasonable deviations in the base classifier score distributions. in speaker verification, the scores may vary considerably between the training and runtime data mainly due to differences in acoustic environments and transmission channels. the obvious weight optimiza- tion strategy, minimizing error rate on the training set, easily overfits [cit] ."
"where determines the desired amount of shrinkage. in (10), the norm has an interpretation as the maximum number of classifiers retained, but this does not necessarily hold for which takes up any positive real value. therefore, rather than based on human judgment, in (11) should be merely considered as a control parameter. in this study, we optimize using cross-validation. a useful insight into choosing a suitable range of possible 's is the desired amount of shrinkage relative to the unregularized solution. that is, set, where are the maximum likelihood weights for (5) and is the desired amount of shrinkage, such as . from a viewpoint of optimization software packages, a more useful form of (11) is its lagrange multiplier formulation, (12) where is the lagrange multiplier. it is known that the larger, the more the norm will be shrunk [cit] . example of (12) on real data can be seen in fig. 1, where two base classifiers are fused. from the example it is clear that weights found by the direct optimization of (5) would lead to non-optimal solution for the test set."
"and with no score pre-warping at all and unclipped z-cal yields same eer and actdcf, but in mindcf there is a slight difference. as the optimization cost of linear calibration and are slightly different, there are small differences in mindcf. in addition, generative pre-warping strategy by mvn also yields different but comparable results to all three unclipped variants."
"we use the llvm [cit] compilation framework to implement the mba obfuscation transformations. to evaluate the effectiveness of constraints solvers against mba obfuscated conditionals, we use the klee symbolic execution engine [cit] and the multi-solver support in klee provided by the metasmt framework [cit] ."
let us denote by z 2 n the quotient ring of integers modulo 2 n and by f n 2 the ring (z 2 ) n of n−tuples of elements in the quotient ring of integers modulo 2 (z 2 ).
"several attacker models have proved to be efficient against obfuscated binaries. each one targets a specific class of obfuscation mechanisms. taint-based analysis, dynamic binary translation combined with control dependencies and optimization transformations are efficient, e.g., against virtualization-based obfuscation. our synthesis-based approach can be considered a different, complementary attacker model to the ones listed above."
"we need now to check that our decomposition is consistent with the original full expression given, since we only used a small number of its coefficients. this check is easy, since we only need to expand our decomposition, and control each coefficient."
"we conjecture that synthesis algorithm like drill-and-join could be used to simplify smt formulae even in the general smt solving scenario. for instance, a formula could be considered as a black-box oracle and synthesized as a preprocessing step, producing a compact synthesized formula that would then be subject to the normal satisfiability procedure. in this sense, synthesis would be used as a part of the simplification procedures already implemented by smt solvers. since synthesis seems to be sensitive mostly to the size of the input space and smt solving mostly to the number of constructs, combining the strengths of the two techniques may result in a system more effective than synthesis or smt solving alone."
"in the condition itv-itv, lasso regularization provided better performance than elastic-net. it shows that estimating the trade-off parameter by cross-validation is not always successful. as a future work we plan to utilize bayesian model selection techniques to automatically estimate both and parameters from the fusion training set."
"outline. the rest of this paper is structured as follows: section 2 introduces mba obfuscation, the drill-and-join synthesis method, and the mathematical theory necessary to understand them. section 3 discusses the practical feasibility of mba obfuscation, determining bounds on the degree of the polynomials that the technique can use in practice. section 4 tests the effectiveness of smt solvers in determining the truth value of obfuscated conditionals and section 5 gives an algebraic simplification technique to greatly reduce the deobfuscation time. section 6 evaluates the drill-and-join synthesis method on the same problem. section 7 discusses related work, while section 8 concludes the paper and points out possible future research directions."
"we observe that multiple solutions can be found for the (α i ) and the (b j ). all these equivalent decompositions are equally valid: each one of them allows the analysis and simplification of the linear mba identity. we make here some assumptions on the decomposition that we will compute, and explain why these assumptions are justified."
"to get a sound synthesis of the target program, we have to explore the whole input space. in the case of our case study as presented in section 2.2.2, the function we want to synthesize is a function on the 32-bit input variables x, x 1 and x 2, i.e. a 96-bit input space. synthesized functions are validated against the black-box oracle by random testing. if the synthesis algorithm fails to synthesize a function or the random testing finds incongruences between the obfuscated and the synthesized functions, the attacker can either decide to consider both branches of the conditional as satisfiable, or decide to drop the branch to avoid adding suspicious branches to the control flow graph. in the first case the attacker risks of exploring unreachable branches of the graph, while in the second case the attacker risks not exploring reachable branches of the graph."
"the coefficients (α i ) of the linear mba identity are now known. we assume now that the constant f d (k) is null. as previously, we show here that an equivalent decomposition, with the same coefficients (α i ), respects this constraint."
"s peaker verification is the task of accepting or rejecting an identity claim based on a person's speech sample [cit] . modern speaker verification systems utilize ensembles of base classifiers to arrive at an accurate verification decision by classifier fusion. the base classifiers might utilize, for instance, different speech parameterizations (e.g. spectral, prosodic or high-level features), models (e.g. gaussian mixture models [cit] or support vector machines [cit] ) or channel compensation techniques (e.g. joint factor analysis [cit] or nuisance attribute projection [cit] )."
"we now turn attention to weight optimization using the three regularizers described above (ridge, lasso and elastic net). fig. 3 shows the effect of regularization to recognition accuracy on devset. for ease of interpretation, we show the accuracy as a function of the normalized regularization constraint rather than the lagrange multiplier . here, is the constraint in (11) and denotes the unregularized (maximum likelihood) weight vector. thus, corresponds to the unregularized solution."
"we denote by deg(f ) the degree of f, i.e. the number of variables in the longest clause of the anf of f . we call f an affine function iff deg(f )"
"note that the times in figure 4 are in microseconds, as opposed to the seconds of figure 3(a) . the results show that the time required by the algebraic simplification is orders of magnitude smaller than the time required by the smt solvers. considering 64-bit words instead of 32-bit words does not change the results significantly."
"the choice of the order in which to examine the branches is non-trivial. however, we note that the problem is similar to test generation for software model checking. therefore, we expect that insight developed for fuzzying tools like sage [cit] could be adapted to this aim."
"we have presented an algebraic approach to simplify mba obfuscation, reducing the complexity of the obfuscation with a polynomial of a given degree to the complexity of obfuscation with a polynomial of degree 1. the approach is able to determine whether the obfuscated function is a constant. this approach severely cripples mba obfuscation, but strongly depends on the structure of the obfuscation and could be easily counteracted by slightly changing it, so a more general approach is required."
"most of the experiments conducted in our paper use klee and the metasmt framework, to be able to compare the efficiency of different smt solvers against opaque conditionals. to apply such methods to real-world obfuscated programs, we have chosen instead to use the s2e symbolic execution engine [cit] . this tool embeds the klee symbolic execution engine and allows us to directly analyze obfuscated binaries, thanks to qemu's dynamic binary translation engine (dbt) frontends. s2e uses stp as its embedded smt solver."
"as future work, we propose to study the integration of alternative black box approaches to drive the concolic execution of obfuscated programs. to the best of our knowledge, such an approach has never been investigated. the idea is to modify the usual concolic strategy to enable compact representations of obfuscated constraints, obtaining more synthetic and easy to read synthesized code. in addition, we expect the simplified constraints to be easier to solve or check for satisfiability."
in this work we try to define an attacker model representative of what can be realistically done by an adversary able to use both static and dynamic analysis tools (symbolic and concolic execution engines) to reach his goal. such a model can be used to assess the robustness of any obfuscation method.
"in speaker verification, (1) is used for computing both the actual (actdcf) and minimum (mindcf) values. the actual cost refers to the dcf value obtained whenever the decision threshold is fixed to a particular value beforehand, whereas mindcf indicates the oracle value (minimum) on the test set that can easily be found by linear search over the range of . therefore, by definition, and the difference actdcf-mindcf can be used as a measure of calibration error in terms of how well the was estimated. [cit], but in this present work we consider only the additive term."
"we conclude that the mba obfuscation method becomes impractical when using polynomials of high degree, due to the time required to produce the obfuscated statements and the increase in execution time. for this reason, in the rest of the paper we will not consider mba obfuscation with polynomials of degree above 5 or 6."
"here, and are the miss and false alarm probabilities as a function of the decision threshold, is the prior probability of a target (true) speaker, is the cost of a miss (false rejection) and is the cost of a false alarm (false acceptance)."
"is of primary interest, since the generation and the execution of the mba expression strongly depend on this degree. on the contrary, the degree of f d is used only in the generation of the mba expression, and this generation depends only linearly on this degree. following this remark, we can tweak the generation of the mba expression, using polynomials from another family. these polynomials will not have the specific properties we used to reduce the mba expression: in this case the reduction will fail. figure 5 . application of the drill-and-join synthesis method to the synthesis of obfuscated linear mba functions using polynomials of degree from 2 to 6 on an input size from 32 to 96 bits. a more generic approach has then to be explored, to deal with more general mba expressions. in section 6 we abandon smt-based methods and present our results on the application of the drill-and-join synthesis algorithm to this problem."
"during execution of the target, we capture the queries to the stp smt solver, and we analyze the time required to synthesize them using the drill-andjoin algorithm."
"a dynamic synthesis method is used to inductively synthesize a target program by learning from its input/output behavior. the target program is considered as a black box oracle and interrogated by the synthesizer, which constructs a function simulating the behavior of the oracle with the highest possible precision."
"most common pre-warping is mean and variance normalization (mvn), also known as z-normalization. mean and standard deviation of the entire score distribution is estimated from the training data and applied to the held out score as . mvn defines affine score normalization whose parameters can also be discriminatively learned, as we will see later."
"the graph in figure 3(a) shows that the mba obfuscation method is effective in slowing down the analysis of conditional statements even when using a polynomial with a small degree. therefore, mba obfuscation is effective in slowing down control flow deobfuscation based on smt techniques, i.e. when a large number of conditional statements have to be deobfuscated and the attacker is not able to spend the required hundreds of seconds for each one. figure 3(b) gives the average size of a solver query (measured by the number of constructs internally counted by klee) and the total number of llvm instructions for mba expressions of 16 variables and polynomials of degree 2, 3, 4 and 5 over integers of size 32 and 64 bits. results for 64-bit average and degree 5 are missing due to a bug in the klee engine. the graph is helpful in understanding the reason why smt solvers are ineffective in addressing mba conditional deobfuscation: the number of constructs expressing the conditional statement grows quickly with the degree of the polynomial used, and smt solvers' computation time is sensitive to this number of constructs."
"in theory, elastic-net should, at least be equal to the best regularized fusion method, in all cases. but we notice that in the itv-itv condition, cross-validation selected, instead of 1, as would have corresponded to the lasso regularization. this will require further study on how to perform more accurate estimation of the parameter."
"for each component function f i, the algorithm works by recursively reducing the dimension of the vector space the function belongs to, until such dimension becomes zero. if the algorithm has a basis for the subspace it is working on, it uses the join map to reduce the dimension of the vector space by at least 1 and calls itself again. if it does not have a basis it calls the drill map to reduce the dimension of the vector space instead, and then calls itself again. the drill map does not require a basis for the subspace, but has to evaluate three functions in the reduced subspace, leading to a potential exponential increase in the number of functions evaluated. for this reason the algorithm uses join whenever a basis is available, and drill otherwise."
". we would like to minimize (5) subject to this constraint. obviously, one can simply enumerate all the possible classifier ensembles to ensure that the size constraint is satisfied, and optimize the weights by minimizing for each of these ensemble candidates and choosing the one that minimizes the cost function."
"the effectiveness of the algorithm can be improved by using a cache of the bases of subspaces, reducing the number of subqueries to the black-box oracle. however, we have not exploited such capability in our experiments."
"convex combination of ridge regression and lasso leads to another regularization technique known as elastic-net (e-net) [cit], which retains the zeroing capability of lasso, but because of the ridge term it does not push base classifier weights to zero as aggressively as lasso or classifier ensemble selection do."
"to summarize, smt solvers are not efficient enough to consider them a sufficient measure of deobfuscation for the concolic execution scenario. in section 5 we present a technique to increase the effectiveness of smt solvers against mba obfuscation, based on simplifying the algebraic structure of the obfuscation. in section 6 we explore the application of the more general drill-and-join synthesis algorithm to synthesize the obfuscated function instead of using smt solvers to determine its satisfiability."
"in this example, stp is able to find inputs to produce both outputs in just 3 ms, while the synthesis approach fails and after 5250 ms claims the predicate to be equivalent to the constant 0xa061440b071544l. arybo was able to synthesize the function is 31 ms, contrarily to the apparently pessimistic claim by the arybo authors that estimate 36 years [cit] ."
"the synthesis of each component boolean function f i follows the expansion approach, in which the target function is iteratively separated in its own subspaces, with each division decreasing the dimension of the vectorial space of the function by at least 1, until the bases of the function are found and recombined in the function f i itself. the drill-and-join synthesis method we consider in this paper has been recently introduced by balaniuk [cit] as an efficient implementation of this idea."
"this approach is very attractive, due to its low complexity: we can reduce easily the analysis of a general mba expression of high degree to the one of an expression of degree only 1. however, this process strongly depends on the generation process of the mba."
"we note that the construction of the polynomials f d and f we assume now that one of the coefficients α i is invertible in z 2 n, i.e. odd, and we call α i0 this coefficient. if this is not the case, all coefficients (except maybe the constant one) of the multivariate polynomial are even. we remove then the coefficient of degree 0, and we simply divide all of the remaining coef-ficients by 2, times, until at least one of them becomes odd."
"this work proposes a synthesis-based attacker model. this complements other attacker models including those based on taint analysis, dynamic binary translation, optimization transformations, and so on. we conjecture that the synthesis approach will be particularly effective when combined with taint analysis, since taint analysis can be used to select the bits affecting the result of the obfuscated conditional and synthesis can produce the deobfuscated conditional as a function of those bits only."
"one and optimize the weights using it, output is then the norm of the weights. final weight vector is the one whose norm is closest to the target, but does not violate it."
"the blue line with xs on the right in figure 2 represents the increase in execution time of the obfuscated constant comparison statement from section 2.2.2 compared to its unobfuscated equivalent statement. as the graph shows, obfuscation with a polynomial of degree 2 to 4 does not increase significantly the execution time, while using a polynomial of degree 5 increases it by 40%, a polynomial of degree 6 almost doubles it, and a polynomial of degree 7 increases it by 14 times. the trend shows that using high-degree polynomials has a considerable impact on the execution time of the obfuscated system: the execution time ratio grows quickly."
"the first step consists in finding precisely the expressions (e i ) used in this formula. this step is easy, due to their bitwise nature. we remark that the expressions (e i ) can become more complex: this is the case of the example given in section 2.2.2 where the first linear identity is multiplied with a variable x. these multiplications can however be easily detected, as it is sufficient to find in the formula the expressions that appear only with a set of bitwise expressions, and with the same power."
"control flow flattening transformations can be reinforced by implementing the dispatcher with cryptographic hash functions and opaque predicates [cit], and in this scenario, it has been shown that statically breaking this obfuscation transformation depends on the ability to statically analyze the opaque predicates. the seed of the hash function is a vector of opaque predicates itself. the challenge for the obfuscator is how to prevent the cryptographic hash function and its seed to be easily detected. diversification techniques based on white-box cryptography can be used to hide the signature of the hash function and its seed."
"up to this point, we have defined the standard fusion framework, assuming a full ensemble of classifiers. now, instead of just optimizing the weights, we are in search of both the optimal classifier ensemble and weights. assume that we have decided on an appropriate size of the ensemble given by integer variable,"
"in this paper, we advocate sparse regularization applied to logistic regression model training in speaker verification. sparse regularization means that, in addition to optimizing fusion weights for the full classifier ensemble, we would like to implement simultaneously classifier ensemble selection by forcing redundant classifiers to have zero weight. classifier selection can be also seen as a feature selection problem [cit] . feature selection methods are generally divided into three groups: wrapper methods that use classification error to select features, filter methods that use a surrogate cost function to select features and embedded methods which jointly select the subset of features and optimize the classifier parameters. sparse logistic regression studied here belongs to this last category."
"the mba obfuscation method replaces simple statements with longer equivalent statements, as explained in section 2.2. in this section we evaluate the size and execution overhead of obfuscated code. the results are depicted in figure 2 . all experiments in this and in the following sections are conducted on a intel core i5 1.60-2.30 ghz."
"we have investigated the effectiveness of dynamic synthesis when used to reconstruct obfuscated conditional statements. this is used by a concolic ex-ecution engine to simplify the symbolic representation of the constraints over the variables, and to determine the satisfiability of the obfuscated conditionals examined."
"recently, deep learning networks, including convolutional neural networks (cnns), have been widely used in image classification and computer vision [cit] b; [cit] . the deep 3d cnns were used to extract the features of 3d medical images for classification [cit] . a multi-task deep learning (mdl) method was proposed for joint hippocampal segmentation and clinical score regression using mri scans [cit] . given the very high dimensionality of the brain mri data, it requires huge computational resources and a large dataset to train a deeper cnn with robustness. since the mri datasets used for ad diagnosis are typically very small compared with the datasets used in computer vision, it remains a major challenge to train a deeper cnn model with a large number of parameters to be learned [cit] . recently, a classification scheme with an ensemble of deep learning architectures was proposed for early diagnosis of ad [cit] . in this approach, the gray matter (gm) image of each brain was split into 3d patches according to rois defined by the automated anatomical labeling (aal) atlas, and different deep belief networks were trained with the patches of different rois and followed by an ensemble with a voting scheme for final prediction. meanwhile, a landmark-based deep feature learning (ldfl) [cit] b) ."
"2) the width of the confidence interval increased as the confidence increased; the greater the confidence interval, the greater the probability of including the actual power value, which is consistent with theoretical estimation. table 2 shows the output weights for each power segment corresponding to the mcmc-arima prediction model after segmentation optimization at the 85% confidence level. the optimal output weights of various power segments differed. segmentation optimization should be used to identify the optimal output weight of each power segment, thereby improving the accuracy of the prediction interval. fig. 8 shows the results of interval prediction for different power segments. fig. 9 shows that, under the same confidence level, the comparison between the prediction result of the segmentation optimization prediction model and that of non-segmented optimization is narrow but ensures tracking of wind power time series changes. the upper and lower limits of the interval represent more accurate forecasts and can better guide decision makers. in order to verify the effectiveness of qga, different optimizing methods were compared. the same training samples and test samples were selected. ga-mcmc-arima and pso-mcmc-arima were used to predict wind power range and compared with the qga-mcmc-arima optimization model proposed in this paper. a comparison of performance indicators is presented in table 3 ."
"natural orifice transluminal endoscopic surgery (notes) is minimally invasive surgical procedure using a flexible endoscope to access to the abdominal cavity via transoral, transcolonic or transvaginal routes [cit] . the major advantages of this method are the absence of associated abdominal wall complications, and providing cosmetic benefits. however, some problems remain unsolved in notes. firstly, although many devices are under development to enable closure procedure [cit], the closure of the internal entry point for notes presents a significant challenge. secondly, usually the dual-channel endoscope is used in notes allowing procedures to be performed in a manner as laparoscopic surgery. the channels are close and parallel to the camera, leading to loss of triangulation and a more technically challenging procedure. some devices have been developed to solve triangulation problem [cit] . however, triangulation is still minimal by these devices. finally, because the endoscope is flexible, attempting to manipulate tissues and organs may lead the endoscope to create an unstable operative platform. the transport (usgi medical, san capistrano, ca, usa) was designed for notes using shapelock technology [cit] . however, because the transport uses wire tension to lock the shape of the shaft, they often suffer from problems of wire breakage and thus cannot be used safely. additionally the mechanisms and structures of transport are complicated, costly, and difficult to achieve mri compatibility. needlescopic surgery (ns) is a laparoscopic surgical technique using instruments and ports smaller than 3 mm in diameter. ns can reduce the surgical incisions in the abdominal wall to a size which is difficult to detect macroscopically. however, ns has not been widely adopted for minimally invasive surgery as involving some limitations. firstly, observations are restricted by the poor scope visualization and instantaneous loss of visual field due to blood and fat mist. secondly, instruments used in ns are easy bending of the shaft caused by its small diameter and leading to risk of organ damage because the instruments themselves act as needle."
"the multi-task deep cnn model captures the multi-level features for joint hippocampal segmentation and disease classification, while the deep 3d densenet model learns the features from the image patches of the hippocampus for disease classification. to integrate these deep models, we further stack an extra fully connected layer above the concatenation of the learned features from deep models for disease classification. the densenet models and multi-task model are individually trained, and a fully connected layer followed by a softmax layer is finely tuned to make the final classification. they are implemented with keras library in the framework of tensorflow. we will show that the proposed multi-model deep network framework outperforms the single-model approaches."
"the status of the grid-connected evs are collected, including the serial number i of each ev, accessing time t i start, preset departure time t i end, the soc start i when ev i accesses the grid, and the preset minimal soc set i at departure. then the population size q is determined, and an initial swarm satisfying the constraints(7 ∼ 9) is generated. 1) two fitness functions are formulated and the fitness value of the particles is calculated based on the defined fitness functions. the fitness function 1 and 2 are used to evaluate swarm's grid load stabilizing performance and battery degradation suppression performance respectively. with the guidance of both fitness functions, the generated v2g strategy could stabilize grid load fluctuation and reduce v2g participants' battery degradation costs at the same time."
"we compared our proposed method to other existing methods based on structural mri data. [cit] . an automatic method was proposed for hippocampal segmentation and hippocampal volumes were extracted for ad diagnosis . instead of just focusing on hippocampal volumes, 93 rois were parcellated in the brain and their volumetric features were calculated to train svm classifiers for ad classification [cit] . to capture fine-level features, the voxel-wise tissue densities were calculated for ensemble sparse classification of ad [cit] . these features are hand-crafted from one-region, multi-region to voxel levels and are widely used for ad diagnosis."
"these populations are independent and assigned with different fitness functions. the main-population represents the coordination between several optimization objectives, and the sub-population can enrich the diversity of the main-population. meanwhile, to improve the algorithm efficiency in the large-scale optimization problem, reducing the demand on population size and computing resources, the sub-population and main-population are divided into several sub-groups again. each sub-group has the same objective function and constraints, but the initial particles are different. during the evolution, the particles in different sub-groups evolve in different directions, several evolution centers are generated in the optimization process and the homoplasy can be avoided effectively. in this paper, two sub-populations and one main-population are set, and every sub-population and main-population is further divided into s sub-group with the particle number popsize."
"the output weights of prediction models of different power segments are unique. if the same output weight is used, the accuracy of the prediction interval will be reduced. therefore, the power interval is divided and qga is applied in this paper to identify the optimal output weight of each power segment. [cit] s, mainly introducing concepts related to quantum computing into gas. in qga, a qubit-based coding scheme is used, wherein a quantum bit is defined by a pair of complex numbers, and a system with m qubits can be described as"
"the proposed deep learning framework was tested on the structural mri data from the baseline visits of 449 adni participants consisting of 97 ad, 233 mci, and 119 nc subjects. both hippocampal segmentation and disease classification tasks were conducted for method evaluation. the classification task was tested to distinguish ad vs. nc and mci vs. nc. we randomly divided the whole mri data set into 5 folds and 5-fold cross-validation was used to train and test the proposed method. each time, in our implementation, the trade-off parameter * in the loss function of our multi-task deep cnn framework evolved from 1 to 0 throughout the training process. we tested the segmentation and classification tasks by setting * to 1 or 0 for single task learning and setting * to 0.5 for multi-task learning. shown in table 5 and 6 is the comparison of the segmentation and classification results by setting different α's for single and multi-task learning as well as the proposed adaptive method. from the results, we can see the multi-task learning with fixed α performs better than single-task learning for classification. the proposed adaptive method performs better than the single-task and multi-task learnings with fixed α for both segmentation and classification. from table 5, we can see better segmentation results on nc subjects than ad subjects even though the training set includes ad subjects. this may be caused by the hippocampal atrophy of ad and its variation among different subjects, which introduces difficulties in both automatic and manual segmentations."
"to address this research gap, our paper aims to outline a reliability assessment approach for electrical power systems considering of wind power uncertainty based on a probabilistic interval prediction model. the contributions of this paper mainly include two parts: one is that the variation ranges of reliability indices by sequential monte carlo methods are obtained for the first time for a power system under an ieee-rts79 reliability test system, which is combined with wind power probabilistic interval prediction model. the variation range of three system reliability indices can be obtained: loss of load probability (lolp), expected power not supply (epns), and loss of load frequency (lolf). these reliability indices thus carry reference significance for power system decision makers. the other contribution is wind power interval prediction model using arima based on mcmcbased bayesian estimation and interval weight parameters optimized via qga. taken interval coverage and interval average bandwidth are selected as evaluation criteria, compared with the genetic algorithm (ga) and particle swarm optimization (pso) algorithm, our simulation results show that the proposed prediction interval model has superior prediction accuracy and general accuracy."
"qga also uses the operation of quantum revolving door to realize chromosome updates. the algorithm thus has the ability to develop and explore, to obtain the optimal solution of the target. commonly used quantum gate transformation matrices have xor gate-controlled xor gate revolving gates and hadamard transform gates. in the quantum genetic iteration of this paper, the population is updated with the following quantum revolving gate:"
"(1) through a reliability test system built, the impact integrated wind power to grid on system reliability index is analyzed: compared with the original system, whether the predicted power upper bound or lower bound is used, the three values of the reliability indicators lolp, epns, and lolf exhibit significant declines."
"3) according to the wind farm power interval prediction model, it is assumed that the wind farm output power is taken as the upper or lower limit of the interval prediction model, respectively, and the reliability result is obtained via simulation to determine the reliability interval of the wind farm in access mode."
"fitness function (1) focuses on peak-shifting performance, so particles in sub-population 1 have a better effect on peakshifting. fitness function (2) focuses on battery life protection performance, so particles in sub-population 2 have a better effect on battery degradation suppression. fitness function (3) is the fitness function of main-population, with high requirement on both optimization objectives, the best solution for multi-objective optimization can be found in main-population."
"in traditional classical statistical theory, the unknown parameters ϕ i and θ j in arima are constant, whereas the bayesian parameter estimation considers unknown parameters ϕ i and θ j in arima as random variables that can be described by the probability distribution. this probability distribution is called the prior distribution of unknown parameters. compared with the conventional estimation method, bayesian estimation makes full use of prior distribution information from the sample information model parameters and incorporates it into statistical inference. the estimator has a smaller variance and square difference, which improves the statistical inference quality. the core idea of bayesian estimation is to obtain the bayesian posterior distribution of unknown parameters via the bayesian theorem and then use the posterior distribution to estimate model parameters."
"the bending distal end consists of six aligned frames that mutually rotate 90° around their axes. the frames are driven by four wires that are 90° apart. for each frame, the rotating angle in the vertical and horizontal direction is ± 40° and ±45°, respectively, because of which the bending distal end can achieve a curvature of ±120° and ±90° in the vertical and horizontal directions, respectively ( fig. 1(a) ). the workspace as an end effector of the outer sheath is given in fig. 1(b) . furthermore, the bending distal end is manufactured in an integrated manner, and therefore, assembly of the frames is not necessary. the resin material (fullcure720, objet geometries ltd., israel) which used in the prototype was authorized as biocompatible material by fda (the u.s. food and drug administration). because the breakdown limit of the bending distal end is about 58 n, it should be strong enough to be used in clinical practice."
"(5) record the best individual and its fitness value. if the value is greater than the current optimal value, then update the optimal value; otherwise, retain the current value."
"as for the computational complexity, the proposed deep learning combination method includes both the offline training and online testing stages. in the offline training stage, the computational cost includes training the multi-task deep cnn model and the densenet model, which take 0.93 hours and 1.40 hours in our experiments, respectively. thus, it takes about 2.33 hours to train the whole combination model. in the online testing stage, it takes 0.29s and 0.85s on average to test the proposed algorithm for segmentation and classification of a given image, respectively, which demonstrates the usefulness of the proposed method in a real application. all the experiments were conducted on a pc with ubuntu14.04-x64 and gpu nvidia geforce gtx1080 ti of 11 gb memory. the over-fitting problem was alleviated by using dropout techniques and data augmentation. the inverted dropout was used on the cnn layers, which performed the scaling at training time, leaving the forward pass at test time untouched."
the flowchart of the proposed active battery anti-aging v2g scheduling method is shown in fig. 3 . the system operation process can be divided into four steps:
"the arma(p,q) model, specifically the autoregressive moving average, is a common power prediction method. wind power is often unstable due to its volatility and intermittent nature. the arima(p,d,q) model, specifically the autoregressive integrated moving average model, is an extension of the arma(p,q) model. it converts the non-stationary time series into a stationary time series using the d-order difference method, and then builds the arma(p,q) model:"
"keeping in the view of above perspective and issues, to suppress the battery aging effect in v2g services and improve the performance of the v2g scheduling system, a novel active battery anti-aging v2g scheduling method is proposed in this paper. the key contributions are as follows: (1) the battery degradation phenomenon during v2g services is quantified by a novel rcc algorithm;"
"there remain several problems with the existing hippocampal analysis methods. first, both hippocampal volumetric and shape analyses depend on accurate segmentation of the hippocampus. but it is still a challenging task to accurately segment the hippocampus due to its irregular shape and blurred boundary in mri scans. second, the hand-crafted features of hippocampal volumes and shapes may not be optimal for the subsequent analysis, which may affect classification performance in disease diagnosis. third, using the hippocampus alone may not be sufficient for discriminating mci from nc subjects. other regions adjacent to the hippocampus such as the parahippocampus and amygdala are also affected in early stage of ad. finally, the visual and texture features of mri scans derived from the hippocampal region can be of great help for ad diagnosis [cit] ."
"the second pinaw in the objective function is the prediction interval average width, reflecting the clarity of prediction, as shown in (10) . this equation avoids the risk of the prediction interval being too wide due to the pure pursuit of reliability. in this case, the effective prediction value uncertainty information could not be obtained, and the decision value would be lost:"
"although the proposed method can jointly learn the feature extraction and classification model to achieve optimal diagnosis performance, it has limitations in medical interpretation and characterization of the learned features relevant to disease (i.e., ad or mci) for clinical application."
"from the above analysis of mcmc theory, the main idea of mcmc is to construct a markov chain with a stable distribution of p(x). it is simple to construct such a markov chain; the most commonly used algorithms are metropolis-hastings (m-h) and the gibbs sampler [cit] . due to the advantages of gibbs sampling in terms of high-dimensional features and sampling paths, gibbs sampler is used in this calculation to generate the markov chain required for mcmc simulation."
"the simulation results indicate that the addition of wind farms in the original rts-79 node system can decrease system load loss frequency and load loss expectations. taking the reliability index lolp at the 85% confidence level as an example, when the wind farm output takes the upper-limit condition, the system lolp value is at least 0.0630, 27.3% lower than the original system. the value at this time is recorded as the photovoltaic power station. when the working condition is the lower limit of wind farm output, the lolp value is 0.0757, (i.e., the lolp value of the system is 12.7% lower than that of the original system). the value at this time is recorded as the lower limit of the lolp of the wind farm. wind farm output exhibited uncertainty. overall, the system reliability ranges between 12.7% and 27.3%, implying a system reliability improvement of 12.7-27.3% after accessing the wind farm. the above reliability index calculation method provides a range that contributes to the reliability of the power grid after considering wind farm access, which offers guidance for wind farm planning, plan output adjustment, and power dispatch."
"the inherent features of v2g scheduling optimization, i.e. high-dimensional and large scale, cannot be neglected [cit] . moreover, in active battery anti-aging v2g scheduling, the objective to minimize grid load fluctuations conflicts with minimizing battery degradation. the trade-off is actually a pareto-optimal point searching problem. in addition, the objective function in v2g scheduling is usually not simply linear or quadratic, so the conventional convex optimization method is not applicable [cit] . further, introducing the battery degradation index turns the optimization objective into a noncontinuous, non-derivable and non-gradient function, and the conventional gradient descent algorithms are no longer effective [cit] ."
"outer sheath without suction from the outer sheath. the view was not clear by the fat mist. on the other hand, the image which suction was applied from the outer sheath was shown in fig. 5(i) . the clear view means that suction created by the outer sheath took effective action against fat mist."
"(3) the wind power output probability interval is obtained by the mcmc-based bayesian estimation arima method, which utilizes prior knowledge and the distribution hypothesis of known data, and to assess observed data based on these probabilities and distributions. use reasoning to make the best judgment. the qga optimization algorithm can be used to determine the best output weight for each power, leading to higher interval coverage and an obtained narrower bandwidth. compared with the ga optimization method and pso, the superior prediction effect of the proposed method is demonstrated."
"after the replacement, particles are generated again for sub-population under the initialization principal in step 2 for keeping the population size. then turn to step 3, the particle velocity and position are updated iteratively."
"to address the above problems in the computer-aided ad diagnosis, we propose a new deep learning framework with a combination of multiple deep cnn models for simultaneous hippocampal segmentation and disease classification using mri data. in our framework, we first construct a multitask deep cnn model for jointly learning hippocampal segmentation and disease classification. then, a 3d densely connected convolutional network (3d densenet) model is developed with the inputs of 3d patches extracted based on the hippocampal segmentation results to learn rich features. finally, the learned features by the multi-task deep cnn model and the densenet model are combined with a fully connected layer to yield a final classification of disease status. the proposed multi-model framework will be shown to outperform each single model method. our method is evaluated for both hippocampal segmentation and disease classification, where we use the baseline t1-weighted structural mri data from the adni database including 97 ad, 233 mci, and 119 nc subjects. in addition, we also test our method on an additional dataset of 135 subjects from the adni mri cohort with the eadc-adni harmonized protocol (harp) for manual hippocampal segmentation [cit] ."
"the information collection & communication technology [cit], the grid load [cit] & v2g capacity estimation [cit] approach and the smart charging pile technology [cit] has been well studied in the existing literature. therefore, in the rest part of this paper, we mainly focus on the v2g behavior management method."
"training a deep model is not an easy task as the current datasets for hippocampal segmentation and disease classification are relatively small compared to the computer vision tasks. to alleviate this challenge, the data augmentation by shifting three coordinates was used to improve the robustness of the model. fig. 9 (a) and (b) show the loss curves of the multi-task deep model for both training and validation on hippocampus segmentation and disease classification, respectively, while fig. 9 (c) shows the loss curves of densenet model for both training and validation on the classifications of ad vs. nc and mci vs. nc. the results show that the loss curves of hippocampal segmentation converges after 40 epochs, while the loss curves of all disease classifications converge when the training epoch grows to 80. the loss convergence of ad vs. nc classification is faster than that of mci vs. nc because the classification of mci vs. nc is more challenging than that of ad vs. nc."
"(2) we apply the limitation calculation method of the wind farm affecting the system reliability as shown in section 4.2, to obtain the improvement range of system reliability under each confidence level. it provides a variable range of reliability index to power grid considering wind farm access, and thus carries reference significance with uncertainty information for planning a wind farm, adjusting the plan output, and guiding power dispatch."
"the above presented v2g scheduling system operates in a rolling way with the controlling interval of 15 minutes. in each time of v2g scheduling, the ev charging information and baseload is predicted by the prediction module through historical data and real-time information, and the v2g behaviors of the evs that have just been connected to the grid (within recent 15 minutes) are scheduled. the abovementioned prediction-decision v2g scheduling process is carried out repeatedly with the system operation."
"to generate better battery protection particles for subpopulation 2, in principle 2, maximum charge/discharge cycles are limited, as well as the constraints on charge/ discharge rate and dod are tightened:"
"in order to verify the effectiveness of mcmc-armia, an arima parameter model estimated by the least squares fit (lsf-arima) and a naive bayes model (nbc) were carried out for comparison and analysis. fig. 10 shows the wind power prediction interval based on the qga algorithm by the above method, and its performance indicators are shown in table 4 ."
"in order to reach the optimal performance of the proposed v2g scheduling algorithm, we prepared multiple algorithm parameter settings. however, not all results are reported in the paper, the comparison is only made with the optimal settings of each algorithm. the parameters of the conventional pso algorithm and proposed mcm method are set as follows for better performance through multiple experiments:"
"the conventional pso algorithm is not able to utilize the modern multi-core cpu resources effectively, the average cpu usage is only 13%, and with the proposed mcm method, the cpu computation resources can be used more reasonable (86% on average). as a result, the calculation time of the conventional pso algorithm within 50 iterations is as long as 476s on average, and this number is reduced to 135s with the mcm method, which validates the effectiveness of the proposed scheduling method. the maximum calculation time of mcm method in the whole scheduling period can be limited within 426s, which indicates that the v2g management system can schedule the charging behaviors of gridconnected evs in time (15 minutes, 900s)."
"using the outer sheath, mps in vivo experiment using a swine (female, 43.5 kg) has been performed for partial gastrectomy (fig. 5(a) ). the needle devices were inserted through two 3 mm access ports in the upper abdominal region. we inserted the outer sheath through lower abdominal region as much as possible to near rectal route. the incision part during insertion of the outer sheath was protected with a lap disk (hakko medical inc., japan). the instruments inserted into the outer sheath were a 3.9 mm industrial endoscope and a 2.8 mm flexible forceps. fig. 5 (b) ~ (i) were views from a 3 mm needle scope (karl storz, munich, germany). firstly, the outer sheath threaded its way through the large intestines and small intestines to locate stomach. we clamped the stomach surface (fig. 5(b) ), and exerted traction on stomach ( fig. 5(c) ) to expose the hypothetical affected area for treatment using the 2.8 mm forceps inserted from the outer sheath. secondly, we resected the hypothetical affected area by radiofrequency 3 mm needle forceps with traction on stomach using the 2.8 mm flexible forceps (fig. 5(d) ). thirdly, we clamped the hypothetical affected area directly by 2.8 mm flexible forceps, and continued resection with countertraction achieved by the outer sheath ( fig. 5(e) ). the hypothetical affected area was successfully resected with the outer sheath assisted (fig. 5(f) ). finally, we removed the resected organ from the outer sheath incision (fig. 5(g) ). moreover, we tested the suction performance of the outer sheath. fig. 5(h) was image of resection moment"
"second, on the basis of this interval prediction model, an ieee-rts79 reliability test system is built, and the variation ranges of reliability indices are obtained for a power system integrating wind power using sequential monte carlo methods. the power system reliability index selects the loss of load probability (lolp, representing the probability of a power outage event in the system); loss of load frequency (lolf, indicating the number of times the system has a load shedding failure per unit time), and the expected power not supply (epns). fig. 1 presents the overall flow chart."
"as shown in fig. 1, our proposed deep learning framework consists of two deep learning models. one model is a multi-task deep cnns for jointly learning hippocampus segmentation and disease classification, which generates a binary segmentation mask of the hippocampus and learns features for disease classification. however, the learned features by the multi-task model are not sufficient for accurate disease classification. a 3d patch covering the hippocampus is extracted based on the centroid of the segmentation mask and input into a 3d densenet model that learns more relevant features for disease classification. finally, a fully connected layer and a softmax layer are appended to combine the learned features from these models for final disease classification. in this study, two classes are considered in the classification task. the input of this framework is a large image patch covering the hippocampus. the outputs are the hippocampus mask as well as the prediction of disease status."
"the user information collection module is used to collect household electricity load and ev's charging demand data on the basis of information and communications technology (ict) [cit] . the real-time v2g charging demand information is sent to the information prediction module for future use, and at the same time, the charging requirements of the evs that have just been connected to the grid (within recent"
"we invented a novel procedural concept called multi-piercing surgery (mps) [cit] . mps is defined as notes-assisted ns. our new proposal in this paper is an ideal rigid-flexible outer sheath to solve the issues of access, and platform stability in notes. the outer sheath could provide a clear visual field, assist to resect the lesion, and remove a resected organ from the body. on the other hand, the tasks of ns in mps are to perform the surgery itself, and open and close the incision safely in the gastrointestinal tract for notes. the outer sheath combined with ns could also solve the problems of triangulation in notes, and organ damage problems in ns. the outer sheath can exchange between flexible and rigid modes, and make instrumental path to the abdominal cavity. in mps, an entrance wound is first created in the intestinal tract for the outer sheath by ns. then, in flexible mode, the outer sheath is inserted through the entrance wound in the intestinal tract, and locates an internal organ. when the outer sheath approaches the target in the abdominal cavity, the surgeon locks the shape of the outer sheath and then inserts flexible instruments easily through the path created by the sheath. then, ns can be performed easily with the outer sheath assisted. once in place, variety of flexible instruments can be inserted again and again without damaging the tissues around the outer sheath."
"for example, mild cognitive impairment (mci) is a prodromal stage of ad [cit] . in the past few decades, neuroimaging technologies have been widely used to discover the relevant biomarkers in the human brain for ad and mci diagnosis. magnetic resonance imaging (mri) is a non-invasive imaging technology that can produce detailed 3d anatomical images of internal body structures such as the brain, and has been widely used to help us understand anatomical and functional brain changes related to ad [cit] . in particular, structural mri scans provide detailed information about the anatomical structures of the brain, which can help detect and measure brain atrophy patterns in ad."
"in this paper, a new prototype of the outer sheath with lockable operating part is presented. in addition, the paper reports the analysis of working space of the bending distal end and shape holding torque of the rigid-flexible shaft. especially, the outer sheath system was first tested in in vivo partial gastrectomy experiments, and successfully preformed in vivo experiment using a swine."
"after the arima model obtains the wind power point prediction model, the value of the point prediction is multiplied by β 1 and β 2, and the obtained results are the upper and lower limits of the interval prediction, respectively. β 1 is upper limit weight of interval prediction model, and β 2 is the lower limit weight of interval prediction, which β 1 and β 2 are obtained via qga segmentation optimization. fig. 2 presents a schematic diagram of the interval prediction model."
"(a) (b) fig. 4 . the network architecture of (a) resnet block1 and (b) resnet block2, consisting of 3d convolution, prelu, bn, and dropout layers."
"(2) a mathematical optimization model is established for the active battery anti-aging v2g scheduling problem, in which the minimal battery degradation and grid load fluctuation are designed as the optimization objectives; (3) a multipopulation collaborative mechanism is developed with the ability to solve the large-scale, multi-objective, and nongradient optimization problem in active battery anti-aging v2g scheduling."
"the operating part is shown in fig. 3 . to achieve smooth rotational movement and reduce backlash, belt and pulley structure is designed to control the bending angle of the distal end. two pairs of wires (fig. 3) for control of the bending angle are fixed on the blocks (fig. 3) . two sets of belt and pulley structure (two pulleys in common to one belt) allow for transmitting rotary movement of the pulleys to translatory movement of the blocks (fig. 3) . therefore surgeons can easily control the bending angle of distal end by internal vacuum space rotating two knobs. bending angles of ±120° in the vertical direction and ±90° in the horizontal direction can be achieved by knob 1 (fig. 3) and knob 2 (fig. 3), respectively. furthermore, surgeons can turn the lever to lock the rotation of the knob by the friction between rubber o ring ( fig. 3) and lever lock (fig. 3), and then the bending angle of distal end can be locked."
"for hippocampal segmentation of subject m, the optimization objective is to minimize the dice loss function which evaluates the capability of our model to segment hippocampal voxels from the background:"
"(2) perform a measurement on each individual of the initial population to obtain a state p(t). when measuring, select 0 or 1 on the corresponding gene position according to the quantum bit probability. the specific method is as follows:"
"the v2g participants' behavior data were collected by beijing electric vehicles monitoring and service center, which is affiliated to national engineering laboratory for electric vehicles and serves as a national big data platform for electric vehicles in china. the monitoring data of a residential area with 40 households were downloaded from the established big data platform and served as the basic simulation data of this paper. as shown in table 1, the individuals' travel behavior data, including the vehicle tamp, vehicle grid-connected time and soc, departure time and soc, battery system parameters, etc. are further extracted from the collected data set to simulate the users' v2g behaviors and verify the proposed scheduling method."
"using the sequential monte carlo method, the values of the reliability indicators lolp, epns, and lolf are calculated according to the flow chart in fig. 3 . to ensure minimum error in the monte carlo calculation, the simulation time is 400 years; thus, the calculated reliability index can converge completely. figs. 11-13 depict a comparison of system reliability indices before and after the access system when the wind farm output is the upper limit and lower limit of the interval under the 85% confidence level. table 5 shows reliability indicators at 80%, 85%, and 90% confidence levels."
"prematurity appears in large-scale optimization problems [cit] . pso algorithm inclines to be stuck in local optimum because of prematurity, and the evolution process may stop before acquiring the actual global optimal solution. two methods are applicable for expanding the search range: one is to expand the population size, but the computation complexity is also increased tremendously; the other is to weaken the attraction of the global best solution, which may cause convergence difficulty [cit] . the homoplasy appears when dealing with the multi-objective optimization problem, limiting much of the search-space and depriving the potential of the algorithm to find a coordinating optimal solution [cit] ."
"alzheimer's disease (ad) is a progressive and irreversible brain degenerative disorder characterized by memory loss and cognitive impairment. at present, there are around 90 million people diagnosed with ad, and it is estimated that the number of ad patients will reach 300 [cit] . to date, no effective drug treatments are available to cure ad, while the existing ad medicines can only ease symptoms or slow down its progression. thus, the detection of ad at its early or prodromal stage is important for the prevention and intervention of its progression."
"in the random charging scenario, it is assumed that ev owners would immediately charge their cars with rated power upon arriving home until the batteries are fully charged. as shown in fig. 5, most evs are connected to the grid during 19:00-22:00 (zone a), while the baseload also booms in this period and peaks at around 21:00, as a result, the grid peak load is elevated to 504kw. while after 00:00 (zone b), most of the evs have been fully charged and the minimum grid load is only 97.5kw. to ensure the safe, stable and economic grid operation, it is necessary to suppress the grid load fluctuation."
"the proposed methods were validated effectiveness under various test environments. the fuzzy logic method was used in literature [cit] to improve the effectiveness of the pso algorithm in the multi-objective optimization problem. the experiment results in a v2g scheduling system indicated that the proposed method can improve the system performance effectively. however, to the authors' best knowledge, there is no published literature considering both the large-scale and multi-objective optimization problems in v2g scheduling at present. thus, to improve the performance of the pso algorithm based v2g behaviors management system, a multipopulation collaborative mechanism (mcm) is developed in this paper."
"in bayesian calculations, we typically use integral methods that require analysis or numerical approximation, including sample-based monte carlo sampling (e.g., important sampling, stratified sampling, and associated sampling), to sample from the posterior distribution to estimate parameters of interest. however, it is often difficult to generate samples directly from an arbitrary high-dimensional joint distribution, which limits the sample-based method. the mcmc method is a simple and effective bayesian calculation method developed recently. it applies the markov chain in the stochastic process to the monte carlo simulation to achieve dynamic simulation (the i.e., the sampling distribution changes as the simulation proceeds)."
"rcc algorithm has been widely used in many fields, such as fatigue damage analysis [cit], remaining useful life prediction [cit] and energy storage systems [cit] . in this paper, the minimal battery lifetime degradation is designed as one of the optimization objectives and quantified by a novel use of the rcc algorithm."
"(a) (b) (c) fig. 9 . the loss curves of the multi-task deep model for (a) hippocampus segmentation and (b) disease classification, and (c) the loss curves of densenet model for classifying ad vs. nc and mci vs. nc on both training and validation, denoted as \"ad vs. nc train\", \"mci vs. nc train\", \"ad vs. nc val\", \"mci vs. nc val\", respectively."
"where n is the number of total voxels on the segmentation output; and are the predicted segmentation result and the ground truth label of voxel i, respectively; is a small value to prevent denominator from being zero. the dice loss function can deal with situations where a strong imbalance exists between the numbers of foreground and background voxels [cit] . as to classification, a fully connected layer is stacked to concatenate the outputs of the compression part and each layer of the decompression part, which integrates information from two sources to enhance classification accuracy. the loss function for classification of subject m is the categorical crossentropy loss to evaluate the difference between the predicted label and the ground truth label as follows:"
"the data were obtained from the adni database, which is publicly available on the website (http://www.loni.ucla.edu/adni). [cit] by the national institute on aging (nia), the national institute of biomedical imaging and bioengineering (nibib), the food and drug administration (fda), private pharmaceutical companies and non-profit organizations, as a $60 million, 5-year public-private partnership. the primary goal of the adni was to test whether serial mri, positron emission tomography (pet), other biological markers, and clinical and neuropsychological assessments could be combined to measure the progression of mci and early ad."
"}, where x (0) represents an initial condition, and x (0), x (1), x (2) · · ·, x (k+1) is a markov chain."
"randomly generate a [cit] number, and if it is greater than or equal to the value of the probability amplitude, then the measurement result is taken as 1; otherwise, the results is taken as 0 (and vice versa)."
particle velocity and position are updated based on the formula (1) and (2) in this step. the fitness functions in subpopulations and main-population are as follows:
"the subjects were recruited from over 50 sites across the u.s. and canada, gave written informed consent at the time of enrollment for imaging and genetic sample collection and completed questionnaires approved by each participating site's institutional review board (irb)."
the v2g behaviors management module formulates the v2g charge/discharge control strategies for every grid-connected ev on the basis of the collected user demand and grid load state information. the optimization objectives are to minimize grid load fluctuation and battery degradation. the control strategies are sent to ev smart charger.
"the performance of different v2g behavior management methods is compared in table 3 . the coordination of ev charge/discharge behavior can be realized by using conventional pso algorithm based v2g management method [cit], which reduces 22.2% peak load and 30.1% grid load std, but it is not able to further suppress the load fluctuation and has poor performance on peak-shaving service. the proposed mcm method can further decrease the peak load and load std with 32% and 60.4% respectively, and the grid energy quality is significantly improved. compared to the conventional v2g scheduling method, the number of full-cycles (nfc) and half-cycle (nhc) in active battery anti-aging v2g scheduling method drops 79.4% and 15.6% respectively, which indicates that the proposed method is capable of suppressing battery degradation phenomenon in v2g service."
"energy is an essential part of modern life and energy management is an eternal topic in modern society. electric vehicles (evs) and power grid are two important components of the energy system. instead of the one-way energy flow from the grid to evs, their bi-directional link enables the flexible, cheap and fast-responding application of the vehicle batteries in the power grid [cit] . therefore, it leads to the concept of vehicle-to-grid (v2g) that effectively integrates evs into the grid as distributed energy resources [cit] ."
the battery degradation phenomenon can be quantified by analyzing the extracted battery number of cycles and dod data. the rcc algorithm is used to evaluate the battery aging in v2g scheduling in this paper.
"a battery anti-aging v2g behavior management method is presented in this paper. by using the rcc algorithm based battery degradation quantification method, the minimal battery aging effect was designed as one of the optimization objectives in the mathematical model. compared to the conventional v2g management method, the battery number of full-cycles and half-cycle are reduced by 79.4% and 15.6% respectively, which indicates that the battery degradation phenomenon during the v2g application is suppressed effectively. the designed multi-population collaborative mechanism can utilize the computational resources reasonably to solve the high-dimensional and multi-objective optimization problem in v2g scheduling. the simulation results revealed that the particle exchange process can boost population evolution and improve the algorithm performance effectively, the peak load and load std were further reduced by 32% and 60.4% respectively, which validated that the grid energy quality can also be improved by the proposed battery active anti-aging v2g scheduling method. this paper mainly focuses on suppressing the battery degradation problem in v2g scheduling. it is assumed that the baseload profile and battery state can be predicted and estimated accurately. but the prediction or estimation errors cannot be avoided in real scenarios and may influence the operation of the v2g management system. for instance, the prediction error of baseload and ev charging information may influence the v2g scheduling results and have a negative impact on system peak-shaving performance. likewise, the battery state estimation error may also influence the v2g scheduling, especially when quantifying the battery degradation phenomenon. future work can be conducted on studying the influence of the prediction error and how to suppress these influences in v2g scheduling."
"the framework of the proposed battery anti-aging v2g scheduling system is shown in fig. 1 . the system is divided into 4 parts: information prediction module, user information collection module, v2g behaviors management module, and ev smart charger. the optimal v2g behavior control strategies are achieved through the cooperation of four modules."
"compared with traditional cnn, densenets have several advantages. first, they can alleviate the vanishing-gradient problem since there is a direct connection from the low to high-level layers. second, feature propagation is strengthened to reuse the low-level features. third, they can reduce the number of parameters. table 4 shows the structure and parameters of our proposed 3d densenet model, consisting of a convolutional layer, 4 dense blocks, 3 transition layers, a global average pooling layer, and a softmax layer. first, a convolutional layer was added to the input layer with stride 2, followed by dense blocks."
"in principle 3, particles are generated for the mainpopulation, the balance between the various optimization objectives is more valued, so all the constraints are treated equally:"
"in the following, we use the mcmc bayesian method based on gibbs sampling to estimate the parameters of the arima model and provide the initial values of the two sets of arima model parameters in the sampling process. fig. 6 shows the markov chain trajectory constructed after volume 8, 2020 figure 7. prediction interval at 80%, 85%, and 90% confidence levels. parameters ϕ and θ are iterated 10,000 [cit] iteration initializations are discarded. fig. 6 indicates that the two markov chains tend to coincide, suggesting that the markov chain formed after gibbs sampling was convergent, and the bayesian estimation of parameters ϕ and θ could be calculated accordingly. results are listed in table 1 ."
"1) build a reliability test system ieee-rts79; specific parameters are detailed in the [cit] . the power system reliability index selection includes lolp, indicating the probability of a power outage event in the system (dimensionless); epns (mw); and lolf, indicating the number of times the system has a load shedding failure per unit time (sub/year). the calculation formula is"
"in the active battery anti-aging v2g scheduling issue, to obtain the optimal v2g control strategy, it is necessary to explore an effective method to overcome the prematurity and homoplasy obstacles. therefore, in this section, a multipopulation collaborative mechanism is developed and provided as a possible solution for the dilemma raised above. the flowchart of the proposed mcm method is shown in fig. 4, and system operation principles can be described by the following 4 steps:"
"(6) if the end condition (i.e., a sufficiently good solution or maximum number of iterations) is reached, then optimization ends. the group optimum at this time is the optimal output weight β; otherwise, return to step (2). (7) output data and terminate."
"the nad of the prediction interval is nad represents the accumulation of target values that fall outside the interval according to the degree of deviation. the smaller the nad value, the higher the quality of the prediction interval."
"the prototype of the outer sheath with endoscope and biopsy forceps can be seen in fig. 4(a) . the prototype has a maximum outer diameter of 25 mm, length of bending distal end 75 mm, and length of inserting part 330 mm. in addition, the model was equipped with one 7 mm, one 3 mm, and two 1.9 mm working channels (fig. 4(b) ). the flexible instruments like endoscope and forceps can be inserted from the 7 mm, 3 mm and 1.9 mm channels, and one of the 1.9 mm channels is used for suction and water jet (fig. 4(c) ). the rigid-flexible shaft consists of three long, flexible toothed links, a bellows tube, and a polyethylene cover. in addition, to get a better lock on the elementary part of rigid-flexible shaft, two short flexible toothed links are added on the base side of shaft (fig. 4(d) ). the outer sheath is connected to a vacuum pump (dtc-41, ulvac kiko inc., japan) and a vacuum controller to alternate between flexible and rigid modes by pushing a button. the sheath is also connected to a roller pump (rp-2100, tokyo rikakikai co., ltd., japan) to jet water. suction can be applied through vacuum supply ports in the operating room. the disposable inserting part can be separated from the operating part, and the outer sheath is separated from the vacuum controller and the vacuum source, to be cleaned and sterilized. all parts of the prototype are made of nonmagnetic material, and this ensures excellent mri compatibility."
"this article is organized as follows: section ii briefly introduces the architecture of the intelligent v2g scheduling system, in which the system working principles and information flows are defined. the background materials of the algorithm used in our work are detailed in section iii. the proposed battery degradation quantification method and active battery anti-aging v2g scheduling method are described in section iv. results and comparisons are provided in section v, followed by concluding remarks in section vi."
"the battery degradation mechanism has been well studied in previous work. however, the conventional battery degradation quantification method, including the electrochemical model [cit] and artificial intelligence algorithm [cit], can only quantify the battery degradation phenomenon on a large time scale (several days or weeks) [cit] . nevertheless, the scheduling horizon in v2g management is usually less than one day [cit], so it is difficult to quantify battery degradation in v2g applications. comparing to the conventional battery degradation quantification method, the rcc algorithm can quantify the battery aging phenomenon in a short period (several minutes or hours) [cit], which is more suitable for the battery degradation quantification issue in v2g scheduling. therefore, the rcc algorithm is used in this paper to extract the charging and discharging cycles and quantify the battery degradation phenomenon in v2g service. the application of the rcc algorithm has been well studied in our previous work: hybrid energy storage system in microgrid [cit], renewable energy system [cit] and energy management system of hybrid electric vehicles [cit] . basically, as shown in fig.2, the cycle counting can be achieved by the following three steps as following: firstly, the data (for the battery the data is the dod that presents the battery charge/discharge cycles) is pre-processed by searching for adjacent data points with the reverse polarity so that the local maxima and minima can be found and stored in a matrix. secondly, compose full cycles by analyzing the turning points and combine these subcycles to get full-cycles together with the summing up of the amplitudes. thirdly, extract and count the number of cycles in varying amplitude store them for later use."
"the rest of this paper is organized as follows. in section 2, the overall flow chart of reliability assessment approach is given. in section 3, a wind power interval prediction model based on qga-mcmc-arima is described. the reliability index calculation model is presented in section 4, and numerical studies of the proposed approach are demonstrated in section 5. finally, conclusions are drawn in section 6."
"in this paper, considering the uncertainty of wind power, the reliability indices of the grid connection are presented with probabilistic prediction interval model of wind power. the variation range of three system reliability indices can be obtained. combining the mcmc-optimized bayesian estimation arima based on qga is used to predict wind power. our findings reveal the following:"
"p n,1 · · · p n,j · · · p n,u p n,u+1 · · · p n,u+w p n+1,1 · · · p n+1,j · · · p n+1,u p n+1,u+1 · · · p n+1,u+w battery life is mainly influenced by the number of cycles, dod and charge/discharge rate. the number of cycles has been considered in the objective function, the dod and charge/discharge rate are restricted by the following constraints in this paper:"
"densenet is more important for disease classification. the combination model outperformed both the multi-task deep cnn and densenet models, indicating that the features of individual models could contain complementary information for disease classification."
"the sequential monte carlo simulation flow chart for the power system is illustrated in fig. 3 the state of components in the transmission system adopts a two-state model, as shown in fig. 4 . the trouble-free running time t ttf of the power generation system components and estimated fault repair time t ttr are calculated as"
this paper aims to outline a reliability assessment approach for electrical power systems considering of wind power uncertainty based on a probabilistic interval prediction model. it is mainly divided into two parts.
"taking a wind farm in the northwest china as an example, the rated power of wind turbine is 2mw and the time resolution is 15 min. [cit], the feasibility of this method is verified via simulation, and the reliability index is obtained."
"the results of the conventional pso algorithm based v2g scheduling [cit] are shown in fig. 6 (a). during grid peak hours, the evs are scheduled to feed energy back to the grid, the evs' charging load is no longer overlapping the baseload and the peak load of the grid is reduced successfully. however, the v2g scheduling is inherently characterized as a high-dimensional, large-scale optimization problem, it is very difficult to get the global optimal solution, which is reflected in the following two aspects: firstly, the conventional pso algorithm can only realize long term load-shifting, but the grid load fluctuation is not sufficiently suppressed: the load profile keeps fluctuating from 22:00 to 06:00; secondly, its peak-shaving performance is not satisfying, with only 8% drop compared to baseload profile, which means that only a small part of evs are scheduled to discharge during peak hours. the aforementioned issue is more serious when considering active battery anti-aging. to further improve the load-shifting ability of the v2g scheduling system, a mcm method is proposed in this paper and the result is shown in fig. 6 (b) . with the proposed mechanism, the v2g scheduling system can not only realize the long-term load-shifting performance but also suppress short-term grid load fluctuation. when compared to random charging, the load peak and load standard deviation (std) is reduced by 32% and 60.4% respectively. to verify the proposed battery anti-aging v2g scheduling method, the battery cycles in the conventional v2g scheduling method [cit] and the proposed one are compared in fig. 7 . subfigure (a) and (b) are the soc profiles, subfigure (c) and (d) are the corresponding charge/discharge cycles statistics by the rcc algorithm. with the proposed battery degradation suppression method, the battery number of cycles during participating in v2g service are reduced significantly: the number of half-cycles drops from 4 to 3, and the number of full cycles drops from 4 to 2, which indicates that the battery is protected successfully by the proposed antiaging algorithm."
"wind power has become one of the most popular renewable energy sources in the world, as it reduces the use of fossil fuels and saves greenhouse gas emission costs. however, the intermittence and volatility of wind power has restricted large-scale integration of wind turbines into power systems and poses new uncertainties and challenges to power system reliability [cit] . it is thus necessary to assess the impact of wind power on power system reliability."
"are the battery number of cycles and halfcycles of ev i in v2g scheduling, which can be calculated by the rcc algorithm described in section iii.b."
"first, the method based on mcmc bayesian estimation arima model combining with upper and lower weight of interval prediction model by qga optimization is used to predict wind power interval. to compare the effectiveness of the qga-mcmc-arima optimization model proposed in this paper, ga-mcmc-arima and pso-mcmc-arima were used to predict wind power range, and the predict interval coverage probability and the prediction interval average width were compared under different confidence levels."
"an active battery anti-aging v2g scheduling method is proposed in this section. firstly, a mathematical optimization model is established for the v2g scheduling issue, in which the minimal battery degradation and grid load fluctuation are designed as the optimization objectives. then, combine with the pso algorithm and the rcc algorithm, the system operation principles and information flows are detailed."
"the particle position and velocity in the sub-population and main-population are initialized in this step. the initial position influences the optimization efficiency directly, to reduce the required computing resources occupation, the particles should be distributed in the search-space as evenly as possible, while the particle validity should also be guaranteed. the particles in different populations are given different initialization principles to improve algorithm performance and efficiency. firstly, to generate better load-shifting particles for sub-population 1, in principle 1, constraints on grid peak power are tightened, while the constraints on charging/discharging rate and dod are loosened. the corresponding initialization principle is as follows:"
"step 1 using mcmc bayesian estimation parameters of arima model, obtains wind power point prediction value from arima model; step 2 initializes qga parameters, including setting the population number, initial position, individual extremum and total extremum; step 3 divides the wind power into different power segment and uses qga to optimize output weight β1 and β2 of prediction interval in different power segments. according to the objective function, the fitness and global value of each particle are calculated in each iteration. finally, the optimal output weight β 1 and β 2 are obtained; step 4 multiply the point prediction values output by the arima model by β 1 and β 2, respectively, to obtain the upper and lower limits of the wind power prediction interval."
"the pso algorithm is used in this paper to find the optimal v2g strategy. in the pso algorithm, each candidate solution is denoted as a ''particle'' without mass or volume in the search-space. the solution set consisting of a large number of particles is called a ''swarm''. each particle is labeled by three properties: velocity, position, and fitness. the position of the particle represents a candidate solution. the velocity determines the flying direction and distance of a particle in each iteration. the particles move in the search-space by updating velocity and gradually approach the optimal solution, and the fitness function is used to evaluate particle quality [cit] . in the conventional pso optimization process, an initial swarm is generated by randomly initializing particle position and velocity firstly. the position and velocity of particle i can be denoted as"
"step 2 and 3 are performed repeatedly, and the particle position is continuously updated before the evolution times k reaches maximum iteration times k max . the global optimal solution gbest k max is outputted as the optimal v2g control strategy."
"in this study, we proposed a new classification framework based on multi-model deep cnns for jointly learning hippocampal segmentation and disease classification. first, a multi-task deep cnn model was constructed to jointly learn the features for hippocampal segmentation and disease classification. based on the segmented hippocampal region, an additional 3d densenet was built to learn the rich and detailed image features for disease classification. finally, the learned features from the multi-task cnn and densenet models are combined to classify disease status. the proposed framework can not only output the disease status, but also provide the hippocampal segmentation result. no tissue segmentation and nonlinear registration are required for mr image processing. the experimental results based on the adni dataset have demonstrated that our proposed approach has achieved promising performance for ad and mci diagnosis."
"in this study, the sequential monte carlo method was used to calculate the reliability index of the system. monte carlo is a numerical calculation method based on probability and statistics and is widely used in power system risk assessment [cit] . the sequential monte carlo method considers the continuity of the system on the basis of the non-sequential monte carlo method, samples the working state duration of all components, forms the timing state of the system, and performs reliability analysis [cit] . compared with the non-sequential monte carlo method, the advantage of the latter method lies in its clear physical meaning, low algorithm complexity, and shortcomings of slow convergence, rendering it suitable for advanced grid reliability planning."
"the baseload demand curve used in our work is also obtained from the aforementioned residential area by smart meter technology. it is worth noting that unusual dates such as the chinese spring festival holiday, the new year holiday, and the weekends are excluded from the data set in advance. the detailed information of the simulation platform is shown in table 2 ."
"2) the personal and global best solution pbest k and gbest k are found through the fitness value, and particle position x i and velocity v i are updated following equation (1) and (2). 3)"
"before the system is newly connected to the wind farm, we must analyze the degree of wind farm reliability. it is difficult to accurately evaluate the wind farm due to its strong randomness. in this paper, based on the interval prediction model with wind power uncertainty information established above, a simple and feasible calculation method for evaluating the degree of influence of a wind power plant on the system is proposed. the specific reliability interval calculation steps are as follows:"
"a deep cnn has been formulated to learn residual functions at the convolutional stages to achieve fast convergence. we define two residual blocks as \"resnet block1\" and \"resnet block2\", consisting of 3d convolutional, batch normalization (bn), parametric rectified linear unit (prelu) activation and dropout layers as shown in fig. 4 . in resnet block1, a residual function is learned by a short connection: the input is added to the output of the second convolutional layer. resnet block2 consists of two convolutional layers and the input of each block is added to both the outputs of the second and third convolutional layers to learn the residual function. the kernels are trained with supervision from the batches of mri data. small kernels have fewer numbers of parameters to train for fast inference."
"the outer sheath consists of a bending distal end for local treatment and route selection, and a rigid-flexible shaft for selective shapelocking of the shaft, and an operating part for simple operation by surgeon."
steps 2 through 5 are repeated m times until markov converges and the first n iterations are rounded off to eliminate the effect of the initial value on the estimate.
"for quality control, only amt workers with 97% or greater approval rates and completion of at least 50 tasks were allowed to participate. additionally, we omitted judgments that were completed in less time than the first quartile among our sample (seven seconds). this led us to omit 7,843 judgments, leaving a total of 31,454 (also omitting i don't know responses)."
our retrieval experiments on the twitter collection were based on a set of test queries which were obtained by asking two users of an experimental twitter search engine to create queries of two types:
"the implemented r-tree consists of a multi-level hierarchy of axis-aligned bounding boxes, which come in the form of rectangular parallelepipeds due to their application in 3d space. each box encases a finite number of entries which can either point to tree nodes or spatially indexed records. these records correspond to the actual x3d objects of an x3dom scene; while the lowest and highest coordinate values among these records define the boundaries of the minimum bounding boxes (mbb) that constitute any r-tree instance. nodes' splitting methodology makes use of the quadratic algorithm [cit], since it provides a satisfying ratio of time complexity and space utilization. this methodology is accompanied by the appropriate insertion operation and together forms the boundaries of each mbb. when the desired 3d objects are successfully spatially indexed, then the coordinates of these boundaries are finalized and the following spatial reasoning process is ready to start."
"to compensate, we turn to eq. 8 to derive an exponential distribution that penalizes older documents to an extent that depends on the strength of evidence that we are dealing with a recency query. we do this in a two-pass approach. that is, we retrieve k documents using ql, estimate r q based on these, and then re-rank the k documents according to eq. 8."
we note that the conjugate prior of the exponential is the gamma distribution with parameters  and  (our hyperparameters) yields an estimate for the exponential rate parameter [cit] :
"in more recent work, dakka, gravano and ipeirotis augment the standard query likelihood framework to account for time [cit] . they approach the problem by considering two types of features for a given document. first, they consider w d which consists of the lexical terms in document d. second, they posit t d which is the timestamp for d. with these definitions in place, we may decompose the likelihood function:"
"to avoid including very common words in expanded queries, we applied a standard stoplist when constructing relevance models. for the trec data the stopwords consisted of the standard list included with the lemur toolkit. for the twitter data we supplemented this list with a brief customized stoplist that removed common hostnames, file extensions, etc."
"in our estimation of  we are guided by motivation similar to eq. 12's, asking, for each element of ̇, how many documents are older? as before, we consider  in eq. 16 as a binomial parameter. to estimate this parameter, we begin with maximum likelihood, where success is the observation that a particular feedback document f fbi is newer than a particular document from the collection at large. we thus have the estimate"
"topological relationships take advantage of the eight topological predicates that are defined in de-9im for 3d space environments [cit] . each one corresponds to a different boundary-based intersection pattern, which classifies an object to a specific topological relation according to another reference object. these relations come with a carefully designed taxonomy to prevent incorrect or nonessential implication of spatial predicates, since only one of the former can hold at a given space and time. thus, the provided topological relations are serially tested one by one against every mbb pair of a 3d r-tree instance, until a topological criterion is successfully met. this first valid occurrence is the one pointing to the best fitting topological predicate for the current mbb pair. first, the possibility of disjointness between two mbbs is tested for, implying that neither their boundaries nor their interior regions are in contact. the same formula has been also included the touch predicate, since the intersection of mbbs' boundaries alone does not affect the result-set of the upcoming directional relations. if these two topological relations are not satisfied, then the next closest spatial relation is initiated which is none other than the equal. in this case, the engaged rectangular parallelepipeds must have in common not only their relative boundaries, but also their entire interior region. however, this kind of relation is rarely met in real-world applications due to its strict constraints, leading the majority of 3d objects to the next formula of our computational model. that is the within spatial predicate which signifies that the first mbb totally encloses the boundary and interior regions of the second, taking also into account the automated attribution of the diametrically opposed predicates contains to the second one. on the other hand, there is always a chance that this pair of mbbs satisfies the inverse operators of these criteria, implying this time a www.aetic.theiaer.org"
"we evaluated our algorithms using three test collections: a twitter dataset, trec ap (disks 1 and 2), and trec la/ft (disks 4 and 5) using the lemur toolkit (http://lemurproject.org). we used no stemming or stop-lists (except in one case indicated below)."
"under bex, bringing query-specific information to specifying the exponential parameter allows recency information to privilege newer documents while tempering this influence for non-temporal queries. comparing exp to bex in tables 6 and 7 speaks to this. however, tsql showed still more robustness and promise. while prior-based methods (exp and exrm) gave significant improvements over their baselines four times, tsql saw 12 significant improvements. also, exp and exrm reduced effectiveness significantly in eleven runs, versus 2 for tsql (no significant declines for tsql on map). while applying temporal priors to relevance models yielded no discernable improvement over baseline rm, tsrm was able to improve over the baseline rm runs on the trec data. table 4 reminds us that we have defined and set many parameters in the course of this paper. an obvious question is, how sensitive are the proposed methods to the parameterization of their models? figure 2 shows the effect of changing the parameterization for each of the approaches we have outlined (except relevance feedback). the plotted data are for the recency training queries on both our trec and twitter corpora. in each panel, the blue line shows mean average precision as we change the rate parameter for li and croft's exponential priors (exp). the blue solid line shows map for the bayesian estimates of time-sensitive exponentials (bex). the dashed red line shows the sensitivity of the temporal smoothing method (tsql). finally, the horizontal black dotted line is map observed for standard query likelihood."
"in section 7 we analyze the effectiveness of our proposed methods of handling recency queries. to contextualize our results, we report comparisons against two baseline systems. the first approach is the simple query likelihood model (ql) of eq. 2. we also report results obtained by the application of time-based exponential priors (exp) as outlined by li and croft [cit] . in our discussion of relevance models, we replace the ql baseline with the kullback-leibler divergence (kld) model [cit] ."
"for instance, in their work on recency queries li and croft proposed using the publication date of news articles to inform a prior distribution over documents [cit] . rather than taking ( ) in eq. 1 to be uniform, li and croft propose using an exponential distribution:"
15. then a second round of retrieval is conducted using the induced relevance model. typically this is done by ranking documents in increasing order of the kullback-leibler divergence between their language models and the relevance model.
"turning to the models proposed in this paper, the bex approach alleviated the risk of temporal conditioning of search results for in comparison to exp. as we hypothesized, the rate parameter of the exponential in eq. 8 was moderated for non-temporal queries, leading to a diminished impact of time in these runs. bex did show declines in performance for recency queries compared to ql. but these declines were less severe than in the case of exp. whereas all exp declines in map were statistically significant, only those on the twitter data were significant for bex."
"the implemented 3d r-tree data structure has been enriched with a novel computational model for the automated implication of spatial relations. this model makes use of successive topological and directional predicates between each pair of indexed objects, eliminating pairwise entries for improved performance. such objects are associated with the cartesian coordinate system, a threedimensional distance metric which specifies their position with a signed triplet of numerical coordinates. the minimum and maximum values of the latter denote each object's boundaries that ultimately take the form of a rectangular parallelepiped. the vertices deriving from this type of geometry supply the computational model with the necessary semantics for the attribution of the appropriate spatial relations. in this way, the spatial reasoning process requires only each object's position to constitute a network of relationships based on the topological and directional relations being described below."
"because recency queries form an important part of many retrieval settings we also tested our approaches on two sets of news text gathered for trec [cit] . details of the trec collections are given in table 2 . we used topic titles as the query text. we classified each topic as -recency‖ or -non-recency‖ based on the temporal distribution of each query's relevant documents. if at least 2/3 of the relevant documents appear prior to the median document time, the query was considered a candidate for recency status. this was an admittedly ad hoc threshold, but it was chosen both for plausibility and in order to generate a suitable number of candidates. we then manually examined each query to determine if it had a bona fide temporal dimension to its relevance. only queries that met the 2/3 criterion and seemed temporally bound were classified as recency queries. 3 all others were called -nontemporal‖ although they may indeed have temporal qualities aside from recency. finally, we only retained queries with at least 10 relevant documents. this final number of selected queries for each trec collection is shown in table 2 . we used trec topics 301-350 as training data to estimate parameters. the procedure described above yielded 17 recency queries and 33 non-temporal queries, which were evaluated on the la.ft data."
"li and croft proposed integrating recency into relevance models through the quantity ( ) in eq. 14. following their use of document priors, li and croft use the exponential distribution for ( ) using eq. 4. we refer to this approach as exponential relevance models (exrm) in contrast to a baseline, non-temporal relevance modeling approach (rm)."
"there are other ways of injecting time into search results. we can, for example, represent t d as a separate dimension in a rank-thencombine approach described by pickens and golovchinsky [cit] . in this approach time becomes yet another feature on which documents can be ranked. the weight assigned to this dimension can either be calculated analogously to eq. 12, or can be controlled directly by the user in an exploratory search setting."
"previous work has shown how recency can be incorporated into ir [cit], particularly under the language modeling framework [cit] . in this paper we build on these findings. our contribution is a group of methods for incorporating temporal information into language modeling ir. each of the proposed methods relies on bayesian estimation where we use time as a factor in our retrieval model. in tests, our proposed methods work as effectively as or better than established temporal approaches for recency queries, while mitigating the risk entailed by temporally informed ranking on queries without an explicit recency bias."
"directional relations refer to another major category of spatial analysis, where linguistic predicates incorporate directional constraints on the tested subjects. in contrast to topological relations which are independent of user's current viewpoint, directional relations are applied to each spatially indexed object according to the default position and orientation of the right-hand coordinate system used by the x3d standard [cit] . each directional relation is bestowed with a dedicated intersection-based formula for the approximation of the best fitting relation in each case. such formulas act as a uniform classification scheme for the directional annotation of those mbb pairs, which have been derived to be disjoint during the stage of topological reasoning. these exclusively accessed spatial extensions deduct the appropriate directional relation for the first parallelepiped of a mbb pair, while at the same time, the second parallelepiped is attributed with the inverse relations of the former skipping unnecessary rounds of spatial reasoning. initially, it is checked whether the rightmost boundary of the current mbb has lower value than -or equal value to -the leftmost boundary region of the reference mbb. on this occasion, the first object is located on the left side of the second object and the opposite relation is attributed to the second parallelepiped. otherwise, if the leftmost boundary region of the tested mbb has a value higher than, or equal to, the rightmost boundary region of the reference mbb, then the first object is located on the right side of the second object, with left relation corresponding to the second parallelepiped. following the same pattern, if the lowest boundary region of the tested mbb has a higher value than the highest possible boundary region of the reference mbb, then the first object is located above the second object. however, in case these two boundaries are intersecting (even at a single point) in 3d space, then the first object is instead found to be over the second object. regarding their highest boundary region check against the lowest one of their reference mbb, the first object is always located below the second object on both occasions. finally, it is checked whether the most posterior boundary region of the first mbb has a value higher -or equal to the frontmost boundary region of the reference mbb. in case it does, then this object is in front of the second object. on the other hand, if the frontmost boundary region has a value lower or equal to the most posterior boundary region of the reference mbb, then the first object is located behind the second object. all formulas used for the deduction of each directional relation are displayed in table 2 ."
"in this paper we propose three overarching approaches to incorporating time into retrieval, paying special attention to the matter of recency queries. we offer the following approaches:"
"in practice, however, we found that the mles obtained from our query information led to models that lent too much influence to time, eclipsing lexical query similarity (e.g. 0.029 leads to an overwhelming age penalty). to mitigate this problem, we propose a bayesian approach, replacing the mle with the maximum a posteriori estimate of r q ."
"walking on the same path, but adopting a different set of linguistic predicates, the implemented mechanism takes also into consideration directional relations. early works [cit] in this area pointed out the fuzzy state of human perception and artificial intelligence systems in selecting the appropriate directional relations between two objects. this inability was not only proceeded from the chosen orientation plane and the objective technique used, but it was also amplified by the fact that neither a qualitative nor a quantitative classification of a space was sufficient to capture similarities between spatial representation and spatial language [cit] . however, a set of directional predicates could adequately reflect linguistic semantics in terms of spatial applicability and accuracy. these commonly referenced predicates were mapped onto the left, right, above, below, over, front and behind directional relationships, which describe the position of each 3d object according to the global coordinate system of the corresponding application. the latest technological advances allowed the automated generation of such relations for real-world indoor scenes with the utilization of a robotic mechanism [cit], while software-based approaches [cit] integrated the four cardinal directions in the form of rtree nodes, to hasten k-nn queries performance at the expense of space utilization. in our work, however, a standalone computational model steps on a r-tree instance for the automated implication of directional relations between the spatially indexed content. such instances are based on a valid rtree variant [cit] that fully complies with the set of properties addressed in the original version of this data structure."
"for each judgment we recorded basic demographic information of the judge, a numeric relevance score measured on a three-point likert scale: 0 (not relevant), 1 (maybe relevant), 2 (relevant), with an option for i don't know. each query/document pair was judged by six workers; queries that received at least one i don't know response were omitted. final numeric relevance scores were obtained by two methods:"
"this paper introduces an interoperable and practical computational model that combines a quadratic r-tree data structure with a set of the de-9im spatial relations, both implemented and optimized for application in web3d space. the model takes advantage of the mbbs formed from the spatially indexed content of x3dom scenes to extract the relative topological and directional relations between each record. adopting such boundary-intersection methodologies, possible complex and heavyweight calculations are thus avoided, significantly decreasing the overall spatial reasoning process. however, despite the fact that r-tree's construction and spatial semantics extraction costs were trivial for all tested case studies, the quadratic algorithm increases the search space redundancy during the last step of its operation. this is because the remaining entries are placed in a single node without any geometrical checks, increasing in this way the area coverage and/or the overlapping mbbs. even though that such a thing takes place on rare only occasions, alternative splitting solutions [cit] may be deployed and replace the current splitting methodology for those domains that require optimal construction and computation costs. finally, the presented use cases demonstrated the semantic capabilities provided by the combination of a spatial reasoning model and a flexible 3d r-tree data structure, along with their utilization potentialities in diverse areas of interest."
"however, the latter constitutes a limiting factor in the problem which confronts a user with regard to information retrieval, making the visual information included in image and video elements of a webpage to be practically unreachable. this searching restriction remains an open problem which must be addressed in a different way, since nowadays a world without web search engines would be irrational."
"as has been stated before, 3d r-tree complies with vr environments authored in x3d language and accessed through the x3dom framework. however, the generic structure of any r-tree instance remains independent of these two data formats. in this way, the result-set of a spatial reasoning operation can be further manipulated by other platforms and semantic services, or presented with various graphical approaches. in this section, a collection of demos is presented and described in order to denote possible application areas of 3d r-tree."
"the bottom three rows of table 6 show that relevance feedback hindered retrieval on the twitter data, with map significantly lower for rm than for ql in all cases but one (non-temporal by voting). this poor performance may be due to short documents in the twitter collection. thus with respect to relevance feedback models, we focus on the results from table 7 and table 8 . as li and croft found, the exrm approach shows little improvement over the baseline rm. several runs, however, show substantial improvement with tsrm. as expected, the bulk of this improvement occurs on recency queries; temporal smoothing appears to have negligible impact on non-temporal queries."
"recency is an important dimension of relevance for many kinds of queries. for these queries, relevant documents must not only be topically appropriate; they must also have been published in the recent past. though research on recency queries is not new, these queries present an especially keen challenge in contemporary ir given the growing popularity of microblogging services such as twitter [cit] . in the context of microblog search, fielding recency queries effectively is of prime importance. this paper proposes three novel approaches to handling recency queries, examining their effectiveness both on microblog data, as well as on more traditional ir data in the form of two trec news collections."
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. li and croft show significant improvement in retrieval for trec topics that are explicitly concerned with recent events."
"1. query-specific exponential re-ranking 2. temporally informed smoothing 3. temporally biased pseudo-relevance feedback approach 1. the chief difference between eq. 6 and the approach offered by li and croft is that eq. 6 allows us to consider not only when a document was published, but also the relationship between that publication time and the query at hand. under approach 1 we apply a more aggressive temporal factor if we have evidence that our query is indeed recency-bound. we accomplish this by incorporating query-specific information to estimate the exponential rate parameter r q to obtain the maximum a posteriori estimate, then applying this result in eq. 6."
"the two highlighted rows of table 5 show our baseline systems (ql and rm). thus the effectiveness of exp, bex, and tsql is gauged against ql and exrm and tsql are compared to rm. we refer to the ql and associated runs as term-based, referring to the runs based on relevance models as feedback runs. the starkest result in table 6 through table 8 is the difference in performance between recency and non-temporal queries. for recency queries, all three non-feedback models improved map for all datasets significantly. results for r-precision and ndcg were also promising for these runs with all models scoring higher than ql, though only occasionally at statistically significant levels. moreover, all three models were comparable on recency queries. the difference in effectiveness (on any of our three measures) between exp and bex or tsql was never observed to be statistically significant on the recency queries. however, the picture is quite different when we turn to the nontemporal queries. the method based on exponential document priors saw a statistically significant decline in map in comparison with the non-temporal ql for all tested non-temporal queries."
"all judgments for recency queries were gathered on december 5, 2010 so that workers would have suitable context for making their decisions (they were invited to consult the current twitter stream while making their relevance assessments)."
"though time has long played a role in ir, new problems such as microblog search change the nature of temporal retrieval. in this paper we proposed methods based on bayesian estimation for incorporating time into language modeling ir. in one approach (bex), we added a query-specific estimation procedure to the use of an exponential penalty for document age. we also used time to perform document-specific language model smoothing (tsql). finally, we used this method to induce temporally smoothed relevance models (tsrm). the methods we propose perform at state of the art effectiveness for recency queries, while showing more robustness than established methods when applied to nontemporal queries."
"where we have replaced  in eq. 3 with  t a quantity that depends on the time associated with document d. the older d is (i.e. the larger t d ), the larger  t ."
"in addition to recency queries, we might consider queries that prefer older documents. for certain precedence searches (e.g., patent search, trademark search, e-discovery) one might wish to know when certain concepts were first mentioned. the formalisms described here might be inverted to bias the results toward older documents. one challenge is to establish a useful reference point in the past that corresponds to the present in recency queries."
"for approach 1, our starting point is the hypothesis that queryspecific temporal information can help us arrive at an estimate of the rate parameter that is more appropriate than a one-size-fits all approach. to illustrate this point, we analyzed the mles obtained from the recency and non-temporal training queries in our microblog data. we found ̂ and that ̂ . the p-value on a one-sided t-test between the mle rate parameter estimates for recency and non-recency queries was 0.057. in other words, the top documents retrieved for recency queries tended to be newer than documents retrieved for nontemporal queries. this in turn would cause us to penalize older documents more strongly for recency queries, as compared to the penalty applied for non-temporal queries."
"our set of organs constitutes the x3dom scene shown in fig 2, which is comprised of the heart, lungs, liver, kidneys, pancreas, spleen, stomach, and ureters organs. each one of them was passed by the mesh simplification procedure provided through meshlab 2 software system to reduce their excessive number of polygons, avoiding however any undesired distortion and keeping their quality levels high. the presented bio-medical environment demonstrates how internal organs can be successfully identified inside a patient's body, even though each organ's shape, size and relative position is highly patient-specific and unique for every human being. the automated extraction of spatial information and its correlation with linguistic semantics is instantaneously done each time users click on an organ. the overall procedure is based on the artificial selection of only the appropriate relations, instead of deducting the entire set of possible relations between r-tree's records. moreover, a 10 percent offset has been also applied to each spatial relation formula to www.aetic.theiaer.org minimize boundaries' error rate and, at the same time, maximize the spatial accuracy of the resultset. in this way, records which have their bounding boxes overlapping each other by only a small percentage amount, are treated as disjoint objects and are granted the necessary directional relations. all the aforementioned features allow users to interact with an intangible information space to not only derive human organs without having a physical contact with them, but to also enhance their spatial reasoning ability with the visualization of the corresponding 3d r-tree instance."
"the most encouraging result comes from our temporally smoothed query likelihood model (tsql). tsql saw a statistically significant decline in performance only once in our experiments (with respect to r-precision on the la/ft data. in all other cases, the decline in performance incurred by applying temporal smoothing to non-temporal queries was negligible."
"the adoption of web3d technologies goes hand in hand with the native support of real-time rendering offered by most modern browsers, allowing an on-the-fly conversion of countless images and video that ship with x3dom, obj and other textual formats of 3d graphics [cit] . so, the question that quickly arises is whether it is feasible to facilitate the searching and retrieval of the information enclosed inside such files. at first sight, it is obvious that several parts of these files comply with a technical standard, specification sheet or api, allowing web users to search for specific features like shapes, colours, urls, animations, etc. [cit] yet, the majority of users are interested in the nontechnical aspects of them, where there is available a vast semantic content about the 3d scene (e.g. what this scene represents, where is an object located in relation to its surroundings). however, this requirement comes in contradiction with today's relatively poor annotated scenes, due to the fact that their objects do not possess the appropriate metadata to support such annotations. additionally, even namespaces and objects definitions do not have an agreed terminology or adhere to set of consistency rules. thus, a research branch in 3d graphics deals with the problem of object identification, which is mainly based on techniques that take advantage of objects' shape and volume data [cit] . meanwhile, designers and producers provide some primitive annotation in their scenes, which is usually limited to a textual description or object definition of a 3d model. on the other hand, most advanced solutions may apply several identification rules that can be perceived as semantic attributes based on human ranking data, but such methodologies have proven to be time-consuming and errorprone [cit] ."
"where  is a mixing parameter that plays a role similar to  in eq. 3. a large value of  gives heavy weight to the feedback terms, while a small  is more conservative, retaining the influence of the original query. (for the rm and exrm approaches, use eq. 16 to estimate word probabilities under the relevance model). this is the relevance feedback approach used by the indri search engine, for instance. of course, in eq. 16  is another tunable parameter. as in our temporally-smoothed document language models, we propose letting the timestamps of retrieved documents guide the smoothing process, giving more weight to feedback terms when the feedback query retrieves newer documents."
"effectively estimating language models for retrieval requires smoothing, which we accomplish using eq. 3. this requires choosing a smoothing parameter . typically this value is constant across all documents. however, in the context of recency queries it is plausible that we should smooth older documents' models more aggressively than newer documents. we elaborate on this intuition below. but the thrust of this approach is that each document's model is smoothed more aggressively for documents that are further from the target time associated with the query."
"in order to avoid the generation of numerous spatial relations, a small only subset -namely a dining table, a colorful orb and a shelf -of its 3d objects was selected to demonstrate r-tree's semantical capabilities. these three objects were indexed into a 3d r-tree instance, where our computational model was applied to extract the spatial relations shown in the same figure. at first, each object is found to satisfy the disjoint topological relation, initiating in this way the implication of their directional relations. the dining table is found to be on the right side of the colorful orb and the shelf, while at the same time, it is placed in front of and below both of them. the inverse directional relationships, left, behind and above, are automatically applied with respect to the colourful orb and shelf without proceeding to extra reasoning procedures. on the other hand, the colourful orb and shelf share a common boundary region, indicating a touch topological relation which also denotes that the colorful orb is over the shelf. finally, the latter object is found to be below the former and the spatial reasoning terminates since no other directional relation pattern is satisfied. all these automatically implicated relations were then forwarded through dec-o to the owl ontology representing this interior space, filling in its corresponding axioms for the semantic representation of 3d space with linguistic predicates. in fig. 1 the implemented computational model can be also applied in domains of medical evaluation for the provision of a different perspective to their relative vr environments. for the purposes of this demo, we took advantage of human body polygon mesh 3d data that correspond to full-body mri images extracted from patients. these data are publicly available from bodyparts3d [cit], a dictionary-type database for anatomy in which anatomical concepts are represented by segments of a 3d whole-body model of an adult human male. it is worthwhile to mention that this library encompasses with the morphological and geometrical knowledge in anatomy, complying at the same time with the fma (foundational model of anatomy) ontological representation model."
"using eq. 2 with estimates from eq. 3 for ad hoc retrieval has shown state-of-the art effectiveness, while easily admitting alterations to the retrieval process."
"contains relation for the first parallelepiped. this kind of relation is also complementary to the encasement of intersecting boundaries, unifying in this way also the covers spatial predicate for the same parallelepiped. finally, there is one last spatial predicate left for those mbb pairs that do not satisfy any of the aforementioned topological relations. this predicate refers to the overlap relation, denoting that the interior regions of these mbbs are partially intersecting in 3d space. the aforementioned series of relationships along with their relative allocation formulas for rectangular parallelepipeds recta (blue) and rectb (red) are displayed in table 1 ."
"these results show two things: for recency queries, applying an exponential prior improves retrieval on our training recency queries, whereas for non-temporal queries, the exponential prior leads to a stark decrease in performance. the decline in performance for non-temporal queries is not surprising, as the temporal re-ranking dilutes the influence of textual similarity in favor of a temporal factor that is presumably not important for these queries."
"in addition to the work that we have cited in the previous discussion, a great deal of interest has guided research into temporal dimensions of ir. time, researchers have found, enters into retrieval in several ways-shaping the notion of relevance, constituting a valuable source of evidence in ranking, and helping developers identify suitable algorithms for particular settings."
"prior to building our test queries and relevance judgments, we compiled a set of training queries and judgments using the same approach described above, except that the authors themselves"
"in a context such as twitter, where relevance often has a recency component, rewarding newer documents during retrieval has obvious appeal. on the other hand, a time-only ranking as used by twitter search fails to capture differences in tweets' relevance to the query. using an exponential distribution to accomplish a blending of time and language model (eq. 3) has been shown to be effective in previous research. however, the typical approach to using the exponential distribution as a document prior by definition ignores query-specific concerns. we hypothesize that the aggressiveness of the exponential penalty should hinge on the extent to which a particular query is sensitive to recency."
"temporal smoothing appears to be an effective way to add time to our consideration during document ranking. however, in this paper we have only pursued a simple application of this idea. in future work it will be of interest to apply the ideas introduced here to other smoothing methods such as bayesian smoothing with dirichlet priors. in this case, the temporal influence would guide retrieval in a more complex way than it does using jelinekmercer. this would be especially interesting due to the different nature of the document types we have analyzed. tweets and news articles obviously differ with respect to length, and thus word frequency."
"however, we propose an alternate approach to incorporating time into relevance models. in many applications, the relevance model of eq. 15 is interpolated with the original query model before the second retrieval via:"
"where  and  are the rate and shape parameters of the geometric distribution, respectively. with this formalization in place we may use in eq. 8. the summation over the observed document times in the denominator of eq. 10 should reduce the temporal influence for non-temporally bound queries. we refer to this approach as bayesian exponential ranking (bex)."
"this paper's treatment of temporal factors in ir is based on the language modeling approach to document retrieval [cit] . in particular, we rely on the query likelihood model. given a query q and a document d we derive a score for d against q that is proportional to the probability that the (multinomial) language model that generated d also generated q:"
"if we momentarily assume that the prior probability distribution is over documents is uniform, we may rank documents in decreasing order of the query's likelihood of generation by the model that generated d. assuming that this model is a multinomial over words in the indexing language, we have the ranking function:"
"several research works in the recent past developed innovative mechanisms for the spatial annotation of 3d content, using either a hybrid metadata acquisition framework [cit] for the management of such annotations in online scenes, or an ontological framework [cit] composed of various owl-dl properties for mapping spatial correlations between 3d objects of indoor environments. however, both approaches lacked the ability to provide an automated spatial reasoning mechanism, since their primary target was the sufficient quantitative and qualitative annotation of a 3d space. in this use case, we deal with this inefficiency by granting to dec-o's object properties the spatial relations estimated by our computational model. this supplement is done automatically with the assistance of apache jena 1 without any input or further action from the enduser of the application. for the presentation purposes of this scenario, we have created and populated with various 3d objects the realistically decorated indoor scene shown in fig. 1. www.aetic.theiaer.org"
where the sum in the numerator of eq. 17 is taken over all k feedback documents for simplicity we refer to the numerator of eq. 16 as ( ̇ ). this allows us to define the maximum a posteriori estimate of :
"the work presented in this paper builds on prior attempts to inject time into queries. we've taken another step toward making time a first-class citizen of the retrieval space, but much remains to be done. the evaluation framework we adopted might be improved by removing some of the arbitrary cutoff parameters, for example."
"the unremitting evolution of web3d technologies over the last decade has led many developers of high-interest application areas to replace traditional 2d presentation formats with 3d graphics content. this continuous progress is reflected on a daily basis by the increasing engagement rates of virtual worlds with science, business and education domains [cit] . typical examples are found in numerous internet applications that promote such things as automobiles, electrical appliances and industrial furniture, while others provide complete interior design solutions for imprinting a realworld room space or creating a new virtual space from scratch. moreover, web3d movement will spread even further thanks to the latest changes in vr technology, which aim at turning an initially expensive technology into a series of affordable commercial products for the masses. even though these applications come from different domains, they tend to share common practices in the design of their 3d models, where area experts sketch such models using appropriate software tools. these tools ultimately output the necessary resources which will be integrated into the corresponding application or webpage. at the same time, the prevalence of search engines combined with the structure of current web, instruct users to employ (mainly) textural descriptions as search criteria."
"we report three effectiveness metrics, mean average precision (map), r-precision (rprec), and normalized discounted cumulative gain (ndcg) to show the effect of different models with respect to both recall-and precision-based considerations."
"which is identical to the standard query likelihood model, but with the addition of the probability of observing a time t d given the query q. eq. 6 gives a flexible way to add temporal information to document ranking."
"two main results emerge from the data reported above: bex and tsql improve recency query performance to nearly the same extent as exp, and bex and (especially) tsql are more robust against failure than exp when applied to non-temporal queries."
"in this paper, we present an object identification algorithm which relies on the spatial characteristics of a scene and the relative position of its objects in the world. a 3d scene does not simply represent the underlying environment, but in most cases, contains an abstract information layer with additional semantics about the environment being described and its objects. however, even though this kind of semantic conceptualization has been successfully addressed with the usage of various technologies and information management systems [cit], an efficient methodology for the automated extraction of spatial relationships is still absent. typical use cases of such relationships can attribute more realistic prospects to the semantic representation of scenes; since they are based on human perception and cognition (e.g. a is in front of b, below c, and on the left side of d). in this way, a formal representation of natural language's concepts not only enhances the spatial awareness of the user, but also improves their ability to understand the spatial relations between objects. a first attempt to extensively annotate such scenes and move from a cad-oriented to a human-oriented spatial environment took place with an artificial intelligence framework [cit] which did not support a spontaneous mechanism for spatial relations. taking it a step further, this work deals with the challenge of providing a web-based and platform-independent algorithm for the automated extraction of spatial semantics from 3d scenes, which could be used to answer regional questions such as \"what model is in front of, behind, on the left of, on the right of, under, over, another model\" or \"where am i in the virtual world in relation to a model\"."
"making the simplifying assumption that the temporal relevance of d does not depend on the document's content, w, we drop w from the joint probability in eq. 5, giving:"
"the rest of this paper is structured as follows. in section 2 we address relevant research works on spatial semantics and those that contributed to the implementation of the extraction mechanism. section 3 presents the available spatial relations and describes step-by-step their application in the underlying computational model, while section 4 makes use of this model to retrieve spatial information from various use cases. finally, section 5 presents our conclusions."
"a common way to use relevance feedback in language modeling ir relies on lavrenko and croft's relevance model formalization [cit] . if our query contains n tokens the probability of a word w under a relevance model is ( ). estimating this conditional probability requires us first to estimate the joint probability ( ). for purposes of relevance feedback, the estimation is typically carried out only over the top k documents found during an initial retrieval, giving:"
"at this point, all indexed objects of a 3d r-tree instance will have been attributed with a set of topological relationships, terminating the reasoning process for evenly arranged or overlapping records. however, an additional level of spatial reasoning is applied for disjoint mbbs, since their spatial representation can be further enhanced with the usage of the directional relations described below."
of special interest in figure 3 is the behavior of the temporal smoothing model. we can see that while the two methods based on the exponential distribution are highly sensitive to their
"given this scenario we may understand  t as the parameter of a binomial distribution, where a -success‖ corresponds to finding that d is old. to estimate  t we have the maximum likelihood estimator:"
"once more, boundary representation methodologies were established in order to deduct the appropriate directional relations for each indexed object. however, a 3d r-tree instance with thousands of objects generates an exponential number of mbb relationships, resulting in a small but considerable increase in the overall runtime. for this reason, the presented computational model takes advantage of web worker scripts [cit] due to its unhindered operation in the background, following the trends of parallel computing on the web [cit] . doing so, ensures that the x3dom's presentation layer remains independent of the underlying 3d r-tree structure, allowing end-users to freely interact with the rest of the provided platform. last but not least, such scripts are also capable of exploiting multicore machines in a more efficient way than the classic javascript programming methodologies, guaranteeing their hardware scalability ."
"today, numerous applications come with an integrated set of topological operators for the deduction of spatial relationships between their datasets. the functionality of such operators complies with the principles set by various topological models, such as the de-9im, rcc or ogc www.aetic.theiaer.org spatial standards. among them, de-9im remains the most widely used standard for building information models and spatially indexing dbms [cit] by defining three different but interconnected geometric regions (interior, exterior and boundary) for each pair of objects which are validated against a set of spatial predicates. these predicates are based on human spatial cognition [cit] and have been proved quite practical and flexible in 1d and 2d applications. in a first attempt to process topological relations in r-tree variants [cit], all possible relations between two-dimensional mbrs were extracted and tested by complex queries, but today, octrees implementations [cit] are supernumerary compared to r-trees in aspects of spatial relations and boundary representation methodologies. premised on such findings, our previous work [cit] aimed to contribute in this area by presenting a uniform three-dimensional topological classification mechanism for r-trees in 3d space."
"most previous studies aim to minimize the total number of shifts for nontarget containers at each container port and therefore to minimize the turnaround time of a vessel. by field investigation, however, we observe that the shifting fee is also an important consideration, and a key point is that the unit shifting fee varies vastly at different container ports. for example, the unit shifting fee for one container move at ningbo-zhoushan port is 49.5 rmb, while it is 100 rmb at guangzhou port. the observation motivates us to investigate the stowage planning problem such that the total shifting fee other than the total number of shifts for a vessel is minimized in a multiple port transportation route. as the containers in each bay of the vessel are individually handled at each port, we consider the stowage planning within a bay on the vessel. we claim that the stowage planning problem under consideration is np-hard since its counterpart with uniform unit shifting fee, that is, the problem to minimize the number of shifts, is 2 scientific programming already np-hard [cit] . we have not found any related work on total shifting fee minimization for the case with nonuniform unit shifting fee. aiming at this problem, we establish an mip model and design an efficient algorithm to produce good stowage planning solutions."
"port. [cit] study the stowage planning problem and propose an approach that generates a near optimal plan for a large container vessel within a few minutes. the approach solves the problem in two steps. in the first step it decomposes the vessel into master bays and assigns containers to the master bays. in the second step, the containers in each master bay are further assigned to specific slots of the bay."
"in the world seaborne trade, most goods are transported by containerships. containership maritime transports have occupied the vast majority of the maritime transport industry, among which container trade is now becoming the fastestgrowing freight segment (see [cit] )."
"if a nontarget container is loaded on the top of a target container in the same stack at port . after finishing loading/unloading operations, container does not change its location, but container must be shifted at the port:"
"port. [cit] deal with a stowage plan for container ships to minimize the number of shifts and first present a binary linear programming formulation to find optimal solutions for stowage planning. due to many binary variables and constraints, they develop the so-called suspensory heuristic procedure."
"we determine the chromosomal gene segments by the number of ports excluding the last port at which all the remaining containers are unloaded and no shifting fee occurs. that is, there are − 1 gene segments. each chromosome is a solution of the stowage planning."
"nontarget container is loaded on the top of nontarget container in one stack at port . after finishing loading/unloading operations, although container has not changed its location, it must be shifted once at the port:"
"each slot (, ) of the bay is stowed with at most one container at any time, and each container is stowed in a single slot during its transportation:"
we present a distribution mechanism to guarantee that each generated solution is feasible. the mechanism is described as follows. the individual feasibility is verified via the weight limitation of the containers in each stack and the height constraint of the stack. we repeatedly produce a number of individuals and discard the infeasible individuals among them until the number of feasible individuals satisfies the required specific scale.
"in this paper, we study the stowage planning of a single bay of a vessel in multiple container ports, focus on the case with nonuniform-shifting fees, and establish a mixed integer programming model. the genetic algorithm and improved genetic algorithm are proposed to solve the model. experiment results show the validity of the model and the proposed genetic algorithms. what is more, by comparing the stowage planning between the case with nonuniform shifting fee and the case with uniform shifting fee, we conclude that a stowage planning in the former case may result in more container shifts but less shifting fees than in the latter case. since load balance of a vessel is also important in shifting activities and it can be affected by several factors, such as the gravity of the vessel and container types. considering a variety of constraints about stowage planning is the next focus of our research."
"the rest of this work is organized as follows. section 2 gives a brief literature review of related works, and section 3 describes the considered problem formally. in section 4, we present the mip mathematical formulation, and in section 5 we propose a genetic algorithm for the problem. numerical results are given in section 6, and finally we conclude the paper in section 7."
"in this section we first give basic notations and then present the objective function and constraints of the mip model., and the objective is to minimize the total shifting fee in all the ports:"
"we repeatedly produce the feasible individuals until they satisfy the required constraints, but the chromosome length of the first algorithm is too long and the effect of cross-mutation operation on improving the fitness of the initial population is affected. so, the main result is derived from the solution of the initial population generated. in this section, based on the original genetic algorithm, we propose an improved genetic algorithm. we called the original algorithm gai and the improved one gaii. the improved chromosome is also composed of − 1 gene segments, the length of each segment is no longer the total number of containers in the route, but the total number of container boxes in the bay. the improved chromosome structure is shown in figure 6 . in the chromosome, there are four container boxes in the bay, and the third container is loaded in fourth container box at port 1. by the chromosome, the third container is unloaded at port 2. note that if there is no container loaded in a container box, the chromosome gene value is equal to 0. in each gene segment, the containers will be loaded according to their weight at every stack. we repeatedly produce the feasible individuals until satisfying the required scale. besides, fitness function and cross-mutation operations are the same as in gai."
"comparison of gai and gaii. as we described in previous section, the stowage plan can be produced by cplex on small-scale instances. in order to measure the solution quality obtained by gai and gaii, we first solve the problem by cplex on small-scale instances. the ga simulation is set up to 100 populations and performed with the limit of 100 generations, coding with matlab language on a computer (inter core i5 cpu, 3.00 ghz; memory, 4 g). the instances used in the experiment are randomly generated. we assume that the shipping line contains 8 ports and the unit shifting fees in each port are 30, 15, 40, 50, 80, 50, 70, and 25 (rmb), respectively. with different capacities of the bay (, ) and the number of loaded containers in the route, we test both small and large instances. the total shifting fees and cpu running times in seconds of the solutions produced by cplex, gai, and gaii, respectively, on the instances are shown in table 2 ."
"we calculate the fitness value of each individual based on the value of the objective function. with the minimization objective, the smaller the objective function value, the better the fitness, and it is more likely to be selected as parents. calculations of the fitness values are based on the slot position of the containers in the bay."
"an efficient stowage planning is one of the most important factors in saving the transportation cost of shipping companies. one vessel may visit several container ports during a voyage, and containers are loaded onto the vessel from some upstream ports and later unloaded from the vessel at downstream ports. if a target container to be unloaded from the vessel is not stowed on the top of a stack, it incurs reshuffle operations as the access to containers follows the top-to-bottom order for any stack. all the involved containers piled above the target one have to be removed temporarily and later reloaded back to the stack after the target container has been unloaded. the unloading/reloading operations of the involved nontarget containers cause extra shifting fees as well as time consumption. an efficient stowage solution to the vessel may greatly reduce such shifting fees in a transportation route."
"for a slightly larger instance where there are five columns by six layers in the bay, 55 target containers are involved in the four ports. the computation time of the solver rises exponentially as the input size increases. together with the np-hardness of the considered problem, we propose a genetic algorithm to solve larger instances."
"from the numerical results in table 2, cplex can solve the small instances with up to 147 containers efficiently but it cannot output solutions for large instances with 196 or more containers (it is denoted by \"/\"). the gai can solve the largescale instances, while the solutions are of low qualities. gaii can achieve better load planning and especially obtains an optimal solution for the instance with 196 containers. it can effectively solve this kind of stowage problem in a short time, which helps to save operational cost for the shipping company during the route."
"in the instance, there are totally 20 target containers to be loaded and unloaded in the 4 ports. the detailed data of the containers, including their indices, weights, departure, and destination ports, are listed in table 1 . for example, container 1 with weight 1 is to be loaded at port 1 and unloaded at port 3."
"the slot locations of containers in the bay may change only at ports during loading/unloading operations, but not on the way between any two ports. that is, the slot status at the time when the vessel is leaving some port is the same as when the vessel arrives the next port. especially at the last port, all the remaining containers in the bay are unloaded: if a nontarget container changes its slot location in the bay at port, then it must be shifted once at the port:"
"as we known, the genetic algorithm has been extensively applied in various applications due to its efficient performance. its basic procedure is shown in figure 3 . in this paper, we adopt the genetic algorithm to solve large instances of the stowage planning problem. we describe it in detail in the following."
"there are several typical ways for crossover operation such as single-point crossover, twopoint crossover, multipoint crossover, and uniform crossover. in this work, we adopt two-point crossover for crossover operation as follows. firstly two different crossover points in a gene segment are randomly generated for two chromosomes selected for crossover operation and the value of the crossover points did not exceed the length of chromosome. the digits between the crossover points in the two chromosomes are then exchanged, while keeping the other parts of each chromosome unchanged. after the exchange two new chromosomes are generated (see figure 5 for an illustration). the mutation operation of each chromosome occurs randomly with a predetermined probability, which is called the mutation rate. in the mutation, two mutation positions in the chromosome are randomly selected and the two corresponding digits are exchanged, generating a new chromosome. the feasibility of the new chromosome is to be verified in the next step of the algorithm."
"profile-based approaches use metadata associated with the user's account or profile. in twitter, such features include real name, description, location, followers and friends. for instance, the simplest profile-based method assigns gender class based on a dictionary look-up of the user's first-name, see [cit] . an alternative approach is to infer a user's gender based on profile colour preference [cit] . when it comes to age inference, profile-based features tend not to be used alone, but combined with content-based features."
where α is a normalization factor and n is the number of tweets that belong to him/her. for each user uj we define a confidence value confj given by the following formula:
"we recruited 22 participants (15 males and 7 females), of which 12 phd students, 7 researchers and 3 master students in the department of computer science both of brunel university and milanobicocca university. all of them were comfortable with computers and familiar with twitter. a dataset for trial was collected using an experimental version of chorus tweetcatcher (tcd) 1 that is able to collect a table of twitter users where each of them has all the attributes listed in table 7 the dataset obtained through chorus tcd was composed of 50 twitter users. the test participants had to inspect 25 gender unclassified users and 21 age unclassified users belonging to this dataset."
"future work of this study will concern how to exploit the knowledge provided by the end-users' refinement. for instance, we could use the inspected twitter users as new instances in the training set obtaining an active learning model able to improve itself each time that new twitter users are refined by tweetclass end-users. moreover, another future work could relate to incorporate in the tool other automatic techniques able to increase the performance of the existing demographic attribute classification or new automatic techniques to identify other demographic attribute such as profession, marital status and so on. since, after the end of the study, a new dataset, with both gender and age label, became available [cit], we want to expand our experiments over this large dataset for a better evaluation of our approach."
"the graph presented in figure 6 is particularly useful for the user to establish an optimal trade-off between accuracy and sample size. it can be used by the end-user for understanding which is the best confidence level threshold to choose based on his/her needs (a social scientist could prefer a highest level of error and refine a lowest number of instances or viceversa). for supporting the end user in the choice of the confidence level threshold, during the age class refinement step, we incorporated the graph reported in figure 6 in the tool interface. this graph has been obtained over the dataset where 55 tweets are used for identifying the age class of each user. it shows the variation of the number of instances that required a refinement (classified with lower confidence level than a certain threshold) and the variation of the error rate with respect to the variation of confidence level. using this graph the end-user can follow two possible strategies for choosing the best threshold: one based on error rate requirements and one based on the size of the user sample required. in the summary panel is also possible to see the sample composition in each step of the process. the information displayed is: number of initial users, number of female, male and unknown classified users during the gender inference phase, number of younger and older users classified with a certain confidence level. it is also possible to see the proportion of age classified users respect to gender. the rectangle below \"gender inference statistic\" label represents percentage of twitter users, belonging to the initial dataset, that are gender classified as female (fuchsia bar), male (blue bar) and unknown (gray bar). while the rectangle below \"age inference statistic\" label represents also the proportion of females and males that are age classified with a confidence level higher than the threshold (green bar) and with a confidence level lower than the threshold (black bar)."
"during the experiments we investigated the best method to identify the age class, among a set of popular classification methods: svm, naive bayes, multinomial naive bayes and k nearest neighbours. moreover, we were interested in understanding if gender attribute value influenced age classification. for this reason, we performed several experiments using different dataset compositions:"
"the gender of each twitter user is determined using the user's first-name that appears in the profile. identifying a person's gender from their name is not always straightforward. for instance, users may use pseudonyms or transpose their surname and first-name. the latter case becomes more problematic if the user's surname is equivalent to a common first-name (e.g. michael stewart)."
"usually, in all the experiments, the number of incorrectly female classified instances was higher than the male one. inspecting these misclassified instances we found that in several cases female users do not write their name, but acclaim related to some male celebrity, such as \"i love you ashton\". after all these observations, since we want to assure to social scientist the best reliability, we decided to use the method g2 to infer gender attribute of each user in tweetclass tool."
"there is a lack of both gender and age labeled datasets in the public domain. given this, we collected a new dataset using twitter api. the absence of suitable datasets is a result of two key factors. first, to gather private information such as gender and age of a user is a resource intensive task. second, issues relating to privacy and twitter data user terms limit data diffusion. indeed, datasets of twitter content or an api that returns twitter content can be downloaded only if they contain or return ids (tweet ids or user ids)."
"once the gender class is assigned, the process continues to the age inference step. during age inference phase the twitter users are classified into two major demographic sets: users below 30 (younger) and users above 30 (older)."
"comparing the efficacy of these and other methods is not straightforward because of the tendency to use different datasets for training and testing. moreover, different studies tend to vary in the intervals used for age classes. despite these problems, it is possible to draw some key conclusions:"
the confidence level can be interpreted as the probability of how sure end-user can be regarding a certain classification. all user instances classified with a confidence level lower than a certain threshold are displayed to end-user.
"during the two refinement phases, we noticed that the attention of the experts was captured by the images, while the textual information was mainly ignored."
"moreover, we analysed performance changes considering either all words contained in the user name field or just the first word that appeared in it. therefore we conducted experiments with the following resulting configurations:"
step task 1 load new tweet dataset 2 start gender inference process 3 do gender refinement phase and set the gender for some authors 4 start age class inference phase 5
"in the post-study survey, the participants were asked to answer questions about easiness of use, easiness of learning, easiness of navigation and easiness of information understanding. their overall satisfaction and confidence toward the interface was high. the main qualitative feedback was to increase the size of tweet windows in order to facilitate their reading."
2. both complained about the absence of options for selecting the confidence threshold that split the entire set of twitter users into instances to refine or not.
"a set of experiments was conducted using several machine learning techniques, aimed at inferring the age class of each single tweet, over the three dataset. the svm obtained the best results among the considered approaches as shown in table 2 experiment outcomes reported in table 3 show the different performances achieved, over dataset a, dataset b and dataset c, using svm with different feature set: only unigram features, only stylistic features and both. note that, in all the experiments, the highest performances were obtained considering all features, while the lowest were obtained considering only unigram features."
"in the age refinement screen they suggested we simplify the top part of the screen designed to set the new age class or gender of a twitter user, but they appreciated the facility to modify gender class at this stage. moreover, they suggested to explain more clearly the meaning of word \"confidence\". in this screen they also found another problem related to going back in the process using buttons."
"using this method we were able to collect a dataset of 386 users. it is composed of 62 younger male, 88 older male, 152 younger female and 84 older female. for each of these users we retrieved between 20 and 200 tweets from their timeline. in this way, we obtained a tweet dataset where 8368 tweets belong to younger male, 12868 to older male, 21288 to younger female and 12002 to older female authors. to create a balanced dataset for both gender and age class attributes, we randomly sampled the dataset retrieving for each gender-age class a number of tweets equal to 8368 (minimum number of tweet in gender-age class combination). using the collected dataset we conducted the following experiments."
"social scientists need a sample of users with specific demographic attribute values and it is very important that these values match the real ones. so, the first configuration seems to be more desirable than the second one. indeed, in configuration 1 the users with names that are considered mostly male or mostly female are classified as \"unknown\". in this way, social scientists can be assured that if a user is classified as \"male\" or \"female\", he/she belongs to this class with high reliability. nonetheless, we decided to analyse both the situations to obtain a much deeper investigation."
1. both participants suggested that a continuous update about the age and gender composition of the current set of twitter users should be available. in this way the end-users can decide with more confidence about the number of instances to refine.
"given this problem, we propose a semi-automatic framework to facilitate and accelerate the human judgment process. the framework relies as much as possible on automatic techniques, essential for handling the huge amount of data that originates from sm, only requiring human intervention for cases that cannot be classified with high confidence by the algorithms. we incorporate this approach into a proof-of-concept tool, called tweetclass, designed to support researchers in the identification of demographic attributes of a twitter user sample. in order to evaluate the capabilities of our tool, our experiments include an extensive analysis of the interface design. moreover, even if our focus is not on the classification method, we investigated the best approach among few popular techniques for facilitating the refinement process for the end-user."
"before beginning the cognitive walkthrough, the participants received a 10 minute of presentation about the tool, which presented the aim and all basic conceptual steps required to obtain it. the presentation described some background to the work and explained the limitations of automatic methods, but no reference to the interface was made at this stage. in this way the participants were not influenced in how to achieve the requested goals. we asked participants to attempt to reach the following two goals:"
"the aim of the interface design is to support the users with all instances that are difficult to classify automatically. any instance classified as \"unknown\" can be processed manually by the endusers through the refinement step. the interface is composed of two main areas: the process timeline viewer (a) and the data viewer (b) on the left and on the right part of figure 4, respectively."
"as reported in figure 2, we developed a crawler, based on the streaming search api provided by the twitter site, able to filter only particular tweets from the stream. we filtered all the tweets containing the word \"birthday\" and identifying the owner's birthday. at this point, we derived the age of each user using the regular expressions. we filtered all tweets including two consecutively digits that were not part of fractions, urls, usernames, hours, dates and three or more-digit numbers. in fact, with high probability, the remaining two digit numbers represent the age. we eliminated all users with follower number greater than 5000 because these users were likely to be celebrities or big companies that would not be representative of behaviour twitter population."
"the results of these experiments are reported in table 1 . note that we were interested in computing only the evaluation measures over the instances classified as \"male\" or \"female\". in fact, due to the huge amount of twitter users, social scientists are likely to be less worried about loss of some users, if this results in a significant improvement in classification accuracy. so, the instances considered \"unknown\" were just discarded and we focused our attention only on male and female classified instances. we observed that the method g2 achieves the best result. in this case, the error rate is only 3%. instead, the worse performances were obtained with method g1. the difference between this method and the other ones was the part of user name field used to derive gender. as already explained, in method g1 all name field was considered, while in method g2 and in method g3 only the first word of name field was used. since typically twitter users fill their name field writing first name followed by surname, taking into account the entire name field could be a problem. indeed, several surnames are also first names. in method g1, this affects the gender classification task increasing the number of gender misclassified instances. for example, a possible name field could be \"rylee ross\" (first name + surname). in this situation, considering all the name field (method g1), user would be misclassified as male. indeed, while \"rylee\" is a mostly female name,\" ross\" can be also used as first name, and, in particular as a male name leading to an overall classification as male user."
"in addition to n-grams, stylistic features have also been studied. for instance, several approaches describe methods to derive gender and/or age based on the usage of smileys, abbreviations, punctuation, possessive bigrams, repeated letters, pronouns, hashtags and other grammatical features [cit] ."
"in the gender refinement screen, the experts clicked on the \"next\" button for selecting other users to refine. actually, they obtained an unexpected effect: the age class identification started. moreover, for the same screen, one of the cognitive walkthrough participant complained about the absence of label indicating which kind of images were shown in the user panel."
"in contrast, hybrid approaches leverage profile data for enhancing the accuracy of results obtained using content-based features. notable examples of the hybrid approach include [cit] ."
"tweetclass combines automatic classification with human interaction. the use of automatic methods is essential to manage the huge amount of data that originates from sm, however a reliable and accurate automatic classification may not be possible for all cases and human intervention may be required. indeed in some cases determining a user's gender or age might be a simple task for a human, based on examination of a photograph or profile description, yet the same task is very difficult to reliably achieve using automatic methods. moreover, for a given a twitter user, humans are able to explore additional information. for instance, they can explore the user's digital footprint on the web. if the name is not meaningful, they can see profile images from other platforms, explore sm relationships and so on. just reading extracts from a user's timeline, can be sufficient to discover nuanced clues that might not be found by automatic methods amongst a much larger corpus of data."
"once we discovered the best configuration to obtain the age class of each single tweet, we decided to study the performances from the user point of view. the experimental campaign was conducted using all the datasets (dataset a, dataset b and dataset c). table 4 shows the outcomes. so, in order to obtain the best outcomes, we table 4 : user-level accuracy performances achieved using svm in combination with the entire feature set."
"to understand if the interface that we designed is intuitive and easy to use for typical end-users, we decided to conduct a formal evaluation using a method called cognitive walkthrough. the cognitive walkthrough entails an usability analyst stepping through the cognitive tasks that a user must carry out in interacting with technology. the aim of a walkthrough is to evaluate the design of a user interface, with special attention to how well the interface supports \"exploratory learning\", i.e., first-time use without formal training. in brief, users start with a goal and some sort of plan(task sequence) as how to achieve the goal. users then look for apparently relevant actions, activate the most probable option, consider the system response (system feedback) and decide whether the right effect has been achieved."
"for our cognitive walkthrough we recruited 2 domain experts (both male with age between 30 and 35). both are social scientists who work in a major market research organization in the uk that conducts surveys for a wide range of major clients, including commercial and government organisations. they are comfortable with computers and very familiar with twitter social media."
"before beginning the study, participants received 10 minute training on final interface. the training session consisted of a brief explanation of the tool's purpose and basic concepts, and a short demonstration of the interface and detailed instruction on the usage of the interface. the tutorial was administered by the same person following a basic script (explanations and demonstrations). in addition to demonstrating the features of the interfaces, the administrator explained basic strategies to complete the tasks (for example, comparing the additional information of the considered user to assign gender class)."
"social scientists, policy makers and marketers are keen to find ways to mine social media (sm) data in order to gain insights into public attitudes and opinion. traditional survey research methods (questionnaires, interviews etc.) are becoming less attractive, due to falling response rates and increasing costs [cit] . at the same time, members of potential target populations are increasingly sharing their views, for free, on sm platforms such as twitter and facebook. for this reason, mining sm is seen by many as a key part of the next generation in survey research methods [cit] . there are several features that make sm based research attractive. that are particularly attractive. first large datasets can be collected relapermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. tively cheaply and are already digitally encoded. second, sm users tend to comment in a responsive, ad hoc manner, allowing a more timely polling of opinion on current events, in comparison to 'designed' research. third, despite being public forums, the perceived anonymity of sm platforms means that views expressed online may often be more honest and expressive than those collected using designed instruments [cit] ."
"the rest of the paper is structured as follows. in section 2 we present previous work related to demographic attribute inference and semi-automatic approaches to classification. in section 3 we explain the rationale behind combining automatic and interactive methods and how we combine them. section 4 describes how we collected our dataset, followed by section 5 which describes the experiments carried out to find the best approach for automatically identifying age and gender class. finally, in section 6, we focus our attention on an essential part of the work, the interface design and the method employed for evaluating it. in particular, here the description of the first prototype is followed by the description of his evaluation that highlighted several problem addressed with the development of a second prototype interface."
"we model the tweet contents, for each gender class, using a feature vector approach. unigram features are selected after a preprocessing phase. chi-square feature selection is used to reduce the number of attributes and to take into account only the most predictive words. we derived the remaining stylistic features additionally using a pos-tagging procedure and partially using regular expressions, for instance the presence of stretched word (hellooo, sunny). since an user can write more than one tweet, user's age class is identified taking into account a sample of their recent posts. single tweets are then classified independently by using classification models such svm or naive bayes. from this classification phase we obtain the label probability distribution and we assign to the tweet the label with the maximum likelihood. once each single tweet of the user is classified the results are aggregated in order to obtain an overall age classification probability for each user. the probabilities for a user to belong to younger and older class are computed using the following formulas:"
"tasks did not have a time limit. once the participants completed the refinement of gender, they repeated the same procedure for assigning age to those user falling beneath the specified threshold."
"through a traditional \"time and errors\" usability test, three dependent variables were collected for both gender and age class refinement tasks: completion time, inter-rater agreement and success rate. task completion time was measured by recording the time when users clicked on a row related to a user until they made a final decision by selecting one of the possible classes. inter-rater agreement is the degree of agreement among two or more evaluators. it describes how frequently they assign the exact same rating (if all give the same rating, they are in agreement) and it gives a score of how much homogeneity, or consensus, there is in the ratings given by judges. it is useful in refining the tools given to human judges, for example by determining if a particular scale is appropriate for measuring a particular variable. success rate is the percentage of users' correct classification decision."
"on the basis of this feedback, a second interface prototype was designed. a key feature in the new interface is an additional visualization component: the summary panel (see figure 5) . it is the main improvement made on the previous version and displays a breakdown of labelled and unlabelled user cases. a new combo box was added which allows the end-user to set the confidence threshold for user refinement. having chosen the threshold, the tool presents all instances that require examination (all instances with a confidence level lower than threshold level)."
"the effect of size variation in the tweet set available for each author to infer his/her age class was also analyzed. in particular, the experiments shown that the variation of accuracy for each twitter user was reduced as the number of tweets available decreased (see figure 3 ). to judge age class from just one tweet is a complex task. therefore, to address this difficulty it is useful to increase the number of tweets examined for each twitter user. in order to balance effectiveness and efficacy of age classification, in the tool a set of 55 tweets was considered for each user."
"as the summative evaluation showed, the visualization of additional information helped users make decisions faster. users who have participated in our trials have been very positive about the interactive approach supported by the tweetclass tool."
"during the procedure we recorded our observations on paper data sheets. we used the \"thinking aloud\" methods, whereby participants were asked to verbalize their thoughts while performing the tasks. comments made by the participants are often valuable complements to observed behaviors in the test, and \"thinking aloud\" can help participants communicate what they are feeling about a tool and problems they may encounter while using it."
"a key barrier to the use of sm data is the absence of explicit and/or reliable demographic attribute data. such metadata is essential in survey research to make comparisons between population groups. without ready demographic data, researchers tend to resort to making subjective judgments by explaining the qualitative characteristics of user's posted content and virtual profile."
"regards the summative evaluation the results that we collected show that the gender refinement phase required less time than the age one. the assignment of a gender to an instances takes around 8.3 sec, while the assignment of an age class takes around 16 sec. this confirms the idea that age classification is also more difficult for human judges than the gender one. a deep investigation of the decision time shows that the users do not tend to slow down in their judgment task as the trials proceeded, with decision time depending on how clear the additional information is for them. figure 9 shows bar charts that represent, respectively, average task completion time to refine user gender and age class. furthermore, we find that in age refinement phase more time is required to load all the information (4 sec) than in gender refinement phase (0.5 sec). indeed, least famous friend information, for user's age class inspection, was collected real-time, while all the other information required for gender refinement was already present as attributes of the user table. indeed, this data was part of the input-data collected, a priori, using chorus tcd."
"the new gender refinement screen is presented in figure 7 . here, for the gender inspection, screen name (a), description (b), profile image (c), banner image (e) and background profile image (d) are shown. essentially, we changed the order of information and we highlighted all the text boxes in order to attract user attention towards text areas. the second age refinement screen is shown in figure 8 . here, the user can see all the information already described for the gender refinement step, as well as external links (d), list of least famous friends (e), user's timeline (f) and user's tweets about \"birthday\" (g). another improvement on the age refinement screen was to move the text boxes above the images and highlight their texts. moreover, we introduced combo boxes (a) and (b) for simplify how to set the new age/gender class. both in age class and gender combo box the actual class value of an instance is shown. when the user selects another class, this class is automatically assign to the selected user. in this way, the process of setting age or gender class becomes more fast and simple. the user can specify their confidence level about refined instances, using a three radio buttons (c) that replace the earlier slider control. now, the user only needs to specify if he/she is confident, semi-confident or completely unconfident about their classification."
"since social scientists are interested in maximizing the accuracy of the assigned demographic attributes, the automatic gender classification should ideally either assign the correct gender if possible or leave the classification to the refinement phase. for this reason we decided to reduce from 5 to 3 the number of classes presented to the end-user, namely (female, male and unknown). during the experiments we used two configurations to identify these three classes:"
4. the pop-up messages that appeared between two phases was not clear. the participants suggested to simplify the messages in order to make more simple and fast end-user choice.
"however, this method is very time consuming. on the other hand, automatic techniques can be used for deriving the demographic attributes, but in some cases (for instance in age identification task) their results are not always reliable [cit] ."
"gender and age class inference is achieved through a process that is summarized in figure 1 . at each major step, the end-user is provided with the option to scrutinize and refine the results of automatic classification algorithms. the most critical part of the pro- cess regards the identification of age class demographic attribute. our experimental data show that gender does influence the age class identification. as such, a two phase hierarchical procedure was used to build the classification model. hence, gender is derived as first attribute to increase the classification accuracy of the user's age class."
we also conduced a summative evaluation of the second interface prototype. this evaluation was quantitative and about testing if the objective of enabling users to make quick and confident judgments was met.
"to answer questions that cover all the different aspects of usability. for instance, the questions regard effectiveness, efficiency, information understanding, and easiness of use of the tool. all these aspects are very important to create a usable interface. indeed, our aim was to create a tool that was usable by a non-technical user, therefore the easiness for learning and using it is essential. also the easiness of information understanding is important: the enduser has to use the additional information to judge demographic attributes of twitter users."
"2. to obtain a sample composed of older users, using both automatic method procedure and refinement phase for just few users for either gender and age class inference. after that, to load another dataset."
"moreover, we studied how effective is the confidence value in finding the users misclassified by the automatic method, and whether the manual labeling had improved the results. we analyzed the agreement between the participants and the automatic method, and we found that the 80% of the misclassifications happen when the confidence value is in the range 50-60%."
"then, we studied the inter-agreement rate between participants for both gender and age refinements. we used an adaptation of cohens' kappa test for multiple raters proposed by fleiss [cit] . fleiss' kappa is a variant of cohen's kappa, a statistical measure of interrater reliability. whereas cohen's kappa works for only two raters, fleiss' kappa works for any constant number of raters giving categorical ratings, to a fixed number of items. it is a measure of the degree of agreement that can be expected above chance. the fleiss' kappa statistic for gender was equal to 77.34%, and for age was equal to 70.45%. we find again that age inference seems to be more difficult than gender. although 70.45% is a good level of agreement is less than the one obtained in gender refinement."
"while this binary categorization may seem too simple, we must consider that age is a difficult attribute to learn. not only does it change constantly, age-sensitive communication behavior differs based on numerous socioeconomic variables, and there is no well known indicator for age on twitter [cit] ."
"we also attempted to understand the accuracy level obtained by participants. we performed a 3-fold cross-validation separating the evaluation given by participants into two groups. each time with the evaluation of 66% of the raters we created a gold standard and with the evaluation of other 33% of the raters, we created the test set from which we computed their accuracy. we found that for gender refinement the level of accuracy reached is equal to 92%, while for age refinement the level of performances achieved is 91%. also from this point of view, we obtained worse results in the age identification, the most difficult task, than in gender derivation."
"after the participants finished all the tasks, they were asked to complete post-study questionnaire about the interface. subjective measures including satisfaction, usability, and learnability were collected along with participants' comments and suggestions during the post-study survey session."
"in this paper, we presented a semi-automatic approach to boost the accuracy of demographic attribution of users contributing to a twitter corpus. we presented tweetclass, as a proof-of-concept tool, that supports social scientist researchers in the identification of demographic attributes of a twitter users sample by combining both automatic and interactive class inference methods. this is a difficult problem because these attributes (e.g. age class, gender) are not directly obtainable from tweets or user account meta-data. as first step we built the hierarchical model to automatically identify demographic attributes, then we developed an interface that enables the user to intervene in the classification process. the interface is necessary for inspecting and refining the twitter users of the initial set for which an automatic classification of their gender or age class is very hard. in this way, the end-users can increase the quality or/and the dimension of twitter user samples."
"the size of publically indexed world wide web has probably surpassed 23.99 [cit] and as yet growth shows no sign of leveling off. even the number of digital images over the web is exponentially rising. as a result prevailing search engines like yahoo, altavista, google return thousands of matches in response to a user query. since, it is assumed that users do not formulate search queries using the best terms. therefore, search engines invoke query expansion (qe) to increase the quality of user search results. query expansion is the process of reformulating a seed query to improve retrieval performance in information retrieval operations. in the context of web search engines, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of data) and expanding the search query to match additional documents. thus, it is useful in reducing query/document mismatch, resulting in a higher ratio of relevant documents in the retrieval results (precision) and a higher ratio of the relevant documents from the collection that are retrieved (recall) [cit] . the ontology's can be used for query expansion in which firstly the query terms must be disambiguated so that they map to a unique ontology concept and then terms related in the ontology to the disambiguated concepts are added to the query. this paper is organized in the following way. section 2 discusses related work done in this domain. architecture and algorithm of the proposed system is presented in section 3 and 4 respectively. finally section 5 and 6 comprise of the results and conclusion."
"step 1: suppose a certain time-based driving style user a. following his habits and the road performance vector shown in table 4, 20 groups of different starting points and destination path combinations are selected as log samples."
"driving characteristics in log trajectories should be mined [cit] before the core process of feature clustering. here, space-time paths of time geography [cit] are used to excavate the driving characteristics (figure 2), in which the two-dimensional coordinates represent the historical space position of the vehicle, while the vertical coordinates indicate the time of the vehicle arriving at the corresponding space position. the relationships among the speed of driving at any time, the acceleration of driving, and the tangent slope of the inclined curve is shown in formulas (1) and (2) ."
"it stores all the context information corresponding to the keyword. when user enters the search query, recommendation engine returns various contexts for the query been fired."
"road and its subsidiary factors [cit], including peripheral environment, traffic laws and regulations, driver factors and other dynamic factors, whose varied details are listed in table 2, can reflect road performance in different extents, and then affect user path selection results. gas station, parking lot, traffic indicator, trunk road lighting rate, parking guidance and traffic guidance system setting 3."
"in this section, an accurate synchronous multi-index quantitative evaluation of road performance modeling system is established by maximizing the advantages of solving dimensions based on a t-s fuzzy neural network [cit], whose architecture is shown in figure 5 ."
"step 2: according to the selection principle of clustering centers, the initial time, economic, and comfort clustering centers are ensured when constraint conditions are satisfied. in this way, the three cluster centers are moderately separated."
"the ratio of green to credit, road traffic density, road evenness, number of crossings, average speed of main roads, blocking rate of intersections, average traffic delays and illegal occupancy lane rates."
"according to the calculation results related to the time and economic performance and the effective road length in table 12, the time consumption rates of the red planning path and blue reference path are 57.362 and 68.016, respectively, and the economic losses are 69.576 and 98.252, respectively. in comparison with the reference path, the personalized path that integrates user a's habits shows improved comfort and economic performance. this result is consistent with the driving characteristics of user a, focusing on time and economic performance. road is relatively unimpeded. moreover, by replacing wan jiali viaduct with going around the city at high speed, guaranteeing the time performance, it avoids the extra cost of passing through the road and the distance from the bypass, and improves the economic performance of the path. according to the calculation results related to the time and economic performance and the effective road length in table 12, the time consumption rates of the red planning path and blue reference path are 57.362 and 68.016, respectively, and the economic losses are 69.576 and 98.252, respectively. in comparison with the reference path, the personalized path that integrates user a's habits shows improved comfort and economic performance. this result is consistent with the driving characteristics of user a, focusing on time and economic performance. table 12 . where, the green represents the log path, the blue represents the literature reference path, and the red represents the personalized path that combines the b's habit."
"the query log provides detailed and summarized information about queries. the query log lists the time that each search occurred, the ip address of the web user performing the search, the number of hits for the search, and the user's query."
"road factors road self-factors 1. road alignment, road length, road intersection number, road level, road width, number of continuous turns, road evenness, isolation of green belts, and scribing isolation."
we verify the practical performance of the proposed algorithm from two levels of the effectiveness of the index performance synchronization evaluation model and the effectiveness of the personalized planning.
"unfortunately, there is no simple formula for a solution of a linear programming problem. a usual way to solve the linear convex optimization problems, including the linear programming, is by using of the interior-point methods [cit] or the dantzig's simplex method [cit] . the solution has to be found in the feasible regioninside of a complex polytope (fig. 5), specified by the constraints (5) . whereas the simplex method travels along the constraints (vertices), the interior-point methods go through the inside of the feasible space. however, all these methods are based on iterative algorithms converging to a solution, and therefore, they are not very suitable for an implementation in a realtime environment."
"distributed caching is a promising strategy for wireless cellular networks in order to reduce content access delay, backhaul data traffic, and transmission cost. this article explored the use of caching in the network with a focus on the processing of user requests. we introduced a novel caching strategy using parallel processing and evaluated its performance in terms of reducing the data access delay and redundant traffic in various caching scenarios. we also studied the capital cost for implementing the proposed caching networks. the tradeoff between the performance in terms of delay reduction and the cost overhead needs to be carefully considered in the implementation of the proposed parallel processing. simulation results have illustrated the effectiveness of the proposed strategy."
"the centralized data access architecture is very inefficient in coping with the exponentially growing traffic. the traffic aggregation points have to deal with an enormous number of data requests and duplicate traffic. 4 adding caches at these points to store certain content can reduce data access delay as well as duplicate transmissions from the servers in the core network. in figure 1, the solid lines denote the transmission links for data and control signals. the light-green cylinders denote the distributed caches. various popular content stored in the caches is represented by different colors. the caches of different layers can work in coordination, where every cache stores different items in order to avoid redundant storage and improve caching efficiency. the black arrows beside the caches denote the content-searching process. in existing caching networks, the suspend-and-wait method is often used for the processing of content requests in the cache. 9, 11 this means that the time delay for data access will increase when a request from the client passes through the cache. the processing of content requests is shown by the arrows at the bottom of figure 1 . the delay can become so large that it outweighs the benefit of using caching as the number of caching layers increases. in particular, when the chr is very low, such waiting time can be meaningless. to resolve this, we propose a parallel-processing-based caching strategy."
"let v denote the average size (in bytes) of the requested content and v  be the size of the request, which is usually much smaller than v. we use li,j to denote the total number of content requests between the two layers. we define bi,j and, i j b′ as the uplink and downlink transmission cost per byte on the network between the layers i and j, respectively. we use \"'\" to exclusively represent the downlink. the traffic volume between i and j is,"
"it is a user interface, where user types the search query. the user search query is forwarded to recommendation engine, which then give user, suggestions, by returning various contexts of the query been fired by the user."
the personalized quantitative evaluation of roads is a weighted summation of the effective characteristics and the corresponding road standard quantification values. the calculation process for the personalized road performance from a user perspective is shown in the last column of table 3 .
"(a) (b) figure 9 . (a) the optimal path model for user a; (b) the optimal actual planning path for user a, detailed performance of the related roads is shown in table 11 . where, the green represents the log path, the blue represents the literature reference path, and the red represents the personalized path that combines the a′ s habit. figure 9 shows that in the landscape area, the blue and red paths are dominated by the surrounding landscape of juzizhou road, which showcases fresh air, beautiful scenery and low noise. the blue line represents the yingpan road tunnel, and the red path shows the orange island bridge across the area, overlooking the orange island scenery. the latter is in line with user a's comfort requirements. in addition, the red route chooses the path from the perspective of user a, considering the comfort factors, and at the same time, it also focuses on the economic factors. in the non-sightseeing section, figure 9 . (a) the optimal path model for user a; (b) the optimal actual planning path for user a. where, the green represents log trajectory, the blue represents literature reference resources route, and the red represents the individualization route based on a. detailed performance of the relevant roads refer to table 11. figure 9 shows that in the landscape area, the blue and red paths are dominated by the surrounding landscape of juzizhou road, which showcases fresh air, beautiful scenery and low noise. the blue line represents the yingpan road tunnel, and the red path shows the orange island bridge across the area, overlooking the orange island scenery. the latter is in line with user a's comfort requirements. in addition, the red route chooses the path from the perspective of user a, considering the comfort factors, and at the same time, it also focuses on the economic factors. in the non-sightseeing section, a small number of branches are allowed to reduce the detour distance to make up for the additional economic losses caused by the bypass of the two ends of the orange island bridge. according to table 11, the economic and comfort performance of the road and the effective length calculation indicate that the cumulative comfort consumption rates of the red planning path and blue reference path are 56.62 and 59.06, respectively, and the total economic losses are 88.28 and 88.19, respectively. in comparison with the reference path, the user personalization path that integrates user a's habits increases comfort performance while decreasing economic performance slightly. the comfort performance increment of user a is contrary to the economic performance increment, and comparing the performance of the red and blue paths is difficult."
step 1: the small effective ball radius r 0 is selected randomly (figure 4a ) to initialize the valid range of the sample. the corresponding plane projection of the valid ball is shown in figure 4b .
"reference example a makes personalized path planning from user c's perspective. the personalized performance and effective length of the related roads are shown in table 13 . table 12 shows that the yun qi road, south second ring, wan jiali viaduct, wanjiali north road, and xianghu west road also appear in the same type of path planning of users b and c. path planning is of different types for users a and b. the same type of road users has different personalized performance, particularly for users b and c. with large effective length differences, the amplitude becomes increasingly small. the similarity obviously improves path planning. path planning at a similar degree increases with the increase of similarity between user feature vectors."
"step 5: the vector elements of the time, economy, and comfort cluster centers are updated according to the membership degree, as shown in formula (7)."
"a system, the performance of the core network can be improved as the workload is partly offloaded to the edge of the network, while users benefit from fast access to popular content on the internet. edge servers such as wireless base stations and local-area-network gateways have recently become more capable in terms of computing and storage."
"step 1: a three-dimensional cluster coordinate system is established based on driving speed, consumption rate for fuel and facilities, total root mean square acceleration, and integrated environmental quality. the time, economic, and comfort information contained in each log vector element are expressed in the cluster coordinate system."
"according to the equilibrium characteristics of the weight distribution of a user's driving style, the overall characteristics of different path tendencies based on user choice is divided into three categories, as shown in the first four columns of table 3, where c 1 3, c 2 3, c 3 3, respectively, represent the number of effective features in user eigenvectors. in addition, to avoid excessive neutralization of the decision-making power of the high weight feature, and ensure the good consistency and universality of the quantitative and the actual evaluation results, an effective definition of the features is presented in theorem 1. in addition, to avoid excessive neutralization of the decision-making power of the high weight feature, and ensure the good consistency and universality of the quantitative and the actual evaluation results, an effective definition of the features is presented in theorem 1."
"we use tul to denote the uplink delay, which consists of the time to transmit the content request through the network as well as the time the servers use for processing requests and searching for requested information in the caches. when there is no cache in the network architecture, the uplink delay is caused only by the transmission. in this case, we write when caches exist, a probability, namely chr, is introduced that the requested content can be provided by a certain cache in the network. in this case, the cache processing time is added to the intermediate layers. we use pi to denote the chr for layer i. if the content is not found at cache i, which has a probability of (1 -pi), the request will be sent farther up and toward the core network. therefore, the uplink delay is calculated according to the time spent in each layer of the network, and the probability of that. we express that as"
"step 6: if the conditions shown in formula (8) are satisfied, the iteration stops. otherwise, the process returns to step 2, where o and o represent the cluster center variables of the current time and the previous moment, respectively."
"let ti,j denote the transmission time delay from layer i to layer j in a hierarchical network including routing and other processing time, and let ti denote the time consumed for local content searching in cache i. the caches at different layers of the communication path cannot interact with each other. this means that the caches do not have the information of what has been cached and where. they also do not exchange information to create any coordination to avoid redundant caching."
the total square root acceleration of the total weight in iso2631 is used to approximate the comfort level of the human body; it is defined as follows:
"to the best of our knowledge, there is no determined relationship between the chr value and the location of the caches. generally, the caches closer to the core network have higher chrs. in this study, the chr values are generated randomly for each cache, while the caches at higher layers have a higher averaged value. the system performance curves are shown in figure 2, with the average chr of all caches increasing from 0 to 0.2. as shown in figures 2a and 2b, when the chr is very low-i.e., the requested data is not likely to be stored in the caches-the system without any caching (the pink lines) generates the lowest data access delay. as the chr increases, the effect of caching in terms of delay reduction can be gradually seen, as more data can be provided by the distributed caches that are closer to the clients. when the caches have a relatively high chr of 0.2, a delay reduction of approximately 30% can be provided by using caching at all of the four layers of the network (the yellow lines). by adding the proposed parallel processing to the caching network (the black lines), the uplink delay can be further reduced by around 5 ms (7%). it can be seen in figure 2c that the parallel-processing caching strategy has the best performance. when the chrs are 0.05, 0.1, and 0.2, the content access delays decrease by 15%, 18%, and 40%, respectively, compared to no caching. this strategy also produces an average of 10 ms further delay reduction compared to the four-layer caching approach. due to the use of parallel processing to save the time spent for content searching in the caches, this strategy is also efficient even when the chr of the caches is low. as the number of caches and the chr gradually increase, the traffic loaded at the p-gw level obviously decreases in figure 2d . when the chrs are 0.05, 0.1, and 0.2, the traffic loads can be approaching 78%, 63%, and 40%, respectively, of the original traffic load in the core network. note that the yellow lines overlap with the black lines in figures 2b and 2d . this is because since the proposed technique focuses mainly on reducing the uplink delay, the performance in terms of downlink delay and traffic reduction is likely to be the same with or without parallel processing. it is worth noting that each time the cache finds the requested information, the cache will send the negative request through the network. an additional signaling overhead is generated for the exchange of control messages. however, considering that the amount of extra traffic is usually negligible compared with the requested data, we do not reveal it in figure 2 ."
"step 3: the invalid samples of time, economic, and comfort cluster centers are deleted, as shown in figure 4e . the corresponding plane projection is shown in figure 4f . the number of samples within the effective radius of each cluster center is counted as n t, n m, n c ."
"it is seen in figure 3c that using the proposed technique, the content access delay can be reduced considerably by nearly half, as the chr increases to 0.2. approximately 10 ms of additional reduction is achieved compared to the noninterative scenario in figure 2c . this is because the system's overall chr is increased by interactive caching. in addition, compared with figure 2d, the traffic load, which is also visibly influenced by the chr, declines significantly. the traffic loads can be approaching 20% of the original traffic load in the core network when the chr reaches 0.2. from figures 2 and 3, we can easily see that the parallel-processing method (the black lines) is obviously better than the nonparallel method under the same circumstances. as the chr increases, the time delay and traffic load are both becoming smaller, which is beneficial for both the customers and the carriers. in general, interactive caching is able to achieve better performance than the noninteractive case. various methods are proposed in \"a cooperative cache-based content delivery framework for intermittently connected mobile ad hoc networks\" 12 and \"self-organizing algorithms for cache cooperation in content distribution networks \"13 to realize the cooperative function of content caching. in the two sets of simulation results, we can see a large reduction in traffic loads. in practice, the performance of the traffic reduction by using caching is more complicated and depends on the demand of the clients, the capabilities of individual servers, and, more important, the volatile operating conditions of wireless cellular networks. therefore, such performance gains might not be seen in actual situations. however, we are optimistic that with the combination of various optimization techniques and efficient network management tools, the proposed caching strategy is able to achieve an improved performance in wireless networks."
"in the lfcm clustering, the validity of the statistics of the number of vectors in each cluster center directly affects the accuracy of driving style judgement. the effectiveness of each cluster center sample is as follows."
"we simulate a simplified wireless cellular network in order to evaluate various caching strategies and show the performance of the proposed technique in terms of reducing access delay and redundant traffic. transmission time between two levels is randomly assigned, ranging from 15 ms to 20 ms, and the time spent in each cache ranges from 2 ms to 5 ms. we refer to the lte-a networks when setting specific values for data transmission and network operation parameters."
"the output of the network is the weighted sum of the fuzzy rules, as shown in formula (16). the weighting coefficient is the applicability of the rules and the result of the road performance level after the output-clarified process."
"in this article, we use content access delay and data traffic reduction as the main measures for evaluating the performance and benefits of various caching strategies. note that we consider only the reduction of the data traffic that is suitable for caching. certain data traffic containing, for example, user privacy and security information is not included in this analysis. for simplicity, we assume that all the caches have already stored the popular files, such as the most recent news videos and images, according to prior knowledge of the content requests. we also assume that the nodes with cache communicate only with other nodes in parent-child layers, not with their sibling nodes."
"we now study the economic overhead of utilizing caching networks incorporating the proposed parallel-processing technique. as it is too complicated to capture the large number of cost parameters in practical networks, which also vary case by case, we introduce a simplified cost model to analyze the cost benefits for utilizing caching. 9 a set of parameters are selected to best indicate the difference in operational costs with and without parallel processing. we analyze the cost of data transmission from when the user's request is sent until the corresponding data has been received by the user. note that the costs for the installation and maintenance of the communication infrastructure are not considered, as such costs are independent of whether distributed caches are used or not. we further assume that at the ran, a user device is connected to the same base station from when a content request is sent until the data is completely received; hence, there will be no additional cost for handover."
step 2: the smallest road vector elements containing users' driving information from the 20 log samples are obtained by the sampling operation. the fuzzy c-means algorithm is used on the tuple cluster of the log sample vector.
"a common data transmission process can be divided into two relatively independent parts-namely, the data request sent to the server and the requested data sent back to the client. for the second part, the latency is determined according to when the client receives the first data packet from the server (or cache), not the complete requested file. the uplink and downlink transmission latency are also assumed to be equal. now, we discuss the two parts respectively."
"this work proposes a novel approach for better image retrieval by query expansion process, overcoming mismatch in query and document vocabularies by addition of terms with similar statistical properties to those in the original query. the query is expanded by the terms from wordnet as well as from previous queries. moreover this work will increase the relevancy of the returned results, as user feedbacks the context of the query been fired prior to the actual search done by the searcher."
". it is observed that in order to achieve a financial benefit from the use of caching, the network should have a reasonably high chr so that the benefit generated from traffic reduction outweighs the cost overhead of the caches. cache devices with low data-processing and storage costs are always welcome. in the choice of using the proposed parallel-processing strategy, the tradeoff between the reduction of the data access delay and the extra processing cost should be considered. this is because the additional cost, neg i j o for transmitting and processing the negative request whenever a cache finds the requested file must be taken into account. besides, since all content requests are still sent to the core network in parallel with the intermediate-cache processing, such signaling causes extra uplink traffic."
the proposed system is effective as it will return most relevant results as per the user query. as it is an interactive process and user feedback the context of the query been fired by them. for instance for the same example where user wants to search for blackberry in context of fruit; user feedbacks the context prioir before the commensement of the actual searcg process. moreover query is expanded based on rule set and wordnet dictionary thus increasing relevancy. expected results of the proposed system are shown in figure 8 below.
"step 3: the hausdorff distance d hd ( f, o) of the log road vector element f to the time, economy, and comfort cluster centers o is calculated in formula (5), where f is the finite set of vector elements, and o is the set of time, economy, and comfort clustering centers."
"step 4: the distribution weight of time, economy, and comfort indexes is obtained through the normalization process shown in formula (11). plane projection is shown in figure 4d, where and represent the euclidean distance of sample to the cluster center and the high-density point sample, respectively; is the truncated distance, which is related to the average percentage of the total sample size and number of neighbor sample points;"
"the optimal path model based on user c's driving habits and the actual planning path, as shown in figure 11a,b, remain unchanged in terms of the descriptions of the colors corresponding to the planning paths. figure 11 . (a) the optimal path model for user c; (b) the optimal actual planning path for user c， detailed performance of the related roads is shown in table 13 . where, the green represents the log path, the blue represents the literature reference path, and the red represents the personalized path that combines the c's habit. figure 11 . (a) the optimal path model for user c; (b) the optimal actual planning path for user c. where, the green represents log trajectory, the blue represents literature reference resources route, and the red represents the individualization route based on b. detailed performance of the relevant roads refer to table 13 . by comparing figures 10 and 11, we can completely see that the blue reference path for user c in figure 11 is consistent with that of user b in figure 10, while the red planning path for user c in figure 11 is only consistent with that of user b in figure 10 at the beginning of the path planning, which avoids the east second ring road and other regional centers and makes full use of time and economic advantages of wanjiali viaduct; moreover, the inconsistent section steers clear of the xianghu road in densely populated areas, replacing it with xi xia road that exhibits pedestrian sparsity, small confluence vehicles, and less traffic. according to the calculation results of time and economic performance and the effective road length shown in table 13, the cumulative time consumption rates of the red planning path and blue reference path are 57.25 and 68.02, respectively, and the total economic losses are 77.69 and 98.25, respectively. in comparison with the reference path, the personalized path of user a shows significantly improved time and economic performance, as shown in table 14 . to embody the private custom advantage of the personalized planning path proposed in this paper, the user c is used as the specific service object in this verification link, except for the example of the reference path (blue in figure 11b ) experiments, adding the same type of personalized path (red in figure 10b ) contrast link."
"distributed caching has attracted major attention in a wide range of research and development for communication systems. for example, in the 3rd generation partnership project's long term evolution-advanced (3gpp lte-a), considered the fourth-generation (4g) cellular network, distributed caching can be implemented at the evolved packet core (epc), the radio access network (ran), and even user devices. the epc refers mainly to the serving gateways (s-gws), packet data network gateway (p-gw), and mobility management entity (mme), which are connected between remote internet servers and wireless base stations. caching is also believed to be an important functionality provided in the upcoming fifth-generation (5g) networks. 2 by implementing caching at wireless base stations (or enodebs), wi-fi access points, and wi-max access points in the ran, data service can be provided without the need of backhaul transmission. the performance of caching at various layers of the wireless network-e.g., at the small base stations and at the user terminals-was studied in \"wireless content caching for small cell and d2d networks.\" 3 the work in \"cache in the air: exploiting content caching and delivery techniques for 5g systems\" investigated the caching scheme in the ran based on the concept of content-centric networking, in order to minimize the content access delay of all users as well as redundant traffic in the network. 4 a proactive caching was proposed in \"living on the edge: the role of proactive caching in 5g wireless networks\" to leverage the existing heterogeneous cellular networks and design predictive radio resource management techniques to maximize the efficiency of the network. 5 the work in \"fundamental limits of caching in wireless d2d networks\" explored the emerging device-to-device communication technology and its potential for caching, where user devices are able to exchange data directly with each other in order to further reduce the pressure on the wireless resources of the ran. 6 researchers have also been working on caching strategies including what content to cache, as well as how the data can be cached at various caches. techniques including cooperative and coded schemes under different pricing scenarios, types of data traffic, and network constraints have been proposed. 7 the work in \"content caching and scheduling in wireless networks with elastic and inelastic traffic\" investigated a caching system where the content requests have both elastic and inelastic delay requirements. 8 in this article, we explore caching networks with the focus on the processing of users' content requests in current multilayer wireless cellular networks. we observed that, as the number of caches at different layers of the network increases, the disadvantage of the hierarchical architecture in terms of processing delay gradually emerges. a cache hit ratio (chr) is used as a key performance metric for caching. it denotes the percentage of content requests that can be serviced by using locally cached data. according to \"to cache or not to cache: the 3g case,\" the chr is usually very low and variable in small population regions, while reasonably high and stable where the number of users is very large. 9 therefore, the cost-effectiveness can be low when considering the time and energy spent on request processing and local content searching. in the worst case, the users may suffer even longer delays with local caching. we introduce a parallel-processing strategy in order to improve the efficiency of the caching networks. theoretical analysis and numerical simulations show the potential of the proposed scheme in terms of reducing both the content access delay and redundant data traffic in the core network. we also carry out cost assessments for the proposed scheme."
"sinusoidal pwm is likely the most straightforward way to generate a demanded output voltage. however, in this case, the magnitude of achievable voltage vector corresponds to vdc/2; it is not the maximum which can be reached by the vsi. therefore, the sinusoidal pwm is often replaced by more advanced approaches."
"garbage field and operation station, pedestrian crossing facilities, to sign and rate of population density circulation area subsidiary factors law and regulation factors 1. internship high speed limit 2."
"the minimum element, which contains the driving characteristics, is defined as the vector eigenvalue and is called the vector element. the hausdorff distance [cit] of each vector element to the time, economy, and comfort clustering centers is calculated, and the contribution of driving information to three different driving styles is measured. the fuzzy c-means algorithm is used to complete the clustering of the characteristics of each vector element. the concrete steps are as follows."
"the clustering results of the log road vector element features relative to the time (red), economic (green), and comfort (blue) cluster centers are shown in figure 3 . step 4: the membership degree of each vector element to the three cluster centers is calculated in formula (6)."
"step 3: the invalid samples of time, economic, and comfort cluster centers are deleted, as shown in figure 4e . the corresponding plane projection is shown in figure 4f . the number of samples within the effective radius r of each cluster center is counted as n t, n m, n c ."
"based on the basic evaluation model for the reliable evaluation of the results of the synchronization performance index, three drives (a, b and c) in two categories, who own their own log trajectory records, are randomly selected from the seven kinds of small classifications of driving users (table 3) to the performance of the path planning algorithm. specifically, inter-class experiments based on different type users a and b are designed to verify the universality of the algorithm. meanwhile, intra-class validation experiment based on different type users b and c, who are in same type but possess different driving feature vectors are added to further verify the personalization level of algorithm. the validation verification framework is design as in table 10 . to facilitate the comparison of path planning results, this paper is based on the background of route guidance in changsha city which is same as that of reference [cit], as well as the user types. the class design verification in table 10 shows that the economy + comfort user a ([0.05,0.32,0.63]) travels with only economy and comfort in mind and thus ignores the time factor. for this user, the economic attention level is 2, the influence coefficient is small at 0.32, the level of the concern for comfort is 1, and the influence coefficient is up to 0.63. the individual values of road performance depend only on the economy and comfort factors."
"the post-part network consists of three sub-networks of the same structure for outputting fuzzy rules of time, economy, and comfort indexes. the input layer needs to be supplemented with a constant term, that is, input parameter 1 of the 0th node, which is used to generate the constant term in the road performance level calculation result. the fuzzy inference layer calculates the consequent of each fuzzy rule, as shown in formula (15)."
"based on the clustering results, the performance of each road can be quantified in a personalized way according to the evaluation method in table 3 . without loss of generality, the latest 20 sample log trajectories of a random time-economic-based user are selected as an example of personalized user quantification."
"this example is the personalized path planning for time + economic user c ([0.56,0.37,0.07]), and user b's planning path constitutes the intra-class verification of personalized path performance. users c and b ([0.35,0.57,0.08]) share similar features, belong to the same time + economic type, pay attention to time and economic factors, while ignore the factors of comfort effect. however, there are two different levels of attention in time and economy. c users are more inclined to time, with a level of 1, a coefficient of 0.56, a second economic factor, a level of 2, and a coefficient of 0.37."
"with the rapid development of the modern automobile industry, self-driving groups are steadily increasing and road traffic pressure is increasing. these changes do not match users' demand for comfortable travel. in addition to the expansion of existing road networks, attempts have been made to avoid the clustering effect caused by the planning of the integration of personalized travel path for users. for example, the full use of existing resources in public transportation can ease traffic pressure and improve user experience in personalized travel. therefore, studying the personalized paths of users is of great social and economic significance."
the space vector pwm for three-phase converters is widely used and its application is almost straightforward. the active outputs vectors of the converter can be expressed by for each switching combination:
"it is observed that an additional term -ti is introduced in the description of the delay between the adjacent layers. this is because the content searching at layer i is processed in parallel while the request is being sent and processed at the upper layer. this overlapping time should be deducted. this is the main advantage of the parallel processing. in addition to the reduction in delays, the expected reduction of data traffic between different layers can be calculated. for example, in figure 1, the reduction between the p-gw and the internet can potentially reach"
"we see several opportunities for future research. first, we discussed the benefits of distributed caching assuming that the popular content has been already stored in the caching devices. in practice, how to dynamically select the caching content out of a massive number of items in the networks is a valuable research topic. second, even if each layer of the network is installed with caches, the issue of how to obtain the maximum benefit by using a limited number of caches needs further study. when considering the deployment of cache devices with the installation of new network facilities such as cellular base stations, the optimal selection of locations becomes essential. cooperative caching techniques have been discussed in \"a distributed caching based on neighbor cooperation in ccn.\" 15 however, huge challenges emerge as network topologies become increasingly complicated. for applications of our approach on a massive scale, the efficiency of cooperative algorithms is of vital importance. finally, distributed caching can be regarded as an important component of the emerging fog network technologies that extend the cloud-computing paradigm to the edge of the network in order to take full advantage of the more powerful local data processing, transmission, and cooperative resource management in edge devices."
"search engine component, web crawler collects, parse, and stores images and url in an image repository to facilitate fast and accurate image retrieval. this repository is then indexed to optimize speed and performance in finding relevant documents for a search query. without an index, the search engine would scan every document in the corpus, which would require considerable time and computing power. sample indexed document is shown in table. 3. table 3 . indexed documents"
"in the lfcm clustering, the validity of the statistics of the number of vectors in each cluster center directly affects the accuracy of driving style judgement. the effectiveness of each cluster center sample is as follows."
"according to the personalized path performance shown in table 6, we reassign the road network performance and use the ant colony algorithm to achieve personalized dynamic route selection based on user driving habits, as shown in figure 8, in which the blue route is a random track in the user log samples, the green line segment is the personalized path of the proposed algorithm, and the red part is the coincidence of the two paths. figure 8 shows that the log sample trajectory from the starting point to the end point is 6-17-23-29-40-46-47-53-59-60 while the integrated user habits in the personalized planning path are presented as 6-17-23-29-40-46-52-58-59-60. the number of anastomosed sections is 9, the number of different sections is 2, and the absolute anastomosis rate is 82%. according to the depth analysis, the sample trajectory sections reach 47 and 53, which correspond to the road consumption values of 1.8 and 2.2, respectively. the different personalized path sections 52 and 58, which correspond to the road consumption values are both 2.0. thus, the cumulative values of the road consumption of the two tracks match at a rate of 100%. similarly, the remaining valid log samples for the user are verified one by one, and the results are weighted to all (the invalid sample weight is 0; otherwise, it is 1). the statistical results show that the absolute anastomosis rate is 89.5%, and the relative anastomosis rate is 98%. figure 8 shows that the log sample trajectory from the starting point to the end point is 6-17-23-29-40-46-47-53-59-60 while the integrated user habits in the personalized planning path are presented as 6-17-23-29-40-46-52-58-59-60. the number of anastomosed sections is 9, the number of different sections is 2, and the absolute anastomosis rate is 82%. according to the depth analysis, the sample trajectory sections reach 47 and 53, which correspond to the road consumption values of 1.8 and 2.2, respectively. the different personalized path sections 52 and 58, which correspond to the road consumption values are both 2.0. thus, the cumulative values of the road consumption of the two tracks match at a rate of 100%. similarly, the remaining valid log samples for the user are verified one by one, and the results are weighted to all (the invalid sample weight is 0; otherwise, it is 1). the statistical results show that the absolute anastomosis rate is 89.5%, and the relative anastomosis rate is 98%."
"data mining is the process of extracting hidden patterns from data. in this paper, association rules are applied. association rules mining are a major pattern discovery technique [cit] . the original goal of association rule mining is to solve market basket problem but the application of association rules are far beyond that [cit] . the major statistics computed for the association rules, support and confidence are given in equation"
"the method for the derivation of processing delay and traffic reduction is similar to what we previously discussed. however, we pay particular attention to the difference in the system chr after considering the interactive performance of the caches. in particular, the probability of a request being processed at each layer is modified as"
"resulting waveforms are depicted in fig. 3 . iv. linear programming linear programming is a well-known class of optimization problems, in which the objective and all constraint functions are linear [cit] :"
"we propose a personalized path decision algorithm that is based on user habits, implementing the navigation service from a previous similar trip group to a specific individual, which can essentially improve the personalization of the existing path planning algorithms. the results and shortcomings of the existing optimization algorithms based on a single optimization target or a single or a few special traffic factors are analyzed. we use the log road track that integrates user's habits to mine driving characteristics, and use lfcm to achieve user's driving style clustering, and get users' driving habits. then, we use t-s based road multi index performance synchronous evaluation model to quantify the road's time, economy and comfort performance. at the same time, we combine user's driving style and feature to get the user's road performance. finally, ant colony algorithm is used to search the shortest path consumption path. the algorithm is based on the user's own log trajectory and uses feature mining and clustering techniques to get the user's habits and implements the navigated service objects from the previous travel groups to the travel individual and customizes personalized path navigation system for users to meet their own driving habits. the experimental results show that the absolute anastomosis rate and the relative anastomosis rate of the method in the simulated traffic network are 82% and 100%, respectively. in the actual traffic environment, three comparative examples (i.e., a, b, and c) of user's personalized path and the corresponding user log path show consistent rates, reaching 89.2%, 100%, and 84.5%, respectively; in comparison with the individual path in the literature [cit], the path coincidence rate significantly increases by 36.9%, 35.3%, and 35.8%, respectively. by improving the personalization level of existing path planning, the travel experience of self-driving users is greatly improved."
"from the minimum effort control point of view, it is necessary to minimize the leg voltage maxima, needed for generating of the desired output voltage vector."
"since in most cases, the dc-link voltage is a constant value, limited by supply voltage source and power switch dimensioning, the main target of an employed control technique is to maximize the output voltage at a given dc level. in other words, it is desirable to minimize the dc-link voltage when generating the output voltage vector demanded by an upper (superior) control loop."
"as mentioned above, minimization of the slack variable vs leads to minimization of actual leg voltage maxima. since the maximum absolute value of all vector components corresponds to the infinity norm defined as"
"results of prevailing search engines like google, yahoo!, altavista, flickr and picsearch for user query \"blackberry\" are shown in figure 7 . for a user who is searching for \"blackberry\" in context of fruit the result seem to be irrelevant and act as a noise. since user generally does not formulate query along with context, therefore efforts are made in this direction in this work."
"as parallel processing affects only the uplink, the downlink delay remains the same as cache dl t . therefore, the total data access delay in the caching network incorporating parallel processing is cache cache ul dl t t + ."
"where i i, c i, and s i represent the environmental quality index, coverage rate, and evaluation standard of the natural environment factors i; m is the species number of i, including the road greening rate, river distribution, and air quality; and i denotes positive correlation with the comprehensive road environment. to minimize the clustering convergence time and avoid the local optima [cit] before the clustering of characteristics, we must fully utilize the human inductive ability to restrict and select the value of the initial cluster center. the following principles are adopted for selecting the time, economic value, and comfort index of the cluster center."
"mature navigation systems and path planning algorithms mainly focus on the fastest path [cit], the shortest path [cit] or the most comfortable path [cit] between specified starting and target points. in recent years, path optimization algorithms of multi-objective optimization [cit] and single-objective optimization [cit], as well as a few traffic impact factors, have deepened [cit] and popular routes planning [cit] . moreover, significant attention has been directed toward personalized path guidance systems. campigotto p [cit] introduces the favorite route recommendation (favour) approach to provide a personalized, situation-aware route based on the information updating through bayesian learning, which was obtained from initial configuration files (home location, work place, mobility options, etc.). a personalized fuzzy path planning algorithm based on the fuzzy sorting of the center of gravity was proposed by nadi [cit], in which a optimization route according to user standard types was formulated through analyzing the uncertainty of user preferences through the expression of fuzzy linguistic preference relations. a personalized recommendation route algorithm based on large trajectory data was proposed by dai j [cit] where both drivers' driving preferences"
"in an interactive-caching scenario, the caches in different layers can interact with each other. caching devices can potentially coordinate in order to reduce duplicate content cached in more than one layer. as a result, the system chr is increased, and thus the redundant data traffic in the core network can be further reduced."
"finally, to avoid model turbulence caused by the order of magnitude of significant road influencing factors, we perform normalization in the anti-fuzzification layer, as shown in formula (14) ."
step 1: the small effective ball radius is selected randomly (figure 4a ) to initialize the valid range of the sample. the corresponding plane projection of the valid ball is shown in figure 4b .
"the user search query is forwarded to recommendation engine, which then give user, suggestions, by returning various contexts of the query been fired by the user. user then feedbacks the recommendation engine by selecting the context, for instance for the user search term shown in figure 4 and the suggestions by the recommendation engine and user feedback are shown in figure 5 and figure. 6 respectively."
"obviously, the contrast experiment on the personalized planning path that integrates user b's habits does not meet the increment in time and economic consumption. therefore, for the performance qualitative comparison link of example 1, the dimensionless and performance comparison results are shown in table 14 . table 14 shows that in comparison with the reference path, the personalized planning path of user c shows greatly improved time and economic performance. this result is in line with the characteristics of the user related to time + economic performance. in comparison with the individual path planning of user b, the personalized path of user c shows improved time and economic performance; however, the benefits outweigh the lack of economic performance. this result is in line with user c's greater attention time than economic performance levels."
"query expansion (qe) is the process of reformulating a seed query to improve retrieval performance in information retrieval operations. query expansion involves techniques such as searching for the synonyms as well finding all the various morphological forms of words by stemming each word in the search query. it also deals in fixing spelling errors and automatically searching for the corrected form or suggesting it in the results. in this work, query is expanded by all the distinct contextual pairs of the words using wordnet."
"the costs of processing data requests and data transmission are evaluated using the same simulated networks as before. in addition, we assume that the average size of a content request is 50 kbytes and that the average size of a requested file is 1 mbyte. we define z as the unit cost for transmitting 1 mbyte of data in the downlink from the core network to the end users. in the uplink, the data transmission and processing cost is 20z per mbyte, and the cost for a negative request is 10z. we roughly estimate the cache-processing costs according to \"to cache or not to cache: the 3g case\" 9 and \"the business case for caching in 4g lte networks.\" 14 the chr varies among different layers of the network. we use a small random value to describe the cost variation for transmitting data between different layers considering the different transmission distances and link capacities. requests (around 10 gbytes of data transmission) without caching is about 56,000z. in a caching network with a total chr of 0.2, due to the cost of cache storage, the total cost is higher than that in a noncaching network. as the number of data transmissions increases, the benefit appears. the cost reduction reaches approximately a third for serving 10 gbytes of data. it is observed that the use of parallel processing results in a cost overhead comparable to that of a normal caching network. such overhead increases with the number of requests. figure 4b depicts the tendency of cost for serving 10 gbytes of data with various chrs. when the chr is very low, caching networks bring no benefits as they require more data storage and management. as the chr increases, the benefit in terms of reducing the data transmission is seen. however, the slope of the decrease is relatively small when parallel processing is used. this again reflects the cost for additional processing and signaling at the caches. only when there are massive content requests, or when the system chr is reasonably high, is parallel processing more cost-effective than a noncaching network. we conclude that parallel processing will increase the cost for the caching networks. there is a trade-off between the further reduction of user content access delay and the additional cost in the choice of using parallel processing. we envisage that, as cache management becomes more advanced, our proposed technique will demonstrate the economic potential of dense, fast response networks in the near future."
"for any vector element of the driving characteristics, if any of the following arbitrary conditions are satisfied, the element can be called an invalid feature; otherwise, it is an effective feature."
"step 5: the vector elements of the time, economy, and comfort cluster centers are updated according to the membership degree, as shown in formula (7)."
the total square root acceleration of the total weight in iso2631 is used to approximate the comfort level of the human body; it is defined as follows:
"based on the driving characteristics of user a, the case is weight coupled with the time, economy, and comfort performance values obtained by the evaluation model. moreover, the individual road performance values in field a are obtained. then, the best path is searched according to the road's personalized cost. in view of the large complexity of road networks, this example only enumerates the personalized performance and effective length of the road related to the path search results, as shown in table 11, where the effective length of the road is the length of the path through the planning path. in table 11, the personalized performance of each road in the personalized path that integrates user a's habits is higher than the personalized performance of the roads in the reference path, which lays the foundation for the integration of personalized path performance advantages of user a driving habits. the optimal path model and actual planning path for user a are shown in figure 9a,b, respectively. in these figures, blue denotes the optimal planning path for users of the same type [cit], red denotes the optimal path to merge user a's driving habits, and green denotes the log track of user a. the optimal path model and actual planning path for user a are shown in figure 9a,b, respectively. in these figures, blue denotes the optimal planning path for users of the same type [cit], red denotes the optimal path to merge user a's driving habits, and green denotes the log track of user a."
"on the other hand, in case of mathematical modelling and simulation results, a pwm block based on linear programming problem can be easily implemented using a library function (e.g. linprog() in matlab environment)."
"road and its subsidiary factors [cit], including peripheral environment, traffic laws and regulations, driver factors and other dynamic factors, whose varied details are listed in table 2, can reflect road performance in different extents, and then affect user path selection results. table 2 . influencing factors of roads."
"therefore, for user a, the increased comfort performance of the red path is greater than the economic performance reduction, the red path is more consistent with user a's driving habits. reference example a makes personalized path planning from user b's perspective, and the path planning process is no longer duplicated. the individual performance and effective length of the related roads are shown in table 12 . table 11 shows that the yun qi road, south second ring, wan jiali viaduct, wanjiali north road, and xianghu west road appear in the path planning of users a and b; however, the road presents a considerable difference in terms of the personalized performance value and its effective length. hence, verifying the performance of the proposed personalized path varies from person to person. the optimal path model based on user b and the actual path, as shown in figure 10a,b, respectively, remain unchanged in terms of the descriptions of the colors corresponding to the planning paths. figure 10 shows that the blue and red paths are driven mainly by the main road or viaduct, and they shorten the detour distance as much as possible. in comparison with the blue reference path, the red path representing the personalized path of user a can effectively avoid the east second ring road, bayi road, south road, furong road, wuyi road, and the bottleneck of a congested road. the road is relatively unimpeded. moreover, by replacing wan jiali viaduct with going around the city at high speed, guaranteeing the time performance, it avoids the extra cost of passing through the road and the distance from the bypass, and improves the economic performance of the path."
the knowledge base is a repository of extracted rules which have been derived using the association rule mining. a knowledge base containing rules is shown in figure 2 .
"the main idea of the proposed technique is that, when a data request arrives at a node with a cache, the node immediately passes the request to the upper-layer node without suspending and waiting. at the same time, this node also searches for the requested information in its cache. the nodes at the upper layers carry on the same process until the request is passed to the server in the core network. this data-requesting process can be seen as a combination of the original process without caching and cache searching in parallel. when a cache finds the requested information and makes sure it is valid, the cache immediately sends a \"negative request\" to the upper-layer nodes of the transmission path. the requested information is sent down to the client. having received the negative request, the upper-layer nodes can stop content searching (and data transmission) for the corresponding original request and start to deal with the next request in their queue. if a node does not find the requested information when it finishes searching its cache, then it can just discard this request. one possible situation is that after the core server (or an upper-layer node) starts to send the requested data to the client, a node in the lower layer finds a valid copy in its cache. then this node sends a negative request upward. having acknowledged the negative request, the upper-layer network stops providing the data. the rest of the requested content is sent from the cache at the lower layer. ideally, this method can almost eliminate the delay in searching for cached data and significantly reduce the redundant traffic in the network. in particular, when the cache's chr is low, the proposed method has a great advantage. although additional traffic overhead and power consumption for exchanging control messages are required at the data aggregation nodes, the added value of the proposed caching strategy is still very obvious."
"wordnet is a lexical database for the english language. it groups english words into sets of synonyms called synsets, provides short, general definitions, and records the various semantic relations between these synonym sets. since google maintains only the word lexicon, it returns synonyms words pair in alphabetical order in response to a given query keyword as discussed by brin and page [cit] . in comparison to this, wordnet produce more meaningful pair of words. hence wordnet has significance in this work to make search proceed in specific direction and retrieving relevant images."
"step 6: if the conditions shown in formula (8) are satisfied, the iteration stops. otherwise, the process returns to step 2, where and represent the cluster center variables of the current time and the previous moment, respectively."
"while the probability of dropping connection requests for service type t of handover users, which are offered to cell i from service type s in its adjacent cell is"
"the spray and wait and the epidemic routing protocols forward messages without taking node mobile patterns into thought, therefore the delivery ratio is too low. now, we present the vibrancy of node. vibrancy of node indicates the activity of a node, or the number one node meets other different nodes within a given interval and remaining energy. in the same interval of time, the more nodes that one node meets, the greater the vibrancy of node. the vibrancy of nodes can dynamically represent the node activity in a given period of time. in this paper, we use the ratio of vibrancy of nodes to dynamically forward the number of message copies."
(1) support of multiple scs with different bandwidth requirements. (2) support of adaptive bandwidth for call services. (3) effects of using our cac policy in wireless access networks.
"the psnr with a type 2 service is set to 33 db while it is set to 28 db with a type 2* service [cit] . we assume that the number of the type 2 services in cell 1 is \"n2\" while the number of the type 2* services in cell 1 are \"n3\". let ω denote the average psnr of both the type 2 and 2* services in cell 1. we have"
"the rest of the paper is structured as follows. section ii goes over related works, describes several typical dtn routing protocols. section iii details the vibrant energy aware spray and wait (vesw). section iv makes concludes the paper."
"in this section, we present a model for adaptive rate multiservice cellular systems. we describe the cac policy and we also analyze the blocking and dropping probabilities of connection requests. figure 1 shows the multirate service cellular systems used in this study while table 1 details some of the important symbols used in this article."
"when two nodes encounter, they will update the vibrancy of nodes at first and exchange vibrancy of nodes with each other, and check the number of message copies l on node a is more than 1, than algorithm will be run on node a to calculate the number of copies sent to node b. and the spray phase continues until there is only one message copy left on node a."
bordercasting node initiates bordercasting to its peripheral nodes. we can observe from the below fig. 3 that bordercasting node 's' bordercasts the query message packet to peripheral node 'f'. this node 'f' will border cast the query message packet to 'x' and 'y'. 'x' and 'y' receive query at the same time. if both of them bordercasts the query then again there are chances that same node may relay the same query multiple times. unless and until x and y check for the qd information they both will not be knowing that the query bordercasted by them is redundant.
"with simulation results we compared zrp with enhanced zrp by involving two enhanced algorithms to enhance the performance of zrp for communication on manets on an urban terrain. by analyzing simulation results we can observe enhanced zrp had performed better in all the simulation sequences with an improvement of 15 to 20 percent when compared to zrp. to further improve the efficiency of enhanced zrp, our future work shall involve a new algorithm to overcome the generation of periodic updates in the routing table to reduce the control traffic. thus by the implementation of our proposed algorithm the time taken to receive the first packet will be minimized, which results in better throughput. so the number of bytes and total packets received at the destination will be increased."
"to reduce this delay we can go for query control mechanisms. query detection uses query source node's id and query id pair. below fig. 1 depicts query detection scheme. where 's' is the source node and destination node 'd' is not in s's local zone. hence s bordercasts the query message packet. nodes j, k, i detects the query packet through qid method-1 and node h detects it through evesdropping i.e qid method-2."
"in our study, we reduced the blocking probabilities and the forced termination probabilities to lower levels but with a loss of the qos for video telephones with a type 2 service. the curves of two services exhibited the same trend. for example, the connection-level qos requires that the blocking probability should be under 2% while the forced termination probability should be under 10"
"the techniques can be applied to single or multiple channel manets to improve both the delay and control traffic performance of zrp. in this scheme, we allow zrp to provide routes to all accessible network nodes, with less control traffic than purely proactive link state or purely reactive route discovery, and with less delay. extensive simulation results reveals that our scheme features better transmission delay, route convergence time, the last packet reception time, system efficiency and system through-put."
"let ϕ i st ðm i þ denote the birth rate of service type t, which is offered to cell i from service type s before entering the cell. we have"
"at present the internet architecture is based on the tcp / ip protocol. this structure carries out message forwarding in the ip network layer by means of ip protocol, and in the transmission layer it requires a certain pathway for a period of time. traditional routing protocols will no longer be suitable to this new type of network architecture. on the other hand, researchers have proposed a class of store-carry forward routing ( fig. 1 ) schemes [cit] for such network environment, where a node receives packets, stores them in their buffers, carries them while moving, and forwards them to other nodes when they encounter each other."
cellular systems provide wide-area network coverage. users move within the coverage areas and they will switch connections among cellular systems according to their roaming agreements. the process of switching connections among cellular systems is known as a handoff or handover.
"after that, node a starts the wait phase to wait for direct communication opportunity with the final destination. the spray phase considers vibrancy of node and remaining energy information for disseminating message copies to delay the network lifetime, as a result, the opportunistic routing scheme proposed in the paper will be called vibrant energy-aware spray and wait (vesw)."
chao and chen [cit] investigated the cac problem with mobile personal communications networks. their study focused on the cac used for multiple-class calls with user mobility. a generic class of coordinate-convex cac policies was considered.
"for example, a node with higher vibrancy of node may encounter more nodes than the one with lower vibrancy of node during the same time interval. so the spray phase can be sped up if higher vibrancy of nodes carries more message copies. on the other hand, it may lead to higher risk of data collection if the node with less remaining energy is allocated with more message copies. based on the above observations, a utility function vibrancy of node and remaining energy information is given here to speed up messages spreading in the spray phase and improve the routing efficiency."
"in the spray and wait, the copies of messages are sprayed with no consideration about any information like vibrancy of node, remaining energy, etc., that may lead to blind spray, energy aware, and low efficiency in data gathering."
"where h j s i t denotes the handover rate of service type t, which is offered to cell i from service type s in its adjacent cell j."
"(1) we evaluated the effect of using a cac policy, which was supported by adaptive bandwidth for call service in wireless access networks. (2) we evaluated the performance of the cellular systems using a policy based on the arrival rates of connection requests."
"this study evaluated the cac policy with an amr services cellular system. we introduced the cac policy functions that were based on the service category (e.g., voice, video, and voice call) and the type of connection request (i.e., a new request or a handover request). initially, we developed an analytical model to evaluate the cac in an amr services cellular system. we evaluated the system performance when the cac policy was in use."
"connection admission control (cac) schemes are used to selectively limit the number of admitted calls from each traffic class to maximize network utilization while satisfying the qos constraints [cit] . in a cellular system, the qos requirements for different services (e.g., voice, real-time video) require a cac to limit the number of connections in each access network [cit] . the cac policy can either accept the connection request and allocate the resources accordingly or reject the connection request. in general, higher priority is given to the requests from handover users rather new users. from the user perspective, abrupt termination of a connection is more annoying than occasional blocking of new connection attempts."
"the six conditions that used in this study, as shown in figure 1, are as follows. while \"condition 2\" is satisfied, a \"single mode service handover connection request is permitted\" from point l to m in cell 4. this increases the blocking probability and reduces the dropping probability. while \"condition 3\" is satisfied, a new \"dual mode full-rate service\" connection request is permitted from point j to d in cell 2. similarly, both the new \"dual mode full-rate service\" connection requests from point a to b in cell 1 and from point a to e in cell 4 follow \"condition 3\"."
"α is a smoothing factor, which determines the influence of the vibrancy of node in former fragment on the vibrancy of node in the current fragment. ṽold refers to the vibrancy of node at the end time of former fragment and has to be updated regularly. in our algorithm we update the vibrancy of node once per hour."
"with the knowledge of the qd obtained a procedure has to be followed in which covered nodes can be pruned by the bordercast tree. this procedure is known as premature node termination. by referring to below fig. 2, we can see that node 's' bordercasts the query message packet to its peripheral nodes. node 'i' receives same copy of query message packet through bordercast tree by the other node as it may be interior member of other routing zone. now 'i' searches for the interior zone member of other node, and uses qd to detect is it necessary to relay that query message packet or no. 'i' identifies that nodes are been covered, it reconstructs the bordercast tree. prunes covered nodes from other node' bordercast tree and deletes the query message received by other node using the criterion called premature node termination."
"this allows delivery among disconnected network components. because nodes in this network are sparse, and the node transmission range is limited, nodes will need to deliver messages before messages are stored for a long time, moving through nodes and then forward messages to other relay nodes or the destination node."
"probabilistic routing protocol using history of encounters and transitivity (prophet) is an evolution of epidemic routing that introduced the concept of delivery predictability. delivery predictability is the probability for a node to encounter a certain destination. prophet forwarding algorithm is similar to the epidemic routing one except that, during a contact, messages are requested only if the receiving node has greater delivery predictability for the destination. prophet is representative for a class of routing protocols that exploit some context information to limit the epidemic routing flood. easw is able to manage and exploit far richer context information with respect to prophet."
"recent advances in voice and video compression techniques have led to an increasing interest in live voice and video services. in this section, we introduce rate adaptation coding techniques used for multiservice cellular systems. we consider the issues associated with adaptive voice and video mechanisms. a review of dynamic schemes used for the adaptation of voice encoders in cellular networks is also provided. the video source is encoded into multiple independent descriptions. mobile cellular operators depend on the available cellular bandwidth to join different descriptions and meet the cellular bandwidth requirements. the introduction of an amr codec for voice services and mdc of video service is conducted as follows."
those conditions comprise the cac policy function and they aim to simultaneously reduce the blocking and dropping probabilities of each service in multi-service cellular systems.
"the rest of the paper is organized as follows: section-ii explains overview of dsr, aodv and zrp for manets. section iii gives a detailed description of our proposed work such that we can have better efficiency. section iv presents simulation based results, evaluation and performance comparison graphs of our previous work. finally, conclusion and future work are presented in section v."
"the amr codec for voice services was the fourth speech compression algorithm to be standardized by the european telecommunications standards institute (etsi) [cit] . the third-generation wcdma system took a novel and a flexible approach to amr. the compression algorithm was identical for both gsm and wcdma, and it decreased identical source encoding rates of 12.2 down to 4.75 kb/s [cit] ."
"we denote the total bandwidth of cell 2 by c 2 . while the \"condition 5\" is satisfied, the \"dual mode full-rate service\" in point f of cell 1 changes into the \"dual mode reduced-rate service\" in point g of cell 2, and the handover connection request from point f to g in cell 2 is permitted. the location of the service type change is point c."
"the remainder of this article is organized as follows. the amr method is introduced in section ii. the analytical model for cellular systems and cac are described in section iii. numerical results are presented and discussed in section iv. finally, our conclusions are stated in section v."
"the average end to end delay introduced is more for zrp in our test case this is because of flooding more query control messages. our proposed algorithm reduces this and there by promising reduced end to end delay. this is as shown in below fig. 10 . the time taken to receive the first packet in case of zrp is higher when compared to enhanced zrp, this is because of the excess control messages flooded in case of iarp and ierp. this is as shown in below fig. 11 . fig. 11 : the first packet received in seconds in our simulation, time constraint is 100 sec, total number of packets and total numbers of bytes at the destination received within this simulation time are reduced for zrp due to increased delay as compared with enhanced zrp. this is as shown in fig. 12 and fig. 13 . in the above fig. 14 we can observe that when network density is less i.e. for 10 nodes number of packets received are more as compared at high network density. this is because of number of nodes increase, number of overlapping zones increase, thus query messages. as the number of the nodes increases in the network, route discovery becomes more complicated, because centralized node routing zones will highly overlap with each other, hence the route request queries will be flooded in to the network, and the intermediate nodes will send same route request queries multiple times, hence the route acquisition delay will have higher percentage as the number of nodes increases. therefore the zone routing protocol have lower throughput when compared to enhanced zrp. at 200 nodes enhanced zrp has low transmission rates but more than zrp. this is as shown in fig. 14 . jitter is the variation in the time between packets arriving, caused by network congestion, timing drift, or route changes. as from the above fig. 15 we can observe that at 10 nodes both the protocols have small jitter value. but at 25, 50, 100 and 200 nodes since query packets will be flooded throughout the network control overhead increases; it consumes more time to reconfigure the route if link failure occurs. hence there will be more time variation between arrivals of packets results in more jitter value."
"in this paper, we propose a vibrant energy-aware spray and wait routing scheme called vesw to improve energy efficiency of spray and wait routing. in vesw, an effectiveness function vibrancy of node and remaining energy information are presented to assign the number of copies between each related pair and avoid the blindness in the spray strategy of spray and wait. the proposed vesw routing outperforms bsw routing protocol in terms of energy aware, reduce network overhead, increase the delivery rate and improves the delivery utility."
"in the case of destination which moves outside the zone of l as shown in the fig. 8, it has no route to destination which is required for the query source, and then a new search is started from the node l. thus the number of ierp route request packets is reduced, because the route reconfiguration is started from the destination failure reporting node instead of beginning from the source. hence this reduces control overhead packets and end-to-end delay time, which results in higher throughput."
"amr has two traffic channel modes in a global system for mobile communication (gsm): adaptive full-rate speech (afs) and adaptive half-rate speech (ahs). the gross bit rate of afs is 22.8 kb/s, whereas that of ahs is 11.4 kb/s. the gross bit rate is the sum of the speech codec bit rate and the channel codec [cit] . a mobile station equipped with amr can request a particular mode, subject to the approval of the base station."
"handover can occur because of the movement of users among different cells, as shown in figure 1 . we considered a general model of multiservice calls with mobility when studying cac policy, because the majority of the literature is concerned with multiple-class calls in fixed networks (i.e., without mobility) [cit] or calls of only a single class [cit] ."
"delay tolerant networks [cit] are sparse multi-hop ad hoc networks in which nodes utilize any pair-wise contact opportunities to share and forward messages. there is not any preexisting internet infrastructure and most of the time it does not exist a complete route from the source to the destination in delay tolerant networks. therefore, delay tolerant networks have to tolerate partitions, long disconnections and topology unsteadiness in general. the main challenge for this environment is that conventional routing schemes did not work properly."
"in equation (5), avg ṽ is the number of other nodes that one node experiences in the network during the period of time from the end time of former fragment to the current time. nodes must first update avg ṽ and ṽ at each connection."
"although, epidemic routing achieves highest delivery of messages, but it makes no attempt to eliminate duplication, and the high delivery rate is at the expense of the network resource consumption, such as store buffer space and transmission bandwidth. vesw aims at drastically reduce the cost of such flooding by vibrant spray, epidemic routing uses to deliver a message is optimal, in the sense that it is the quickest one to deliver the message. easw exploits vibrancy of node and particular route."
"the physical medium used is the well known 802.11dsss phy with a data rate of 2 mbps. the mac protocol used is 802.11 mac protocol, configured in a manet mode on an urban terrain. more precisely we use only distributed coordination function (dcf) of the protocol."
"manet is a collection of wireless nodes that can dynamically form a network to exchange information without using any pre-existing fixed network infrastructure with rapid configuration of wireless connections on-the-fly [cit] . due to rapidly changing connectivity, network partition, higher error rates, collision interference, bandwidth and power constrain together pose new problems in network control-particularly in the design of higher level protocol such as routing and in implementing applications with quality of service requirements. routing in manets is one of the primary functions that, each node must achieve network functions at any point of time. routing enables connections between nodes those are not within each other's vicinity. to serve this purpose, various proactive, reactive and hybrid routing protocols are been proposed."
"epidemic routing, nodes act as a relay for other nodes. each node maintains a summary vector that indicates the set of packets which it has. when two nodes encounter within their transmission range, they exchange their summary vectors to check whether any packets are available, and basing on this information, each node can determine if other nodes have some packets that previously reach this node, and then they exchange packets which they don't have in common. thus packets are disseminated in broadcast mode, as if infectious diseases spread, and at least packets are expected to reach the destination eventually. this packetspread will continue until all the nodes have a copy of the packet or its ttl (time-to-live) expires."
"according the assumption about the random waypoint mobility, all nodes move independently. it will not do great help for improvement of delivery probability but lead to extra delivery overhead and energy consumption using the encounter history information in opportunistic routing. when a node has only one copy left, it switches to direct transmission, means that it will forward this message only to its destination."
"in spray and wait scheme binary spray and wait (bsw) is proposed to speed up the spray phase and improve the routing performance. in the spray phase of bsw, any node a that has more than one message copy hands over to another node b half of its copies when it encounters node b. it's proved that bsw can minimize the spray time and achieve optimal performance under the condition that all nodes move in an iid manner. it will therefore serve as our baseline. however, remaining energy and vibrancy information of all nodes are not considered during the spraying of message copies that may result in blind spraying and low efficiency in data gathering."
"assume that there are a total of four cells in a cellular system and two call services. one of the services is a \"single mode service\" while the other is a \"dual mode service\". the bandwidth occupied by the \"single mode service\" is fixed and it cannot be adapted. the \"dual mode service\" has two service modes, where one is an adaptive-rate service that has not been adapted, i.e., the \"dual mode full-rate service\", whereas the other service mode is an adaptive-rate service that has been adapted, i.e., the \"dual mode reduced-rate service\"."
"in proactive routing protocols viz., destination-sequenced distance vector (dsdv) [cit] and optimized link state routing protocol (olsr) [cit], routes between source and destination are maintained regardless of the data traffic. such a strategy avoids the need of finding routes for each message and is especially efficient when the nodes are relatively stationary and traffic is relatively heavy. unlike proactive routing protocols, reactive routing protocols [cit] viz., dynamic source routing (dsr) and ad hoc on demand distance vector (aodv), find route only when node attempts to send a message. it avoids the need of frequent link and route updates therefore substantially reduces energy consumption [cit] when the traffic load is light or the network mobility is high. hybrid routing protocol utilizes both reactive and proactive protocol mechanisms."
"however, previous studies of call blocking and dropping performance with cac have not considered adapting multiservices in cellular systems. mobile cellular systems can now support multiple call services, such as voice and video calls. the bandwidth of these call services is usually fixed. if the bandwidth of these call services can be adapted, the cac used by such adaptive multirate (amr) services may reduce the call blocking and dropping probabilities, thereby increasing cellular system utilization to meet a guaranteed level of qos. for example, g.722 can change the bandwidth of voice call services while multiple description coding (mdc) can vary the bandwidth of video call services."
"basic idea of zrp is based on the idea that querying can be done more efficiently than flooding, by directing route requests to target peripheral nodes. since neighbouring routing zones heavily overlap, each node may forward same query messages multiple times. thus leading to increase in the number of control packets. also there are chances that these query messages may be forwarded again inward instead of moving towards the destination. this unnecessarily adds the delay in the system."
"moving in the network, the node will encounter more and more nodes. in the real network situation we will add a number of new nodes from time to time. some nodes have been in the network for a long time, whose vibrancy of node are larger. however, new nodes vibrancy are smaller. therefore, we divide time into a series of fragments at the same length. we calculate vibrancy of node on each fragment and consider the influence of the vibrancy of node in former fragment on the vibrancy of node in the current fragment. we divide time fragments into n."
in this section the methodology is presented .this methodology is used in order to isolate the impact on network performance. we used a well known network simulator qualnet version 5.0.
"when delivery rate stay the same, we consider the relationship between network overhead and delivery utility. delivery utility is defined as the ratio of the number of messages received by destination nodes to the number of messages forwarded by relay nodes, as shown in equation (1)."
"from above graphs of fig. 10 thru fig. 158 we can observe that enhanced zrp has performed better than zrp to promise better throughput, end to end delay, jitter and number of bytes received. zrp was not up to the task and it performed poorly throughout all the simulation sequences, hence putting itself out of competition."
the evolution of wireless cellular systems over the last decade has created a demand for multimedia services with guaranteed levels of quality of service (qos).
"after studying the operation of zrp and enhanced zrp we can observe that, zrp uses transmission of excess control messages there by resulting in poor qos, by studying zrp operation of route discovery we observe that zrp uses additional time as it uses iarp, ierp and brp. thus leads to more number of control packets. when a destination node is not found in the local zone of the source node it initializes ierp. zrp takes time for inter communication between ierp and iarp. each node maintains routing table of their local zone. this adds unnecessary traffic in the network. this causes route acquisition delay. after discovering the route to the destination the data packets are encapsulated by two protocols. hence it takes more time for data packet to reach to the destination."
"where p denotes the total energy consumption of one node for receiving kr bits and transmitting kt bits, and e is a factor indicating the energy consumption per bit at the receiver circuit."
"during the past two decades, the use of biometrics for person identification has been a topic of active research [cit] . several schemes have been proposed by using fingerprints, face iris, retina and speech features, all of which can provide a fairly good performance in several practical applications [cit] . however, performance significantly degrades when they operate in an un-constrained environment. because there are practical applications that operate in un-constrained environments, several biometrics have been developed to carry out person identification in these environments. among them, gait recognition has received considerable attention [cit] . particularly, those gait recognition methods that do not depend on human walking models [cit], has been shown to significantly increase accuracy and reduced computational complexity by using information extracted from simple silhouettes of moving persons [cit] . in general, several aspects may degrade the performance of gait recognition methods, e.g., clothes, shoes, carried objects, the walk surface, time elapsed, and view angles. among them, the view angle, which corresponds to the angle between the optical axis of the capturing camera and the walking direction [cit], is an important factor because the accurate performance of most appearance-based approaches strongly depends on a fixed view angle [cit] ."
"we only found two published ags associated with publicly available annotated app review datasets. the problem we encounter is that either annotated datasets were not published or when they had been published it is unclear what annotation rules/guidelines were applied. in other domains, e.g., laptop and restaurant, the standard guidelines and benchmark datasets are contributed by the research community semeval to perform the task of product feature extraction and its evaluation. similar to the sevemal research community, the app review mining research community could contribute standardized guidelines and benchmark datasets to help researchers in the development of systems performing fine-grained sentiment analysis at app feature-level."
"note that we only simulated the application of our new ags on the labeled datasets. we expect that the application of the new ags by actual people could have resulted in more useful annotations of app features in the first place. the application of our new ags automatically removes app features that are longer than 3-words. however, in the direct application of our new ags, a longer app feature might simply have been annotated with fewer words rather than completely been removed as we did in simulation step 3. for instance, a 5-word app feature sorting functionality in board section annotated in app category 'social' of the shah-i dataset could be labeled as an admissible 2-word app feature sorting functionality."
"to study research question rq we use all given design variables 1 to 4 and adopt ccv as training procedure to explore the effects of the data processing steps simulating the changes in the ags. we assume that the quality of the training data annotations and the accuracy of the app feature extraction model are positively correlated. thus, we use model accuracy on the test set to assess the impact of a change in the ags. we train and evaluate crf-based app feature extraction models on all our annotated datasets after each data processing steps and assess their accuracies to approximate how each step affects the quality of the annotations."
"we adopt the conditional random field (crf) [cit] ), a supervised learning method to train the models for all our experiments. crf is a sequence tagging model which tags each word in the app review text with a label. [cit] we use the bio labeling scheme, where the tag b is used to annotate the first word of each app feature, i labels the rest of the words inside the app feature and the label o is used to tag all words that are outside of the app feature. we use an implementation based on crfsuite 10 [cit] on laptop and restau-rant product review datasets. [cit], they are summarized in table 2 ."
"in this paper, we study the impact of ags by controlling other design parameters as much as possible. we used four different labeled datasets annotated with two different ags. for the app feature extraction technique we adopted the supervised crf method. to our best knowledge, this is the first study that explores the impact of ags and labeled datasets for app feature extraction from user reviews. as a result of our study, we propose several changes to the existing ags to avoid the annotation of useless app features."
"we presented a two-phased technique for program slicing based on the formal executable semantics of a while programming language, given as a rewriting logic theory. the first phase, called language semantics specification analysis considered an exhaustive inspection of the language semantics to extract a set of side-effect language constructs. the second phase was to perform program slicing as term slicing, using the previously computed set of primitives, the input (term representation of the) program and the slicing criterion. both the formal definition of our language and the semantics-based slicing technique are implemented and tested in the maude system."
"results of every ann prediction are summarized into a table where w1 0 and w2 0 are the first and the second-layer bias indicators, # x i is the dimension of the model input, # hid is the number of the hidden neurons, # w is the number of weights, and % w is the percentage of the weights considering the maximum number as a reference."
"the intrinsic complexity of a modern software system imposes the need for specialized techniques and tool support, both targeting the system design and analysis aspects. it is often the case that these two aspects of the software development are inter-dependent. on the one hand, abstraction techniques are widely used solutions to reduce, in a systematic way, the size of the system under consideration. however this abstraction-based simplification process is usually dependent on quality and performance-driven refinements which, in turn, would benefit from tool support. on the other hand, the development of useful tool support requires sound techniques to ensure the correctness of the produced results. one possible solution to integrate techniques and tools development is to use a formal and executable framework such as rewriting logic [cit] . thus, a rewriting logic general methodology for design and analysis of complex software systems could and should rely on a formal executable programming language semantics to ground the development of both abstractions and tools."
"the results of this sensitivity analysis point out that while parameters like n st certainly depend on well-defined input parameters, others like q and h instead show a low correlation with the input parameters, and for this reason, they are probably not able to provide a general formula for predicting the pump performance in a reverse mode, knowing those of a pump in a direct mode. as a whole, the study showed how the application of two different methodologies from a mathematical point of view leads to absolutely consistent results."
"an estimation approach permits one to seek all possible models; nevertheless, it is necessary to identify those having a physical meaning. starting from this consideration, [cit] presented a comparison of different approaches that can be used for the prediction of a pat: a physics-based model; two models called \"gray,\" supported by literature data and based on a priori knowledge of the physical process but without a priori knowledge of the parameters influencing the process; and finally an evolutionary polynomial regression (epr) model using field data. values predicted by the four models and compared to the original literature experimental data [cit] showed a good agreement with the latter, even if the epr goodness is clearly well-founded on the input database dimension. rossi and renzi [cit], using a non-dimensional approach, adopted artificial neural networks to forecast both bep and pats performance in reverse mode by fitting operating data extrapolated from technical literature. however, the obtained formulas in both applications, even if showing a very good fitness, are extremely complicated and characterized by a polynomial structure with numerous terms that are difficult to interpret from a physical point of view."
"the fitness of every prediction model was based on the evaluation of coefficient of determination (cod), and epr-moga returned a number of models thanks to a pareto set that returned the best compromise between the model parsimony and the experimental data fitting. in the following tables, only the best models of every analyzed output and the relative cod, number of input candidates, and number of polynomial coefficients are summarized."
"the third row in each section of table 3 ( step 3: simulation ii) shows the performance of our models after removing app features that do not contain a noun. this step was motivated by the assumption that app features not containing a noun (such as running or runs) are too unspecific to be useful for the developers. since the sanger ags instruct to annotate implicit features represented by a single verb, we expected a significant drop of the number of app features for the sanger and shah datasets and also an over-proportional reduction of the number of singleword app features. surprisingly, it turned out that the number of app features dropped equally strongly for the guzman dataset, and for all datasets the portion of single-word app features also significantly decreased but not much as compared to step 2 (simulation i), while the type-token ratio slightly increased. the german dataset sanger still has a high portion of single-word app features (now 74%). the average number of app features per review narrows down after this step to the range [0.39,1.42] and is decreasing for all the datasets. overall, compared to the performance obtained after step 2 (simulation i), the recall remains roughly the same for all datasets. in terms of precision, we expected it to improve. if the annotated feature set contains short and vague verbal aspects that also would be used as non-aspect terms in the text (e.g. using or updating), it might be very difficult for the model to detect certain instances of these words as features."
"in this paper we investigate, from a semantics-based perspective, the interdependent relationship between the program slicing general technique and its afferent tool-the program slicer. modifications (i.e. extensions or abstractions) at the level of the programming language, and which are carried out at the level of the program, should be automatically reflected in the program slicing tool support. therefore, we propose a static technique for program slicing which is based on a meta-level analysis of the formal executable semantics of programming languages. our program slicing builds on the formal executable semantics of the language of interest, given as a rewriting logic theory, and on the source program."
"step (1) of our program slicing method covers these meta-executions of the semantics to identify the set of syntactic constructs which results in state updates. this coverage employs unification [cit] and an adaptation of the backward chaining technique [cit] . during the language semantics analysis, the algorithm unfolds the middle layer (i.e. the semantics rewrite rules) into a special tree, applying labels to the visited nodes. this label-based classification is used to identify the set of side-effect constructs and to prune the unfolding tree."
"it contains annotated app reviews in english language from six different app categories. most app categories contain reviews from one app only: angrybirds from games category, tripadvisor from travel category, picsart from photography category, pinterest from social category and whatsapp from communication category. the only exception is the productivity category which contains reviews from two apps: dropbox and evernote. [cit], 400 reviews were annotated for each app. however, from table it can be seen that the guzman dataset includes less than 400 reviews in each category. this is because the guzman dataset 8 only contains reviews with at least one annotated app feature."
"a separate discussion must be made for the results of the analysis conducted for the flow ratio q and head ratio h. the anns methodology found, respectively, a pareto front size of only four solutions for the q (table 6 ) and two for h (table 7), which, except one for q, were all characterized by a very low coefficient of determination. this situation was dramatic for h, highlighting how input data did not permit a fit for the returned models. starting from this result, an attempt was made by changing the transfer function from hyperbolic tangent to linear. this attempt gave slightly better results for h but worse for q, and the obtained results are summarized into tables 8 and 9, respectively. however, as verified in reference [cit], the obtained results for q and h did not satisfy in terms of prediction and accuracy."
anns methodology does not furnish a formula or a well-defined relation to describe a physical process but permits one to evaluate the real weight of each input candidate parameter.
"in the literature, much research [cit] ) is dedicated to automatic extraction of features from product reviews in the laptop and restau-rant domains. the best results have been achieved using supervised learning approach such as conditional random fields (crf) and convolutional neural network (cnn) [cit] . we know of only one study that used supervised sequence tagging model (i.e., crf) [cit] on german app reviews."
"we have at our disposal four annotated app review datasets: guzman dataset, sanger dataset and two versions of shah datasets. we present some characteristics of these datasets in table a of appendix 6 . note that we do not show data per individual app but aggregated per app category. each review dataset is characterized using the following information: a) the total number of reviews; b) the total number of sentences in all reviews; c) the total number of annotated app features in (tokens); d) the number of distinct app features (types); e) the number of app features consisting of a single word only; f) the number of app features consisting of at least two words; g) the type-token ratio of annotated app features (the number of feature types divided by the number of feature tokens)."
"the tool is started by loading the slicing.maude file available at http://maude.sip.ucm.es/slicing. it starts an input/output loop where modules and commands can be introduced. once the module in section 3 has been introduced, we have to introduce the sort of variables and the sorts where we want to detect side effects. with this information, the tool can compute the rules and the functions generating side effects:"
"as a whole, the common interest of part of the scientific community is still to define a general methodology for pump performance in a reverse mode, known as the optimal conditions of operation of a pump. table 1 summarizes some of suggested models to predict pat performance: flow ratio (q) and head ratio (h). expressions have been subdivided in chronological order and grouped into two classes, the first based on the bep parameters [cit] and the second based on the specific speed number (n s ) [13, [cit] ."
"all of the aforementioned studies related to app feature extraction use different techniques, review datasets and ags; therefore, the results reported in these studies are not directly comparable to each other. additionally, without having access to different datasets and ags, it is difficult to assess the quality of the annotations that were used to evaluate the systems. [cit] together with its ags and we use this dataset as one of the annotated experimental training sets (guzman dataset) in our study."
"after convincing ourselves that the simulated application of new ags actually results in more useful app feature annotations, we checked whether this effect also propagates to the set of extracted app features. table 5 shows the impact on the number of useful and useless app features in model's extracted app features, when training crf models with the original annotated datasets and when training crf models using the annotated datasets after the simulated application of our new ags. we picked the same app categories as before from each of the english datasets, i.e., from categories 'photography' (guzman), 'social' (shah-i), and 'game' (shah-ii). we manually classified each app feature as either 'useful' or 'not useful' and then compare how the numbers of useful and not useful app features change when simulating the application of our new ags. the actual type counts for each of the three apps are:"
"although both ags can be used to label the same information-features in app reviews-they have differences which may influence how well the data annotated with these guidelines can be used to train a model for automatic feature extraction. 1) using feature annotations not comprised of exact words used in the review text will make any automatic use of these annotations very difficult. although this practice is discouraged in the guz-man guidelines, it is not explicitly prohibited."
"we use a cross-category validation (ccv) training procedure in our study. the ccv training procedure assumes that the annotated training data consists of app reviews belonging to several different app categories. then the reviews of each app category are held out in turn and the model is trained on the reviews in the other app categories. finally the trained model is evaluated on the held-out reviews. we use cross-category validation instead of cross-app validation because both in guzman and shah datasets, with one exception we have the reviews of just one app in each of the app categories. in sanger dataset, although each category contains reviews from several apps, we do not have the app name annotations attached to each review and thus we could not separate the reviews of different apps into different subsets."
"the technical literature (see table 1 ) shows a considerable dispersion in terms of formulas to predict the performance of a pump in turbine mode. the parameters that influence the phenomenon in each formula not only change, but above all, the exponent with which to represent them is different for each of them. in the light of the above-mentioned studies, the present paper conducted a sensitivity analysis on a pat efficiency while knowing the pump efficiency in direct mode. sensitivity analysis has been carried out by comparing results of artificial neural networks (anns) and epr methodologies with the aim to understand the real weight of every input parameter on the evaluation of a pat performances while knowing those of a pump in a direct mode."
"we plan to extend this work on the following several directions. first, we incrementally analyze the impact on adding various side-effect constructs to the whilel language, such as pointers, exceptions or file-manipulation operations. second, we address other types of programming languages and their specific sideeffect constructs. for example, if we consider assembly languages, it happens that various arithmetic instructions visibly modify a particular register value, and invisibly affect subsequent conditions in the program, via modifications of special arithmetic flags (i.e. overflow, sign, etc). third, we improve the algorithm in the first step in our slicing technique in order to obtain a better (smaller) overapproximation of the produced set of side-effect syntactic constructs. we believe that pursuing these directions would further improve the current semanticsbased program slicing technique and produce a useful design and analysis tool for language developers."
"our study is restricted to the use of the crf model which limits app features to be annotated as consecutive words. therefore, when limiting annotations to a maximum 3-word app features, it might be impossible to annotate app features consisting of consecutive words; in such cases crf (or any other sequences tagging model) cannot be applied or we would have to drop those app features (or soften the rule of having maximum 3-words app features). for instance, a 5-word app feature edit pictures in a high quality can be reduced to the following two meaningful representations of 3-word app features: edit high quality or edit picture quality. however, both 3-word app feature representations are not consecutive."
"the anns methodology found a pareto front size of eight solutions for the head rate h bept (table 4 ). results highlighted again that the ionn model characterized by the maximum number of inputs and hidden neurons was also characterized by the higher cod value. irrefutable role of h bepp, which was present in all models, followed by the specific speed n sp and efficiency η bepp . the best model, indicated with a gray line (model 7), was selected by taking into account both its parsimony in terms of parameters and its prediction performance, also confirmed by the technical literature [cit] ."
"for example capture full resolution, decorating pictures and online scrapbooking seem to be clearly referencing to some functionality in the app of categories 'photography' and 'social'. aspects are not useful when they are too generic to be connected with a specific functionality (e.g., share or version 1.5.1). as shown by the study [cit], nonfunctional aspects of an app (e.g, easy to use) can be identified with high precision using language patterns. therefore, our concern in this study is to extract app functional aspects."
"we can also define addition between natural numbers. first, we define the operator _+_, where the underscores are placeholders and that has attributes stating that it is commutative and associative. then we specify its behavior by means of equations. these equations can be conditional, as shown in add1, where we check that the first argument is 0, and can use patterns on the left-hand side, as shown in add2:"
we denotes the specification s after these transformations. note thats does not contain the r i terms of the rules in s. we comment on this later when describing the first slicing step algorithm.
"note that the color code in it signifies that lborder is the stack varr, asr (i.e., red for lborder). the second iteration of the loop makes r curr the horn clause:"
"where insert just introduces the value at the end of the buffer. reading is performed by using the rule readr1. it tries to extract the next value from the buffer and, if it is not the err value, updates the state with it:"
"more specifically, our tool traverses the rules in the module indicated by the user to find the rules that modify the state of the terms modified by the side effects (env and rwbuf in our example). with these rules and using the predefined unification commands available in maude our prototype can generate and traverse the hypernodes, thus computing the first step of the algorithm in the previous section. for the second part, it checks the left-hand sides of these rules, discarding the information of the side effects terms to focus on the instructions. it then uses this information to traverse the initial term (containing the program we are analyzing) checking the terms that appear in each of these terms. if any of the variables given as slicing criterion are used, then the rest of the variables appearing in this term are added to the current set and the process is repeated until the fixpoint is reached."
"epr identified 11 non-dominated models for q with varying structural complexity and performance; however, it can be seen that there was a very low cod for each of the identified models. table 14 reports the main characteristics of these formulas. meanwhile, epr identified only five models for h and table 15 reports the main characteristics of these model. the retrieved models do not furnish cod with interesting values despite equation (27) for the first case and equation (30) for the latter being quite similar to those retrieved thanks to literature test rigs [cit] ."
"the rule inc1 also uses this update operator to increase the value of x in the state. the new value is computed by first obtaining the value v of x and then adding 1 using the auxiliary ap function, that applies the given operation (addition in this case) to the values:"
"we motivate our program slicing approach, starting with an alternative view on the language semantics. we elaborate on both structural and functional aspects of this view. structurally, the formal definition of the whilel language consists of three layers. at the top level there are the pure syntactic language constructs (i.e. the syntax), at the bottom there is the language state, while the middle layer contains the semantics equations and rewrite rules. this arrangement is important for the functionality of the definition. when we execute a program through these layers, its statements are decomposed into smaller syntactic constructs (i.e. found at the top layer) which, through transformations in the middle layer could result into state updates. in this way, a program execution establishes connections between the syntactic constructs and state updates, in other words which constructs yield side-effects."
"step iii removes all app features that are longer than three words. we believe that useful features cannot be too long because otherwise they become too specific and noisy. we attempt to simulate the change in ags that would limit the maximum length of an app feature to three words with a very crude heuristic that just removes the longer features from the dataset. although a better heuristic would be to develop a set of rules to shorten the app features appropriately we opted here for the simplest strategy, believing that it will be good enough for our purpose of testing the potential effect of such a guideline."
"we will illustrate what we mean by \"more relevant for software developers\" in two steps with the help of examples. in the first step, we demonstrate that the simulated application of our new ags actually produces an annotated dataset that contains a larger share of annotated app features that are useful to software developers. in the second step, we demonstrate that this positive effect of our new ags also propagates to the set of extracted app features."
"our research question rq started from the observation that the app feature annotations and thus subsequently also automatic feature extraction results varied considerably on different datasets, even though they used the same ags to annotate the app features and even when the annotated app reviews themselves were identical (shah-i vs shah-ii). we hypothesised that these differences are at least partly due to the ags that were used to label these datasets. we proposed several changes to the ags and evaluated the effect of their simulated application using the evaluation results of the crf modeling. the proposed changes in ags include (a) instructing to annotate only consecutive words as app features, (b) discouraging the annotations to the references of the app itself, (c) instructing to annotate only noun phrases as app features, i.e. every app feature must contain a noun, and (d) instruct to annotate app features with the length of maximum three words. when simulating the application of our ags, we were able to retain the precision of the app feature extraction. moreover, the annotated features in both training and test sets become more informative and less noisy."
"in order to guarantee a more general applicability of the prediction model for pat performance, the analysis is supported by an input literature database [6, 13, 16, 30, 31, 33, [cit] consisting of pumps operating in a range of specific speeds from 9 to 80 (rpm) and characterized by several impeller sizes."
"epr returned 11 models for q bept, where the first three are simple formulas, while the others are complex models and also difficult to explain from a physical point of view. for these reasons, table 11 summarized the first three and one of the remaining ones. however, by analyzing the obtained models, equation (13) has been chosen for its physical meaning; figure 3 compares literature data with that obtained by the selected formula."
"previously, several techniques have been used for automatic app feature extraction from app reviews, including (a) unsupervised topic modeling, (b) rule-based methods, (c) supervised machine learning approaches. while unsupervised and rule-based approaches only require annotated data for evaluation purposes, supervised machine learning methods also need it for training the model. in either case, the quality of the annotations can considerably affect the evaluation results. when the annotations contain many complex app features that are difficult to extract, then model performance will be artificially low, especially when these features are infrequent and thus not of much actual interest for app developers. on the other hand, when the annotations contain many short and frequent app features that are easy to detect but not informative for developers then model performance will be artificially high."
"the next few iterations of the loop make opr an element of both rmix and nose in the same way as lkup. we skip over these steps and explain from the iteration with the lborder stack containing upd, asr."
"the data shows for each of the three sample cases that the ratio between the number of useful and not useful app features is increasing when applying our new ags. we computed the percentages based on type counts of app features because there can be cases like, for example, the app feature editing. the app feature editing occurred 13 times in the app of category 'photography' before the simulated application of our new ags and five times afterwards. we assume that eight occurrences of editing were removed due to the guideline 'only annotate app features containing a noun', i.e., because after simulating the application of our new ags editing was predicted to be an app feature when it was used as a noun. note that the word editing when used as a verb is not helpful for software developers because it does not provide information about the purpose or object of editing and thus it is difficult to decide whether the mentioning of editing is related to the edit functionality as such or just a special situation in which something was edited. on the other hand, if editing is mentioned in the grammatical form of noun, it is more probable that whatever is said in the sentence with the word editing refers to the edit functionality in general. a similar case is pinning mentioned in the reviews of the app in category 'social'. here three of the seven original app feature predictions disappeared after simulated application of our new ags."
"the main goal of our study was to investigate the impact of ags and annotated data on extracting app features from app reviews and to improve existing ags such that (1) the performance of the app feature extraction task gets better in terms of f1 score and (2) the set of extracted app features is more useful to software developers. section 4 (results) presented the step-by-step impact to the performance of the app feature extraction when simulating the effects of changing the used ags. it turned out that with our proposed new ags, a small performance improvement over the baseline situation could be achieved. however, this is not the only advantage of our new ags. in the following we argue that not only the performance of the app feature extraction task can be improved but that the set of annotated and extracted app features itself is more relevant for software developers when using our new ags."
"where g j,k,v (x, y) is the (x, y)-th gray value of the gei of j-th sequence captured at the v-th view angle, which corresponds to the k-th class; b j,k,v,t (x, y) is the (x, y)-th value of the binary silhouette of the t-th frame of the sequence; k, j and v are number of classes (persons), sequences per class and view angles per sequence, respectively; and n f is the total number of frames in the walking cycle. figure 2 shows a set of normalized binary silhouette images representing a walking cycle of two different persons, and the corresponding geis. figure 2 . examples of gait energy image, last column, computed by using a set of normalized binary sihouette images representing a walking cycle."
"step (2) takes the term representation of the input program together with the results from the previous step, represented as subterms, and does program slicing through term slicing."
"even though we used the translated sanger ags when annotating the shah dataset, the performances of shah-i-based and shah-ii-based models are very different. the performance of the shah-iibased model is even worse than the performance of the guzman-based model where a different ag was used. we speculated that one possible reason for the difference in performance could be that the sanger ags explicitly instruct annotating references to the app itself as a feature. following this instruction automatically increases the number of single-word features and lowers the type-token ratio as the repeated mentioning of the app itself increases the token count but not the type count. when inspecting the shah-ii dataset, we noticed that the annotator seemed to have ignored this instruction. since the frequent annotation of references to the app itself in a review seems to artificially boost the performance of the feature extraction models, while it does not have any practical value to correctly predict the occurrence of a feature referring to the app itself, we decided to remove the annotations of app-references in our datasets."
"in addition to the two datasets available from other researchers, we created the shah dataset. the app reviews included in this dataset were selected by randomly sampling 500 [cit] for their study. the app categories and apps in each category are the same as in the guzman dataset. [cit], since each app has its own user rating distribution, we used a stratified sampling procedure to sample the reviews using the distribution over ratings as stratum. for measuring the inter-annotator agreement, we adopted the dice coefficient [cit], which ranges between 0 and 1 where 1 means total agreement and 0 total disagreement. the dice coefficient value between the two annotators was 0.28 which denotes a low agreement between the annotators. because of that, we decided to treat the annotations of both annotators as different datasets resulting in two annotated shah datasets: shah-i and shah-ii containing the annotations of the first and the second annotator respectively."
app features in the upper part of table f in appendix 6 correspond to a random sample of those app features that remained in the set of app features after simulated application of the new ags. the numbers behind each app feature correspond to the token count before and after the simulated application of the new ags. in some cases the token number changed. app features in the lower part of table correspond to those app features that were completely removed from the set of app features after simulated application of our new ags. the ideal impact of the simulated application of our new ags corresponds to removing all useless app features and keeping only the useful app features. we calculated the impact of our ags based on the numbers of manually classified 'useful' and 'not useful' app features in three app categories before and after the simulation of new ags (see table 4 ). the actual numbers (based on type count) for each of the three apps are as follows:
"several approaches have been developed for gait representation. a suitable approach is the spatio-temporal gait representation, called gait energy image (gei), proposed by han and bhanu [cit], which extracts the human silhouettes of a walking sequence. then, the extracted binary silhouettes are preprocessed to normalize them such that each silhouette image has the same height and their upper half is centered with respect to a horizontal centroid [cit] . a gei is obtained as an average of the normalized binary silhouettes, as follows [cit] :"
"the sanger ags 5 [cit] to annotate app features, subjective phrases, and relationships between them. we translated these guidelines from german into english. sanger ags define as an app feature \"anything that is part of the application or in some form connected with the app\". this includes existing and requested app features, bugs and errors as well as entities referring to non-functional features such as usability, design, price, license, permissions, advertisements and updates. the guidelines explicitly instruct to annotate the mentions of the app itself as a feature. instructions also ask to annotate implicit features represented by a single verb such as runs. annotators are encouraged to keep the annotated features as short as possible although a particular length limit is not set. the sanger guidelines specifically require not to include function words into annotated app features, which probably also influences the length of the annotated app features. although no explicit mention about annotating consecutive vs non-consecutive words as features is made, all example features only consist of consecutive words."
"based on the statistics, several differences between the datasets are visible. firstly, the guzman dataset differs strongly from the other datasets by having multi-word features twice as often as single-word features. in contrast, the shah-ii and sanger datasets have more single-word features than multiword features and in the shah-i dataset the numbers are balanced. several reasons may account for these differences, including how each annotator interpreted the ags given to them, but we believe that this difference might also characterize the differences in the ags themselves."
"epr returned eight models for h bept, where the first three are simple formulas while others are complex models and also difficult to explain from a physical point of view. for these reasons, table 12 summarizes the first three, and for the sake of example only, one of the remaining. however, analyzing the obtained models, equation (18) has been chosen for its physical meaning; the selected model is also quite close to what is reported by the technical literature [cit] . epr returned eight models for η bept . in this case, also, the first formulas, except the first one characterized by a very low cod, are rather simple while the remaining are complicated. for these reasons, table 13 summarized the more interesting obtained models, it equation (22) was selected both for its simplicity and for its good compromise between parsimony and data fitting."
"the algorithm in fig. 2 computes the set of basic syntactic language constructs which may produce side-effects, by inspecting the conditions and the right-hand side of each rewrite rule in the definition. for this inspection, we rely on unification [cit] and a backward chaining technique [cit] . the algorithm unfolds the semantics rewrite rules into the inspection tree it such that the final tree contains a superset of all rules which can be called during a rewrite execution \"rew"
"the second slicing step algorithm terminates, because there exists a finite set of program subterms, and it produces a valid slice, because it exhaustively saturates the slicing criterion. moreover, the result is a minimal slice w.r.t. the set of side-effect syntactic constructs given as argument. however, the obtained slice is not minimal mainly because the set of side-effect syntactic constructs obtained in the first slicing step is already an over-approximation."
"the second row in each section of table 3 ( step 2: simulation i) shows the performance after filtering out app features referring to the app itself. applying step 2 simulates a change in the ags, i.e., the explicit mentioning that references to the app itself should not be annotated. as can be seen in figure a in the appendix 6, in step 2, all datasets have a high type-token ratio in the range [0.68, 0.78]. all english datasets have a low share of single-word features in the range [35%, 38%], while the one german dataset (sanger) still has a relatively large portion of single-word features (77%). as expected, all results on datasets with previously high performance (shah-i and sanger) drop considerably, especially the recall on the shah-i dataset."
"the sanger dataset 9 [cit] . it contains reviews in german language. the sanger dataset has the same number of reviews in each app category. the reviews in each category come from 10 to 15 different apps but the origin of each particular review is unknown to us. in addition to app features, this dataset is also annotated with subjective phrases and relations between features and subjective phrases. however, we only use the annotated app features and ignore all other annotations."
"electric vehicles (evs), as an alternative to fuel vehicles (fvs), are considered zero emission cars. this statement is actually a half-truth; in fact, while evs drastically reduce polluting emissions, more evs will require more and more energy and the question becomes: how will this energy be produced? the european environment agency (eea) [cit] has tried to make assumptions about the future number of evs and what their impact could be. [cit] will be 80%; [cit] will represent half of the vehicles in circulation. it is only a hypothesis, but in future, evs will be widespread and governments, even if the times remain unknown, will have to face the following problem: the fvs will be replaced by evs and the electricity demand will increase very fast."
"2) annotating non-consecutive app features, allowed in guzman guidelines, restricts the types of models that can be used. in particular, sequence tagging models like conditional random fields and recurrent neural networks, which produce state-of-the-art results on the feature extraction task [cit], can only process features consisting of consecutive words."
"anns represent a data-driven technique useful to model non-linear relationships and, more generally, complex phenomena barely modeled by classical methods. [cit], 11, 3497 5 of 17 does not return explicit equations but conducts a sensitivity analysis with the aim to quantitatively describe the observed phenomenon, obtaining one or more output given an input data set."
"the proposed gait recognition framework consists of three stages: computation of geis, joint model estimation, subspace learning using dlda and person recognition, as shown in figure 1 . a detailed description of each of these stages is described next."
"the language specifications, the valid term runpgm(x : ls, y : p s), and the set a of side-effect-sources (pre-computed based on p s). output: the basic syntactic constructs (non-recursive operators of sort ls) which induce side effects in the program state (of sorts p s). rp\", where rp is a ground term with runpgm as top operator. more to the point, we assume that there exists in s an operator runpgm which is used for executing programs based on the language semantics specification s. note that usually runpgm contains as arguments the program term and the initial program state, i.e., runpgm is defined over ls (language syntax) and p s (program state) sorts. in other words, the inspection tree is an over-approximated result of a rule reachability problem."
"this paper proposed a framework for view-angle invariant gait recognition that is based on the estimation of a single joint model. the proposed framework is capable of classifying geis computed from sequences acquired at different view angles. it provides a higher accuracy, with a lower computational complexity than other previously proposed approaches. the estimated joint model used in the framework, which is based on dlda, helps to reduce the under-sampling problem with remarkable results. evaluation experiments indicate that it is possible to obtain a projection matrix independently of the gallery subset, which allows us, in several practical applications, to include new classes without the need for recalculating the projection matrix. the evaluation results also show that proposed scheme improves the performance of several previously proposed schemes, although its performance still degrades when the incoming angle and the gallery angle are different. therefore, in the future it should be interesting to analyze the possibility of developing a gait recognition scheme based on a global model which would be able to keep the same performance independently of the difference between the incoming and gallery angles."
"in summary, we can state that by simulating the application of modified ags we achieve a feature prediction precision comparable to that received after the application of the original guzman and sanger guidelines. the advantage of the models created based on annotated datasets achieved by simulating the modified guidelines is that the predicted app features are more useful for developers since they are crisper (only one to three words of length) and correlate better with actual app features than with pseudofeatures such as references to the app itself. the new rules included in the modified (improved) guidelines can be summarized as follows:"
"exact match requires the predicted and annotated app features to match exactly. for instance, if the annotated feature is to upload video then in order to count a match the predicted app feature must consist of exactly the same words. if the model predicts upload video as feature leaving the particle to untagged the prediction is counted as false positive under the exact match scheme."
"we use the standard approach to specify the whilel programming language in rewriting logic. first, we define the (abstract) syntax, followed by the language configuration (i.e. the necessary semantic entities to define the behavior) and the language semantics (i.e. equations and rewrite rules)."
"partial match allows a mismatch when comparing predicted app features with human-annotated features in the evaluation set. we allow a difference of one word. under the partial match scheme, the predicted feature upload video will be counted as true positive even if the human-annotated feature is to upload video, whereas the predicted feature video would be a false positive because it differs from the humanannotated feature by more than one word. similarly, a predicted feature failed to upload video would be counted as true positive under partial match but an even longer predicted feature like failed to upload video to will be counted as false positive."
"our algorithm allows label propagation under certain conditions, with a labeled node summarizing the information of its corresponding subtree. the results of the language semantics analysis are used as contexts in the second slicing step to infer a safe program slice. note that the termination of the algorithm in fig. 2 is ensured by the fact that the specification has a finite number of rules, and that any rule in it that was already labeled is not unfolded anymore. notice also that the algorithm is independent of the order of the labels in a hypernode. in fact, we consider the labels as a list just for ease the presentation, but hypernodes can be just considered sets of labels, since the algorithm relies on the existence of a rule label generating side effects to work."
"the anns methodology found a pareto front size of nine solutions for the flow rate q bept (table 3) . results reveal again that the ionn model characterized by the maximum number of inputs and hidden neurons was also characterized by the higher cod value and the solutions were characterized by an 89% average reduction in total parameters and the complete set of inputs was selected only once. results revealed an irrefutable role of q bepp that appeared in all models, followed by the efficiency η bepp . on the whole, models having q bepp and η bepp as input variables revealed higher performance in terms of parsimony. the best model, indicated with a gray line (model 7), even if not characterized by the higher performance, was selected by taking into account both its parsimony in terms of parameters and its prediction performance."
"we evaluate all results by computing precision, recall and f1-score of the predicted app features. since app feature annotations themselves may be noisy and ambiguous, which manifests itself in low agreement between annotators (for instance [cit] reported an agreement of 53% on annotated app features), we adopt type-based evaluation methods using both exact and partial matching between predicted and human-annotated features (see part 4 in table 2 )."
"the rest of the paper is organized as follows: section 2 presents the related work. section 3 introduces maude and presents an example that will be used throughout the rest of the paper. section 4 describes the slicing algorithm and the main theoretical results, while section 5 shows our maude prototype of the technique and outlines its implementation. finally, section 6 concludes and presents some subjects of future work."
the second main difference manifests itself in the average number of app features per review. this quantity is largest for the guzman dataset and smallest for both shah datasets with the sanger dataset falling in-between. we attribute this difference to fact that the guzman dataset only consists of reviews that contain at least one annotated app feature while the other datasets may also contain reviews without a single annotated app feature.
"epr returned 11 models for qbept, where the first three are simple formulas, while the others are complex models and also difficult to explain from a physical point of view. for these reasons, table 11 summarized the first three and one of the remaining ones. however, by analyzing the obtained models, equation (13) has been chosen for its physical meaning; figure 3 compares literature data with that obtained by the selected formula."
"in this framework, and considering the complexity and difficulty to define a general law for the bep of a pat, a beneficial strategy is to adopt an estimation approach for the pat selection that investigates the role of the candidate input parameters with the aim to define the model structure."
"before we describe the contents of table 3, we summarize characteristics of the labeled datasets used in our study before and after each processing step. the exact numbers can be found in figure a of appendix 6 . the annotated datasets at the baseline, i.e., before applying step 1 (pre-processing) correspond to those described in section 3.2. several phenomena can be observed when comparing the evolution of dataset characteristics from before step 1 to after step 4. due to the nature of the data processing steps, the numbers of app features steadily decrease in all datasets, both token-wise and type-wise, and only for the sanger dataset, the portion of single-word features stays high (with a small reduction from 0.84 to 0.76). one explanation for the high portion of single-word features in the sanger dataset could be that the german language allows for noun compositions replacing multiple-word noun phrases. in the following, we highlight interesting results shown in table 3 . for each experiment, the table shows precision, recall and f1-measure when applying the evaluation procedures exact match (type), and partial match (type) as described in section 3.4. the first row in each section of table 3 (step 1: pre-processing) shows the performance after filtering out non-consecutive app features and removing reviews that do not mention app features. for all datasets precision is consistently better than recall for both evaluation procedures. partial matching, as expected, yields better performance than exact matching. the performance varies largely between datasets. the models built using the guzman and shah-ii datasets clearly perform worse than those built using the shah-i and sanger datasets. when looking at the dataset characteristics, one sees the following similarities between the datasets on which the models perform better as compared to the datasets where the models perform worse:"
"in conclusion, the results of this study may represent a starting point for future research, thanks to experimental apparatus and numerical approaches, that will be able to make use of this sensitivity analysis and to focus its efforts on the most influencing input parameters, such as the specific speed n st, on the evaluation of pat performances."
"we created two new labeled datasets shah-i and shah-ii in english, using the english translated version of the german sanger ags. the translation from german to english of sanger guidelines is performed using google translation service. in order to make sure that the translated guidelines have sufficient quality to be used for the annotation of reviews in english language, one of the author of this paper, who is a native german speaker and have a full command of the english language, read the english translated version of the ags and found it adequately accurate for the annotation task."
"the validity of our results depends partly on the reliability of the annotations of the shah-i and shah-ii datasets. in addition, the assessment of the usefulness of the results produced when using our simulated ags depends on the reliability of the subjective classification of annotated and extracted app features into 'useful' and 'not useful'. since each of these tasks was performed by one person, reliability might be limited. however, since we not only used our own annotations (i.e., datasets shah-i and shah-ii) but applied our analyses also to datasets published in the literature and the trend of our results was similar for all our used datasets, we believe that the existing limitations of reliability for the mentioned tasks is not a major threat to validity of our results."
"starting from the state of the art and the literature data, anns and epr models have been applied for predicting pats performances. anns and epr are both nonlinear global stepwise regression approaches providing a sensitivity analysis and a symbolic formulation, respectively, of the relation between inputs and output datasets. the analysis has been supported by an input literature database [6, 13, 16, 30, 31, 33, [cit] including 33 pumps operating with a wide range of specific speeds and impeller sizes. the aim of this study has been to forecast bep performance of a pat starting from the operative data referred to bep of a pump in direct mode. the input values for model training are represented using flow rate (q bepp ), water head (h bepp ), efficiency (η bepp ), and specific speed (n sp ) related to the bep in direct mode; whereas output values for model training are represented using flow rate (q bept ), water head (h bept ), efficiency (η bept ), and specific speed (n st ) related to the bep in turbine mode, as well as flow ratio (q) and head ratio (h). on the whole, an input dataset of 132 records and 198 output data points were adopted into the study."
"anns models, even if able to describe complex phenomenon learning from data, do not permit the return of an explicit formula and do not provide a clear relationship between input parameters and output. an alternative is represented by epr techniques [cit] that use an evolutionary search to determine the structure of the polynomial expression and the least squares approach to determine the regression coefficient."
"3) [cit] to learn to extract both app features and their relations to subjective phrases. in the context of plain app feature extraction these features can be considered pseudo-features, as they not give any useful information to the app developers. 4) similarly, the instruction in sanger guidelines to annotate standalone abstract verbs such as runs is probably motivated by the joint task of learning both app features and subjective phrases. in the context of app feature extraction these aspects will likely cause problems because they are difficult to distinguish from other generic verbs not labeled as app features. also, these very generic app features are likely of very little value to the developers."
"x p is fed into the knn stage, where x p is compared with the features vectors x g (s) stored in the database. next, the distance between the input vector and those contained in the gallery is estimated, keeping the k vectors x g (j) with the smaller distance. finally, the class label of the input to which the gei belongs is the class with the larger number of previously estimated k projected vectors. the classification process is illustrated in figure 8 ."
"in this section we discuss the semantics-based program slicing. this approach consists of two steps, namely the language semantics specification analysis and the program slicing as term rewriting. we introduce the algorithm for the semantics based analysis and present its execution on the whilel language case study. we end this section with the results of the second step of the program slicing algorithm, applied on the example program in fig.1 (left) . we consider s a specification of a program language semantics given in rewriting logic where we make the distinction between the languages syntax and the program state, via their different sorts in s. for example in the semantics of the whilel language described in section 3 the language syntax is given by the sort com, while the program state is formed by sorts env and rwbuf."
"unlike what was found for other parameters and as already happened for the ann predictions, the application of epr to q and h did not provide useful results."
"maude functional modules [6, chap. 4], introduced with syntax fmod ... endfm, are executable membership equational specifications that allow the definition of sorts (by means of keyword sort(s)); subsort relations between sorts (subsort); operators (op) for building values of these sorts, giving the sorts of their arguments and result, and which may have attributes such as being associative (assoc) or commutative (comm), for example; memberships (mb) asserting that a term has a sort; and equations (eq) identifying terms. both memberships and equations can be conditional (cmb and ceq). maude system modules [6, chap. 6], introduced with syntax mod ... endm, are executable rewrite theories. a system module can contain all the declarations of a functional module and, in addition, declarations for rules (rl) and conditional rules (crl)."
"since the size of x is too large, a dimensionality reduction method must be used. dlda is a suitable approach, because it is effective in separating classes and reducing the intra-class variance, while reducing the dimensionality. the discriminant properties of the dlda ensure that the classes defined by different view angles can be discriminated well enough. in other words, when the training set contains several view angles, the discriminant properties of dlda can effectively separate the classes represented by the different view angles in the projected subspace; thus allowing for the characterization of query view angles even if they are not included in the training set [cit] . thus, the dlda is used for estimating a joint projection matrix w from the input matrix x."
"in some cases it is not fully clear why an app feature was removed or kept. these cases could be due to inaccuracies of the pos tagger used in step 2 of the simulated application of our new ags. for example, it is unclear why only two out of three occasions of free were removed in category 'photography' as it is hard to imagine a context in which free could be considered to be a noun in a review text. overall, our new ags removed most of the useless app features in app categories 'social' (shah-i) and 'game' (shah-ii). when removing the not useful app features, the lower performance (53%) on app category 'photography' (guzman) is due to a large number of annotations referring to mobile devices, app versions, app updates and non-functional app features."
"the paper conducted a sensitivity analysis on the input parameters that most influence a pump performance in reverse mode and offers a comparison between anns and epr methodologies in order to increase knowledge about different models for the prediction of pat behavior. literature data summarized several studies based on experimental tests or numerical application, and overall, these methods can be subdivided into a first category based on the best efficiency point (bep) parameters and a second one based on the specific speed number n sp . starting from this knowledge, the paper provides a numerical analysis with the aim to understand the real weight of every input parameter on the output ones."
"overall, the paper, thanks to the application of anns, highlighted the real weight of each input parameter on the outputs, and this is certainly an advantage and benefit to the scientific research on this topic in the near future. whereas, epr application returned general formulas with a high coefficient of determination values, the selected formulas for every output parameter had values of cod of about 0.75, except for the specific speed n st, where the value was equal to 0.95. both methods were not able to relate the flow ratio q and head ratio h to the input candidate variables, in agreement with reference [cit], and this should mean that it is probably necessary to deepen this aspect with wider experimental tests."
"we train and evaluate supervised crf models on all datasets, i.e., the guzman dataset, the sanger dataset, and the datasets annotated by our student annotators using cross-category validation (ccv) settings. the ccv training regime assumes that reviews of different app categories share enough common information that a model trained on the reviews belonging to one set of apps or app categories will generalize to the apps or app categories whose reviews the model has not seen during training. in this training regime, we select to hold out one app category and train the model on the app reviews of all other categories. finally, we test on the app reviews of the heldout app category. this procedure is repeated until all categories have been held out in turn. then we use the average feature extraction performance of these models as a proxy for evaluating the goodness of the ags. we simulate several changes in ags and assess their effect using the performance of the predictive crf modeling. using this procedure, we are able to propose several changes to the app feature ags that improve the quality of the annotated app reviews for both training and evaluation purposes."
"anns models, even if able to describe complex phenomenon learning from data, do not permit the return of an explicit formula and do not provide a clear relationship between input parameters after an initial selection of the number of hidden neurons (s), the input matrix, and the transfer function (k), the training is performed by solving a least squares non-linear optimization problem using the levenberg-marquardt approximation [cit] and the adaptive search direction [cit] . the core of the modeling problem concerning the ann construction is that, normally, the user has to select the dimension of the model's input and its components; therefore, in this application the selection of both the model's input and the number of hidden neurons for iodnn is a combinatorial problem because, for a given accuracy, there is a great number of possible combinations to examine in order to look for the optimal solution in terms of the coefficient of determination (cod). finally, all possible results are selected by minimizing three objective functions: the number of input variables, the number of hidden neurons, and (1 − cod)."
"step (2) uses the resulting set from step (1) to extract safe program slices based on a specified set of variables of interest. though our slicing technique is general, we exemplify it on the maude specification of the classical while language, named whilel, augmented with a side-effect assignment and read/write statements. next we present a quick, high-level overview of the semantics-based slicing methodology, grounded on the design of a formal executable semantics."
"type-based evaluation counts and evaluates each app feature type only once, regardless of how many times it occurs in the review texts. in order to cluster together different instances of the same app feature type, the features are first lemmatized using snowball 13 stemmer available in nltk library and then matched based on their lemmas. the type-based evaluation procedure is unbiased by the frequencies of the single app feature types. while the token-based evaluation measures can become artificially high when the annotated training and test set contain a single highfrequency simple one-word app feature, the typebased evaluation gives equal credit to all different app features, regardless of their frequency."
"with the aim to discover a relationship between the specific speed number n st, the flow ratio q, and the head ratio h, the candidate input data set the model structure adopted in this study as the following:"
"the slicing command follows the notation shown in the previous sections. the keyword slice is followed by the program we want to debug, the keyword wrt and the list of variables that will be used by the second step of the algorithm described in the previous section. the tool outputs the label of the rules inducing side effects and the name of the variables that have been computed during the slicing stage:"
"our procedure for program slicing consists of the following two steps: (1) a generic analysis of the formal executable semantics, followed by (2) a data dependency analysis of the input program."
"the anns methodology found a pareto front size of 11 solutions for the specific speed number n st . full details of results are reported in table 2 . results revealed that the ionn model characterized by the maximum number of inputs and hidden neurons was also characterized by the higher cod value; however, models more parsimonious in terms of hidden neurons and inputs exhibit optimal performances and the difference between the first and the remaining was irrelevant. moreover, solutions were characterized by an 88% average reduction in total parameters and the total inputs were selected only twice. for the sake of clarity, the sensitivity analysis revealed a preeminent role of n sp that appeared in all models, followed by the water head h bept and efficiency η bept . these data confirm the decisive role that n sp exercising on n st, also confirmed by literature [cit], and starting from this consideration, the best model, indicated with a gray line (model 8), has been selected by taking into account both its parsimony in terms of parameters and its prediction performance."
"thirdly, also the type-token ratio of app features is largest for the guzman dataset indicating that the proportion of distinct app features is largest in this dataset. this can be explained by the large number of multi-word app features: the longer the features the more likely they consist of a unique sequence of words."
"in detail, the paper makes use of a multi-objective genetic algorithm (moga) strategy to address the optimal design of ann [cit] with the aim of finding the optimal trade-off among parsimony and accuracy for the returned models. in addition, epr has been applied with the aim to identify a relation between some of the input candidates and every output parameter."
"during classification, the system uses a gei of the person to be identified, x pg, which is projected into a dimensionally reduced space, using equation (16), as follows:"
"the ann approach, while containing the intrinsic limit of not providing a well-defined formula, gives the user the possibility to identify the structure of the best model, emphasizing the correlation among the most influential input factors and the output [cit] . the epr technique, instead, is able to provide a clear formula, but in this study, this research was not simple and understandable in all cases. the use of one methodology did not exclude the other because while ann needs a preliminary selection of the transfer function, the latter does not need an a priori knowledge of the model structure."
"where s is any of the j geis corresponding to any of the k classes from any of the v view angles, available in the gallery set. figure 7 shows the block diagram of the gallery estimation process."
"the epr technique is a two-stage technique for constructing symbolic models: structure identification and parameter estimation. the epr searches for symbolic polynomial structures in the first stage using a simple genetic algorithm (ga) and estimates constant values of polynomials by solving a least squares (ls) linear problem in the second stage, thus assuming a biunique relationship between a structure and its parameters [cit] ."
thus z unitizes s (b) and reduces the dimensionality from d to r. let us now diagonalize matrix z t s w z using the pca as follows
"the future road transport and electric power sectors will become more closely linked if there is a wide uptake of electric vehicles in the european union (eu). recent findings from the eea show that, if a hypothetical 80% [cit] will be electric, an additional 150 gw of additional electricity generation capacity would be needed across the eu. the risk, however, is that despite the great advance in mobility, global emissions will not drop and pollution will double. if most world governments, in fact, will keep relying on fossil fuel, the problem of pollution will shift from cars to power plants. the european environment agency suggests covering the additional 150 gw required by the advanced scenario for powering evs with renewable energies with the following subdivision: 87 gw from wind power, 45 gw from photovoltaic, 24 gw from hydropower, and 13 gw from biomass [cit] ."
"app marketplaces provide app users a channel to submit feedback in the form of a review. users in these reviews provide valuable information such as feature requests, bug reports, user experience, and evaluation of app features [cit] . the analysis of opinions expressed about different features of an app in user reviews offers opportunities and insights to both app users and app developers. for app developers, it is useful to monitor the \"health\" of app features in the context of release planning [cit] as well as to evaluate product competitiveness and quality [cit] . from the users' perspective, such information helps decide which app to select from a wide range of competing apps. both apple's app store and google's play store receive enormous amounts of reviews every day rendering a manual analysis infeasible and demanding automated methods. one standard approach towards automation is to generate sentiment summaries of a product at feature-level involving two steps [cit] : 1) identification of app features (also called aspect terms or opinion targets in the opinion mining literature) in user reviews, and 2) determination and aggregation of sentiments expressed on product features identified in the previous step."
"in order to avoid overfitting of returned models to training data, epr tries to exploit the known principle of parsimony by finding the optimal compromise between the model simplicity and the accuracy of the regression-based returned model. epr can achieve this purpose by penalizing the complexity of the returned expressions, controlling the variance of coefficients a j with respect to their values, and, finally, controlling the variance of polynomial terms with respect to the variance of residuals. once this procedure has been completed, the user obtains a set of optimal model solutions of increasing complexity and different accuracy according to the above-described reasoning. in order to fully exploit the abilities of the epr, the set of candidate exponents needed to build the matrix es has to include the exponent 0 among others. this allows the search procedure to include in the best formulas the most important inputs among those available."
"exploiting the fact that rewriting logic is reflective, a key distinguishing feature of maude is its systematic and efficient use of reflection through its predefined meta-level module [6, chapter 14], a feature that makes maude remarkably extensible and that allows many advanced metaprogramming and metalanguage applications. this powerful feature allows access to metalevel entities such as specifications or computations as usual data. in this way, we can manipulate the modules introduced by the user, develop the slicing process, and implement the input/output interactions in maude itself."
"the last statement highlights a very interesting question in terms of water resources because the hydropower generation, which is probably the oldest renewable energy source [cit], is nowadays extensively exploited and the energy produced is already sent into the electric grid. hydropower is already a mature technology in europe, with an estimated total installed capacity of 294 gw [cit] . most"
"the rest of this paper is organized as follows. section 2 of this paper summarizes the adopted methodology, section 3 describes the considered case study, and finally section 4 presents the obtained results and a comparison between two adopted methodologies."
"the same table shows how there is still no clarity on this issue, expressions are often very different from each other in numerical terms, and the parameters on which they depend on are not always the same."
"this update is in charge of the upd equation, that removes the variable from the state (if the variable is not in the state it remains unchanged) with the remove function and then introduces the new value:"
"the guzman ags 4 [cit] to annotate their evaluation set. these ags define an app feature as a description of specific app functionality visible to the user (such as uploading files or sending emails), a specific screen of the app, a general quality of the app (such as time needed to load or size of storage) or a specific technical characteristic (e.g. a network protocol or html5). the ags encourage to annotate the exact words used in the text but do not enforce it. the guidelines explicitly allow for annotating app features consisting of nonconsecutive words."
"the performance results shown for steps 1 to 3 were achieved with annotations of app features (aspects) consisting of any number of words. since long aspects potentially have a negative effect on model performance, we investigated whether imposing a maximum length could achieve better performance. figure 1 summarizes the outcomes of our experiments. for the sanger dataset, the performance was uniform across all choices of cutoffs. therefore, we only show the average performances of the three english datasets. the two plots shown in figure 1 show the minimum, maximum, and average f1-score for app features containing a number of words not greater than 1, 2, 3, 4 and with infinite length. the upper plot corresponds to exact type-based evaluation, while the lower plot represents partial type-based evaluation. in both plots, the best average performance is achieved when the app features consisting of more than three words are removed. the performance of our models after limiting the number of words in app features to a maximum of three words is shown in the last row of each section in table 3 ( step 4: simulation iii-3). the effect on performance is uniformly positive for all datasets. the precision for partial typebased evaluation is in the range [68%,82%]."
"the idea behind view transformation approaches is to transform the features vectors from one domain to another by estimating the relationship between the two domains. these transformed virtual features are then used for recognition [cit] . view transformation approaches do not require synchronization of gait data of multiple views of the target subjects. therefore, these approaches are suitable for cases where the views available in the gallery and probe sets are different [cit] . these approaches may employ singular value decomposition (svd), e.g., [cit] or regression algorithms for the matrix factorization process during the training stage [cit] . the principal limitation of these approaches is that the number of available images is limited to a discrete set of training views and recognition accuracy degrades when the target view and the views used for training are significantly different."
"in this section, we present the results of our study and answer the research question (rq). for better readability, we only show aggregated results for each dataset in table 3 . detailed results at app category level can be found in the appendix 6 (tables b to e)."
"step 1, it seems that the language used in the review datasets (german in the case of the sanger dataset and english in the case of the shah-i dataset) does not have a distinguishing impact on model performance. the impact of the two ags seems to be mixed."
"table f in appendix 6 shows samples of app features in the original labeled datasets and in the annotated datasets after the simulated application of our new ags. we randomly picked one app category from each of the english datasets, i.e., in categories 'photography' (guzman), 'social' (shah-i), and 'game' (shah-ii). we manually classified each app feature as either 'useful' or 'not useful' and then compared how the numbers of useful and not useful app features change after simulated application of our new ags. not useful app features have bold font."
"the main limitation of our study is the simulation of the new ags on the labeled review datasets which resulted in removing a number of app features rather than reformulating them according to the new guidelines. regardless of that, we believe that the real application of the new ags by human annotators would have produced a set of app features that are useful and refer to the functional aspects of an app. however, this hypothesis is yet to be confirmed by evaluating the proposed ags by giving them to real human annotators for labeling app features in user reviews."
"the exponents ranging from −3 to 3 with a step of 0.5 were selected to limit the dimension of the search space, and consequently, the complexity of the identified models. the models' size m was fixed to four terms (flow rate q bepp, water head h bepp, efficiency η bepp at the bep, and specific speed n sp ). moreover, a bias was selected into the program and ls parameter estimation was constrained to search for the set of positive polynomial coefficient values a j . the moga optimization model adopted into the study focused its attention on the optimization problem of three objective functions: the minimization of the number of input x i, the minimization of the number of terms m, and, finally, the maximization of model accuracy. the epr-moga conducts a number of generations, that in this case was 1080, a number that depended on the number of candidate inputs, length of training set, exponents, and maximum number of monomial terms of each model."
"we expected that removing such features from the annotated set would thus improve precision. the results show, however, that the precision increases only on the shah-i dataset while on all other datasets it dropped. this can happen if the short and vaguemeaning verbal features share the same characteristics with the self-references-the distinct number of such features is small but their frequency is high, in which case it is relatively simple for the model to spot them and removing these features from the annotations causes the precision to drop."
"we present maude syntax for functional modules specifying the natural numbers in the my-nat module. it defines the sorts nznat and nat, stating by means of a subsort declaration that any term with sort nznat also has sort nat:"
"the anns methodology found a pareto front size of only three solutions for the efficiency η bept (table 5), even if these solutions were all characterized by a cod of about 0.83. paying attention to the results, it was very interesting to verify that η bept depends on the pump efficiency η bepp . solutions were characterized by an 91% average reduction in total parameters. the best model, indicated with a gray line (model 2), as selected by taking into account both its parsimony in terms of parameters and its prediction performance."
"the rest of the paper is organized as follows. in section 2, we describe the proposed framework in detail; section 3 provides the evaluation results; we conclude this paper in section 4."
"step i removes all annotated features not referring to app functional or non-functional aspects but only to app itself either by the app name or by explicitly using the words such as app or application and other similar pseudo-features. this step simulates the change in the sanger ags such that the command to annotate the references to the app itself are removed. because guzman ags do not require to annotate such pseudo-features this simulation step only changes the annotations of the sanger and shah datasets. after this step, the reviews without any annotated app features are removed again to ensure that the feature distributions over all datasets are similar."
the data processing steps adopted in our study comprise one pre-processing step and three steps for simembeddings/ 13 http://www.nltk.org/ modules/nltk/stem/snowball.html ulating the changes in ags. the pre-processing
"a formal executable semantics of programming languages provides a rigorous mechanism to execute programs and in extenso, to implicitly or explicitly have access to all the program executions. the rewriting logic implementation-the maude system [cit] -comes with reachability and fully-fledged ltl model checking tool support. thus, the notion of execution could be extended from program execution to analysis tool execution (over the same particular program). in these two settings, it is often important to simplify the executions with respect to certain criteria. one such simplification is called slicing [cit] and, when applied on programs, it defines safe program fragments with respect to a specified set of variables."
"a mathematical drawing method that constructs a perspective view allows a designer to visualize the spatial experience of a proposed conceptual configuration. thus, a forced perspective is often used to create the illusion of spatial depth (pérez-gómez & [cit] . the light and lighting distribution in an interior scene can also create a sense of depth. however, the cause-andeffect relationship of scene-based luminance contrast and its impact on depth perception could only be determined when alternative experimental environments were established by recent developments in digital representation."
this experimental design had two objectives: (1) to explore the binocular disparity between the reduced-and full-scale environments and determine the manner in which this influences depth perception and (2) to verify the visual realism produced by the computational framework with respect to images from the physical model.
"many methods have been developed to measure perceived distance along with a statistical model to measure perceived distance from collected data. visual matching is the most commonly used method. it requires participants to view a test target and adjust a comparison target to match the perceived distance of the test target. however, when performing visual matching using a two-dimensional representation of a three-dimensional environment, participants may be distracted by the two-dimensional cues instead of judging the three-dimensional depth. in contrast, a method that forces the participant to make a binary choice allows them to respond quickly and therefore records a more intuitive and reliable perceptual judgment [cit] ."
"as illustrated in figure 5 (a) and (b), the discrepancy suggested by the misaligned features increases from the 1:1 to 1:10 scale settings. thus, the visual differences in the stereo representation could be attributed to the distance between the two viewpoints and target of focus. therefore, to reduce the disparity between the two viewpoints to one-tenth, we can simulate the stereo-viewing mode of the full-scale environment from the 1:10 scale setting (figures 4 and 5) ."
"the objective is to design a guaranteed-cost preview control law, with the form remark 4. the preview repetitive controller (7) consists of four parts: the first part is repetitive control, the second term represents output feedback, the third and the fourth represent the preview action based on the future values of the previewable reference signal and the disturbance signal. besides, considering that the system states are not measurable, the static output feedback is used instead of taking the state feedback."
"in this section, we will construct an augmented dynamic system that includes previewed information, error vectors, and states of the system by using the l-order forward difference operator. moreover, the augmented performance index will be also given."
"in lid3, the most informative attribute is selected as the root of an ldt, and the tree will be expanded with branches associated with all possible focal elements of this attribute. for each branch, the attribute with maximal information gain in all free attributes will be the next node until the branch reaches the specified maximum length or the maximum class probability reaches the given threshold. the learning process forms a level order traversal based on the training data."
"although a lab-controlled environment allows more precise parametric control of the experiment setting, significant effort is often required to construct the environment, and the spatial scale is often limited to that of interior spaces. this disadvantage can limit the investigation of depth perception to a short range, and the environment used to perform the perceptual study may be difficult to reproduce in order to evaluate the design application of the research finding. to this end, digital technology provides an alternative environment for investigating depth effects that otherwise cannot be investigated in a real setting. for instance, meng and sedgwick used a computer environment in which the presence of shadow in nested surface contact relations could be controlled to investigate its effect on distance perception [cit] . however, although the digital environment can practically create any scale and condition of a three-dimensional space, whether a depth effect that is revealed in a virtual environment can be applied to actual perception in a real environment is dependent upon the visual realism provided by the computer environment."
"decomposition 1 all features could represent some meanings in a message, for example, interjection, attraction, behaviour, benefit, etc. hence, the feature set extracted from the smsspamcollection is decomposed to five subsets semantically in terms of their specific meanings. decomposition 2 as some features may have approximate meanings or roles, they could be combined together to be one feature, which can be implemented easily with the excel formula: 'if(or(x 1,x 2,...x k ),1,0)'. for example, 'award' and 'prize' are combined to be one feature, 'award/prize', similarly, 'sex/girl', 'reply/send', 'cash/price','free/0p..9p'','stop/click'. thus, the number of features is reduced to 14 from original 20. in addition, the purpose of 'sex/girl' could be to attract the attention of the message receivers. hence, it is placed to the subset of attraction. the new set of 14 features are decomposed to the following four subsets. decomposition 3 in this decomposition, subset one and subset two in decomposition 2 are combined to be one subset, as the general purpose of these features could be to attract the attention of message receivers. therefore, the 14 features are decomposed to three subsets. the data with a subset of features are used to train and test an ldt. the outputs of the trained ldt are the appropriate measures of 'ham' and 'spam' for each tested message. if the message is more appropriate to be described with 'ham', it will be identified to 'ham', otherwise, 'spam'. if the estimated label is the same as the real label of the message sample, the identification is a true decision making. in terms of definitions, tpr, tnr, and a are calculated. the sensitivity, specificity and general accuracy for each subset of features are observed, and shown in fig. 3 (a) ."
"here, we demonstrate the use of the proposed approach on a benchmark database from uci machine learning repository [cit], and evaluate the performance of different linguistic attribute hierarchies embedded with ldts, and compare comparing them with the performance of the single ldt."
"depth cues can be categorized based on different criteria such as static or dynamic, pictorial or kinetic, monocular or binocular, and absolute or relative cues. in general, depth cues based on physiological feedback such as the convergence of two eyes focusing can provide absolute distance perception, but only at a short range. in contrast, static pictorial cues enable the perception of relative distances at greater ranges. however, some pictorial cues such as atmospheric perspective can only help to establish a relatively near or far distance for very distant scenes [cit] ."
"this paper presented a design method of a guaranteed-cost preview repetitive controller for a class of polytopic uncertain discrete-time systems. using the l-order forward difference operator, an augmented dynamic system was first constructed. based on that, a guaranteed-cost static output feedback controller was then designed. by incorporating this controller into the original system, the guaranteed-cost preview repetitive controller was obtained. simulation results verified the proposed method to be very effective. our future work includes ilc and high-order ilc with preview compensation. moreover, the case of continuous dynamics will also be included."
"the first lah with 5 subsets (denote as lah 1 (5)) is a cascade of ldts, i.e. the output of the ldt in lower layer will be the input of the ldt in upper layer(see fig. 5 ). in lah 2 (5), shown in fig. 6, ldt(s 5 ) and ldt(s 1 ) are at the bottom layer of the lah. in lah 3 (5), shown in fig. 7, ldt(s 5 ), ldt(s 1 ), ldt(s 2 ) are at the bottom layer of the lah. in lah 4 (5), table ii lists all sensitivities, specificities, accuracies and the areas under roc curves for the five lahs. it can be seen that lah 1 (5) -lah 4 (5) have similar performance. lah 5 (5) has different performance to that the previous four lahs have, its sensitivity drops, and specificity increases. but the accuracy of lah 5 (5) is close to that of lah 2 (5), and lah 5 (5) obtained the largest area under the roc curve. namely, when the ldt(s 4 ) is placed at the same layer with other four ldts, the features in other subsets could affect the performance, so that the sensitivity of lah 5 (5) drops, not keeping the level of sensitivity obtained by ldt(s 4 ) at different layers. and ldt(s 2 ) are at the bottom layer of the lah. in lah 3 (4), shown in fig. 12, ldt(s 1 ), ldt(s 2 ), ldt(s 3 ) are at the bottom layer of the lah. in lah 4 (4), shown in fig. 13, ldt(s 1 ), ldt(s 2 ), ldt(s 3 ), ldt(s 4 ) are all at the bottom layer of the lah."
"their performance in tpr, tnr, accuracy and the area under roc curve are listed in table iii . the results are similar to that of decomposition 1. lah 4 (4) has the best performance in specificity, accuracy and the area under roc, but its sensitivity is worse than that of other lahs. the performance in tpr, tnr, accuracy and the area under roc for the three lahs are listed in table iv . similar to the previous two decompositions, the specificity, accuracy and the area under roc are increasing (non-decreasing) as the place of ldt(s 4 ) drops from top to bottom. but the sensitivity of lah 2 (3) is larger than that of lah 1 (3) while lah 2 (3) keeps the same specificity as lah 1 (3). however, lah 3 (3) is with the dropped sensitivity again as the lahs for previous two decompositions. (2) best roc curves for different decompositions table vi lists the performance of lahs with the best area under the roc curve for the three decompositions. two single ldts were trained and tested with the two full sets (20 and 14) of features, and their performance is listed in the table as well. fig. 4 shows the roc curves for the five solutions. regarding the area under roc curve, lah 3 (3) is the best in the all listed solutions; regarding specificity, lah 5 (5) is the best; and regarding sensitivity and accuracy, ldt (14) is the best. it is surprising that the two single ldts have worse performance in the area under roc curves than the listed lahs, as this is different to the convention."
"binocular disparity refers to the differences of the retinal images perceived by each eye. although the assumption that some differences in the perceived images might not considerably affect the perception of a general scene may be common, this assumption may not apply when viewing a visual target at different distance scales. this means that viewing a reduced-scale physical model might be different from viewing a full-scale environment, at least when perceiving the depth of objects in space. in other words, if a visualization tool uses a reduced-scale environment to envision the depth effect of luminance contrast, knowing whether the luminance contrast depth cue has the same effect at both short and long distances is important."
"deep learning is a branch of machine learning based on a set of algorithms that attempt to model high level abstractions in data by using a deep graph with multiple processing layers of nonlinear processing units for feature extraction and transformation. each successive layer uses the output from the previous layer as input, forming a hierarchy from low-level to high-level features. a linguistic attribute hierarchy has similar properties to the generic deep learning. intermediate attributes in upper layer are functions of their child features (attributes) in lower layer. hence, a linguistic attribute hierarchy (lah) can be viewed as another form of deep learning. unlike the generic deep learning, which constructs a neural network architecture, such a linguistic attribute hierarchy has a flexible tree structure with all attributes as leaves, and all non-leaf intermediate nodes and the top decision node of the tree can be represented by a machine learning model (e.g. linguistic decision tree [cit] and neural network [cit] ), fed with their child nodes. an lah, embedded with ldts, integrates the linguistic rules of ldts in the hierarchy to form a hierarchical decision making or classification. the linguistic rules in the hierarchy are produced by the embedded ldts (equivalent to the nonlinear process units in generic deep learning) in terms of hierarchical process of decision making or classification (see details in section ii.c)."
"in this section, some lahs embedded with ldts are constructed and trained. each ldt is fed with a subset of features. the construction of an lah from bottom to top will be in terms of the sensitivity of identifying spams from low to high. namely the ldts are fed in turn with the order s 5, s 1, s 2, s 3, s 4 ."
"design is a process in which a designer proposes, analyzes, and refines a solution to achieve specific design goals [cit] . in this repetitive process, creativity is essential for developing a design solution. however, evaluating and predicting the effects of a design solution are also important. different design considerations require different evaluation methods. for conceptual considerations such as structural stability, numerical data may be used for assessment; for perceptual quality, various methods of visualization can assist in the design process."
where b ∪ f j represents the new branch obtained by appending the focal element f j to the end of the branch b. the probability of f j given b can be calculated as
"these previous studies investigated two factors that affect the illusory depth effect of luminance contrast: (1) the visual realism of images produced by a computer-generated pictorial environment and their realism with respect to the visual realism of real images of a physical scene, and (2) the effect generated by stereoscopic displays. however, the combined effect of visual realism and the stereoscopic display of a physical scene has remained unexplored. the difference in scale at which physical and digital models are constructed was also considered as a possible confounding variable."
it should be pointed out that the condition (35) in theorem 2 is not a convex problem. the following theorem can translate it into an lmi problem.
"(2) true negative rate (tnr, specificity) measures the proportion of negatives that are correctly identified as such. therefore, specificity quantifies the avoiding of false positives. it is calculated with t n/n, where, t n is the number of hams that are correctly identified, and n is the number of hams in the database."
"if we do not set the threshold of probability during the training process of an ldt, fed with n features, each branch in the single ldt will have n edges. if we use each focal element to represent an edge in an ldt, and each variable (feature) could be described with τ focal elements, then each non-leaf node in an ldt will have τ child nodes. namely, the single ldt is a τ-branch tree with τ n leaves (branches). in the implementation, the learning threshold is set to the maximum 1. namely, if the probability of positive or negative gets 1, the node will be a leave (i.e. no further learning process). therefore, real number of branches of a single ldt is less than the maximum. assume ρ i is the number of input attributes for ldt i in an lah. an ldt will be a τ-branch tree with τ ρ i branches. the number of total branches in a hierarchy: from table vii, it can be seen that the branch numbers of all lahs in figs. 5 -9 for decomposition 1 are less than that (697 branches) of the single ldt, trained with the 20 attribute data set, and the branch numbers of all lahs in figs. 10-16 for decompositions 2 and 3 are less than that (421 branches) of the single ldt, trained with the 14 attribute data set. moreover, due to the set of features are decomposed to subsets of features, the number of input features for each ldt in an lah is less than the number n of total features. namely, the length of each branch is short than that of branches in the single ldt. this indicates the computing complexity is reduced very much."
"note that the controller is still generated, based on the traditional lqr approach. with different weighting matrices q e and r, various gain matrices can be obtained, which indicates that the controller is non-optimal in a true sense. in recent studies, plenty of methods have been reported to solve this type of optimization problem, such as the adaptive particle swarm optimization method [cit] and genetic algorithms [cit] ."
"this study reviews previous studies that investigated the manner in which luminance contrast can be used as a depth cue, as well as perceptual studies conducted to validate the visual realism of visualizations produced using high dynamic range (hdr) imaging technologies. two hdr acquisition techniques were employed to generate visual stimuli for psychophysical experiments: a radiance map (in which multiple low-dynamic range images of a physical-scaled model were combined to create a single hdr image) and physically based rendering. the current study has two objectives: to examine the visual realism of hdr acquisition methods and to determine the influence of full and reduced model scales on depth perception when images are viewed binocularly."
"lahs constructed with decomposition 1 five ldt-based lahs (figs. 5 -9 ) are constructed and trained, and their performance, tpr tnr, accuracy and the area under the roc curve, are listed in table ii . an ldt is fed with a set s of features is denoted as ldt(s)."
"in the following, we will give some sufficient conditions for the existence of a guaranteed-cost static output feedback controller (33) for the augmented dynamic system (27) . theorem 1. for the augmented dynamic system (27) with performance index (28), if there exists a positive defined parameter-dependent matrix p(θ) and an invertible matrix"
"about the author nan-ching tai received his master of architecture and phd. in the built environment from the university of washington. he is currently a faculty member at department of interaction design of national taipei university of technology. his teaching and research interests are in the area of visual language. he believes that sketching freehand analytically is a means by which to think and create in the field of design and thus can be considered as a native visual language. to communicate and realize a design idea, he believes computer graphics are an advanced common visual language. he notes that the development of high-dynamic-range-imagingrelated technologies continue to advance visual realism. to this end, his research interests focus on developing a computational framework for creating a pictorial environment that can reflect perceptual reality and using this alternative environment to investigate the various issues related to space perception in the field of design."
"two particular experiments were conducted in this study. the first compared the perceptual equality between the experimental scenes generated using radiance maps and computer simulations created using radiance. the second investigated the effect of binocular disparity on depth perception in reduced-and full-scale environments. the results of both experiments confirm that luminance contrast can be an effective visual cue to increase the perceived distance of a visual target in an architectural scene, regardless of the hdr acquisition method employed. the experimental results also indicate that the influence of the reduced-and full-scale environments on the illusory depth effect from luminance contrast is not considerable for binocular views. however, analyses of the results reveal that representations generated by computer simulations are more reliable for investigating the effect of luminance contrast depth cues. this study concludes that computer simulations, when employing hdr imaging and autostereoscopic display methods, can provide the visual realism necessary to investigate the effect of luminance contrast on depth, and that the effect of binocular disparity in reduced-and full-scale environments of the scene is insignificant. however, to design and visualize possible applications of luminance contrast for enriching spatial perception, computer simulations that use hdr-related technologies can be a practical and reliable option."
"(1) true positive rate (tpr, sensitivity, recall) measures the proportion of positives that are correctly identified as such. therefore, sensitivity quantifies the avoiding of false negatives. it is calculated with t p/p, where, t p is the number of spams that are correctly identified, and p is the number of spams in the database."
"in this paper, a linguistic attribute hierarchy (lah), embedded with ldts, is investigated for the deep learning of attribute semantics for spam detection. the set of features extracted from the message spam database is decomposed to subsets of features in terms of the attribute semantics. each subset will be fed to an ldt to form an intermediate or the final decision varialbes in an lah, and an ldt in upper level of the lah could be fed with a subset of features and/or some intermediate attributes from lower level. a case study is performed on the sms message spam database from the uci machine learning repository."
"results of decomposition 2 decomposition 2 made some adjustment of features based on decomposition 1. the number of features is reduced to 14, and s 5 is combined to s 2 . from fig. 3 (b), it can be seen that the sensitivities and general accuracies are increasing as the order from s 1 to s 4, but s 1 and s 2 obtain higher specificities than that s 3 and s 4 do. fig. 3 (c) shows that the combination of s 1 and s 2 in decomposition 2 improves the sensitivity, although the sensitivities still keep increasing in the order from s 1 to s 3 in decomposition 3. it can be seen that the specificity of s 1 in decomposition 3 does not drop as its sensitivity increases. hence, it seems reasonable to combine s 1 and s 2 in decomposition 2 to form the decomposition 3."
"the 'curse of dimensionality' in decision making modelling is the exponentially increasing number of rules associated with the increasing number of input attributes. this is a major bottleneck that blocks the applications of decision trees. with n input variables, described with τ distinguishable labels, a full decision tree could produce τ n number of rules. the research [cit] has shown that a linguistic attribute hierarchy could help tackling the 'curse of dimensionality' problem, and improve the performance of decision making or classification."
"in this paper, we proposed another form of deep learning, a linguistic attribute hierarchy, embedded with linguistic decision trees, for spam detection. a case study was carried out on the sms message database from the uci machine learning repository. it has been shown that the decomposition of features plays an important role in the construction of the linguistic attribute hierarchy, which directly affects the performance of decision making by the constructed linguistic attribute hierarchy. in the experiments, three decompositions of features (attributes) were investigated, and the lahs were constructed based on strategies for the three decompositions semantically. the performance of lahs for different decompositions were examined in term of sensitivity, specificity, accuracy and roc curve. according to the experimental results, the ldt with the highest sensitivity should stay in the top layer, different to the layers where other ldts with lower sensitivity in order to obtain a high sensitivity of the lah, but in order to obtain large area under roc, all ldts should be placed in the bottom layer of the lah. however, without decomposition, a single ldt trained by the full set of features cannot obtain a larger area under roc than an lah could have. the experimental results show that the features related to the benefits and finance have important impact on the sensitivity of identifying spam messages. this observation matches the convention of human knowledge. the process of experiments has demonstrated the use of a hierarchy of linguistic decision tree approach for deep learning of feature impact on spam detection. hence, an lah, embedded with ldts, provides a transparent approach to in-depth analysing feature impact to the spam detection. this approach can not only improve the performance of spam detection when the semantic attributes are constructed to a proper hierarchy, but also efficiently tackle 'curse of dimensionality' in spam detection with massive attributes. the automatic knowledge based decomposition of features and the optimisation of linguistic attribute hierarchies and high performance spam detectors will be the future work."
"visual perception of the lighting distribution of a scene is derived from complex interactions between light, the built environment, and the visual system. to generate an alternative environment in which the scene-based lighting distribution can be parametrically manipulated to determine its effect on depth perception, tai and inanici developed a computational framework incorporating a physically based lighting simulation software called radiance and perceptually based tone-mapping to generate a perceptually realistic computer-generated environment [cit] . this environment was later used to investigate the interrelationships of light, architectural configuration, and the resulting luminance distribution, which revealed that luminance contrast can be used as an effective design strategy to create illusory spatial depth in an architectural scene [cit] . to validate whether luminance contrast is effective in both real and virtual environments, a comparative study conducted identical experiments using radiance maps generated from captured images of a physical model and radiance simulations. results revealed that the radiance simulations create physically accurate luminance distribution data with physically based parameters. the matching experimental results from both settings also demonstrate that the computational framework can generate a perceptually realistic static representation for monocular vision [cit] . the effect of binocular disparity on both visual realism offered by the alternative experimental environment and the illusory depth effect from luminance contrast was subsequently investigated using three display modes: standard single-image view, anaglyph 3d, and autostereoscopic display [cit] . the study's results demonstrated that, even with binocular vision, the luminance contrast created an effective illusory depth effect. in addition, observers were more confident in their perceptual judgments when viewing images using an autostereoscopic display."
"the constant stimuli method was used to measure the perceived distance by requiring a binary response such as indicating whether a test or reference target is perceived to be nearer [cit] . in this method, the location of the test target remains constant, whereas the reference target's location ranges from the farthest distance needed to perceive the reference target as actually farther than the test target to the nearest distance needed to perceive the reference target as actually nearer than the test target. for the method of constant stimuli, only an odd number of reference scenes is required. observers need only produce a simple judgment on each pair of test and reference targets, thus generating a more intuitive response. with this method, observers compare the relative distances of the test and reference targets several times, and the collected data are processed using ogive or probit analysis to derive a psychometric function. the psychometric function reveals the approximate value of the independent variable when the test target is perceived to be at the same distance as the reference target. figure 7 shows the seven reference scenes (as a monocular view) of the physical model and computer simulations. each reference scene uses the same lighting condition as that of the even condition test scene; however, the location of the visual target was placed at 12, 13, 14, 15, 16, 17, and 18"
"(4) roc curve and area under roc curve (a roc ): in statistics, a receiver operating characteristic (roc), or roc curve, is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. the curve is created by plotting the true positive rate (tpr) against the false positive rate (fpr) at various threshold settings."
"to compare the effect of binocular disparity of the experimental scene generated from the physical-scale model to that of the full-scale environment, the experimental scenes were presented in three forms: \"standard single-image view,\" \"reduced stereo view,\" and \"full stereo view.\" to prepare the images for stereoscopic display, the pair of tone-mapped images were placed side-by-side in an image-editing program and saved in jpeg stereoscopic format (jps). the jps image was split into halves and each half projected to the corresponding eye when shown on an autostereoscopic display device. for the \"full stereo view\" condition, two cameras were set 6 mm apart in the physicalscale model to simulate the visual target's location in a full-scale architectural configuration. for the \"reduced stereo view,\" the two cameras were placed 60 mm apart to simulate the viewing conditions of the actual distance in the scaled physical model. a third set of experimental scenes was created with a single-camera setting and shown in a standard, single-image mode. this set of images was used as the baseline for comparing the two sets of stereo images. figure 6 presents the experimental scenes for the different conditions. the 12 conditions are labelled according to the luminance contrast condition (\"e\" for \"even\" and \"c\" for \"contrast\"), hdr acquisition method (\"p\" for \"physical\" and \"s\" for radiance simulation), and camera viewpoint distance (\"00\" for single-camera setting, \"06\" for 6 mm, and \"60\" for 60 mm) (figure 6 )."
"to measure the perceived distances of the visual targets under different conditions, collected data were statistically analyzed using probit analysis [cit] . figures 8 and 9 show the probit regression curves for each condition using the hdr acquisition methods. the x-axis represents the location of the reference scene targets, whereas the y-axis represents the probability that the test scene target was perceived as nearer than the reference target. the intersection point between each regression curve and the dashed line at 0.5 (which represents 50% probability) is the point of subjective equality (pse) for each condition. the pse is considered to be the measured perceived distance of the visual target under that specific condition. table 1 summarizes the measured perceived distances of the test scene targets between the luminance contrast conditions of \"even\" and \"contrast,\" and lists the increase as a percentage for each condition. the pses generated using a radiance map to create the visual stimuli deviate slightly compared to those generated using computer simulations (which all seem to coincide). the same is true of the regression curves. ignoring these marked differences, we can see that the regression curves of the radiance maps and computer simulations are extremely similar in terms of positive shifts of the pses from the \"even\" to \"contrast\" conditions. these results concur with previous studies, which found that luminance contrast is an effective cue for creating illusory depth effects regardless of the hdr acquisition method employed (figures 8 and 9, table 1 ). figure 10 compares the increase of the three viewing modes. in both the \"physical\" and \"simulated\" conditions, the stereo-viewing mode in the full-scale environment showed the lowest increase of the illusory depth effect. in addition, the increase for images generated by computer simulation for hdr acquisition were much more similar than when radiance maps were used. a possible explanation for this is that the parametric control for generating experimental scenes of the computer simulation method is more precise. the output computer simulations are thus more consistent in appearance, and this yields more consistent perceptual judgments (figure 10 )."
"be the tracking error. throughout this paper, two assumptions are made: a1: the reference signal r(k) and disturbance signal d(k) are periodic, both of period l."
"from fig. 3 (a), it can be seen that the sensitivities are much lower than the specificities for each subset of features, and sensitivities are increasing as the order from s 1 to s 4, but s 5 has the lowest sensitivity. the subset of benefit related features (s 4 ) is with the largest sensitivity. this demonstrates the capability of each subset in identifying positives or avoiding false negatives. s 1, s 2 and s 5 have similar specificities, and s 3 and s 4 have similar specificities, but which are smaller than that of s 1, s 2 and s 5 . s 4 obtains the highest general accuracy."
"lid3 [cit], an extension of the well-known id3 algorithm [cit], is used to train an ldt based on a given database. the induction is guided by a modified measure of information gain according to label semantics."
"previous studies have established that perceptually realistic computer simulations provide pictorial cues that can match those of real scenes. however, the perspective view presented on common display devices does not address the distance scale issue. figure 1 (a) presents a scene in which a sculpture is viewed in a gallery space. figure 1(b) shows the same setting with everything at a reduced scale of 1:10. the two images present no visual difference. however, when synthesizing the two perceived images for each eye to simulate stereo visual perception, the reduced scale of the setting increases the angle between the two eyes and target of focus. figure 2 shows that the disparity of the overlapped images for each eye is considerable in the reduced-scale setting. therefore, when using a scale model to envision the depth effect, whether directly or through captured images, this effect may be influenced by the reduced scale resulting from binocular disparity. (figures 1(a), (b) and 2)."
"mass assignments of attributes: as all features extracted from the smsspamcollection database are binary variables, the mass assignment on a discrete label is 1 if the attribute value is 1, otherwise 0. two-fold cross validation: data is split into two approximate equal partitions. one is used for training, and the other one is for testing, and reverse the process."
"many depth cues that help to perceive depth can be design strategies to create a sense of depth in architectural space. lighting in a scene has also been determined to be an effective depth cue for creating a sense of depth and is therefore may be used as a possible design strategy to enrich spatial experience. this study is part of a series of continuing studies that investigate the manner in which we can create a reliable computer environment to establish a relationship between lighting and its effect on the depth perception of a scene. moreover, we examine the manner in which we can use the same computer environment to assist in the creation of illusory depth. this is a comparative study that examines closely different model scales and their influence on predicting stereo illusory depth effect resulting from luminance contrast in interior lighting."
"in our study, a scale model of the architectural environment was constructed at a 1:10 scale. computer simulations at this configuration were conducted with a physically based lighting simulation program and presented to observers using stereoscopic display technology. to account for binocular disparity, images of the scale model were captured from two viewpoints, as in previous studies. the two viewpoints were set apart by 6 and 60 mm to simulate the typical lateral distance between the left and right eyes, which correspond to the full-and reduced-scale environments, respectively."
"the function of a subset can be decomposed recursively. assume each attribute is described with a set of labels, and subsequent label expressions are derived. in a linguistic attribute hierarchy (lah), functional mappings between parent and child attribute nodes can be defined in terms of weighted linguistic rules that explicitly model both the uncertainty and vagueness, which often indicates our knowledge of such aggregation functions. now, assume that the introduced intermediate attributes are approximations of the goal variable y with the same domain and description labels. for an example, in fig. 2, z 1 is an approximation of y based only on x 1 and x 2 . unlike general deep learning, a neural network is constructed through layer by layer training, the new type of deep learning will construct a tree of ldts. hence, an lah can present a hierarchical decision making or classification. with this property, it is easy to perform semantic attribute deep learning through a hierarchy of ldts."
"for the dynamic system (27) with performance index (28), if there exists a positive defined parameter-dependent matrix x(θ); invertible matrices w, u, and g(α); and a scalar β, such that"
"the constant stimuli method does not require large number of participants; however, it does require each participant to make a large number of perceptual judgments so that the measured response may be derived from the collected binary data. the advantage of this method is that the measured response can be more intuitive and reliable; however, the disadvantage is that the collected data may not be suitable for studying individual differences because of the small number of participants. as a result, the analysis of the experiment results is limited to the visual realism for a normal viewing task of the investigated display methods. therefore, experiment participants must have normal or corrected-to-normal vision capable of viewing digital images on a computer display. ten qualified participants volunteered for the experiments. all of them were adults (ages 20-42), with normal or corrected-to-normal vision, and none experienced discomfort when viewing stereoscopic images. experiments were performed in an artificially lit research lab, thereby ensuring a stable lighting environment for the duration of the experiments. the experimental scenes were displayed on a toshiba satellite p850 laptop computer capable of displaying autostereoscopic images. each test scene was paired with one of the seven reference scenes, and each pair of test and reference scenes was presented to each participant 10 times. the location of the test and reference scenes (whether on the left or right side of the paired images) and the sequence of images were randomized. for each combination of test and reference scenes, participants generated 10 perceptual judgments. presented with 12 conditions for the test scenes along with seven reference scenes 10 times, each participant produced 840 perceptual judgments. each combination of paired images was then judged 100 times."
"the fundamental reason that the privacy protection provided by the above prior schemes is weak is because their indexes are distinguishable for the same number of data items but with different distributions. in bucketing schemes, for the same number of data items, different distributions in data values will cause buckets to have different distributions in sizes because they need to balance the number of items among buckets. in order preserving schemes, for the same number of data items, different distributions in data values will cause cipher-texts to have different distribution in the projected space. leveraging domain knowledge about data distribution, both bucketing schemes and order preserving schemes allow the cloud to statistically estimate the values of data and queries."
"we also used density-based clustering of applications with noise (dbscan) which is an unsupervised learning technique. dbscan groups the data points that are nearest neighbors of each other with the ultimate goal of forming dense regions. outliers whose nearest neighbors are not close enough, are clustered in low density regions [cit] ."
"in internet of things (iot) contexts, smart user devices can be recruited for participatory or opportunistic sensing campaigns. verification of genuine users of these devices is of paramount importance for the following reasons: high rejection rates may trigger biometric authentication and may de-incentivize users to offer their built-in sensors as a service whereas high false acceptances may result in reduced trustworthiness of the sensory data. with the convergence of iot and social networks, social internet of things has emerged which has many advantages including network navigability, service scalability and increased trustworthiness of acquired data. this paper has studied continuous verification in siot where online behaviometrics of mobile users collected via smart phones is considered by extracting features from smartphone sensors and users' social network interactions. we have presented a continuous verification scheme that uses social behaviometrics collected from a set of users. we have used real traces collected over several months. those traces are sent to a cloud server and analyzed with two machine learning techniques, namely the support vector machines (svm) and density-based clustering of applications with noise (dbscan). our results show that genuine users can be verified without any disruption 97% of the time whereas the users can keep using the devices 90% of the time without any disruption. her research focuses on processing and interpreting signals which arise from the human body. her research is funded from various sources, including the national science foundation, department of homeland security, and private industry, among others. she has started her own business, testified for congress, and has over 100 publications and several patents. volume 5, 2017"
"the cpa attack is a major threat to in-network caches in the current internet [cit] . numerous research efforts [cit] have been devoted to mitigate this type of attack. in particular, they analyze the characteristics of the network flows in terms of different ip addresses in a certain period, and thus are able to identify and throttle a malicious client. however, in ndn, we need to analyze the network traffic in terms of hierarchically structured packet names, rather than wellformatted ip addresses. due to the differences between packet names and ip addresses in nature, we need to re-design the cpa detection mechanism for ndn networks [cit] ."
"proof: (sketch) fm sketch relies on a hypothesis that each hash value is uniformly distributed over the scalar range or equivalently over the set of binary strings, i.e., every bit in this hash value will be 0 or 1 with the same probability [cit] . when using a hash function to generate a hash value (i.e., a binary string), an intermediate binary string is first partitioned into a number of substrings (i.e., bundles). certain transformations are then performed on these bundles to propagate randomness further in order to generate a more random binary string."
"we simulated six lda attacks, lda1, lda2, lda3, lda4, lda5 and lda6. as listed in table iii, each compromised consumer sends malicious interests per second to request unpopular content, while the regular traffic follows a zipf-like distribution with a specific α. in these attacks, the compromised consumers send out malicious interests from second 2. we have two observations: (1) in lda2, lda4 and lda6, the cs hit rate in the gateway router drops almost to 0, and that in lda1, lda3 and lda5 also declines significantly (see fig. 4(a) ). in lda5, the number of the attack requests is only equal to the number of the regular requests and the larger α (i.e., 1.1) is used to generate regular traffic in this case. as a result, the cache hit rate maintains at a higher level compared to the simulation results of the other lda attacks."
"monte carlo hypothesis test [cit] constructs several repeated tests and estimates the underlying distribution according to the test results. to provide sufficient samples for all the repeated tests, monte carlo hypothesis test re-uses the latest sample by randomly re-sampling with replacement from this sample (i.e., bootstrapping). therefore, monte carlo hypothesis test needs less samples to estimate the underlying distribution compared to a conventional hypothesis test. we perform a monte carlo hypothesis test by four steps: (1) we acquire a sample of size n from the interested data; (2) we randomly select an observation on the sample as a resample, record its value, and put it back so that it has a chance to be drawn again. in this way, we generate several resamples of size n from the original sample; (3) we compute an estimation of the interested data from each resample, and display all the estimations computed from the resamples in a histogram, which represents the sampling distribution of the interested data; (4) we obtain an appropriate estimation of the interested data according to the aforementioned distribution and the selected level of significance."
"the training module is composed of a learning algorithm that runs in a sliding window of a set of data. the training procedure is based on four factors described in section iii-b.3; i) social activity rate, ii) sociability factor rate, iii)the number of sessions each user has produced per day, and iv) location which is provided by the mobile devices' built-in sensor. in our approach, each user has vectors of feature sets where each vector represents the user behavior throughout a day. the training procedure is updated on a daily basis."
"we make three key contributions. first, we propose the first privacy preserving range query scheme that is secure under the widely adopted ind-cka model. second, we propose pbtrees, basic pbtree construction and query processing algorithms, and two pbtree optimization algorithms. third, we implemented and evaluated our scheme on a large real world data set with 5 million data items. experimental results show that our scheme is both fast and scalable. for example, for a query whose results contain ten data items, it takes only 0.17 milliseconds."
"to throttle the cpa attacks accurately in ndn, we need to find out the name prefixes used by the attacker for sending malicious interests. when a router monitors the interests possessing a common name prefix, it can observe that the number of distinct content requested by these interests would increase dramatically when the attacker performs a cpa attack on it (section iii). in general, we can rely on this symptom to detect cpa attacks in two phases, a traffic monitoring phase and an attack identification phase. in the traffic monitoring phase, the router monitors interests possessing a common name prefix, such that it can find the burst of distinct content requested by these interests. the fm sketch (section ii-b1) is designed for estimating the number of distinct items among numerous scanned items. thus, in this phase, the router can simply rely on the fm sketch to monitor the interests which possess a common name prefix, such that it can find the burst of the distinct content requested by these interests."
"a pbtree for n data items is a full binary tree with n terminal nodes and n − 1 nonterminal nodes, where all n terminal nodes form a linked list from left to right and each node is represented using a bloom filter. each terminal node contains one data item, and each nonterminal node contains the union of its left and right children. for any nonterminal node, the size of its left child either equals to that of its right child or exceeds by one."
"in this paper, we propose the first range query processing scheme that achieves index indistinguishability, under the ind-cka [cit], which provides strong privacy guarantees. the key novelty of this paper is in proposing the pbtree data structure and associate algorithms for pbtree construction, searching, and optimization. we implemented and evaluated our scheme on a real world data set. the experimental results show that our scheme can efficiently support real time range queries with strong privacy protection."
user characterization is done by extracting a combination of features from both users' interaction over online social network services as well as the built-in sensors of the smartphones. the details of the model are provided in the following sections.
"during the traffic monitoring phase, we apply the lfm sketch to estimate the number of distinct content being requested corresponding to each name prefix. during the cpa identification phase, we compute the interest traffic monitoring result (i.e., the harmonic mean of the indices for the leftmost \"0\" bit in the bitmaps) and compare it with a threshold to detect a potential cpa attack."
"we evaluated the performance of elda in ndnsim [cit], a popular ndn simulator. all of our simulations were performed on a local machine, equipped with an intel core i7 3.4g cpu and 16g ram, running ubuntu 13.04 with kernel version 3.8.8. all the primary parameters we used in ndnsim are provided in table ii ."
"one-wayness: for each prefix pi, we use the r secret keys to compte r hashes: hmac(k1, pi), · · ·, hmac(kr, pi). the purpose of this step is to achieve one-wayness, that is, given prefix pi and the r secret keys, it is computationally efficient to compute the r hashes; but given the r hashes, it is computationally infeasible to compute the r secret keys and pi; furthermore, even given the r hashes and pi, which is the case in chosen plaintext attacks (cpa), it is still computationally infeasible to compute the r secret keys."
"the proposed framework can verify user 1 with 100% under 15%-85% and 50%-50% situations otherwise the verification success ratio is approximately 95% which means that the cumulative authentication error probability for user 1 is 5% from day 7 through 76. these performance metrics for user 2 for all settings is almost the same, which is approximately 83% except for 85%-15% which differentiated on day 70 and ended by 75%. the framework could be able to verify user 3 with 100% success rate when α and β are 15%. user 4 experiences 100% success rate in verification for all set of configurations. this is mainly because the user's exhibiting a visibly inactive behavior most of the days during the data collection. user 5 shows better performance under 15%-85% and 30%-70% settings. user 6 is more sensitive to each setting. the framework has approximately 100% success rate in verifying user 6 under the case, 15%-85%, and then for the rest of the settings, 50%-50% has the best match for the user verification. to summarize, the proposed framework performs better when the system relies more on long term activity than short term."
"(2) when an lda attack happens, the average round-trip time (rtt) of regular interests increases 2 to 3 milliseconds in terms of different α of the regular traffic and attack frequency of lda attacks. when we used a large α and low malicious request frequency in the simulation, the average rtt increases more slowly (see figure 4(b) ). we conclude that, the lda attacks affect the overall performance of ndn network more seriously while sending more malicious interests per second. on the other hand, the lda attacks degrade the performance less when the α of the regular traffic is large."
"+ n by optimizing the leftmost \"1\" bit searching process, since m is exponentially small than n (detailed in appendix). in summary, the performance of our lfm sketch is significantly improved compared to the hyperloglog fm sketch."
"driven by lower cost, higher reliability, better performance, and faster deployment, data and computing services have been increasingly outsourced to clouds such as amazon ec2 and s3 [cit], microsoft azure [cit], and google app engine [cit] . however, privacy has been the key road block to cloud computing. on one hand, to leverage the computing and storage capability offered by clouds, we need to store data on clouds. on the other hand, due to many reasons, we may not fully trust the clouds for data privacy. first, clouds may have corrupted employees who do not follow data privacy policies. [cit], a google engineer broke into the gmail and google voice accounts of several children [cit] . second, cloud computing systems may be vulnerable to external malicious attacks, and when intrusions happen, cloud customers may not be fully informed about the potential implications on the privacy of their data. third, clouds may base services on facilities in some foreign countries where privacy regulations are difficult to enforce."
"besides privacy guarantees, a privacy preserving range query scheme should be efficient in terms of query processing time, storage overhead, and communication overhead. the query processing time needs to be small because many applications require real-time queries. the storage overhead refers to the data that cloud needs to store other than encrypted data items. it needs to be small because the volume of data stored on the cloud is typically large. the communication overhead refers to the data transferred between the data owner and the cloud, other than encrypted data items, and the data transferred between data users and the cloud, other than the precise query results. it needs to be small due to bandwidth limitations and the extra time involved in uploading and downloading."
"the rest of the paper proceeds as follows. we first review related work in section 2. in sections 3 and 4, we present our basic pbtree construction and query processing algorithms and two pbtree optimization algorithms. in section 5, we prove that our scheme is secure under the ind-cka security model. in section 6, we show our experimental results. we conclude the paper in section 7."
"in the experimental results, when the contextual parameter weights are properly set, the probability of fa is reasonably low. however, the impact of applying different machine volume 5, 2017 learning algorithms and the possibility of further reducing fa should be investigated. this article aims at presenting the concept of continuous verification; however feasibility study of various machine learning algorithms on the proposed system is included in our future research agenda. the members of the participant pool in this research demonstrate fairly similar behavior to each other (i.e. mostly graduate students). inclusion of a heterogeneous participant pool in the experiments is expected to increase the impact of the normalization module of the system, diversify the behavioral clusters and in turn, result in lower fas. moreover, including additional applications in the analysis would improve the accuracy. however, improvement in the accuracy would come at the expense of additional computational overhead. hence, investigating the optimal number of apps to ensure the trade-off between computational complexity and accuracy in continuous verification is another important future research direction."
"next, we present a solution based on secure keyed hash functions (hmac) and bloom filters to make our pbtree privacy preserving. for each node v, we use a bloom filter denoted by v.b to store the prefixes of a node's prefix families. we assume that the data owner and the users share r secret keys, denoted k1, · · ·, kr, other than the symmetric key for encrypting and decrypting data items. consider a pbtree node v, where set l(v) consists of n prefix families and set u (v) consists of m prefixes p1, · · ·, pm. let w be the number of bits that each data item contains. our node randomization algorithm consists of the following three steps."
"in terms of accuracy p b w d outperforms the bucket scheme [cit] by orders of magnitude. for instance, for the maximum query result size of 90 in our experiments, the false positive rates recorded by p b w d are, 2.12, 21.38, and 39.96 times lesser than the bucket scheme with respective bucket sizes being 10, 50, and 90."
"with the rapid growth of smartphone and personalized mobile device usage, and along with the recent advances in internet of things (iot) where tremendous number of devices are interconnected, continuous authentication on personalized devices has become possible. smartphones with various types of sensors have the potential for continuous monitoring of phenomena like road condition for smart transportation, public safety and emergency preparedness [cit] . with the widespread adoption of iot devices, their use as a base for user verification is expected to grow [cit] . on the other hand, effective incentives are needed in order for the users to provide the resources in their smart mobile devices as a service. the incentives to improve participation and integration of the personalized devices to the iot environment can be either monetary or non-monetary. however, recruitment of the users is expected to be performed implicitly [cit] ."
"to guarantee an accurate traffic monitoring result, the strawman solution needs to use a large set of hash functions [cit] . even worse, all the hash functions need to be called when a new interest is received, and then all the corresponding bitmaps need to be updated. this requires a large amount of computation and memory resources."
"machine learning is the core of user verification thus, the system is trained with feature sets collected by the front-end application, and user verification is performed based on each interaction through the device. the system components are presented in figure 1 . figure 2 presents the system architecture that includes main modules and methods, namely monitoring, data collection, normalization, training and verification modules. table i presents the notation. as mentioned earlier, personalized smart devices are envisioned to be integrated into the iot ecosystem in order for the iot applications to recruit those devices in various sensing campaigns by accessing their built-in sensors. in iot sensing applications, to acquire the sensed data from personalized smart devices in a trustworthy manner, device level security must be ensured. therefore continuous user verification should be positioned at the core of the personalized device-iot integration. in the rest of this section, more details on each module are provided."
"2) detection scheme: based on our lfm sketch, we design elda, an efficient and lightweight scheme for detecting cpa attacks in ndn. similar to the strawman solution, elda contains two phases, a traffic monitoring phase and an attack identification phase."
"to achieve sub-linear search efficiency, we organize f (d1), · · ·, f (dn) in a tree structure that we call pbtree. we cannot use existing database indexing structures like b+ trees because of two reasons. first, searching on such trees (such as b+ trees) requires the operation of testing which of two numbers is bigger; however, pbtrees cannot support such operations for the cloud because otherwise pbtrees will share the same weaknesses with prior order preserving schemes [cit] . second, their structures for different sets of data items are often different even if the two sets have equal sizes; however, for any two sets of the same size, their pbtrees are required to have the same structure, i.e., the two pbtrees are indistinguishable. in this paper, we organize f (d1), · · ·, f (dn) using our pbtree structure."
"we have developed a front-end application that runs in the background of an android phone, and monitors users' volume 5, 2017 interactions through a smart device. the application collects data from five popular social network services which are; facebook, twitter, linkedin, skype and whatsapp. the collected data is stored in the form of sessions, and each session presents the corresponding user's interaction through the device. basically each session data includes session id, application name, the time that the session started, the time that the session ended, the duration of that session, the amount of data used in the session and the initial location where the session started. the amount of data used is the amount of cellular or wi-fi data consumed by the social network service application."
"the adversary can only see a set of matching data items for each trapdoor, which is captured using these two patterns. therefore, each bloom filter can be viewed as a match for a distinct, but not necessarily unique when viewed along the pbtree, set of trapdoors. the uniqueness is not possible since each range query can match multiple distinct trapdoors. theorem 1. the pbtree scheme is ind-cka secure under the pseudo-random function f and the encryption algorithm enc."
"the effectiveness of elda. to show the effectiveness of elda, we evaluate both the detection rate and the false positive rate, while performing a series of damage fla attacks (the attack intensity is measured in terms of kilo packets per second, i.e., kpkts). the detection rate is defined as a ratio between the number of correctly detected cpa attacks and the actual number of cpa attacks. a false positive may happen if the detection scheme falsely classifies a non-attack event as an attack event, and we define the false positive rate as a ratio between the number of false positives and the total number of cpa detection events. in fig. 5, we have two observations: (1) the detection rates of both elda and the baseline scheme are 100%, regardless of α and the malicious request frequency; (2) the false positive rates of both schemes are zero. we conclude that both schemes can detect cpa attacks effectively. the efficiency of elda. we evaluate the efficiency of elda in terms of the processing time and the network throughput. this is measured in terms of kilo packets per second, i.e., kpkts. since the processing time and the network throughput are both calculated regardless of the type of request packets (requesting popular content or unpopular content, and being a regular request or a malicious request), we did not discriminate the type of requests and thus set the α value to 0.9 here. as shown in fig. 6(a), the baseline scheme needs approximately 4 µs to handle a newly arriving interest, while elda only needs approximately 0.8 µs. moreover, the baseline scheme can only process less than 200,000 interest packets per second, while elda is able to process 1,300,000 interest packets per second(see fig.6(b) ). we conclude that elda is much more efficient than the baseline scheme. resource consumption of elda. to evaluate the resource consumption of elda, we traced elda's cpu and memory usage (see fig.7 ). similarly to the efficiency simulation, we set the α value to 0.9. when monitoring interest traffic, the baseline scheme almost monopolizes the cpu, while elda only uses 10% of the cpu. meanwhile, the baseline scheme requires more than 100kb memory to monitor a set of interests, each of which possesses the same name prefix. in contrast, elda only requires 488 bits. to identify the prefix used by attackers and throttle the corresponding malicious interests, a router usually needs to monitor interests with different name prefixes simultaneously. to achieve this, the baseline scheme may require much more amount of memory. for example, if the router needs to simultaneously handle 10,000 different name prefixes, the overall memory required in the baseline scheme will be 1gb, while in elda, it will be only 5mb. compared to the baseline scheme, elda requires significantly less computation and memory resources."
"in the attack identification phase, the router periodically obtains the interest traffic monitoring result (i.e., the estimation result of the fm sketch), and compares this result with an empirical threshold. if the result exceeds the threshold, the router reports a potential cpa attack immediately."
"our experimental results show that, the cost of pbtree construction is reasonable, both in terms of time and space. for the chosen datasets, figure 6 (a) shows that, the average time for generating, the p b b is 276 to 3443 seconds, the p b w is 338 to 7500 seconds, and the p b w d is 357 to 14027 seconds, respectively. the average time required for the p b w d construction is higher due to the equal-size partition algorithm and common prefix computation overhead involved. however, as we show later, the query processing time for p b w d is smaller compared to the other two variants of the pbtree, and the false positive rate is lower as well. figure 6(b) shows that, the pbtree sizes range from 1.598gb to 18.494gb for the data sets, and also, for a specific data set size, the p b b, p b w, and p b w d index structures are of the same size, respectively. finally, the pbtree construction incurs a one-time off-line construction overhead."
"our experimental results show that, the width and the depth optimizations are highly effective in reducing the query figure 10 show the average prefix query processing times and false positive rates, respectively, on different data sets, for prefix queries issued on the corresponding p b b, p b w and p b w d structures. figure 7 (a) and figure 7(b) show the average query processing time and false positive rates, respectively, for different prefix query result sizes, on the p b b, p b w and p b w d structures, which are built on 5 million data items."
"in this section, we study the destructive effects of both the lda attack and the fla attack. in addition, we evaluate elda by comparing it with the most effective approach in the literature."
mobile user data collected from the device is uploaded to a private cloud-based server. the server stores the raw data from all users in a database. the database is queried for training and verification purposes.
"in the past several decades, internet has evolved from a simple communication network for point-to-point conversations to a sophisticated global system that supports communication needs of all types of services, including packet delivery for information-intensive business, e-commerce and contents [cit] . existing internet architecture relies on ip address to guide packet transmissions. however, originally designed to suit the need of connection driven end-to-end communications, this transmission infrastructure is inefficient in supporting content oriented applications. as a promising architectural design for the future internet, ndn [cit] was proposed to bridge the gap between the current internet architecture and the information centric new applications. ndn leverages in-network caching as well as multipath communication to implement the name-based content (data) retrieval effectively and efficiently."
"to minimize the resource consumption during the monitoring phase, we design a novel lightweight flajolet-martin sketch (lfm sketch), which can efficiently estimate the cardinality of the requested content."
"once the session data is transferred to the server, the raw collected data is converted to several metrics of interest. this process is called normalization. in this study, two social verification metrics, namely the social activity rate and sociability factor are defined."
"to enhance the effect of both the lda and the fla attack, an attacker usually needs to require the compromised consumers to send an extremely large number of malicious interests (i.e., interests requesting unpopular content or nonexistent content) during a short period. in addition, for both the lda and the fla attack, when sending out malicious interests, the compromised consumers will use the same name prefix. in this way, the malicious interests will be forwarded towards the content provider corresponding to the targeted name prefix [cit], and will converge on the routers along the critical transmission path in a short period, which will lead to a situation that the responded content will deplete those routers' content stores. in summary, to perform an effective cpa attack, the attacker always needs to send a large number of malicious interests in a short period, requesting distinct content whose names possess the same prefix. therefore, when a victim monitors the interests which possess the same name prefix used during a cpa attack, it can find that the number of the distinct content requested by the these interests dramatically increases. this is a significant symptom which can be used to identify cpa attacks."
"support vector machines (svms) are one of the most wellknown and effective supervised learning techniques. we used svm to learn user sociometrics and verify them based on these phenomena. it is worth mentioning that the feature vectors need to be normalized prior to being sent to svm. in addition, we apply soft normalization which is the output of subtracting each data point from the mean values and scaling by twice the standard deviation as shown in eq. (9) ."
2) the attackers usually send a huge number of malicious interests in a short period to perform cpa attacks. how to accurately and timely capture the symptom of the attack traffic is a challenge.
"a cpa attack will significantly degrade the performance of an ndn network. to defend against this attack, several countermeasures are proposed in the literature. the existing proactive countermeasure [cit] mitigates cpa attacks with the cost of disturbing regular cache usage. to throttle the attack traffic accurately, multiple approaches are proposed to detect cpa attacks [cit] . however, both their time and space complexity are too high. when a cpa attack happens, the attack itself has already imposed a huge burden on the victim. thus, a detection solution should be kept as lightweight as possible to avoid consuming too many additional resources and overwhelming the victim. meanwhile, the detection scheme should be efficient enough to capture the symptom of attack traffic at wire speed, when maintaining high detection accuracy."
"the performance of the proposed framework under the dbscan algorithm on the collected dataset was also evaluated. figure 6 illustrates the results of continuous authentication through an unsupervised approach, namely dbscan. similarly, verification performance is better when long term activity is assigned higher weights. however the best settings for the users under dbscan are 15%-85% and also 50%-50%. at the end of the training period, the authentication error probability for all users is below 0.1 % except user 6. this corresponds to the situation when user behavior on social network applications has been verified as an anomaly so the back-end server sends a biometric authentication triggering signal to the front-end device. in figure 6, for each user, the anomalies marked by the ml-based continuous authentication (which are observed by an increase authentication error probability in the plots) is due to users' having different social behavioral patterns in weekdays and weekends. for instance, user-1 has been found anomalous from day-13 to day-15 with increasing authentication error probability, whereas user-4 has been found anomalous from day 26th to day 29. over the weekends, most participants follow different profiles, provide less data as they interacted less frequently over social networking applications. furthermore, the participants' travel patterns change over the weekends, which in turn, affects the feature set of the ml-based authentication framework."
"f(6),f (7) f(1) f(6) f (7) f ( we now describe the query processing algorithm on the above tree. for a pbtree t, we use t.root to denote the root node of t, t.lef t to denote the left subtree of t, and t.right to denote the right subtree of t . for a node v, we use l(v) to denote the label of v, which is a set of prefix families, and u (v) to denote the union of all prefix families in"
"the query result size is another important factor since the worst-case run-time search complexity of pbtree is given by o(r. log n ) where r is the query result size. but the challenge is that, since the data values are not in any particular sequence, it is difficult to know which range queries can generate the desired query result sizes after the pbtree is built. to handle this issue, prior to the pbtree construction, we sort the data items and determine the appropriate range queries, which will result in the desired query result sizes and use these queries in our experiments. for our experiments, we chose query ranges which result in query result sizes varying from 10 to 90 data items."
", which is less than 1. therefore, in summary, the computational complexity of finding the leftmost \"1\" bit in a generated permutation is less than m · (2 − ("
"where n is the total number of the hash values generated over a new item, bias and se represent the bias and standard error in the estimate process respectively. to get enough substrings for generating n permutations (n should be larger than 1024 [cit] ), m is always larger than or equal to 7. therefore, the whole randomness reduces little when we use a group of generated permutations instead of a group of original hash values in the estimate process, and thus our lfm sketch can achieve similar estimate accuracy as fm sketch."
"1) the attacker may mimic legitimate requests, thus the malicious requests are indistinguishable from the legitimate ones. to identify cpa attacks, we need to analyze these attacks to choose suitable symptoms for detection purpose."
"an ndn user retrieves name-based content by three steps: (1) to request name-based content (data), a consumer (user) first sends an interest (data request packet) identified by the name of the requested content. (2) after having received this interest packet, a router records this interest and its ingress interface into its pit (pending interest table) . note that the duplicate interests are aggregated to one entry by appending the ingress interface list. if the requested content has been cached in its cs (content store), it will satisfy this interest with the cached copy. otherwise, it will search its fib (forwarding information base) to longest match this interest's name and find the egress interfaces, and forward this interest from these interfaces to the next routers. if this interest can not be satisfied by the routers along its transmission path, it will reach the corresponding content provider (content producer). (3) when the content packet is sent back, the router finds the corresponding pit entry and forwards the content packet to all the ingress interfaces listed in the pit entry. in addition, the router removes the corresponding pit entry, and caches the content packet in its cs."
"the data collection module is responsible for storing sessions in a standard format so that they can be analyzed more conveniently. to do that, after the session record is created, it is converted to the json format and sent to the private cloud server. the cloud server and the analytics performed over the cloud are illustrated in figure 2 ."
"in this section the results of applying svm on the dataset are presented. the behavioral data have been collected for 76 days. the system was trained within the first week of data and svm was set to six different classes corresponding to each user. as mentioned before two normalization techniques are applied, namely the soft normalization and hard normalization to the results of the ml processes. the proposed framework is also improved by dynamically adjusting the contextual parameter of weights α and β in long term sociability signature which is shown in eq. (3) and eq. (7). to be able to analyze the impact of the contextual parameters on the performance of the proposed framework, wide range of values have been set in the form of ((α )-(1-α )) for social activity rate, and in the form of ((β )-(1-β )) for sociability factor as follows: 15%-85%, 30%-70%, 50%-50%, 70%-30% and 85%-15% where each set refers to α ( β ) and 1-α (1-β). for example, 15%-85% means α and β are equal to 15%. figure 5 illustrates the results under svm with soft normalization technique. by applying different values for α and β, it can be concluded that the performance of verification by using svm has better results when α or β are low."
8 insert s into l. 9 while (subsets si and sj are mergable in l) do 10 merge si and sj into one subset sij . replace si and sj with sij in l. 11 if l contains only two subsets s1 and s2 then 12 return s1 and s2.
"unfortunately, in-network caches may become victims of malicious behaviors. a malicious user can violate the content locality of the in-network caches by performing a cache pollution attack (cpa) [cit] . in this way, it can cause a large number of cache misses and significantly degrade quality of services for regular users. there are primarily two types of cpa attacks: locality-disruption attack (lda) and false-locality attack (fla) [cit] . an lda attack continuously requests distinct unpopular content, by which it keeps pushing the unpopular content into the cache of the routers along the transmission path, and thus squeezing out the popular content. to perform an fla attack, the malicious user repeatedly requests the same set of unpopular content, by which it is able to keep this set of unpopular content in the cache, and thus reduce cache hits of content requests from legitimate users."
"to simulate background traffic, each legitimate consumers sends 3000 regular interests every second. the name of an interest consists of a name prefix (i.e., the first component of the name) and a distinct random number (i.e., the second component of the name). we used five optional name prefixes throughout our simulations: \"/google.com\", \"/amazon.com\", \"/youtube.com\", \"/yahoo.com\" and \"/facebook.com\". we manipulated the random number to distinguish different types of interests (i.e., regular interests, interests requesting unpopular content, and interest requesting nonexistent content), such that a different type of interest will use a random number from a different range. the request frequency of different content by legitimate consumers followed three zipf-like distributions with different parameters. their α (the popular content will be requested more frequently with a larger α) was 0.7, 0.9 and 1.1 respectively, while the number of the different content was 10000."
"sociability of users is not limited to their data consumption but it is also a function of the time they spend on mobile social network applications. therefore we define the sociability factor metric as another verifier. similar to the social activity rate, the sociability factor also has instantaneous, short term and global components that ultimately lead to a normalized sociability factor value. thus, instantaneous sociability factor per app is calculated as the total time that a user spends on a social networking app in a single session as formulated in eq. (9). short term sociability factor (sf ) is defined as the average time that a user spends on a particular social network app in a session over a short time window, e.g., a day, as formulated in (6) where t"
"implementation of a baseline scheme. to show the effectiveness of elda, we also implemented the most efficient detection scheme [cit] in the literature as a baseline for comparison. this scheme monitors the interest traffic, and calculates the number of each newly requested content. it identifies a cpa attack by checking whether the requesting frequency of a recently requested content deviates from a threshold, which was obtained based on the expectation and the variance of the past requesting frequency for this content."
"we analyze the computational complexity of the leftmost \"1\" bit search of fm sketch and lfm sketch as following. to find the leftmost \"1\" bit in a hash value, fm sketch searches the hash value from the leftmost bit, and the computational complexity is"
this scenario mimics the situation where random user pairs were selected to use each other's mobile device for five consecutive days after the platform has been trained.
"each figure illustrates the authentication error probability (aep) under the proposed system during the 5-day period after a user's behavior has been learned (i.e., converged authentication error probability). the time when the user behavior has been learned also denotes the time when the smartphone can be safely recruited for opportunistic or participatory sensing purposes within the iot architecture. user is recruited for opportunistic or participatory sensing purposes in the iot context has to be in an implicit manner. thus, the authenticity of the smart device user should not undergo biometric authentication frequently. as (10) formulates, aep t stands for the disruption probability that results in after biometric authentication has been triggered: the ratio of the cumulative value of false rejections or true rejections (fr and tr) starting from the beginning of training moving to the end of the time of interest (t) to the cumulative value of total acceptances and rejections. disruption (aep) affects user experience negatively and may result in de-incentivizing users in participating iot sensing through their smart mobile devices. on the other hand, false acceptance may lead to reduced trustworthiness of the sensory data acquired through built-in sensors of these devices. therefore, we also present volume 5, 2017 the false acceptance probability, and the impact of the contextual parameter weights (α − β) on the number of false acceptances."
"in this section, we first present our pbtree construction algorithm, which is executed by the data owner. this algorithm consists of three steps: prefix encoding, tree construction, and node randomization using bloom filters. second, we present our algorithm for computing the trapdoor for a given query, which is executed by the data users. with the pbtree of n data items and the trapdoor for a given query, the cloud is able to process the query on the pbtree without knowing the value of the data items and the query."
"social activity rate corresponds to the relative amount of data that a user generates when using social networking applications. the absolute data usage of a user is normalized by the data usage of all active users. social activity rate of a user is a function of the user's short term (daily) and instantaneous social activity rates. instantaneous social activity rate denotes the data usage by a particular social network application in a single session. thus, d eq. (2) formulates user's short term (daily) activity, which denotes the average data usage that is spent on social network app in a session per day."
"since the conventional fm sketch (e.g., the hyperloglog fm sketch) only uses a small fraction of information in each generated hash value (i.e., the leftmost \"1 bit), it requires to generate a large number of hash values. our novel lfm sketch tries to fully utilize all the bits of a hash value, such that we can eliminate the unnecessary overhead resulted from the hash operations. our lfm sketch outperforms the existing fm sketch in two aspects. firstly, we perform less number of hash operations during the monitoring phase. our key idea is to use only one hash function instead of a set of hash functions. for each scanned item, we first use a hash function to generate a hash value, and then re-arrange the bits in this hash value, generating a large number of permutations, which can be used to replace the hash values generated by multiple hash functions in the existing fm sketch. as it is shown in section v, by using one hash function in the aforementioned way, we achieve the similar estimation accuracy as using multiple hash functions. secondly, we further optimize our lfm sketch by skipping the permutation generation process. we present our lfm sketch in fig. 2 (for convenience, we also include the steps of the hyperloglog fm sketch as a comparison), with a detailed explanation in the following. for each incoming item, we first generate a hash value (step a-1). compared to the hyperloglog fm sketch algorithm (step b-1), our lfm sketch significantly reduces the number of hash operations. to guarantee estimation accuracy, we need an equal number of random values comparable to the hash values in the hyperloglog fm sketch. thus, we split the hash value into a group of substrings (step a-2.1), and re-arrange these substrings to generate their permutations (virtual step a-2.2), which will be used as the hash values in the hyperloglog fm sketch. we further update a bitmap (step a-3) by setting its j-th bit as 1, where j is the index of the leftmost \"1\" bit of the corresponding permutation. periodically, we obtain the index of the leftmost \"0\" bit in each bitmap, and compute the harmonic mean of these indices, estimating the cardinality of the scanned items (step a-4 and a-5)."
training strategy builds a profile for each user based on the collected data. training is performed continuously on a sliding window of data over time. this allows capturing naturally altering patterns of user behavior.
"in ndn, it is difficult for a malicious user to perform the fla attack in a conventional way. when a piece of content is retrieved, the routers on the transmission path will cache a copy of this content, which will be used to satisfy subsequent requests for this content [cit] . therefore, the repeated malicious requests for the same content will be satisfied by the copy cached in the neighboring routers, and will not be forwarded to other routers. thus, repeatedly sending malicious requests for the same content can only increase this content's cache hits in the neighboring routers, i.e., a conventional fla attack can only affect the neighboring routers of the malicious users, rather than the overall ndn network. however, we successfully construct an fla attack which can affect the whole ndn network. in our construction, we smartly throttle the newly arriving content to prevent the unpopular content from being squeezed out."
"finally, figure 8 (a) and figure 8 (b) show the average range query processing time and false positive rates, respectively, for different query result sizes, where the corresponding indexes, p b w d, linear, binary and bucket, are built on a data set of 5 million data items."
"by now the pbtree is fully constructed from data items d1, · · ·, dn by the data owner. the data owner sends the encrypted data items and the pbtree to the cloud."
"the pit size of each router is 15000, and the cache size is 1000 (which is 1% of the total number of distinct content used in the simulations). since a cpa attack can significantly affect the in-network caches using all different types of replacement strategies [cit], we chose lru (least recently used), which is commonly used by the in-network caches [cit] ."
"in this section, we propose elda, an efficient and lightweight scheme for detecting cpa attacks. specifically, in section iv-a, we provide a strawman solution, in which we directly rely on the fm sketch (see section ii-b1). this simple solution, however, is not efficient enough due to the large number of redundant hash operations performed in the fm sketch. in section iv-b, we propose a novel lightweight fm (lfm) sketch, and design elda based on this lfm sketch."
"we chose the gowalla [cit] data set, which consists of 6,442,890 check-in records of users, over the period of feb. 2009 to oct. 2010, and extracted the time stamps. now, given that each time stamp is represented as a tuple : year, month, date, hour, minute, second, we performed a binary encoding for each of these attributes and treated the concatenation of the respective binary strings as a 32-bit integer value, while ignoring the unused bit positions. we perform our experiments on 10 fixed size data sets varying from 0.5 to 5 million records with a scaling factor of 0.5 million records, respectively, chosen uniformly at random from the 6 million-plus total records in the gowalla data set."
"2) anomalous condition denotes the situations where we have created spoofed identities by mapping a randomly selected user's patterns onto the records of a particular user volume 5, 2017 after the continuous authentication system has been trained to verify the social behavioral context of the corresponding user. this could result in false acceptance which is also aimed to be minimized."
"given data items d1, · · ·, dn, the data owner encrypts these data using a symmetric key k, which is shared between the data owner and data users, generates an index, and then sends both the encrypted data denoted (d1) k, · · ·, (dn) k and the index to the cloud. given a query, the data user generates a trapdoor and then sends it to the cloud. the index and the trapdoor should allow the cloud to determine which data items satisfy the query. yet, in this process, the cloud should not be able to infer useful information about the data and queries. the useful information in this context includes the values of the data items, the content of the queries, and the statistical properties of the data items. other than encrypted data and encrypted queries, together with query results, the cloud may have information obtained from other channels, such as domain knowledge about the data (e.g., age distribution). however, even with such information, a privacy preserving range query scheme should not allow the cloud to infer additional information about the data based on past query results."
"the performance evaluation of pbtree is dependent on two factors: query types and query results size. we consider two query types: prefix and range queries. a prefix query is a query specified as a single binary prefix, whereas, a range query is specified as a numerical range and is likely to generate more than one binary prefixes. the prefix queries are effective in evaluating the performance of pbtree under the two types of optimizations we have described, and the range queries are effective to evaluate the performance of pbtree against other known approaches in literature. for each data set, we generate a distinct collection of 10 prefix query sets, where each prefix set contains 1000 prefixes, and similarly, we generate 10 distinct range query sets, where each set contains 1000 range queries. the average number of prefixes for denoting a range in our range query sets vary from 5.93 to 9.6 prefixes, respectively."
"the p b w d structure exhibits higher query processing efficiency and records lower false positive among all pbtree structures. from the figures, we note that, for the same the query result sizes, p b w d executes, 2.153, 2.309, and 2.533 times faster than p b b, respectively; and the corresponding false positive rates in p b w d are, 0.88, 0.8, and 0.83 times, smaller than in p b b. in comparison, for the same query result sizes, p b w executes 0.516, 0.406, and 0.444 times faster than p b b, respectively; and the corresponding false positive rates in p b w are, 0.25, 0.186, and 0.21 times, smaller than p b b."
"a detailed description of the lfm sketch is provided in algorithm 1. on line 2, a new item is hashed, and the generated hash value is split into m substrings. we find the leftmost \"1\" bit in each substring and store the results into an index set (line 4-5). line 7 begins a loop to calculate the index of the leftmost \"1\" bit in each permutation. for each permutation, in accordance with the sequence of substring identifiers in the corresponding permutation pattern, we iterate the index set to find the first nonegative value and calculate the index of the leftmost \"1\" bit in this permutation (line 8-17). as long as the leftmost \"1\" bit in this permutation is found, we aggregate this result into the corresponding bitmap (line 18). to obtain the estimation result, we search the leftmost \"0\" bit in all the bitmaps (line 21), and calculate the harmonic mean of these positions, obtaining the cardinality estimation of the scanned items (line 22). est: the estimation of cardinality of the items; 1: while the estimation period is not end do 2:"
"a significant amount of work has been done in privacy preserving keyword queries [7, 8, 11, [cit] 21, 22, 27, 28, 36, 39] . however, these solutions are not optimized for range queries."
"in elda, we apply a monte carlo hypothesis test to estimate a proper cpa detection threshold. according to chebyshev's inequality [cit], the traffic monitoring results will fluctuate around their expectation under an upper bound with high probability. since this upper bound is related to the expectation and the standard deviation of the recent traffic monitoring results, we can estimate it according to the recent traffic monitoring results so as to get a threshold for distinguishing cpa attack traffic from regular traffic. specifically, we do a monte carlo hypothesis test [cit] on the recent traffic monitoring results to verify the detection accuracy of a candidate detection threshold, in which we take a pre-set level of significance to indicate whether a candidate threshold is an accurate estimation of the upper bound or not. if a test result cannot reach the pre-set level of significance, we further enlarge the candidate threshold until we can achieve a qualified test result. we then return the corresponding candidate threshold. since the monte carlo hypothesis test can be performed on a small sample, the size of the sample applied to estimate our detection threshold is reduced and the performance of elda is further improved."
"in this paper, we propose the first privacy preserving range query scheme that achieves index indistinguishability. our key idea for achieving index indistinguishability is to organize all indexing elements in a complete binary tree where each node is represented using a bloom filter, which we call a pbtree (where \"p\" stands for privacy and \"b\" stands for bloom filter). pbtrees allow us to achieve index indistinguishability because it has two important properties. first, a pbtree has the property of structure indistinguishability, that is, two sets of data items have the same pbtree structure if and only if the two sets have the same number of data items. the structure of the pbtree of a set of data items is determined solely by the set cardinality, not the value of data items. second, a pbtree has the property of node indistinguishability, that is, for any two pbtrees constructed from data sets of the same cardinality, which have the same structure, and for any two corresponding nodes of the two pbtrees, the values of the two nodes are not distinguishable. thus, our scheme prevents cloud from performing statistical analysis on the index even with domain knowledge."
"having said that behavioral biometrics can be applied in smart environments, smart cities can be considered as another application area [cit] . ziegler [cit] provided a comprehensive research on the applicability of adapting behavioral biometrics in smart environments. the author described four possible smart environment applications including smart homes, smart media devices, smart traffic systems and smart health in which implicit identification mechanisms can be applied."
"to mitigate the cpa attacks in ndn, we propose elda, an efficient and lightweight cpa detection scheme. we first analyze the cache pollution attacks in ndn, discovering a common attack symptom for both of the lda attack and the fla attack. in addition, we design a novel lightweight fm sketch (lfm sketch) which can capture the attack symptom in the interest traffic. finally, we propose elda based on the lfm sketch. both the theoretical analysis and the simulations demonstrate elda could effectively and efficiently detect cpa attacks in ndn."
"the normalized social activity rate (a normal i ) is aggregated overall social activity rates of a user averaged by the maximum social activity rate in the pool of active users as shown volume 5, 2017 in eq. (4)"
"two machine learning (ml) approaches are used, namely support vector machines (svm) [cit] and density-based spatial clustering of applications with noise (dbscan) [cit] to authorize user access to mobile devices. svm is a supervised learning method that basically defines hyperplanes which separate the data into different groups while dbscan groups the data points that are nearest neighbors of each other, and aims at forming dense regions. we also present a set of selected users where we randomly injected daily behavioral patterns of other users to each of these users for randomly selected five days after behavioral patterns have been learned. the experiments have been carried out under the following two scenarios: 1) normal condition denotes the scenarios where user identities were not spoofed, and the only possible false alarm in continuous authentication could be the false rejection. this results in the system to fall back to biometric authentication to validate the user, even though it is a legitimate user. the aim is to minimize false rejections."
"the straightforward implementation of the above query processing algorithms requires to check each row of m [a,b] at each visited pbtree node. note that for a row"
"the cpa attacks usually target to violate the content locality in ndn network caches. when a cpa attack is successfully performed, the legitimate users will not be able to retrieve the expected content from the network caches. instead, they may need to turn to the distant content provider, which will bring higher delay and reduce network throughput. cpa attacks can be characterized into two types, a locality-disruption attack (lda) and a false-locality attack (fla). an lda attack continuously requests distinct unpopular content, ruining the cache content locality. in contrast, an fla attack repeatedly requests the same set of unpopular content, creating a false content locality at caches."
"we further optimize the aforementioned virtual steps (i.e., virtual step a-2.2 and virtual step a-2.3), such that we don't need to really generate any permutation, and thus improve performance. actually, a permutation is related to a rearrangement of the elements of an ordered list. if we assign an identifier to each element in the list according to its position, a specific permutation procedure can be represented by an ordered list of these identifiers (i.e., permutation pattern). these permutation patterns are fixed for whatever we permute. we can take advantage of the permutation-pattern to obtain the position of leftmost \"1\" bit in the corresponding permutation without generating it. specifically, we assign an identifier to each substring of the original hash value. for each substring, we store the index of its leftmost \"1\" bit into an index set (step a-2.2). we can get the index of the leftmost \"1\" bit in any permutation by iterating the index set according to the order of the identifiers in the corresponding permutation pattern (step a-2.3)."
"proof: (sketch) according to lemma 1, the randomness of a permutation used in lfm sketch is no worse than a hash value generated in fm sketch. although the generated permutations may contain repeated leftmost substrings, the probability that two permutations contain the same leftmost \"1\" bit is quite small. this probability can be computed as following:"
"since an ndn router will discard repeated interests due to the interest aggregation mechanism in its pit, a conventional fla attack is difficult to be performed, which requires the attacker to repeatedly solicit the same set of content. however, we successfully construct a new fla attack by two steps: (1) the attacker requests distinct unpopular content to deplete the content stores of the routers along the critical transmission path (e.g., gateway routers). (2) the adversary sends a large number of interests with forged names to overwhelm those routers' pit tables (i.e., an interest flooding attack [cit] ). in this way, newly arriving interests will be dropped due to the insufficiency of pit space, and thus no new content will arrive. consequently, the unpopular content will stay in those routers' content stores."
"3) the cpa attack itself has already consumed a large number of computation and memory resources of the victim. utilizing minimal computation and memory resources from the victim to detect cpa attacks is not straightforward. to handle the aforementioned challenges, we investigate the impact of cpa attacks in ndn and propose a lightweight detection solution. our solution takes advantage of the difference between the attack traffic and the regular traffic. we demonstrate that our approach provides a better performance at a lower cost, and is able to detect both the lda and the fla attack accurately."
"fla attacks. we also simulated the damage effect of six fla attacks, fla1, fla2, fla3, fla4, fla5 and fla6. the compromised consumers request unpopular content from the second 2 and request non-existent content from the second 3. the name prefixes of these malicious interests all are \"/yahoo.com. we have three observations: firstly, when the compromised consumers send more malicious interests per second, the performance of the gateway router is degraded significantly. secondly, the fla attacks have the damage effect on the pit (see fig.4(d) ), i.e., the available space of pit declines to 0 and no more subsequent interests can be forwarded. in this way, the attacker keeps the unpopular content in the cs by blocking the subsequent content, and thus the cs hit rate significantly degrades. thirdly, the fla attacks affect the gateway router more significantly compared to the lda attacks (see fig.4(e) ). the fla attacks delay the average round-trip time much more compared to the lda attacks (see fig.4(b) )."
order preserving schemes have weak privacy protection because they allow the cloud to statistically estimate the actual values of both data items and queries [cit] .
content provider gateway router router 1 router 2 router 3 consumer 2 consumer 3 consumer 4 consumer 5 consumer 7 consumer 6 consumer 1 consumer 8
"to evaluate the performance of pbtree, we considered three factors and generated the various experimental configurations. the metrics considered are: the data sets, the type of pbtree construction, and the type of the queries. based on these metrics, we have comprehensively evaluated the construction cost of the pbtree, the query evaluation time and the observed false positive rates."
". among all possible query result sets r of the same size a, we use ma be denote the maximum expected number of false positives. thus,"
"once the continuous authentication platform has been trained, in order to imitate the situation where identities were spoofed which can be due to exchanging mobile devices between users or stolen devices, we introduce artificial noisy patterns to the social behavioral profile of each user in particular days. the noisy patterns are created by copying a usage pattern on the records that belong to another user."
"we performed experiments with three variants of the pbtree: the basic pbtree without any optimizations, denoted as p b b, the pbtree with width optimization, denoted as p b w, and the pbtree with both depth and width optimizations, denoted as p b w d. we have not performed experiments for the case of the pbtree with only depth optimization due to the following reasoning: when searching on a bloom filter we may need to perform two checks, which is twice the effort. if a query prefix is not found in the bloom filter, then we need to perform a second check, using a different set of hash functions, to check if the prefix is a common prefix in the bloom filter. as a result, depth optimization is more effective when combined with width optimization because width optimization aggregates the common prefixes in a systematic manner. therefore, we focus only on the performance evaluation of p b b, p b w and p b w d."
"elda can effectively detect cpa attacks. according to theorem 1, lfm sketch can achieve nearly the same estimation accuracy as fm sketch. elda relies on lfm to monitor the interests possessing a common name prefix, and thus is able to detect the burst of the distinct content requested by these interests accurately. in addition, elda can identify a potential cpa attack by using a proper threshold generated by monte carlo test."
"without loss of generality, we view the pbtree as a list of bloom filters, where each bloom filter stores a distinct set of prefixes and answers user queries. therefore, we note that, that the proof of security of pbtree is equivalent to proving that any given bloom filter is ind-cka secure satisfying the following properties: (a) the contents of the data items are not revealed from the structure of the bloom filter in which they are stored or from the contents of other bloom filters, and (b) given any two bloom filters, with different number of data items, they are indistinguishable to an adversary. we consider a non-adaptive adversary, which has a one-time finite trace of the search history consisting of a finite set of secure trapdoors and their corresponding search results. to complete the proof, we demonstrate the construction of a probabilistic polynomial time simulator s, which can simulate the secure index using only this finite trace of the search history. the adversary interacts with the simulator as well as the real index and is challenged to distinguish between the results of the two indexes with non-negligible probability. we consider the length of the key s as the security parameter in the following definitions:"
"in this section, we compare elda with the baseline scheme in terms of effectiveness, efficiency and resource consumption. we will show that compared to the baseline scheme, elda maintains the same effectiveness while having much better efficiency and significantly less resource consumption."
"the idea of merging iot and social network phenomena under the concept of social internet of things (siot) has been emerging [cit] . this convergence has many privileges including network navigability, service scalability and increased in level of trustworthiness by connecting the objects that interact on frequent basis [cit] ."
"in addition to all, we are currently extending the feature set and collecting a richer set of data to reduce verification and training duration. in addition, energy-efficiency is an important concern for mobile platforms; therefore energyefficient continuous verification mechanisms are also being developed within the ongoing research efforts."
"in our simulations, we deployed a tree-like network topology (see fig. 3 ). as the common part of real internet topologies, the tree-liked topology is commonly present in the local networks. cpa attacks traffic will converge at the routers along the critical transmission path, e.g., the gateway routers. in this network topology, we deployed eight consumers, three edge routers, one gateway router and one content provider. we assume consumer 2, 4 and 7 are malicious (i.e., they have been compromised by the attacker and only send malicious interset packets)."
"to evaluate the damage effect of cpa attacks, all the compromised consumers send malicious interests following our attack model described in section iii. the name in a malicious interest also consists of a name prefix and a random number. in our simulations, the attacker will use \"/yahoo.com\" as the name prefix. implementation of elda. when implementing elda, we used murmur hash function [cit] to generate hash values due to its efficiency. each interest name is hashed to a 32-bit hash string, which is further split into 8 substrings. in this way, we are able to estimate the cardinality of the requested content accurately, since we are allowed to use at most 8! bitmaps. in our implementation, we randomly selected 256 bitmaps. when using monte carlo hypothesis test to obtain the threshold, our significance level is 0.5%, and the sample size is 10."
"1) we analyze both the lda and the fla attack, and discover a significant symptom in both attacks, by which we design a across-the-board scheme to detect both of them. 2) we design a lightweight flajolet-martin sketch, namely, lfm sketch, which can capture the symptom of attack traffic effectively and efficiently. in addition, we construct elda, an efficient cpa detection solution, which uses the lfm sketch to monitor interest traffic with high performance and low resource consumption. 3) we analyze the security of elda, and evaluate its performance in ndnsim [cit] . in section ii, we provide thorough background knowledge on ndn and introduce the primitives we rely on in our design. we then present our threat model in section iii. we design an efficient scheme to detect cpa attacks in section iv, and analyze its security in section v. we study cpa attacks in ndn and evaluate the effectiveness of our design in section vi. we outline the related work in section vii and conclude in section viii."
"this module is an android application that runs as a background process over the operating system. the application collects and updates user location information every 5 minutes. it monitors access to facebook, twitter, linkedin, skype and whatsapp applications. these interactions are recorded in sessions. each session has a session id, duration, initial location and the amount of data that is used during the session."
"is a weighted sum of the short term sociability factors where t k denotes the k − th short term sociability factor used in the calculation, and β is a weight factor for each mobile social network app. finally, as expected, the normalized sociability factor (sf normal i ) is the aggregated overall sociability factors of a user scaled by the maximum aggregated sociability factors in the active users pool as shown in eq. (8) ."
"there are two key technical challenges. the first challenge is the construction of pbtrees by data owners. we address this challenge by first transforming less-than and bigger-than comparisons into set membership testing (i.e., testing whether a number is in a set), which involves only equal-to comparisons, and then organize all the sets hierarchically in a pbtree. this transformation helps us to achieve node indistinguishability because the less-than or biggerthan relationship among pbtree nodes is no longer statistically meaningful. the second challenge is the optimization of pbtrees for fast query processing on the cloud. we address this challenge by two ideas: pbtree traversal width minimization and pbtree traversal depth minimization. the idea of pbtree traversal width minimization is to minimize the number of paths that the cloud needs to traverse for processing a query. we prove that the pbtree traversal width minimization problem is np-hard, and propose an efficient approximation algorithm. the idea of pbtree traversal depth minimization is to minimize the traversal depth of the paths that the cloud needs to traverse for processing a query; in other words, we want the traversal of many paths to terminate as early as possible."
"physical location refers to an absolute geographic position on the surface of the earth, usually expressed as a pair of geographic coordinates. an address like downing street 10, london, uk can be derived from location coordinates and from this, one is able to understand in which country, city and specific area the geographic point belongs."
"as mobile phones are primarily communication devices [cit], a common task that their users often accomplish in order to start a new communication session is that of contact retrieval. there exist several mobile interfaces from which a contact can be retrieved: contact list, recent call list, speed dial widgets, notifications etc. from all these alternatives, the contact list appears to be the most popular choice of users when placing a new call [cit] . however, the mobile contact list repository may contain hundreds [cit] or even thousands (especially when importing contacts from social networking and 3 rd party communication services) [cit] of items and at the same time users are reluctant to delete unused contacts [cit] . as a result and taking also into account the screen size limitations of handheld devices, the cognitive load required for the task of contact retrieval increases when performed on a mobile phone."
"it is well-known that the lms-type algorithms widely used in afc suffer from biased estimation due to signal correlation [cit] . consequently, the feedback path estimate can be erroneous if decorrelation is not carefully considered. although the pem-based pre-filter [cit] has provided certain amount of decorrelation, further improvement is achievable by inserting additional signal processing into the forward path of the ha [cit], usually placed at as shown in figure 4 . existing methods include frequency shifting (fs) [cit], phase modulation [cit], time-varying all-pass filters to introduce phase shifts [cit], linear predictive coding vocoder [cit], to name a few. in general, quality degradation might be introduced by these decorrelation methods and thus there is the trade-off between the sound quality and the decorrelation ability for afc improvement."
"in our original work, we used a temporal training window to assist prediction. this approach would have been problematic here, since it would result in empty training sets for low communication activity clusters that the user had not visited during the training period. to overcome this obstacle, we made a slight modification of the algorithm and switched from a temporal training window to a numerical one, by computing for each user the calls made per day and multiplying with the number of days we wanted to include in our training. using a training window of 15 days (as in our previous work) we traverse the users' call logs and for each call after this period we simulate the prediction procedure that provides weighted suggestions for presentation in an adaptive interface with 1, 3 and 5 suggestion positions respectively. the prediction accuracy results for each user type group are presented in table v where for each number of suggested contacts, the left column shows the percentage of successful predictions for the base experiment (b1, b3 and b5 respectively) and the right column shows the impact of physical location as the difference of the two percentages (ǻl1, ǻl3 and ǻl5 respectively). we should note that users who present no mobility were omitted from this table, since there is no difference in prediction accuracy when running the two experimental simulations."
"in summary, it can be seen that although user location seems to be regarded as an important contextual cue for predicting the next contact to be called, it has been utilized together with other context cues without being analyzed for its effect and without investigating its efficacy."
"to perform our analysis, we clustered for each user all geo-located calls at a city level. there are several algorithms for spatial clustering of geographic tracks, with one of the most widely used in bibliography being dbscan [cit] ."
"in the future, apart from mixing physical location with recency, we intend to analyze other available datasets, including one dataset collected in an experiment that we organized in order to evaluate our adaptive user interface. we hope to be able to get further insight regarding the effect of physical location and other contextual cues (semantic location, activity, scheduled calendar events etc.) on call prediction."
"in this paper, we attempt a thorough examination of physical location as a context dimension for predicting the next callee. we analyze call logs from the nokia mobile data challenge dataset [cit] and we present interesting patterns regarding the effect of physical location, on a city level, on mobile communication. we show that using a location dependent frequency dimension for predictions instead of normal frequency, does not necessarily lead to improvements in the accuracy of predicting the likelihood of communication with contacts for all types of users included in the dataset under review and we also discuss the possible reasons behind this limited impact."
"finally, exploiting co-occurrence of behavior patterns of mobile users is another approach for predicting outgoing call events and providing ui call shortcuts [cit] . semantic place (e.g. home, work, outside) is considered part of the user's context in this case. as the prediction engine provides suggestions according to a confidence prediction level and not when this level is below a specific threshold (no guess situation), the researchers present the trade-off between recall and precision. to directly compare with other research efforts on call prediction, one should consider the case where recall is 100% and we can see that in this case the accuracy is close or even lower to other outgoing call predictors (around 30% for 1 suggestion, 45% for 3 suggestions and 55% for 5 suggestions)."
"without any feedback control mechanism, the frequency responses of the ha processing g(e jω, n) and the feedback path f (e jω, n) form a closed-loop system which exhibits instability that leads to howling. the nsc [cit] states that the closedloop system becomes unstable whenever the following magnitude and phase conditions are both fulfilled [cit] :"
"to investigate the role of location context in predicting calls, we used the nokia mobile data challenge dataset [cit], which is probably the richest mobile dataset currently available, especially regarding collected location attributes [cit] and tracked users' social interactions (calls, short messages, scanned bluetooth devices), location, media creation and behavior (applications usage, activity detection etc.). the number of participants reached 185 (38% female) and more than 240.000 calls were logged, while more than 26.000.000 location points were recorded [cit] . location sources include gps traces providing raw spatial coordinates, wireless lan access points whose location was estimated by available gps data and gsm cell towers. however, to protect participants' privacy, position accuracy of gps tracks was diluted. also, since users could switch gps or wi-fi off at will during the collection, many events in the dataset are not spatially tagged."
"it is worth noting that in practice we need to truncate the signal for the all-pass network to be realizable. therefore, the warping performance will depend on other factors such as the length and the type of the window function used."
"an observation of users' call logs and a step by step application of the algorithm on them revealed that there are often failures when users are moving from the main communication cluster to other locations, since the training sets used in these 'secondary' clusters fail to capture recent call trends. a possible improvement could arise from a combination of the location dimension with the recency dimension and this is a direction for future work we intend to follow."
"this research is part of the open speech platform (osp) [cit] funded by an nih/nidcd initiative to enable psychophysical research beyond what is currently capable in support of hearing healthcare. this paper is related to improving acoustic feedback reduction for form-factor accurate, audiologic research in the field using behind the ear, receiver in the canal (bte-ric) transducers, hardware, embedded software, and application software we developed [cit] . in order to compensate for mild to moderate hearing loss, commercial hearing aids (has) and osp provide an average gain of 35-38 db. in the emerging form factors for advanced has and hearables, including conventional bte-rics, there is a significant acoustic coupling between the microphones and loudspeakers (called receivers in the telephony and ha communities). this acoustic coupling varies significantly based on surroundings (e.g. hats, scarves, hands, and walls that come in close proximity to the transducers) and can cause the system to become unstable, when the audio content includes characteristic frequencies of the system. this instability results in brief \"howling\" artifacts and they are of immense annoyance to the ha users."
"in many practical situations, it is convenient to reuse the multichannel compression modules [cit] in ha processing for freping. for specific types of hearing loss (e.g. sloping, cookiebite, etc.), increasing the gain in higher frequency bands aids to fulfill the magnitude condition of nsc and freping hinders the phase condition to occur. thus, freping provides a way for simultaneously optimizing the parameters of multichannel compression and frequency lowering [cit] in has for individual hearing loss. in this work, we limit ourselves to negative values of α so that freping always shifts spectral content lower."
"for evaluation, we compare the feedback-compensated signal e(n) with the clean signal x(n), using the hearing-aid speech quality index (hasqi) [cit] which has been adopted in prior afc works [cit] . the hasqi score ranges from 0 to 1, where a higher value indicates better quality. figure 7 presents example spectrograms of the feedbackcompensated signal for several cases. we can see that freping effectively reduces the howling components present in the red boxes, resulting in improved quality. figure 8 demonstrates advantage of using freping by showing the average hasqi score over the 50 speech files for various gain settings. from the results we see that both the basic (lms) and advanced (slms) afc algorithms can benefit from freping. this indicates the ability of the proposed frequency warping method to further improve feedback reduction on top of many afc approaches. moreover, compared to the fs, freping demonstrates better performance under all the gain settings."
"on the other hand, it seems that there is a third user type: users for which we observe important deviations regarding outgoing communication patterns among the different clusters. such a case is presented in fig. 7, where we can see the percentage of outgoing calls to each contact per cluster for user 5947. it is obvious that this user has a completely different communication behavior within each cluster. however, this user type could also include users with observed deviations only in some of the clusters, which means that their behavior changes only in some of the places that they make phone calls from. consequently, one could expect that prediction accuracy might improve for this type of users if the physical location context is considered in a prediction algorithm."
"we evaluate the proposed freping system using computer simulations in matlab at a sampling rate of 16 khz. we implemented a 6-band system using a set of bpfs with non-uniform bandwidth whose center frequencies are 250, 500, 1000, 2000, 4000, and 6000 hz, respectively. frames of 128 samples with 50% overlap were utilized. the hann function was applied for windowing. 25 male and 25 female speech signals from timit database were used for simulations."
"due to the fact that the available location data from gps receivers were modified to protect the participants' privacy and since the accuracy from sources such as wireless lan access points and cell towers is low, we decided to examine the impact of physical location to outgoing call communication as a first step of our research and leave semantic location for future research. this context dimension could prove useful for scenarios like the following:"
"coefficient adaptation typically, lms-type algorithms are carried out for coefficient adaptation using the pre-filtered signals u f (n) and e f (n) to update the afc filter w(n) as:"
"several context dimensions have been proposed for adaptive algorithms, location being one of them. it is unanimously accepted in literature that location appears to be one of the most important aspects of context in mobile communication [cit], while mobile users rank location as the second more important aspect of context [cit] . however, for the case of adaptive contact interfaces this consensus seems to be based on little more than an assumption, since none of the research efforts have ever examined the actual impact of the caller location on selecting the contact to start a new call with."
"in this work, we wanted to examine the effect of physical location context, exclusively, on outgoing call prediction. for this reason, we modified our original prediction model [cit] to use only frequency as a contextual cue, in order to exclude the effect of recency dimension. for the purpose of establishing a performance baseline, we define the prediction metric to be the total frequency of outgoing calls for each contact. such a choice simulates the \"frequently called\" screen of the contact list application on some android devices. in order to compare with the case when physical location context is enabled, we perform a second experiment where we replace this dimension with the frequency of outgoing calls with each contact within the cluster that the user is at the time of prediction. in other words, we only take into account calls made from the place where the user is located. as explained in the previous section, we would expect a noticeable impact of this contextual cue mainly for users with a deviated communication frequency distribution among different location clusters."
"in this paper we examine the impact of physical location as a contextual cue for predicting the next callee, a function useful for the design of adaptive contact list interfaces. as already discussed in the introduction and related work sections of this paper, location is considered in literature to be an important contextual cue for mobile users and is often highlighted as a possible candidate dimension for outgoing call prediction. however, as far as we know, this is the first attempt to assess the effect of location on call prediction and we believe that this is the main contribution of our work."
"of course, we cannot unreservedly generalize the conclusions from this analysis. although at this time there does not exist a more comprehensive publically available dataset on which to test our algorithms, it is probable that our dataset did not contain users that fell into the situations described in the scenarios cited in section iii.b. it is possible that physical location plays an important role in communication behavior for people that their lifestyle involves frequent travelling and communication with more contacts than the average user. we should emphasize at this point that results are encouraging for some of the users belonging to the high communication deviation group, which counts only 6 members, a number probably too low to lead to statistically safe conclusions. however, we believe it is important that this analysis indicates that physical location may not be as useful a context dimension for call prediction for the majority of mobile users as one might expect and as literature on this subject suggests. although location in our work is based on a spatial granularity level that can be assumed to carry semantic interpretation (city), it is possible that finer-grained semantic representations of location (e.g. \"home\", \"work\", etc.) might yield better results."
"freping is an extreme version of fs [cit] and it plays a similar role for decorrelation. it introduces nonlinear frequency shifts and the distortions appear to be perceptually benign based on informal subjective assessments. as instability is most likely to occur at the high-frequency region, it is reasonable to manipulate the high-frequency content while keeping the low-frequency region intact to avoid degradation in quality. by providing additional decorrelation, freping can reduce the afc bias and thus a better feedback path estimate can be obtained, thereby improving the magnitude condition in nsc. on the other hand, freping also helps avoid the microphone and receiver signals from remaining continuously in phase with each other. this prevents the phase condition in nsc to hold at the same frequency at two consecutive instants. consequently, the input and output sounds could not build up in amplitude as effectively. therefore, the likelihood of instability is reduced."
"adaptive interfaces that take advantage of mobile context could offer considerable help to the users while searching for a contact. for the case of making an external call, such interfaces could include adaptive contact and speed call lists. such solutions require a context aware algorithm that predicts at any time the next contact to be called. in previous work [cit], we proposed such an algorithm that uses the context dimensions of frequency and recency of communication and whose top suggestions feed a speed dial list, facilitating contact retrieval [cit] ."
"another observed type includes users that while they make phone calls from different clusters, their communication pattern remains practically unaffected when changing physical location, i.e. they call the same contacts regardless of their current location. as a typical example, we can see in fig. 6 a graph showing the percentage of outgoing calls to each contact broken down by location cluster for the user with id 6168. this user makes phone calls from two clusters, while from table ii we can see that the majority of this communication for this user takes place within cluster 2. for all contacts that are called in both clusters we observe percentages of outgoing calls that are very similar, irrespectively of the location cluster. of course, in the cluster with the majority of communication there are calls to contacts that are not called from the other cluster. however, for this user we observe that the percentage of calls to each of these contacts is very low, below 5%. for this type of users one would again expect that physical location context would not add much value to the prediction procedure."
"as we can see, the results indicate that incorporating the physical location context in the prediction algorithm does not necessarily lead to improved prediction accuracy. when the differences are not statistically significant, the prediction results are not impacted by location. in most cases where the results are statistically significant, the differences are only marginal, while there are users for which the algorithm provides slightly worse or slightly better results. for users that belong to the 'low communication deviation' group, with similar call patterns within all clusters this seems normal. next, we analyze some typical examples, representative of the majority of users existing in the dataset under review, in order to explain the results for users who follow different communication patterns in different clusters (medium and high deviation user types). firstly, we observe that some of them place the majority of their outgoing calls mainly from one cluster, thus deviations of communication patterns within the other clusters play a minor role in prediction accuracy. such a typical case is user 5947 (high deviation) which mainly communicates from cluster 1, as shown in table ii . in fig. 8, we observe that for this user the contact frequency distribution for cluster 1 almost coincides with contact frequency distribution for the total outgoing communication."
the ids which is host based watches all the ongoing activities on an individual information system host. it makes sure that not a single security policy of information system is being disrupted [cit] . the host based ids are installed only on a single terminal/host and are given the responsibility of observing the status of that specific host(or server) only.
"to attempt to answer the above questions 20 different uci datasets are used with 53 different measures to study the impact of \"interestingness measures\" on associative classifiers. section 2 describes the interestingness measures and their properties and introduces 53 different probability-based objective measures reportedly used in association rule mining. in section 3 some related works studying interestingness measures are highlighted. the methodology of using interestingness measures in the three different phases of an associative classifier is discussed in section 4. experimental results, comparing the impact of interestingness measures on classification accuracy and the number of generated classification rules, are illustrated in section 5."
"although the loudness can vary in many ways, we assume that the loudness varies from a large (positive) 0 to a minimum constant value min ."
"the attack traffic that is correctly categorized is measured as true positive as shown in green block and the correctly categorized normal traffic is measured as false positive as shown in red block. after all the evaluation and tests as shown in 4th final confusion matrix in fig. 5, our proposed system showed the detection accuracy greater than 99% in the categorization of network traffic. our method proved that the artificial neural network algorithm used is capable of detecting ddos and dos attack traffic during the flow of genuine iot network traffic. it also improves the stability and reliability of the iot network by signaling the response system at the right time to avoid the network disruptions, thus improving and enhancing the performance of the iot network."
"misuse detection: or signature-based detection. the behavior of the system is compared with previously known types of attack patterns (or signatures). those action patterns which may present a security threat should be described and stored in the system. afterwards, the misuse detection technique attempts to recognize any type of bad behavior with respect to these stored patterns in the system [cit] ."
"anomaly detection: targets on normal behaviors, instead of attack behaviors. firstly, these types of systems define what comprises a normal behavior which is normally carried out by an automated training and then intrusion activities are flagged that differ from this normal behavior by a specified threshold [cit] ."
"hence, a suitable selecting measure based on an original rule set is not necessarily a suitable selecting measure for a pruned version of that rule set. the tables for rule reduction are not shown for lack of space. what is noteworthy is that the redundancy removal could even prune more rules from rule sets already pruned by measure-based pruning."
"we have used eight node sensors to compose an iot network. seven client nodes and one server relay node for data analytical purpose. network tap is used to capture traffic to avoid any kind of hindrance or change in the live traffic. the sensor node sends data towards server node and server node acknowledges by sending receive data reply that is based on data itself. this process enables the sensor nodes as shown in fig. 3, to acclimatize their behavior and respond to the ongoing phenomenon."
"in order to validate the validity of dlba, we selected the 14 benchmark functions to experimentize. the benchmark set include unimodal, multimodal, high-dimensional, and low-dimensional unconstrained optimization benchmark functions, where 1 -3 are unimodal functions, 4 -14 are multimodal functions; 1 -9 have certain theoretical minimum and 10 -14 have uncertain theoretical minimum."
"first, can \"interestingness measures\" have any effect on the associative classifiers on its three different phases: rule generation, pruning and selection, so that the mining algorithm improves both in terms of increasing classification accuracy and decreasing the number of rules?"
"verhein and chawla utilize the fisher exact test's (fet) ρ-value to extract only statistically significant rules and introduce a new measure, class correlation ratio(ccr), to select only the rules that are more positively correlated to the class they predict rather than the other classes [cit] . for classification, they use a strength score to rank the rules, a combination of ρ-value, confidence and ccr. they show on 6 uci datasets that they can outperform other classifiers including a confidence-based associative classifier."
"the impact of using different interestingness measures on each individual phase of the associative classifier was highlighted above. here the goal is to study the impact of using different interestingness measures both in the pruning and the selection phases together. for this reason, the best measures found in measure-based pruning are combined with the best measures found in selection phase for each dataset. for cases where the strategy using highest ranked rule is adopted for prediction, redundancy removal is also used after the measure-based pruning. the results for f-measure changes are shown in table 6 . in this table, the percentage of f-measure changes using measure-based pruning and using different measures in the selection phase are compared with that of the combination of these two phases. the results show that not only combining the best interestingness measure of each phase does not improve the f-measure, but, there are some cases with significant decrease in the f-measure. table 6 : comparing the changes of f-measure with the best measure used in measure-based pruning for f-measure improvement, the best measure used in selection phase, and the combination of these two measures. global support is used for rule generation and the selection phase is based on the highest ranked rules. fc is the short form for f-measure change."
"aiming at the phenomenon of slow convergence rate and low accuracy of bat algorithm, we put forward animproved bat algorithm with differential operator and lévy flights trajectory (dlba) based on the basic framework of bat algorithm (ba), the purpose is to improve the convergence rate and precision of bat algorithm. in this paper, we define the frequency fluctuations up and down when the bat tracking prey, which influence the bats' location problem; it is more graphic to simulate the bat's behavior. moreover, it can make the algorithm effectively jump out of the local optimum to add lévy flights and differential operator, the main reason is because lévy flight has prominent properties in the previously mentioned and the differential operator can guide bats find better solutions, sequentially, increase the convergent speed. in addition, bats' position variation is influenced by the pulse emission rate and loudness as well. firstly, pulse emission rate causes update of position, and more and more new position can be explored, consequently, increasing the diversity of population. secondly, loudness is designed to strengthen local search and to guide bats find better solutions."
there is a large number of objective interestingness measures available in the literature. the 53 probability-based objective rule interestingness measures that we could find in all related literature are shown in table 1 .
"while powerful in pruning the search space due to the antimonotonicity of support, the support-confidence framework has been criticized in the context of association rule mining by many authors [cit] . for instance, it is difficult to tune. choosing a large minimum support may lead to having only rules that contain obvious knowledge and missing exceptional cases that are interesting. on the other hand, assigning a low minimum support yields a huge number of rules which could be redundant or noisy."
"three different experiments are conducted to find the impact of using 53 different measures in measure-based pruning. two of these experiments are based on rule reduction. in the first experiment, the goal is to find the minimum number of rules without jeopardizing the f-measure (keeping it above 95% of its original). in the second experiment, the aim is to find the minimum number of rules without changing the maximum possible accuracy. the goal of the last experiment is to eliminate the misleading rules in order to improve the f-measure. for these experiments the selection measure is fixed on confidence."
"for the local search part, once a solution is selected among the current best solutions, a new solution for each bat is generated locally using random walk:"
"generating rules in association rule mining or with associative classifiers can lead to a very large set of rules which make it impossible, for even domain specialists, to study. sifting through thousands or even millions of rules, inevitably containing irrelevant ones and noise, is impractical. to solve this problem, interestingness measures can be used for filtering or ranking association rules."
"second, if there are any improvements, is it possible to probe the best measure or measures which can beat the other measures for improving the results base on either the accuracy or the number of rules in all cases? there is a possibility that no one measure can be found to be effective in all circumstances. in this case, are there any relevant dataset characteristics or measure properties that can help build a classifier in order to predict an effective measure for a dataset?"
"associative classification is a relatively new paradigm for classification relying on association rule mining and naturally inherits the most commonly used interestingness measures, support and confidence. these are not necessarily the best choice and no systematic study was undertaken to identify the most appropriate measures from the myriad measures already used as filters or rankers for relevant rules in different fields."
"the mlp (multi-layer perceptron) is a category of artificial neural network trained by supervised learning procedure. the mlp is utilized for the detection of intrusions which were established on an offline analysis approach. also, in a diverse outlook, it was used to detect intrusion in a data on a network by comparing it with the som self-organizing maps."
the primary purpose of developing an intrusion detection system is to detect and identify the potential threats efficiently. there ids system can be classified into two categories:
"using these properties, we clustered all the 53 measures in table 1 with an agglomerative hierarchical clustering algorithm using average linkage. having each measure as a vector of properties, the distance of two measures is based on a hamming distance. figure 1 shows different levels of this clustering till the maximum distance among measures in each cluster is 0.25."
"associative classification [4, 23, 24 ] is a rule-based approach recently proposed to classify data by discovering associations between a set of features and a class label. to build an associative classification model, association rules whose consequent is a class label are generated using an association rule mining technique. research shows promising"
"algorithm (see [cit] ). first of all, let us briefly review the basics of the ba for single-objective optimization. [cit], in order to propose the bat algorithm inspired by the echolocation characteristics of microbats, the following approximate or idealised rules were used."
"any kind of unapproved or unauthorized activities in a network or a system are called intrusions. an ids (intrusion detection system) is a group of the tools, mechanism, resource to identify, assessment which describes intrusions [cit] . intrusion detection is usually a part of a comprehensive protection system which is installed in a device or around a system and it can't be taken as a stand-alone measure of protection. intrusion can be defined as: \"any type of activities which tries to manipulate the truth, secrecy, or the resource availability\" [cit] . in the vice-versa intrusion prevention techniques are referred to as the first line of defense against these intrusions. nevertheless, as in any type of security system, total prevention of intrusion is impossible. the node compromise and intrusion heads to secret information like security keys being disclosed to the intruders in the system, which results collapse of the security mechanism [cit] . consequently, idss are developed and designed to make intrusions public, before disclosing the system resources which are secured. idss are consistently acknowledged as a second line of defense from the view point of security. idss are considered as web equivalent of the robbers' alarms that are being utilized in physical security systems currently [cit] . the normal functional requirement of idss are; \"high true positive rate, measured as the percentage of anomalies detected, and low false positive rate, measured as the percentage of normalcy deviations detected as anomalies\"."
"while there is a wide range of interestingness measures, most are only studies in the context of association rule mining. few are proposed exclusively for associative classifiers but all studies on interestingness measures for associative classifiers consider only the rule selection phase. we investigate all these measures on all 3 phases of an associative classifier."
"where best is the current global best location (solution) which is located after comparing all the solutions among all the bats in generation, is the bat individual in the bat swarm, and this can be achieved by randomization."
"in another work, [cit] compare interestingness measures based on formal definitions and experimental results. 5 interestingness measure properties were described. three of these properties along with three other properties were used to group 20 different interestingness measures. 5 different groups have been obtained with a hierarchal ascendant clustering using the average linkage and manhattan distance. another clustering was done based on experimental results from 10 different datasets. they also found 5 clusters that match the previous clustering."
"where sm is the selecting measure, support is the support of the rule, and length denotes the length of the rule which is equal to the number of attribute-value pairs in the antecedant. for taking into account the average of measures for all matchable rules, first all rules that apply to the unknown object should be grouped based on their class labels. if r c table 2 : results on 20 datasets using global support with threshold of 1%, with selecting based on the highest ranked rule and the rules' average of measures. \"max acc\" denotes the maximum possible accuracy, \"fm\" denotes the macro average f-measure and \"acc\" denotes the accuracy of the classifier."
"nowadays, since the evolutionary algorithm can solve some problem that the traditional optimization algorithm cannot do easy, the evolutionary algorithms are widely applied in different fields, such as the management science, engineering optimization, scientific computing. more and more modern metaheuristic algorithms inspired by nature or social phenomenon are emerging and they become increasingly popular, for example, particles swarms optimization (pso) [cit], firefly algorithm (fa) [cit], artificial chemical reaction optimization algorithm (acroa) [cit], glowworm swarms optimization (gso) [cit], invasive weed optimization (iwo) [cit], differential evolution (de) [cit], bat algorithm (ba) [cit], and so on [cit] . some researchers have proposed their hybrid versions by combining two or more algorithms."
"(2) power law asymptotics (\"heavy tails\"). due to the these remarkable properties of stable distributions, it is now believed that the lévy statistics provide a framework for the description of many natural phenomena in physical, chemical, biological, and economical systems from a general common point of view."
"is a fixed parameter. in dlba, the position of each bat individual are updated with (10), which is different from original ba. this can preferably incorporate the echolocation characteristics of microbats:"
"in this paper, we tested 14 typical benchmark functions and applied them to solve nonlinear equations, the simulation results not only showed that the proposed algorithm is feasible and effective, which is more robust, but also demonstrated the superior approximation capabilities in high-dimensional space. this is not surprising as the aim of developing the new algorithm was to try to use the advantages of existing algorithms and other interesting feature inspired by the fantastic behavior of echolocation of microbats. numerically speaking, these can be translated into two crucial characteristics of the modern metaheuristics: intensification and diversification. intensification intends to search around the current best solutions and select the best candidates or solutions, while diversification makes sure that the algorithm can explore the search space efficiently. this potentially powerful optimization strategy can easily be extended to study multiobjective optimization applications with various constraints, even to np-hard problems. further studies can focus on the sensitivity and parameter studies and their possible relationships with the convergence rate of the algorithm."
"in addition, lévy flight haves the prominent properties increase the diversity of population, sequentially, which can make the algorithm effectively jump out of the local optimum. so, we let these bats perform the lévy flights with (11) before the position updating:"
"a ddos detection method is presented in this research article using ann for iot network. the detection method is mainly based on categorization of legitimate traffic patterns and attack traffic patterns. the proposed system is simulated and tested in an organized simulated iot network and the obtained results proved more than 99% detection accuracy. the system successfully identified the attack traffic and performed well in true and false negative accuracy. in the future, the system will be trained with latest threat patterns and will be tested to check its reliability with the modern world technology."
"there are many different rule interestingness measures widely used in machine learning, data mining and statistics. in a study of 38 different measures, geng and hamilton [cit] classify the interestingness measures in 3 main categories: objective, subjective and semantics-based measures. objective measures are those that are not application-specific or user-specific and depend only on raw data. subjective measures are those that consider users' background knowledge as well as data. as a special type of subjective measures, semantic-based measures take into account the explanation and the semantic of a pattern which are, like subjective measures, domain specific. for simplicity, our work only focuses on objective measures."
"dlba not only has superior approximation ability in lowdimensional space, but also has excellent global search ability in high-dimensional situation. table 4 is the experimental result that dlba performs 50 times independently under the high-dimensional situation. as shown in table 4, we can be conscious that the dlba is effective under the multidimensional condition, and acquired solution has higher accuracy, even approximate the theoretical value."
"an interesting future study would be to identify the relevant features of a dataset or a rule set that would help indicate the appropriate interestingness measure to use, and in this way exploit these features to build a predictor for best measure to use in the associative classifier given a specific training set."
"inspired by yang's method, we propose an improved bat algorithm based on differential operator and lévy-flights trajectory (dlba) based on the basic structure of ba and re-estimate the characters used in the original ba. in dlba, not only the movement of the bat is quite different from the original ba, but also the local search process is different."
"in section 3, we gave the design framework of dlba. the implementation and comparison of improved algorithm are presented in section 4. finally, we concluded this paper in section 5."
"(iii) external adversary: it is an external entity that is not considered as a member of the system and has no authorization to access it. an adversary aims to gather the information about the users' of the system with the malicious intentions for example, creating financial losses and subverting the user's reliability. it also, causes flaw in the system by maneuvering the data sensed and transmitted [cit] ."
"subsequently, discussing the iot features and how they can be used in several scenarios, it is now time to discuss and to identify the potential threats confronting the communication mechanism in iot. there are three primary entities which poses threats to the privacy and security in iot:"
"there are many rule interestingness measures already used in machine learning, data mining and statistics. many different measures are introduced in the field of association rule mining as filters or rankers to weed-out the least relevant rules. all those measures can be directly applied to associative classifiers as well, although never tested or reported in the literature. this work focuses on probability-based objective rule interestingness measures for associative classification. using these interestingness measures, there are two questions that should be answered:"
"the maximum possible accuracy shows the maximum accuracy that is achievable if for each test object, the right rule is selected from the set of available rules. hence, if for a test object there exists at least one rule that applies to that object with the same class label, that object is considered as a correct classification, otherwise, it is a misclassification. this evaluation measure is useful to evaluate the pruning and see whether the essential rules are pruned or preserved."
"this study is to answer the question whether other measures are more suited for the different phases of the associative classifier, and an attempt to identify the best measure for each phase. the results clearly indicate that many interestingness measures can indeed provide a better set of classification rules (i.e. a drastic reduction in the number of rules) and a more accurate classifier. however, there was no single measure that was consistently impacting the rule set for all datasets tested, even though for each dataset, some interestingness measure was successful in reducing the rule set or improving the effectiveness of the classifier. these measures are introduced for each individual phase. the results show that the measures that are the best in one phase are not necessarily the best measures for the other phase. another observation is that using the combination of the best measures in pruning and selection phases does not improve the accuracy of the classifier which means that the best selecting measure for an original rule set is not the best for the pruned version of that rule set. this observation shows that there might exist some rule set characteristics that have effect on selecting the best measure. hence, for each pruned rule set, the appropriate selecting measure should be probed."
"to summarize the results, there are interestingness measures that can be used as filtering measures and be able to reduce the number of rules significantly in all datasets without jeopardizing the accuracy of the model. in other words, the filters are capable of identifying unnecessary rules from the model. however, this drastic improvement in the number of rules is not necessarily observed in terms of accuracy. the change in accuracy remains stable, but some positive improvements in the accuracy (f-measure) were noted. another observation is that, no single measure can be declared as a winner for all types of datasets. there are some measures that have more impact than others."
"local and global supports with a threshold of 1% are used as anti-monotonic measures.to remove the conflicting rules, a minimum confidence threshold of 51% is used. no pruning method is used here and the measure used in the selection phase is confidence with two different approaches, selecting based on the \"highest ranked rule\" and based on the \"average of rules\". rule sets generated only using local/global support and confidence are called \"original rule sets\". all other results are compared with the results of these rule sets."
"in table 3, the five functions ( 10 -14 ) independently runs 100 times under the 24000 fes; we can clearly see the precision of dlba is obviously superior to the bat algorithm. some benchmark functions can easily attain the theoretical optimal value. in addition, the standard deviation of dlba is relatively low. it shows that dlba has superior approximation ability."
"azevedo and jorge compared 10 different interestingness measures in the selection phase of an associative classifier [cit] . using 17 different datasets from the uci repository they showed that the strategy selecting the applicable rule with the highest conviction measure yielded the best result. overall, conviction, confidence and laplace were the only measures that could produce competitive classifiers in their experiments."
"after showing that even confident rules can have negative correlations, arunasalam and chawla propose a new measure called complement class support (ccs) which guarantees rules to be positively correlated [cit] . their experiments on 8 uci datsets show that their measure is better suited than simple confidence for imbalanced datasets."
"while using the highest ranked rule in selection phase, this pruning can be used to remove the rules that are never used in prediction. hence, the f-measure and accuracy does not change. table 3 shows a huge percentage of rule reduction while using the redundancy removal pruning on original rule sets generated with global support. on the other hand, using this pruning method while predicting based on the rules' average of measures, changes the number of rules as well as f-measure and accuracy. the percentage of change in f-measure and accuracy are also shown in table 3 . the results show large reduction of f-measure in some datasets. hence, although redundancy removal pruning can reduce a table 4 : impact of measure-based filtering when maximizing the f-measure and the winning measure (with its minimum threshold used). global support is used for rule generation and the selection phase is based on the highest of rules. rr, fc, ac and mpac are short forms for rule reduction, f-measure change, accuracy change and maximum possible accuracy change respectively."
where g is called the training set and n is the total number of training samples. it is presumed that b i is a group variable from an infinite set
"results. the 2d landscape of schwegel's function is shown in figure 1, and this global minimum can be found after about 720 fes for 40 bats after 6 iterations as shown in figures 2, 3, and 4. we adopt different terminated criteria aiming at different benchmark function, we perform 100 times independently for each test function, and the record is given in tables 2 and 3 ."
"(i) malicious user: it is the possessor of the iot-device with power to carry out attacks to acquire the secrets of the device manufacturer, and also to acquire access to secret functionality. the malicious user reveals the shortcomings in the system to retrieve information, selling secrets to 3 rd parties, even launches an attack on the systems [cit] ."
"for evaluation, each classifier is assessed based on the number of rules its model contains, the macro average f 1 -measure, accuracy and maximum possible accuracy. henceforth, f-measure refers to macro average f1-measure."
network based ids: theids which is network based is responsible for the protection of the whole network environment from any type of intrusions. this type of ids architecture requires comprehensive knowledge of the system status and also monitors the different components of the network as well as the transactions which carry out between them [cit] . a network ids watches the actions on an entire network and carries the traffic analysis for possible security breaches or threats. agent technology performs a primary role in this type of ids architecture.
"(ii) bad manufacturer: it is the builder of the device with the capability to explore the underlying technology to retrieve the information of the iot devices, or users. aforementioned, a manufacturer can intendedly introduce holes in the security design which can be exploited subsequently to access the user's secret data and also revealing it to 3rd parties. similarly, the manufacturing of badly secured things results in the compromise of their users' privacy. adding to that, in the context of iot where different devices connect one another, a device manufacturer just to harm the reputation of their competitors; can attack their devices [cit] ."
"the measures are ranked based on the f-measure improvements in each dataset. oddmul, ccs, cnfrmc, conv, lap, loe, and zhang are the measures with the most top ranks when the highest ranked rule is used for selection and confc, ccc, lev, conv, ccs, loe and ex&cex are the measures with the most top ranks when the rules' average of measures strategy is used. many measures never achieve a top rank with any dataset."
"most of microbats have advanced capability of echolocation. these bats can emit a very loud and short sound pulse; the echo that reflects back from the surrounding objects is received by their extraordinary big auricle. then, this feedback information of echo is analyzed in their subtle brain. they not only can discriminate direction for their own flight pathway according to the echo, but also can distinguish different insects and obstacles to hunt prey and avoid a collision effectively in the day or night."
"in dlba, the frequency fluctuates up and down, which can change self-adaptively, and the differential operator is introduced, which is similar to the mutation operation of de, the frequency is similar to the scale factor of de/best/2. so, the frequency updated formulae of a bat are defined as follows:"
"the feed forward algorithm is the simplest form of devised artificial neural network. the information moves in only forward direction, which starts from the input layer, the hidden layer and finally to the output layer respectively. the algorithm states as follows; let v l is the number of total elements which excludes the bias elements. hence, the network parameters are"
"as mentioned above, an associative classifier is composed of three major phases: rule generating, rule pruning and rule selection. it is possible that the proper choice of an interesting measure could have an impact on each of these phases. at a higher level though, one can generalize the associative classifier into two stages: the learning, which encompasses the rule generation and pruning; and the classification, which is the selection of appropriate rules to fire."
"there is a diverse class of ids methods which are established on different formats, compositions and schemes. following are the description of those frequently used techniques:"
"som is a category of artificial neural network which is trained by unsupervised learning method to generate less dimensional, discrete representation input space of samples of training called map."
"the effect of using different selection measures, in the third phase of the associative classifier, is only on the improvement of the f-measure. there is no change in the number of rules per se as the learning model is already built. table 5 shows the best measures for f-measure improvement for each dataset. from the results, it can be inferred that there are some significant improvements in f-measure, specially when predicting is based on the highest ranked rule."
"in our research setup, we have an external intruder who attacks the iot network as shown in fig. 4 . server node is the only target of target of attackers because the server node analyzes, keeps the record and responds to the sensor nodes. the attacker launches the dos attack by 922 t. ahamed figure 3 : normal condition sending 10 7 packets towards the target node from a single host and the same amount of attack traffic was launched from four hosts to launch ddos towards the target node. we used a custom c script to craft udp packets to use them as attack packets. therefore, the server node comes to a halt and do not respond. the sensor nodes are not able to accommodate their changed behavior and finally cause the halt in the monitored system. therefore the detection of these attacks at the right time is very important to allow the uninterrupted working of sensor network and assure the reliability of the network. in this section we will evaluate the performance of designed artificial neural network intrusion detection method as discussed in the previous section. we trained the network with the defense scheme to protect iot from cyber attacks using ai principles 923 figure 4 : attack situation following parameters as shown in table 1 and the amount of sample patterns used for categorization is shown in table 2 . neural network confusion matrix is shown in fig. 5, is used to plot"
"figures 10, 11, 12, 13, and 14 are the distribution map of optimal fitness; that is the selected functions independently perform 50 times under the multidimensional situation. figure 13 show that two \"straight line\", we can see in the figure that dlba reaches the global optimum(−1), however, ba fluctuates around 0. in order to display the fact of fluctuation, we magnify the two \"straight line\", and the amplifying effect are depicted in figures 15 and 16 . according to the experimental results which are obtained from selected test functions, dlba presents higher precision than the original ba on minimizing the outcome as the optimization goal."
all the properties were introduced in the context of association rules. they can be used for finding similar measures or to find the appropriate measure for a problem domain if the required measure properties for that domain are known.
"all the measures were clustered in different experiments. some of the measures behave similarly in all the cases. hence, in future work, selecting only one measure from each group as a representative, is sufficient."
"in the current world more objects are associated with the internet than the people. and this difference will pursue to grow further, as the objects increases in their capability to directly interact with the internet or evolves into the physical objects of data which can be accessible through the internet systems. this scenario approaching towards greater independence in object interaction with the internet is generally interpreted as the (iot) internet of things [cit] . the greater number of devices which avails the internet services is growing faster and to have them connected every time by wire or otherwise will establish a mighty source of data information at anyone's finger tips [cit] . to enable communication between smart machines is an advanced technology, but the technologies which compose the iot are not fresh for us. iot, as understood by its name, is the method of converting data to any virtual-platform on current internet infra, which is retrieved from diverse class of things [cit] . the main concept of iot is to grant independent exchange of important information between unseen embedded distinctly identifiable devices in the real world around us, overwhelmed by the dominant technologies like rfid (radio-frequency identification) and wsns (wireless sensor networks) sensed by the sensor nodes and processed further for decision building [cit], in order to perform an automated action."
"furthermore, various studies have shown that the flight behaviour of many animals and insects has demonstrated the typical characteristics of lévy lights. a recent study by reynolds and frye shows that fruit flies, or drosophila melanogaster, explore their landscape using a series of straight flight paths punctuated by a sudden 90 ∘ turn, leading to a lévy flight-style intermittent scale-free search pattern [cit] . studies on human behaviour such as the ju/'hoansi huntergatherer foraging patterns also show the typical feature of lévy flights [cit] . the conclusion that light is related to lévy flights is proposed by barthelemy [cit] . subsequently, such behaviour has been applied to optimization and optimal search, and preliminary results show its promising capability. [cit] ). the general form of nonlinear equations with real variables is described as follows:"
"similarly, confidence is not a perfect measure as it considers nothing beyond the conditional probability of rules which may lead to confident associations but between statistically independent items."
"gives the upper bound of accumulative firing counts and (10e) makes sure that if a transition is persistent, it is fired using the on/off strategy."
"finally, let us notice that s ′ is a cfpn, so for sure controller a can drive s ′ to its final state in finite time [cit], implying that by applying on/off+ controller to s, the final state is also reached. remark 7. the results of proposition 6 can be naturally extended to continuous-time tcpn by taking sampling period θ tending to 0."
the control problem addressed here is to design a control action u that drives the system from the initial marking m 0 to the desired final marking m f .
"in this paper the net system is considered to be subject to external control actions, and it is assumed that the only admissible control law consists in slowing down the firing speed of transitions [cit] . under this assumption, the controlled flow of a tcpn system is denoted as:"
"notice that we have shown the results of different methods for a particular example, but it does not indicate one method is definitely better than another in a general sense."
". notice that, only transitions with positive values in the corresponding firing count vector should be considered. in the following, it is assumed that transitions in a coupled conflict relation are enabled. notice that this condition is rather weak since it is, for instance, verified by any system that can reach a positive marking."
"currently, accuracy 1 is the most common metric to evaluate a neural network model while performance or time is an important metric to characterize deep learning workloads (for example, fathom workloads [cit] and convnet implementation benchmarks on desktop systems [cit] ). owing to the difficulty in measuring energy, only a few studies exists that actually 1 top-1 or top-5 accuracy for image classification task [cit] and coarse-grained energy measurements for inferences on a mobile device [cit] . our work focuses on enabling researchers to evaluate existing and future convnet models on the metric of energy-use in addition to accuracy and performance."
"to overcome these extreme bad cases, first we fire the fast transitions and stop the slow ones for some time periods, expecting that the flows (speeds) of the slow transitions are increased, i.e., we will try to balance the fast and slow transitions. after that, the pure on/off+ controller is applied until the final state is reached. we will first shown how to classify the slow and fast transitions, then this balancing strategy is prensented."
"the opportunities for future work include characterizing these algorithms and providing a model for energy consumption to understand which design choices (layer types, filter sizes) in the model architecture leads to energy-savings."
"a minimum-time on/off controller has been proposed for structurally persistent pn [cit] . the essential problem of this standard on/off controller is that when there is conflict, this \"greedy\" strategy of firing transitions may bring the system to a \"blocked\" situation (see ex.2 for a example). in this work, the on/off control scheme is further investigated and three heuristic extensions are presented, ensuring that the final state is reached in finite time, even if the minimum time is not guaranteed. by forcing the conflicting transitions firing proportionally, we obtain the on/off-plus (on/off+) controller. but the drawback of this method is obvious: the firing speeds of transitions in a conflict relation are decided by the slower ones, and the overall system may be highly slowed down. therefore, the second extension, balanced on/off (b-on/off) controller is proposed, trying to balance the fast and slow transitions before applying the pure on/off+ controller. the third method is a combination of model predictive control (mpc) and on/off strategy: solving the conflicts using mpc and firing other transition using on/off strategy. the first two methods have very low computational complexity, while using the on/off-mpc controller we may reach the final state faster, but with considerable higher computational complexity. some comparisons are made by using different control methods and parameters. this paper is organized as follows: section 2 briefly recalls some basic concepts of cpn. in section 3 the standard on/off controller is recalled and its main drawback is stated. three on/off strategies are proposed in section 4. section 5 compares these control methods by using an assembly system. some conclusions are given in section 6."
"the problem of the on/off controller may arise from the incorrect manner of solving the conflicts (e.g., between t 2 and t 3 in fig.1) . two transitions t a and t b are in a (structural) conflict relation if"
"this controller a always exists, because if the firing rate of t j is big enough, case (1) can always be satisfied, using a positive control input u k (t j ). for case (2) we simply use the on/off strategy and the same firing rate as in s."
"we propose the development of an evaluation framework, as shown in figure 1, to support coarse and fine-grained energy profiling to provide visibility into the energy consumption during the inference phase of deep neural networks on a mobile device. our methodology focusses on power measurements made using the on-board power monitoring sensor ti-ina3221x [cit] available on the jetson tx1. the tx1 runs with linux kernel (3.10.96) and ubuntu 16.04. to measure energy at a fine-grained level we integrate vendor-specific tools such as arm streamline performance analyzer [cit] to the caffe deep learning framework [cit] ."
"the system model in fig. 4 represents an assembly system. there are two kinds of input raw materials stored in p 1 and p 2 . the material a, b are first processed by p roc a1, then the obtained semi-products are further processed by p roc a2 and p roc a3. in the other processing line, material b is sequentially processed by p roc b1 and p roc b2. then final produces are obtained after assembling all the semi-products."
"from (7), the estimations of number of steps that the transitions in t 2 need to fire, are at least d times greater than the ones of transitions in t 1 . if we fire the transitions in t 1 and t 2 proportionally, transitions in t 1 are obviously slowed down by the ones t 2 ."
"in this work, three on/off strategy based extensions are presented. it is proved that they can drive a general cpn system to the desired final state in finite time. some comparisons are also given. the advantage of these on/off based controllers is the low computational complexity. as a future work, we will compare our methods with other control strategies, and consider how to identify the most suitable controller in different situations."
"energy optimization efforts have emerged to enable deep learning applications to execute efficiently on current mobile systems. understanding the energy-use at specific phases (such as layer-wise energy consumption) of the application in terms of computation and data movement backed-up with actual energy measurements should help researchers develop better models of energy consumption and guide the development of application-specific optimizations for deep neural networks on mobile devices. for example, understanding the impact of different implementation choices on the energy-efficiency for a convolution operation."
"in order to overcome this problem, we will force the flows of transitions that are in coupled conflict relation to be proportional to the given firing count vector, while for the other transitions the on/off strategy is applied."
"the main component of the evaluation framework is its power sampling method that reads the power samples from the power monitor unit through the sysfs. the power monitor provides system-level power at vdd in, cpu-level power at vdd cpu and gpu-level power at vdd gpu. with the help of arm streamline, annotation markers were inserted into the code to mark the beginning of each layer in the neural the application selected was an inference using squeezenet model [cit] on a single rgb image (224 x 224 pixels) taken from imagenet dataset [cit] ."
"in this section, the previous control methods are applied to a cpn model of an assembly system. the simulations are performed on a pc with intel(r) core(tm)2 quad cpu q9400 @ 2.66ghz, 3.24gb of ram."
"the procedure of on/off+ controller is similar to the one of standard on/off, except the last constraint of lpp (5), which means that, in any time step k, if transitions t a and t b are in conflict, the following will be forced:"
"petri nets (pn) is a well known paradigm used for modeling, analysis, and synthesis of discrete event systems (des). with strong facility to depict sequences, concurrency, conflicts and synchronizations, it is widely applied in the industry for the analysis of manufacturing, traffic, or software systems, for example. similarly to other modeling formalisms for des, it also suffers from the state explosion problem. to overcome it, a classical relaxation technique called fluidization can be used."
"recent fast-paced developments of deep neural networks in various application domains such as computer vision and natural language processing, gaming and others has resulted in the emergence of innovative applications being developed for mobile and embedded systems [cit] . these systems pose constraints on the demand for computational resources and power which requires rethinking in several aspects: algorithm design, software implementation and hardware design. deep neural networks such as convolutional neural networks (hereafter referred to as convnets) [cit] are commonly employed in vision-based applications. to enable deep learning on these \"edge devices\", optimization efforts are spreads across all levels: compact model designs [cit], compression and pruning [cit], reduced precision [cit], scheduling strategies [cit], device-specific software implementations [cit] and application-specific hardware [cit] ."
"a pn system is bounded when every place is bounded, i.e., its token content is less than some bounds at every reachable marking. it is live when every transition is live, i.e., it can ultimately occur from every reachable marking."
"notice that, if the value of d is too big, all the transitions are put into t 1, then it is equivalent to applying on/off+ controller directly. on the other side, if d is too small, all the transitions are put into t 2, then they are all stopped."
"mpc is usually used for optimizing trajectories satisfying certain objective functions. in our problem, the aim is to reach the desired state as soon as possible, i.e., minimizing the time. even if it is difficult to obtain a minimum time control by using an mpc approach, we will consider this method for transitions in conflicts while for the others we will keep the on/off controller. we will show that in some situations the number of steps to reach the desired final state is smaller than for on/off+ or b-on/off controller, but with higher computational complexity."
"in order to prove the convergence of alg.2, it is first shown that the original system with the on/off+ controller is equivalent to a cfpn system with a particular controller, i.e., the same state trajectory can be obtained. it is clear that m f is reached in the cfpn system, implying that it is also reached in the original one."
"let us notice that, different from cfpn, in a pn with general structure, its minimal firing count vector may be not unique for given initial and final states."
"here τ q,m denotes the two-way travel time between the q-th potential target position in the grid and the m-th transceiver. note that in this model α is a sparse vector: the value of q-th pixel equals the target reflectivity if there is a target at this pixel, and otherwise it is 0. therefore,"
"the minutiae templates of 2,000 file fingerprints from nist sd4 are used to reconstruct fingerprints for fingerprint identification experiments. each reconstructed fingerprint is matched against 2,000 file fingerprints as the gallery to obtain 2,000 type-i attacks and against 2,000 search fingerprints as the gallery to obtain 2,000 type-ii attacks. the cumulative match characteristic (cmc) curves of type-i and type-ii attacks for four reconstructions, as well as original fingerprints, are compared in fig. 16 . similar to the verification experiments, the identification performance of type-i attack is very high. the rank-1 identification rate of the proposed algorithm with a fixed ridge frequency is 99.05%. when the ridge frequency around minutiae is utilized in reconstruction, the rank-1 identification rate is improved to 99.95%, which indicates that the features in the reconstructed fingerprint images are very close to the features in the original images."
"where e i s (r i ) is the dissimilarity term, e i c (r i ) is the compatibility term and ω c is the weight parameter. the dissimilarity term measures the dissimilarity of orientation field and frequency field between an initial fingerprint patch and its selected reconstructed fingerprint patch"
"theorem iii.2. with the same setup as in theorem iii.1, there exist constants c 5, c 6 (where c 5, c 6 may depend on w and ) such that"
"an important challenge when detecting stationary targets through walls using electromagnetic (em) waves is to locate the targets in the presence of wall em reflections, which are relatively strong compared to behind-the-wall target return [cit] . therefore, it is necessary to mitigate the wall reflections prior to revealing the target positions."
"if the ridge frequency associated with minutiae is available, the delaunay triangulation can be used to interpolate the ridge frequency field. suppose that b is the set of n b blocks with minutiae and dt b is the delaunay triangulation of b. the"
"have a strong concentration behavior: the first 2n w eigenvalues are close to 1, while the remaining eigenvalues are close to 0. this behavior enables us to use the first slightly more than 2n w modulated dpss vectors to represent sampled bandpass signals [cit] ."
"by noting that the wall return is a finite-length sample vector arising from sampling a bandpass signal in the band [−t l,m ∆f, −t 0,m ∆f ], from theorems iii.1 and iii.2, we expect that the wall return will be well-approximated by the dictionary d m ."
"is a spiral fourier multiplier, f(·) and f −1 (·) are the fourier transform and inverse fourier transform, respectively, the cosine term in (2) can be converted to a sine term (in quadrature to the cosine),"
"for a fingerprint image i (x, y), its phase ψ(x, y) can be obtained by demodulation [cit] . before the demodulation, the offset (or dc) term a(x, y), which can be estimated as the mid-value in a local area, is removed from i (x, y). the offset removed image i (x, y) is"
"the am-fm fingerprint model proposed by larkin and fletcher [cit] represents a fingerprint image i as a hologram, i.e., consisting of 2d amplitude and frequency modulated fringe pattern:"
"although the reconstructed fingerprints, as shown in fig. 13, are very close to the original fingerprints from which the minutiae were extracted in terms of orientation field, ridge frequency field and minutiae distribution, it is still difficult to fool a human expert because the reconstructed fingerprints are ideal fingerprints (without any noise) and have the synthetic appearance. future work will investigate to make the reconstructed fingerprints more realistic. the proposed method for orientation field reconstruction only considers the local orientation pattern. the use of global orientation prior knowledge as well as singular points may further improve the ridge orientation reconstruction. the ridge frequency field used in this paper can be either a fixed priori or reconstructed from the ridge frequency around minutiae. future work will investigate frequency field reconstruction directly from the minutiae position and direction."
"where a(x, y), b(x, y) and n(x, y) are, respectively, the offset, amplitude and noise, which make the fingerprint realistic, and ψ(x, y) is the phase which completely determines the ridge structures and minutiae of the fingerprint. according to the helmholtz decomposition theorem [cit], a phase ψ(x, y) can be uniquely decomposed into a continuous phase ψ c (x, y) and a spiral phase"
"the loopy belief propagation algorithm [cit], which has been shown to perform well on graphs with closed loops, is adopted to find the optimal reference orientation patches. the orientation field reconstructed by the orientation patch dictionary may also change the orientations in the blocks containing minutiae. a modified gaussian filtering is proposed to preserve the orientation around minutiae:"
"we point out that a modified fourier-domain omp algorithm may also be able to offer improved target detection. to be precise, we modify omp so that in each iteration when we pick one pixel in the grid, we also choose its neighbors. for example, when the red point in fig. 1(b) is selected, the nearby 48 black points are chosen along with the red one. this subfourier basis corresponding to the 49 points can be viewed as a surrogate basis for the subspace spanned by sampled bandpass signals in this local band. this is closely related to the main modification of our proposed dpss-aided mp algorithm, in which a bandpass modulated dpss basis is used to represent the sampled bandpass signals. fig. 3 shows the singular values of a sub-fourier basis corresponding to the 49 points (where the red point is randomly picked), which indicates that the effective dimension of this sub-fourier basis is smaller than 49. the target reconstruction result is shown in fig. 2(e) ."
"ingerprints are ridge and valley patterns present on the surface of human fingertips [cit] . the purported uniqueness of fingerprints is characterized by three levels of features [cit] (see fig. 1 ). global features, such as pattern type, ridge orientation and frequency fields, and singular points, are called level-1 features. level-2 features mainly refer to minutia points in a local region; ridge endings and ridge bifurcations are the two most prominent types of minutiae. level 3 features include all dimensional attributes at a veryfine scale, such as width, shape, curvature and edge contours of ridges, pores, incipient ridges, as well as other permanent details. among these three types of features, the set of minutia points (called minutiae) is regarded as the most distinctive feature and is most commonly used in fingerprint matching systems. an international standard iso/ [cit] 4-2 [cit] has been proposed for minutiae template representation for the purpose of interoperability of matching algorithms. fvc-ongoing [cit], a well-known web-based automated evaluation platform for fingerprint recognition algorithms, has set up a benchmark to evaluate fingerprint matching algorithms using this standard minutiae template format."
". by applying p ψq to the vector z, and by judiciously choosing ς (which we discuss below), we can remove essentially all the target return corresponding to the spatial region near grid point q. this is the main difference between our proposed algorithm and the conventional mp algorithm. the full dpss-aided mp algorithm for target detection is shown in algorithm 1."
"it was believed that it is not possible to reconstruct a fingerprint image given its extracted minutiae set. however, it has been demonstrated that it is indeed possible to reconstruct the fingerprint image from the minutiae; the reconstructed image can be matched to the original fingerprint image with a reasonable high accuracy [cit] . there is still a room for improvement in the accuracies, particularly for type-ii attack. the aim of fingerprint reconstruction from a given minutiae set is to make the reconstructed fingerprint resemble the original fingerprint. a successful reconstruction technique demonstrates the need for securing fingerprint templates. such a technique would also be useful in improving the matching performance with iso templates as well as addressing the issue of template interoperability [cit] . it also could be used to improve synthetic fingerprint reconstruction and restore latent fingerprint images [cit] ."
"suppose the wall clutter r w m is well-approximated by a dictionary d m, which we discuss below. we cancel the wall return by constructing an operator p (φmdm) that operates on the measurement vector z m ."
"the outline of this paper is as follows. in section ii, the main problem is illustrated. our main approaches to cancel the wall return and detect the targets are in section iii. section iv presents some simulations to support our proposed methods. finally, some conclusions are given in section v."
"before moving on, we note that different bandpass modulated dpss dictionaries could be constructed using different dpss bandwidths and modulating frequencies from those suggested in (8) . for example, one could also employ a dictionary such as"
"] jm to mitigate the wall return modeled in (1). however, em simulations show that there also exists antenna ringing before the wall, which in addition to the wall reverberations can affect the detection of targets [cit] . in consideration of the possible antenna ringing, we choose the bandpass modulated dpss dictionary defined in (8) ."
the continuous phase patch dictionary is used to reconstruct fingerprint image patches based on the reconstructed orientation field and ridge frequency field in section iii-b. global optimization is then adopted to obtain the reconstructed fingerprint image.
"in this paper, we have explained how a modulated dpss basis can help in mitigating wall return and detecting stationary targets after the wall clutter has been cancelled. this is possible thanks to the remarkable efficiency of the bandpass modulated dpss dictionary in representing finite-length sample vectors of bandpass signals. experiments show the effectiveness of the modulated dpss basis in cancelling the wall return and promoting target detection."
"however, if the target does not fall precisely on the grid, or in the more general case, if the target is not a point and the target return is as defined in (2), then α will not be sparse. especially when the dynamic range of the target reflectivities is large, then the target return corresponding to the largest target reflectivity may dominate and sparse solvers may fail to detect the other targets."
i) the orientations at the blocks with minutiae are replaced by the directions of minutiae (modulated by π). ii) the cosine and sine components of the double values of the orientations at the blocks without minutiae are smoothed using a gaussian mask (with standard deviation value of 2). iii) repeat
suppose there are k targets behind the wall. the target return observed by the m-th antenna corresponding to the nth frequency can be expressed as
"secondly, we build a classical utkin observer with linear injection that is used in next section to introduce a constructive sliding mode observer design framework. (2) we define the following observer described in state-space representation [cit] by the following equations:"
"as gwa experiments in rice continue to accumulate in the literature, imputation can enable future gwa meta-analysis 39 on agronomically critical traits by allowing the combination of germplasm and phenotypes across published studies. increasing access to imputation tools is one measure to encourage interoperability of crop genomic resources, promote re-use of publicly available and internally collected data, augment the impact of individual data sets, and inform future gene bank resequencing efforts. to facilitate imputation analysis for rice researchers, we developed a publicly available imputation web tool called rice imputation server. this service harnesses the results of this study and leverages haplotype diversity found within the rice reference panel. one critical feature that will likely contribute to long-term utility of this tool is its modularity; it is set up to readily accept upgrades to either the imputation software (e.g., a new version of impute2) or data (e.g., a higher density version of rice-rp) as they develop. future directions for improvements to the service may include offering regionspecific imputation for researchers interested in data at certain loci of interest rather than genome-wide and incorporating indel or other non-snp variant imputation capacities. to help users manage their resulting large data bundle, we offer \"snp filter\" text files that can feed directly into plink1.9 along with the user's imputed plink binary data set as a functional approach to filtering genome-wide snps. making imputation capability more accessible to plant research communities may improve the power of phenotype-genotype association studies, enhancing the productivity of basic investigations into molecular mechanisms and simultaneously accelerating translation into productive, stresstolerant, and nutritional plant varieties that are urgently needed to enhance global food security."
the electric circuit of the dc servomotor armature and the free body diagram of the rotor are shown in figure 1 . for simulation purpose we will assume the following experimental values for the physical parameters [cit] :
"phenotype data. [cit] wet season at the international rice research institute in the philippines using standard crop management practices for irrigation and pest control. grain was harvested at maturity and stored for 6 weeks under controlled relative humidity to equilibrate for moisture content. in total, 10 g samples of paddy rice were dehulled and polished by placing grains into 10 ml capsules with fused aluminum oxide and abrading gently for 1 h in a paint shaker. twenty whole-milled rice kernels were ground in a udy cyclone mill (sieve mesh size 60), 100 mg of rice powder were weighed into a 100 ml volumetric flask, and 1 ml of 95% ethanol and 9 ml of 1 m sodium hydroxide are added. amylose content was measured using the standard iodine colorimetric method iso 6647-2 [cit] 40 . absorbance of the solution was measured using an auto analyzer 3 (bran + luebbe, norderstedt, germany) at 600 nm. amylose content was quantified from a standard curve generated from absorbance values of four standard rice varieties (ir65, ir24, ir64, and ir8)."
"detection of an fnp in ssiia. the secondary association peak on chromosome 6 contained ssiia, the gene primarily responsible for gelatinization temperature and known to be pleiotropic for amylose content [cit] . here, we found only modest improvement in association strength compared to an unimputed gwa study (fig. 2) . the mssnp of this region increased its -log 10 (p-val) from 11.3 to 12.2. this small difference is likely due to the fact that the original mssnp from the hdra data set was already located very close to the known functional gene (supplementary fig. 11a ), just~10 kb downstream, and theoretically strongly tagged the underlying fnp. the new mssnp detected using imputed data was found at position 6,752,887, and localized within exon eight of ssiia ( supplementary fig. 11b-d) . from the imputed data set, there were 62 markers within ssiia, two of which were significant for amylose content (supplementary fig. 11d ). the second of these snps had a -log 10 (pval) nearly equal to that of the mssnp, at 12.05, and localized at position 6,752,888, just next to the mssnp. upon comparison to known functional variants of ssiia, we discovered that these two adjacent snps matched a previously reported gc/tt polymorphism that explained 62.4% of variation in pasting temperature, another cooking and eating quality trait, which corresponded to an amino acid change at leu781 of ssiia 34 . in our ind panel, the major haplotype ss.1 (gc; 72% frequency) was associated with higher amylose content and the minor haplotype ss.2 (tt; 28% frequency) was associated with lower amylose content, though distributions were overlapping ( supplementary fig. 11e ), consistent with the relatively minor role reported for ssiia in modulating amylose content. we checked 431 japonicas (185 temperate and 246 tropical japonica) that had genotype data at this fnp to see if these haplotypes were aligned with either japonica subpopulation. ss.1 was observed in the vast majority of tropical japonicas (91% ss.1 and 9% ss.2), while temperate japonicas had a random assortment of both haplotypes (49% ss.1 and 51% ss.2)."
"data availability. the full rice-rp dataset is available at the mccouch group site, rice diversity (ricediversity.org), the snp-seek database (http://snp-seek.irri.org/ download.zul), and european variant archive (project accession: prjeb26328 (https://www.ebi.ac.uk/ena/data/view/prjeb26328)). imputed data on the hdra panel may be obtained by subsetting the rice-rp. the imputed indica dataset used in our gwa study (based on imputation from the 160k snps), along with the kinship matrix, amylose content dataset, and r script used to run the analysis may be accessed at rice diversity."
"unknown uncertainty using a sliding mode observer implemented in mainframe environment by using more sophisticated control systems and artificial intelligence strategies. this research work is based on our previous results in implementation of different control systems strategies and now we are interested to prove the effectiveness of real-time implementation of our proposed smo control strategy. closing, the main objective of our research is to develop a more suitable, accurate and consistent real-time nonlinear smo control strategy to be used in our future real-time fdi control strategies implementation development."
"cultivated asian rice (oryza sativa l.), the first crop genome to be fully sequenced 11, 12, has been a pioneer species in genomic research. it is rich in open-access resources, including diverse germplasm samples available as publicly available purified (homozygous) seed stocks 13, data sets from single-nucleotide polymorphism (snp) array genotyping and resequencing [cit], and a recent pan-genome initiative that aims to provide a union set of all genes within the species 19 . despite these features, there currently exists no rice data set that has been vetted to serve as a cosmopolitan reference panel for imputation and no systematic approach to facilitate integration across different resources. one of the specific challenges for the rice community is the fact that o. sativa is comprised of deeply differentiated subpopulations across two varietal groups (vg) due to the combined effects of natural and human selection: indica vg: indica (ind), aus (aus); japonica vg: temperate japonica (tej), tropical japonica (trj) and aromatic (aro). the five subpopulations are distinguishable based on ecological adaptation, grain quality characteristics, and complex physiological parameters, as well as population genetics metrics such as linkage disequilibrium (ld) decay 13 ."
"having the ability to efficiently impute rice genetic data opens the door to a wide range of applications that extend beyond the few demonstrated in this study. perhaps of the greatest immediate impact, it facilitates integration of data sets. crop researchers commonly handle a range of germplasm types, resulting in genomic data sets of varying marker densities on independent collections of samples, each developed to address specific problems of interest. imputation facilitates the integration of these resources by mapping them onto a common genomic framework without the added time and expense of re-genotyping. it also article offers the option of waiting to make a final selection of germplasm or snps until after integration, tailoring the best selection possible for the researcher's need. imputation provides the means to update previously genotyped panels quickly and at little cost, often dramatically increasing the resolution of existing resources."
"when we conditioned our model on the fnp and re-ran association analysis, we found that most associations disappeared, consistent with the expectation if those markers were in ld with the fnp (fig. 4b, c) . interestingly, we also discovered a new snp located at position 1,768,000 bp in the fifth exon of the wx gene that was not significantly associated with amylose content in the original analysis but emerged as significantly associated upon conditioning with the fnp (fig. 4c) ."
"to see whether markers of low imputation accuracy were dispersed or grouped, we examined the distribution of accuracy values across the physical distance of chromosome three for ind1, ind2, and ind3 ( supplementary fig. 6a) . a half dozen clusters of markers with low accuracy were observed in ind1, with half as many observed in ind2 or ind3. interestingly, two clusters toward the end of the chromosome appear to be shared, one cluster between ind1 and ind2, and a different cluster between ind1 and ind3. geographical association of the three indica subgroups (supplementary fig. 6b ) and the fact that ind2 and ind3 are more widely dispersed while ind1 is primarily found in north asia could suggest that genetic architecture related to local adaptation may underlie different imputation performance. ind2 and ind3 accessions were additionally found outside of asia, as far as south america and africa. further analyses would need to be done to document whether a higher frequency of de novo mutations or introgressions (e.g., perhaps from japonica subpopulations) may have given rise to unexpected haplotypes in ind1 and whether these relate to different selection regimes, proximity to wild ancestors, or specific adaptation to northern versus southern climes in asia."
(15) and the precise structure of injection signal matrices gains is to be determined. the existence conditions for a smo of the form (13) that rejects the uncertainty class of dc servomotor load torque described in (2)
"the observer gain matrix is chosen in order to make the spectrum of the matrix ( to lie in, where the pair matrices ( is observable due to the fact that the pair is also observable."
x y z u iso */u eq o1 0.57798 (9) 1.01436 (8) (7) 0.0125 (7) c20 0.0692 (10) 0.0500 (8) 0.0602 (10) 0.0099 (7) 0.0124 (8) −0.0014 (7) c21 0.0672 (9) 0.0435 (7) 0.0511 (9) −0.0003 (6) 0.0027 (7) 0.0006 (6)
"without to lose the generality we can choose the coordinates transform matrix such as: (8) that converts the triple into, where the lines of the matrix span the null space of the vector, and also:"
(23) with a symmetric positive definite design matrix. for a particular selection of matrix we get: (24) now a robust observer design is well defined and can be described by the following equations:
", and so, by chance, the same structure as in the canonical form. the system triplet can be put in the following form [cit] :"
"(ii) output equation: (3) where we assume that the only the state is measurable, and the input command is a function defined by the armature voltage (dc power supply) and is designed in closed-loop as a state feedback control law. also the load torque is considered unknown which is bounded and has a bounded derivative."
"to extend the capability of performing imputation out to the greater rice research and breeding community, we developed a web-based application called the rice imputation server that utilizes impute2 at the backend (fig. 6 ). this publicly available service allows users to upload their own data, e.g., genome-wide snps generated by gbs methods, and receive imputed data sets back. we integrated the 3krg and hdra panels as a single, phased rice-rp at 5,231,433 snps via reciprocal imputation and phasing and provided four \"snp filters\" to facilitate downstream trimming of imputed data sets, which may be desirable depending on the end application. these snp filters represent the set of (1) genic snps, (2) exonic snps, and (3) putative splice sites. other options for filtering include basic plink1.9 utilities such as ldbased pruning and random thinning, which the user may also opt to perform after imputation (fig. 6 ). this service is available at http://rice-impute.biotech.cornell.edu."
"the dynamics of the dc servomotor actuator is described by the following input-state-output equations [cit] : (1) . where is the dc servomotor torque developed to the shaft, is the load torque, and is the counter electromotive force (cemf) of the dc servomotor."
"remark: a big advantage of this development in this formulation of the sliding mode observer (smo) design framework is that there is no requirement for (a, c) to be observable [cit] ."
"g enetic imputation is an approach to infer genotypes at unobserved sites in a study data set using haplotype information from a typed reference data set. imputed panels can be used to improve resolution for genome-wide association (gwa) and to integrate data sets 1, 2 . leveraging haplotype diversity information from publicly available cosmopolitan reference panels for imputation has been widely used in human genetics research, beginning with reference panels derived from the first hapmap projects and then from the 1000 genomes project as sequencing technologies improved [cit] . in contrast to the consortium approach taken by the human genetics community, crop researchers have conventionally addressed resource development in a more distributed fashion. this may be related to the intrinsically flexible nature of plant populations; a single species can exhibit a range of mating habits and environmental adaptations, and individuals can be readily propagated. these features give rise to the many types of plant collections that are genotyped, e.g., biparental and multiparental mapping populations, locally adapted landrace collections, diversity panels, and various generations of elite breeding material. as a result, myriad genomic data sets of varying quality and density on independent collections of samples have accrued. imputation has the potential to provide a generalized solution to enrich genomic data sets and facilitate data integration across germplasm resources, especially important for sparse data sets generated from platforms such as genotyping-by-sequencing (gbs) or skim sequencing used commonly in plant research [cit] ."
"here, we set out to provide easily accessible imputation capacity for rice researchers worldwide. we examine parameter effects on imputation in rice using impute2 and impute the hdra panel to 4.8 m snps using 3krg as the reference panel. we demonstrate the benefits that imputed data can provide to enhance the resolution of rice gwa studies using grain amylose content as a case study. we assemble a rice reference panel (rice-rp) by merging the hdra and 3krg data sets via reciprocal imputation (table 1, fig. 1 ) and develop a web-based service called the rice imputation server to increase imputation accessibility to rice researchers throughout the world. this work paves the way to boosting research efficiencies by enabling investigators within both basic and applied research domains to integrate discrete data sets and to augment marker density, improving the power and resolution of genomic studies in rice."
"this work was partially supported by an iron and steel institute of japan (isij) research promotion grant. supplementary materials acta cryst. (2012) . e68, o2595 [doi:10.1107/s1600536812033545] 2,7-dimethoxy-1-(2-naphthoyl)naphthalene takehiro tsumuki, atsumi isogai, atsushi nagasawa, akiko okamoto and noriyuki yonezawa"
extended haplotype homozygosity. ehh analysis ( supplementary fig. 9c ) was performed for the mssnp of the wx gene using the r package rehh 51 . a 200-kb region flanking each side of the focal snp was used.
"in the course of our study on electrophilic aromatic aroylation of 2,7-dimethoxynaphthalene, 1,8-diaroylnaphthalene compounds have proven to be formed regioselectively with the aid of suitable acidic mediators [cit] . recently, we have reported the crystal structures of several 1,8-diaroylated naphthalene homologues exemplified by 1,8-dibenzoyl-2,7-dimethoxynaphthalene [cit], [2,7-dimethoxy-8-(2,4,6- trimethylbenzoyl)naphthalen-1-yl](2,4,6-trimethylphenyl)methanone [cit] and [2,7-dimethoxy-8-(2naphthoyl)naphthalen-1-yl](naphthalen-2-yl)methanone [cit] . the aroyl groups at the 1,8-positions of the naphthalene rings in these compounds are connected almost perpendicularly and oriented in opposite directions."
"to formulate any practical control application is not a straightforward task due to a more or less mismatch between the actual plant and its mathematical model used for the controller design. this mismatch could arise from unknown external disturbances, plant parameters, or modeling errors. to design a suitable control law in order to provide the desired performance to the negative feedback closed-loop control system in the presence of these disturbances or uncertainties is a very difficult task for any control engineer. this has led to an intensive research to develop some kind of robust control methods that are supposed to solve this problem. the sliding mode control (smc) is one of particular attractive approach to design a robust controller due to its potential as a robust control method. a sliding mode control (smc) is characterized by a suite of feedback control laws and a decision rule [cit] . the decision rule is a sort of switching function that has as its input some measure of the current system behavior in order to generate as an output a particular feedback robust controller which should be used at that time instant [cit] . the sliding mode control (smc) strategy is designed such that the variable structure control systems have to be capable to drive the system states on the convergent phase trajectories and then to constrain these to lie within a neighborhood of the switching function that can be represented by a switching line or switching surface, such is shown in figure 2 [cit] . in this approach the dynamic behavior of the control system may be adapted to a particular choice of switching function, and also the feedback closed-loop response becomes totally insensitive to a particular class of uncertainty in the system, more precisely proving a high robustness feature to all kind of system uncertainties. the main drawback of using the sliding mode control (smc) techniques to design a large variety of control industrial applications is the necessity to implement a basically discontinuous control signal which hypothetically must switch with infinite frequency to provide total rejection of the uncertainties [cit] ."
"he most important common actuator integrated in a forward path of the control systems structure is the dc servomotor. it directly provides rotary motion and, coupled with wheels or drums and cables, can also provide transitional motion. the cause roots of the majority faults in control systems are the result of unexpected failures, interferences as well as the age of their crucial components. in our research defective measurement and control loops equipment, in particular a sensor and an actuator are under investigation. more precisely, we assume that the dc servomotor control system is subjected to an unknown but bounded uncertainty, and its speed is controlled in closed-loop feedback by using a state feedback control law with an embedded t sliding mode observer (smo) that is integrated in the closed-loop control structure. the dc servomotors are preferred in control area applications because they possess a high start torque characteristics, high response performance, and easier to be linearly controlled etc. also the dc servomotor speed can be adjusted by varying its input voltage. therefore, the dc motor control is better compared to all ac induction motors that need expensive frequency drivers. the real-time dc servomotor speed control can be easily understood and interfaced with matlab/simulink or anylogic multi-paradigms hybrid simulator in the case of uml-rt implementations [cit] with a fast response. furthermore, the preliminary results of this research will be useful for us for future exploration of using a sliding mode observer to develop more attractive control strategies, for example the detection and isolation of the faults (fdi) [cit] that occur in the actuators and sensors, and also for trust modeling in multi-agent systems [cit], will be an interesting future directions of our research. whenever these critical situations come out the control systems could lose the control, require much more energy, and could operate harmfully. therefore to operate in real-time at high energy efficiency and to guarantee the equipment safety and reliability it is important to develop suitable fdi strategies capable to detect and diagnose any time every faulty control system components and consequently corrective and reconfiguration actions should be initiated promptly [cit] . effectively the existing methods to identify and to adjust the equipment failures are mostly labor-intensive task, and consequently sustained, rhythmic and error-prone [cit] . in the majority of these situations the windings currents are recorded to determine the health of the dc servomotors currents compared to statistical evaluation that necessitates considerable human knowledge, hence error-prone that could generate severely equipment operation. in these conditions the problem of control systems monitoring and fault diagnosis becomes a critical issue, of high complexity that need to be"
"the most comprehensive o. sativa genomic data sets released to date, accompanied by publicly available germplasm, are the 3000 rice genomes (3krg), genotyped using illumina shortread next-generation resequencing technology 14, 20, and the rice diversity panels 1 and 2,~1500 diverse rice varieties genotyped using a genome-wide high-density rice array (hdra) 13 (hereafter referred to as the hdra panel). these panels sample across the geographic and genetic space occupied by o. sativa's five distinct subpopulations. the 3krg project yielded a set of 29 m biallelic snp markers, an 18 m base snp set, and a 4.8 m filtered, high-quality snp set as part of its release 1.0 (http://snp-seek.irri. org/download.zul). the hdra panel was assayed using a 700 k snp array. collectively, the 3krg and hdra panels are believed to be representative of global o. sativa diversity."
"as a part of the course of our continuous study on the molecular structures of these type of homologous molecules, the crystal structure of title compound (i), 1-(2-naphthoyl)-2,7-dimethoxynaphthalene, is discussed in this paper."
"one limitation to the data sets (table 1 ) developed in this study is that variants present depend on snps found in the 3krg 4.8 m data set, which had been originally filtered for 0.01 global minor allele frequency. this dependency can lead to the potential of missing rare variants in the final imputed data set. rare snps, e.g., those private to a specific subpopulation at low frequency that were filtered out of the 3krg 4.8 m snp set would be absent. it would be of interest in the future to generate an updated rice-rp derived from an unfiltered initial reference panel, or one that accounted for subpopulation-specific snp frequency information. such strategies would help retain rare variants in final imputed data sets."
"in all the above equations we set in order to solve the problem of designing a sliding mode observer (smo) by using matlab/simulink software package. the model of the original system in simulink is shown in figure 3, and the evolution of the states, i.e. angular speed and armature current are shown in figures 4 and 5. the state-space representation model of the dc servomotor in canonical form and the utkin classical observer dynamics model embedded in the same integrated control structure are represented in matlab/simulink, as are shown in figure 6 and the evolution of the both model and estimated states, namely the angular speed and the armature current are shown in figures 7 and 8. the dc servomotor angular speed and armature currents smo residuals are shown in figures 9 and 10. in figure 11 are shown the smo errors dynamics modeled in simulink."
"vii. conclusions in this paper, we have studied the possibility of using a sliding mode observer strategy design to a dc servomotor actuator with disturbance uncertainty that is integrated in the same control system structure. the implementation in real time of smo proposed control strategy will be very useful for our future developments in fault detection and isolation (fdi) control applications based on the equivalent signal injection principle [cit] . this new fdi control strategy will be design in the future work based on the injection signal principle [cit] . the main contributions in our research are summarized briefly as follows:"
"now the dynamics of the observer is described by the following similar equations: (5) where the pair represent the estimated values of the transformed components state vector, and is the observer gain matrix, given by (6) the dynamics of the system errors is described by the following first order differential equations:"
"sarah k. in contrast, the application of sliding mode methods in combination with observer control problems provide the ability to generate a sliding motion on the error between the measured plant output and the output of the observer such that to ensure that a sliding mode observer (smo) produces a set of state estimates precisely matching with the actual output of the plant. also the analysis of the average value of the applied observer injection signal, the so-called equivalent injection signal, contains useful information about the mismatch between the model used to define the observer and the actual plant. furthermore, the discontinuous injection signals in this case don't remain anymore a drawback for software based observer frameworks in control applications [cit] ."
setting the observer matrix gain l to 0.01 the dynamics of the linear observer and of its error are described by the following first order differential equations:
"(21) the corresponding observer is described by the following equations: (22) where is a stable design matrix, let us to take it let us to consider also a symmetric positive definite matrix for that is a unique solution of the lyapunov equation:"
"genotype information. snp information on 1568 individuals genotyped on the high-density rice array 13 and 3024 individuals genotyped as part of the 3000 rice genomes project 14 were previously published and now have been assigned germplasm doi's for the first time (supplementary data 1) . these two germplasm steps for imputing rice genetic data using the rice imputation server. the user uploads input files in oxford format (.gen/.sample) per chromosome. the imputation server utilizes chromosome recombination maps and the rice reference panel (rice-rp) haplotypes to impute the user's data out to 5.2 m snps with impute2 and returns imputed results in plink binary format. the user can filter the imputed data using plink1.9 to produce a final data set of desired snp density and composition panels are hereafter referred to as the hdra and 3krg panels, respectively. an intersecting set of markers between the 18 m snp resequencing data set (http:// snp-seek.irri.org/_download.zul) and the 700 k snp array data set were filtered for a maximum of 5% missing data per marker in each of the two panels and a minimum of 1% minor allele frequency across the combined data set. the resultant set of~160 k high-quality snps was used in imputation analyses (position list of 160 k snps can be found at snp seek; http://snp-seek.irri.org/_download.zul). population structure was analyzed on the 4591 combined sample set using faststructureand the cut-off used to assign samples to specific subpopulations was 75% ancestry. one 3krg accession (iris_313-8921) was dropped from this study due to excessive missing data at the 160 k snps. the filtered, 4.8 m snp data set from 3krg release 1.0 was used as the high-density reference set of snps and was selected due to its filter on missing data (max 0.2). by design, the 4.8 m snp data set had been filtered for 0.01 global minor allele frequency in the 3krg panel. an overview of data sets used in this study is found in table 1 and supplementary fig. 1 ."
"the dynamics errors of the sliding mode observer model (30) attached to the dc servomotor actuator with disturbance uncertainty (the load torque, ) are modeled in simulink and shown in figure 13. their dynamic evolution, i.e. armature current residual and angular speed residual, is shown in figures 14 and 15. the smo control switching function around sliding line is calculated according to (28) and (31) and is shown in figure 16 . the state-space representation of the robust sliding mode observer model (26) in canonical form is represented also in simulink, and the evolution of the both model and estimated states, namely the angular output speed and the armature current in control systems literature rarely we find details about the real-time software and hardware implementation aspects, and no sufficient attention is given about the algorithms and the sampling time selection. usually the implementation aspect and real-time control systems design are connected together but in the most cases this connection is always ignored. furthermore the real-time control systems design is treated from control perspective ignoring the implementation aspects of the control algorithms. fortunately, recently the real-time implementation and design aspects get a considerable attention from part of control engineering community due to the introduction of new software tools like matlab/simulink with its rtw (real-time workshop) and the rtwt (real-time windows target) toolboxes. [cit] with simulink running on two processors windows os machine. certainly these new real-time platforms do the implementation of real-time experiments easier and save much time but on the other hand they have some drawbacks regarding a good perception of the real-life problems that could appear during the real-time implementation of the control systems."
"where w o i is the i-th entry of w o and (r * a p ) i,j is the (i, j)-th entry of r * a p ."
"as the position graphs were examined, it was seen that the robot has arrived to the determined position as soon as possible. the average settling time is 0.4 seconds and the longest settling time is about 0.7 seconds. the error between the desired position and the reached position is due to the gaps in the mechanical connections used at the joint of the robot. the results of position control values are summarized in table 3 . table 4 ."
"e purpose of the experiments performed in this work is to generate a set of average retrievability scores for subsets of the collection. namely, for each topic, the average retrievability is calculated for the retrieved relevant documents (ret.rel), retrieved non-relevant documents (ret.nonrel), not retrieved relevant documents (notret.rel) and the not retrieved non-relevant documents (notret.nonrel)."
"to compute the retrievability scores for documents, we rst generated queries from the collection and then issued the queries to each of the di erent con gurations (collection, retrieval model, parameter se ing). e method used for generating queries was as follows. e collections were indexed in the lucene4ir framework. documents were tokenised using a shingle tokeniser which creates shingles of 2 terms to index."
we proceed by first creating a correlation modeling matrix g that captures pertinent connections between the convolved context windows from the sentence and the relation class embedding w l introduced earlier in section 3.1:
"nowadays, surgical applications, entering the body through a small incision with laparoscopic procedures are performed and are expanding the use of robots for these operations. in existing systems, conventional robots are used as body of surgical systems. surgical procedure is carried out by a separate mechanism which is attached to the end of the robot and conventional robot is used to positioning this mechanism. these mechanisms are called remote center of motion (rcm) mechanism [cit] . the purpose of the rcm mechanism, provide rotating around the incision point to prevent potential damage of the body tissue by the robot. instruments enter through this incision point into the body with mechanical control and the robot works. for reasons such as robots in the operation field covering many places, the difficulty of controlling and maintenance the robot, focuses on fixing the rcm mechanisms to working area and direct drive. for this purpose, two different mechanisms profile as a parallel and spherical are developed. in addition, the spherical mechanisms are designed in two different profiles as serial and parallel. in the aforementioned method, mechanism is fixing either end of a robotic arm or directly to the working zone. for this reason, it can be said rcm point's position is fixed. however, although body structure during operation assumed to be constant, as a result of very small movements happen resulting from the organism's liveliness, for precision surgical applications, researchers tend to develop mechanisms as rcm point moving. for this purpose, hybrid (serial-parallel) mechanisms have emerged. in the hybrid mechanisms, working around the rcm point performed by the serial module, shift the rcm point is performed by the parallel module. in this way, requiring very small linear movement and host rotate around incision point such as eye operations, surgical success is increases [cit] ."
"fuzzy control provides a formal methodology for representing, manipulating, and implementing a human's heuristic knowledge about how to control a system. fuzzy systems have been used in a wide variety of applications in engineering, science, business, medicine, psychology, and other fields. for instance, in engineering some potential application areas are aircraft/spacecraft, automated highway systems, automobiles, autonomous vehicles, manufacturing systems, power industry, process control, robotics, etc. the fuzzy controller block diagram is given in fig. 13, where shown a fuzzy controller embedded in a closed-loop control system. the plant outputs are denoted by y(t), its inputs are denoted by u(t), and the reference input to the fuzzy controller is denoted by r(t). the fuzzy controller has four main components: (1) the \"rulebase\" holds the knowledge, in the form of a set of rules, of how best to control the system. (2) the inference mechanism evaluates which control rules are relevant at the current time and then decides what the input to the plant should be. (3) the fuzzification interface simply modifies the inputs so that they can be interpreted and compared to the rules in the rule-base. and (4) the defuzzification interface converts the conclusions reached by the inference mechanism into the inputs to the plant. basically, you should view the fuzzy controller as an artificial decision maker that operates in a closed-loop system in real time. it gathers plant output data y(t), compares it to the reference input r(t), and then decides what the plant input u(t) should be to ensure that the performance objectives will be met [cit] . matlab program was used for robot control. after the fuzzy control algorithm is written, the differential equation is solved by the runge kutta method. the obtained fuzzy parameters are optimized by genetic algorithm method. for each variable (q1, q2, q3 and q4) a membership function is defined. the fuzzy parameters of each membership functions are given in table 1 . the graphic of fuzzy logic control input membership functions is shown in fig. 14 . the graphic of fuzzy logic control output membership functions is shown in fig. 15 . in the matlab simulation, the reference values of control are given in table 2 . position-settling time graphics for each motor is shown in fig. 16 . as can be seen from the graphs, the longest settling time is about 0.7 seconds."
"the size of a [tree] these belong to three different relation classes, component-whole(e 2,e 1 ), entity-origin(e 2,e 1 ), and instrument-agency(e 1,e 2 ), respectively, which are only implicit in the text, and the context is not particularly helpful. more informative word embeddings could conceivably help in such cases."
where u is a weighting matrix learnt by the network. then we adopt a softmax function to deal with this correlation modeling matrix g to obtain an attention pooling matrix a p as
"literature of robots used for surgical purposes has been scanned and an original surgical robot which is not in literature has been designed. robots to be used in surgical operations must be fixed. the patient also needs to be fixed to ensure that the required center of rotation is not moving. in this study, the mechanism has 4 degrees of freedom and the rcm point is designed to be controlled vertically downwards, and the patient's obligation to fix has been abolished. all parts of the designed robot were created with 3 dimensional design program, solid model assembly of the robot was obtained and animations were realized. the designed manipulator has an original design capable of both serial and parallel motion. it is aimed at achieving a wider working volume at depth by performing the movements vertically downward. in the design phase, it is aimed to expressing both kinematic and dynamic equations in a simple way, and providing convenience in mathematical operation. mathematical expressions of a mechanism with circular joint are more complex and controllability of the circular mechanisms is more difficult than mechanisms in which have linear joints. therefore, linear joints are predominantly preferred in the designed system. with this design, the system will not be mounted on the operating table but will be positioned in the upper areas of the table, and then the operational maneuver area of the health personnel and the operator working during the operation will be expanded and the operational capability will be increased. mounting state of the designed robot is shown below. depending on the robot's limbs and the strokes of the linear motors to be used, the working space of the robot has been observed in a 3d environment."
manually engineered methods svm [cit] 82.2 dependency methods rnn [cit] 77.6 mvrnn [cit] 82.4 fcm [cit] 83.0 hybrid fcm [cit] 83.4 sdp-lstm [cit] b) 83.7 drnns [cit] 85.8 sptree [cit] 84.5
"for our analysis we used three trec collections using three parameterised retrieval algorithms. e four collections employed are associated press 88-90 (ap), aquaint1 (aq), and trec disks 4 and 5 (t45). details of these collections can be found in table 1 . e three retrieval algorithms featured are bm25, pl2 and language modelling with dirichlet smoothing (lmd), all implemented in the lucene4ir 1 search package, based on lucene. for tuning the parameters for bm25, pl2 and lmd, a parameter sweep is performed across their b, c and β parameters, respectively, to allow insights into the e ects these parameters are having on document retrievability. map is calculated for each topic on each collection using each model and used in the analysis stage as an indicator of system performance."
end-to-end methods cnn+ softmax [cit] ) 82.7 cr-cnn [cit] 84.1 depnn 83.6 deplcnn+ns [cit] ) 85.6 stack-forward * 83.4 vote-bidirect * 84.1 vote-backward * 84.1
"the disgusting scene was retaliation against her brother philip who rents the [room]e 1 inside this apartment [house]e 2 on lombard street. fig. 3 plots the word-level attention values for the input attention layer to act as an example, using the calculated attention values for every individual word in the sentence. we find the word \"inside\" was assigned the highest attention value, while words such as \"room\" and \"house\" also are deemed important. this appears sensible in light of the ground-truth labeling as a componentwhole(e 1,e 2 ) relationship. additionally, we observe that words such as \"this\", which are rather irrelevant with respect to the target relationship, indeed have significantly lower attention scores."
"finally, we multiply this attention pooling matrix with the convolved output r * to highlight important individual phrase-level components, and apply a max operation to select the most salient one [cit] for a given dimension of the output. more precisely, we obtain the output representation w o as follows in eq. (12):"
retrievability was introduced as a document centric evaluation measure by azzopardi and vinay with the intention of evaluating the access to the collection provided by the retrieval mechanism [cit] . retrievability evaluates the likelihood that a document will be retrieved from the collection when given some arbitrary query without considering relevance. a document d has a retrievabilty score r as de ned by the following equation:
"in this study, üneri at al. present the design of a new generation, cooperatively controlled microsurgery robot with a (rcm) mechanism and an integrated custom micro-force sensing surgical hook (fig. 8) . a parallel sixbar mechanism has implemented that mechanically provides the isocentric motion, which minimizes the translation of xyz stages. the resulting robot manipulator consists of four subassemblies. xyz linear stages for translation; b) a rotary stage for rolling c) a tilting mechanism with a mechanical rcm and d) a tool adaptor with a handle force sensor. [cit] fig. 8. er2 robot and close up view of its end effector [cit] none of the mechanisms mentioned in the literature except the hybrid mechanisms, the rcm point cannot set. especially the structure of parallel mechanisms does not allow this. in hybrid mechanisms, the working parallel mechanism is positioned via a serial manipulator, so that the rcm point can be shifted. this increases the number of degrees of freedom of the mechanism and therefore the number of motors and limbs that need to be controlled."
"e ndings suggest that good systems do tend to make relevant documents more retrievable. intuitively, this makes sense, if we tune our system, such that the relevant documents are more likely to be retrieved, then the system should perform be er. however, doing so, is likely to increase the overall bias, as expressed by gini, for instance. and so, may have dire consequences on the retrieval performance of other sets of topics. in future work, it will be of interest to explore this relationship further with respect to the overall system bias and with respect to other performance measures, collections and across individual topics."
"is tokeniser removed stop words, applied porter stemming and only accepted terms longer than 3 characters long before stemming. a list of bigrams was then generated from the index by returning the shingles indexed along with their document frequencies and collection frequencies. bigrams that occur 4 or more times were taken, returning a sizeable list of bigrams to be used in the retrievability estimation (see table 1) ."
we propose a novel distance function δ θ (s) that measures the proximity of the predicted network output w o to a candidate relation y as follows.
"to this end, we propose a novel multi-level attention-based convolution neural network model. a schematic overview of our architecture is given table 1 : overview of main notation."
"finally, figure 3 shows plots of the odds of retrieving the relevant and not retrieved (nonret.rel) vs non-relevant and not retrieve (notret.nonrel). here we see, that the there is greater disparity between the odds and map. is is perhaps to be expected, because these relevant items are not contributing to the map score. interestingly, the odds tends to be below one across each model (where as for the other aspects the odds exceeded one, and corresponded to good performance). is suggests that these subset of relevant items at best had an equal chance of being retrieved."
"in this study, it has been mentioned a robot which is designed to be used in laparoscopic surgery. the most important feature of the robot is that the motions are realized as vertical as possible and the dynamic loads are reduced. the design was mostly created using linear joints. the only rotary joint used to provides the rotation of the robot. that is, the rotation of the point p (x, y, z) depends to only the rotary motor. thus the motion equations of the robot are obtained in a very simple way [cit] . the purpose of the robot is to turn and orient the cannula around the rcm point. it is also possible that the rcm point can be adjusted instantaneously in the vertical axis with the realized design. this will be especially effective in minimizing any mistakes that can occur during a surgical operation. the robot does not occupy space in the field of operation thanks to its structure, so there is more working area for the operation staff."
"to compute the performance of each system, we used the trec topic titles as the query for each topic. when discussing relevant and non-relevant documents, only those included in the qrel le were considered. un-judged documents were excluded from the analysis reported here."
"it has been hypothesised that fairer retrieval systems are be er performing systems [cit] . however, this has only been shown in particular circumstances and has not been generalised. instead of making such a broad claim, it is perhaps more realistic to pose the hypothesis that retrieval systems that contain li le unwanted biases, thus being fairer, are more likely to improve performance by allowing documents to be judged purely on a query by query basis. in this work, a related hypothesis is proposed; that be er performing systems actually exhibit a bias towards the relevant documents for a query, prior to retrieval. a system that performs well in terms of a trec style performance evaluation will be more likely to retrieve relevant documents than non-relevant documents, a priori."
"fizzy drinks and meat cause heart disease and diabetes in our work, we apply the idea of modeling attention to a rather different kind of scenario involving heterogeneous objects, namely a sentence and two entities. with this, we seek to give our model the capability to determine which parts of the sentence are most influential with respect to the two entities of interest. consider that in a long sentence with multiple clauses, perhaps only a single verb or noun might stand in a relevant relationship with a given target entity."
"in the example given above, the verb corresponds quite closely to the desired target relation. however, in the wild, we encounter a multitude of different ways of expressing the same kind of relationship. this challenging variability can be lexical, syntactic, or even pragmatic in nature. an effective solution needs to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure."
"this kind of rcm mechanisms is usually acted as a basic component of 2 degree of freedom (dof) or multi-dof rcm mechanisms because of its simple structure. the most familiar rcm mechanisms in applications are based on a parallelogram structure, which can easily compose 2-dof rcm mechanisms. fig. 1a-f shows a basic configuration of parallelogram-based rcm mechanisms. [cit] robomaster1: in this study, hadavand at al. studied on a double parallelogram robot called robomaster1. a main feature of the double parallelogram mechanism is the fact that the center of rotation is located at a tunable distance from the body of the mechanism. so, this mechanism can be efficiently transferring the remote center of rotation to the back of surgeon's hand without limiting surgeon's maneuvers. for having a stable rcm mechanism, they added another double parallelogram to the mechanism with some offset from the first one as is illustrated in fig. 2 development of a novel mechanism for minimally invasive surgery: in this paper, a novel robotic system that can assist minimally invasive surgery is proposed by wang at al. the system has two subsystems: a 3-dof arm part and 4-dof instruments. the arm part has a new remote center-of-motion mechanism, while the 4-dof instruments with the diameter 8mm can increase the dexterity during the surgery. the structure of the double parallelogram is simple, and it can fulfill the requirements of the incision point constraints without extra degree of freedom. so it is adopted in many mis robot systems as the arm part ( fig. 3 ) [cit] ."
"the symbolic design of the mechanism is shown in fig. 9 . fig. 9 shows the front view of the mechanism, and fig. 10 shows the 3d design of the mechanism. the upper holder is mounted on linear motor no. 3, the lower holder is mounted on linear motor no. 1. the cannula passes through the cannula bearings which are mounted in these holders. the vertical movement of the linear motors 1 and 3 changes the \"ϕ\" which is the cannula angle. linear motor number 3 is mounted on linear motor number 1. the placement of this component is shown in fig. 11 . the aim of the designed mechanism is to change the angle made by horizontal rotation of the cannula passing through this point without losing the center of rotation point, which is defined as rcm point, and also to rotate the cannula around this point. the variables used when moving cannula are the cannula angle \"ϕ\", the rotation of the robot about z axis \"q2\", the displacements of the motors that move the holders vertically \"q1\" and \"q3\" and the amount of cannula advance \"q4\". these variables are shown in fig. 12 ."
"to obtain an information-enriched input attention component for this specific word, which contains the relation relevance to both entity 1 and entity 2. the second variant (variant-2) interprets relations as mappings between two entities, and combines the two entity-specific weights as"
"results of the experiments detailed in section 3 are presented in the following subsections, breaking down the research question to examine the three aspects of retrievability and performance. due to space limitations we only present the plots for the aq collection, however, table 2 provides the correlations across all the collections and models used. e plots presented show the map scores across the parameter sweeps as well as the odds of retrieving a relevant item over a non-relevant item, given the model, collection and parameter se ing. be seen that as odds increases, so too does the map, however, for most models, there is a small o set between when the odds peaks and when map peaks."
"we have presented a cnn architecture with a novel objective and a new form of attention mechanism that is applied at two different levels. our results show that this simple but effective model is able to outperform previous work relying on substantially richer prior knowledge in the form of structured models and nlp resources. we expect this sort of architecture to be of interest also beyond the specific task of relation classification, which we intend to explore in future work."
"given the hypothesis that be er performing systems exhibit a bias towards the relevant documents, the following research question was derived: do systems with be er performance also make the relevant documents more retrievable than the non-relevant documents? is question is investigated across three di erent aspects:"
"using the l 2 norm (note that w l y are already normalized). based on this distance function, we design a margin-based pairwise loss function l as"
"while position-based encodings are useful, we conjecture that they do not suffice to fully capture the relationships of specific words with the target entities and the influence that they may bear on the target relations of interest. we design our model so as to automatically identify the parts of the input sentence that are relevant for relation classification. attention mechanisms have successfully been applied to sequence-to-sequence learning tasks such as machine translation [cit] and abstractive sentence summarization [cit], as well as to tasks such as modeling sentence pairs [cit] and question answering [cit] . to date, these mechanisms have generally been used to allow for an alignment of the input and output sequence, e.g. the source and target sentence in machine translation, or for an alignment between two input sentences as in sentence similarity scoring and question answering."
"in the context of retrievability, if all documents were equally retrievable the gini coe cient would be zero (denoting equality within the population). on the other hand if only one document was retrievable and the rest were not, the gini coe cient would be one (denoting total inequality). many factors a ect the retrievability bias (denoted by the gini coe cient). ese include: the retrieval model, the parameter se ings, the indexing process, the documents and collection representations/statistics -as well as how the system is used by the user (i.e. the types of queries and the number of documents that they are willing to examine, denoted by the c parameter)."
"ere are also fewer signi cant correlations possibly meaning the relationship is not as stable here, or di erent way of analysing the data would be more appropriate."
"in figure 1 . the input sentence is first encoded using word vector representations, exploiting the context and a positional encoding to better capture the word order. a primary attention mechanism, based on diagonal matrices is used to capture the relevance of words with respect to the target entities. to the resulting output matrix, one then applies a convolution operation in order to capture contextual information such as relevant n-grams, followed by max-pooling. a secondary attention pooling layer is used to determine the most useful convolved features for relation classification from the output based on an attention pooling matrix. the remainder of this section will provide further details about this architecture. table 1 provides an overview of the notation we will use for this. the final output is given by a new objective function, described below."
"attention-based pooling. instead of regular pooling, we rely on an attention-based pooling strategy to determine the importance of individual windows in r *, as encoded by the convolutional kernel. some of these windows could represent meaningful n-grams in the input. the goal here is to select those parts of r * that are relevant with respect to our objective from section 3.1, which essentially calls for a relation encoding process, while neglecting sentence parts that are irrelevant for this process."
"1. our cnn architecture relies on a novel multi-level attention mechanism to capture both entity-specific attention (primary attention at the input level, with respect to the target entities) and relation-specific pooling attention (secondary attention with respect to the target relations). this allows it to detect more subtle cues despite the heterogeneous structure of the input sentences, enabling it to automatically learn which parts are relevant for a given classification. 2. we introduce a novel pair-wise margin-based objective function that proves superior to standard loss functions. 3. we obtain the new state-of-the-art results for relation classification with an f1 score of 88.0% [cit] task 8 dataset, outperforming methods relying on significantly richer prior knowledge."
"given a sentence s with a labeled pair of entity mentions e 1 and e 2 (as in our example from section 1), relation classification is the task of identifying the semantic relation holding between e 1 and e 2 among a set of candidate relation types [cit] . since the only input is a raw sentence with two marked mentions, it is non-trivial to obtain all the lexical, semantic and syntactic cues necessary to make an accurate prediction."
"input attention composition. next, we take the two relevance factors α 1 i and α 2 i and model their joint impact for recognizing the relation via simple averaging as"
"convergence. finally, we examine the convergence behavior of our two main methods. we plot the performance of each iteration in the att-input-cnn and att-pooling-cnn models in fig. 4 . it can be seen that att-input-cnn quite smoothly converges to its final f1 score, while for the attpooling-cnn model, which includes an additional attention layer, the joint effect of these two attention layer induces stronger back-propagation effects. on the one hand, this leads to a seesaw phenomenon in the result curve, but on the other hand it enables us to obtain better-suited models with slightly higher f1 scores."
"after this operation, we apply convolutional maxpooling with another secondary attention model to extract more abstract higher-level features from the previous layer's output matrix r."
"the main steps of the proposed symmetry detection algorithm is: 1) dense edge extraction, 2) multi-scale edge description with a local orientation histogram h p as a texture feature, 3) edge triangulation to find symmetry pairs, and 4) voting based on weighted pairs to detect a global bilateral axis."
"for data privacy and user privacy, a solution of data storage, manipulation and query is presented in this paper. in main database files, data are stored in key/value pair which is a typical nosql storage structure and are encrypted with elgamal homomorphic encryption scheme. keys in index are ciphertext of combinations of real keys in big blocks (in our experiments, one block is set to 1024 bits), which are encrypted with paillier encryption scheme [cit] which is an additive homomorphic cryptosystem. when a key is queried, comparison can be done on ciphertext in blocks that improves efficiency of query. protocols of data manipulation and query among data owner, service provider and querying user is given. algorithm for data updating and querying are implemented to verify usefulness of the solution. as to implementation of our solution, berkley db, a typical key/value pair model database, is chosen to construct a prototype system. it is an excellent storage solution for nosql database for its high efficiency and convenience."
"a series of experiments have been done to test efficiency of our solution. some vital parameters, like quantity of data, thread number, and length of key, have been changed to find difference. fig. 5 illustrates query efficiency of our solution. [cit] 0, 30000, 40000, 50000, 75000 and 100000, while y axis is average time used for a single value query. and we can get 3 curves when the length of key is set to 5, 10 and 20 decimal bits respectively. (as in binary, it should be 1, 2, and 4 bytes approximately.) on the whole, query efficiency is much better when length of key is not so long. the reason lies in that when the key is short, more keys can be put into one single block, therefore a comparison on block is equivalent to much more comparisons on single key."
"13 send the index file to sp and replace old one; we know, the query algorithm is oblivious for data user. the data user encrypted additive reverse of queried key at first, and send it to service provider. then the service provider extends it into queried block, and chooses proper index to multiple the queried block with a key block, and as follows, the product is sent to data owner one by one. when the data owner receives the product, it decrypts the cipher and decomposes the plaintext to find which one is 0, which means the according key in the block is equal to queried key. the serial numbers of the key are sent back to the service provider, service provider can get the key in main data file and get queried data for the data owner. this process will terminated when the queried key is found or all blocks in the index is searched. to database applications, querying is the most common operation. in our solution, all of the three roles, service provider, data owner and data user must participant in the querying process. to preserve both data owner and user privacy, querying process is more complex than in traditional database system. fig. 4 presents the querying protocol in detail."
"in fig. 6, tuple number is set to 50,000, to illustrate query efficiency variation on thread number and key thread number is vital for most service providers in clouds. and parallel process of key words comparison can improve query performance drastically. in our prototype system, block comparison is divided into several parts simply, query efficiency can be heightened further with sophisticated technology of parallel programming. note that thread number is a hardware-depended parameter. from the figure, we can see, query time increases more quickly when tuples are more than 50,000. and it means performance of our solution is more better to middle scale database."
"where * denotes the 2d convolution operation. although local edge orientation is important to detect symmetry, the surrounding texture around an edge segment is also an important feature especially for natural images. thus, we introduce a local orientation histogram h p as the weighted directional contribution of all neighboring points r in the corresponding grid cell. as described in figures (1b-1d), the histogram count at a given orientation τ k is:"
"homomorphic encryption scheme provides a good solution to privacy preservation for database system. we present a storage solution for nosql database using homomorphic encryption algorithms. protocol of data querying is proposed, and algorithms for data manipulation are given also. in indices, keys are composed into big blocks to improve the performance of encryption and decryption, therefore accelerate the process of data manipulation and query. although paillier and elgamal encryption scheme is not so efficient comparing to symmetric encryption schemes like des and sha. but it is good enough for some cases that users pay more attention on information security than computation performance. future work includes improving efficiency of the system and extending system functionality, such as extended query on range, aggregation, and join."
"when a data owner outsources his data to service provider in cloud, some preparation works must be done. at initial stage, data are encrypted in key/value pair, indices are constructed with algorithm 1. then, service provider uses the indices to improve query efficiency. and maintenance of data and indices are task of the data owner. when there are some tuples to be inserted or deleted, data owner must arrange an insert or delete operation on main database files which is stored at service provider, reindex the changed data and sent the index to service provider. 9 //n 2 is sqare of n in encrypting algorithm; for the data encrypted under homomorphic encryption scheme, order of data is not kept in ciphertext. in order to simplify data maintenance, data in plaintext can be ordered in time, which means the data owner need to attach the data when it is put into database. when a fresh tuple is going to be inserted, it must be at the end of database files. when a tuple is going to be deleted, it will not be deleted physically, it will only be marked as 'deleted'. the 'deleted' data will not appear in any query results, and can be rearranged after a certain amount of manipulation. let updating operation be done by deleting and inserting operation, data inserting and deleting algorithm are given as algorithm. 2 and algorithm. 3 respectively."
"from flickr photo sharing website, liu's vision group provides a single-symmetry groundtruth for synthetic/real images [cit], 2011 [cit] datasets are 134, 14 and 75 respectively). the quantitative comparisons are presented in figure 3 with respect to each version of psu dataset. our method is ranked first for all psu version with a difference of more than 10% among the nearest : comparison of accuracy rates for our method \" [cit] \" against the baseline algorithm \" [cit] \" [cit], its improvement \" [cit] \" [cit] and one of the recent algorithms \" [cit] \" [cit] . the used datasets are: different versions of psu (2010, 2011, 2013) with provided groundtruth [cit] and ava with our groundtruth."
"in this section some symbols are defined for simplification. owner means data owner, sp is service provider in cloud, and user data user for query. we use paillier crypto-system to encrypt keys and values in nosql database. system parameter is taken as n. enc(m, pk) is the function to encrypt plaintext m with public key pk, and dec(c, sk) function to decrypt ciphertext c with private key sk. denote the public key and secret key of data owner as (pk owner, sk owner ). denote the public key and secret key of user user i as (pk useri, sk useri ). all data are encrypted using homomorphic encryption algorithm, each one of owner, sp and user publishes his public key and uses the private key to decrypt ciphers."
"where φ k is the angle associated with τ k . if j k (p) denote the modulus of wavelet coefficients at point p, local edge characteristics j p and τ p are obtained by seeking the maximum wavelet response and orientation over all orientations. formally:"
"our experiments are conducted on bdb database system on windows 7. we implements the generalized paillier system with basic java package, and the elgamal scheme is from open source library of bouncy castle [cit] . all experiments of the solution are implemented in java with jdk 1.7 and the prototype system are run on personal computer with intel 2ghz processor and 2gb memory."
"our method investigates cicconet's edge features [cit] within loy's scheme [cit] by adding neighboring-pixel information resulting a better symmetry represen-tation in the voting space. additionally, it overcomes the scale limitations by adding multi-scale information."
"consider environment illustrated in fig. 1, data owners outsource their data and query services, but the data is private assets of them and should be protected against the service providers and querying users in some extent. on one hand, data owner can update, query and authorize access of data, while the service providers in cloud should know nothing about especially detailed data, and query users should know not more than the exact answers for what she/he is querying. on the other hand, query users partially supported by national science foundation of china (61103232, 61272402, 61202294),guangdong provice nature science foundation (10351806001000000, 10151064201000028), guangdong science technology plan project (2010b010600046, 2011b090400325), guangzhou science technology plan project (12c42101606)."
homomorphic encryption allows specific types of computations to be carried out on ciphertext and obtains an encrypted result which is ciphertext of the result of operations performed on the plain text. the additive homomorphic property of a homomorphic cryptosystem is
"in section 2, we analyze related work of symmetry detection. then we describe the details of the proposed algorithm in section 3. we present the experimental results on psu and ava datasets in section 4 and the conclusion in the last section."
"in addition to the geometric edge information, the symmetry degree of the two regions around p and q can be measured by comparing their corresponding local orientation histogram h p and h q . due to symmetry considerations, one of the two histograms must be shifted and reversed before comparison. if d i is the histogram intersection distance, and h * q the reversed histogram, the texturebased symmetry measure is given by:"
"need to query data from cloud, but the query might disclose sensitive information, behavior patterns of the user. for example, when alice searches a website, such as facebook, for friends who share the similar backgrounds (e.g., age, education, home address) with her, she should not disclose the query that involves her own details to the cloud. privacy of data owners and query users are defined as data privacy and user privacy respectively."
"we propose a novel method for symmetry detection preserving the shape and texture information inside an image. this work contributes with: (1) introducing a local edge orientation histogram, (2) utilizing multi-scale edge extraction to exploit the full resolution image. in addition, we introduce groundtruth of a global symmetry axis inside images extracted from the large-scale aesthetic visual analysis (ava) dataset [cit] . this dataset is composed of artistic photographs which involved in on-line professional photography competitions."
"nosql database is defined as the next generation databases mostly because of the following characteristics: being non-relational, distributed, open-source and horizontally scalable [cit] often more characteristics apply such as: schema-free, easy replication support, simple api, eventually consistent / base (not acid), a huge amount of data and more. from nosql data modeling techniques [cit] data model for nosql database can be cataloged into key/value or tuple store, bigtable style databases, document databases, and graph databases. berkeley db is a robust solution on which to build a nosql system, and the storage system of its key/value pair is more efficient than other database. that is the reason for us to choose it as our base database in our experiments."
"in this protocol, data user prompts a query by encrypting additive inverse of queried key with public key of data owner and sends it to the service provider. the second step starts when the service provider receives a query request. the service provider chooses proper index, and then a 1024 bits big integer m is composed by repeating the queried key several times. then multiplies m by each key block respectively, and sends the results to data owner. when the data owner receives the products, he decrypts the cipher and decomposes the plaintext to find 0 in each product. this procedure can be ended when the key is found or all products are searched. then the serial number of equal key is sent back to the service provider, and all the queried data are sent to the data owner. at last step, the data owner decrypts the data with his owner private key and encrypts it with public key of the data user and sent the result to him."
"today cloud computing and data outsourcing provide much convenience for kinds of enterprises. for instance, enterprises can concentrate on their main business while outsourcing their complex data management and query service to service providers in cloud. these service providers in cloud focus on data management, and provide high quality service. but in such kind of computing pattern, a bottleneck, privacy preserving of data owners and query users, seriously restricts progress of cloud computing."
"during the query process, additive reverse of the queried keyword is encrypted before sending to the service provider, and the queried data is sent by data owner, therefore the service provider can get no information about what the data user is querying on the database. security can also be enforced by adding disturbing data when the data owner requests query data from service provider. and as to the data owner, during the query process, only product of the queried block and index key blocks are received and decrypted to find the order of equal ones, while the queried key is kept invisible. the data owner do not know which index is chosen and cannot deduce what the data user is querying. it is obvious that we cannot complete query without leaking no information about the user and what she is querying. at least, queried result must be sent back to her. what we really want to do and can do is to limit the information leakage as much as possible. from the analysis above, we can see, confidence of data owner can surely be protected for homomorphic encryption scheme is used. data privacy and user privacy are all kept by the scheme we propose to some extent."
"in this section, we illustrate the query details first, and then propose the query protocol. as the keys are in block, the queried key must be repeated n times and together to meet each key in a key block. fig. 3 presents an example of query procedure. as shown in fig. 3, there are 4 keys in a key bloc. then in plaintext keys 0, 1, 10, 2 is combined into a big integer as 000001010002, and then encrypted into ciphertext. as to the queried key, the additive reverse is used to composite into block, then the queried block is −002 − 002 − 002 − 002 when the queried key equals to 2. we use additive reverse of the queried key to transfer minus into addition operation in plaintext, then multiplication on ciphertext can be used to implement subtraction, according to the additive homomorphic property of paillier encryption scheme. find the block b t in which t is the ith key;"
"this paper introduces multi-scale edge features combined with local orientation histograms, in order to develop a reliable global symmetry detection among variants of visual cues. quantitative and qualitative comparisons show a substantial advantage for our proposed method on different types of public datasets. our model can be improved to handle complex images with non-centered viewpoints and large degrees of perspective view. it can be extended to solve the over-fitting of axis endpoints. the future work is to introduce a stable balance measure; describing the existence and degree of global axes in terms of both geometrical and symmetrical properties inside an image, and to integrate this measure within existing retrieval systems of visual arts."
"the rest of the paper is organized as follows. section ii provides background information on homomorphic encryption scheme, nosql database and index. section iii describes the query protocol and main algorithms, security analysis is also given in the section. performance and analysis of experiments results are shown in section iv. finally, section v gives conclusions and future works."
"the aim of this part is to evaluate the results of our method and to compare it to concurrent methods from the state-of-the-art. the evaluation of symmetry detection had received re-surging interest in the main computer vision conferences. indeed, symmetry detection challenges [cit] 1, [cit] 2 [cit] 3 . they all rely on a dataset proposed by liu's group publicly available with an associated groundtruth and evaluation method. we then use these three general purpose datasets for our evaluation and in addition, we provide an additional aesthetic-oriented dataset. to build this fourth dataset, we extracted some symmetrical images from the aesthetic visual analysis (ava) collection and generated axis groundtruth among those images."
"the best bilateral symmetry axis inside the image is detected by searching for the global maximum of the mirror symmetry histogram h s . for that purpose, we solve the orientation discontinuity problem by extending the voting space in a circular way. the spatial extension of this axis can be determined as the convex hull of the set of feature points contributing to this axis. figures (2a-2d) show the symmetry detection for some example in terms of two voting histograms."
"the baseline algorithm to detect a global bilateral symmetry axis was introduced by loy and eklundh [cit] . the general scheme is as follows: (1) extraction of local-feature points using lowe's sift [cit], (2) matching of pairs of those keypoints based on the similarity of corresponding mirror sift descriptors, and (3) use of the pairs in hough-voting space to find the best symmetrical candidate. many attempts proposed refinements to loy's general scheme, especially mo and draper [cit] in selecting all symmetry candidate pairs instead of finding closest matches at each point, and using less complex hough voting scheme. the improvement produced more particles than [cit] for the symmetry axis, to elongate the output axis according to the global texture information of an image. these intensity-based methods depend mainly on the properties of sift features, and their capabilities for symmetry matching with respect to intensity and orientation in the neighboring pixels. in the presence of smooth objects with noisy background, the global symmetry information is lost due to a small number of extracted feature points inside an image."
"where φ k and φ r are angles associated with τ k and τ r, j r and τ r are defined the same way as for p (equations (5)- (7)) and δ x is the kronecker delta. h p is subsequently 1 normalized and circular shifted so as the first bin corresponds to τ p ."
"symmetry is a very important measure in some computer vision applications such as aesthetic analysis [cit], object detection and segmentation [cit], depth estimation [cit], or medical image processing [cit] . this paper focuses on detecting a single bilateral symmetry axis by exploring the similarity between regions which are mirror reflected around an image axis."
"in the case of the nonsticky finger, the convergence rate varies from 66% to 90% with the highest percentage for the lowest rotation amplitudes. this low convergence rate can be explained by the fact that the hand has only three fingers. indeed, the regrasping can occur if and only if two of the three fingers can grasp the object, which is very dependent on the object's shape. the convergence rate increases to 95% in the case of sticky fingers. therefore, it appears simpler to find an admissible trajectory when adhesion is considered."
"the paper is organized as follows: section ii recalls the recent advance of mcdm with aspiration levels. three types of cles are reviewed in section iii. then, section iv describes the problem and offers a solution. the approach is then illustrated by solving the bdap provider selection problem in section v. finally, section vi presents a comparative analysis while section vii concludes the paper."
the presence of adhesion has a strong impact on the trajectories and fig. 10 . images sequence describing the trajectory generated by the planner for a 206
"thanks to their semantics-based computational strategy, these two approaches can solve multi-granular linguistic gdm problems. but the advantages of the proposed approach is prominent: firstly, we take the linguistic aspiration levels of experts into account. secondly, the uncertain decision information, including the performances and aspirations, is elicited by natural languages taking the form of several types of cles."
"qualitative information, taking the form of subjective assessments, exists in almost everywhere of the real-world decisionmaking problems [cit] . such subjective assessments, instead of exact numerical values, are convenient to represent information originating in complex, ill-defined and unstructured problems. for instance, an expert would like to use terms such as low and high to evaluate the capability of data analysis of a software. to handle subjective assessments in qualitative decision-making (qdm), the techniques for computing with words (cww) [cit] are required. the fuzzy linguistic approach, whose core concepts involve linguistic variables, might be the most commonly used one in cww because it strengthens the feasibility, flexibility and reliability of decision models [cit] . the values of a linguistic variable take the form of words and expressions constructed in natural or artificial languages [cit] . a suitable operation of these values could narrow down possible gaps existing in human-computer communication. in classical models, such as the linguistic 2-tuple model [cit] and virtual linguistic model [cit], the values of a linguistic variable come from a linguistic term set (lts) defined by the syntactic and semantic rules. based on certain classical models, the recent developments of qdm are two-fold, i.e., they concern the developments of linguistic representation models and the construction of novel qdm approaches."
"these current developments have delivered great contributions to mcdm with aspiration levels. the merits of three categories of investigation are prominent. the probabilitybased methods and fuzzy aspiration-based methods have the advantage to model uncertainties of representing aspiration levels, whereas the reference point-based methods pay more attention to model the psychological behavior of decision makers. the interactive methods seem to be a wonderful way to follow the idea of satisficing heuristic. yet the optimization models can reduce the participation from the experts."
"(2) the proposed solution heavily depends on the semantics of cles. it would be also interesting if other approaches can be developed based on the ordered structure of the ltss. in this case, some other theoretical issues, such as the order relations of the set of cles, should be addressed at first."
"the goal of the m 3 qdm problem is to select the most desirable alternative(s) from a, according to the linguistic information provided by the experts."
"during finger gaiting (i.e., detaching a finger from the object and eventually repositioning it), the pull-off force between the fingers and the object has an important impact. indeed, when regrasping, the finger pulls the object with a measured value w po . this pull-off force may disturb the grasping equilibrium. in other words, the remaining grasping fingers must compensate for the pull-off force caused by the removed finger to guarantee the object's stability. this can be formalized as"
"(7) similarly to the equilibrium problem, it is possible to use the convex hull formed by the n − 1 remaining fingers to test if the grasp is stable during finger gaiting. in this case, if the external perturbation (−w ext − w po ) is included in the hull, then the grasp is stable."
"second, the utilities and aspiration levels of experts are not well taken into account. most of the papers mentioned above simply substitute the experts' utilities with the evaluation values. nevertheless, the utility function may not be regarded as an increasingly or decreasingly linear function with respect to the evaluation values. to the contrary, people usually make decisions based on the aspiration levels (or targets) instead of the performances of alternatives because of the limited cognitive resources and incomplete information. the concept of aspiration levels, which rests at the core concept of bounded rationality [cit], plays a key role in multi-criteria decision-making (mcdm). real-world decisions could plausibly be made by accepting the first encountered alternative which meets a sufficiently good target [cit] . it is rational and essential to consider aspiration levels in qdm. till now, there are a number of studies focusing on aspirations in the mcdm problems. refer to section ii for more details. however, very few of them consider the qdm problems with complex linguistic information. feng and lai [cit] presented an aspiration-based approach for heterogeneous information. however the linguistic information in that study can only take the form of either single terms (for a certain criterion) or ults (for another criterion). thereby, the second motivation of this study is to introduce the idea of aspiration into the field of qdm where the decision information takes the form of multi-types of cles."
"(1) the proposed group consensus reaching algorithm is not efficient enough. as can be seen in the case study, the size of the entire group is large, thus the interactions with the experts could be time-consuming. it would be interesting to develop a more efficient algorithm. for instance, an automatic approach, which revises the most inconsistent opinions based on the collected information, could be developed to enhance the group consensus."
"2) navigation constraints: three constraints must be taken into account when navigating inside and between the maps. the first one is the rolling constraint and thus the navigation in the m k maps. indeed, in order to manipulate an object with n fingers considering rolling without sliding, all the fingers must roll the same distance and in the same direction on the object surface. consequently, the rolling constraint induces a unique available path in an m k map (depending on the radius of the fingers). thus, each node in m k has a maximum of two neighbors in the same map. as an example, we consider the map and the grasp depicted in fig. 4 . the grasp corresponds to the element m 2 (160, 65), in which the fingers have the same radius. thus, the neighbors of this grasp are m 2 (160 + δ, 65 + δ) and m 2 (160 − δ, 65 − δ) (with δ representing the step of the discrete representation). indeed, for a clockwise rotation, the next grasp will be m 2 (160 − δ, 65 − δ); otherwise, the next grasp will be m 2 (160 + δ, 65 + δ)."
"the effect of this attractive force on the slippage limit condition is that the friction cone is shifted as depicted in fig. 2 . in addition, contrary to the macroscale, where only positive grasping forces can be applied (push the object), in microscale, it is possible to apply negative forces (pull the object) as long as the force lies in the modified friction cone."
"a case study regarding bdap provider selection is presented to demonstrate the proposed m 3 qdm approach. in order to focus on the illustration of the approach, we consider only a simple case of the selection problem. also note that our major focus is the provider selection rather than the details of big data auditing."
"this paper has been motivated by the problem of bdap provider selection. the m 3 qdm approach is necessary because it is quite natural that multi-criteria and multi-groups of experts are involved and multi-granular linguistic information, taking the form of cles, is inevitable in the focused problem. moreover, linguistic aspiration levels have also been considered in the approach. the semantics of cles is utilized to aggregate the linguistic information with distinct granularities and obtain the value functions with respect to linguistic aspiration levels. the approach has been identified by a case study. based on the completed study, we can draw the following conclusions:"
"for example, a lts with 9 terms can be found in (1) . the semantics of a term s α is a fuzzy set defined in the domain u, usually represented by a trapezoidal fuzzy number (trafn) (a, b, c, d), where"
"the next section gives an overview of the related work in automatic dexterous micromanipulation, while section iii gives a general formalization of this problem. section iv details our methodology to compute stable grasps and generate trajectories. finally, section v presents several trajectories for in-hand manipulation with sticky fingers, which are compared to classical nonsticky finger trajectories."
"step 3: associated with the optimal weights, the three value matrices can be fused to a group value matrix this matrix serves as the first five columns of the overall value matrix with respect to all criteria. repeating the above process for all six subsets of criteria, the overall value matrix can be derived."
"the collisions between the fingers are also tested in order to guarantee that the generated trajectory is reachable by the manipulator. in addition, the fingers cannot have two contact points or more with the object's surface. thus, depending on the object geometry and on the finger radius, some nodes in m k and d k are discarded. moreover, in the case of polygonal objects, we consider that the fingers cannot be positioned on the vertices."
it can be seen that the rankings of alternatives with respect to the criteria in c 1 are different. this can be concluded that the consideration of aspiration levels in linguistic setting would greatly influence the final decision.
the manipulated objects are made of acrylic and have the following dimensions: 6 mm long and 4 mm tall for the object depicted in fig. 16 and 4 mm long and 4.5 mm tall for the object depicted in fig. 18 .
"in order to change the contact position, the finger must be detached from the current contact position p c 1 by moving to p a,c 1 . then, the finger moves freely from p a,c 1 to a position near the new contact p a,c 2 . finally, the finger is placed on the object by moving from p a,c 2 to the contact position p c 2 ."
"even if the context of this study was micromanipulation and microassembly, it is possible to imagine extending these results to macromanipulation. indeed, the benefit of using sticky finger is independent from the manipulation scale. thus, considering robotic hands with suction effect at the fingertips may improve the dexterity of the hand. [cit] ."
the overall value matrix with respect to all the criteria can be formed after the consensus of all groups has been improved by the algorithm in table 3 .
"the next steps of this work will involve the validation of these new trajectories on an experimental micromanipulation setup (see fig. 1 ). in addition, the current planner will be improved to automatically select the initial grasp to generate globally optimal dexterous in-hand manipulation trajectories and will be extended to nonplanar objects. visual feedback control will also be considered to compensate for the manipulation errors. furthermore, taking into account the temporal aspect of the finger placement (especially during the initial grasp) will be investigated as well as different model of the contact finger-object."
"it can be observed that we extend the existing concept of fuzzy aspirations by two aspects. on the one hand, the experts can express their aspiration levels by means of either hfltss or ltwhs. on the other hand, the two types of cles can be used arbitrarily by the experts according to the linguistic expression in mind."
"beyond the current dexterous micromanipulation approaches, the proposed method developed in this paper is not limited to squared objects. indeed, arbitrary shapes are considered, and the rolling constraint of the fingers during the object rotation is explicitly taken into account. furthermore, the sticky forces that may exist in the microscale are taken into account in the finger path planning."
"based on the two arguments, this paper proposes a novel qdm approach established in linguistic setting. the originality of the proposed approach is implied by the following: (1) two types of cles, i.e., hfltss (including ults), and ltwhs, are included simultaneously to represent uncertain linguistic information. experts could express their opinions by means of these cles without any limitation. we focus on these cles because they frequently emerge in natural languages. (2) experts' linguistic aspiration levels are considered. when making decisions, one looks for the alternative which satisfies the experts' aspirations to the highest extent. the experts' aspirations could be conveniently expressed by the two types of cles as well. (3) the focused qdm problem involves multi-criteria, multi-groups of experts and multigranular linguistic information. thus the proposed approach can be referred to as the m 3 qdm one."
"for the last action (adding a finger), the cost, c a, corresponds to an approximation of the distance between the last position of the finger and the new one. this value represents an approximation of the distance required to put in contact the finger with the object."
"except for some specific concentration of decision-making with single criterion utility function [cit], most of the existing studies contribute to mcdm. among them, most studies link aspiration levels to probabilities where risk choices are involved, some link them to reference points, and others consider the fuzzy aspirations. the following is organized based on this taxonomy."
"an important parameter of the graph search is the cost function used to characterize the distance between two nodes. considering the way we can navigate through the graph, we define three cost functions corresponding to the three possible actions (rolling, placing a finger on the object, and removing a finger)."
"the concept of aspiration levels plays an important role in managerial decision-making. in the satisficing model [cit], subjects seek an alternative or solution that meets aspiration levels, instead of maximizing the expected utility in the classical sense. in the conventional utility theory, the utility function is monotone with respect to the performance values. however, ample and substantial empirical evidence indicates that individual preferences cannot be described by the conventional concave or convex utility functions [cit] . the satisficing heuristic works as follows: if a solution (or a small set of solutions) can be found to satisfy the stated aspiration levels, then it is accepted; otherwise, the aspiration levels should be relaxed. if too many solutions are admitted by the aspiration levels, then they should be tightened [cit] . the consideration of aspiration levels would benefit to decrease the complexity of the problem in hand, because of the subject limitation of cognitive capabilities [cit] ."
"hfltss, which were developed for the situation where the experts are thinking of several terms at the same time [cit], tend to list all original terms involved in a comparative linguistic expression. generally, a hflts is an ordered finite subset of s (τ ), which can be generated by the following transformation function e g h :"
"the trajectories being generated in the object frame and executed without feedback, the manipulation system must be well calibrated. thus, the homography between the camera frame, the object frame, and the actuators frame is computed. thanks to these homography matrices, the computed trajectory for each finger is converted in the actuator frame and executed."
"(1) the range of linguistic expressions is extended. according to definition 1, more cles are involved. based on the proposed semantics-based approach, we can deal with these cles by the same framework. the focused cles include most of the natural way to express uncertainties in linguistic setting. the experts are permitted to use either hfltss or ltwhs to express both linguistic aspiration levels or performance values. hence, the experts can concentrate on the evaluation rather than stating their opinion by a fixed simple grammar."
"2) polygonal object: similar tests have been performed on the polygonal object, and the statistical results are depicted in fig. 14. once again, when using sticky fingers, the manipulation is about 35% faster."
"the paper focuses mainly on the development of the m 3 qdm approach. following the conventional utility approaches and aspiration-based approaches, we develop a common syntactic rule to represent the focused types of cles, and then define the value functions of cles based on their semantics. the approach is then illustrated by the problem of selecting a bdap."
"the proposed approach relies on three assumptions: the existence of adhesion forces, an estimation of the adhesion strength, and an estimation of the friction coefficient."
"in the trajectories presented above, the initial grasp was selected randomly among all the possible stable grasps. this means that the optimization performed by the planner is constrained by this initial choice, which is unlikely the most optimal, although it is theoretically possible to generate the manipulation trajectories for all the possible grasps and select the best one. however, the additional computation time has to be shorter than the time saved in the optimal trajectory execution, which is not the case yet. to do so, the generation time of the initial grasp and manipulation trajectories has to be optimized."
"in sum, the most prominent feature of the m 3 qdm approach is that it considers multi-criteria, multi-groups of experts, and multi-granular linguistic aspiration levels and evaluation values simultaneously."
"the last constraint deals with adding a finger. to switch from m k to m k +1, it is required that the k fingers used to form the grasp in m k are also used in the grasp with k + 1 fingers (obviously at the same position on the object surface)."
"during all the manipulation process, the object must be in equilibrium. an n -finger grasp is in static equilibrium if and only if the following equation is satisfied:"
"let us consider the general case of six-dof manipulation using n fingers. in order to manipulate the object, fingers must apply a grasping force on the object's surface. this action can be modeled in three ways: punctual and frictionless contact, punctual and frictional contact, and soft contact. we assume that, in microscale, the applied forces can be modeled using the punctual frictional model, and thus using the coulomb law. this means that no slippage happens as far as the applied force lies in a 3-d cone"
"(2) the consideration of aspiration levels in linguistic setting could greatly influence the final decisions. in real world problems, therefore, it is worthwhile to mine the aspiration levels of the experts."
"in robotics, rotations can be obtained in two ways. the first one, which is the most common industrial solution, consists in using a basic tweezer placed on a robot, which rotates the carried gripper. thus, this approach uses a simple gripper, such as a parallel jaw gripper, and the accuracy is limited by the backlash and the eccentricity in the robot's joints. the second way consists in using a dexterous hand to perform in-hand rotation [cit] . this method, which requires a more elaborated gripper, can be very versatile as a single \"hand\" can manipulate a large range of objects. the microhand fingers can be mounted on simple translation actuators with high repeatability. in this case, the accuracy is dependent on the contact between the fingers and the object surface."
"the differences between the trajectories with and without sticky fingers are due to the stabilizing effect of the adhesion forces. fig. 11 shows this effect on the m 2 map of the arbitrary shaped object presented in figs. 7 and 9 . the blue and yellow areas correspond, respectively, to the m 2 maps with and without the stickiness effect. as predicted, the stability area is significantly larger when pull-off forces are considered. this means that there are more options to manipulate the object in presence of adhesion forces."
"moreover, fig. 12 represents similar results for a regrasping map in d 3 . it can be seen that, without adhesion, 100% of the equilibrium nodes in m 2 (yellow part in fig. 11 ) are stable regrasping nodes in d 3 (yellow part in fig. 12 ). conversely, with sticky fingers, the admissible regrasping nodes (blue part in fig. 12 ) represent only around 18% of the equilibrium nodes (blue part in fig. 11) . this difference is due to the fact that adhesion that acts as a stabilizing force acts also as a disturbing force when a finger is detached from the object. the (two) remaining fingers must resist to this force, which is not possible in all the equilibrium grasps."
"in the evaluation process, three context-free ltss, denoted by s (4), s (6) and s (8), are available. the semantics is shown in fig. 6 . associated with the set of linguistic hedges in (5) the experts are allowed to express their aspiration levels and evaluation values by means of ults, hflts, or ltwhs."
"the determination of the criteria of bdap provider selection is quite different from that of traditional decision support systems (dsss). for instance, when developing an enterprise resource planning systems, most of the necessary techniques are common knowledge for all the potential providers. when facing big data, however, current techniques for almost all aspects of big data processing are scattered in different companies and institutions, and are far from meeting the ideal requirements [cit] . this would result in the difficulty and uncertainty regarding assessing the quality of providers with respect to big data techniques-related criteria. as can be seen in table 4, the selected criteria are classified into two classes. the first class focuses on the ability of processing big data and making informative decisions. two subsets of criteria, namely data curation (c 1 ) and auditing decision support (c 2 ), are involved. data curation refers to the ability of capturing, cleaning, aggregating, identifying, and protecting data. it prepares high-quality data for data analysis tools. auditing decision support focuses on the techniques and technologies involved in the bdap that are effective enough to support big data-based auditing decisions. the criteria in the second class are frequently considered in the provider selection of traditional information systems [cit] . these criteria are classified into four subsets, namely, service quality (c 3 ), integration (c 4 ), economics (c 5 ), and professionalism (c 6 ). consequently, the proposed hierarchical model contains 6 groups of 24 criteria."
"exterous manipulation has been an active field of research in macroscale for decades [cit] . indeed, the grasping and manipulation planning problems have been explicitly formalized [cit], and various approaches to perform dexterous manipulation have been studied, such as rolling [cit], sliding [cit], and this paper has supplementary downloadable material available at http://ieeexplore.ieee.org, provided by the author. the material consists of a video, viewable with vlc media player, presenting simulated dexterous manipulation tasks, the experimental setup, and the experimental validation of these original finger trajectories. this video illustrates the developed method as well as the experimental results. the video clearly shows the added value of adhesion forces for dexterous in-hand micromanipulation. the experimental setup includes three cylindrical fingers of 1-mm diameter, each one mounted on a two-degree-of-freedom translation table based on piezoelectric actuators. the obtained results illustrate that original dexterous manipulation can be successfully performed in an open-loop control. the size of the video is 4.5 mb. contact ja.seon@outlook.com for further questions about this work."
"in order to validate this new manipulation strategies, the proposed trajectories exploiting adhesion forces have been tested experimentally. a typical trajectory that does not exploit adhesion forces can be viewed in fig. 1 ."
"3) trajectory generation: this operation consists in navigating in the generated maps using the a * algorithm. this heuristic graph search algorithm provides a complete and optimal path between the initial and the goal node. a * has been used in micromanipulation [cit] and also in microassembly [cit], but, based on our knowledge, it has never been used for planning in-hand micromanipulations. we chose to implement an a * algorithm for our trajectory planner in order to obtain optimal manipulation trajectories."
"in finger gaiting, to guarantee that a given finger i can be pulled-off from the object without disturbing the grasp, we ensure that the resulting wrench w po i, induced by the pull-off force f po i, is included in the convex hull formed by the friction cones' wrenches of the n − 1 remaining contacts. the regrasping map, corresponding to the detachment of the ith finger in an n -finger grasp, can be formalized as"
"this subsection is devoted to defining the value functions with respect to three types of linguistic aspiration levels. similar to other aspiration-based solutions in fuzzy circumstance, we consider fuzzy aspiration levels which are expressed by linguistic expressions and refer to them as linguistic aspiration levels. in the proposed approach, aspiration levels and performance values can be represented by the cles defined in definition 1. for convenience, a cle is denoted as ll in the sequel."
"(1) the proposed model enlarges the range of values that can be assigned to a linguistic variable. linguistic expressions, taking the form of hfltss and ltwhs, are available to represent opinions under uncertainties. the use of cles increases the flexibility of modeling uncertainties."
"note that, in (3), the selected 2(ς + 1) points are out of the domain u and thus could be called virtual points. they are selected so that the semantics of ltwhs could be represented by means of trifns, as can be seen in theorem 1."
"(2) the processing of multi-granular linguistic information is very easy. for the convenience of evaluation, a set of ltss are defined on the same domain. during the assessment, the experts can select the ltss according to their preference and/or the acquisitus knowledge. when handling this multi-granular linguistic information, the proposed approach defines value functions by means of the probability of a performance value achieving its aspiration. this makes the linguistic information be operated as easy as usual."
we chose to consider two values to evaluate the performance. the first one is the cost of the computed trajectory based on the cost function described in the previous section. this value estimates the distance traveled by the fingers during the manipulation and is equivalent to the trajectories' execution time. the second one is the convergence rate of the trajectory generation.
"planning finger trajectories during finger gaiting can be achieved using the same algorithm presented above (a * ). indeed, in this case, the nodes correspond to the spatial coordinates and the algorithm must find a path between the initial finger position and the new desired position by avoiding collisions with the object and the other fingers."
"if ll (ca) takes the form of ''lower than s α+1 '', then the value function of ll is the same as that in (18) . secondly, if ll (ca) takes the form of h t, s 0, then the value function of ll is:"
", where l and r are real numbers, a linguistic variable can be defined by a syntactic rule to present the names of values and a semantic rule to identify the meaning of each value [cit] ."
"2) experimental validation: the trajectories presented previously have been tested on the experimental setup. fig. 16 represents an images sequence, which shows the achievement of the 132"
"remark 2: different from the existing models of linguistic hedges, such as the powering hedges defined by zadeh [cit], volume 7, 2019 ltwhs are proposed to model the uncertainty of using single linguistic terms which is implied by the weakened hedges. the transparent feature of ltwhs is that they focus on a new type of cle, which is frequently considered by the experts under uncertain circumstances."
"it is clear that hfltss and ltwhs are not mutually substitutable. they models different types of cles which are both frequently emerged in natural languages. mathematically, there are usually two ways to depict uncertainties of a variable. the first one is to determine the boundaries of the variable and then form an interval. the second one is to seek for a constant that could be the real value and then decide a radius to indicate the range of the variable. obviously, hfltss model uncertainties of cles by the former manner and ltwhs model those by the latter manner."
"2) algorithm characteristics: for these trajectories, there is only one cost function linked to the displacement of the finger in the space. thus, this displacement is evaluated by using the euclidean distance between two points."
"moreover, it is interesting to note that the trajectories used to move the object depicted in fig. 18 were generated for a planar object (as depicted in fig. 8) . however, the real object have a shape similar to the lego's shape (a square with a cylinder on the top). this means that it is possible to manipulate more complex objects as far as the contact points have the same profile."
"moreover, the translation of the object is simply performed by the translation of all the fingers, which does not affect the stability of the grasp. thus, we will focus in this study on the rotations."
"the previous trajectories seem to be more efficient when adhesion forces are exploited. indeed, on the arbitrary shaped object and on the polygon, the trajectories require less finger gaiting steps and also less displacements of the fingers. a statistical analysis has been done to confirm that manipulation exploiting adhesion forces are better. trajectories were generated with and without sticky fingers for the two objects presented above and with the same simulation parameters. in every test, the initial grasping configuration is chosen randomly in the admissible initial grasps using two fingers. moreover, note that with random initial grasp, it is possible that the algorithm does not converge to the desired rotation."
"for the convenience of notations, let us consider an attribute x whose evaluation value x falls in the domain [x min, x max ]. in the conventional utility theory, decisions are made based on the concept of expected utility [cit] . based on some axioms, the von neumann and morgenstern utility function u(x) is frequently considered. in the aspiration-based model, the value function is aspiration-oriented and depends only on whether the aspiration is achieved [cit] . in this sense, a decision maker has only two different utility levels, that is,"
"ideally, the deviation would approach 0 if the group's opinion is highly consensual. in our m 3 qdm problem, the experts in each group are not assigned weights. to reduce the experts' work as much as possible, we intend to figure out whether there is at least one weighting vector, which results in an acceptable consensus degree of the group. thus, the following model can be formed:"
"in the case of micromanipulation, this contact law is slightly modified. indeed, adhesion acts as an attractive force (f po ) between the finger and the manipulated object. this force, called pull-off force, represents the force required to detach the finger from the object. in presence of this force, the coulomb law can be rewritten as"
"both approaches have different performances in microscale. indeed, backlash and eccentricity (typically around 10 μm [cit] ) might be comparable to the object dimensions (typically around 1 to 100 μm). consequently, in the first way, the accuracy is highly disturbed by backlash and eccentricity in the robot joints. thus, this paper focuses on the second way proposing to perform microobject rotation using in-hand dexterous manipulation."
"similarly, placing a finger is divided into two phases. the first step consists in moving freely in the space from p 0 to a position near the new contact p a,c . then, the second step consists in moving along the contact's normal to precisely place the finger on the object."
"dexterous manipulation in microscale is different from the macromanipulation, since gravitational and inertial forces are dominated by the surface forces such as van der waals, electrostatic, and capillary forces [cit] . these attractive forces change the manipulation paradigm as the objects stick to both the manipulating fingers and the substrate. thus, planning finger trajectory for effective dexterous manipulation in microscale is significantly different from macroscale."
"as this experimental setup is at millimeter scale, so adhesion forces are not predominant. thus, to exhibit the behavior that is encountered in microscale, a polymer was deposited on the fingers to enhance the adhesion forces. it generates a behavior that is not identical to the microscale behavior but almost similar. moreover, in section v-c, we highlight that the adhesion strength has no impact on the stable grasps and finger gaiting configurations. thus, even if this polymer does not accurately mimic the microworld, it is sufficient to fall back to microscale specificity where adhesion forces are predominant over other external forces."
"based on the semantic assumption specified in section iii-c, the computation of the value functions is very easy. we illustrate the procedures by the following example:"
"when handling uncertain linguistic information in the qdm framework, there are at least two limitations in the current developments which constitute our motivations. first, linguistic expressions which can be used by experts are not flexible or diversified enough. most of the existing studies assume that the linguistic expressions, with respect to a certain criterion, can take only one form of either single terms, ults or hfltss, and should be subjected to a specific syntax. this is mainly for the convenience of computational process. it is however, not a suitable or feasible choice to limit the types of linguistic expressions. for instance, some auditors are authorized to assess the degree of reliability of a big data based auditing platform (bdap) by the lts: [cit] only defined the computational model for ltwhs while the issue on how to apply them in practical qdm problems needs to be addressed. second, the experts might make use of any types of linguistic expressions for evaluations according to their individual linguistic conventions. it is essential to enable computing with all the types of cles mentioned above in a single qdm problem, or even in the process of evaluating a single criterion. therefore, the first motivation of this paper is to handle the qdm problems in which the experts can express their opinions in a more fulfilling, fruitful and flexible way. it would definitely facilitate the process of expressing evaluations without the limitation imposed by certain types of linguistic expressions."
"for the first case (rolling), the cost function is defined as the rolling distance needed to go from the current node to the next one. this distance is the arc length between two adjacent positions on the object surface and is noted l roll ."
"these simulations focus on illustrating the trajectory generation when considering sticky fingers. we consider the manipulation with a three-finger hand, where each finger can translate in the plan. this means that all the possible grasps and in-hand manipulation are represented by three maps: m 1, m 2, m 3 . two maps (d 2 and d 3 ) are used for regrasping and finger gaiting."
"in order to illustrate the impact of adhesion on the fingers' trajectories, we consider the same simulation but without any pull-off forces (f po ) between fingers and the object."
"as shown in fig. 3, the proposed solution for the m 3 qdm problem includes four procedures. the main aims and tasks are illustrated as follows:"
"as the fingers might be compliant, the fingers base displacement achieved by the actuator to detach the object from the object depends of the finger stiffness, its length, and its radius. the cost, c r, for detaching a finger from the object corresponds to the minimal distance applied by the actuator to guarantee that the finger is detached from the object."
"another precaution can be taken regarding the friction coefficient. indeed, in this paper, various friction coefficient have been used (0.3 and 0.26). in fact, we chose to take these values to be sure that we never overestimate the friction coefficient. indeed, never overestimating the friction coefficient ensures that the algorithm will never use a regrasping node that may lead to instability. we have measured a friction coefficient around 0.7 between the object (made of acrylic) and the fingers (made of steel). thus, the chosen values are acceptable."
"we will analyze the proposed m 3 qdm approach by comparing it with some similar techniques. without loss of generality, we will conduct the comparisons by using the linguistic information with respect to the criteria in c 1, i.e., the data in table 5 ."
"finger gaiting [cit] . however, it has not been largely investigated in micromanipulations, which is usually limited to simple pick and place operations for simple objects [cit] . in this paper, we show that in-hand dexterous micromanipulation is a promising way to control the rotation of microobjects for microsystems assembly."
"the potential applications of this micromanipulation approach are very large. indeed, micromanipulation techniques and more particularly dexterous micromanipulation can find applications in various fields such as scientific instrumentation and manufacturing [cit] . for example, several applications in the watch industry require to precisely orientate microparts to achieve the watch assembly. this is also useful in optics to assemble complex microoptoelectromechanical systems) [cit] or optical fibers [cit] . moreover, some other applications can be found in the nanooptoelectromechanical systems) assembly and manipulation [cit] ."
"in this paper, we propose to exploit these adhesion forces to enhance the stability and the strategies of micromanipulation. the main contribution of this paper is the development of the first trajectory planner for in-hand dexterous micromanipulation that takes into account adhesion forces. original fingers' trajectories are proposed, and the benefit of exploiting adhesion forces is demonstrated. the content of this paper has been partially mentioned in a previous conference paper [cit] . this article presents a more detailed methodology and trajectories validated experimentally considering different conditions of use."
it is enough to illustrate the process in section iv-b by the provided information with respect to one subset of criteria. the collected linguistic information with respect to c 1 is listed in table 5 .
"on the maps, but it is important to also evaluate the impact of its strength. indeed, in the previous examples, various adhesion forces have been used (0.6 μn in fig. 7 and 1.5 μn in fig. 8 )."
"in this paper, we have not studied the releasing task of the microobjects. in fact, previous works in the literature tried to solve this issue. one interesting way to perform this releasing task is to use the dynamics of the micromanipulator to overcome the adhesion forces [cit] . however, in most of the industrial microassembly tasks, the manipulated object is inserted, welded, or glued with another part [cit] . in these cases, the adhesion forces between the two parts is higher than the adhesion between the fingers and the object meaning that the releasing can be done easily."
"the procedure of deriving the solution can be found in appendix vii. based on (27), the optimal value of g(w), denoted by g * (w), can be derived. the consensus index of the group g m can be represented by:"
"moreover, the heuristic is chosen as the euclidean distance between the current position and the goal position. again, this heuristic never overestimates the distance to the goal, so it is an admissible heuristic for the considered cost function."
"the experimental setup (see fig. 15 ) consists of three cylindrical fingers of 1-mm diameter made of steel, each one mounted on a two-dof translation table. each table is actuated using two smaract slc-1730 piezoelectric actuators with a repeatability of 1.5 μm. the scene is observed using two high-resolution (2560 2018 pixels) cameras from jai."
"where f t 1 and f t 2 are tangential components of the force, f n is the normal component, and μ is the friction coefficient."
"although it is hard to compare with other techniques through a direct way, we can analyze their characteristics to illustrate the strengths and weaknesses of the proposed approach. table 7 lists some features of some similar techniques of qdm. we discuss the techniques from the following aspects."
"the trajectories were validated experimentally and in simulation considering different conditions of use (sticky and nonsticky fingers) and on various object shapes (arbitrary and polygonal shape). these results show that in-hand micromanipulation using adhesion forces can significantly enhance dexterous in-hand manipulation. thus, original trajectories can be generated and executed with a relatively low error."
"we assume that the manipulation is performed using cylindrical fingers and that the geometry of the object is known, meaning that the object's surface is represented by a discrete regular sampling. we propose to generate the finger trajectories based on a two-step method. given an object shape and the number of fingers of the manipulation system, the first step of the trajectory generation consists in computing the set of all stable grasps and admissible finger gaiting configurations. this step can be achieved offline and is done only once for a given object. the second step consists in navigating between these configurations to define a path from the initial configuration to the desired one."
"being different from hfltss which emphasize their boundaries, ltwhs start from one original term that could possibly be the real value, and modify this term by a weakened hedge. a ltwh, denoted by a 2-tuple h t, s α, can be generated by the following transformation function e g h :"
"moreover, it is well admitted by the micromanipulation and microassembly community that adhesion forces exist and are predominant in the microscale. the trend for the last two decades was to try to get rid of these forces. since our new paradigm was to take advantage of these forces, if these forces are not strong enough to be exploited, specific chemical treatments can be applied to enhance the adhesion forces [cit] . thus, the uncertainty on adhesion forces can be handled by making sure that they are the predominant forces. in addition, it is also possible to plan the trajectories considering a worst-case scenario: the rotation are planned considering that each finger is sticky and the regrasping are planned considering that only the removed finger is sticky. this will force the algorithm to only use a subset of the d maps."
"the experiment has been reproduced with the polygonal object. the experimental results are displayed in fig. 18 . again, it can be seen that the desired motion is successfully achieved, thanks to the sticky fingers. the differences between the computed trajectory and the real one are shown in fig. 19 . the rotation angle of the manipulated object reached in this case 199.25"
"in this paper, a trajectory planner for dexterous micromanipulation was presented. this planner is based on an a * graph search algorithm in order to generate optimal and complete trajectories that take into account the specificity of the microscale: the adhesion forces that predominate over gravitational and inertial forces."
these i n maps are very similar to the previous maps but are just used to select the admissible initial grasp when the substrate is sticky.
"the problem of dexterous manipulation using adhesion forces has been formalized. however, the algorithm was developed and the data were generated for planar manipulation. one possible approach to extend the proposed method to 3-d manipulation is to perform successive 2-d rotations. in this case, the fingers shape has to be modified (sphere, probes, etc.), since cylinders would not be suitable. if the contacts rolling is not negligible as it was the case in this paper, then the nonholonomic constraint has to be taken into account."
"the second constraint is related to removing a finger. in this case, a node a in m k is linked to a maximum of k elements in m k −1 corresponding to the possible removal of each of the k fingers. for instance, when changing from a three-finger grasp to a two-finger grasp, a maximum of three neighbors are possible. however, this link is established only if the finger can be removed"
"the use of linguistic hedges is complicated than that of conjunctions in cles. this is because there are two different interpretations of hedges in psychology. a hedge with inclusive interpretation expresses the degree of uncertainty of using single terms in a qualitative manner, whereas a hedge with non-inclusive interpretation modifies a term to another [cit] . the purpose of ltwhs is to model the uncertainty of using single terms. thus we focus only on the hedges with inclusive interpretation. accordingly, the cles generated by definition 1 is syntactically right if the hedges included in (4) are considered to express the uncertainty of using single terms."
"based on the traditional framework of mcdm problems, this section develops the m 3 qdm approach based on the preliminaries presented in the above section. the qdm problem is described in section iv-a and then the framework of the m 3 qdm approach is presented in section iv-b. finally, each procedure of the approach is specified in a subsection."
"the results below present trajectories for two types of objects (arbitrary shaped and polygonal) and in two different conditions of manipulation (with and without sticky fingers). note that every initial grasp is taken randomly in the set of admissible initial grasps. furthermore, different finger's bases are tested to highlight the fact that our algorithm is suitable for several planar manipulation setups."
"however, there are some limitations in the existing fuzzy aspiration-based methods. only single terms and ults are available in the methods. this would limit their applicability to complex problems in which the experts may prefer to express their opinions by various types of linguistic expressions due to their language custom and the degrees of uncertainties. moreover, multi-granular linguistic information is inevitable in complex problems because one lts may not be suitable for the entire evaluation criteria. but this has not been considered in the existing methods. all these identified limitations and omits are the issues to be addressed in the following sections."
"w c i (6) where w po is the wrench induced by the release of the n th contact. considering (6), the regrasping problem consists in finding"
"in addition, when the pull-off force between the object and the substrate is not negligible, we have to consider a third set of grasps that represents the initial grasps enabling to detach the object from the substrate (resist to the external wrench w po,sub induced by the substrate) and stably pick up the object. this can be formalized as"
"if the linguistic aspiration levels take the form of ''greater than s β−1 '', then the value function of ll is the same as that in (14) ."
"the methodology presented in the previous section has been implemented and tested to generate trajectories for planar manipulation. in fact, in microassembly, most of the objects made using microfabrication techniques are planar. the results presented in this section are thus applicable in microassembly."
"based on the drivers of the use of big data in the audit process [cit], proper platforms and infrastructures should be implemented so that the big data techniques can be adopted. especially, the new regulation released by chinese government, mentioned in section i, also delivers intensive requirement of designing bdaps for certain industries. selecting appropriate bdaps would be significant for implementing the full audit coverage in china."
