text
"in our protocol, the images are encrypted by block permutation, pixel permutation and image segmentation. fig. 7 shows the encryption of the image. the fig. 7 (a) ∼ (d) show the original images. they contain different shades of color and gray images. the fig. 7 (e) ∼ (h) show the visual effect of the encrypted image. encryption operations are based on grayscale images. because the image segmentation will make the gray value greater than 255, the encrypted image will be a white blank image, where the normalized method is used to display the image likes fig. 7 (e) ∼ (h). obviously it is impossible to guess the image content from the encrypted image."
"image features are widely used in various fields such as object detection [cit], image retrieval [cit], information hiding [cit], fingerprint detection [cit], etc. lbp (local binary pattern) features are widely used in many fields of computer vision because of the simple calculation of lbp features and good effect. in this paper, we propose a secure method to extract the lbp features from the encrypted image. the images are typically encrypted by block permutation, pixel permutation, and image segmentation. the specially-designed encryption can support direct extraction of the lbp features even from the encrypted images at the cloud. additionally, it can be made that the extracted feature is also encrypted but support similarity computation."
"as shown in fig. 6, for the ciphertext-only attack (coa) [cit] model, we consider an ideal functionality f and the corresponding information leakages of our scheme. ciphertext-only attack refers to exhaustive attack when only"
this study developed the japanese word segmentation system to investigate whether the system is feasible to improve the participants' listening comprehension and enhance their learning effectiveness and interests. the research hypotheses are listed as follows:
"the modern japanese writing system consists of several thousand kanji, adopted from chinese characters, and two phonetic scripts (kana), hiragana and katakana. japanese language learners must overcome the difficulty of appropriately segmenting japanese texts and then learn japanese nouns. therefore, the main focus of the system is to appropriately segment the japanese captions and retrieve nouns from the captions as the learning materials. this study implemented similarity measures to distinguish the nouns retrieved from japanese wordnet. mecab is a new morphological analyzer and part-of-speech (pos) tagger that segments japanese captions [cit] . other common morphological analyzers are chasen, juman, and mecab. [cit] stated that the accuracy of japanese segmentation by mecab is better than that by chasen and juman. this study used python, a programming language, to integrate japanese wordnet with mecab. figure 1 displays the framework of the japanese segmentation system."
"as a new information technology, cloud computing provides the data owners with a wealth of storage and computing resources. by outsourcing large amounts of multi-media data and complex computations such as image feature extraction operations to the cloud, data owners can reduce local data management and huge computational burden. the data owners get a huge profit, but outsourcing inevitably brings security problems, because the data owners have very little control over the data after the outsourced data. specifically, images may involve sensitive information such as personal identity, geographic location and even social relationships. therefore, uploading the unprotected multimedia data to the cloud server may cause personal privacy to be leaked."
"there are some shortcomings in the above researches. their schemes require huge storage and compute costs and they still do not address the issue of a reduction in the number of directional features resulting from modifying sift extraction steps. since the feature extraction operations include mathematical calculations such as addition, subtraction, multiplication and division, the selection of encryption algorithms is particularly important. to satisfy mathematical calculations, some schemes use homomorphic encryption, but additional comparison schemes are needed to extract extreme points. some schemes use order-preserving encryption to ensure that the ciphertext order is the same as the plaintext order to facilitate comparison of ciphertext sizes. the time complexity of homomorphic encryption is too high, and the image owner takes a lot of time to encrypt the image locally. the extracted features must be decrypted before they can be used, which increases communication costs. it is not guaranteed that the original properties of sift are not changed. the existing privacy-preserving feature extraction algorithms mainly focus on sift or similar features to sift."
"there are three entities in the proposed scheme, the image owner, the cloud server s 1 and the cloud server s 2 . the image owner needs to save the image to the remote server. to protect privacy, the owner needs to encrypt the images before uploading the images. in our scheme, we generate two encrypted image sets. both servers receive different sets of encrypted images. after receiving the encrypted images, the cloud servers perform the same calculation operations on the encrypted image sets, and then a communication between s 1 and s 2 will be carried out. finally, the lbp feature is extracted from the encrypted image by s 1 . we call our scheme as pplbp."
"in the past few years, the explosive growth of knowledge has led to a variety of ways to spread knowledge and education. the transformation of education model has induced the smart campus, which implements education by combining information and information technology to meet the various needs of students and schools. specifically, there are many new learning applications and services in smart campuses. a typical example is to feedback the student's position status in real time by continuously monitoring and analyzing various students' image information (such as cloud computing [cit] platform). in the daily life of campus, billions of digital images are generated every day. we can continuously monitor and analyze information of students by analyzing image features. however, this leads to an unaffordable storage and calculation problem. smart campus images can be stored in cloud servers and smart campus image features can also be extracted through cloud computing to alleviate storage and computation problems. nevertheless, there are many challenges that need to be solved, such as the security problem of smart campus images in cloud computing."
"the number of japanese learners is increasing and would exceed the population of other foreign language learners, except english learners in taiwan. as the number of japanese language learners increases, japanese language education gradually gains more attention in taiwan. many enterprises adopt the results of the jlpt as a basis for employment, promotion, and pay rise. therefore, japanese language teachers must understand students' learning motivation and needs to effectively enhance students' learning effectiveness and willingness to learn japanese language. those second language researchers indicated that listening comprehension is often treated as a cinderella skill in language learning [cit] ."
"outsourced image data can reveal the privacy of image owners. extracted image features may also reveal important privacy information. an attacker could guess the image contents by analyzing the image database in the cloud server. therefore, it is important to protect the privacy of the image by means of encryption. in order to protect the privacy, images should be encrypted before outsourcing. the encryption is common way to protect information, but the encrypted images (i.e., ciphertext) will hinder the operations normally performed in plaintext. this leads to the fact that the extracted image features in ciphertext domain lose the effectiveness of original features."
"hypothesis 1: providing general japanese captions are helpful to improve japanese beginners' listening comprehension. hypothesis 2: providing the segmented japanese captions can improve beginners' satisfaction. hypothesis 3: providing the segmented japanese captions and learning materials in the pre-listening activity can improve the students' learning effectiveness, memory maintenance, and overall satisfaction with regard to the system."
"finally, the encrypted images are uploaded to the cloud server. the process of image segmentation is defined in algorithm 3 and the whole process of image encryption is defined in algorithm 4."
"this description method allows researchers to capture the details of the image well. in fact, researchers can use it to get the most advanced level in texture classification. because lbp features have the ability to depict local texture features of images, they are widely used in the fields of image retrieval [cit] and face recognition [cit] ."
"in the present study, a number of privacy protection outsourcing solutions have been proposed. these works mainly focus on the calculation of digital data or text data which can handle a variety of mathematical problems, including modular exponents [cit], sequence comparisons [cit], linear equations [cit] and knn searches [cit] . on the other hand, in recent years, some existing works focus on extracting image features from encrypted images such as scalar invariant feature transform (sift) [cit] and histogram of oriented gradient (hog) [cit] . they use paillier cryptographic system [cit] or somewhat homomorphic encryption [cit] to protect the privacy of images without affecting the extraction of image features. the application of privacy-preserving data in the ciphertext domain has been extended to the areas of multimedia content retrieval [cit] and face recognition [cit] ."
"the proposed scheme includes three algorithms which are respectively executed by three entities, i.e., genkey and encimg algorithms executed by the image owner, lbp feature extraction executed by cloud servers. then, other algorithms like image retrieval or face recognition are performed by using lbp features."
"in order to protect the image owner's image privacy, the image is encrypted before outsourced to the cloud sever, as described in follows. the image consists of two types of information, color and texture information, which require appropriate protection [cit] . in our scheme, the color information is protected by image segmentation, and the texture information is protected by shuffling the pixel position."
"the feature extraction operation requires interaction between s 1 and s 2 to complete the comparison operation. one of the most famous protocols for comparing private data of two parties is yao's millionaires' problem [cit] . inspired by their ideas, we also use secure multiparty computation to build our comparative agreement."
". except the image encryption, the image owner would like to outsource the computation and storage tasks to the cloud server as many as possible. moreover, the data owner sends the application request to the cloud server."
"for the feature extraction system that protects users' privacy, this paper takes into account the scenario shown in fig. 1. figure 1 . system models."
"in the activity i, the participants were asked to watch the second part of the first video, from up on poppy hill, with general japanese captions and a combination of kanji and kana scripts. the purpose of the activity was to investigate whether the japanese texts and sounds would lead to dual cognitive load, which could be measured using the questionnaires and observations. in the activity ii, the participants were asked to watch the first part of the second video, spirited away, with segmented hiragana captions the aim of which was to reduce the participants' cognitive load, allowing them to stay focused on listening. in the activity iii, the participants were asked to learn the retrieved nouns, which were classified into highly or moderately similar ones according to the corpus based on word similarity in the japanese wordnet as well as other related nouns provided by the instructor as the learning materials in the pre-listening activity. activity iii preceded watching the second part of the second video, spirited away. therefore, the participants could understand the video content better because of the pre-listening activity. figure 3 shows the differences between two segmented japanese captions of the video, spirited away. figure 3 (right) shows the general japanese captions, a combination of kanji and kana scripts, without segmentation. figure 3 (left) shows the japanese hiragana captions, a string of morphemes annotated with parts of speech (poss) using morphological analysis. the sentence in the snapshot of figure 3 (left), まえのほ うがいいもん, means that i prefer the previous one. after segmentation by mecab, the sentence becomes まえのほうがいいもん, which means better before. noteworthy, the words within parentheses in the beginning of the sentence, (千尋), indicate the female speaking character. thus, this study removed the speaking character indicators from the original japanese captions before using mecab to segment the japanese captions. moreover, the kanji characters (e.g., 前 and 方) are difficult for novice to pronounce. consequently, all kanji characters were transformed into hiragana in the segmented japanese captions."
"our goal is to protect the privacy of image content while enabling the cloud server to execute the lbp algorithm on it. specifically, we treat the image content's information (pixel values and descriptive features extracted from images) as the data owner's personal information."
"in our security model, we consider the cloud server to be ''honest-but-curious'' and ''independent''. that is, the cloud server correctly implements the security lbp algorithm. however, the cloud server tries to learn additional information from encrypted data and all of operations performed by it. in our scheme, the data owner uploads the encrypted images to the cloud server, which performs all operations on the encrypted images. however, the cloud server has no other information than the encrypted lbp features, and the encrypted lbp features do not reveal any information about the image. therefore, the privacy of image content can be preserved from the cloud server. similar to a secure multiparty computing scenario, we assume that cloud entities are ''independent'' of each other. here, s 1 and s 2 would explicitly state non-collusion."
"(1) as far as we know, our proposed secure lbp extraction scheme is the first privacy-protected lbp extraction scheme in the ciphertext domain. this algorithm can be used for many lbp-based applications while protecting privacy."
"this study adopted the nielsen's attributes of usability (nau) and the after scenario questionnaire (asq) as tools to evaluate the usability of the learning abilities. the results of the questionnaires revealed that the learners have positive feedback on learnability, memorability, efficiency, accuracy, subjective satisfaction of the integration of the segmented captions in the experimental activities, which facilitate the participants to learn japanese. the results indicated that the videos with the segmented captions can effectively increase the participants' learning motivation and that most learners are willing to adopt the segmented captions of the system to learn japanese, which is worthy of further promotion and use."
"overall, 24 participants were asked to watch the video spirited away for 58 minutes in the experimental activity iii. the pre-listening activity required the participants to learn japanese vocabulary retrieved from the captions, spirited away, and classify them into highly and moderately similar nouns according to the word similarity in the japanese wordnet. in addition, the instructor also provided related vocabulary to help participants recognize the meanings of the retrieved japanese nouns and comprehend the video content to prevent the participants from message interference and distraction. therefore, the participants can stay focused on the listening comprehension activity. table 2 lists the generated nouns, which served as the learning materials of the pre-listening activity before watching the video. table 3 shows the mean value of4.83 on learnability and efficiency, with standard deviation between 0.246 and 0.253, indicating that the participants had a better understanding of the video content in this experimental activity compared to the experimental activity ii. the participants stated that the pre-listening activity could help them memorize japanese vocabulary. thus, they had sufficient time to accomplish the activity. [cit] three-phase comprehension model, indicating that the participants can relate the mental representation of the vocabulary to their prior knowledge. conclusion the study developed a japanese word segmentation system. this study aimed to investigate whether the segmented japanese captions effectively improve learners' listening comprehension. furthermore, it aimed to understand whether the supplementary learning materials of the pre-listening activity, which contain the retrieved nouns from the japanese captions based on the wordnet's ontological structure, help learners comprehend the learning content. the experiment lasted for two months. the participants completed three experimental activities, which involved watching two japanese videos, one with general japanese captions and other with segmented japanese captions. concerning activity with segmented captions, they participated in the pre-listening activity before watching a video every other week. the study adopted the nielsen's attributes of usability (nau) and the after scenario questionnaire (asq) as tools to evaluate the usability of learning activities. the results revealed that learners provided positive feedback on learnability, memorability, accuracy, and subjective satisfaction with the integration of the segmented captions in learning activities. the experimental results showed that the videos with the segmented captions can increase the participants' learning motivation, and most learners were willing to adopt the japanese word segmentation system to learn japanese. thus, it would be worth to further promote and use this system."
"the participants were required to watch 90-minutevideo, from up on poppy hill, with general japanese captions. the descriptive statistics showed that the mean value on learnability, efficiency, and accuracy of the system is 4.53 and the standard deviation is between 0.208 and 0.213, indicating that the learners can accurately comprehend the video content. thus, the experimental activity i can enhance learning effectiveness. table 1 shows that the mean value is above 4 on learnability items, sufficient time to complete tasks, and ease of completing tasks, indicating that the learners agree that the learning activity is helpful to memorize the video content. moreover, the length of the learning activity is sufficient to complete tasks without causing the cognitive burden on learning, allowing the participants to complete tasks easily. the mean value of the overall subjective satisfaction reached 5.30, showing that the learners are satisfied with the learning activity. however, with regard to the learning context, additional supplementary information may be included to assist the learners to complete tasks."
"in our scheme, the lbp histograms are calculated from encrypted grayscale values of images. due to the pixel permutation in blocks, the extracted lbp histograms are also the permutated ones. in this case, the cloud server cannot extract valid lbp features without the secret permutation keys. otherwise, the server can guess the content of encrypted images by searching the database with the lbp features generated from the specially selected images. with a simulated image i s, s can simulate lbp histograms of the simulated images. the computational complexity of a distinguisher d in distinguishing the histogram is 256! which means a 1684 bits security strength."
"smart campus can enjoy rich cloud computing resources by outsourcing image features extraction to cloud computing platforms. however, there are few solutions for outsourcing secure image features extraction. in this paper, we propose a secure out-sourcing lbp scheme. the proposed scheme uses block permutation, pixel permutation and image segmentation to protect the privacy of outsourced images. the secure lpb features can be extracted by the server from these encrypted images. the secure lbp features can be used directly for many applications and retain the most characteristics of the original lbp features. we analyzed and evaluated the safety of the proposed scheme. we further conducted a large number of experiments on the proposed scheme. the experimental results show that the secure lbp in this scheme can be used in image retrieval and face recognition. in the future, we plan to research more secure extraction of image features in the field of encryption."
"the fig. 8 (a) ∼ (d) show the original lbp. we found that the original lbp will leak a lot of image texture information. the fig. 8 (e) ∼ (h) show the lbp after pixel permutation and image segmentation in the encryption process. obviously it is possible to guess the image texture without block permutation. in particular, the texture information of the face image is more pronounced likes the fig. 8 (e) and (g). the fig. 8 (i) ∼ (l) show the pplbp which will not reveal the image texture information. so we use block permutation to 30398 volume 6, 2018 protect the texture information of the image. since the size of the image after encryption is only one-ninth of the original size, we have magnified the size of fig. 8 (e) ∼ (l) for more intuitive."
"the individual interviews were conducted with eight learners, which is a small sample size. in experimental activity ii, the participants were asked to watch the first part of the japanese video, spirited away, for 68 minutes, with hiragana captions containing no kanji character in the captions. the results of the interviews indicated that the interviewees could comprehend the video content of the from up on poppy hill mostly in the experimental activity i; however, messages sent through multiple channels as well as complex structures, codings, and visual messages of the system distracting the participants' attention from comprehending the video content. in addition, the interviewees also stated that the hiragana captions can assist them with pronunciation of the japanese captions but are not helpful to comprehend the video content. japanese hiragana is a phonetic script that does not have any meaning; however, each japanese kanji is an ideographic character that has its own meaning. since chinese characters are similar to kanji ones, taiwanese students who learn japanese as a foreign language have a better understanding of reading japanese captions but neglect the pronunciation of those chinese characters in captions while watching japanese videos."
"then, the encrypted lbp feature is extracted from the encrypted image in the same way as in plaintext image. the only difference is that, in plaintext domain, the features are calculated from overlapping blocks, but in the encryption domain, the features are calculated from non-overlapping blocks. the whole process of feature extraction is defined in algorithm 5. the feature extraction process is illustrated in fig. 5 ."
"the image content in our scheme is protected by three times of block permutation, pixel replacement and image segmentation. our security proofs follow the paradigm in secure multi-party computations [cit] ."
"cloud server stores the encrypted image for the data owner and provides the lbp feature extraction service for the data owner. in our system, the expected image processing results are a set of lbp features and lbp-based application results. the cloud consists of two entities, which are independent cloud server providers. the functions of these entities are as follows:"
"server 1 and server 2 receive the encrypted images. they receive different encrypted images respectively. then they calculate the received encrypted images respectively. server 2 sends the result to server 1. the lbp feature is finally extracted by server 1. in the following article, server 1 and server 2 are respectively written as s 1 and s 2 ."
"for s 2, it only has the encrypted image set c 2 . the encrypted image set c 2 does not contain any image information."
"before designing and developing the japanese word segmentation system, the researchers considered issues of how to (1) retrieve the japanese captions, (2) correctly segment japanese captions, (3) select important japanese vocabulary for language beginners to improve their listening comprehension, and (4) automatically yield learning materials of the pre-listening activity from the system. japanese captions could be retrieved from japanese captions websites. in addition, using google2srt can also download xml captions (closed captions) from youtube videos and covert them to srt format recognized by most video players. this study adopted mecab to segment japanese texts into separate morphemes by adding spaces and used wordnet to retrieve nouns from the japanese captions as learning materials of the pre-listening activity. the nielsen's attributes of usability (nau) and the after scenario questionnaire (asq) were adopted as tools to evaluate the usability of the learning abilities. the nau uses 5 items to evaluate the attributes of the system regarding learnability, efficiency, memorability, accuracy, and subjective satisfaction [cit] .the asq uses 3 items to evaluate the overall satisfaction of the learning activities [cit] . after completing the three experimental activities, the participants were asked to fill out both questionnaires. the learners responded to items on a 7-point likert-scale, with a score of \"1\" indicating strongly disagree and a score of \"7\" indicating strongly agree."
"the simulator s knows the number of images and the size of images. however, s can only fill the images with the random generated pixels. in order to analyze the security of the image content, we observe the data of each cloud entity. for s 1, it only has the encrypted image set c 1, and there are also differences c 1i − c 1j − c 2i − c 2j between the center point within the block and the non-center point within the block after block permutation and pixel permutation have been performed. as long as the client generates a new key each time to encrypt the image, the image segmentation mechanism provides security for the image content. the strength of the ciphertext security depends on the security of the pseudorandom generator function used. c 1i − c 1j − c 2i − c 2j is already encrypted by image texture protection. its security strength is 256"
"in the application, lbp histograms are used. as shown in fig. 9 is lbp histograms of fig. 8 (a) with different encryption schemes. we normalized the histograms to facilitate analysis of the results. fig. 9 (a) shows the histogram of the original lbp. fig. 9 (b) shows the histogram of the lbp without pixel permutation. we can find out by comparing the histograms that there are some differences between the two, but they are very similar. fig. 9 (c) shows the histogram of pplbp. obviously, the histogram of pplbp differs greatly from the previous two. so we think pixel permutation can protect lbp values and histograms very well."
"this section evaluates the performance of the proposed scheme in terms of encryption effectiveness, retrieval accuracy and face recognition rate. [cit] on a linux operation system with intel(r) core(tm) i7-6800k cpu @ 3.40ghz and 16 g memory."
"after receiving the encrypted images, the cloud servers divide the images as the image owner does. s 1 calculates the pixel difference between the center of the block and the pixels around it as (c 1i − c 1j ). s 2 calculates the pixel difference between the center of the block and the pixels around it as (c 2i − c 2j ), then sends the differences to s 1 . after obtaining the above information, s 1 subtracts the received differences from its own differences. we will explain the principles of mathematics as follow:"
"in our model, the user only needs to prepare a copy of the encrypted image as an input and then send it to the server for other operations. moreover, the server generates the lbp through a secure multiparty computation framework without having to know or learn anything to compromise the privacy of the user."
"this study retrieved nouns from the japanese captions of the two videos. however, more than 700 frequently used nouns were retrieved from the japanese captions of each 30-minute video; those retrieved nouns need to be further classified and organized as the learning materials for the pre-listening activity. this study did not filter nouns based on the high frequency words because names, locations, objects, and issues are not the focus of the videos, and they appear repetitively in the captions. this study implemented similarity measures to distinguish the retrieved nouns in japanese wordnet and obtain a similar effect of concept clustering. many similarity measures exist in japanese wordnet, including (1) wu-palmer similarity, (2) resnik similarity, (3) path distance similarity, (4) lin similarity, (5) leacock chodorow similarity, and (6) jiang-conrath similarity. this study adopted the wu-palmer similarity measure to sort the retrieved nouns into groups of high similarity (0.7-0.9), moderate similarity (0.6-0.7), and additional important nouns provided by an instructor as the learning materials for the pre-listening activity. figure 2 displays the experimental procedures. [cit] stated that visual images effectively facilitate learners to recall oral messages and store them in long-term memory. learners can easily remember what they hear and read of target-language information along with sounds and visual images. two japanese animation movies were selected as supplementary learning materials for listening comprehension with an aim to enhance students' japanese learning experience during the entire semester. this study recruited 30 participants from the department of tourism at a university. they were asked to watch two videos divided into four parts. the participants were asked to watch the first part of the first video with the chinese subtitles, which can help them comprehend the video content. however, the instructor found that most participants focused only on the chinese subtitles and ignored the japanese pronunciation from the class discussion. thus, this study conducted the following three experimental activities and investigated whether the participants could combine what they hear with what they see when watching the videos to enhance their japanese listening comprehension."
"(2) we encrypt images by block permutation, pixel permutation, and image segmentation. these three steps can well protect the image content without affecting the direct extraction of lbp features. in this way, our secure lbp feature can be extracted on multiple cloud servers without requiring additional communication between the image owner and the cloud server. (3) other features extracted by the privacy preserving image feature extraction scheme are encrypted and can only be used after the feature has been decrypted by users. in contrast, lbp features extracted in our scheme can be used directly without decryption and the application effect on image retrieval is not bad. the rest of this paper is organized as follows: in section ii, we elaborate on the re-search status of the privacy-preserving of image feature extraction. in the next section, we introduce system model, security model and some preliminaries. in section iv, we formally present the scheme design. in section v, we describe analysis privacy, of the proposed scheme. in section vi, we describe evaluation of the correctness, of the proposed scheme. finally, conclusions and future work are given in section vii."
"in addition, the cloud server in our solution also provides lbp application services. in order to achieve the purpose of protecting privacy, the cloud server does not have to understand the results from the request sent by the data owner. the cloud returns extracted image features or feature-based operational results to the data owner. in other words, the cloud server is powerful in completing the requested task, but does not compromise data privacy"
", to be outsourced for cost saving and efficient utilization. for privacy preserving, the image database needs to be encrypted before being uploaded, generating two encrypted image sets"
"where (x c, y c ) denotes the position of the center pixel, i c and i p denote the brightness of the adjacent pixels. s(·) denotes a symbolic function:"
"input: c 1, c 2 and size of blocks 1: at c 1, use size of blocks to block the images. 2: for ∀block do 3: c 1i − c 1j, c 1i is the center of the block, c 1j is an inner non center point 4: end for 5: at c 2, use size of blocks to block the images. 6: for ∀block do 7: c 2i − c 2j, c 2i is the center of the block, c 2j is an inner non center point 8: end for 9: s 2 sends all differences to s 1 10: for ∀block do 11: c 1i − c 1j − c 2i − c 2j 12: end for encrypted text can be accessed by adversaries. the execution of our scheme involves the interaction between cloud servers and users, which is defined as the real experiment. the honest-but-curious and independent cloud servers are defined as the adversary a. the simulator s is defined to simulate the view of the adversary a by using the functionality f only."
". the image owner sends the encrypted image database c 1 and c 2 to s 1 and s 2 . besides, the set of secret keys k is reserved by the image owner. a summarization of the algorithms is presented in fig. 3 . in the following sections, we present a detailed introduction of our pplbp protocol. volume 6, 2018"
"watching videos with captions apparently assists individuals in understanding the video content and improves their listening comprehension [cit] . therefore, this study aimed to investigate the use of different captions and attempted to provide the supplementary learning materials for the pre-listening activity to further enhance japanese language learners' listening comprehension. two japanese videos, from up on poppy hill and spirited away, were selected as video watching activities every other week. the experiment lasted for two months."
"inputs: inputs for the labour and birth caremap were: (a) clinical practice guidelines for care of women with diabetes in pregnancy, and; (b) input and consensus from midwives and diabetologists."
"the development process of a contemporary caremap is a subject that has gained significantly less attention in the literature. only 19 out of the 115 papers provides any detail regarding the development process. of these only 6 describe the development process with any deliberate nature or clarity [cit] . from the rest, the steps to develop the caremap can only be inferred [cit] ."
"to resolve this problem an entity relationship model, shown in figure 2 that describes the relationship among structural elements of a caremap, demonstrated in table 2, is proposed. the elements are inspired by the standardised pictorial elements seen in uml and hard state chart notations. following this, the standardised structural model of the caremap is demonstrated in the content model shown in figure 3 ."
"as caremaps evolved into graphical representations we begin to find contemporary caremaps presented as a separate but complementary component to the clinical pathway or clinical practice guideline [cit] . more recent caremaps are linked to or provide a graphical flow representation for a clinical practice guideline (cpg) or surgical event [cit] . while retaining the purpose and flow, many of those seen today annexed to cpgs have even dropped the title [cit] . a summary of the relevant elements of each caremap type is included in table 1 ."
"some see standardising of care as limiting their ability to make decisions based on the patient presenting before them, creating 'cookie-cutter medicine'. however, caremaps are a form of standardised clinical documentation that improve patient safety and outcomes while still allowing clinicians to select the most appropriate path for their patient. caremaps evolved during the last three decades from primarily text-based approaches developed by nurses, to flow-based visual aids prepared by doctors as representations of clinical screening, diagnosis and treatment processes. these contemporary caremaps are presented in a variety of ways and with differing levels of content. contemporary caremaps lack standardisation. this paper presents one solution for standardising caremap structure and content, and an approach for caremap development distilled directly from analysis of the entire pool of literature. the development process was evaluated and refined during the development of caremaps for case studies in obstetrics and midwifery: (a) labour and birth, and (b) management of patients with gdm. the resulting caremaps were validated by expert consensus, with the labour and birth caremap also being developed as a state transition machine enabling rapid digital validation against a dataset of synthetic patients."
"connection between a series of linked caremaps figure 4 presents the proposed development process divided into six phases. during the initial phase the conceptual framework should be decided, and a multidisciplinary team assembled. the next phase clarifies current practice and anticipated variance. a review of available evidence is the final step prior to production of the caremap. once developed, it should be evaluated and once agreed, implemented. as figure 4 shows, caremap development is a lifecycle process. as new knowledge for the particular condition or treatment or variance is identified, the caremap should be reviewed [cit] ."
"a single article was located that attempts to describe a systematic process for contemporary caremap development [cit] . authored by a veterinarian and a lawyer, this article focuses more on standardising care process representation into a clinical caremap for the purposes of cost containment and provides the example of mapping a surgical procedure [cit] . given their focus and particular caremap construction which, through their own exemplar application only includes a temporally-ordered single-path representation of the gross steps of patient care, their paper might only be considered formative at best. by their own admission they deliberately limited the relevant data analysed during the input design phase to only what is truly critical for identifying and understanding outliers, which results in its lack of clinical applicability and distinct lack of detail surrounding each care process [cit] . their method requires significant work to adequately support true standardised clinical caremap development."
"each activity in the caremap represents a specific medical process. diagnosis, treatment and ongoing monitoring/evaluation are three medical activities that are consistently observed [cit] . it is common for a caremap to contain a set of targeted outcomes [cit] . time, described either as a duration or inferred from the step-by-step nature of the dynamic care process, is often part of the caremap [cit] . finally, an explanation associated with the activities and/or arrows captured in the caremap may be present [cit], [cit] . the explanation helps to better describe an activity or to justify the flow from one activity to another."
"the rest of this paper is organised as follows: section 2 discusses caremap terminology, history and evolution. section 3 defines the problem of standardisation and section 4 reviews related literature. section 5 presents the methodology and results of a literature review on the primary elements of caremaps. the proposed standardised caremap model is described in sections 6 and 7 and validated in section 8 through the conduct of two case studies in the area of midwifery and obstetrics. the paper is then summarised and concludes with proposals for future work."
"caremap is a term currently used to describe a graphical representation of the sequence of patient care activities to be performed for a specific medical condition. caremaps have existed in some form for around forty years [cit] . the literature suggests they originated in the nursing domain, incorporating and extending the critical pathway method and bringing established project management methodologies into healthcare delivery [cit] . caremaps are intended to standardise health services by organising and sequencing care delivery, ensuring a standard of care and timely outcomes using an appropriate level of resources [cit] . the caremap can also help track variance in clinical practice, as it provides a simple and effective visual method for identifying when treatment or patient outcomes have deviated from the routine evidencebased pathway [cit] ."
"more recent caremaps have tended towards representation as a flow diagram made up of clinical options for a particular condition and resulting in multiple possible paths based on: (i) symptomatology; (ii) diagnostic results, and; (iii) how the patient responds to treatment [cit] . caremap examples can be found in many healthcare domains, including: paediatric surgery [cit], nursing [cit], oncology [cit], diagnostic imaging [cit], obstetrics [cit] and cardiology [cit] . even within these examples there exists significant variance in complexity level, design approach, content and the representational structures used. there is currently no standardised method for the development or presentation of a clinical caremap [cit] . changes in format between like documents and poorly designed materials increase ambiguity and create confusion for the clinician [cit] . standardised approaches to documentation ensure that each time a clinician approaches that type of document, the content and format meet their expectations, can be read quicker, are better retained, and improves patient safety and outcomes [cit] . for this reason our paper asks: how can caremaps be an effective tool to standardise healthcare when caremaps themselves are not standardised?"
was used wherein the information scientist created an initial version of the caremap based on the clinical practice guideline (cpg) and evidence derived from the treatment statistics. the initial caremap was revised and refined during a number of sessions with the clinicians. the resulting labour and birth caremap for middlemore hospital is shown in figure 5 . [cit] ) . the treatment paths for each woman were digitally compared against the caremap in figure 5 to ensure a valid path solution resolved for every recorded birth.
(1) [cit] . ccm's caremaps were similar in form and function to existing clinical pathways and were applied to specific patient populations that were commonly treated in high numbers in hospitals [cit] . this organisation then went on to trademark the double-capitalised version caremap but had not within the first decade undertaken any research to demonstrate effectiveness of the concept whose invention they claimed [cit] . (2) that caremaps naturally evolved as an expansion of earlier case management and care plans [cit] . (3) [cit] 's at the new england medical centre (nemc) [cit] . [cit] .
"as part of a project to design and build a populationto-patient predictive learning health system (lhs) to reduce clinical overuse and empower patients to actively participate in their own care, queen mary university of london's pambayesian project (www.pambayesian.org) is creating a bayesian network (bn) model [cit] to predict treatment needs for individual mothers with gestational diabetes mellitus (gdm). the process initially required creation of three caremaps, for (1) diagnosis;"
"analysis: initially each paper was reviewed using standard content and thematic analysis [cit] and concept analysis [cit] ) to identify and classify terminology, construction and content elements and infer development processes."
validation was performed through consultation seeking consensus from three diabetologists with tertiary care experience treating obstetric patients under the cpgs used in the caremaps' creation.
"future work should address a standard approach for identifying and representing the decision points within a caremap, digital imputation of the caremap, and representation of caremap logic in other computer-aware and algorithmic forms, including bayesian networks or influence diagrams [cit] . these can form part of a learning health system and provide population-to-patient level prediction."
"methodology for evaluation of proposed standard for caremaps: case studies are a grounded comparative research methodology with a welldeveloped history, robust qualitative procedures and process validation [cit] . the case study approach provides a real-life perspective on observed interactions and is regularly used in information sciences [cit] . case studies are considered as developed and tested as any other scientific method and are a valid method where more rigid approaches to experimental research cannot or do not apply [cit] . both the standardised development process and resulting caremap are evaluated using case studies of examples from the author's other works."
"early caremaps were text-based and holistic. rather than focusing on just the immediate primary diagnosis or intervention, nurses developed them to focus on the entire scope of care that might be necessary for the patient during their hospitalisation event. these traditional caremaps considered elements such as anxiety, rehabilitation, education, prevention and coping strategies and were intended to restore the patient to a normal quality of life [cit] ."
"pambayesian: patient managed decision-support using bayes networks. kd acknowledges funding and sponsorship for his research sabbatical at qmul from the school of fundamental sciences, massey university."
"the literature was used to establish consensus on common structure, content and development processes previously used in the creation of caremaps, and which may be relevant in defining standard caremap and development processes. the case studies are used to evaluate and refine each. the research was conducted following the overall approach presented in figure 1 . to address the stated aim of this paper, we focused our research on tertiary care (hospital-borne) caremaps and specifically the following three components whose characteristics came out of the thematic analysis and make up the review framework: structure what is the representational structure and notation for expressing contemporary caremaps?"
"the three main content types that were consistently captured in the contemporary caremaps were diagnosis, treatment and management/monitoring. these are broad content types related to a set of specific medical activities and data captured as shown in table 3 . following the structural model, an exemplar content model is presented in figure 3 . the three main content types represent different caremap levels, while the described medical activities are the components of that type of caremap. the proposed standard content model represents the information that should be captured in a caremap."
"proponents see standardisation of care processes as an effective method for reducing healthcare service delivery costs and variation, while increasing quality, safety, efficacy and outcomes, improving the patient experience and overall quality of life [cit] . yet we see that healthcare remains one of the slowest industries to adopt process standardisation or to demonstrate it has positive impacts on patient safety and outcomes [cit] . this in part is due to clinician resistance; with attempts at care standardisation derided as 'cookbook' or 'cookie cutter medicine' that some say can only be effective after they have set aside the unique needs of individual patients [cit] . given the current overuse issues and financial crisis pervading healthcare service delivery globally, standardisation of key documentation can help clinicians deliver managed care, which is seen to reduce incidences of inappropriate and ineffective care, resource consumption and overall cost [cit] . caremaps, clinical and critical pathways, clinical flow diagrams and nursing care plans are observed with vastly different content and appearance within the same journal, from hospital to hospital, and sometimes even from ward to ward in the same hospital. while much literature presents caremaps and other clinical documents such as clinical pathways, and texts exist for the development of traditional text-based caremaps, a gap exists with regards to presenting a standard for the development and structure of contemporary caremaps. this research seeks to differentiate contemporary caremaps from other forms of clinical documentation, and to present one possible solution to standardising their development, structure and content."
"contemporary caremaps are presented as flow diagrams. however, as described in section 6.1 there is neither a consistent caremap structure nor a good representation of the elements included in a caremap."
"the labour and birth process represents an excellent example for a first-pass evaluation case study to assess the development process for caremaps. labour and birth has easily defined start and end points, limited temporal variance, and a finite number of easily identified treatment paths."
"as described in table 1, caremaps have evolved from wordy texts [cit] to illustrative graphs [cit] . most contemporary caremaps present either as monochromatic, i.e. black and white [cit] or full colour [cit] flow diagrams: a well-known process modelling tool [cit] . generally, each flow diagram has its own boxes and notations, and the most common is a rectangle that represents a process step, usually called an activity. contemporary caremaps contain a set of activities representing medical care processes. however, the literature shows there is no consistency in the way that an activity is represented. different shapes such as rectangular boxes with rounded [cit] or square corners [cit], plain text [cit], or even arrows [cit] have been used. in some cases, activities that lead to different mutually exclusive pathways are presented by a diamond [cit] . the flow from one activity to another is illustrated with arrows [cit], or simple lines [cit] . the literature lacks a clear description as to whether a caremap should have an entry and an exit point. in some cases neither is present [cit], while in others these points are an implicit [cit] or explicit part of the diagram [cit] . finally, most of the reviewed caremaps contain multiple pathways and they are often presented as multi-level flow charts [cit] ."
"if used consistently, the methods presented in this paper will bring standardisation to caremaps and ensure that, as clinical staff move between busy units in a tertiary care setting, they are not distracted from the patient in effort to understand the care flow model. every caremap would be familiar and time can be given over to treating their patient, not trying to understand the document."
"literature review: a search using the terms 'caremap', 'caremap', and 'care map' was conducted across a range of databases. a citation search was also performed on all included papers. this search yielded 1,747 papers. once duplicates, papers not based in the nursing, medical or healthcare domains and those using the term \"care map\" in other contexts were removed a core pool of 115 papers remained."
"where the intersection occurs is: (a) between the first two descriptions and in the way that staff of ccm have sought to elevate differences between clinical pathways and their model of caremaps; identifying that the former represented a firstgeneration concept while the latter improves on it by adding consideration of variance and outcome measurement [cit], and; (b) [cit] 's."
"(a) clinical practice guidelines for intrapartum care at middlemore hospital; (b) input and consensus of midwives and obstetricians, and; (c) publicly available incidence and treatment statistics from the nz ministry of health."
development: an iterative development process was used wherein the decision scientist and midwifery fellow worked together to deliver an initial version of the caremap based on the clinical practice guidelines (cpg) and clinical experience. the initial caremap was revised and refined during a number of sessions with the clinicians. figure 6 presents the resulting clinical management caremap for gdm.
"in summary, the presented methodology is capable of creating an accurate price signal to reflect characteristics of all the participants, facilitating the optimal operation of the mgc."
"internet of things (iot), is a cloud of interconnected physical devices, which can communicate with each other over the internet. physical devices such as microcontrollers, microprocessors, actuators, and sensors will not directly communicate with the internet; they do so by using an iot gateway. this entire infrastructure is known as iot infrastructure. for example we can take a home lighting system, where all the switches are been connected to the main controller which is connected to the internet. the smart farm, embedded with iot systems, could be called a connected farm, which can support a wide range of devices from diverse agricultural device manufacturers. also, connected farms could provide more intelligent agriculture services based on shared expert knowledge."
"the compiler is implemented as a multi-pass compiler in java. the compilation process consists of four distinct phases: parsing, type checking, code generation, and code allocation. all phases access the abstract syntax tree (ast) of the program and a shared symbol table. the implementations of parsing and type checking largely follow established approaches for compiler design. the code generation phase only differs from typical compilers in that it does not directly generate machine code. in the final code allocation phase, the compiler maps the compiled classes and interfaces to the available node types. a wsn may contain nodes with different capabilities that serve different purposes in the network. not all of these nodes require the full functionality of the mpl program and part of the program is only ever executed on the nodes of a specific type. the code allocation phase allows one to remove unnecessary classes from the final code images and thus to reduce memory demands. in contrast to possible local code optimizations by the downstream c compiler, the allocation algorithm of the mpl compiler can take the entire control-and data-flow of the complete application, including remote invocations of abstractions, into account."
"the action class is further divided into two subclasses of actions. local actions are executed locally on a single node to implement basic operations, e.g., reading a sensor. in the makesense framework, local actions also define the interface to the node hardware. hardware operations, like reading sensor values or storing data on flash, are exposed to the user as local actions. in addition to predefined local actions, the user may also define custom local actions within the framework."
"based on these requirements and the makesense framework, we designed mpl, a high-level macroprogramming language for wsns. as determined by requirements iv and v, a goal for this programming language is to provide an expressive programming environment that is familiar to a large set of programmers. this made java [cit] a natural choice as basis for the design of mpl as it is well-established and widely used language with appropriate features. building upon java's object-oriented model also provides a stepping stone to implement the makesense meta-abstraction hierarchy in accordance with requirement i. each meta-abstraction maps to an interface and abstractions can be added by implementing a meta-abstraction interface. the language is purely objectoriented with the exception of a limited set of primitive data types, including integers, chars, and booleans."
"7.1.1 hardware. our sensor nodes are equipped with the low-power msp430 microcontroller from texas instruments (48kb of flash memory and 10kb of ram) and a radio transceiver with a maximum data rate of 100kbps. the selection of inertial sensors is driven by cost concerns, accuracy, form factor, power consumption, and interfacing capabilities. as accelerometer, we choose the lis3lv02dq three-axial sensor [cit] ]. the price is around $15 and the typical power consumption is 2mw. the list of features include user-selectable full scale of ±2g and ±6g, i 2 c/spi digital interface, programmable threshold for wake-up/free-fall, and various sample rates up to 2.56khz. as compass, we choose the three-axial micromag 3 magnetometer sensor [cit] . the price range is $60 and the typical power consumption is 1.2mw. the micromag3 uses the magneto-inductive (mi) sensing technique [cit] ] and provides a relatively large field measurement range (±11 gauss) on the digital spi interface. the compass sensor and the accelerometer are connected to the node using a shared spi bus."
"as shown in the fig. 6, if the value of optimized p i spot solved by equation (18) locates between pex and pim, the operation of inner ders is settled and the energy is self-balanced in the i-th micro-grid. in the case that p i spot is lower than pex, dgs prefer to sell electricity to the utility grid at price pex. similarly, fls is more willing to purchase electricity from the utility grid when p i spot is higher than pim."
"for each signal, each beat is classified into one of the 12 states, giving a sequence of states, labeled a through l, an example of which is given in figure 1 . a first order transition probability matrix is constructed for the sequence obtained, and these transition probabilities along with marginal probabilities are used as features making a total of 156 features, of which 40 were selected based on feature importance analysis using xgboost [cit] and some manually curated methods."
"having the accelerometer sampled ten times faster than the compass can generate problems, as both sensors share the same spi bus of the msp430 microcontroller. fortunately, the compass is able to complete its measurements in the background, meanwhile releasing the spi bus for accelerometer sampling. the only complication is that the compass requires explicit read commands for each of the three axes. as a consequence, the sampling tasks of the two sensors have to be interleaved. the detailed scheduling strategy is explained in section 7.5. 7.1.3 preliminary sensor evaluation. as a first step, we conduct a preliminary evaluation of the sensors, in order to study their accuracy and robustness to external influences. we present in figure 6 several representative tests for each sensor. figure 6(a) shows the accelerometer output when released freely to swing as a pendulum (for clarity only the z axis output is plotted). the graphic follows closely the actual movement and the noise level is low. in contrast, the second experiment presented figure 6 (b) exhibits considerable noise during movement. in this case, the accelerometer is placed on a toy car that accelerates forward and then backward, as suggested by the low-pass filtered signal plotted with dotted line. during stationary periods, though, the sensor output remains stable, with only single bit fluctuations. a similar experiment is performed for figure 6 (c). the result of the velocity integration from eq. (2) is presented at different sampling frequencies supported by the accelerometer. the lower 40 and 80 hz frequencies exhibit large velocity fluctuations, whereas 160 and 320 hz provide a velocity progression that correlates well with the experiment. because the improvement from 160 to 320 hz is not significant and scheduling becomes more problematic at higher frequencies, we choose the 160 hz sample frequency for the accelerometer. figure 6 (d) depicts the output of the magnetic compass when rotating around the z axis. the signal describes a smooth sinusoid recording the intensity of the magnetic field on the y axis from −h y to h y, as expected. in the second test of the compass, the sensor is mounted on a toy car pulled 25 meters through a straight hallway from our building. the hallway is lined with large concrete pillars containing metallic reinforcement. figure 6 (e) shows the results of two such experiments, which both confirm strong influences of the nearby metals on the compass sensor."
"based on the decoupled soc function proposed in section 3.2, the relation between price and charging/discharging amount is modeled. additionally, the expected profit for a single action of ess can be calculated without requiring the information of complete operation cycle. as shown in tab.4, regardless charging or discharging, ess is allowed to obtain profit through the optimized strategy. this indicates that the presented tiered framework enables the self-benefit action decision of ess in the decentralized energy interaction."
"in the existing literature, most applications are based on a uniform global clearing price for the energy market, which results in a der located in different micro-grid gaining the same benefit. to improve on this limitation, optimal power flow [cit] and the coordination of different micro-grids [cit] are taken into account. on this basis, in order to ensure the profitability of distributed resource owners, and reflect the competitiveness of aggregators, a novel clearing method corresponding the above hierarchical optimization is proposed, which is able to quantify the contribution of each der and can be considered as a win-win method."
"as potential competitors, aggregators are not willing to publish their balance information. that makes the coordination between aggregators more difficult. therefore, a trading optimization method is used to determine the optimal spot price. according to formula (20) (22) define the unit energy of trading as x, for each aggregator, the sell and purchase prices are obtained by using:"
"the current compiler prototype only distinguishes between a more powerful gateway node and the regular sensor nodes, but more complex allocation schemes are conceivable. the allocation procedure is based on the dependency graph generated by the type checker. for the gateway, the allocation process starts at the main method. this method is the central entry point of the mpl program and is always executed on the gateway machine. starting from the main method, all classes used by this method are recursively collected. only the compiled code of these classes is deployed on the gateway. for the nodes, the process starts at the set of actions that are defined in the mpl program. the compiler determines for each action, if it is used by a remote action (e. g., tell or report). only those actions can be executed on a remote device and thus on one of the nodes in the wsn. for each of those actions, all required classes are recursively collected. with the current prototype, this set of classes is deployed on all nodes. in the future, we will also take the capabilities of the nodes into account and build independent sets for each node type. in a second step, all actions requiring capabilities that are unavailable at a specific node type are removed from the respective set."
"the above features even though proved to be statistically significant in classification don't really capture the temporal dynamics of the signal, which is to say, how a beat differed from other beats in terms of energy of the beat, energy of states of individual beats, duration of beat etc, hence a markov chain analysis [cit] was done to capture this, which we found to be useful to improve the accuracy of the results."
"6.2.3 full simulation. the full simulation includes both the velocity and the steering models described in the previous sections. this combination entails that the steering control value of the previous iteration is fed to the friction model of the velocity simulation. the current steering control value is inverted if the velocity becomes negative, to properly simulate backward driving. our simulations show that the follower controller works adequately when subjected to the model of dynamics explained in section 6.1. the follower manages to closely keep to the leader trajectory and shows realistic changes in speed when taking turns. for detailed results on how field test results match the simulations, see section 8.4.3."
"the different meta-abstractions define extension points that can be instantiated by concrete implementations of these abstraction types. for example, the target meta-abstraction could be instantiated by logical neighborhoods or another group-defining programming abstraction. several instances of the same abstraction may exist at a time and an application may employ different abstractions instantiating the same metaabstraction concurrently."
"6.2.1 velocity simulation. the velocity simulation follows the model described by eqs. (2), (7) and (8). in figure 4 (a) we represent the simulated environment, which accepts the throttle t and brake b control signals from the velocity controller, and yields the simulated velocity v."
"the current implementation is based on the contiki multithreading library that provides a platform independent interface to switch the current stack. this interface is implemented for all major hardware platforms supported by contiki. based on this library, we implemented a custom thread scheduler running as a concurrent contiki process that schedules runnable threads in a round robin fashion. a major drawback of full-blown multithreading is the comparatively high memory overhead as each thread has its own stack that needs to be constantly kept in memory. to restrict the memory demand, the maximal number of concurrent threads is limited. if the maximum is reached, attempts to create further threads fail and an error is signaled to the user program. in the extreme case of an application that does not itself require multithreading, the whole application can be executed in a single thread. a separate thread running in parallel to the operating system is still required to support blocking operations without interfering with the operating system functions."
"the final model is a weighted ensemble of 4 xgboost and 4 ann classifiers trained with feature bagging, manually tuning the parameters to get optimized ones. figure 3 gives flow chart of the methodology used for classification."
"the core goal of the language design was the provision of an implementation of the makesense framework. as a consequence and in line with requirements i and ii, the language needs to be able to cleanly expose the abstractionbased extensibility features of the makesense framework."
"8.4.1 error accumulation. in figures 11(a) and 11(b), we notice that the follower generally ends up further relatively to the finish line than the leader (the random tests are irrelevant in this case, as there is no finish line). as mentioned before, this is caused by the errors accumulating in the acceleration integration process and the inherent delay of the motion detection technique (see section 3.1). the pertinent question that remains is how does the error accumulation look like in practice?"
"in this paper, java agent development framework (jade)-based multi-agent system is applied to model the participants in mgc. due to too much information transmission, transactions of agents are sorted out into fig. 2, which shows the starting time, finishing time, relationship, and functions of each agent. the functions of related agents are further explained as follows: 1) dno agent: dno agent, as a grid-connected reserve power source, is responsible for calculating the total electrical consumption or injection of mgc, in order to plan the distribution network scheduling. the total net demand of each aggregator agent is monitored in a specified interval, with the difference between scheduled quantity and actual quantity used to determine whether the penalty function is launched."
"improvement of the crop productivity is a major challenge in the countries like india, the technological improvement is a mandatory work to improve the crop productivity to support and sustain the need for ever green population of our country. in the past several sensor driven network have been proposed to successfully monitor the large agriculture field. however most of the technology does not offer on the data mining technique and predictive analysis, which limits the data usage to accurate state of the field and crop. we propose a novel technology by means of which gathered data from physical sensing devices is mitigated in the cloud, where a machine learning technique could in real time produce not only the alerts corresponding to the current state of the environment and the crop but at the same time can offer predictive analysis of the future state of environment as well as crops."
"mpl programs typically need to access functions of the underling hardware and software platforms. in mpl, such platform functions are also exposed through the previously introduced abstraction interface."
"in the following, we provide a description of the hardware and software implementation. we start by presenting the sensors used, the interfacing to the nodes, the placement on the toy cars, and the calibration procedure. next, we describe the actuation on the follower vehicle, the scheduling on the sensor nodes, and the implementation of the calculations from section 3. we end this section with a description of the debugging and benchmarking of the controller implementation."
"we use fuzzy control because of several advantages with respect to the specifics of wsans. firstly, fuzzy logic can be implemented on limited hardware and is computationally fast [cit] . secondly, it handles unreliable and imprecise information (which is usually the case with sensor data), offering a robust solution to decision fusion under uncertainty [cit] . thirdly, fuzzy-based methodology substantially reduces the design and development time in control systems [cit] ]. finally, fuzzy controllers handle nonlinear systems (most real-life physical systems are nonlinear) better than conventional approaches [cit] ."
"3) aggregator agents: aggregator agent is responsible for organizing its signed ders, acquiring and sorting prediction data, computing optimal operation, performing energy trading with other aggregators through trading platform agent, and transmitting strategy commands to der owner agents. 4) der owner agents: der owners agent needs to select an aggregator agent independently, report the power prediction to its selected aggregator agent, and execute the control strategy provided by the aggregator agent."
"it is observed that the major profit of each stakeholder is determined in tier i through the inner self-balance scheme. the total profit, as illustrated in tab.2, has been 1298.59 cny after executing tier i process. it increases only a slight amount in tier ii and iii, reaching to 1311.17 cny and 1316.26 cny respectively. this fact demonstrates that the presented pricing methodology is able to significantly motivate local energy balance within a micro-grid and efficiently reveal the requirement of supply-demand balance via price signal. additionally, the scheme provides a novel business and technique platform, allowing the flexible energy resources to satisfy the globally optimized energy balance in a win-win manner. for example, the dgs in 1mg2 plan to sell high-cost generation 159.385 kwh in tier i. although their generation decreases to in tier ii 158.227 kwh, the profit enhances because of sharing the benefit of cost-effective generation replacement. it is observed that all of the ders increase their profit during the tier i, ii, and iii processes. 3) impact analysis for aggregator assignment 16 in the presented framework, the ders can be assigned to any aggregator according to their decisions. in this section, the impact of aggregator assignment is investigated. here, we take the utility-scale wind generators, 1uwt, as an example. as a generation unit, 1uwt obtains the higher profit when its output can be more easily consumed."
"multi-agent system (mas), a widely accepted method of distributed modeling, can simulate the agents with independent ability of reaction, decision-making, communication and expansion. and the approach is popular in distributed resource scheduling [cit], decentralized energy optimization [cit], and coordinated control strategy [cit] . in addition, some studies adopt mas to analyze the solution of game theory, offering another way to investigate the competitive and cooperative energy transaction within mgc [cit] ."
"the scoring function used for the competition treats extremely noisy signal to be a separate class, but since our analysis doesn't do any such special handling for noise classification, the validation score will be reported as per the below scoring function."
"this section describes the conceptual wsn abstraction framework developed in the makesense project [cit] . previous research has demonstrated that a large number of wsn programming abstractions exist that solve common challenges of wsn programming [cit] . wsn programming abstractions tackle issues such as node addressing, definition of communication patterns, and distributed data processing. a typical example of such programming abstractions is logical neighborhoods [cit], a system that allows to define groups of nodes based on their state and to communicate with them in a way similar to sending broadcast messages to physical neighbors. programming abstractions, like logical neighborhoods, already simplified wsn programming, but they were difficult to combine with other abstractions in a single program."
"the definition of the logical neighborhood relies on a set of node attributes (\"role\", \"type\", and \"temperature\") provided by the run-time environment. to allow simple access to such node attributes and operations in embedded code, these are exposed as static methods of a predefined system class. in this example, the system methods gettype and gettemperature are used in the embedded code to determine the value of the corresponding attributes associated to the target node (i.e., the type of attached sensor and the sensor value). in line 9 of the example, a new instance of logical neighborhood is created, which is configured by the embedded code. this logical neighborhood instance is associated to a new stream instance in line 11 to define the set of nodes from which the data should be streamed towards the sink. after also specifying a local action to read a sensor value and a data operator to aggregate the individual readings, the stream is started in line 17. finally, the program waits for a result and stores the returned result in the result variable, as soon as available."
"the calibration procedure compensates for both the magnetic field distortion (due to ferrous or magnetic nearby structures) and the inclination relative to the reference horizontal plane (see also section 3.2). we drive the vehicles in a circular movement and collect the readings on the x and y compass axis. instead of the ideal circle centered in (0,0), we usually get an offset ellipse. from the minimum and maximum recorded values we determine the scale and offset calibration coefficients that project the ellipse back to the desired circle (for the details of this method see caruso [cit] )."
"this requires a move from the still prevalent node-centric programming model towards a more holistic view of the network that hides low-level details. to this end, a growing number of macroprogramming abstractions have been designed that simplify programming of a specific distributed computing aspect (such as assigning roles to nodes [cit] or defining a subset of nodes to communicate with [cit] ) by offering a domain-specific language. however, integrating multiple of these abstractions into a single program is still difficult. there also exist a number of macroprogramming languages that include a fixed set of abstractions, but new abstractions cannot be added easily."
"choosing the right sampling strategy for the two sensors creates an intricate trade-off among accuracy, power consumption, and scheduling feasibility. for an accurate velocity integration, it is imperative to sample the acceleration at a rate higher than the nyquist frequency of any significant noise in the input data. not following this rule causes aliasing effects that map frequency components of the noise to seemingly arbitrary positions in the spectrum. we establish experimentally the sampling frequency of 160 hz as an optimal value for our system (see details section 7.1.3). the sampling rate of the compass is also configurable, but special attention is required, because it relates inversely proportional to the achievable sensor resolution and subsequently to the angular sensitivity. we choose to use the sampling frequency of 16 hz, which provides theoretically an angular sensitivity higher than 0.5"
"usually, statistical features such as means, standard deviations of systole, diastole intervals, and signal complexity features are used as features for classification and these are enough to give decent results. but these features fail to completely capture the temporal information of the signal."
"in an effort to reduce greenhouse gases emissions, distributed renewable energy has developed rapidly in the last few decades. as a result, the power system is now exposed to the unpredictable nature and operational fluctuations of distributed renewable energy. therefore, the significant challenge is to maintain stability and secure operation of the wider power system. to maintain system stability, renewable generation is often curtailed, leading to a large amount of energy being wasted. to tackle this problem, various strategies have been proposed in the last few years, including demand response, energy storage to facilitate distributed renewable energy consumption. a 'smart distribution grid', involving flexible loads (fl), energy storage systems (ess) and distribution generators (dg) has gained significant research interest in this regard. this paper, is focused on energy trading in the 'distribution network market', where there are a number of participants known as aggregators. the aggregators strike contracts with distributed resource owners who have the freedom to choose their own aggregators. in this context, how to control and coordinate various distributed energy resources (ders) to meet the local energy demand, and how to optimize the energy flow through trading and competition among different aggregators are some of the challenging problems that need to be solved."
"a multitude of different systems aims to raise the abstraction level of wsn programming by providing high-level macroprogramming frameworks. these systems either represent the wsn as a distributed database [cit], or provide more sophisticated frameworks on-top of c [cit] or with custom highlevel languages [cit] . database-like interfaces are usually limited to data collection applications, while more complex frameworks usually require the use of an unfamiliar language. systems of both categories are usually monolithic and do not provide a well-defined interface to integrate applicationspecific abstractions. in this regard, mpl extends the state of the art by providing an extensible macroprogramming framework that supports in-network control logic and is based on java, a widespread programming language."
"to model the dynamics of the vehicle realistically, we take both the friction and the throttle capacity of the motor into account. the effective acceleration a e, that is, the result of the net force applied to the vehicle, is given by"
"in this test, the spot prices at eight and seventeen o'clock have the same value, 0.875 cny/kwh. that means qsoc at seventeen o'clock has recovered to its initial state and ess complete an operation cycle. during these times, the total expected profit as well as the total actual profit of ess is equal to 43.291 cny/kwh. this test can validate the feasibility of the decoupled soc value function as well as price-responsive model of the engaged ess in dynamic pricing scheme."
"finally combining all features together we obtained a total of 180 features, of which 72 features were eliminated based on the feature importance obtained through random forest and xgboost [cit]"
"in some cases, it is necessary that the plug-ins generate different code for devices with different roles in the network. for example, the plug-in might differentiate between regular nodes and the network gateway. the gateway code might, for example, need to perform additional bookkeeping that is not required on the regular nodes. to support this, the plug-in may generate different c files for each of the available node types. the code is only linked with the correct binary image, in this case. like the mpl compiler itself, plug-ins are implemented in java."
"as a rule of thumb, the higher spot price, the lower optimal state of charge. the pspot and q + soc are negatively correlated and one-to-one correspondence. therefore, the relationship of these two variables can be 8 mathematically expressed as:"
"despite the integration errors at the leader and the swaying behavior of the follower, the latter still manages to keep on the right trajectory, especially when the leader is driven with frequent changes in heading. this effect is shown both during the random tests and in an additional test that lasted for more than 10 minutes. this is partly explained by the fact that errors tend to compensate for one another. in addition, the limited velocity of the vehicles improves the overall performance. when the leader does not drive faster than what the follower can manage, the latter follows correctly, even when the estimate of the leader velocity rises beyond its constructive possibilities."
"another problem is that in the energy storage model, the profit is usually calculated according to the difference between electricity price for charging and discharging during a full-cycle [cit] . when the electricity price in the next period of time is uncertain, it is difficult to realize the global optimization of energy storage bidding strategy. in the energy market, every charge or discharge action of stored energy is to maximize profits. therefore, the decoupled state of charge (soc) value is proposed and established in this paper by quantifying the potential value of each charge or discharge action to assess bidding strategy in any interval."
"as an energy service provider, ess makes profit by charging or discharging at the appropriate time. it is clear that every charge and discharge action is intended to gain profit. generally speaking, the profit of an ess is measured based on the price difference between these two actions. however, this method has a problem that it cannot differentiate each individual benefit gained from these actions. to tackle this, a new price function, called decoupled state of charge function, is proposed to reflect the profit of each charge/discharge action."
"in the makesense [cit] project we have therefore analyzed existing wsn abstractions, classified them based on few fundamental dimensions, and developed a conceptual framework that allows the composition of arbitrary abstractions according to predefined sound rules. the framework is also extensible in that abstractions that were not known by the time the framework was designed can be added later."
special thanks go to niclas finne for his support of the implementation of the mpl language run-time and compiler and to bengt-ove holländer for his significant assistance with the implementation of the example programs used in the evaluation. the research leading to these results has received funding from the european union
"-both nodes record clearly the steep acceleration and deceleration. the leader velocity accumulates few errors and returns close to zero at the end of the experiment. the follower has a larger negative offset, which means that he drives faster than he thinks. -the follower matches the leader velocity during the periods with less steep acceleration, for example, around 1s and 16s, but falls behind during high peaks, such as 4s and 12s. the reason lies in the constructive limitations of the follower car, which cannot accelerate as fast as the leader. overall, the follower moves slower but, since it incurs a delay in braking, it ends up at 2m distance to the leader, which is is approximately the same distance as the cars started at."
"the communication protocol required by our approach is straightforward: the leader broadcasts its movement parameters periodically, while the follower just listens for the incoming data packets. this scheme ensures the scalability of the solution by allowing, theoretically, an indefinite number of followers. nevertheless, a method of logging the behavior of both leader and follower is needed for testing and evaluation purposes. to solve this practical problem, we use the following simple scheme (see figure 16 )."
"to isolate problems specific to the compass and heading controller, we test the ability of the follower to maintain a specific heading without velocity control or postprocessing. the follower starts with a preprogrammed desired heading and changes it with ±180"
"the operating cost of distributed renewable generation is negligible due to the free resources such as wind and solar. obviously, more electricity is generated more profits are made. however, renewable generation curtailments frequently occur due to generation uncertainty, leading to significant energy waste. the maximum profit, therefore, can be obtained by finding the equilibrium point between generation benefits and risk cost. first, the risk cost is quantified as follows:"
"we are using a light weight communication protocol mqtt, by using whichwe are having control over the irrigation in the form of controlling the relay. in the mqtt protocol the user has to subscribe to the server \"iot.eclipse.org\" or \"test.mosquitto.org\", and at the same time the user has to connect to the \"rupam/agro\". the commands are passing through mqtt protocol to control the agriculture field."
"in tier ii stage, aggregators optimize the operation scheduling of the contracted ders to enlarge the demand of cost-effective renewable energy. considering the fact that 1uwt sell its generation with relatively low price in tier i, aggregator 1 coordinates the contracted ders to make response to this situation. as a result, the electrical demand increases 92.688 kwh and the other high-cost generation scheduled by aggregator 1 reduces 3.933 kwh. that is to balance the 96.621 kwh of 1uwt generation which little increases from 96.256kwh as tier i plan. besides, the final supply-demand balance is accomplished at 987.556 kwh for the given interval. similarly, aiming at maximizing the profit, aggregator 2 re-schedules the energy balance by reallocating 14.055 kwh to 2mg2 and 2mg5, while reducing the equivalent amount in 2mg1, 2mg3, and 2mg4. the growth and decline of the third bars in fig. 9 -a and transfer manner in fig. 9 -b reflect this dynamic scheduling process. at the end of tier ii, aggregator 1 obtains the lower spot price than aggregator 2. the presented framework allows them to make more global energy trading based on their redundant resources. if parameter x in formulas (23) and (24) is set as 0.5 kwh, the trading process is as follows: aggregator iterates back to tier ii to generate its bidding strategy. that is to calculate the expect price of re-schedule when net demand of aggregator declines or increase 0.5kwh. the result showing that the expect price of aggregator 1 still lower than that of aggregator 2. hence, aggregator 1 enhances generation, decreases consumption and augments discharge to selling electricity to aggregator 2. through 128 times iteration, 64kwh electricity is traded from aggregator 1 to 2, as fig. 9-b shown. 2) the prices and profits analysis the spot price and total profits of all shareholders are shown in tab.2. after tier i calculation, the spot price generated by a micro-grid varies in the range from 0.428 cny/kwh to 0.598 cny/kwh. these prices reflect the supply-demand balance of each individual micro-grid. for example, the ders belonging to 1mg1 has relatively higher dg generation but lowest flexible load. therefore, they have the lowest spot price due to the large supply abundance. on the contrary, due to the lack of dg, the ders in the group 2mg2 encounters the highest price because of the shortage of self-balance capability. in tier ii, the micro-grids spot prices for aggregator 1 and aggregator 2 come to around 0.384 cny/kwh and 0.526 cny/kwh. it is noted that even the micro-grids or ders operated by the same aggregator encounter the different prices. that is because the extra regulation fee determined by formula (21) is charged. this price difference also indicates that there exists supply-demand complementarity among the different aggregators. if the energy trading is accomplished in tier iii, the aggregators' profits will increase since energy is more reasonably utilized. accordingly, the spot prices of aggregators will dynamically change. the prices for participants after trading nearly converge to 0.455 cny/kwh."
"a real time data visualization can be analyzed in the thing speak, which is an iot hub. the following figure shows the real time data visualization."
this section outlines the field experiments performed with the system at our university campus. we first test the behavior of the heading controller and subsequently we evaluate the complete leader-follower system.
"-at the end of each sampling period, the leader transmits its movement parameters. -upon receiving the message from the leader, the follower sends its own data."
"in the following, we describe the first two phases, together with the simulated vehicle model. the third phase is explained in section 7.7. the simulations are cross-validated with field experiments in section 8.4.3."
"in the initial case, the spot prices for aggregator 1 and 2 are 0.456 cny/kwh and 0.532 cny/kwh. apparently, aggregator 2 has less supply adequacy in self-balance process, which results in the higher spot price. in this comparison, 1uwt is assigned to aggregator 1 and 2 respectively. and then, two independent simulations are executed, results of which are recorded in tab. 3-a. in general, both aggregator 1 and 2 are able to increase the income by means of including 1uwt in their operation processes. this is because that enhancing utilization of economical dgs in an optimal manner can assist in decreasing generation cost, accordingly enlarging the energy consumption. specifically, the results show that if 1uwt selects aggregator 2 as operator, 1uwt can have more profits, i.e. 3.707 cny. compared with aggregator 1, aggregator 2 has higher spot price which actually makes the output from 1uwt more valuable. on the other hand, the inclusion of 1uwt enlarges supply adequacy of aggregator 2, leading to the larger spot price reduction. as a result, more demand response is motivated to increase energy usage due to the lower cost. therefore, more profits are achieved when 1uwt is assigned to aggregator 2."
"the main nomenclature used throughout the letter are listed below for quick reference. constraints, and methodologies. as the number of micro-grids increases in the neighboring area, the interactions among them need to be monitored, controlled and managed. consequently, the concept of micro-grid community (mgc), which interconnects and coordinates a cluster of adjacent micro-grids with a designed topology and control structure, is proposed. reference [cit] addressed the challenge of coordinating micro-grids with energy exchange through a hierarchical energy management system. reference [cit] investigated a dynamic economic dispatch approach for an agent-based mgc. an optimal interconnection strategy using minimal cutset-based methodology is investigated in reference [cit] to enable a mgc with a higher adaptability, and to accommodate operational fluctuations."
"to enable the commercially flexible operation of a micro-grid community, a novel dynamic pricing scheme with full consideration of multiple stakeholders' economic pursuit was studied to facilitate decentralized energy trading. based on the models of distributed energy resources with price response behaviors, a three-tiered architecture including micro-grid balancing, aggregator scheduling, and trading optimization was presented to discover the spot price that motivates the efficient utilization of distributed energy resources regardless of their location or property ownership. the methodology facilitates micro-grids with high-shares of renewable energy, improves the utilization of renewable energy consumption, and mitigates against the unavailability of traditional centralized dispatching. by means of multi-agent simulation, the results of a realistic case study show that the scheme is able to realize a win-win framework to reliably optimize the operation of micro-grids and distributed energy resources. in addition, it can also guide owners and aggregators to act appropriately according to an iteratively updated price signal, avoiding the underutilization of distributed energy resources. it is observed that the presented methodology provides sufficient business flexibility and can be readily expanded to apply to more markets. although acquiring win-win results by simulating realistic cases, there are several aspects limiting the scale applications. from the algorithm perspective, some parameters might be difficult to obtain in real applications, such as the price elasticity of the demand, and the uncertainty of generation. moreover, if the renewable energy proportion in the micro-grid community is small, the profit of local transactions may be outweighed by the cost for establishing the distributed trading platform and communication facilities. with further research and development, distributed renewable energy combined with local transactive framework represent a possible solution for future microgrid operation."
"node-local functions are exposed as local actions. this typically includes tasks like reading of sensor values and control of actuators, but the same facility could be also used to implement local data storage or data processing algorithms. we expect such platform-related functions to be implemented by wsn experts using c code with direct access to the underlying operating system. to support such an approach, methods in mpl can be declared as native, in which case the actual implementation of the method is provided by an external c implementation. access to mpl language features is possible via a predefined c interface. amongst other things, this interface provides the means to access, manipulate, and create mpl-defined objects within user-provided c code."
"the results presented so far open several points of discussion. in the following, we briefly examine the effects of error accumulation and error canceling, as well as the accuracy of our simulations with respect to the experimental results."
"as we can see, the trigonometric computations are quite involved for a sensor node microcontroller. in addition, the variation of roll and pitch angles when the vehicles are moving cannot be measured without a gyroscope."
"user can analyze the output in the thingspeak, which will give the graphical notations of all the values. thingspeak is an internet of things application and is an open source. thingspeak can also acts as an application programming interface in order to store and retrieve the data using the http protocol over the internet or via a local area. we can see the graphs of temperature value, light value, soil moisture value and the rainfall value."
"where pex is the price of exporting energy to dno. and kpen means the penalty coefficient which is set as 2 in this paper. in this special case, when local generation is redundant, the optimal scheduling is to export forecasting quantity to dno. if penalty increases, the optimal generation scheduling will be more conservative but helpful to increase system stability. and vice versa, dg always tries to bet the opportunity that real generation is more than forecast generation due to less penalty of default. as shown in fig.3, rather than simply set the marginal cost of dg as a constant as indicated in the literature [cit], we consider its variation is in the range of 0 and kpen·pim in this study. if the spot price is at the point 1 in fig.3, more (less) generation at point 1 enables the marginal cost higher (lower) than spot price, which requires to reduce (enhance) generation for profit. the equilibrium that the marginal cost equals to the spot price represents the optimal generation which may not be the forecasting generation. and the profit of generation scheduled at point 1 can be calculated as:"
"the training data consists of pcg signals of varying length, anywhere between 5s to just over 120s [cit] hz. for training, all the signals were re-sampled to 1000 hz and features were extracted. since pcg recordings were collected under uncontrolled environments, they were corrupted by various noise sources. hence signals were filtered with a band-pass butterworth filter of the frequency range, 25 hz to 400 hz to remove high-frequency noise as well as artifacts such as baseline wandering. the signal spikes were then removed using schmidt spike removal technique [cit] and the signal was normalized to zero mean and unit variance. reference annotations for four heart sound states (s1, systole, s2, diastole), for each heart beat, were then obtained for the pre-processed signals using springer's segmentation algorithm [cit] which is a state of the art solution for heart beat segmentation."
"we take therefore a simpler approach and assume that the tilt will remain constant during driving. as a consequence, the tilt compensation from eqs. (5) and (6) can be replaced by the calibration procedure described in section 6.3. our experimental results show that this method is robust to tilt effects whilst driving, as long as the vehicles remain on a relatively flat surface."
"lessons learned. the main lesson learned during the implementation on sensor nodes is that sampling inertial sensors requires a major share of the available cpu power and preferably a multitasking operating system. scheduling all the tasks is difficult, especially due to the compass sensitivity to various interferences. the large amount of computation involved in inertial navigation algorithms is another potential source of hard-to-identify bugs. introducing a feedback loop from the sensor node to the simulator proved an efficient debugging method for this problem. from field experiments, the most important lesson learned is that the constructive limitations of the vehicles can have both a negative impact (such as the lack of a braking system) and a positive influence (the limited engine power of the follower bounds the effect of error integration in velocity). compass calibration remains a necessary step for producing accurate experimental results. driving the toy cars proved to be challenging, as collisions at 30 km/h would destroy the sensors mounted on top. in general, experiments were more time consuming than expected, required a dedicated, large open field and, last but not least, were dependent on the occasional good weather."
"to provide programmers with additional flexibility and to allow the creation of dynamic data structures, mpl also supports dynamic memory allocation and the standard new operator. nevertheless, garbage collection is not supported, as this would add a significant run-time overhead. dynamically allocated objects need to be destroyed explicitly by using a newly introduced delete operator. dynamic memory allocation currently relies solely on the malloc implementation of the underlying target platform without additional optimization."
"9 where ql0 and pl0 are the initially scheduled demand and price of fl. fig. 5 . demand response profit of fl as shown in fig. 5, spot price is pl0 at point 1. when the spot price changes, fl will regulate the load quantity to acquire subsidies and pursue higher profits. the subsidy determined by the market is pspot. the maximum profit of regulating is solved on the equilibrium point of the subsidy and the use value. therefore, the priceresponsive model can be deduced when formula (14) is equal to spot price:"
"as presented in figs. 8 and 9, the mpl-based code also makes use of more statically allocated memory. the overhead in static memory consumption is mainly caused by the additional data structures required for object-orientation support in mpl. as the relative overhead decreases with application size, we expect the relative overhead in the evaluated scenarios to represent a worst case scenario. more complex applications should exhibit a less significant relative overhead. further optimization of the object representations can likely also significantly reduce the static memory demand of mpl-based programs."
the heading (or azimuth) φ is computed from the components of the magnetic field intensity h measured by the magnetic compass (see figure 2 ).
"the heading φ indicates the vehicle orientation with respect to the magnetic north pole. in order to map φ to the interval [−π : π ], we have"
"-inputs. as inputs we use the error e and the change in error e between the actual values of the movement parameters and the desired values. this is a common approach for improving the controller stability when the output value is close to the optimal operating point. -fuzzy sets. for an increased control granularity, we define five fuzzy sets for each input, namely nm (negative medium), ns (negative small), ze (zero equal), ps (positive small), and p m (positive medium). -membership functions. to leverage the computational effort, we use triangular membership functions spaced equally with distance n, as depicted in figure 3 . the width of the membership functions influences the aggressiveness of the controller, that is, what margin of error it tries to achieve. -rule-based inference. we use the max-min fuzzy inference method over the rule set specified in table i . -output. the controller output is decided by defuzzifying the aggregated inference result according to the center of gravity (cog) method."
"the simulation framework we developed plays an important role in designing and tuning the fuzzy logic controller. more specifically, we revert to simulation in the following phases:"
"getting the right scheduling on the limited sensor node is the most challenging part of the implementation. as explained in section 7.1.2, the sampling tasks of the accelerometer and compass must be interleaved. in addition, the influence of radio operation on the compass described section 7.2 creates additional dependencies between the tasks associated with these two resources. it is essential therefore to devise the task deadlines in such a way that the execution sequence generated by the scheduler fulfills all these requirements. on the leader, the process is simpler because we know when the radio transmission occurs (at the beginning of a new sampling period) and we do not have a control task (the leader car is driven remotely). figure 8 (a) sketches the execution sequence on the leader during one sampling period (task durations are not to scale). the heartbeat of the system, termed as the timer handler task, samples the accelerometer at 160 hz. being slower, the compass sampling requires two trigger tasks, compass sampler and compass reader, for initiating the measurement and reading the data, respectively. in addition, the x and y axis cannot be read simultaneously (see also section 7.1.2). at the end of the sampling period, the data processor task updates the movement status based on the measured data and prepares the message to be transmitted by the outradio task."
"the spot price p i spot has been derived for each micro-grid in tier i. but these prices are usually not equal due to the different micro-grid characteristics. a price-driven coordination is needed to avoid underutilization of cost-effective ders. for example, the dg in the low-spot price micro-grid is likely to seek opportunity to sell its generation to the neighboring micro-grids with more profitable spot price. it is the same case for either ess or fl. in doing so, the spot prices of micro-grids will be approaching to the optimized spot price of aggregator. similar to tier i, the objective in tier ii is solve the global spot price of aggregator which makes the total imbalance electricity of a specific aggregator δq"
to evaluate the performance of mpl we selected a small set of wsn applications and implemented each of them as a macroprogram and as functionally-equivalent contiki/c programs. we then compared the performance of both implementations based on a set of typical software performance metrics for wsns.
"the time duration generated by segmentation algorithm for the four heart sound states: s1, systole, s2, diastole were used to get few selected, typical statistical features such as mean and standard deviations of each state lengths, powers, ratio of each state length to rr interval, ratio of systole to diastole intervals, ratio of amplitude of systole to diastole, number of zero crossings, etc. this comprised a total of 38 features."
"two major types of meta-abstraction can be distinguished. actions represent anything a node or a set of nodes can execute. this can be simple commands, like reading a sensor value, or more complex operations such as requesting aggregated values from a group of nodes. modifiers allow a more precise specification of the behavior of actions. for example, a modifier could be used to specify which nodes should be part of the group that provides the aggregated value."
"while the contiki/c-based programs do not make use of dynamic memory allocation, some features of the mpl programming model rely on dynamically allocated memory, e.g., to handle concurrent execution of actions. this introduces a slight additional overhead, but the demand of dynamically allocated memory is comparatively little. none of the applications allocated more than 420 bytes at a time on any node in our test scenarios. the computational overhead introduced by the use of mpl turns out to be very low in the experimental scenarios, as shown in fig. 5 . in the collect and the hvac scenario, the mpl-based code actually employs less cpu cycles on the most active node of each network than the respective contiki/c-based code. this demonstrates that features like virtual method dispatch do not introduce a significant overhead in terms of execution speed and energy consumption in typical applications."
"to summarize, the contributions of this paper are as follows: (1) the marginal cost of distributed generation is deduced by utilizing risk associated with the der generation, which subsequently determines the optimal 4 generation response to market price. ( 2) the single charge or discharge profit of ess is quantified by decoupling the full-cycle profits based on a value curve and historic data, which can promptly generate a suitable bidding strategy instead of adopting global optimization. (3) a win-win framework is proposed to maximize profit through addressing dynamic guiding prices during a three-tiered optimization within a mgc. (4) an impartial clearing is designed according to the contribution of each response behavior, facilitating a fair sharing platform. (5) the presented scheme is useful to optimize the ders configuration that higher profit can be obtained in the scarce resources micro-grid."
"java agent development framework is a development platform for simulating multi-agent system based on java language. it has several unique characteristics. (1) the functions, behaviors, algorithms of each agent can be modeled separately, forming a clear format and architecture. (2) each agent is equipped with an inbox and outbox. and jade library provides numbers of communication samples, like \"request\", \"propose\", \"inform\", \"reply\", \"agree\", \"refuse\", facilitating unobstructed transactive between agents. (3) the information flow and content can be readily monitored by the provided functions. these characteristics make it clear and systematic to design a micro-grid community and its transaction."
"formula (11) represents the orange area between point 1 and point 2 in fig 4. similarly, when the spot price comes to pspot,4 from pspot,3, the expected profit of charging can be calculated using:"
"in general, the quantity of ess transaction is related to the soc and the spot price [cit], the relationship of which can be formulated using fess(·) as follows:"
"nevertheless, our macroprogramming language also differs from the standard java me [cit] framework by targeting even more resource constrained devices. in addition, java me does not provide wsn-specific extension points to integrate existing concepts that abstract from typical wsn challenges such as communication and distributed data processing. in comparison to standard java, wsn-specific extensions significantly increase the utility of our macroprogramming language. some low-resource java virtual machines also exist that are targeted at low-power embedded and networked systems [cit], but they still require a comparatively significant amount of resources. for example, the squawk virtual machine uses 80 kb of program memory and consequently targets more powerful arm-based embedded platforms [cit] . our approach differs in that it generates customized c code which is in turn compiled into optimized machine code for the intended target platform hence reducing the introduced overhead."
"where qsoc is the state of charge, and qess is the optimal adjusted quantity. after the adjustment, the state of charge can be updated as:"
"the specific significance of formula (12) is the green area between point 3 and point 4 in fig. 4 . the calculated results of the above two formulas are decoupled state of charge value. when qsoc returns to the initial point 1, the sum of decoupled state of charge value is the shaded areas in the fig.4 . it is equal to the real profit computed by the common method based on the difference the charge and discharge, proving the reasonableness of the proposed method."
-linear tests. the leader drives a 20m straight line. -square tests. the leader drives a square of side length 15m. the finish line is perpendicular to the start line. -random tests. the leader drives along a random path for 30s with varying velocity.
"the remainder of the paper is organized as follows. section ii briefly reviews the current state of the art. section iii summarizes the makesense abstraction framework. in section iv we derive a set of requirements for a language implementing this framework. section v introduces design decisions and implementation details that enable us to implement a language that is capable of meeting these requirements. section vi introduces the architecture of the underlying compiler framework and provides a closer look to the plug-in interfaces that enable the required extensibility. finally, the performance and overhead of the approach for a typical set of applications is evaluated and discussed in comparison with more conservative c-based implementations in section vii."
"on the contrary, for the micro-grid characterized by positive net load, 2mg2 for instance, it is more appropriate to contract with the aggregators with more inclusion of dg asset. in this case, the higher profits of stakeholders will be created. tab. 3-b shows the results of the studied cases."
the iot based architecture also offers real time realization and analysis of data which can be used across the globe in conjunction with the parameter been monitored through other parts of the world to understand the abnormal behavior of the similar kind of the crop. our result shows that the proposed system has a very optimal latency for controlling the system as well as high packet delivery rates and accuracy for mitigating the data. the system can further been improved by incorporating new self learning techniques which could deployed in the cloud to understand the behavior of the sensing data and can take autonomous decisions.
"in this paper, we introduce the design and implementation of a java-like macroprogramming language for the makesense framework. a preliminary evaluation demonstrated that it is possible to implement a high-level object-oriented macroprogramming language with a reasonable overhead. despite the positive results of the evaluation, current limitations open up an avenue for future work. the performance of the system can be further improved to make it even more suitable for the resource-constrained devices typically found in wsns. especially, memory consumption of the generated code still leaves significant room for improvements. memory consumption could be, for example, improved by a more sophisticated strategy for code allocation. in the current implementation, code allocation operates at class level. instead it would be possible to extend these decisions to individual methods and attributes. in addition, it would be useful to make the allocation algorithm work with a larger number of node types and to take the actual program behavior into account. to make the system more useful in practice, we also intend to improve debugging support. finally, compatibility and dependencies among abstractions and between abstractions and the underlying protocols are not satisfactorily handled by the framework. the selection of suitable abstractions still requires manual intervention and some degree of expertise. ideally, this selection would be largely automatic based on an abstract set of user-defined requirements. we currently explore possible solutions in the relyonit project [cit] ."
"first, we list out the values of these 4 features for all the beats in all the recordings, then for each of the features, 2 thresholds were selected, which divides the training data at 33 percentile and 66 percentile points rendering them into 3 parts [cit] . thus a total of 8 thresholds for 4 features in combination, give us 12 different regions in 4-dimensional space of 4 features. these 12 regions are used as 12 states of a markov chain."
"for energy management at mgc, a hierarchical structure is often employed, including micro-grid optimization and coordination [cit], aggregator optimization with market transactions [cit], aggregator optimization with aggregator transaction [cit], ders transaction with micro-grid transactions [cit] . in a mgc market, ders in a micro-grid may be entrusted to different aggregators while ders located in different microgrids may select the same aggregator, as shown in fig 1. that is, ders assemble based on the profits rather than the geographical location. based on this, an iterative three-tiered optimization approach is proposed in this paper to achieve the overall optimal operation of a mgc, tier i to solve the individual micro-grid optimal operation problem; tier ii to solve the aggregator scheduling or coordination optimization problem; tier iii for aggregator trading optimization problem."
"an object created with static is available for the complete run-time of the application. for example, the allocation of the rectangle rec_global in listing 1 employs this feature. static objects are represented by global variables in the generated c code. their memory is statically allocated as part of the program image."
"-both the leader and the follower measure consistently the changes in heading. we can see clear turns corresponding to the rectangle corners at intervals of approximately 5s. a certain amount of swaying by the leader, especially around 3s and 13s, cannot be avoided because the leader is driven manually. the final difference in heading between the two is 11"
"the operation scheduling predicted and reported by all ders according to their own wishes is generally not the optimal operation scheduling. the operation scheduling should be optimized for profit. this section will address two problems: how do ders respond the spot price? how much profit will be obtained by the response? thus, the price-responsive models of dg and ess are deduced, and that of fl are sorted out via existing research."
the increased abstraction level and the use of powerful programming abstractions enables a reduction of user-written code by more then 50% in comparison to an implementation in low-level code for a set of typical applications.
"the specific significance of formula (16) is the green area between point 1 and point 2 in fig. 5 . similarly, when the spot price decreases, like pspot,3, the load quantity will increase. fl also gains profit, which is shown as the orange area in fig 5. it can be calculated using:"
"from frequency domain analysis, fft statistics, autoregressive moving-average (arma) features [cit], melfrequency ceptral coefficients (mfcc) features [cit], sample entropy [cit], music features [cit], octave band features [cit] etc. were extracted and it comprised a total of 102 features."
"distributed team coordination in wsans faces a number of challenges. first, executing all the tasks on the node (sensor sampling, processing, communication and control) may easily exceed the computational and memory resources available. consequently, we must find the right scheduling that trades off between accuracy and responsiveness, on the one hand, and sampling frequency and wireless communication duty cycle, on the other hand. second, actuator nodes must run a navigation control loop for regulating the movement of vehicles. designing and implementing a suitable controller for this purpose is far from trivial on limited hardware. third, using inexpensive, low-power inertial sensors means a relatively low accuracy and robustness to noise. therefore, calibration, filtering, and dynamic error compensation are strongly required for improving the quality of measurements."
"to control them, we use a separate msp430 microcontroller and dedicated circuitry. the two microcontrollers communicate on a separate software i 2 c interface, so that the spi-based dialog with the sensors is not affected. the following control commands are available."
"during the last years, the use of wireless sensor networks (wsns) significantly increased [cit] while wsns also started to make their way into commercial and industrial real-world applications. nevertheless, a more widespread wsn adoption is still hampered by the unavailability of easy-to-use development tools. as of today, most wsn applications are still implemented in low-level c code and their design requires in-depth knowledge of the specifics of embedded systems and low-power wireless communication. consequently, wsn programming is usually carried out by wsn experts. to gain more widespread use, wsn development needs to be more accessible to domain experts and programmers without a strong wsn background."
"a gateway node listens to all incoming packets and logs them to a pc. packet losses are identified based on sequence numbers. in addition to the movement data, the messages sent by the follower include also the controller outputs and raw sensor data. this is a valuable feature because we can reproduce the experiments in the pc-based simulator and correct or tune the controller accordingly."
"fig . 4 demonstrates that the plain contiki/c applications require the user to write more than twice as much code to ac- complish the same task. in addition, further low-level technical details are exposed in the contiki/c program. this indicates that the use of a high-level macroprogramming language like mpl can reduce the effort needed for the implementation of typical wsn applications. the programmer needs to write less code to achieve the same result."
"to make the language suitable for resource-constrained devices as demanded by requirement vi, some limitations compared to standard java were necessary. the most significant difference is the absence of a virtual machine. programs are instead translated into c code targeted at the contiki platform that can be further processed by the established tool chain to generate deployable binary images."
"we model two types of noise (throttle noise t n and accelerometer noise a n ) as uniformly distributed random variables. the simulated noisy velocity measurement v sensed is input to the velocity controller. the controller tries to achieve the desired velocity v desired using its throttle t out and brake b outputs. figure 4 (b) shows the structure of the heading simulation. the heading controller has two inputs (the desired heading φ desired and the measured heading φ sensed ) and outputs the steering control value s. the car drives at constant velocity and, with each iteration of the simulation, the current heading φ is updated with the current steering control value s using eq. (9)."
"our system demonstrates that wsans can sense, reason, and react as a group with distributed intelligence, without any intervention from the back-end and despite the hardware limitations of sensor nodes. as concluding remarks, we briefly iterate the main advantages and limitations of our method, name the practical lessons learned, and outline the future work directions."
"to answer this question, we go back to the square test depicted in figure 13(a) . the top plot shows an estimation of the accumulated error in velocity for both leader and follower (dotted lines). the error seems to start accumulating when the cars make the first turn, probably because of the small changes in tilt angles. this is also the moment where the difference in the integrated velocity of the two vehicles starts to be significant. we attribute the worse performance of the leader to the soft suspension and the weaker mounting of the sensor. figure 13(b) shows the corrected velocities, where the linear error estimation is subtracted from the raw data. the matching between the two signals is much better and corresponds to the actual behavior observed in the field."
"figs. 6 and 7 present the respective text size of the compiled mpl and contiki/c implementations of the evaluated applications. it can be seen in fig. 7 that the program image of the mpl programs is significantly larger for the evaluated applications. a cause for the increase in program size is the run-time environment required for some of the advanced features of mpl, like the support for multithreading. it should be noted that the overhead is largely constant (i.e., it does not grow with program size) and the relative overhead should be lower for larger, more complex applications. the program memory demand of the applications is still well within the limits of typical wsn platforms."
"-the velocity increases correctly in the first 6s, more precisely until the first turn. from this moment, the leader accumulates errors and ends up with a high value at the moment of stopping. the velocity is reset through the motion detection technique explained in section 3.1. the follower does not exhibit this problem. its speed remains relatively constant from 6s until 21s, when it finds out that the leader has stopped and, consequently, applies the braking procedure. the follower velocity returns to a value much closer to zero compared to the leader. -looking back at figure 11(b), we understand the effect of error accumulation in the leader velocity. the follower tries to keep up and accelerates to its maximum. the leader detects that it stopped with a certain delay and then informs the follower. the latter also needs some time to brake, so it eventually stops at 10.7m away from the leader."
"the experimentation with newly introduced markov features proves to be successful, improving the result by as much as 5% on the validation data. the validation scores are consistent with the test scores, hence over-fitting is minimal. the overall accuracy could be improved with better insight into the cause of abnormalities, type of abnormalities, and even if an abnormality is observed, knowing what region of pcg is subjected the same would be helpful. moreover, noise, impossible to separate in the frequency domain, makes it very hard to classify, hence should be dealt in a better way."
the graph in the above figure suggests that the temperature value is analyzed in the thingspeak iot hub. the graph is plotted with temperature value v/s date.
"-the electromagnetic field created by the car motors, which has a strong effect on the compass output. to alleviate this phenomenon, we use a 5mm shielding metal plate mounted below the sensor board. -the batteries of the sensor node. surprisingly, changing the batteries or even swapping the places of the two batteries has a visible impact on the compass readings. the easiest solution is to place the battery pack as far from the compass as possible. -the radio transceiver. we could not identify the exact cause of this influence."
"the system architecture is composed of sensors (temperature, moisture, rainfall and the light), which are installed in the agriculture field. these sensors will be collecting the environmental parameters. the sensed data is mitigated into the cloud through an iot gateway (thingspeak); thingspeak gives a real time data visualization."
"a number of local actions for commonly used functions are predefined by the language. as seen before, a subset of these functions is also available as static methods of an automatically generated system class to enable access from embedded code. fig. 2 gives a high-level overview of the mpl compiler architecture 1 . primary input to the mpl compiler is a macro-program written in mpl, possibly consisting of several source files. the macroprogram is supplemented by information about the system's capabilities, such as the hardware features and on-board sensors of deployed nodes. this information is used by the mpl compiler to aid optimization and the allocation of functionality to the different nodes. in addition to these inputs, the mpl compiler has access to a repository of components implementing the macroprogramming abstractions and runtime functionality. as output, the mpl compiler generates platform-specific source code for each node type of the target wsn, e. g., gateway and regular nodes, which is translated into a deployable binary image by the regular platform toolchain. our current prototype generates c-code for the contiki operating system [cit] . it is intended that later versions of the compiler will be extended with further code generators for different platforms."
"in this case, the transformed magnetic intensities h x and h y to be used in eqs. (3) and (4) [cit] ] are as follows."
the final component of the vehicle model is the heading. we assume that the front wheels of the vehicle can steer with an arbitrary angle between −δ and δ. the equation for updating the current heading φ is
"the operator auto can also be used in a second role within the initializer expression of member variables. if used in an initializer, auto ties the lifetime of the newly created object to that of the host object. the class rectangle in listing 1 demonstrates this use of the auto operator. the memory required for the two point instances is automatically allocated if a new instance of the rectangle class is created. this type of automatic allocation is implemented by directly embedding the representation of the dependent object in the c representation of the host object. consequently, the required storage space becomes part of the memory demand of the host."
"(i) the language needs to be suitable to reflect the previously described meta-abstraction hierarchy supporting also later addition of instances of the abstraction classes. (ii) as some programming abstractions employ sophisticated custom languages for their configuration, the language requires a convenient mechanism to integrate such domain-specific languages. (iii) individual nodes may need to handle several remote actions at once. consequently, the language needs to support concurrent execution of tasks. (iv) the language needs to be expressive enough to implement moderately complex algorithms, it needs to support at least basic mathematical and logical operators, conditionals, and basic looping constructs. (v) the programming language should be familiar and easy to use for a large number of programmers. it should be especially appealing for typical domain experts. (vi) last but not least, the language needs to be adequate to generate efficient code suitable for considerably resource-constrained devices, such as wireless sensor nodes."
"for tuning the fuzzy controller membership functions, we run a series of simulations with different controller configurations. for example, figure 5 shows the average absolute error in the heading of a simulated vehicle driving a straight line, plotted against the width of the fuzzy input membership functions. each point in the plot is the average result of 20 simulation runs. as we explain in section 5, the heading controller becomes more aggressive when the input membership functions of figure 3 get narrower. on the one hand, if the controller is too aggressive, it can produce undesired oscillations in steering. on the other hand, if it is not aggressive enough, the follower has low responsiveness and performs suboptimally. figure 5 provides an indication of the optimum value. we currently use a width of 30"
"the scheduling on the follower is complicated by the asynchronous incoming messages from the leader. moreover, for logging and debugging, we also need an outradio task on the follower. as shown in figure 8(b), it is possible to receive data over the radio (the inradio task) while sampling the x axis of the compass sensor. in this situation we try to minimize the time overlap between the compass and radio tasks by introducing two delays: (1) the outradio task is initiated only after compass reader finishes and (2) the sampling of the y axis is postponed until the radio transmission is over. the data processor task has a similar function as on the leader, but its completion triggers the controller inference described in section 5."
"let pex and pim denote, respectively, the price of electricity export into and imported from the utility feeders, the spot price motivating the choices of inner ders in micro-grids needs to follow the constraint:"
"improvement of agriculture field has become biggest challenging for the countries like india, so new technologies have to be adopted. we have implemented a novel methodology of physical parameter monitoring, data display, data integration to the cloud, alert generation and predicting the future values with the help of matlab analysis. we have used temperature sensor, rainfall sensor, light sensor and the moisture sensor. these sensors have been installed in the agriculture field to collect the data, and thus data is mitigated into the cloud with the help of iot hub(thingspeak).so user can have a real time data visualization, with the help of matlab analysis user can predict the future parameter values. by predicting the moisture value user can have control over the agriculture field by using a mqtt, by sending commands. mqtt is a machine to machine communication protocol which is based on pub-sub service. in this we have subscribed to the servers by creating channels, server could be \"iot.eclipse.org\" and \"test.mosquitto.org\". the remote monitoring solution that we offer can be monitored in real time through any remote devices including mobiles or tablets. this provides the flexible for the data visualization, data understanding, and the predictive analysis also given the scope for the farmers to prepare for the advanced data which might appear in the future."
"(1) controller design and evaluation using synthetic, idealized data; (2) tuning the controller parameters using real sensor data from experiments as input; (3) debugging the implementation of the controller on the sensor node platform."
"the proposed system is 4g compatible, it is not limited by the bandwidth and high latency, which is experienced for the gprs based system. at the same time the mqtt protocol itself free, the overall operating cost for controlling mechanism is null in comparison with gsm based system, where each sms's has a cost which is determined by the tariff of the service provider."
"the presented methodology is performed to the studied case. and the results at 12:00 p.m. for market shareholders based on the tiered dynamic pricing scheme are shown in fig.7 . forecasted dg generation and load demand. the following three bars denote the updated scheduling after executing tier i, ii, and iii processes. different color means different ders. it is noted that the blue area represents the electricity quantity difference between generation and load demand. specifically, the total forecasted demand (adding the value of yellow part of each first bar together) [cit] kwh, which is 323 kwh less than the forecasted generation. the decentralized self-balance of this global energy difference can be driven through the proposed dynamic pricing. first of all, by launching the routines in tier i, micro-grids reduce the imbalance energy between generation and demand side by utilizing the response capability of inner ders. therefore, compared with the first bar of each participant, the second bar has much smaller or even none of the blue block. the inner self-balance of electrical energy is enabled. apparently, the utility-scale energy resources operated by dno, 1uess and 1uwt, did not run energy interaction in this stage. specifically, the scheduling of 1uess keeps unchanged. whereas, according to formula (5), 1uwt plans to sell 96.256kwh to utility grid at price 0.342 cny/kwh."
"in fig. 4, optimal soc curve expresses the value of the corresponding qsoc. and pspot,n represents spot price at point n. when spot price comes from pspot,1 to pspot,2, qsoc will adjust qess to point 2 (q + soc ) based on formula (9) . so the price-responsive model that shows the relationship between pspot and qess can be expressed by:"
"-in contrast to the results of ending positions (from figure 15), the values in table iv offer just the relative image of what the nodes \"think\" about their velocity and heading. more specifically, these values include the inherent sensor and integration errors. -the heading correlation coefficient in linear tests is smaller than in square and random tests (0.64 compared to 0.99). the reason is that the heading varies much less in linear tests, as there are no turns, and therefore any small difference in the compass readings of the two nodes has a strong impact on the correlation. -the performance in random tests is slightly worse than indicated by the results of ending positions. a detailed analysis of the logs reveals two main reasons. firstly, during random tests, the leader is subjected to steep acceleration and steering at times. due to constructive limitations, the follower cannot match this behavior, but later on he recovers the difference. secondly, the vehicles drive longer distances in the random tests than in the other tests and thus accumulate higher errors through acceleration integration. however, their velocity is physically limited, which explains why the actual ending results are better than what the sensor nodes compute."
"this result holds only when the compass plane is perfectly horizontal. in practice, we must compensate for the roll (θ ) and pitch (ψ) tilt angles, which can be inferred from the accelerometer output when the system is standing still [cit] ."
"as expected, the choice of a higher-level language does not significantly affect the total number of transmitted radio messages, as shown fig. 10 . nevertheless, as shown in fig. 11, the total amount of transmitted data is significantly higher for the mpl-based applications. this is mainly caused by the fact that the mpl code transmits complete objects that need to be serialized. an implementation providing a similar level of flexibility and features as the mpl code would be far more complex and consequently even more difficult to implement and maintain. at the same time its resource consumption would probably be much closer to the mpl-based implementation. consequently, we conclude that the overhead for supporting a high-level macroprogramming language with object orientation is still reasonable for resource-constrained devices, like wsn nodes."
"in view of these challenges, the key contributions of this article are as follows. first, we devise a miniaturized, low-cost navigation system using low-power wireless sensor nodes equipped with three-axial accelerometers and magnetic compasses. second, we explore fuzzy logic as a lightweight and robust control solution for coordinating the group movement in a leader-follower fashion. third, we report on all development phases, covering simulation, controller tuning, inertial sensor evaluation, calibration, scheduling, fixed-point computation, debugging, and benchmarking. finally, we evaluate the performance of our prototype system through field experiments and we discuss the most important results. visit followme [cit] to see a video demonstration of the system."
"distributed actions can be used to request some action from multiple remote nodes. distributed actions typically define some form of communication between the nodes. the makesense meta-abstraction hierarchy distinguishes between three basic communication patterns. the tell abstraction is used for one-to-many communication. it allows to execute other actions on a set of remote nodes. dually, the report abstraction is used for many-to-one communication, e.g., to request sensor values from a set of nodes. on the remote node, the requested data is extracted by employing a local action that has been associated with the report. finally, collective actions provide a default interface for abstractions that implement peer-to-peer coordination patterns, such as global assertions [cit] or role assignment [cit] ."
"where ql is the energy demand of fl. the formula (13) can be integrated into a demand response function where pl expresses the value of the corresponding demand, ql:"
"the three driving patterns are illustrated in the close-ups from figure 10 . we perform 20 experiments for each type of test. we vary the driving speed, alternate between driving forward and backward (in random tests), and change the path orientation (in linear tests). in each experiment, the distance between the leader and the follower is 2m at the starting line. at the end of the experiment, we measure the final heading of the vehicles (using a regular compass), the relative distance between them, and the distance to the finish line (for linear and square tests). in addition, we log the velocity and heading computed by the two sensor nodes, as well as the throttle and steering outputs of the fuzzy controllers running on the follower. although not an absolute reference, these logs provide useful information about what is actually happening on the two nodes. therefore, figures 11 and 15 represent actual measurements of heading and distance, while figures 12 through 14 and table iv present the values estimated by the nodes from the onboard sensors. details are presented in the following sections. the logs also provide the means to calculate the packet loss as identified by missing sequence numbers. the results listed in table iii show the average percentage of packets lost during the tests. the random tests show less packet loss, possibly due to the fact that these tests were performed closer to the gateway node. 8.2.1 linear tests. the linear tests are particularly valuable to assess whether the follower can maintain a constant heading while imitating the velocity of the leader. figure 11 (a) depicts the ending positions and headings of the two vehicles with respect to the finish line (thick horizontal line at 0m). we see that the follower succeeds to copy the leader movement in most of the experiments. the final relative distance is 3.7m on average, which means that the follower deviates with approximately 9cm per traveled meter (the initial relative distance is subtracted when computing this value). the difference in final headings is 8"
"debugging the nodes in the field is impractical and therefore we rely on lab reconstruction of failed tests. the follower controller cannot be tested without an environment that responds appropriately to its actions, therefore the input values for the follower controller are obtained from simulation. the leader data recorded from failed field tests is used as input to the simulation to reproduce the situation. in order to find platform-specific problems, we run the software on a gateway sensor node with a serial connection to the pc. the pc provides the input values for the software function under test and reads back the results. most of the platform-specific problems relate to different integer size computations. the msp430 is a 16-bit processor, meaning that the c-compiler generates 16-bit calculations by default. if 32-bit calculations are not specified explicitly, 16-bit calculations are generated for the node and 32-bit calculations are generated for the pc. as a consequence, the software works on the pc and fails on the nodes."
"the makesense project improved this situation by providing a unifying framework in which existing and future wsn programming abstractions can be easily integrated. to allow such extensibility, the makesense framework employs the concept of meta-abstractions. wsn programming abstractions can be grouped into classes that aim to solve similar issues and that usually expose very similar interfaces. we call such a group of similar abstractions a \"meta-abstraction\", i.e., an abstraction of abstractions. based on this insight, a hierarchy of typical wsn programming abstractions was developed in the makesense project [cit] . this hierarchy forms the basis of the makesense framework as displayed in fig. 1 ."
"in the last section, the price-responsive behaviors of market members are modeled, providing the action criterions to stakeholders while facing price variation. this section will address dynamic spot prices, guiding ders response to enable the energy balance within a mgc. besides, a clearing mechanism is designed to quantify the contribution of each response behavior during a three-tier optimization. the schematic framework shown in fig.6 will be further elaborated in the next segments."
"a fuzzy controller executes three basic steps: fuzzification, inference, and defuzzification. during fuzzification, the numeric input values are mapped to fuzzy sets by applying the membership functions. based on the fuzzified inputs, the controller infers through its if-then rule set and produces an aggregated fuzzy output. the final control action is derived by defuzzifying this aggregated fuzzy output. in our case, the control objective of the follower is to adapt the velocity and heading according to the leader movements. since the vehicle has separate motors for accelerating and steering, we decompose the problem into two independent controllers, with the benefit of simplifying the design and tuning. the two controllers are structurally identical. the only differences lie in the range of the input values, the definition of the membership functions, and the postprocessing operations."
"limitations. as any inertial navigation solution, errors can accumulate through acceleration integration. without stopping periods or external reference sources (e.g., gps), the error can potentially grow to infinity. additionally, our current implementation does not support dynamic tilt compensation (for example, when moving on inclined terrain). the compass utilization is restricted to environments without strong magnetic influences, typically outdoors. with respect to convoy-like applications, the main limitation is the lack of any information and subsequently control of the relative distance between the vehicles (see also future work)."
the most plausible explanation is a combination between the electromagnetic field around the antenna and the significant power drain when the node is transmitting or receiving. our strategy to avoid this problem is to introduce an additional delay between compass sampling and radio tasks (see scheduling details from section 7.5).
"micro-grid management as a promising decentralized solution for coordinating control of distributed generators, local controllable loads and energy storage units at the distribution network has become a hot research area in both academia and industry. it is widely acknowledged that micro-grid is capable of providing a number of benefits, including resiliency improvement [cit], security enhancement [cit], adaptive ders scheduling [cit], local balance [cit], and investment deferral [cit] . to solve an optimization problem in a micro-grid, a mathematical programming method is often used. a comprehensive review for micro-grid optimization in terms of optimal power flow is presented in reference [cit], which summarizes various objective functions,"
"future work. we plan to pursue two directions for future work. one is to incorporate relative distance estimation (for example, rssi information) into the fuzzy controller, in order to apply our solution to convoy-like applications. the other one is to explore group interactions among heterogeneous moving entities (not only vehicles on wheels) using the same general idea. figure 17 shows the decision surface of the heading controller. the steering output corresponds to the vertical axis and ranges from -1 (maximum left turn) to 1 (maximum right turn). figure 18 shows the behavior of the heading controller. the simulation is instructed to change the desired heading 90"
"in conclusion, error accumulation becomes significant due to changes in tilt angles and increases linearly in absolute value. whether the error can be identified and compensated for at runtime remains an open problem."
"advantages. the method we described enables autonomous mobile team coordination based solely on compact, low-cost wireless sensors and actuators. the solution is not restricted to vehicles on wheels, but supports any moving entities capable of determining their velocity and heading. therefore, it creates a promising basis for both machine-to-machine and human-to-machine spontaneous interactions in the field. another advantage is the constructive robustness to errors: from any initial or intermediate faulty orientation, the follower is able to return to the correct heading, due to the usage of a global reference system. furthermore, making use of a fuzzy controller facilitates the implementation on resource-constrained sensor nodes and handles robustly the noisy sensor data as well as the rough mechanical capabilities of the vehicles. with respect to the wireless communication, the leader-follower model is inherently scalable and tolerates transitory packet losses."
"we propose a novel iot sensor network for monitoring the environmental parameter in an agriculture field. mitigating this data to the cloud through a secured iot hub (thingspeak), and then run and predictive analysis and the machine learning on the gathered data (mat lab deployed at thingspeak).the proposed system overcome the limitations of traditional gprs based system through protocols like mqtt, secured http, which not only ensured that the data is safe and secured, but the entire communication is over an authorized secured socket layer which gives immense security to the data.the proposed system also have bandwidth compatible and cross level compatible, therefore the client level solution can be run either on the mobile, pc or on the tablet. the remote monitoring solution that we offer can be monitored in real time through any remote devices including mobiles or tablets. this provides the flexible for the data visualization, data understanding, and the predictive analysis also given the scope for the farmers to prepare for the advanced data which might appear in the future."
"in this paper, we describe how this conceptual framework can be instantiated by means of a concrete language and we also describe a compiler to translate that language into efficient executable code. to sustain the extensibility of the conceptual framework, the language is object oriented and inspired by java and thereby easy to use for programmers familiar with java or c++. the language differs from java to support efficient compilation to resource-constrained wsn devices and to support the extensibility of the above conceptual framework. the compiler offers a plug-in interface to support addition of new abstractions and automatically distributes application functionality among the gateway and the sensor nodes."
"similar to the velocity simulation, we consider two types of uniformly distributed noise: steering noise s n (related to inaccuracies in the steering mechanism and external influences, such as a rough road surface) and compass noise c n . in order to create a more realistic response of the steering, we apply a running average filter at the controller output. this corresponds to the actual behavior of the vehicle steering, which exhibits a certain delay in reacting to high rates of changes."
"where, kcha is the charge coefficient for energy exchange. in this operation tier, p agg spot can be solved through formulas (20) (21) as shown in fig.6 . the micro-grids, whose spot prices still satisfy the constraint (19), are cleared at respective p i spot and the utility-scale ders are settled on the basis of p agg spot . besides, the coordination profit of which can be calculated using the method proposed in section 3, namely the orange area in fig.7."
"we implement the leader-follower system on two toy cars. the follower is modified to allow the sensor node to control its actuators. the leader car remains unmodified apart from the sensor node attachment. for a video demonstration of the system, visit followme [cit] ."
"the experimental results offer the possibility to validate and refine the simulation vehicle model presented in section 6. for this purpose, we use the data recorded during the field tests as input to the simulation framework. figure 14 shows the simulated velocity and heading of the follower, as compared with the perceived velocity and heading derived from sensor data. we notice that the field results validate the simulation output. the most significant difference occurs in the steering control output, with the real follower steering slower than in simulation. the reason is not the maximum steering angle the vehicle can achieve, but rather the delay between a change in the steering signal from the controller and the actual change in the heading of the vehicle. table iv gives a quantitative view on how the simulation matches the real behavior. similar to section 8.3, we provide three statistical indicators for each type of test: the mean and standard deviation of the absolute error, and the correlation coefficient of the recorded signals. in general, the real and simulated behaviors match well. the correlation coefficient in linear tests is relatively small for similar reasons as explained in section 8.3. a detailed analysis of individual tests shows that a more accurate friction model is needed to further enhance our simulations."
"we noticed that when we do beat by beat analysis, there is a significant variance in the following features, namely: the energy of beat, power in the region above 200 hz, systole to rr ratio, diastole to rr ratio. the first 2 of the 4 features account for variation in amplitude component of the signal, and the remaining 2 account for the time component variations."
c. ders located in micro-grids might belong to various owners who may select any appropriate aggregator. d. the target of mgc commercial operation is to enable autonomously optimized coordination of ders while satisfying the interests of market participants.
"to implement actual applications, several abstractions need to be combined and augmented with application-specific functionality. this requires a programming language that allows to define the interaction between individual programming abstractions and to implement algorithms for custom data processing. a number of fundamental requirements for such a language can be defined:"
most object-oriented languages primarily employ dynamic memory allocation for objects and even though it is convenient for the programmer-especially if supported by garbage collection-it is ill suited for memory-constrained devices and reduces the efficiency of the program.
"the logical neighborhoods abstraction employs a custom declarative language to define membership of nodes in a specific neighborhood, used as domain-specific language within mpl. this use of the functionality can be seen in lines 1-7 of listing 2. in this example, a logical neighborhood hightemperature is defined that contains all nodes equipped with a temperature sensor and that read a high temperature value. the threshold value x is left as a parameter in the embedded code, and is later bound to the actual value in the main mpl program by means of an appropriate bind call in line 10."
"a. at least two micro-grids equipped with a variety of ders exist in mgc. b. multiple aggregators, as agencies of ders operators, are allowed to conduct transactive energy business in mgc through short-term energy trading for economic profits pursuit."
"to illustrate these aspects we employ logical neighborhoods [cit] and a stream abstraction as an example. a wrapper has been implemented in the makesense project, that integrates an existing implementation of logical neighborhoods in the makesense framework [cit] by instantiating the target meta-abstraction. here, logical neighborhoods is used in combination with the stream abstraction that implements the report meta-abstraction. the stream abstraction has been created from scratch within the makesense project [cit] ."
"for the intended usage scenarios of mpl it is often necessary to execute several tasks in parallel. to support this in a user-friendly way, multithreading functionality is needed for the language as also indicated by requirement iii. the multithreading capabilities of mpl can be accessed via an interface that is closely modeled after the java thread interface."
"1) blink to radio. this application scenario represents oneto-many communication. such communication patterns are often used to distribute commands or configuration settings to a number of nodes. during run-time, the gateway process regularly sends a command to all sensor nodes requesting them to toggle their led. 2) collect. this application scenario consists of a simple data collection application. data collection is a typical task for wsns and as such is an important component of many real world applications. in the evaluated application, temperature and light sensor readings of all nodes are periodically sent to the gateway. at the gateway, the readings of all nodes are averaged and the result is reported to the user as a command line output. 3) hvac application. the last application, originally developed in the makesense project, implements a simple ventilation control system that regulates ventilation based on co 2 readings. this scenario represents a simple realworld wsn application. our setup consists of sensor and actuator nodes deployed in two rooms. the control process for each room is offloaded to one of the nodes located in the respective room. all applications were implemented by an average programmer without a strong background in wsn programming. before implementing the applications, he was provided with an introductory tutorial for the respective technology. during the experiments, the mpl and the contiki/c programs was executed in identical simulated environments with five to six nodes and a gateway 2 . the wsn was simulated in real-time with cooja, while the gateway code was executed in parallel in a 64-bit linux environment. the latter communicated with the simulated network via a serial socket and a simulated interface node 3 . each application was executed until a scenario-specific termination condition was met. the same termination condition was used for the mpl and the contiki/c implementation."
"-in the interval 4-8s, the leader accelerates to a velocity higher than what the follower can manage. during the rest of the test, the follower succeeds to keep up. eventually, the velocity is reset through the motion detection technique explained in section 3.1. the two cars end up very close to the finish line: 0.1m and 0.3m. -although the final difference in heading is only 6"
"using the debugging method from section 7.7, we determine how fast the controller and its subcomponents execute. table ii shows the typical execution times. the time spent in the fuzzification, inference, and defuzzification stages for the heading and velocity controllers is identical. notably, the execution of the whole heading controller takes longer than the velocity controller because a modulo function is used to map the heading error inputs to the range [−π, π] (see eq. (4))."
"1) the number of source lines of code is employed as an approximation of the programming effort for each application. we are aware that source lines of code are a very imprecise metric, especially if comparing different languages. nevertheless, it can provide an initial idea of the relative complexities of the evaluated applications if consistent code formatting is used. 2) code size is an important metric for wsn applications, as sensor nodes typically only possess limited program memory. 3) memory consumption also needs to be kept low as random access memory is also a severely limited resource on sensor nodes. we need to distinguish the space required by statically allocated objects and the amount of heap space employed by dynamic memory allocation. especially the use of dynamically allocated memory should be reduced, as dynamic memory allocation requires significant over-provisioning of memory resources. we measure heap allocation by linking the applications to a modified malloc implementation that tracks heap usage. effects like fragmentation are not taken into account. 4) to compare the computational overhead, we count the cpu clock cycles spent on a typical execution of an application in a controlled environment. 5) the communication overhead is assessed by recording the number and size of radio messages sent during application execution. ins wsn, energy consumption is often dominated by wireless communication, so that this metric also provides some insight on energy consumption."
"in tier i, a micro-grid is motivated by managing its inner ders to make profits while keeping energy balance of dg, ess, and fl. and the ders in a micro-grid determine the operation strategy by optimizing their benefits based on the varying spot price and the proposed price-responsive models (5) (10) (15), forming their optimal supply/demand curves. therefore, the objective of i-th micro-grid in tier i is to solve the intersection of the supply and demand curves [cit] . that is to seeking a spot price of i-th micro-grid which makes the its net demand δq"
the above graph shows that light value is changed with change in the sensor. the below figure shows the graph of moisture value. the value is being analyzed by taking vaious reading of the soil value
"as depicted in figure 1, we consider the scenario of a leader vehicle, whose trajectory has to be copied by follower vehicles. the result is a moving ensemble that can be controlled from a single point or can follow a unique mission plan deployed only on the leader. theoretically, synchronous movement is achieved when all the vehicles maintain the same velocity and heading with respect to a reference system. when moving, the leader computes its velocity and heading from sensor measurements, and broadcasts this data periodically to the followers. each follower determines its own speed and heading, compares them with the information received from the leader, and takes the necessary action to correct its trajectory if necessary."
the random tests involve steering in random directions and large variations in velocity. each of the 20 experiments lasts 30 seconds (there is no finish line). figure 11(c) shows the relative distances and headings of the two vehicles. we obtain an average of 6m final distance between vehicles and a final heading difference of 8
"training data [cit] consists of 3153 recordings of which 242, labeled extremely noisy to be correctly classified are eliminated as no special method is employed to detect such a noise, some of which don't even contain pcg signals. 296 of the rest that is marked for validation by physionet are taken as a validation set and are removed from the training set. thus, the training data comprises of 2615 recordings, containing 454 abnormal recordings (class 1) and 2161 normal recordings (class 0), roughly a class ratio of 1:4, and the validation set comprises of 296 samples with a class ratio of 1:1. test data is not revealed to the participants."
"where q + soc is the optimized qsoc. it doesn't need to be adjusted any more at q + soc . thus, formula (6) can be rewritten as:"
this method has the advantage of avoiding collisions implicitly. the drawback is that scheduling the sampling and communication tasks on the follower becomes problematic (see section 7.5).
"where a x and a y are the components of the acceleration vector. the time step for integration is considered equal to unity. a known problem of integrating acceleration is the accumulation of errors in time. if no external reference is available, the error can potentially increase to infinity. our assumption is that the vehicles do not move continuously, but also have stationary periods during which the velocity estimate can be reset to zero. for determining the \"standing still\" situation, the sensor nodes continuously analyze the variance of the acceleration over a sliding time window with respect to a threshold determined experimentally. the downside of this approach is that it introduces a certain delay between the actual moment of stopping and the detection of standing still."
"programmers can customize an action's behavior by using modifiers, e.g., to select the set of nodes that participate in a report. this separation of concerns enables a more flexible interaction among abstractions. the makesense framework provides two subtypes of modifiers. target modifiers are used to implement the aforementioned selection of nodes, to limit the scope of operation of any distributed action. data operators can be used with report actions to define additional data processing to be applied to the collected data, such as aggregation."
"the rest of this paper is organized as follows. in section 2, the overall architecture of mgc is detailed in the description of the market participants and their responsibilities. the price-responsive models for dg, ess, and fl are deduced based on generation risk, decouple soc value, and the elasticity of demand in section 3. section 4 details an iterative three-tiered optimization according to the power market equilibrium of supply and demand., a realistic case study is demonstrated in section 5 to verify the efficiency and advantages of the presented transactive energy scheme. the conclusion follows in section 6. a typical distribution network has a series of ders in different geographical positions, owned by different families or different organizations. due to the lack of awareness on global operating information, it is difficult for owners to manage and trade energy optimally. however, the owners of ders can entrust the scheduling of their assets to one of the professional aggregators. in this case, mgc is formed, which has the problems of the influence of geographical location, the management optimization within an aggregator, and the competitive trading between the aggregators. as shown in fig 1, mgc is composed of a number of micro-grids with dg, ess, and fl connecting to the widely distributed connection points. the larger scale resources, named as utilityscale devices, might be connected directly to the main feeder. in the studied mechanism, several aggregators are authorized to participate in energy trading in the range of mgc. the net demand of an aggregator monitored from smart meters must equal its pre-scheduled profile. otherwise, the aggregator will be penalized by the distribution network operator (dno) according to the quantity of imbalanced electricity. the risk of penalty encourages the aggregators to utilize the local market to balance fluctuations. in addition, a trading platform is established for mgc to facilitate energy trading among aggregators. therefore, mgc, as a cluster of neighboring micro-grids connecting to the same feeder, is technically defined as follows:"
"the organization of this work is as follows. section 2 contains the related work, section 3 is about the proposed work, and section 4 covers the implementation and result analysis. finally conclusion and future work has been discussed in the section 5."
"2 the results clearly show that the compass cannot provide 2 the influence pattern is consistent, as the results of the two experiments correlate well. in figure 6 (e), an artificial offset is introduced between the two signals, in order to enhance visibility. useful measurements when moving close to large metal objects, making indoor use infeasible in most cases. additional experiments show that the magnetic field caused by the follower's motor has a significant influence on the compass sensor as well. this effect requires the compass to be shielded or placed at sufficient distance from any actuator."
"support for object-orientation in mpl had to be implemented in c in a standard-compliant way relying on structures and function pointers. the approach taken is conceptually similar to the approach taken by c++ [cit] . most notably, dynamic dispatch is implemented with the help of virtual method tables instead of relying on the more flexible but also more memory-demanding hash-table-based approach typically found in java implementations."
"surprisingly, the random tests show better performance than the linear and square tests with respect to the finishing position, although the distance covered is larger. we identify two factors that can contribute to this result. firstly, repeated turns in the same direction (as in the case of square tests) may lead to accumulated errors, while random paths and velocities tend to cancel out the errors (see also section 8.4.2). secondly, the follower sways less than the leader when steering (random tests involve more steering) and consequently can match the velocity of the leader easier than in straight line sections, where the difference of engine power becomes more visible. figure 15 summarizes the results of final differences in distance and heading. the initial relative distance of 2m between cars is not subtracted from these results. the error bars are plotted with 5 and 95 percentiles. in order to better characterize the continuous performance of the leaderfollower system, table iv provides detailed statistics from all the logged tests. for both velocity and heading, we compute the mean and standard deviation of the absolute error between the two vehicles at any moment in time, as well as the correlation coefficient 3 of the signals recorded in each test. we make the following observations."
"therefore, to meet requirement vi, mpl encourages the use of static memory allocation. in contrast to java, it is not only possible to allocate new objects on the heap, but instead the language also provides additional operations to support different allocation schemes. like in java and in contrast to c++, objects are always accessed via references. to support alternative allocation strategies, we introduce two new allocation operators: static and auto, which return a reference to a static global object or an object allocated on the stack."
"to this end, the implementation of a programming abstraction within the language consists conceptually of three distinct components: (1) an mpl class, (2) a run-time module, and (3) (optionally) an mpl compiler plug-in. the mpl class serves as a means for mpl code to interface with the abstraction. abstraction classes slightly differ from regular mpl classes in that their methods are typically not implemented by mpl code. instead, each abstraction provides an additional run-time module that implements the required functionality in low-level c code. this increases efficiency and allows one to reuse existing implementations. in addition, the implementation of programming abstractions will typically be conducted by wsn experts that are already familiar with c programming on embedded devices. finally, as indicated by requirement ii, programming abstractions may employ their own domainspecific language. to support a domain-specific language, the abstraction also needs to provide a compiler plug-in to translate the domain-specific code into code executable on the target platform. these plug-ins employ a compiler interface that is described in more detail in section vi-b."
"all communication-related tasks are handled by distributed actions. direct access to functions to manually send or receive individual messages is intentionally not provided by the default implementation of mpl to shield the programmer from lowlevel details of communication. nevertheless, such facilities could be provided by dedicated abstractions, if needed."
"sensor placement is an important aspect that can seriously affect the system performance. both the accelerometer and the compass need to be mounted rigidly to the vehicle frame, in order to minimize the level of vibrations during movement. because we use different vehicles for leader and follower, the placement of the sensors is not identical. to account for the difference in placement we use the calibration procedure explained in section 7.3. as indicated by the preliminary evaluation presented in section 7.1.3, the placement of the compass sensor requires additional care. during our experiments we have identified the following factors influencing the compass."
"where sagg is the full set of aggregators, and h or j represents an aggregator in the set. formula (23) means if aggregator h sells x kwh extra electricity to the market (through increasing inner generation or reducing electricity consumption), its spot price will be amended to p h sell . similarly, formula (24) means if aggregator j purchases another x kwh from market, optimal price of it will be reduced to p j buy . then, the aggregator h and j report these two optimized sport prices to trading platform. if the following formula is satisfied, the trading between aggregator h and j is achieved. the mgc converges to the optimal solution when no more trading is successfully deal with. the optimal solution can be found through various optimization algorithm. in this paper, the simple well-known dichotomous exhaustive search method is adopted."
"energy balance in tier i is merely related to the resource conditions in one micro-grid, the pricing scheme of which is unable to motivate the globally optimization for a cluster of micro-grids. to address this challenge, the process of aggregator scheduling is adopted in tier ii to coordinate renewable generation and consumption among the micro-grids and utility-scale ders. the stakeholders are therefore allowed to utilize their flexible resources to seek higher profit in a wider area."
"some programming abstractions employ embedded code to enable extensive configuration, as described in section v-c. the abstraction-specific code cannot be handled by the mpl compiler itself, instead these abstractions need to provide compiler plug-ins to analyze and translate the abstractionspecific code. the mpl compiler plug-in interface allows these plug-ins to communicate with the mpl compiler at compile-time. each compiler plug-in is essentially an independent little compiler with its own parser and code generator. due to the fundamentally different nature of the objectoriented mpl code and the typically declarative embedded code, plug-ins cannot reuse the parsing and code generation functionality of the mpl compiler. fig. 3 gives an overview of the interaction between the mpl compiler and the compiler plug-ins. embedded code fragments are extracted by the mpl compiler and passed to the compiler plug-in provided by the abstraction that uses the code fragment. the compiler plug-in translates the domain-specific code into c code. the resulting c code is compiled with the native tool-chain and linked with the compiled binary image of the mpl program. the generated code can communicate with the abstraction-specific run-time component."
"to capture the temporal dynamics, we take pcg signal beat by beat and assign each beat a symbol/category based on different thresholds set on features (ratios of systole intervals to rr interval, diastole intervals to rr interval, beat energy, the power of frequency component above 200 hz). thus, a sequence of symbols for the entire signal is obtained. we then extract features out of this sequence for classification. one of the ways we employed is to create a markov chain with symbols being the states of the matrix and the resulting transition probabilities are used as features. these features along with marginal probabilities of states and rest of the acoustic features like sample entropy, instantaneous frequency analysis etc. are used to train an ensemble/bag of 4 class boosted tree classifiers [cit]"
"pixel data can be compressed or not, when the data is transmitted as compressed format, the value representation is ob, on the contrary, the value is ow. for the uncompressed pixel data, the sequence is usually from top to bottom, from left to right and the data are encoded and stored as a continuous bit stream. for the compressed pixel data, they can be stored in segment and delimited by a series of bound items to support the image compression process with un-pre-known length."
we therefore learn the dictionary pair d illum and d norm on which the sparse coding coefficients of two illumination conditions have stable bidirectional linear transformations.
illumination variation is one of the major challenging tasks which humbles the performance and robustness of face recognition systems. it is shown that the variations of signal intensities on the same face images due to illumination conditions are often larger than structural variations of the face identity [cit] .
"overall, the face illumination normalization task is processed in two steps. we first explain how to build an enhanced dictionary-pair based on the patch-pair clustering. then we describe how to generate a normalized face image by using the pre-learned dictionary-pair. beside of simply implanting the dictionaries for transformation, the cluster information is also considered to adjust the performance of the normalization task."
"the behavior of a c4isr system is specified in an application activity diagram model and it is the main concern of the paper. such model is built with a uml paradigm which extends uml 2.1 constructs of class diagram and activity diagram with the meta-concepts of capability ontology. it describes which operational nodes participate in the activities to realize the mission goals, what activities are executed in the course and their time sequences, what capabilities are required, and so on."
"the object of grayscale operation is image function and the structure element is a grayscale form of a small window, that is, the grayscale value of each point in the window need be determined when the window is determined. the current point of the image corresponding to the form operation is usually made as a center in the structure elements."
"the tab.2 shows the mappings between add modeling constructs and generated simulation instances. applying a programming language, the simulation program can be generated from those instances. after translating aads into instances, we are prone to use proper oo programming languages to realize and execute them. for example, c# language is used to construct the class templets of operationalnode, capability and info, as show in fig.3 ."
"dicom is the standard used for the storage and transmission of medical images which can provide the interface standards and protocols for the manufacturers and users of the medical imaging equipments [cit] . with the extensive use of pacs in the hospital, the understanding of the dicom standard has become increasingly important, and physicians make more and more needs for the post-processing of the medical images. then, the interpretation of dicom medical image files, the reading of medical image data, the display and processing of image processing are very important."
"in this paper, a novel face identification framework based on dictionary learning and illumination normalization was proposed. firstly, a couple of dictionaries are jointly optimized from normal illuminated and irregular illuminated face images. secondly, a model based on gmm was proposed to improve the framework's capability to modeling the data under varying lighting conditions. we adapted each model to a certain portion of the selected samples and then combined them together to form a complete model. this paper is organized as follow. we first provide a study on the state-of-theart methods for reducing illumination effects on face images. we then demonstrate our contributions, the effectiveness and accuracy of the proposed algorithm."
"(11) media storage application profile this part provides the application selection mechanism and strategies in media storage, gives the general pattern of the mechanism and some examples."
the program implementation of erosion operation and dilation operation are not difficult [cit] . the part code of erosion using visual c++ is given as follow.
"in order to ensure the quality of edge detection, we do not directly binary the raw data of dicom image in this method of edge detection, but first do opening and closing operations by making them as grayscale images, which requires to adjust the data range and variable types of grayscale operations by morphological, so as to meet the features of large scope of dicom image data relatively."
"(1) be appropriate for network environment the early dicom versions were only used for point to point data transmission, but dicom3.0 supports network environment based on the osi, tcp/ip and other common industry standards, so as to create the conditions for telemedicine."
"the popularly applied methods include petri net [cit], executable uml (xuml) [cit] etc. but when the petri net applied, the requirements models built with uml have to be transformed into those of petri net, and some part of model information might be lost during the transformation due to the different model semantics. xuml is designed to describe the software behavior from the view of program execution and cannot directly applied to describe the behavior of the models of system requirements and architecture unless the modeling semantics are refined and the action semantics are redefined."
"to overcome the face image variations due to changes in different lighting condition, many methods have been proposed. existing methods addressing this problem mainly falls into two main classes: (1) passive methods and (2) active methods. the passive methods aim to overcome this problem by exploring the visible spectrum image in which face appearance image has been changed by different lighting variations. while the active methods employ active imaging techniques to obtain face images that are captured in a specific illumination condition. in this study, we mainly focus on the passive methods, since our contribution is also a passive method."
"in many applications, the accuracy of face recognition systems in some specific environment can reach a satisfactory level. however, it is still challenging in some uncontrolled environments. for example, the face images that are captured with variations in illumination, different face poses and different expressions. in particular, the variations of the illumination conditions, which cause dramatic signal intensity changes in the face image appearance. this is one of the most challenging problems for a practical face recognition system. specifically, different directions and energy distributions of the ambient illumination as well the 3d complex structure of the human face, leads to huge differences in the shading and shadows on the signal intensities of the face images. these intensity variations of the face images pose another difficulty which can be much larger than the variations due to different face identities. combining together, the variations of signal intensity of both face appearance and local face features caused significant problems for automatic face identification."
"the dilation is the dual operation of inverse operation of erosion, and it can be seen as the erosion operation to the complement set of the original image."
"where we try to learn the dictionary pair d illum and d norm from the patch sets, while keep the sparse code ∧ of the illuminated patches and the normal patches being equal. we therefore combine these objective functions (eq. (9) and (10)), and force the irregular illuminated and normal illuminated image patches to make the same sparse codes, by considering:"
"architectural analysis of information systems (is), such as command, control, communication, computers, intelligence, surveillance, and reconnaissance (c4isr) system requirements is a hard and challenging work. most architecture development methods, such as uk ministry of defence architecture framework (modaf ) [cit] and us department of defense architecture framework (dodaf ) [cit], recommend unified modeling language (uml) to model the capability concepts and c4isr requirements. dodaf provides a set of uml meta-models to define its meta-model data groups [cit] . in our early research, capability requirements modeling language (crml) was proposed and applied to c4isr capability requirement analysis [cit] . crml extends uml meta-model [cit], with the various views such as class diagrams and activity diagrams etc."
edge detection is a basic problem in computer vision and image processing which objective is to identify the points which brightness changes apparently in digital images or to use an algorithm to calibrate out the image edges.
the conditional probability is then calculated with respect to eq. 7 using the mean vector and covariance matrices computed in the t-th step. (b): update the priori probability of models as
"with the clustering results obtained, an additional problem comes out: besides the similarity of the patch textures, what other property can we summarize for each model? to further analyze, we learn the distribution of the patch locations that are concerned by each cluster. for one cluster, we highlight the image areas where the involved patches come from, so more frequently one location is related, the brighter that"
"bitmap file header contains the file type, file size, storage location and other information, and it is defined by the structure bitmapfileheader in windows, the length of this structure is fixed for 14 bytes."
"the role of dilation operations in mathematical morphology is to combine the background points around the image into the image. if the two objects are very close with each other, the dilation operation can cause two objects connect together. dilation is useful to fill the holes after image segmentation."
"palette is optional. if the image has palette, it is actually an array and creates the corresponding relation between array and colors, the value is the number of colors used for the bitmap. the type of each element in the array is a rgbquad structure, accounting 4 bytes, it is defined as follows: 1 byte is used for the blue component, 1 byte for the green component, 1byte for the red component, 1 byte for padding (set to 0). the image data are set after the palette [cit] ."
"the identification accuracy of different processes is presented in table 4 . based on the proposed method, dl-in, the face identification accuracy is significantly improved."
"in this work, we have presented a dictionary learning algorithm for illumination normalization (dl-in) problem. based on a jointly optimized dictionary-learning strategy, we trained a dictionary pair from the irregular illuminated and normal illuminated image coupled patches. for enhancing the ability of adapting more complex distribution, we propose to use a gmm model for image patch pre-clustering. for illumination normalization, we fused the normalization results from each cluster. according to the face identification results, the effectiveness of the proposed patch-based illumination normalization is demonstrated. the proposed dictionary-learning strategy is able to be adapted for face aging simulation or for face sketch synthesis tasks in the future work."
"where the function f() is called grayscale transformation function, which is used to describe the conversion relationship between the input and output grayscale values. the point operation can be completely determined if the grayscale transformation function is made."
"comparing the bmp image with the standard dicom image format, we can see that the bmp image includes only the characteristics of the object image corresponding to the image information in dicom image, lacks of a range of information and other features which need be added to follow up. we can manually add them in the process of capture, can also obtain them from the patient information database. but, some characteristics of image iod information are required to obtain from the imaging equipment, such as the current window width and position, pixel space ratio [cit] . thus, the software programming process of converting the bmp file into dicom file is shown in fig. 2 ."
"(2) the effect will be weakened if the size of brightened detail of the input image is smaller than the structure element and the weakened extent depends on the grayscale value surrounding these brighten details and the shape and amplitude of the structural elements. the grayscale value of points which grayscale value of edge part is relatively large by the erosion operation can reduce. then, the edge will draw back to the areas which grayscale value is greater than adjacent areas."
"(3) the image data of (7fe0, 0010) in pixel data element is generally 16-bit or 12-bit, we need adjust the window width and window bit of the original data into 8-bit grayscale data. the so-called window width is the range of the image data display, window bit is the center value of the image data. we can adjust them according to the following equation."
"the main contain includes the data formants and directory and management specifications in storage process. the more key is the dicomdir file which plays an important role in the directory classification of the images. (15) security profiles the dicom standard contains very detailed personal information of patients. the data security in the processes of storage and transmission is critical, and the implementation of encrypting transmission of information is necessary. in this chapter, the dicom standard working group cites the widely used information encryption standard to promote the transmission and storage more reliable and secure [cit] ."
"( the code of pixel data is determined by distribution bit number, storage bit number and the highest bit number, and the distribution bit number must be greater than the storage bit number. the pixel data of dicom image is often 16-bit or 12-bit. if 16-bit format is selected, each pixel has two bytes; if 12-bit, the bytes distribution of per pixel is more complex, and we can determine the distribution bit number, storage bit number and the highest bit number by distinguishing the elements values of (0028,0100), (0028,0101) and (0028,0102)."
the concrete realization process of erosion operation is the process of using structural elements to fill image. this process depends on shift which is a basic euclidean space operation.
"from the architectural viewpoint, the concept operationalnode defines a logic node which possesses capabilities for completing a mission. the communication requirements between nodes are specified by the needlines, and exchanging information associated with the needlines describes the contents of communication. capability describes the resources, such as information systems, weapon platforms and materials, which are owned by an operational node and which are required for a mission. operationalentity describes the participants of activities. the operational entities are the main component of an operational node which executes activities. activity describes a number of operations to realize a mission. mission defines a business goal and task details of an enterprise."
"for comparison, we choose the discrete cosine transform (dct) [cit], the discrete wavelet transform (dwt) [cit], the well-known illumination invariant features, self-quotient image (sqi) [cit] and local binary pattern (lbp) [cit] are also employed."
(3) information defining object this chapter detailedly gives the composition of information objects and concept connotation abstracted from the medical reality of the dicom standard. it is an important part in studying the standard and in clinical diagnosis.
"(5) contour extraction contour extraction means to vectorize the edge lattice vector. the method is to search along the image boundary, to store the coordinates records of points searched on the boundary in the points list. the result is that each contour line is represented as a point column. the boundary in the edge image got by this method is closed and single-pixel wide. we can use the more efficient eight-step tracking algorithm to vectorize them."
"in this paper, we suggest a method of generating an executable model for requirements validation by providing action semantics of domain models, focusing on the domain of c4isr system architectures. firstly, a capability meta-concept model is provided extending the uml constructs. then, an algorithm mapping application activity diagrams (aads) to simulation instances is proposed. and, a case study is given to show the final executable codes and demonstrate the applicability of the method."
"the swimlanes of the activity diagram represent operational nodes. for example, xxx mintercepton is an instance which is defined by the class missileintercepnode and owns a capability of capabilityofmissileintercept which has valued properties effectiverange (type: int, value: 100, unit: km), firsthitratio (type: float, value: 0.85), etc."
"the illumination plane ip(x, y) of an image i (x, y) corresponds to the best-fit plane can be expressed by a linear approximation of i (x, y), given by"
"the rest of the paper is organized as follows. section ii describes the research background, including architectural modeling and simulating methods. section iii describes the language of capability requirement modeling and the extending method of aads. section iv gives an explicit and formalized definition of aads. section v discusses in detail the algorithm of generating executable capability requirement models. section vi demonstrates the applicability of the method by analyzing a missile intercept example."
"an enterprise is one or more organizations sharing a definite mission, goals and objectives to offer an output such as a product or a service [cit] . there are many different approaches to enterprise system engineering and integration, which leads to various enterprise architecture frameworks, such as generalized enterprise reference architecture and methodology (geram) [cit], zachman framework [cit], dodaf [cit], the open group architectural framework (togaf) [cit], treasury enterprise architecture framework (teaf) [cit], etc."
"the set of images of a convex lambertian object obtained under a large variety of lighting conditions can be approximated by a low-dimensional linear subspace. e.g., basri and jacobs [cit] proved that the set of all lambertian reflectance functions obtained with arbitrary distant light sources is close to a 9-dimentional linear subspace. [cit] presented a generative appearance method for recognizing human faces under variation in lighting and viewpoint conditions. zhou and chellappa proposed a 2-d approach, which combines lambertian reflectance model with eigen light field [cit] . in their work, the authors integrate three factors of identity, pose and illumination, so that the three components can interact with each other in a mutually manner. this combination model meets the assumption that illumination-invariant identity is not invariant to different poses. it can be generalized to novel illuminations without any limited identity, pose or illumination."
"among them, the dodaf provides an architectural modeling methodology for defense information system, nowadays referred to c4isr system [cit] . such mission-critical systems are growing in complexity as more computing devices are networked together to help automate tasks previously done by human operators [cit], which brings tremendous difficulties in system engineering and integration. the system engineers have to pay great efforts on simulation to validate the built architecture models."
"therefore, the coupled dictionary-learning (dl) problem turns into a single dictionary-learning (dl) task. eq. (11) is then solved by a two-step algorithm. we firstly fix the dictionary pair d c, leave eq. 11 as a lasso problem. a number of l1-optimization algorithms can be employed for solving it effectively, such as lars [cit], fista [cit], etc. in our work, we select lars [cit] (least-angle-regression) as the l1-optimization solver for its stability and efficiency. then in the second step, we fix the sparse matrix ∧, and update the dictionary pair as:"
"recalling the discussion in 3.1, besides the basic patch-pair dictionary learning, multi-models are also constructed based on the pre-clustering of image pair patches for the enhancement of normalization performance. therefore, to normalize an illuminated face image i x, choosing the appropriate transform model is critical. thus, in this section, the normalization process is described in two steps. above all, the normalization task is firstly initialized by the model selection. supported by the prior clustering results, we are able to calculate the posterior probabilities for choosing the proper transform models. afterwards, with respect to the selected illumination models, to meet the goal of illumination normalization, the illuminated image is projected into the chosen dictionary models and then the normalized image i y can be synthesized."
"(2) respond to data exchange and related instructions the older versions are confined only to data transfer, but dicom3.0 specifies and defines the semantics of the instructions and data by using the concepts of service classes."
"erosion and dilation are not reciprocal and their combination constitutes the other two basic mathematical morphology operations, opening and closing, the former is first erosion and last dilation, and the latter is first dilation and last erosion."
"consider that we have k models/clusters, denoted as m 1, m 2, ...m k then the probability of a patch-pair (x i, y i ), where (x i, y i ) r p are patches extracted respectively from the normal illuminated images and the irregular illuminated images. the conditioning on the k-th model is:"
"after the file format conversion, the medical image can be displayed. the software are programmed by using visual c++, and the image display is implemented by the class cview [cit] . the function ondraw() can convert the image to a device independent bitmap (dib) and the display it. the specific operations are related to two functions, bitblt() and stretchblt(). the former function can copy the bitmap pixels from a memory device context to display device environment or printer using logical coordinates. the latter one also copies a bitmap from one device to another scene."
"(1) introduction and overview it briefly introduces the scope of the dicom standard relating, points out the background of standard drafting and its meaning and purpose, and lists the other standards which dicom standard references."
"although the edge of human tissue is generally smooth, there are sharp corners, burrs, and so on, due to the noise effects and the limitations of imaging technologies. binary opening and closing operations can smooth the edges."
"firstly, according to the translation algorithm, the aad can be translated into simulation instances, for example, the instances xxx mintercepton and oncapability are shown in fig.5 . they look like executable programs, but are only pseudocode templates, which can not be executed. they have to be further translated into ones in a programming technology, for example, the code document object model (codedom) [cit] . codedom is a kind of mechanism that contained in .net framework. it is used to generating modularization code and compiling dynamically."
"(4) service specification class in this chapter, the standard divides the communication types which may be involved into several detailed equivalent categories in image communication, regulates and defines them."
"region will be. just as demonstrated by figure 3, each subfigure shows the distribution of the involved patch locations in one cluster, where one can notice that besides the similar textures of these patches, there also exits a distinctive spatial distribution in each cluster model. instead of being randomly scattered in the image, the patches with similar texture trend volume 6, 2018 to concentrate around the same face structures, which present another prior information for the afterwards face normalization step."
"pixel data element (7fe0, 0010) is the most important data unit of dicom file, and it contains the necessary data for medical image display. the other data elements closely relating to pixel data element are as follows."
"the computation of dilation operation is also carried out point by point. the operation result is the sum of grayscale values of a local point and the corresponding one in structure element, and we should select the maximum one. contrary to erosion, dilation can increase the grayscale of the whole image. the results of dilation operation of the parts which grayscale values change greater have larger difference contrary to the original image. for the parts which grayscale values change more gently, the grayscale of all points change very little in addition to increasing the size of core values by dilation operation. after dilation operation, the edge has been extended [cit] ."
"firstly, we should define some parameter variables of image stored, and initialize the variables. secondly, according to the requirements of transferring syntax, it traverses the relevant data elements of the dicom files, then extracts the useful data elements from 0002, 0028, 7ef0 groups, respectively stores them into the previously defined variables, and closes dicom files. it converses 12-bit or 16-bit image data into 8-bit grayscale data. finally, it opens a blank document. in accordance with the requirements of dicom files, the data extracted from dicom files will be written to the new file."
"(1) to obtain image information is actually the traversal of all the data elements of dicom file. when traversing the data elements, some information contained in data elements is irrelevant to the image, in order to improve the speed of program traversal, we can only read the useful data elements."
"mathematical morphology makes the rigorous mathematical theory as the basis, focuses on studying the geometry structure and the relationship of the images. it uses a special tool, structural element, to measure the image shape and construct different structural elements to obtain different image processing results. mathematical morphology has become a new kind of image processing theory and method, which basic ideas and methods have significant impacts on image processing theory and technology. morphological operation is divided into binary and grayscale computing operations, respectively, corresponding to the binary image processing and grayscale image processing. their basic algorithms include erosion, dilation, opening and closing. by using these basic algorithms and their combination, we can implement the analysis and processing of the shape and structure of image, including image segmentation, feature extraction, boundary detection, image filtering, image enhancement and restoration, and so on."
"with the improvement of medical devices level, most hospitals have been already equipped with a variety of digital imaging equipments, and, pacs, picture archiving and transmission system, has been established. pacs is very important in modern hospital, the key technical problem needed to be solved is to unify the image data formats and data transmission standards of all various digital imaging devices. dicom is generated for this purpose."
the edge detection method based on zero crossing can locate the edge by finding the zero-crossing point of the second derivative of the image. we usually use laplacian operator or nonlinear differential equations to get it.
"(3) define the standard level the former dicom can only provide the minimum requirements of the dicom standard that the medical device should follow, and dicom3.0 clearly describes some necessary conformance statements to achieve a particular level."
"the conversion process can be divided into three parts. part one is document reading, including the basic parameters information of image and image data reading. part two is the shift, capture, window level transformation of image data, etc.. part three writes 8-bit grayscale image data, bitmap file header, information header and color table into the bmp bitmap file."
"(1) preprocessing the opening and closing operations of grayscale are carried out to dicom image data. the operations can realize the functions of clipping and filling valley to local area of image, and can remove noise, smooth grayscale of the body target to facilitate the separation of the image and enhance edges."
"(10) media storage and file format for data exchange the chapter 10 specifies the model and functional division of medium storing dicom files, defines the dicom file format, and standardizes the logic relation and encoding format of dicomdir catalog files."
"in above example, the instances of operationalnodes, oncapabilities, onactivities and infos are added to the object structure in codedom, including namespaces, properties, and methods and so on, which are stored in codecompileunit. codedom offers a compiling mechanism which can compile code objects dynamically, as fig.6 . therefore, the graphic models are ultimately transformed into executable models."
"where m xk, m yk are the mean vectors, and xk, yk are the covariance matrices of the patches in matrix m k ."
"based on the trained dictionaries d k illum and d k norm, when having an irregular illuminated face image, we can easily get a sparse projection with respect to d illum . the corresponding normal face image patch basis d norm will be combined according to these coefficients to generate the output normalized image patch y. this synthesis problem is solved by optimizing:"
"the file headers and data structures are very different in dicom image and bmp image. dicom image stores a lot of medical information of data elements in the data collection, such as patient name, age, hospital name, imaging time, checking site, and so on, besides the image size, height, width, number of bytes per pixel image, and other essential information."
"as a result, we can obtain an important conclusion that we can remove the image of these pixels by the erosion operation if the structural elements used for erosion are larger than some of the image pixels. take advantage of this conclusion, we can choose different sizes of structural elements to remove the pixels with different sizes of the image. moreover, we can separate the two pixels if there is small connectivity between two image pixels through the erosion operation."
"the grayscale images occupy most parts in medical imaging files, such as, ct images. this image only contains the information of brightness grayscale, without color information. grayscale representation means to quantify the brightness values. because the densities of the three-primary colors, red, green and blue, in the rgb representation, the grayscale level is usually divided into 0 to 255. 0 means the darkest (all black) and 255 is the brightest (all white). point operation is a commonly used technology which can change the grayscale range that the image data occupy. an image can produce a new output image after point operation, and the grayscale range of the input pixel determines range of the output pixel."
"(6) data dictionary it includes all the encoding and coding instructions of data elements in the dicom standard. the standard uses unique identifier, which is unique in all international standards, makes dicom more workable and reduces the conflict. the details provided in this chapter are widely used in the era when the internet is not widespread, but it is used very less now."
"the result image showed by medical image conversion almost retains all important medical information of the original dicom images. the software can process the images with a variety of forms, such as, flipping, inverting, grayscale, rotation, middle color, saturation, intensity detection, and so on. it offers a convenient and practical tool to learn how to parse the dicom standard and complete medical diagnosis. as an international standard of medical image archiving and communications, dicom is the basis for all medical imaging technologies. it is very important to study dicom standard and file format. based on the analysis of dicom format and file composition, dicom format is converted to bmp format and the medical image is displayed in windows, and many processing operations, such as grayscale, edge detection, and so on, are realized to the medical images. the experiment proves that the system can achieve a better display and processing for dicom medical images provide good conditions for the clinical diagnosis of doctors and digital storage, and communications of image, besides, it can lay the foundation for subsequent work. the system can run stably, has strong adaptability and can be connected well with pacs."
"where i standard and i norm denote the corresponding image under normal illumination condition and the normalized image of the same subject respectively, with d being the pixel number in a single image. the individuals of the 11-th illumination condition in cmu data set are considered as the gold standard normalization. the assessment of illumination normalization results are reported in table 1"
"(2) when reading the data element (0002, xxxx), the value representations of all the data elements are represented, that is, explicit vr little endian. as the little-endian byte order, the byte orders should be first exchanged. we should pay special attention to the data element (0002, 0010), and the value of the data element can determine the file transfer syntax [cit] ."
"in this paper, display and processing of the medical image based on dicom format are focused on, and they are realized by software programming. the system can provide convenience and basis for medical diagnosis and remote consultation."
"however, some problems might arise if uml is directly applied to the capability analysis. it is a semi-formal and weakly constrained modeling language. as a result, it's hard to build rigorous and formal system weizhong zhang phd. tel: +86−13851564082. e-mail: willson_zwz@163.com models. the normal uml models are not executable because they describe the dynamic behavior of models with natural language, and hence cannot be directly used to verify and validate the system behaviors in stage of analysis and design."
"having made the uml extension and defining the domain modeling semantics, we can translate aad models into simulation instances for requirements validation. an algorithm is provided as follows"
"dicom is a standard name specifically for medical image storage and transmission established by american college of radiology (acr) and national electrical manufacturers association (nema). after the development of many years, it has been widely accepted by medical device manufacturers and medical community and is very popular in medical equipments."
"(5) introduce the concept of generalized information object information objects include not only graphics and images, but also studying, reporting and other general information objects."
"where e data (a, b) is the data fidelity term to represent data description error, e reg is the regularization term to regularize the coding coefficients. in the proposed model, we will then jointly optimize the two dictionaries (d illum and d norm ). eq. (5) is then turned into the following dictionary learning and ridge regression problem:"
"the capability meta-concept model, as shown in fig.1, defines the capability ontology of c4isr systems [cit], which contains basic concepts that describe c4isr system architecture. those concepts include operationalnode, operationalentity, activity, mission, capability, information, etc."
"when there are 16 distributed bits, 12 storage bits and the highest bit is 12 in each pixel, the pixel occupies only 2 bytes and uses the low 12 bits. when the highest bit is 12, each pixel uses the high 12 bits of the 2 bytes. when there are 12 distributed bits, 12 storage bits and the highest bit is 11 in each pixel, the pixel only occupies 3 bytes, the contents of the middle byte is divided into two parts, respectively, belonging to the front and back byte, then each pixel has 12 bits."
"step 3: for each activity, the object inflow is regarded as its input and the object outflow is regarded as its output. the input and output, which are instantiated by the class stereotyped with information, are treated as parameters and return values for the operation invocation. the nodes decision, fork and join are respectively processed through following steps:"
"(5) data structure and coding dicom standard provides dedicated medical image files. the formats are regulated specifically in this chapter, the data used and the character encoding requirements are provided."
we take an example to illustrate implementation of the simulation instances. fig.4 shows an aad model of the activity of missile interception for the air defense system.
"recalling the discussion in the previous section, for enhancing the robustness of the normalization transform, beside the basic patch-pair dictionary learning, we involve a multimodel strategy, which aims at separating the training patch pairs into different models/clusters, and then training a set of dictionaries for each patch model in order to guarantee the global transform performance."
"with the development of computer technology and artificial intelligence, the demand on edge detection should focus on the whole image, not be limited to the point of discontinuity of grayscale. the future trend of edge detection is the combination of doctor's experience and advanced technology and the practical and efficient methods of edge detection [cit] ."
"this section presents several methods for illumination invariant representation for face recognition. these methods can be classified into three main categories: retinex theory based, gradient based, and local binary pattern based methods."
"the paper presents an approach a capability meta-concept models, for providing an executable capability modeling language to realize simulating of requirement models. the approach extends uml paradigm by adding the domain modeling constructs of dodaf, defining the structure and semantics of operational nodes and related concepts, and building mappings between the surface grammars of aads and the execution instances. codedom in .net framework is used to support dynamically compiling and generating the executable capability requirement models. and, a case study is provided to show the availability of our method. the future research will be on modeling evaluation indicators of mission effectiveness for the capability requirements, generating executable models for the indicators and thereby realizing integration of simulation and evaluation for the c4isr system."
"(6) edge classification the length of pseudo-boundary is generally shorter, and we should determine it is outer or inner boundary, which can be discriminated by points of the contour line. if choosing contour as starting point, we can make a ray to any horizontal direction and determine the crossing point of this horizontal ray and the other contours. if the number of crossing point is odd, then the contour represents the inner boundary; or the number is even, then the contour line represents the outer boundary. in this method we can remove the needless inner boundaries."
"in the experiment section, assessments are conducted for the proposed illumination normalization (dl-in) algorithm via the cmu pie data set [cit] . the cmu pie data set includes 68 individuals, for each individual, 20 face images are captured under 20 illumination conditions (lights from 20 different directions). we used the face images with no background light for the training and test process."
"erosion operation is carried out point by point and the computing involves the values of grayscale value and structural elements of the points around it. the operation result is the difference of grayscale values between a local point and the corresponding one in structure element, and we should select the minimum one."
"for assessing the performance of the proposed dl-in method on normalizing both known and unknown illuminated faces, we randomly divided the 20 illumination cases into gallery and probe groups as demonstrated in figure 5 and figure 6, which represent the images for the training process and the images for assessment. in the proposed work, the patch pair sets for training are built with a subset of the cmu data set. the training patch set contains 10 illumination cases for 15 individuals. we choose patch dimension as 5x5 pixel 2 ."
"in our work, we proposed a patch-based dl method, in order to study the unified projections between the normal illuminated image patches (light comes from the front) and the irregular illuminated image patches (light comes from variety of directions). the face images under different illuminations are cropped into patches. then a gmm clustering is applied on the patch sets, to estimate a spatial distribution of the face patches. for each cluster, we learned a coupled dictionary sets, when projecting the normal illuminated image patch and the irregular illuminated image patch through each dictionary, we obtain a unified sparse coding (shown in figure. 1)."
"we compare the novel proposed dictionary-learning illumination normalization (dl-in) method with several other wellknown illumination invariant features in a face identification problem. we randomly selected 10 face images from the cmu-pie dataset as gallery (reference) faces, and the rest 10 images as probe (query) faces. all gallery and probe face images are processed via the proposed method and then provided for identification. after the proposed illumination normalization (in) process, the normalized image intensity are directly employed as features for face identification, without other dimension reduction or feature extraction process. for face identification, the sparse representation classifier (src) [cit] was selected."
"where α is the regularization parameter to balance the terms in the objective function, and d x,i, d y,i r p are the atoms of d illum, d norm respectively. the objective function in eq. (6) is not jointly convex to d illum and d norm . however, it is convex with regard to each of them independently. therefore, we can design an iterative algorithm, which is described in details in the following, for alternatively optimizing d illum and d norm in eq. (6) . due to the complex structures in the images of nonstandard illumination, learning only one pair of dictionaries is not enough to cover all variations of the illumination normalization. for instance, when light came from different orientations, it generates significantly different appearance in different facial regions. therefore, the solution is to build different models for different clusters of similar image patches. each model will be more stable when learning from patches that are closer in the input space. and thus, for normalizing an irregular illuminated face image, how to select an appropriate normalization model is critical."
"the image data array of the two images are quite different, dicom images are stored sequentially, the first byte of the array represents the upper left image pixels, and the last byte represents the lower right image pixels. but, bmp image is stored from bottom to top, that is, the first byte of the array indicates the lower left image pixels, and the last byte indicates the top right image pixels [cit] ."
"the lambertian model is based on lambert's law [cit] . specifically, according to lambert's law, when a light ray l(u l ) of intensity l coming from the direction u l reaches a surface point with albedo ρ and normal direction v r, the signal intensity at the point i is given by"
"the edge detection method based on search first calculates the edge strength, is usually expressed with the first derivative, and then, estimates the local edge direction by calculation, usually the direction of the gradient, and uses this direction to find the maximum value."
"in order to improve the operating speed, we should change the edge detection algorithms. because the erosion operation scans the entire image according to the order from top to bottom, from left to right, while the difference operation also do scan the whole image. by analyzing the process of erosion operation, the subsequent erosion operation is only relevant to the original data and structural elements and is not affected by the result of erosion operation. we can modify the erosion algorithms and combine erosion and making difference. after changing the rules of erosion computing, the result is the edge image by scanning the whole image only once, and the efficiency is improved. in addition, before edge detection, by opening and closing the binary data, we can smooth the edges and reduce operating amount of following operation [cit] ."
"dicom standard image is a special format and it has complex types, various combination formats. its display and processing need specially developed image processor, but the most current applications do not support it. combining the clinical use, we find that physicians usually simply select the specific pieces of image as the basis for conclusions in the disease diagnosis. the constituting characteristics of the image pixel data have many similarities with the common images, such as bmp, but also, the latter is suitable for image analysis, feature extraction and other image processing. on the other hand, there are still some medical images using bmp format to exchange information and diagnosis in the process of the digital hospital information construction. in order to facilitate the exchange of documents, we achieve the file conversion between the dicom files and bmp files, the popular bitmap file in windows system."
"in the following paper, we assume that the input image is f, the structural elements is g, d f and d g are the domains of f and g, s and x are the vectors of an integer space."
"the window processing of grayscale is to limit a window range. the grayscale value remains unchanged in the range. the value is set to 0 if it is smaller than the lower limit, and it is 255 if greater than the maximum."
"according to the previous discussion, we can have a prior estimation of the patch texture and location distribution for each dictionary model. therefore, for a given illuminated image patch x i, the posterior probability of x i belonging to each patch cluster can be computed and further becomes the weight of the assumption of the final corresponding normalized patch y i . the model selection proportion is computed as shown in figure 4 :"
"the architecture models and simulation models are usually built in different ways, because the architecture modeling aims at capturing higher level of system requirements and the modeling paradigms usually lack of rigorous and executable semantics. one of the popular solutions is to find semantic mappings between the two different modeling paradigms and transform the architecture models into the simulation models. as for building dodaf executable architecture models, the research community focuses on mapping between uml and discrete event system specification (devs) modeling."
"in the image reconstruction, we designed three test protocols, namely exp1, exp2 and exp3. in exp1, the individuals and illuminations are both provided for the training and test sets. in exp2, the same illuminations are provided for training and test sets but not the same individuals. in exp3, neither the same individuals nor the same illuminations are provided for training and test sets. the normalization face results from three protocols are demonstrated in figure 8 -10. as shown in the top row of figure 8, we notice that the illuminations are well normalized. however, there exists an obvious block phenomenon, which is produced by the patchbased combination and may also relate to the pre-clustering strategy in the image reconstruction process. for eliminating the block phenomenon, we further propose to apply a denoising method based on the local pixel grouping [cit], in which the authors developed a two-step framework. for denoising purpose, the algorithm combined pca (principal component analysis) and local pixel grouping for a better preservation of the local image textures. the denoised normalization images are reported in the second row of figure 8 . while in figure 9 and figure 10, we report only the denoised normalization images. figure 9 shows the illumination normalization results for unknown individuals, with known illumination conditions during the dictionary training process. the performance is visually good thanks to the employment of the clustering based local information, but the average diff value is larger than the average diff value for exp 1, which denotes that the normalization results are actually affected by the unknown individuals. figure 10 shows the normalization results for face images whose identity and illumination conditions were not provided for training process. for protocol ex3, we can notice that partial shadows from the input irregular illuminated images are not normalized completely. this is because such local patches were not provided for the training process. therefore, a generally normalized image is synthesized, but with some residual shadow remained."
"(2) binary the data distribution of different tissues and organs of dicom image has relatively fixed regional range, so we can do the threshold segmentation for the result of the first step according to the medical knowledge not to grayscale histogram, and get the binary image. because only the edge detection is carried out, binary processing of image can simplify the calculation and increase the running speed."
"(4) edge detection we do erosion operation, get the difference between the data before and after operation and detect the edge information. although the morphological method can achieve better results, there are still many deficiencies. such as, there are extra pseudo-boundary and needless inner boundary in the image. the edge image got by the above method is still dot-matrix data, and it is very difficult to exclude the inner boundary. this method has filtered a lot of noise information. if using larger structural elements, we can reduce the number of pseudoboundary, but the computation will be larger and the quality of the real boundary will be impacted. to resolve these problems, the processing steps should be added. after the vectorization of the dot-matrix data, we will do specific treatment according to the features of various types of boundary and get satisfactory results."
(2) conformance the chapter 2 describes that dicom standard is a multi-level specification and a professional standard that can be referred to follow. any implementation of the standard in engineering can be achieved in some part or all specifications in the product. the product statement must clarify the extent of compliance with the product standard. conformance states the content requirements and general format of the product complying with the dicom standard.
"first, the algorithm explores the benefit of adding an and node. to that end it searches for a node candidate b and b such that (i) both b and b were top-active at t, and (ii) the prediction error"
"to define an animat, an initial graph must be specified. any graph will do. for instance, it can be a tabula rasa with need, sensor and motor nodes only. it can also be a graph with complex memory structures and reflexes. once it has been initialized, the animat can then be put into an arbitrary world, where it will learn and make decisions continuously and automatically. to measure the performance of an animat in a given world, we study how its vitality develops over time. in this section we shall give several examples illustrating how the generic animat can learn to eat, drink, move, navigate, and conceptualize its world by forming spatial and temporal patterns."
"consider a bee animat living in the world shown in figure 27 . when the bee visits a flower it collects the nectar, transforming the flower into grass. each flower diffuses a scent into its surroundings. the intensity of the scent from a flower at a given distance follows the inverse-square law (intensity ∝ 1/distance 2 ). the attractive and repulsive flowers have, respectively, positive and negative scents. the initial memory of the bee is shown in figure 28 . the energy level of the bee changes depending on three factors: metabolism, scent intensity and whether nectar is collected."
"is minimal. if this prediction error is sufficiently small, the node b seq b is added to the graph. whenever a new node is added to the graph, new node candidates are formed (by definition 18). now, let us turn to the forgetting rule, which works in the opposite direction and removes nodes."
"where r is an individual response from the set of responses r of the neuron. for each cell the performance measure used was the maximum amount of information a cell conveyed about any one stimulus. this (rather than the mutual information, i(s, r) where s is the whole set of stimuli s), is appropriate for a competitive network in which the cells tend to become tuned to one stimulus. (i(s, r) has more recently been called the stimulus-specific surprise [cit] ."
"here we illustrate the benefit of structural learning in the case of seq node addition. again we consider a sheep that drinks and grazes, but this time the sheep lives in a world that figure 9 : the green block represents grass that is good to eat and the blue block water that is good to drink. the middle block represents a swamp where eating or drinking leads to vomiting and thus to decreased water and energy levels. figure 11 : \"animat\" is the sheep animat. \"control\" is similar, but its dynamic concept formation is switched off. they both start out with a blank slate. \"animat\" adds an and node at time 25. it manages to survive, while \"control\" dies."
"we emphasize that the model is closely linked to neurophysiological research on visual object recognition in natural scenes, and explicitly models how the system could operate computationally to achieve the degree of translation invariance shown in complex natural scenes by inferior temporal cortex neurons [cit] . moreover, the deformation or pose invariance that can be shown by inferior temporal cortex neurons is also a property that can be learned by this functional architectural computational model of object recognition in the ventral visual system, visnet [cit] ."
"to bound the growth of each neuron's synaptic weight vector, w i for the ith neuron, its length is explicitly normalized [cit] which is commonly used in competitive networks [cit] ]. an alternative, more biologically relevant implementation, using a local weight bounding operation which utilizes a form of heterosynaptic long-term depression [cit], has in part been explored using a version of the [cit] rule [cit] ."
"here we consider a fish animat that learns to swim toward reward gradients and away from punishment gradients. we use two similar worlds to show this. the worlds are built from seven blocks which give energy to or take energy from the animat. the animat has one sensor node for blue, one need node for energy, and motor nodes for moving left and right. the animat starts on the leftmost block and it bounces back when it hits a wall. both worlds are as shown in figure 24 . the rewards are shown in table 5 . thus, in world 1 the animat starts in the \"good\" end and in world 2 the animat starts in the \"bad\" end of the world. the results of two 500-tick simulations are shown in figure 25 for world 1 and figure 26 for world 2. table 5 : energy changes when the fish reaches a given block by moving left or right. the fish starts to the left at the \"bad\" end and learns to move to the \"good\" end."
"contains both good water and bad, poisonous water. the problem is that the sheep cannot differentiate directly between the good and the bad water with its sensors. by learning that the bad water always appears in a certain context, in this case close to sand, the animat can learn to avoid drinking it."
"the alignment problem formulated by bostrom [cit] concerns alignment of the behavior of artificial agents with human desiderata, e.g. expressed as utility or reward functions. this problem is intrinsically hard in the real world because of the unpredictability of what situations might arise in the future and the difficulty of making changes to the utility function once an autonomous agent is operational [cit] . computer simulation combined with reward and punishment given by humans might contribute strongly to alignment, but just like in the case of dogs there is no guarantee for reliability. generally speaking, autonomous agents are safety-critical, whether they operate in a software environment or in a physical body. thus one would ideally want to prove that these agents always (or never) do certain actions in certain situations. in fact, testing is not enough, since it is impossible in practice to test all the relevant cases. it has been argued that neural networks have a low degree of transparency and are unsuitable for safety-critical decision-making [cit] ). there seem to be no efficient methods for formalizing and proving safety properties of neural networks at present. this suggests that neural networks alone are not suitable at this stage for controlling autonomous agents. logic-based systems, on the other hand, have a long tradition of formal verification and safety-critical decision-making. they do not seem to match the neural networks when it comes to learning, however. the animat model uses logic-based concepts and so they have a relatively transparent semantics that lends itself to theorem proving. thus one may, e.g. impose the restriction on the animat that a proof must have been found before the animat is allowed to change its graph architecture."
"where n is the number of neurons in the layer. to set the sparseness to a given value, e.g., 5%, the threshold is set to the value of the 95th percentile point of the activations within the layer. the sigmoid activation function was used with parameters (selected after a number of optimization runs) as shown in table a2 ."
"the experimental results corroborate that ppy microwires exhibit significant reversible sensing response to the presence of nh3 and h2s gases in terms of change in conductivity. ppy microwires may become attractive material for gas sensing in many industrial and environmental applications because of its low cost, ease of production and ease with which their properties can be modified by adding substituents and derivatives to the monomer during synthesis. however, more research is still needed to improve the sensitivity, selectivity and reusability of ppy microwires as gas sensing material to play significant roles in the area of sensors in future."
this is a filter that leaves the average activity unchanged. the second stage involves contrast enhancement. a sigmoid activation function was used in the way described previously [cit] :
"ravish garg, vijay kumar, dinesh kumar, and s.k. chakarvarti / [cit] vol. 3 no. 1 pp. 1-13 6 undoped state by chemical reduction [cit] . ravish garg, vijay kumar, dinesh kumar, and s.k. chakarvarti / [cit] vol. 3 no. 1 pp. 1-13 7 fig. 6 . variation of resistance of ppy microwires on removal of nh3 gas environment fig. 7 . variation of resistance of ppy microwires in h2s gas environment ravish garg, vijay kumar, dinesh kumar, and s.k. chakarvarti / [cit] vol. 3 no. 1 pp. 1-13 8 fig. 8 . variation of resistance of ppy microwires on of h2s gas environment fig. 9 . recovery transient of resistance of ppy microwires after interaction with nh3 gas ravish garg, vijay kumar, dinesh kumar, and s.k. chakarvarti / [cit] vol. 3 no. 1 pp. 1-13 9"
"there are several agent architectures that could potentially be used for modeling artificial animals, although they were not primarily constructed for this purpose, e.g. opencog [cit], aera [cit], micropsi [cit], and nars [cit] ."
"we turn next to compare the operation of visnet, as a model of cerebral cortical mechanisms involved in view-invariant object identification, with artificial, computer vision, approaches to object identification. however, we do emphasize that our aim in the present research is to investigate how the cerebral cortex operates in vision, not how computer vision attempts to solve similar problems. within computer vision, we note that many approaches start with using independent component analysis (ica) [cit], sparse coding [cit], and other mathematical approaches [cit] to derive what may be suitable \"feature analyzers,\" which are frequently compared to the responses of v1 neurons. computer vision approaches to object identification then may take combinations of these feature analyzers, and perform statistical analyses using computer-based algorithms that are not biologically plausible such as restricted boltzmann machines (rbms) on these primitives to statistically discriminate different objects [cit] . such a system does not learn view invariant object recognition, for the different views of an object may have completely different statistics of the visual primitives, yet are the different views of the same object."
"we have implemented the prototype system generic animat, which is available at https: //github.com/ni1s/animats. the system is a simplification of the model described in the previous section and it is integrated with minecraft via the malmo interface [cit] . at present, the implementation does not support multiple animats."
"our model makes use of ideas from several research fields. from reinforcement learning, we use elements of local q-learning, multi-objective reinforcement learning, and the u-tree algorithm that were mentioned above. our home-made single agent version of local qlearning avoids the problem that agi agents are unlikely to visit exactly the same state more than once [cit] ."
change in resistance (conductivity) was observed on the interaction of ppy microwires with ammonia/hydrogen sulphide gas and recovery of the original state back on the exposure to air was also witnessed. the whole process of reversible reactions may be represented by the following plausible mechanism.
"for this scenario to operate, the ventral visual stream needs then to implement view invariant recognition, but to combine it with some translation invariance, as the fixation position produced by bottom up saliency will not be at the center of an object, and indeed may be considerably displaced from the center of an object. in the model of invariant visual object recognition that we have developed, visnet, which models the hierarchy of visual areas in the ventral visual stream by using competitive learning to develop feature conjunctions supplemented by a temporal trace or by spatial continuity or both, all previous investigations have explored either view or translation invariance learning, but not both [cit] . combining translation and view invariance learning is a considerable challenge, for the number of transforms becomes the product of the numbers of each transform type, and it is not known how visnet (or any other biologically plausible approach to invariant object recognition) will perform with the large number, and with the two types of transform combined. indeed, an important part of the research described here was to investigate how well architectures of the visnet type generalize between both trained locations and trained views. this is important for setting the numbers of different views and translations of each object that must be trained."
"ppy microwires were synthesized via template assisted chemical polymerization using tem as template which is explained by authors somewhere else in earlier publication [cit] . in this technique, the template was sandwiched between two chambers in a chemical cell in such a way that the template acted as a separating wall between the two chambers. one chamber was filled with an aqueous monomer pyrrole solution and other was filled with oxidant reagent of ferric chloride. chemical polymerization takes place within the pores of the template as the monomer and the oxidant reagent diffuse towards each other and yields array of aligned-ordered ppy microwires of determined morphology due to confined directional growth within pores synthesized ppy microwires were exposed to the nh3 vapors generated from liquid nh3 and h2s gas generated from kipp's apparatus at room temperature in indigenously developed gas chamber with concentration 200 ppm to study the sensing behavior. for the preparation of the sensor using ppy microwires sensing elements, a piece (size 10 mm x 10 mm) of prepared sample of tem impregnated with ppy microwires was pasted on the conductive side of indium tin oxide (ito) coated glass slide (size 10 mm x 20 mm), working as one of the electrodes in two probe method. after that, the matrix of host material (polycarbonate) of impregnated tem was dissolved and removed by pouring the dichloromethane drop by drop for about 20 minutes on the sample that was dried out finally. the fig. 1 illustrates the scanning electron microscopy (sem) image of released solid cylindrical ppy microwires, from the matrix of host material of polycarbonate tem, with average diameter and length 1.6 um and 20 um respectively corresponding to the dimensions of pores within tem. the density of microwires is equal to pore-density which is nearly same as of flux density (10 6 ions/m 2 ) of irradiated ion beam for the formation of latent tracks during the fabrication of tems [cit] . after that, another slide of ito conducting glass of the same size was placed on the first slide shifted by 10 mm to have the space on the conducting side of both glass slides for electrical contacts. the sensor, containing ppy microwires sandwiched between glass slides, was hold in a small clamper as shown in image of actual assembly in fig. 2 . the experimental setup, for measuring the change in resistance of the sensor (ppy microwires) with the exposure of gas, is shown schematically in the fig. 3 . to check the sensitivity, the prepared sensor was kept in the glass chamber having removable lid and the gas injected into the glass chamber via inlet and concentration of 200 ppm was achieved. the sensor was initially exposed to gas and i-v ravish garg, vijay kumar, dinesh kumar, and s.k. chakarvarti / [cit] vol. 3 no. 1 pp. 1-13 4 characteristics were measured at regular intervals of time using keithley 2400 series source measurement unit via two probe method by stepping the voltage in the range 0 to 10 volts. later, inlet of glass chamber was closed and the lid was removed to expose the transducer with air. again the i-v characteristics were measured with time and change in electrical resistance was estimated."
"where r is the activation (or firing rate) of the neuron after the lateral inhibition, y is the firing rate after the contrast enhancement produced by the activation function, and β is the slope or gain and α is the threshold or bias of the activation function. the sigmoid bounds the firing rate between 0 and 1 so global normalization is not required. the slope and threshold are held constant within each layer. the slope is constant throughout training, whereas the threshold is used to control the sparseness of firing rates within each layer. the (population) sparseness of the firing within a layer is defined [cit] as:"
"we have described a computational model for artificial animals by defining generic rules for perception, learning, and decision-making. these rules constitute a basic developmental model for arbitrary animats in arbitrary ecosystems. we presented several experiments with a prototype implementation of the animat model. the experiments showed how animats can adapt to different ecosystems and learn to survive by, e.g. starting from an empty memory structure and gradually populating it with nodes and connections. the results obtained indicate the generality of the model and show that the animats are capable of learning to eat, move, and navigate in a number of different environments. it was also shown that the ability to form new nodes sometimes means the difference between life and death. the results obtained suggest that dynamic architectures are more powerful than static architectures. the animat model described was intended as a proof of concept. clearly, it can be developed further in several directions. for instance, the structural learning rules could be improved and straightforward probabilistic rules that form memories at random moments could be added. second, to enable symbolic learning and reasoning, a working memory and a rewrite engine could be added, possibly along the lines of . third, the exploration techniques could be refined, e.g. by avoiding actions that are believed to bring strong punishment while exploring. finally, the animat model needs to be tested in much more complex and challenging ecosystems. in particular, it must be tested in ecosystems with multiple animats."
"another difference of hmax from visnet is in the way that visnet is trained, which is a fundamental aspect of the visnet approach. hmax has traditionally been tested with benchmarking databases such as the caltech-101 and caltech-256 [cit] in which sets of images from different categories are to be classified. the caltech-256 dataset is comprised of 256 object classes made up of images that have many aspect ratios, sizes and differ quite significantly in quality (having being manually collated from web searches). the objects within the images show significant intra-class variation and have a variety of poses, illumination, scale and occlusion as expected from natural images. a network is supposed to classify these correctly into classes such as hats and bears [cit] . the problem is that examples of each class of object transforming continuously though different positions on the retina, size, isomorphic rotation, and view are not provided to help the system learn about how a given type of object transforms in the world. the system just has to try to classify based on a set of often quite different exemplars that are not transforms of each other. thus a system trained in this way is greatly hindered in generating transform invariant representations by the end of the hierarchy, and such a system has to rely on a powerful classifier such as a svm to perform a classification that is not based on transform invariance learned in the hierarchical network. in contrast, visnet is provided during training with systematic transforms of objects of the type that would be seen as objects transform in the world, and has a well-posed basis for learning invariant representations. it is important that with visnet, the early layers may learn what types of transform can be produced in small parts of the visual field by different classes of object, so that when a new class of object is introduced, rapid learning in the last layer and generalization to untrained views can occur without the need for further training of the early layers ."
suppose the animat gets surprised at t + 1. then the learning algorithm will consider the possibility of adding a new node. let i be a randomly selected need node subject to surprise at t + 1.
"the bottom up saliency map generated by the gbvs code (acting as a surrogate for the dorsal visual system) for one of the scenes is illustrated in figure 3b . the saliency map has of course no indication of which peak is a trained object, nor of which object it might be."
"15. the interactive dynamics between animat and world is described in table 4 . figure 16 shows how the animat learns to survive. table 4 : interaction dynamics for the copepod animat. at left is the perception matrix: how input to the sensor nodes is generated. at right is the reward matrix: how \"energy\" values (which can never exceed 1) change depending on the situation. the negative numbers represent metabolism costs. the effects of actions are as expected: e.g., the action \"forward\" causes the animat to move forward, the action \"eat\" causes an algae block to be replaced by a water block. figure 16 : vitality curve for the copepod animat (\"vitality\" here is the same as \"energy level\"). \"control\" shows the same animat performing random actions. reinforcement learning helps the animat gradually to learn a strategy that lets it survive: eating when it encounters green blocks and preferring forward motion to rotation. note how quickly \"control\" declines and dies."
"over the next few months, the bear encountered red berries of different types many times and avoided eating them because of its belief (4). when winter approached, the bear was very hungry. it came to a forest with red berries that happened to be shiny red cranberries."
"since it was so hungry it decides to take a risk despite belief (4) and try those berries. then it got a big positive surprise, since the berries were delicious. this surprise caused it to add the following belief to its memory:"
"the world is shown in figure 12 . the animat starts with the same memory as in the previous example (figure 10 ). it adds the node \"red seq blue\" the first time a red block is encountered (one-shot learning). figure 13 shows its vitality curve."
"conformations are used for specifying locations and positions of objects and animats of different kinds. in block worlds, for example, the conformations may be radically simplified. for instance, to specify the conformation of a rigid object or animat in a blocks world, it may suffice with two or three integer coordinates. now we are ready to define the animats. animats can model animals such as frogs and bees, or control artificial agents, such as robots. the activity pattern is included in this definition in order to enable perception of temporal sequences. we will soon give several examples of animats, but first let us define their environments. figure 3 shows an ecosystem. several examples of ecosystems are given in section 7. algorithm 1 shows how the animats are being updated at every tick. the algorithm is generic: it is the same for all animats. the conformation of the animat is not updated by this procedure. instead it is updated by the physical laws of the environment, whether they are modeled as in the case of computer simulations or real as in the embodied case. in the next two sections we will describe the learning and decision-making steps of this algorithm in detail."
"figure 2 provides an example, where the red and node is the only top-active node. in general, many nodes can be top active at the same time. intuitively, the top-active nodes describe how the present moment is perceived in terms of the perception nodes of the memory structures, at the maximum amount of detail."
"fourth, it will be useful to investigate in future the incorporation of more powerful synaptic learning rules when training with the large number of transforms needed when learning invariance for both view and translation transforms of objects. with visnet, we have so far used an associative (hebbian) synaptic modification rule (with a trace of previous firing in the postsynaptic term), for biological plausibility [cit] . however, to explore further the potential of the overall architecture of visnet, it will be of interest to investigate how much performance improves when error correction of the post-synaptic firing with respect to the trace of previous neuronal activity is incorporated to implement gradient descent. gradient descent [cit] or optimized slow learning [cit] have been found useful with different architectures."
"let us illustrate the point of structural learning and top activity with a little story. imagine a bear that tasted berries for the first time: berries that happen to be blueberries. then the bear experienced reward via a rise in its blood-sugar level. the consequent positive surprise caused it to form the node \"berry\", e.g. as a small sphere, and set q energy (berry, eat) to a positive number. thus the following belief was added to its memory: action \"eat\" when perceiving \"berry\" leads to reward."
"among conducting polymers, ppy is most extensively studied because pyrrole (monomer) is water soluble and can easily be oxidized. pyrrole is commercially available and possesses good environmental stability and redox properties. the ppy can be synthesized either chemically or electro-chemically [cit] and its molecular chain structure can be modified conveniently by copolymerization or structural derivatives [cit] . electrochemical method is complex and expensive, so it is not apt for mass production, whereas chemical method is simple and inexpensive, thus advantageous for mass production in industry. bulk quantity of ppy can be achieved in the form of fine powder via oxidative polymerization of monomer by selecting transition metal ions in water or various other solvents and its conductivity depends on the reaction conditions and reagents used in the oxidation. with the advancement of nanotechnology, synthesis of low dimensional structures of conducting polymers is an emerging subfield of the polymeric electronics arena for the development of various sensors [cit] ."
"imagine an animat whose nervous system is as shown to the left in figure 4 . suppose further that the animat perceives \"red\" and \"blue\" and attempts the action \"drink\". it is surprised as its energy level rises. the surprise should trigger learning; but what should be learned? one option is to add the belief:"
"on exposure to ammonia (nh3), the protonated ppy is converted into free ppy due to redox reaction, thereby decreasing the conductivity and hence increase in resistance of microwires sensor was observed. nitrogen atom of the ammonia makes a coordinate bonding with the free atomic orbital of the dopant proton (h + ). this leads to the deprotonation of ppy nitrogen atoms which facilitates the decrease in charge carriers resulting increase of electrical resistance [cit] . on the contrary, a decrease in resistance of ppy microwires was observed when it was exposed to the vapours of weak acid hydrogen sulphide (h2s). this may be due transfer of proton from acidic gas to ppy resulting in increase of doping level of the polymer backbone and hence, increase in conductivity and decrease in electrical resistance of microwires was achieved [cit] ."
"where δw ij is the change of the synaptic weight w ij that results from the simultaneous (or conjunctive) presence of presynaptic firing x j and postsynaptic firing or activation y i, and α is a learning rate constant that specifies how much the synapses alter on any one pairing. the pattern associator was trained for one trial on the output of visnet produced by every transform of each object. performance on the test images extracted from the scenes was tested by presenting an image to visnet, and then measuring the classification produced by the pattern associator. performance was measured by the percentage of the correct classifications of an image as the correct object."
"[more detailed information may become available with repeated fixations on different parts of an object, and this has been investigated in computer vision [cit] .] third, we have not utilized top-down attention in the developments described here. top-down attention, whereby an object or set of objects is held active in a short term memory which biases the competitive networks in visnet, can in principle improve performance considerably [cit] b; [cit] ) . indeed, we have developed and successfully tested a reduced version of visnet in which top-down attention does facilitate processing [cit], and this approach has also been used in computer vision [cit] . another type of top-down effect is that task requirements can influence fixations in a scene [cit] . we plan in future to incorporate top-down attention into the full, current, version of visnet, to investigate how this is likely to improve performance, especially for certain selected classes of object."
"s.w. wilson also suggested the animat path to ai with the goal of creating ai by modeling animal intelligence [cit] . animal intelligence has been studied and modeled extensively in comparative psychology and artificial-life [cit] . to be of direct relevance to agi, those models should (i) be computational, (ii) represent agents individually rather than collectively, and (iii) represent cognitive development explicitly. these criteria rule out the vast majority of models, including ecosystem models [cit] in which populations of animals are modeled by numbers, representing e.g. sex, age, and biomass distributions -whether those models are simulationbased [cit] or analytical [cit] . evolutionary game theory sometimes models agents at the individual level as e.g. finite or cellular automata; however, these models are of limited interest to agi so long as they are static: i.e., no learning takes place at the individual level [cit] . nervous systems are ubiquitous in the animal kingdom and play a fundamental role in animal intelligence. successful artificial neural network techniques include deep learning [cit], long short-term memory [cit] with the ability to store arbitrary values for arbitrary long time-periods, and neural turing machines, with their great generality, but also slow convergence and need for external memory resources [cit] ."
"our main strategy towards agi is an elaboration of the animat path to ai. our approach is based on the idea that the nervous systems of animals ranging from fruit flies to elephants are essentially conformationd by the same fundamental mechanisms, many of which have already been identified and modeled computationally."
"the better choice is option (1): it is more general and, therefore, potentially more usefulalbeit at the risk of overgeneralizing . it could be the case that the action \"drink\" in the state \"blue and red and green\" leads to punishment instead. mechanisms are required that enable one to form more fine-grained beliefs as the need arises: i.e., to handle exceptions to general principles. now let us make the intuition about surprise more precise."
here we show that the generic animat can learn to navigate like a braitenberg vehicle. we model a bee that navigates in a landscape with scenting flowers.
"the surprise of a perception node b at time t + 1 w.r.t. the need node i is defined as follows: when the animat is surprised, a new node will be added to the graph. the surprise indicates that the animat needs a more fine-grained ontology to be able to identify similar situations in the future. the node candidates do not belong to the graph, but they have local q-values and r-values that are initiated and updated just like the local values of the perception nodes of the graph."
"reverse process takes place on exposure to air and removing the ammonia environment. ammonium ion (nh4 + ) decomposes into ammonia gaining protons from ppy which leads to recoup the initial state of ppy, thereby decreasing the resistivity back [cit] . in case of removal of hydrogen sulphide environment, the h2s becomes free from doping of ppy backbone, thus reverts back to the prior state of ppy by increasing the resistance [cit] . this mechanism supports functioning of ppy microwires as gas sensor. the electrical resistance was found to attain saturation and change in resistance became almost zero with the time in the presence and removal of gaseous environment as depicted in figs. 5 to 8. in both the cases, a drift in recovery from the original value of electrical resistance (conductivity) was observed as illustrated in figs. 9 and 10 because of the formation of hydronium ion takes place due to presence of moisture in the ambient air."
"fifth, if a strong saliency peak occurs due to something in the background scene that is close to an object, or due to another trained object, how will the system respond? we suggest that the general answer is that the asymmetry that is present in the receptive fields of inferior temporal cortex neurons in cluttered scenes that is related to the asymmetries caused by the sparse probabilistic forward connections of each neuron and that enables two instances of the same object close together to be correctly identified in terms of both object and position provides the solution, but it will be of interest to investigate this in detail."
"a variety of toxic gases that can be harmful to human society have been found from the nature for example ammonia (nh3) and hydrogen sulphide (h2s). the sensors and their performance have become increasingly important to detect these toxic gases for monitoring and protection of environment and human health, because even small concentration of these gases can be fatal [cit] . ammonia is a poisonous colourless gas with a pungent and suffocating smell, recognized as one of the primary irritants to the human body. in ammonia gas sensing, the most important issue of the concern is that even its small dose becomes fatal. as defined by occupational safety and health administration, usa [cit] ), a 15 minutes exposure limit for nh3 gas of 35 ppm by volume in the ambient air can create fatalities. h2s produced from industrial processes is another highly toxic and corrosive gas, and its low concentrations above 250 ppm can lead to death [cit] . it is generated when the sewage becomes stagnant or septic for a longer time. this gas has adverse effects on the environment by causing acidification of rivers and lakes, destroying fish stocks and flora which result in release of metals from pipes and earth."
"various biological bases for this temporal trace have been advanced as follows: [the precise mechanisms involved may alter the precise form of the trace rule which should be used. földiák (1992) describes an alternative trace rule which models individual nmda channels. equally, a trace implemented by temporally extended cell firing in a local cortical attractor could implement a short-term memory of previous neuronal firing [cit] .]"
"figure 4: left: an empty graph with only sensor, need, and motor nodes. right: the same graph with a belief added encoding the experience of drinking when the sensors \"blue\" and \"red\" are active."
"we ran a 200-tick simulation of the bee in the bee world. figure 29 shows a midsimulation plot of how the bee has moved in the world so far. the result of the simulation is shown in figure 30 and figure 31 . the initial memory of the bee. the bee has two sensors sensitive to the direction of the scent gradient of nectar from flowers in its 9x9 blocks neighborhood. one is active if the scent gradient is to the left of the animat or within 22.5 degrees to the front-right, and the other is active if the scent gradient is to the right of the animat or within 22.5 degrees to the front-left. thus, there is an overlap of the sensitivity of the \"left\" and \"right\" sensors and they will both be active if the scent vector is within +/-22.5 degrees of the forward direction of the animat."
"from that day forth, no surprise caused it to modify its beliefs about berries anymore. at that point it had learned everything it needed to know about which berries are edible. in fact, the beliefs (3)-(5) capture all it needs to know about berries to live a long life in that forest. this story illustrates the point that memory formation should be exactly as finegrained as required from the perspective of need satisfaction. it also illustrates the point of basing decisions on the most detailed memories that are activated in a given situation, in other words the top-active nodes."
"to illustrate the expressive power of our graph model, consider a piece of music m for a simple digital piano. let there be one sensor node for each key on the piano; then m can be identified with a finite stream of inputs to the sensor nodes, while repeated and and seq nodes represent chords and melodies respectively. in a similar way, for any given piece of music played on the same piano, there is an action node that reproduces or plays this particular piece given that the motor nodes represent the piano's keys."
"here we illustrate the benefit of structural learning in the case of and node addition. consider a sheep animat that lives in the world shown in figure 9 . figures 6 and 10 show its memory at start and after convergence, respectively. figure 11 shows how its vitality develops over time."
"to conclude, the animat path to ai seems very promising and natural to us and we believe in the strategy of gradually refining a generic animat model so that it can survive in ever larger and more complex classes of ecosystems. by restricting attention to a general type of problem that faces all animals -that of surviving (and reproducing) in different ecosystems -one may approach the agi challenge in a uniform and stepwise fashion. this is the type of problems that all animals were evolved to solve. the animat path to ai is in sharp contrast to strategies for agi that focus on problem domains such as iq-tests, theorem proving, language comprehension, and strategic board games."
"our perspective on motivation and emotion largely follows the micropsi model [cit] ) and its theoretical underpinnings, including psi-theory (dörner, 2001) . motivation is described as a set of physiological, social and cognitive needs, which the agent must keep in homeostasis. different models can have different sets of needs. each need has a target and current value; their difference manifests as an \"urge\" signal informing the agent's decisionmaking. all actions are directed either toward the satisfaction of a need (appetitive) or the avoidance of its frustration (aversive). even serendipitous behavior serves needs: e.g., exploration (reduction of uncertainty) and competence (skill acquisition). the satisfaction of a need creates a pleasure signal proportional to the rate of satisfaction, amount, and relative importance of the need. the frustration of a need creates a pain signal. pleasure and pain act as reinforcement signals for learning."
"we ran a simulation with the sheep animat. figure 7 shows the memory after convergence and figure 8 shows its vitality curve. table 3 : status changes for the need node \"water\". figure 8: vitality curve of the sheep, where vitality is the minimum of \"energy\" and \"water\". \"control\" shows the same animat performing random actions. multi-objective reinforcement learning helps the animat to learn a strategy that lets it survive: alternating between eating grass and drinking water depending on the most pressing need. \"control\" quickly declines and dies."
"the trace update rule used in the baseline simulations of visnet [cit] ) is equivalent to both földiák's used in the context of translation invariance [cit] explored in the context of modeling the temporal properties of classical conditioning, and can be summarized as follows:"
"-an action node is true at t either if (i) it has an incoming arrow from a node that is true at t (reflex), or (ii) a is selected at t by the animat's policy, as specified in section 6. figure 2 shows an example of activity propagation."
"this was calculated for the subset of cells which had as single cells the most information about which stimulus was shown. [cit] and subsequent papers, the multiple cell information was calculated from the first five cells for each stimulus that had maximal single cell information about that stimulus, that is from a population of 35 cells if there were seven stimuli (each of which might have been shown in for example 9 or 25 positions on the retina)."
"the learning rates for each of the four layers were 0.05, 0.03, 0.005, and 0.005, as these rates were shown to produce convergence of the synaptic weights after 15-50 training epochs. 50 training epochs were run. the developments to visnet that facilitated this principled approach to the learning rate, combined view and translation invariance learning, etc, and the parameters used, are described in the appendix."
"equivalently, the sensor nodes are assigned boolean values. this input is the basis of the animat's perception. the input signals to the sensor nodes are propagated through the graph as follows:"
"with 48 images extracted from the the 12 test scenes (8 illustrated in figure 3a ), performance was 90% correct (43 correct/48), where chance with the four objects is 25% (fisher test p 0.0001)."
"here we model a copepod (a planktonic crustacean) that lives in the ocean and feeds on planktonic algae; its world is shown in figure 14 and its memory development in figure figure 12 : this world is a long path that begins with water and grass blocks, where the animat can learn to eat and drink. then come the poison blocks for the first time. each poison block has a sand block to its left. this enables animats that are capable of sequence learning to differentiate between water and poison. figure 13 : \"animat\" is the sheep animat. \"control\" is similar, but it has its capacity to add seq nodes switched off. the animats start with a blank slate. \"animat\" adds a seq node at time 75. it survives, while \"control\", unable to learn sequences and contexts, dies at time 100."
"most animals are autonomous in the sense that they are able to live and reproduce without any interaction with humans. moreover, they have no interfaces that enable human engineers to reprogram them or human users to provide them with direct commands in a formal language. yet many animals can be trained by humans for various purposes. for instance, dogs can be trained to become service dogs, guard dogs, avalanche dogs, narcotics dogs, sled dogs, or hunting dogs. in that case the training targets the animals' existing reward systems and their ability to learn by association and positive reinforcement [cit] . dogs can also live in the wild similarly to wolves and survive without any interaction whatsoever with humans. the memory of the bee after convergence. the bee has learned to turn left when the \"left\" sensor is active and right when the \"right\" sensor is active. an and node was added and the bee has learned to move forward when this node is active. just like animals, animats can develop with any amount of interaction with humans. moreover, they can develop in a computer simulated environment or have a robotic body and develop in the real world. for instance, they can first be trained in a simulated environment and then inserted into a robotic body and operate in the real world. now let us consider for a while how animats can be used for serving humans. to be able to train the animat, the human trainer must have access to its reward system. one way is to give reward and punishment to one of the existing homeostatic variables, e.g. energy. another way is to add a homeostatic variable that is directly controlled by humans. if desirable, this homeostatic variable can be given priority over the other homeostatic variables by re-scaling the values of the other homeostatic variables. reward and punishment that target this additional homeostatic variable might then be provided by the human users, e.g. in the form of verbal praise and blame. then the animat might be trained similarly to dogs."
"the belief (3) guided the bear to many good meals of blueberries. one day the bear encountered red berries for the first time. guided by its belief (3) it ate those berries. but the red berries happened to be poisonous red honeysuckle berries, that caused the bear to vomit and thus lose energy. in response to this negative surprise it added the following belief to its memory:"
"we use a memory model based on transparent neural networks ) that we simply call graphs. the benefit of these graphs lies in their relatively transparent semantics, which makes it comparatively easy to define efficient rules for developing memory structures gradually. a fundamental learning mechanism is to add nodes to the memory whenever a sufficiently large prediction error occurs with respect to need satisfaction. the graph model enables our system to support several types of learning, including online, one-shot, and transfer learning. the decision-making algorithm has the permanent goal of keeping all the needs as satisfied as possible for as long as possible."
"in this section we introduce the graphs that are used for modeling memory structures. the natural numbers are used for indicating temporal order, as will be clarified below. figure 1 gives an example of a graph. many more examples will be given below. intuitively, sensor nodes model sensors: receptor cells with ion channels sensitive to e.g. cold temperature, mechanical pressure, or acidity. need nodes model interoceptors: receptor cells that measure the level of satisfaction of various needs. for instance the need nodes may represent water (osmoceptors), energy (insulin receptors), protein (amino-acid receptors), oxygen (co 2 receptors), integrity (nociceptors), sleep (melatonin receptors), heat (thermoreceptors), proximity (pheromone receptors), and affiliation (oxytocin receptors). motor nodes model muscle-controlling motor neurons. and nodes are used for modeling the usual big conjunction (that can have several conjuncts). seq nodes are used for modeling sequences. a seq node is activated if its incoming nodes have been activated in the order indicated by the annotation on the corresponding arrows. action nodes model neurons that in turn activate motor neurons, either in parallel or in sequence. in parallel case, no annotations are used. in the sequences case, the natural number annotation is used to indicate order of execution of the motor nodes. action nodes can be activated either directly in the form of reflexes (arrows in the graph), or indirectly via decision-making, as we will see in section 6. arrows model synaptic connections that propagate activity through the graph. in the animat model, time is modeled in discrete time steps or ticks. at each tick, input activity is transmitted from the environment to the animat in the form of input activity to the sensor and need nodes of the animat's graph."
"homeostatic agents have homeostasis, or need satisfaction, as their sole objective, i.e. to regulate their homeostatic variables and thus survive as long as possible [cit] . homeostatic decision-making combines naturally with models for hormonal control (avila-garcía and cañamero, 2005), cognitive modulation [cit], and personality traits [cit] . moreover, homeostatic agents can be naturally linked to reinforcement learning by defining reward as the difference in need status from one time to another. in the case of multiple homeostatic variables, or multiple needs, it may be natural to use multi-objective reinforcement learning [cit] rather than combining multiple reward signals into one with the help of a merge function. for an animal with water and energy as needs, no quantity of water can compensate for an energy deficit and vice versa. running out of any of those resources may lead to instant death. moreover, there is an upper bound to the amount of energy and water that a given animal can ingest."
"here we consider a toad that lives in the same world as the frog. the toad cannot jump; it can only move forward by alternately extending its hind legs. if a single leg is extended then the other leg is automatically folded. an action to extend an already extended leg results in that leg continuing to be extended. the toad is equipped with proprioception sensors that indicate which hind legs are folded. the initial memory of the toad is shown in figure 21 . the result of a 100-tick simulation is shown in figure 22 and figure 23 . figure 23: vitality curve of the toad. \"animat\" is the toad with proprioception sensors and \"control\" is similar but with no proprioception sensors."
"eight of the 12 test scenes are illustrated in figure 3a . each scene had each of the objects in one of the four poses. the aim of the combined visual processing was for the dorsal visual stream to detect the salient regions in these 12 scenes, and then for the salient regions to be passed to visnet to perform the view (and translation) invariant object recognition for every object in the scene. visnet had been trained on the 4 objects in each of the 4 views, but not on the background scenes, and it was part of the task of visnet to identify each of the four objects in every scene without being affected by the background clutter of each scene [cit] . the objects used in this investigation were common types of object with which the human visual system performs good view invariant identification, people and vehicles. two people and two vehicles were chosen to provide evidence on how the system might operate with typical stimuli for which view-invariant identification is necessary and is performed by the human visual system."
"second, we have used a generically sound and well-known approach to bottom-up saliency, an approach developed by koch, itti, harel and colleagues [cit],b) . however, it is possible to tune saliency algorithms so that they are more likely to detect objects of certain classes, such as faces or cars. this may greatly increase the capability of the approach described here, and we plan to test how much improvement in performance for the detection and then identification of certain classes of objects can be obtained by incorporating more specialized saliency algorithms. many saliency approaches and algorithms that are of interest for future research are available [cit] . for example, contextual information may be useful, such as the fact that sofas are not usually found in the sky, and that people are usually tall, skinny objects on the ground [cit] . we emphasize that in the system described here, only one fixation is assumed for each object in a scene, consistent with the fact that single neurons in the inferior temporal visual cortex provide sufficient information for object and face identification during a single fixation and in only 20-50 [cit] ) ."
"feedforward connections to a layer of v1 neurons perform the extraction of simple features like bars at different locations, orientations and sizes. realistic receptive fields for v1 neurons that extract these simple features can be represented by 2d-gabor wavelets. [cit] derived a family of discretized 2d-gabor wavelets that satisfy the wavelet theory and the neurophysiological constraints for simple cells mentioned above. they are given by an expression of the form"
"is minimal. if this prediction error is sufficiently small, the node b and b is added to the graph. second, if no and node is added, the algorithm proceeds by exploring the benefit of adding a seq node. to that end it searches for a node candidate b seq b such that (i) b was top-active at t − 1, (ii) b was top-active at t, and (iii) the prediction error"
"the policy selects actions aimed at keeping the vitality of the animat as high as possible, for as long as possible. it weighs up the animat's present status with expected status changes in the future. these expectations are in turn weighted by their estimated reliability. an animat with the two needs \"energy\" and \"water\" will be likelier to drink if water is its most urgent need. on the other hand, if its experience indicates that it would lose large quantities of energy by doing so, it might nevertheless refrain."
"neuroplasticity refers to the capacity of animals to alter their nervous systems in response to changes in the environment. not only does the connectivity between neurons change, but neurons may also be added and removed continuously in a life-long process [cit] . artificial neural network models are frequently based on static architectures that are only plastic in the sense that their connectivity patterns develop over time. several neural network models also allow nodes to be added, however. for instance, the cascade-correlation architecture adds one hidden neuron at the time [cit] ) and the progressive neural networks grow new columns while retaining previously acquired knowledge [cit] . in the opposite direction there are regularization techniques [cit] and pruning methods [cit] for reducing the size of neural networks, while improving generalization."
"(1) to demonstrate with a biologically plausible model of the ventral visual system how it could operate to implement view invariant object/person identity recognition with a generic model of the dorsal visual system that produced fixations on parts of scenes that were salient. how would the combined cortical visual areas operate with the dorsal visual system not encoding object identity but only saliency; and the ventral visual system being unable to find objects efficiently in large natural scenes, but able to perform view invariant object recognition once fixation was close to an object? (2) how closely and effectively would a simple, generic, bottom-up saliency system modeling part of the functions of the dorsal visual system find objects in a complex scene, and how accurately would the center of the object be fixated? the accuracy with which the center of the object is fixated is crucial to understand, for this defines how much translation invariance must be incorporated into the ventral visual system for the whole system to work. (3) can visnet be trained for both view and translation invariance? this has not been attempted previously with visnet, and for that matter view invariant object recognition is not a property of most computer vision models (see discussion). (4) if visnet can be trained on both view and translation invariant object identification, can it be trained with sufficient translation invariance to cover the visual angle needed given the inaccuracies of the saliency-based fixation mechanism in finding the center of an object, and yet be trained with sufficient views to provide for view-invariant object identification? (5) how well does visnet generalize from trained views to untrained views of an object? this is important, for it influences how much training of different views is required, which could have an impact on the capacity of the system, that is on the number of objects or people that it can correctly identify with the required translation invariance. (6) how well does visnet perform in object identification when the objects appear in natural scenes with fixation not necessarily at the trained location, and when views intermediate to those at which visnet has been trained are presented? that is, how well under the natural scene conditions can visnet ignore the background and identify a trained object despite it being presented in a view and position that were not trained?"
"here we show that a sheep animat can learn to eat and drink. consider a sheep with two needs: energy and water. its world is shown in figure 5 and its initial memory in figure 6 . the interaction dynamics between animat and world are described in tables 1, 2, and 3. table 2 : status changes for the need node \"energy\"."
"here we show a frog animat that learns how to move. first, let us consider a frog that lives in the world shown in figure 17 . figure 18 shows the initial memory of the frog. the frog consumes energy from metabolism at each time tick. it can only gain energy by moving to new blocks and ingesting the food that is available there. it can only move to a new block by jumping, i.e. by extending both hind legs simultaneously. the result of a 100-tick simulation is shown in figure 19 figure 20: vitality curve of the frog. the frog has learned how to jump after three ticks. the sporadic dips in the vitality curve are due to exploration of non-optimal actions."
"-a seq node b is true at t if the following holds for all the children b i of b: the node b i was true at time t − k i, where k i is the annotation on the arrow leading from b i to b."
"to visnet had background parts of the scene included (e.g., figure 3c ). these background features did not produce large decreases in the performance of visnet, given that visnet had been trained on the objects but not on the backgrounds [cit] . this is important for the processes of invariant visual object identification in novel complex natural scenes described here. further, if there was a low amplitude saliency peak containing only part of the background scene and not an object, then visnet did not respond to this as a trained object. when errors were made by visnet on the object identification, the confusions were as frequent between the classes of people and vehicle as within these classes."
"section 2 describes our strategy for agi. sections 3 defines our variety of graphs that are used for modeling memory structures. section 4 defines the animats. section 5 presents the learning mechanisms and section 6 the decision-making mechanisms of the animats. section 7 presents the results of eight simulations of different animats in different environments. section 8 contains a brief discussion about potential applications of the animat model along with some ethical considerations. section 9, finally, draws some conclusions."
"many researchers have investigated conductive polymer, including ppy, for gas sensing applications due to advantages of low cost, easy fabrication on various substrates, high sensitivity, short response time, good mechanical properties and room temperature operation (nath and contractor, ravish garg, vijay kumar, dinesh kumar, and s.k. chakarvarti / [cit] vol. 3 no. 1 pp. 1-13 3 2004; [cit] . according to reports available in the literature, ppy material is sensitive and exhibits significant reversible change in the conductivity when exposed to various gases [cit] such as ammonia, hydrogen sulphide, ethanol etc. the efficiency of a sensor basically depends on its interaction with the analyte in a way to provide reasonable sensitivity and fast response time. to improve the sensor performance, a better approach is to form the array of micro/nanostructures of the sensing material causing an increase in the electroactive surface area which facilitates an easier diffusion of the analyte [cit] . therefore, with this motivation in the present work, authors had synthesized ppy microwires via chemical polymerization technique using track etch membrane (tem) as template and investigated their use as gas sensor at room temperature. the ppy microwires were exposed to ammonia gas and hydrogen sulphide, and sensing behavior was observed in terms of change in electrical resistivity/conductivity using two-probe technique."
"-a motor node m is true at t if the following holds: (i) m is connected to an action node a via an arrow without annotation such that a was activated at t, or (ii) m is connected to an action node a via an arrow with annotation k such that a was activated at t − k."
"on the other hand, it is illustrative to consider that some of the earliest eas were open-ended techniques [cit] . recently, other eas have been developed that share this feature, but are mostly aimed at specialized domains, such as artificial life environments [cit] and interactive search [cit] . in particular, this paper studies the algorithm proposed by lehman and stanley called novelty search (ns) [cit], an ea that abandons an explicit fitness function. instead, ns focuses the selective pressure on finding unique solutions; i.e., individuals that introduce novel information into the search process with respect to the current search progress."
"the combined information in panels a through c can be displayed as a single phase diagram (fig. 2d ) that shows the different dynamical regions. the experimentally relevant ones are color coded the same as in fig. 1 with the characteristic time evolution of the concentration shown as insets. just as in the experiments, three dynamical regimes co-exist: a low amplitude high frequency synchronized state (green) at intermediate exchange rates and s r for a fixed density as shown in (f) (for variation of period and amplitude as a function of bead-density see fig. s1 )."
"data classification is one of the most common tasks in machine learning. posed as a learning problem, classification can be either supervised or unsupervised. unsupervised classification is also referred to as data clustering, where the goal is to organize a set of unlabeled elements into distinct groups using little or no prior knowledge [cit] ."
"for a two-cluster problem with a reasonable degree of difficulty, the initial generations of a gp search should be expected to contain random clustering programs. for the cluster descriptor, behavioral space is organized on a two dimensional surface, such that one axis ul considers the number of ones on the left hand side, first bits; see figure 1 . notice that the middle valley of the fitness landscape corresponds to basically random clustering functions, with worst case scenario performance. hence, ns will push the search towards either of the two global optima, β c 0 and β c * finally, given the above binary descriptor, a natural dist() function for equation 1 is the hamming dis- tance, which counts the number of bits that differ between two binary sequences."
"it is well understood that an ea concurrently samples three different spaces during a search. first, genotypic space, which corresponds to the encoding used to represent each valid solution. second, phenotypic space, which represents the problem space where solutions are expressed. finally, objective or fitness space, that corresponds to the space of performance criteria. in some cases these spaces are clearly distinct, but for some representations it is not [cit] ."
"where αi is the behavioral descriptor of the ith-nearest neighbor of k in behavioral space with respect to distance measure dist, a domain-dependent measure that depends on how descriptors are expressed. if the average distance is large then the individual lies within a sparse region of behavioral space and it lies in a dense region when the measure is small. fitness is only considered at the end of the search, when the individual with the best fitness score is selected as the final solution produced by the ns algorithm. an important aspect is to determine which individuals should be considered when sparseness in computed, since the population changes with every generation, and the sparseness value for the same individual can vary over time. in ns, the proposal is to consider individuals in the current population as well as individuals that were considered to be novel in previous generations; hence, an individual has an intra and inter-generational neighborhood. therefore, individuals with a sparseness value above a minimal threshold ρmin, the second parameter of ns, are added to a population archive. the archive stores novel individuals, and the individuals in the archive are used along with the population to compute the sparseness value. an advantage of using the archive is that it can mitigate backtracking during the search, serving as a memory of the search progress."
"since its original proposal [cit], and in later works [cit], most applications of ns have focused on er, such as navigation [cit], morphology design [cit] and gait control [cit] . as stated before, search algorithms that explicitly contemplate behaviors seem well suited for robotics, since most high-level robotic tasks can usually be solved in structurally different ways, thus guaranteeing a multimodal search."
"for a better explanation lets consider a synthetic problem where a ground truth clustering is provided. then, suppose that the number of samples from each cluster is l 2, and that they are ordered in such a way that the first l 2 elements in t have a ground truth label of ω1. let x represents a binary vector, and function u(x) returns the number of 1s in x. moreover, let ko be the optimal clustering program that achieves a perfect accuracy on the data set based on the ground truth labeling. then, the descriptor of ko is given by"
"the remainder of this paper is organized as follows. section 2 describes the concept of behavioral space and the ns algorithm. in section 3 the clustering problem is defined and related work is discussed. afterwards, section 4 presents the proposed ns-based gp algorithm for data clustering and the proposed behavioral descriptor for evolved clustering programs. then, section 5 presents the experiments and an analysis of the results. finally, a summary of the paper and concluding remarks are given in section 6."
"gp has been used extensively for data classification, see for instance [cit] . in particular, several gp-based clustering systems have been proposed, using grammar based evolution [cit], theoretic probabilistic interpretations [cit] and hybrid multiobjective algorithms [cit] . however, one underlying characteristic that previous methods share, is that they all rely on an explicit fitness function, just like any evolutionary search. instead, in this paper fitness is substituted by a behavior-based search based on the novelty criteria. nevertheless, a measure of clustering performance is presented next, since it is required to compare different results and to select the final solution returned by the ns algorithm."
"densities; a quiescent state (red) exists at high exchange rate but low densities; and finally a large amplitude, low frequency state (blue) at high densities and a wide range of exchange rates."
"we also note that the degree of information exchange between the oscillators (coupling strength) serves as a \"switch\" to access the different states and their regimes, suggesting intriguing engineering applications in similar physical systems. finally our description may serve as a template to explain some natural phenomena such as the observation that the frequency of synchronized chirps in grasshoppers increases with temperature 2 ."
"the dynamical interplay between the oscillators, the medium and the rate of information exchange is reminiscent of global coherence phenomena in neurology such as in epilepsy or mobbing crowds in social networks 7 . indeed, the abruptness of the super-synchronization transition may have implications in biology where synchronization plays an important role in many contexts, including the cell cycle 24 and cooperative behavior in bacterial cell colonies 25 . in addition, the system reaches a high level of self-organization as a consequence of the participation of the medium in the process of oscillator synchronization, brought about by the strong coupling between oscillators. of course, this behavior is analogous to the so-called mobbing behavior found in hyper-communicated social media when the individuals not only have access to their immediate neighbors but they communicate with distant neighbors via social media."
"thus the primary difference between the green and blue states is the following: in the former case beads synchronize among each other and as the coupling increases eventually reach a state of full or complete synchronization with each other, sharing a common phase, frequency and amplitude. in this regime, as suggested by fig. 2g, the dynamics of the medium is distinct from that of the beads immersed in it. however, as the coupling is further increased, in the latter case, there is a second dynamical transition where the already perfectly synchronized beads now also synchronize with the background medium with a common dynamical signature. to distinguish between the well-known complete synchronization state and the newly observed second dynamical transition reported here, we term the blue state a super-synchronized or mobbing state reflecting the strong harmony among the fully synchronized beads with the medium they are immersed in and whose active role is crucial for reaching this state."
"lehman and stanley proposed ns, an ea that eliminates the need for an explicit objective function [cit] . evolution is not guided by a measure of quality, but by the a measure of uniqueness. to measure the novelty an individual introduces into the evolutionary process, the ns algorithm describes each individual within behavioral space."
"in panels b and c we plot the period and amplitude of oscillations as a function of the stirring rate for the same bead density. each data point is an average of multiple realizations of the experiment for a given stirring rate up to a maximum of 1500 rpm (which is the limit of our experimental apparatus). both figures confirm the existence of two distinct synchronization regimes separated by a sharp jump transition in both period (50% increase) and amplitude (25% increase) prompting us to term the blue region as a super-synchronized state or mobbing state, similar to an equivalent phenomenon in sociology 7 ."
"where n is the number of data elements, k is the number of clusters, and ui is the nearest neighbor within the same cluster of x. then, the inter-cluster separation is computed similarly, but now considering the closest neighbor from the other cluster, given by"
"the phenomenon of synchronization among coupled oscillators is fairly ubiquitous in natural and manmade systems. examples in biology include the synchronized flashing of fireflies 1, the chirping of crickets 2, cardiac pacemakers 3, yeast cells 4 and the firing of neurons 5 . in social systems coherence occurs in cooperative crowd effects 6, 7, while in non-living physical systems synchronization is seen, for example, in arrays of josephson junctions 8 and semiconductor lasers 9 . last, but not least, systems of coupled chemical reactions 10 provide representative examples in chemistry."
"a limitation of most eas is their tendency to converge on local optima, a common result in many real-world problems with multimodal and irregular fitness landscapes. within ec, diversity preservation is usually incorporated into an ea to overcome this shortcoming. however, most proposals can be regarded as ad-hoc solutions that must continuously attempt to balance exploration and exploitation during the search. conversely, through the search for novelty alone, diversity preservation introduces the sole selective pressure during evolution. the core assumption behind ns, that suggests why it can find high-fitness solutions, is that at the beginning of a search, most random solutions will exhibit behaviors with bad fitness values. therefore, as ns progresses and it leads the search towards novel regions in the search space, this should also lead towards better behaviors and thus better fitness values; i.e., when initial random solutions have low performance then the search for novelty could lead towards quality during evolution."
"this section presents an experimental evaluation of ns-gp clustering. all of the clustering algorithms were executed 30 times to find statistically significant results. table 2 compare the performance of ns-gp with two baseline methods, km and fc. the table presents two comparative views of average performance over all runs. first, the algorithms are compared based on their cdr score, and the cdr of the ground truth of each problem is also presented. additionally, using the ground truth, a classification error was computed, based on the ordering suggested by each clustering method. in general, the results indicate two notewor- thy trends. first, ns performs much worse on the simpler problems, it seems like it is basically doing a random search. on the other hand, ns noticeably outperforms the control methods on the harder problems, this is especially true for the hardest, problem 5. second, it seems that a lower ρmin encourages better performance in most cases. a detailed view of how the data is being clustered can provide a different analysis of the results. figures 3 -7 present a graphical illustration of the clustering output achieved by each method. all figures show the ground truth clusters for visual comparison, along with a typical clustering output from each method. these figures confirm the data presented in table 2, ns-gp performs worse on the easy problems and performs better on the difficult ones. tively. each figure presents a similar plot that shows how the sparseness of the best individual (based on fitness) evolves over each generation. the plots are averages of the 30 runs of each experiment and present a curve for each problem. a horizontal line shows the corresponding threshold value for each configuration, set to 20 in figure 8 and set to 40 in figure 9 . in both cases it is possible to observe a similar pattern. for the easy problems (1, 2 and 3) the sparseness value of the best individual at each generation does not reach the threshold, and is therefore not included in the population archive. this means that the individual is lost, and maybe it is never found again. this explains the reason why ns fails to produce good results on the easy problems. it appears that since the problems are not difficult, then novelty does not necessarily lead to quality, and ns is not influencing the search as desired. on the other hand, for the more difficult problems (4 and 5) it is clear that good solutions almost always correspond with novel solutions; i.e., the solutions that are being introduced into the archive of novel solutions also exhibit good fitness values. in these cases, the search for novelty is indeed guiding evolution towards solutions that perform better. this is reasonable, since for difficult problems the initial (random) programs will mostly exhibit bad performance, and novelty can lead towards quality."
"finally, to illustrate how evolution progresses with the ns-gp clustering algorithm, figure 10 current population and population archive) based on the cdr value at four different generations. for this difficult problem, it is clear that ns is progressively exploring the search space, and finding better solutions."
"in summary, instead of using fitness to guide the search, ns uses a measure of novelty to characterize each individual. more precisely, a sparseness measure establishes how novel an individual is within behavioral space, with respect to the current population and novel solutions generated in previous generations. such a measure of novelty is dynamic; i.e., it can produce different results for the same individual depending on the population state and search progress. in ns, the sparseness ρ around each individual k, described by its behavioral descriptor β, using the average distance to the k -nearest neighbors in behavioral space, with k an algorithm parameter, is given by"
"one approach is known as semantics in gp literature [cit], with its corresponding semantic space, which corresponds to the space of possible program outputs. semantics is an important concept in gp because many genetically different programs can share the same semantic representation. however, only analyzing program outputs might not be the best approach to analyze performance in some domains. for instance, consider er problems, where evolutionary algorithms are used to search for robust controllers of autonomous robots [cit] . the goal of the er approach, is to find high quality solutions introducing as little prior knowledge as possible into the objective function. in this scenario, the correspondence between program inputs, outputs and induced actions is much less clear. moreover, evolution in an er system is performed within real or simulated environments, where noisy sensors and the physical coupling between actuators and the real (simulated) world can produce a non-injective and non-surjective relation between program output and the actions performed by a robot."
"while these three spaces have been the focus of most research, other spaces have recently been considered within the field. first consider that fitness gives a coarse and global evaluation of an individual's performance, averaging out differences in program quality on different fitness cases. considering this, some researchers have proposed a finer grained approach to analyze the performance of each individual."
", ..., 0l). moreover, for a two-cluster problem, an equally useful solution is to take the opposite (complement) behavior and invert the clustering, such that a 1 is converted to a 0 and vice-versa. this mirror behavior is defined by the descriptor"
"a behavior, defined in this way, is a higher-level description of the semantics of a program. an individual's behavior is described in more general terms, accounting not only for the program output but the context in which the output was produced. if contextual information is not relevant, then semantics and behaviors could be regarded as equivalent. therefore, in essence, fitness, program semantics, and behavior provide different levels of abstraction of a program's performance. at one extreme of this scale of analysis, fitness provides a coarse grained look at performance, a single value (for each criteria) that attempts to provide a global evaluation. at the other end of the analysis scale, semantics describe program performance in great detail. on the other hand, behavioral descriptors lie in between fitness and semantics, providing either a finer or coarser level of description, depending on how behaviors are meaningfully characterized within a particular domain."
"clustering has been studied in various domains, such as computer vision and web development [cit] . given the variety of areas where clustering is applied, a multitude of clustering methods have been developed. most of the clustering techniques are objective driven, such as the well known kmeans (km) that uses a hard partitioning of feature space [cit] or the the fuzzy c-means (fc) algorithm that instead uses a soft partitioning [cit] ."
"the time evolutions of x s and x i in the red, green and blue regions plotted in fig. 2f through h make this effect more apparent. both the red and green states are characterized on average by lower concentrations in beads than in the medium, with the main difference being the appearance of oscillations in fig. 2g. in fig. 2h, however, the signals for the medium and the bead are practically indistinguishable suggesting an identical phase, period and nearly identical amplitudes."
"natural evolution, as an open-ended process, is different from the conventional engineering approach towards search and optimization. nevertheless, evolutionary algorithms (eas) are abstractions of neo-darwinian evolution that are guided by an objective function and mostly lack the open-ended feature of their natural inspiration. nonetheless, many eas have distinguished themselves as innovative search techniques that frequently solve difficult problems in diverse domains [cit] . moreover, since the search is guided by fitness, most of the traditional algorithms tend to converge on local optima unless proper heuristic methods are integrated into the search for diversity preservation."
"this work presents a ns-based gp algorithm for automatic data clustering. to the authors knowledge, the work represents the first attempt to leverage ns to solve this common problem in pattern recognition and machine learning, since previous applications of ns were primarily focused on robotics. this paper develops the idea of program behaviors and how they relate to semantics, and suggests that behavior based search, such as the ns algorithm, can be applicable to many problem domains. for the clustering problem addressed here, a domain-specific behavioral descriptor was proposed and its fitness landscape was analyzed. the ns-gp algorithm was also compared with two standard clustering methods, k-means and fuzzy c-means. initial results are informative and encouraging. the experiments suggest that ns is not well suited to solve easy problems, failing drastically on the examples given here. however, this is not discouraging for researchers from real-world domains, easy problems such as these rarely come up. conversely, the ns-based gp exhibits very good results on the difficult test cases, outperforming the control methods. it seems that for easy problems, the exploration capacity of ns is mostly unexploited or maybe even limits the search; i.e., if random solutions have a high fitness then novelty could easily lead the search towards worse results. on the other hand, since randomly generating a high-performance solution is less probable for difficult problems, then the incentive for behavioral exploration is incremented and the search for novelty can indeed lead towards quality during evolution. future work will center on exploring further experiments in this domain, comprehensively testing different parametrizations of the ns-gp search, evaluating performance on real problems, and comparing the algorithm with other gp systems for data clustering."
"therefore, another approach towards describing performance in er research is to focus on the behavioral space of a problem [cit] 's [cit] . a behavior is a description β of the way an agent k (robot in er and program in the gp case) acts in response to a particular stimulus within a particular context c. a context c includes the internal description the agent has of its environment and its own internal state 1 . stated in another way, a behavior β is produced by the interactions of an agent k, the output y and a context c. in behaviorbased robotics, for instance, behaviors are described at a very high level of abstraction by the system designer. in er, on the other hand, researchers have recently proposed domain-specific numerical descriptors that describe each behavior, allowing them to explicitly consider behavioral space during evolution [cit] . the justification for this is evident, given that the objective function is stated as a high-level goal, then population management should take into account the behavioral aspect of the solutions."
"precise characterizations of synchronization have been made through analytical considerations (phase models based on the kuramoto-family of models 11 ), while coupled electrochemical oscillators 12, reactors 13 and well-mixed populations of catalyst-loaded oscillatory beads have proven to be excellent experimental templates 14 . the latter is particularly interesting, being scaleable to a large population and known to have a panoply of dynamical behaviors ranging from phase synchronization 15 to amplitude-entrainment through external driving 16 to quorum sensing effects 17 . because of their rich phenomenology, systems of globally coupled beads provide an ideal setting to investigate potentially new dynamical behavior. in particular, there are few experimental instances demonstrating non-trivial dynamics beyond the synchronization transition, with the notable exception being oscillator death 18 where an initially synchronized population abruptly ceases oscillations with increased coupling strength. furthermore, while a large amount of effort has been dedicated to examining transitions to synchronization, relatively little is known about the potential existence of multiple states of synchronization. examples abound in nature, such as the observation that the frequency of synchronized crickets adjusts according to the ambient temperature 2 . in neurology, pathologies are known to occur due to abnormal synchronization in pyramidal neuronal cells, so called interictal epileptogenic discharges 19 . these, however, are distinct states of synchronization from epileptic seizures which are global high amplitude patterns found in eeg recordings 20 occurring when globally connected groups of neurons communicate (through spatial transfer of neurotransmitters) with a faster time-scale than that of neural oscillations 21 ."
"here we report on our results in the search of multiple synchronized states in a large population of beads loaded with ferroin ( ( ) + fe phen 3 2 ) as a catalyst and immersed in a catalyst-free belousov-zhabotinsky (bz) solution 22 . our experimental setup, described in 16 and som sec. s1, consists of a continuously stirred tank reactor (cstr), where beads are immersed in the reaction mixture which is then stirred at different rates to adjust their interconnectivity via transport-facilitation of signaling species. a redox cycle occurs through the oxidization of ferroin by reagents in the solution resulting in the production of an autocatalyst activator hbro 2 and an inhibitor − br . the oxidized ferroin reacts with the solution regenerating the reduced form of the catalyst and the inhibitor, with the cycle repeating itself when the latter falls below a particular threshold. a combination of the stirring rate and bead density represents the coupling strength; if one fixes the latter, then the former plays the role of the control parameter in our system."
"to summarize, we document the existence of a novel dynamical state in a population of coupled discrete chemical oscillators. this super-synchronized or mobbing state-characterized by large amplitude, low frequency oscillations-resides in the strong coupling limit (as measured by information exchange and oscillator density), a particularly surprising result, given the conventional wisdom of a complete cessation of oscillations in this regime. the detailed study of an idealized numerical model suggests the origin of this new mobbing state is a result of the interplay between the dynamics of the beads and the medium in which they are immersed. specifically, the super-synchronized state is accessible only when the flow of signaling species (autocatalyst) between the beads and the medium is minimized, a condition one can achieve either by increasing the density of oscillators, the exchange rate or indeed both."
"therefore, the goal of this paper is to present an application of ns on a common problem in machine learning and pattern recognition. in particular, unsupervised data clustering is solved with a ns-based gp system, using a domain specific behavioral descriptor. this work is an extension of recent research where ns was applied to supervised data classification [cit] . results are encouraging, the ns algorithm is indeed capable of solving clustering problems and in some cases outperforming standard clustering techniques. in particular, ns seems better suited for diffi-cult cases, where the explorative abilities of the algorithm can be fully leveraged."
"in order to check the robustness of this effect we conducted several experiments for multiple combinations of stirring rate and bead density. the results are compiled in the \"phase\" diagram shown in fig. 1d where each point corresponds to multiple realizations of experiments conducted for a fixed pair of density and stirring values. the characteristic time evolution of the redox potential in each region shown in fig. 1e allowed us to demarcate three distinct dynamical behaviors of the system. in addition to the green and blue states one also observes a globally quiescent state (red curve and points) at low bead density and high stirring. intriguingly the figure suggests that the large amplitude blue state can be accessed directly from the quiescent red state either through an increase in density or-for a relatively narrower region of parameter space-an increase in the stirring rate. that is to say, while the strong coupling regime leads to oscillator death (as previously established), still stronger coupling leads to the emergence of the reported mobbing state."
"a representative example is shown in fig. 1f where we plot the time evolution of the redox potential for three different stirring rates in the same experiment. the three regimes are clearly visible with the green and blue states now separated by an intermediate red state. indeed, the boundary separating the quiescent and high-frequency oscillatory regimes is reminiscent of a quorum sensing transition as previously reported in 17 . the transition from the red state to the blue state is even more dramatic-the system directly transitions from a steady state to that with large amplitude oscillations-and may be considered a hyper quorum sensing effect."
"we propose a structured approach for realizing compliance monitoring components for business processes. we assume that compliance controls are implemented as business activities or groups of business activities, and in effect, we monitor for the execution of these compliance controls. we evaluate our approach through a number of scenarios in order to determine the impact of our approach and tool support on the productivity and effort a developer puts into specifying compliance monitoring directives."
"in addition to the dsl, we define code generation templates, which generate the compliance monitoring component based on directives specified with the dsl. we demonstrate our approach through an mdd framework. within this framework, we implement a dsl for specifying the compliance directives, as well as the necessary code generation templates for generating the compliance monitoring components."
"there are a number of issues that we consider to be possible limitations and/or threats to the validity of the results that we obtained during our experiments. we consider these from two angles, that is, threats related to the data that we used in the experiment, and threats related to the method we followed to carry out the experiment."
"domain specialists are interested in events of significance in their domain. in some cases, it is not possible to directly observe such special events because they occur as a combination of a number of other events. however, through event pattern languages (epls) [cit] also known as stream query languages [cit], one can configure an event processing engine to aggregate what are termed low-level events into complex (high-level) events. low-level events are not an abstraction of other events and do not have semantic significance on their own within a specific domain, whereas complex events are an aggregated abstraction of a number of other low-level and/or complex events [cit] . epl's are the primary tool through which cep-based components can be configured to filter and correlate low-level events to yield other higher-level, more semantically significant events [cit], that is, special events."
"during the risk assessment exercises, an organization identifies the compliance concerns that it is required to fulfill, and then makes decisions regarding which controls to implement in order to address those concerns. controls may constitute or be applied to subprocesses or activities within a business process, that is, parts of the process may be designed and implemented such that they realize a control that fulfills a particular regulation. consider, for example, the case of the well-known segregation of duty (sod) control. in this control, it is mandatory for certain activities to be executed by two distinct persons in order to avoid fraud. the control may be implemented in such a way that when the same person attempts to execute conflicting activities, the system does not allow this action to happen or allows it but reports this as a violation of compliance."
"regarding the method we followed in carrying out the experiment, the first threat we can identify is that of the choice of the metric. while li and henry clearly state that size metrics are predictors of maintenance effort, they also point out that these metrics are not the sole predictors [cit], that is, they should be considered in conjunction with other metrics. oman and hagemeister [cit] actually propose combining a set of metrics (considered to have influence on maintenance effort) into a single index of maintainability. in a more ideal situation, we could consider multiple metrics for evaluation. we were, however, limited by the choice of a metric that could provide an analysis of both dsls side by side. moreover, most metrics we came across are designed with procedural and/or object-oriented languages in mind. we did not come across metrics that have been explicitly designed with a focus on declarative languages of the kind that we analyzed in this work."
"in order to illustrate the relationship between compliance and business processes, we present here a number of illustrative scenarios from industry case studies of business process compliance. we use the bpmn standard notation [cit] for the diagrams illustrating these scenarios."
"activity instance's are also related to an activity execution order that specifies the order in which a group of activities occurs. for our domain model, we currently define three possible activity execution order options, that is, a sequential order (sequence)-activities occur one after the other in a fixed order-a parallel execution order (parallel)-activities occur simultaneously, that is, all activities occur but do not have a fixed order-an option execution order (option)-activities occur in a mutually exclusive manner, that is, strictly one of the activities shall occur. we have chosen these three options as they can express the most common workflow patterns that occur in business processes. in table 1, we show the five elementary control flow concepts from the workflow management coalition [cit] that are found in most business processes. we illustrate how these patterns are represented using the activity execution order options. besides activity instance's, a process instance fragment is associated with a number of boolean condition's, which specify constraints on data elements (from activity instance's) that are considered for monitoring. they (boolean condition's) are grouped into two kinds, filter's and assertion's. filter's specify data categories or conditions that should be ignored or considered in a monitored process fragment. assertion's on the other hand represent conditions that are expected to be true in the monitored (activity instance) data."
the pattern section of the template is where the activity execution order is specified. we map the activity execution order to event patterns using a mapping scheme that we present in table 2 .
"in our work, we focus on monitoring such compliance controls (especially the automated and hybrid controls) in the context of process-driven soas, which we discuss in the next section."
"overall, we would like to argue that the characteristics of our dsl syntax are in tune with the mental working style or approach of a programmer, that is, through referencing short-term memory for tasks at hand and long-term memory to make broader connections [cit] . with fewer variables and operations to consider, the programmer is better able to quickly understand and maintain the source code. the reuse feature also fits within this model, described in henderson-sellers [cit] as chunking and tracing, that is, the programmer typically chunks together related pieces of information and mentally refers to it from one point of view. this is very much what the dsl provides with the reuse feature."
"one of the challenges with respect to monitoring of compliance is the ad hoc implementation of compliance controls, in most cases using niche products to implement a particular compliance requirement. cep can today be considered as one such niche product in compliance monitoring for business processes (a technique known as business activity monitoring (bam) [cit] ). in this section, we evaluate our approach based on the illustrative scenarios presented in sect. 2.4. we present a quantitative comparison of the dsl presented in sect. 3 alongside an epl for realizing cep-based monitoring; this comparison serves to highlight the differences between our dsl and the epl and to initiate a discussion concerning the pros and cons of our proposed approach."
one of the first steps in developing a dsl is to analyze the domain for which the dsl is being developed [cit] . the domain model in fig. 6 captures the main concepts of the business process monitoring domain. the model is illustrated with uml class diagram notation.
"in a very general sense, all forms of monitoring are related to compliance of some sort. monitoring is concerned with observing the state of a system to ensure that it fulfills a particular goal and to inform an interested party when things go wrong. with regard to our work, the purpose of monitoring is to ensure that our business processes actually execute compliance controls as expected at runtime. whereas the compliance controls are built into the system at design and implementation time, monitoring provides the required runtime validation and ensures that compliance of business processes is auditable [cit] . in this section, we discuss a number of other related works aimed at ensuring or monitoring for compliance at runtime."
"rozinat and van der aalst [cit] present an approach for conformance checking of business processes. while the previously discussed approaches target design time and runtime compliance checking, this approach is more retrospective with respect to execution time, that is, checking for conformance after business processes have already been executed. the authors mine event logs and process models to check conformance from two angles, the fitness, that is, how close do the actual execution logs match with the process model, and the appropriateness, that is, how well does the model describe the actual process execution recovered in the execution logs. this conformance check (specifically the fitness conformance) is similar to how we approach compliance checking; however, we do not consider an entire process in our checks. instead, we consider a subset/excerpt of activities that are specifically in a process for the purpose of checking compliance. the authors, however, do raise an interesting qualitative issue concerning the appropriateness of models. in our mappings from business activities to event detection patterns, we provide all possible combinations. in cases where there would be multiple parallel activities, the possible combinations from the mappings would be exponential, and yet not all possible combinations shall exist in the actual execution of the process."
"in mahbub and spanoudakis [cit], the authors propose an approach similar to ours in that they provide a mapping from business-level information to monitoring patterns. in their work, however, they map from bpel4ws to event calculus. the differences in mapping between the two approaches may have some implications on how they are converted into a running monitor. expressing the patterns using event calculus informs of the changes that happen in the system through time, whereas expressing the patterns using event algebra as we do simply states what are the operations performed on the events."
"in process-driven soas, services are orchestrated in order to realize a business process, each service executing a particular function. from the perspective of business operations, however, the business process is realized by composing a number of business activities. our approach is a systematic method of realizing a monitoring infrastructure for observing compliance in a process-driven soa. compliance of a business process is determined by monitoring controls applied to the process' business activities. in our approach, we refer to organizations that already have in place controls and/or a method of defining and documenting these controls for compliance. we illustrate an overview of our approach in fig. 5 ."
"to measure these qualities, we chose two size metrics to compare programs specified by both dsls. the size metrics we utilize are number of variables (nv) and number of operations (no). variables refer to identifiers that indicate data to be monitored for a particular compliance rule; operations are considered in the traditional sense, that is, an action or procedure involving operands/variables. size metrics are considered as relatively good predictors of maintenance effort even though they are not the sole predictor [cit] . considered from this angle, nv and no both provide indicators of effort and productivity of programmers using our compliance specification dsls. once we decided on the metrics, we applied them to the compliance rules from our illustrative scenarios. we applied the metrics to both the epl and dsl rules."
"our main contributions are a structured approach and tool support to realizing a compliance monitoring component for process-driven soas through model-driven development (mdd) techniques. our approach essentially involves iteratively identifying the controls to be monitored, capturing them (controls) through a domain-specific language (dsl) that we have developed, and finally generating an event-based compliance monitoring component. the resulting component performs runtime monitoring of compliance controls that are realized as process activities or subprocesses. our tool enables mapping of process workflows into patterns of events that are (re)used within our dsl. we believe this mapping scheme is a step in the direction toward automated management of monitoring components. considering that business processes and their compliance controls are constantly changing, we feel that our approach enables rapid development and evolution of a compliance monitoring component. in previous work [cit], we demonstrated the feasibility of event-based compliance monitoring; we developed a prototype of such a compliance monitoring component and analyzed its performance and scalability by running test scenarios realistically mimicking a large-scale process-driven soa with event monitoring. in this work, we use a number of scenarios from industry case studies to evaluate the impact of our approach on effort and productivity of a developer when coding compliance monitoring specifications."
"we applied the goal-question-metric technique [cit] to define appropriate metrics for comparison of our dsls. we selected two main issues to compare in the dsls; effort-how much of the developer's resources (e.g., time) are required to maintain compliance specifications-and productivity-how productive is a developer who is using the dsls for compliance specifications."
"risk-reduction measures are normally implemented as so-called controls [cit] . a control is any measure taken to assure a compliance concern is met. controls may be broadly classified into preventive controls and detective controls. preventive controls aim to avoid risks, whereas detective controls warn of the occurrence of risks. both classes of controls can be realized as manual controls, automated controls or hybrid controls (a combination of the previous two) [cit] . for instance, a door alarm system (manual), a software system realizing segregation of duty requirements (automated), and management reports on system errors/faults (hybrid) are all implementations of controls. during risk management exercises, organizations make decisions regarding required controls and their implementation."
"business processes comprise a collection of related, structured business activities within or across organizations, which produce a specific service or product for a particular customer [cit] . within a process, there typically exists a combination of manual and automated steps (activities/tasks). business activities may be either atomic or non-atomic [cit] . atomic activities are also known as tasks, whereas nonatomic activities are termed subprocesses. subprocesses constitute a number of business activities executed in a specific control flow. upon completion of this flow, the subprocess is said to have completed execution. whereas a business process is primarily aimed at satisfying a particular client need, one of the major considerations taken into account when designing processes is compliance concerns."
"cep is a set of tools and techniques for analyzing and controlling a complex series of interrelated events [cit] . events are observed as they occur and are correlated and aggregated in order to discover and respond to certain event pat-terns [cit] . these techniques have a number of application areas, including policy enforcement and regulatory compliance [cit] . events represent the occurrence of an activity within a system. they may originate from a number of sources, for example, rfid tags, network traffic data, and enterprise application components, and they may contain information, for example, the event source, or time of occurrence. this information enables us to analyze the events and determine how they relate to each other."
"from the results presented in the table, we observe a number of things; when we compare the nv for the epl and the dsl, the epl in most cases has double the value of nv as compared to the dsl. the epl is a verbose language and does not provide possibilities for reusing code. the dsl on the other hand enables extraction of some common business process patterns and possibility to reuse them in multiple rules. we include a count of nv from reused rules in the bottom row of the table (epl does not have reuse features). this reuse drastically reduces the nv that are needed by the dsl for each rule."
"regarding more generic approaches for monitoring business processes and soa, that is, not specifically monitoring as applied to compliance checking, we consider some approaches that incorporate monitoring logic into business processes ( [13, 15, [cit] ), other leverage cep ( [cit] ), and finally business protocol monitoring ( [cit] ). [cit] present an approach to monitor service compositions, that is, business processes. they embed annotations into bpel code, which annotations are later transformed by a preprocessor into bpel statements. in addition, they implement a monitoring web service. this web service is a gateway to an external server component that monitors a process execution. while this approach has the benefit of sticking to the standard services paradigm, the nature of web services, as the authors also recognize, is to be stateless. this implies that it is not possible for the monitoring web service to maintain state while monitoring an entire process; yet, state is required for monitoring a service composition over a period of time. we encountered similar issues in our implementation of the monitoring infrastructure and instead opted to use a messaging queue as a gateway rather than a web service."
"in table 3, we present the results from applying the nv and no metrics to our set of compliance rules based on the scenarios from sect. 2.4. note that in the nv-column, we have a value nvu in brackets. the nvu count represents the number of unique variables in a rule. we perform this count as well to provide further analysis and/or comparisons."
"the rest of this paper is structured as follows. sect. 2 gives background information concerning compliance and compliance monitoring components. also included in this section are a number of illustrative business process compliance scenarios. next, sect. 3 explains our proposed approach, and in sect. 4, we evaluate our approach based on the scenarios taken from industry case studies. section 5 compares our work to the related work, and finally sects. 6 and 7 present discussions and a conclusion to this work."
"under the filters subsection, we specify a number of conditions for limiting the type (and consequently amount) of activity instances considered for a particular monitoring directive. for instance, data may only be of interest below a certain threshold value, and so, compliance monitoring components would only observe these data values and ignore the rest. in fig. 7, [cit] 7:59-73 table 1 mapping business process patterns to activity execution order a loanamount less than eur 1 million and ignores the rest."
"typically, when business compliance is discussed, one thinks of the goal to ensure that the systems of an organization comply with regulatory or legislative provisions or similar business provisions originating from parties external to the organization. common examples include regulations set forth in the basel ii accord, 1 the dutch corporate governance code, 2 and the sarbanes-oxley act (sox), 3 to name a few. these cover issues such as auditor independence, corporate governance, and enhanced financial disclosure. compliance provisions may also originate internally from organization policies concerning how the internal processes are executed. we use the term compliance concerns as an umbrella covering these internal and external provisions. figure 1 gives a typical high-level overview of steps an organization goes through to realize compliance. in the diagram, rectangles with rounded edges represent activities, and arrows show transitions from one activity to the next. we also use dotted lines to group activities under common themes. compliance concerns are related to risks an organization and its stakeholders face in achieving their mission. they provide guidance regarding measures to take to prevent occurrence of these risks, for example, section 404 of the sox act requires public companies to annually assess and report on the design and effectiveness of internal control over financial reporting. however, the specifics of how to implement risk-reduction measures are not normally addressed in these guidelines. these specifics have to be handled on a per organization basis."
"in this scenario, a customer goes to a bank to apply for a loan. in fig. 2, we present an excerpt of the process model indicating steps typically followed. in this excerpt, a person with the role credit broker performs a number of tasks upon receiving a loan request. upon completion, the loan application may be delegated to a supervisor or to a post processing clerk, depending on the size of the loan requested by the applicant."
"the last subsection, assertions, is used to specify expected values in monitored data. should these expectations not be fulfilled in actual runtime data, a compliance control has been violated. for instance, in our sample when the loanamount value is less than eur 1 million, it is expected (assertions) that a person with the role creditor executes the first three activities and then another person with the role postprocessingclerk executes the last activity. if persons with different roles than what is expected execute any of these activities, this is reported as a compliance violation. the dsl thus enables a developer to specify compliance monitoring directives for (subsets of) a process instance to which controls have been applied."
"such code generation templates are specific to the type of event processing engine or technology that one may be using. they need to be implemented initially, and once done, the technology-specific code can be quickly and repeatedly generated. in fig. 8, we illustrate a sample code generation template as well as the resulting code that is generated when this template is applied to the sod compliance rule from fig. 7 ."
"in this scenario, we consider three compliance regulations that fall under the category of sod requirements. this requirement essentially states that activities that pose a risk of fraud or error should not be carried out by the same person. in the loan application process, sod accounts for two of the compliance rules designed into the process. in fig. 2, we see that a loan application is initially processed by a credit broker, who performs preliminary checks on the customer's application files, banking privileges, and then creates a loan file. however, the next tasks of checking the customer's credit worthiness and approving the loan have to be performed by a post processing clerk. additionally, if the loan exceeds a threshold, in this case defined as eur 1 million, the loan application has to be verified by a supervisor. one final compliance rule implemented in the business process (although not shown in the diagram) states that all final approvals of loans have to be carried out by a person with the role manager."
"organizations have to continuously adapt their processes and systems to match ever-changing compliance require-ments. our approach provides a clear change strategy: whenever a compliant business process is changed, the change impact affects the activity execution order, which in turn affect a set of cep rules. hence, the explicit trace links in our approach foster understandability, changeability, and maintainability of our event-based compliance solutions."
"organizations need not invent all controls from scratch. a number of established norms or standards define and describe standard controls that can be adapted and implemented. the control objectives for information and related technologies (cobit) 4 framework, for example, describes control objectives that guide an organization in making choices about which controls to implement. such norms and standards are fairly generic and abstract and must be mapped to a concrete systems implementation."
"compliance with regulations, laws, and policies is a requirement for organizations to avoid negative consequences. these organizations thus have to monitor their information systems to ensure that they still adhere to these compliance concerns. considering that many organizations today implement their systems based on process-driven soas, we are proposing an approach for monitoring business processes for compliance in such process-driven soas."
we propose a systematic approach to map the compliance controls (activities or subprocesses) that are defined in business processes into monitoring queries that can check for this compliance at runtime. we now discuss some of the advantages and limitations of this approach.
"during execution of a business process, the activities executed as well as the order in which they occur depend on actual data in the system. a process monitor is typically configured to observe occurrence of certain conditions within a specific subset of the entire process execution. we capture this idea of a subset of the process in the form of a process instance fragment class. comprising each process instance fragment is a number of activity instance's. each activity instance represents the occurrence of either an atomic activity instance, that is, a single activity, or a group of activities (activity group instance). moreover, each activity instance is associated with specific data elements, for example, activity name, whose actual values may differ depending on the process instance at execution time."
"when we refer to data, we consider the compliance rules as well as their implementation. we have a number of compliance rules in our experiment; however, it is always a question whether there is enough of them to make more general conclusions. the number of rules, therefore, poses a threat to the generalizability of our proposed approach. in addition, one could argue that the implementation of the compliance rules in the two different languages is also a threat, because there is the possibility of bias, in favor of the dsl, in the implementations. we minimized the effect of this threat by reviewing the compliance rules in both dsls to eliminate any unnecessary code."
"in the claims handling scenario, an insurance company has to monitor the process of fulfilling or denying insurance claims from its customers. the insurance company would especially be interested in keeping track of denied claims as these are of interest to regulatory bodies. the process excerpt for the claims handling business process is shown in fig. 4 ."
"the resulting compliance monitoring code provides configuration information to the event processing engine during its initialization; as business processes are executed, the event processing engine observes business activity events to determine compliance of the executing processes."
"although our focus is on monitoring process-driven soas, we feel that our approach is usable even in a situation where there exists ad hoc implementations of compliance regulations. this is due to the fact that the monitoring is event-based, and so, regardless of the technology in use, if it is possible to emit events, one only has to ensure that the format of the event is appropriate. this is easily done using wrappers or transforming adaptors. therefore, it is already possible to use this approach for monitoring during compliance even during a transition from ad hoc to more systematic methods of implementing compliance controls."
"in this scenario, we consider the business process from a travel management agency (tma) that handles travel arrangements for corporations. employees of corporations that are tma clients are able to arrange their travel through services provided by tma. tma has signed contracts with these corporations and has to monitor its applications to ensure that employees using these services adhere to policies agreed upon. we consider two specific policies; first, each corporation decides on preferred suppliers for specific services in order to ease payments. therefore, employees arranging their travel are encouraged to use these preferred suppliers unless it is absolutely necessary to choose a different one. the other guideline for employees when arranging travel is to arrange their travel whenever possible two weeks in advance of the expected date of travel. figure 3 shows the business process model for this scenario."
"using the existing norms and standards that provide guidance on which controls to use, an organization can make choices on how to concretely implement the prescribed compliance controls for their business processes. the implementation of controls usually does not follow a generic strategy and hence business compliance is reached on a per-case basis, that is, organizations use ad hoc, hand-crafted solutions for specific compliance concerns [3, [cit] . this usually means that a separate project is started and develops an individual, custom solution for the compliance concern to be addressed. such solutions usually do not follow a clear architectural concept and result in hard-coded controls spread over the systems, possibly with dependencies to other controls. consequently, it becomes quite a task to ensure compliance and keep up with constant changes in regulations and laws. in this article, we present our approach as a generic strategy to realize a runtime compliance monitoring component in the context of process-oriented soas. our approach essentially follows a model-driven development paradigm for realizing a runtime compliance monitoring component."
"we provide a mapping for control flows to event detection patterns. we feel that such a mapping provides a good basis for automating the process of realizing the compliance monitoring infrastructure. however, such a mapping of bpmn control flows to executable diagrams applies to private (internal) business processes. our approach is, therefore, limited to realizing the monitoring infrastructure for internal business processes. there might be some challenges applying it in a setting with cross-organizational business processes, stemming from the fact that business process execution (event) data are generally not shared across organizations. the organizations only externalize a predefined interface to partner organizations. therefore, without access to the event data from within an organization, we are not able to express the event patterns and consequently are unable to generate a monitoring solution."
"in order to ensure the requirements in the scenarios are adhered to, we implement business process monitors to observe the scenarios during process execution. we use these illustrative scenarios to present our approach in the next section and in the evaluation of our approach in a later section."
"the identification of common activity execution order patterns might foster reuse of compliance rules. whenever another business process can be mapped to the same sequence of technical events, we can identify the same business events. hence, even if the business process activities are not the same but can be mapped to the same event trail, reuse of existing compliance rules is possible."
"muehlen and rosemann [cit] present a process monitoring approach that aims to not only monitor the process execution but also the economic impact of processes on a business. this approach achieves this by taking three views of process monitoring, the process view, resource view, and objects view. they present an architecture for a process monitoring and control system. the essential component for the monitoring is an evaluation method library that contains algorithms for performing the calculations based on monitored data. the library is extensible with the possibility for one to plug in customized evaluation algorithms."
"in addition to the dsl, the prototype comprises transformation templates for converting the dsl directives into code for a compliance monitoring component. in this particular implementation, our target platform was an event-based monitoring component-the esper event processing engine [cit] . therefore, our code transformation-and-generation templates transform dsl compliance monitoring statements into corresponding event processing language (epl) (cf. sect. 2.3) queries for the esper engine."
"essentially, we propose an mdd approach to developing a business process compliance monitoring component. our aim is to realize an event-based monitoring component from these requirements. we provide a dsl with which a developer can specify compliance monitoring directives. 5 the developer draws these directives from the documentation of controls and discussions with compliance domain experts. the developer is able to specify controls, activities to which they are applied, and data from a particular activity required to verify its control. typically, each compliance monitoring directive would comprise an event or pattern of events representing business activities, from which compliance control data are extracted."
"mation systems in organizations. soa-based systems are designed to have different functions encapsulated as services. a process-driven soa [cit] additionally introduces a process engine that orchestrates these services to perform the different activities that make up a business process. in large-scale process-driven soa systems, multiple process instances are executed and coordinated on multiple process engines [cit] . all process instances are realized through invoking operations from a pool of services that are within and sometimes beyond the boundary of an organization."
"in this section, we present background information concerning business compliance and provide a scope for the kind of compliance that we deal with in this article. we also present limitations associated with the approach to implementing compliance in organizations today and how these"
"note that the epl is also a dsl specialized for the purpose of defining queries over event-streams and is, therefore, capable of specifying compliance monitoring rules as well. one may wonder why we need the compliance monitoring dsl at all. the compliance monitoring dsl is used to express compliance control monitoring directives in terms that are in the process-driven soa domain. this design serves three purposes; first, the developer works with process-driven soa domain concepts and, therefore, is not continuously translating soa domain concepts into epl statements. secondly, the code specified under the dsl can be reused for different technology implementations. incase there was a different cep engine to be used, the same compliance code could be used with only the code generation templates requiring changing. and lastly, the developer is better able to communicate and share implementation source code with the (business process or compliance control) domain experts, bringing them closer to the implementation. of course, this implies that a system developer using the dsl is familiar with the basic concepts of processdriven soas and perhaps has some experience implementing them. in the next section, we present an evaluation of our approach."
"the second distinguishable section contains process monitoring directives. here, we specify the process subset that we are interested in monitoring. a process monitoring directive begins with the processinstancefragment keyword. each directive then has up to three subsections to be defined, activities, filters, and assertions. under activities, we use the activity definitions from the declaration section. we may also define activities directly under this subsection; however, this limits the possibility to reuse such definitions in multiple compliance rules."
"based on this domain model for the process monitoring domain, we derive the concrete syntax of our dsl for specifying compliance monitoring directives for business processes. we implemented a prototype model-driven process monitoring framework to demonstrate our dsl. the prototype is built on top of an mdd framework (frag [cit] )."
"a high precision phase analysis technique was introduced, which checks for sudden changes in the phase and amplitude of the extracted enf signal [cit] . this technique does not rely on a pre-built reference database, but there were cases where the deleted or added video clip could have the same phase as the proceeding clip. hence, there are no observable phase or amplitude changes to utilize. as the enf signals are embedded in multiple harmonics along with the nominal frequency range, a multi-estimator model could enable a more robust extraction of enf signals from a weak spectral component [cit] . the estimator model states that the frequency variations of the harmonic spectral range have a larger variance when compared to the nominal frequency. it has also been observed that for different types of recording environments, recording devices with different microphones like dynamic or electric microphones result in enf traces with high snr in specific harmonic ranges as compared to the rest of the spectra. the extraction process includes combining multiple spectral frequency ranges, resulting in a robust signal with a low computational requirement."
"monitoring and managing the network is a critical task for network operators. it is necessary for guaranteeing the security and the performance of the network. it also provides valuable knowledge (e.g., network load, type of traffic, peak hours) use-ful for deploying and assessing new network services, such as the ndn protocol, or scaling existing services. to this end, specific virtualized functions were implemented for traffic monitoring and analysis, in particular for the detection and mitigation of network attacks specific to ndn. in this respect, each virtualized network service deployed in the application layer of the virtualized node is linked with an element manager (em), which integrates a network monitoring function (provided by mmt (montimage monitoring tool)), along with a distributed sdn controller (dsdnc). these virtualized mmt probes and distributed sdn controller pairs allow to distribute the complexity of traffic monitoring over different virtualized network functions in order to consolidate, inter-correlate and aggregate monitored data (pre-processing) before sending them to the mmt operator for deeper analysis in the context of unveiling network anomalies or attacks."
"cybercaptor's inputs are the network topology, vulnerability scans of the machines, fixed and variable costs for applying elementary remediation, operational costs for the infection of a given machine or denial of service and an up-to-date vulnerability database (the nvd database [cit] ). its outputs are the complete attack graph, all the extracted attack paths, their scores and a list of remedies (i.e., list of actions to perform) for a given attack path."
here f s is the sampling frequency and n is the number of fft bins used. the instantaneous frequency estimate of the enf signal is then given as
"since multiple action combinations can be applied, all combinations are proposed so that the operator can choose the best one according to functional / business needs. once a remediation has been chosen, the attack paths are recomputed to take into account the topology changes."
"a major asset of sdn and nfv is to provide a high level of programmability to networks. this can be used to enforce complex security policies, detailed monitoring and fast reaction on threat detection. in the doctor project, collaboration between thales and montimage resulted in a cyber-monitoring and reaction toolset that leverages sdn and nfv concepts, and is adapted to the particular context of ndn. the montimage monitoring tool provides network information on topology, metrics and alerts of the ndn and nfv/sdn network to thales' cyber-captor tool, which relies on an analysis of attack graphs to assess possible attack paths and their level of risk. we describe these two components and their functionalities in this section."
"our prototype is monitored and secured thanks to a pro-active approach with cybercaptor and a reactive approach thanks to montimage monitoring tool, both tools being complementary and needed to secure such a complex and innovative architecture. montimage specializes in the development of network and application monitoring tools for performance and security analysis. his main interests are designing state-of-the-art tools to test and monitor applications and telecommunication protocol exchanges, and the development of software solutions with strong performance and security requirements. he has participated in many research collaboration and product development projects for alcatel, ericsson and montimage (e.g. diamonds-itea2, [cit], doctor-anr). [cit] and has published many papers and book chapters on sdn/svn, testing, network monitoring, network security and performance."
"where f c is the range of enf variations, and it is typically 0.02 hz in us and varies in european and asian countries. f v is the spectral band of interest in each of the k harmonics and f o is nominal frequency. the weight obtained from each spectral bin is normalized and combined with different spectral bins to compute a combined spectrum of all harmonics containing enf. the normalized weight represents the snr of harmonic frequency in different bands. the noise in some frequency band can be eliminated for the spectral bands with very low snr. the approach is computationally more intensive for edge devices, therefore, a fog node was used to perform a second pass on enf estimation on the audio recordings with more robust extraction by eliminating the false alarms produced by the edge devices. the discussion of the edge-fog-cloud hierarchy is beyond the scope of this paper-interested readers may find the architecture description in our related publications [cit] . along with robust audio-based enf extraction, video-based enf extraction module could be added. the video module includes processing video frames with moving subjects, which requires higher computational power for computing super-pixels, averaging pixels per frame or per row, and using alias frequency for enf estimation. although this additional processing can be integrated with the fog node with higher availability of computational power and double authentication process. the drawback is that for the major part of the day, the chances of presence of light source is low, hence there is no source of enf in the video recordings."
"the impact score is defined as the sum of local impacts for all vertices of the attack graph. the local impact for each vertex is defined by the user, often motivated by operational aspects. by default, each rule (e.g., vulnerability exploitation, network access) has a constant local impact."
"in a study [cit], etsi identifies the threat surface of nfv as the union of the threats to generic virtualization and networking. nfv being an implementation of cloud computing technologies for networking, we surveyed attacks that have been performed against cloud computing systems and hypervisors and analyzed the impact of such attacks on nfv."
"as shown in fig. 6, with the variance of gaussian noise increasing from 1 to 7, the average classification rate with pixel value intensity decrease from 90.7% to 73.1%, lbp feature from 91.3% to 78.9% and hog feature from 92.43% to 79.38%. moreover, we observe that with the variance of gaussian noise grows, the occ and rbf svm classifiers depict increasingly notable advantage over the linear svm classifier."
"a laptop was used as a fog node to estimate the same enf signals to verify the signal correlation in the second pass. power recordings were made using a step-down transformer and a voltage divider circuit [cit] and given as an input through a 3.5 mm audio jack. to reserve the computational power, the recordings were made in mono channel instead of a stereo channel. the signals were recorded at the sampling frequency of 8 k hz and was down sampled to 1000 hz for estimating the signals."
"as a supervised learning model, the well-known support vector machine has already gained its dominance in classification issues. an interesting property of svm is that it is an approximate implementation of the structural risk minimisation (srm) induction principle that aims at minimising a bound on the generalisation error of a model, rather than minimising the mean square error over the data set."
"from the sensitivity analysis against rotation shown in fig. 4, we can see that gabor feature shows the lowest sensitivity against rotation, since its recognition rate remains the highest among the four features under the same conditions."
"the doctor project advocates this type of deployment strategy that allows ndn to operate and to be assessed in real contexts without entailing high risks and costs. a typical use case could be the provision of a service (e.g., http web traffic) consumed by real users generating real traffic patterns. from the perspective of users and the internet, the deployment of ndn must be transparent and the services must continue uninterrupted. to achieve this aim, dedicated gateways that convert the http requests and responses respectively into interest and data packets have to be implemented and deployed. fig. 2 illustrates the operation of an ndn/http gateway. basically, an http client sends an http request (red arrow) to the ingress gateway which transforms it into interest packets by mapping the initial url to a name prefix. these are sent through the ndn network via standard ndn routing to the egress gateway, thus benefiting from ndn mechanisms such as caching. the egress gateway collects the unresolved interest packets, reconstructs the http request and sends them to the corresponding web server. the server then sends the data in response to the egress gateway (green arrow) in the form of http messages which, in a similar way, creates data packets and sends them through the ndn network to reach the http client via the ingress gateway."
"the basic idea behind the histogram of oriented gradient descriptors is that local appearance and shapes within an image can be described by the distribution of intensity gradients or edge directions. the implementation of these descriptors can be achieved by dividing the image into small connected regions, called cells, and for each cell compiling a histogram of gradient directions or edge orientations for the pixels within the cell. the combination of these histograms then forms a descriptor. to improve accuracy, the local histograms can be contrast-normalized by calculating a measure of the intensity across a larger region of the image, called a block, and then using this value to normalize all cells within the block. the normalization results in better invariance to changes in illumination or shadowing."
"even though ndn is considered as a clean-slate approach eventually aiming to replace the current ip-based data plane, it appears that such a deployment will reasonably not occur in one shot. to address this, several studies [cit] show to what extent ip and icn (information-centric networks) can co-exist by leveraging sdn. however, if each of these solutions brings a proof of feasibility, they also induce some limits (e.g., inability to carry standard ip traffic, need for an extension of the openflow protocol) due to the antagonist nature of these two networking paradigms. consequently, a progressive deployment approach, standing for a serial combination of these protocol stacks, seems more realistic. this relies on the deployment of icn islands inserted in the global ip network. here, dedicated icn/ip gateways are required to enable data transit through a boundary between heterogeneous domains. this solution can be of great value where icn presents proven advantages when deployed on a particular topological location; and, nfv appears as a promising means to enable such a deployment strategy."
"the alignment method is conducted as follows: firstly, we manually find the eye locations of each face image. then we rotate the images to make the two eyes in the same horizontal line. finally, we scale and crop the face images to ensure the two eyes to be at fixed positions."
"attack paths are scored according to various metrics, in order to automatically present the most relevant paths to an operator. this is done by assessing the criticality of each attack path or the likelihood of their occurrence. attack path scores have 2 components: impact score and risk score."
"cybercaptor does not directly depend on sdn or nfv, but it can improve its efficiency through the combination of both technologies. more specifically, the control plane centralization allows obtaining information on the network's configuration from a single point: the sdn controller. the controller keeps track of all the allowed flows in real time, while in a classical network one would need to periodically gather information on the configuration of firewalls and routers. similarly, the nfv orchestrator can provide information concerning software versions and configuration of the vms without launching scans. furthermore, the remediation recommendations provided by cybercaptor can be, after being validated by an operator, directly sent to the sdn controller and/or nfv orchestrator to be applied. this enables much faster and less error-prone information collection and remediation enforcement than can be achieved manually."
"the original svm was proposed to solve binary classification problems by finding the optimal separating hyperplane. it has achieved great success in linear separable problems. however, most practical problems are not linearly separable. to cope with this problem, previous research has found solutions by using kernel tricks, which maps the data to a higher dimension. the kernel function effectively solves the nonlinear problems, but in the meantime, it also brings limitations in practice: first, the complexity is highly dependent on the size of the training data, which means as the size of the training set grows, it takes increasingly long time to test a smile. another limitation is the lack of theoretic support on how to choose kernel functions and setting parameters for specific problems."
"the orchestrator is in charge of placing the vms on the nodes, triggering automatic scaling, chaining in the case of service chaining, live reconfiguring the vnfs (e.g., changing firewall rules in the case of a firewall vnf), etc. thus, orchestrators are critical elements that centralize all configuration information. attackers can target them either to disrupt services (dos), to gain information on the infrastructure, or even to take control of the data path of the vnfs. for instance, the create_images_and_backing method in libvirt driver in openstack compute (nova), using kvm live block migration, does not properly create all the expected files. this allows attackers to obtain snapshot root disk contents of other users via ephemeral storage. in an nfv over openstack environment this could be used to steal cryptographic keys from other vnfs, enabling further eavesdropping, data modification, or impersonation."
"the performance impact of the monitoring probe can be reduced when it focuses only on part of the network traffic. besides, the monitoring tool can analyze specific vnf security issues and apply advanced algorithms to detect pre-identified risks and attacks targeting the single vnf. it can be adapted to the specific requirements of ndn nodes to analyze ndn activity and detect any abnormal behavior. however, the monitoring tool installed in each vnf consumes part of the memory and cpu allocated for the vnf. this can have an impact on the network operation and can add delays in communications. furthermore, the monitoring tool will have only local visibility of the vnf traffic which compromises the detection of collaborative attacks or attacks involving different network paths. this last limitation is addressed by the sharing of data between mmt probes (p2p cooperation) and by performing centralized analysis (done by the mmt operator) in order to improve intrusion detection capability."
"as regards feature extraction in the training and testing processes, we choose pixel value intensity, lbp, hog, and gabor features. three classifiers, including linear svm, occ svm and rbf kernel svm, were applied in the experiments. we divide the face images into 4 similar sets and each contains similar number of smile faces and nonsmile faces. all of the sets are applied to a fourfold crossvalidation, which means when one set is used as the test data, the other three sets are used as training data. the procedure repeats four times for each set. test variables are shown in table 1 ."
"in order to launch a false frame duplication attack, the attack code works in a controlled environment. it is recognized that environmental factors change continuously, like the light intensity of the surroundings due to daytime or nighttime, an object's position in the point of view (pov) of the camera, or the introduction of new objects [cit] . if there are visible differences between the pre-recorded frames used for attack and the current genuine frames, the security personnel may beware of it immediately. hence the attack code continually looks for any change in the camera's pov and updates the pre-recorded frames with the new changes made in the environment. the environment monitoring allows the attacker to always have up-to-date recorded frames which can be triggered at any instant. for example, using simple facial recognition software in the attack code, an attacker can launch the attack upon detecting a specific face (or as simple as a quick response (qr) code). in this paper, for demonstration we will use a face detection based trigger to launch the attack, and collect the surveillance feed for analysis (discussed further in section 3)."
"algorithms to extract the enf signals from video recordings along with the audio samples can be developed simultaneously. for example, enf traces can be detected in video recordings using optical sensor measurements with indoor lighting [cit] . a light source was required during video recordings. additionally, the availability of enf traces in surveillance camera video recordings made using average pixel intensities per frame was confirmed using frequency aliasing. an alternative approach to extract enf fluctuations from cmos camera recordings uses rows from each video frame leveraging a rolling shutter technique [cit] . this technique cannot be universally applied to all cameras since the idle period at the end of each frame varies per camera manufacturer. although for pre-determined surveillance cameras, the idle period can be estimated beforehand and improve enf sampling frequency.a super-pixel based approach divides a video frame into a group of pixels with similar pixel intensity as known as super-pixels [cit] . the instantaneous light condition variations in these super-pixels are used to detect the presence of enf in a given video file without investing a lot of processing power and time on video files with no enf traces. enf is also used as a source for multimedia synchronization, where normalized correlation coefficient estimates the lag between peak correlation values which determines the shift required for synchronization. an absolute error map obtained between enf signals from a reference database and estimated from media recordings allows tampering detection and timestamp verification [cit] . the error map technique requires computing the absolute error map for every index and shift of signal along with a line detection algorithm with an exhaustive point search and measurement. an enf error map algorithm benefits in a situation where the media files are pre-recorded and reasonable computing resources are available."
"concretely, the lbp operator is employed to obtain the lbp value of each pixel of the image in the uniform mode. then the whole image is divided into same-sized small blocks of which a histogram is calculated, containing information about the distribution of the local texture, such as edges, spots and flat areas. finally, regional histograms are concatenated to build a global description of the face image."
"the development of software defined networking (sdn) in the past few years and the more recent introduction of network functions virtualization (nfv) promise to simplify network management, with a significantly increased flexibility and real-time reconfiguration. the first application has been to apply this new sdn paradigm to existing networks, in order to simplify them. however one can go further in network softwarization, leveraging virtualization in both the control plane and the data plane, to build a fully virtualized network stack. due to softwarization and standardization of hardware, development costs and times are shortened, and the development of innovative network stacks from scratch is made possible. in this chapter, we present the doctor architecture, which makes use of sdn and nfv to implement ndn, a networking paradigm in which routing is based on content names rather than host addresses. the goal of this paper is double: following a presentation of the ndn paradigm, we show how sdn and nfv can be used to provide an infrastructure layer on top of which the ndn stack can be deployed. in the context of doctor, we aim at running this stack in a production network involving real users. we will detail the innovative aspects of the envisioned virtualized infrastructure, from the design of the architecture, to its monitoring and interconnection with the ip-world. we then focus on how to address security in this infrastructure. we perform a survey of the vulnerabilities introduced by nfv, sdn and ndn, and sort them in categories depending on the targeted components. for each attack we identify the target (sdn, nfv or ndn), review possible remediations and assess their feasibility. we finally propose a practical monitoring solution depending on nfv orchestration to collect information on network topology, and on sdn to perform real-time remediation actions. this monitoring is performed by the cybercaptor tool and montimage monitoring tool (mmt) [cit] ."
"sdn and nfv promise a greater flexibility in networks, by the means of a separation between the control and data planes, a centralization of management via controllers and orchestrators and the massive use of virtualization for the data plane, at the expanse of an increasing complexity of the infrastructure. we showed through the architecture of the doctor project how these emerging technologies allow deploying novel network stacks such as ndn that can co-exist with ip thanks to network slicing while bringing new services like optimizing content distribution at the network level."
"3) discrete cosine transform (dct): the dct-based normalization [cit] sets a number of dct coefficients corresponding to low frequencies as zero to achieve illumination invariance. we apply each of these methods to grayscale face images and then conduct face detection with the four features and three svm classifiers mentioned above. the results can be seen in table 2 . abnormally, it seems all the illumination normalization methods fail to work except the case of he method employed with occ and rbf svm classifiers using lbp feature. all the other methods with the four features, however, bring the recognition rate down. it might be caused by the complexity of illumination conditions in the real world. another possible explanation could be the characteristics of a smile face mainly rely on the structure of the face itself and the state of the organs, especially the mouth. since these illumination normalization methods do not work as expected, the following experiments omit the illumination normalization step."
"cybercaptor provides information on possible remediation actions to prevent the exploitation of identified attack paths. this corresponds to a list of actions that need to be carried out on the network topology that will disable the attack path. a remediation action is an elementary change in the topology. each remediation action roughly corresponds to a different precondition. for instance, a patch remediates a vulnerability, a firewall rule remediates a network access, and moving a vm protects it from security incidents on a particular host."
"current networks generally consist of heterogeneous and vendor-locked hardware and software components, with little or no support for interoperability. this leads to complex network management. this vertical segmentation prevents telecom operators from rapidly deploying new services. moreover, innovation cycles are often long, meaning that network operators are reluctant to introduce new paradigms or technology. new networking solutions require being fully designed (often including cumbersome standardization procedures), evaluated, monitored and secured to ensure that they do not disturb existing services and can provide rapid return on investments. faced with these limitations, telecom operators are adopting new approaches for building networks stemming from the wide adoption of virtualization techniques in data centers. virtualization provides greater flexibility in sharing hardware resources, which result in cost reductions and faster service deployment. we are thus seeing the emergence of network softwarization consisting in building network functions virtualization components, which are treated as virtualized software instances deployed in virtual machines (vms). in turn these virtual network functions (vnfs) can be chained and managed via software-defined networking controllers to create end-to-end communication services."
the rest of the paper is organized as follows. section 2 provides the background knowledge of enf and the related work regarding the attacks on a surveillance system. section 3 illustrates the feasibility of launching a frame duplication attack at the edge through actual implementation. section 4 introduces our method to detect false frame injection attacks utilizing the enf signals embedded in the recorded audio and provides available techniques on video recordings. section 5 presents the experimental results that verify the effectiveness of the proposed method. section 6 concludes this paper along with a brief discussion regarding our future work.
"both power and audio recordings with enf traces were made simultaneously and the estimated enf signals were compared based on the correlation coefficient obtained. we have implemented a visual-data layer replay attack and collected both the original and attacked audio recording along with the power recording simultaneously. strong enf traces were observed at 300 hz for both power and audio recordings. figure 9 presents the estimated enf from the power, original audio, and attacked audio recording, respectively. the attacked audio includes pre-recording a selected period of time and was replayed to mask the current original recording. the attack was launched at 300 s and a clear deviation between the original recording (green signal) and the attacked recording (red signal) can be observed. the part of the recording which was replayed is clearly seen from the signal comparison as the enf estimates do not match, which indicates the possibility of forgery attacks on previously recorded media files. the correlation coefficient between the power enf signal and the attacked enf signal will be lower for the replayed part of the recording. figure 9 conceptually validates the idea that enf traces to distinguish an anomaly incurred by the injected frames."
"the novelty of ndn [cit] relies on the key concept of naming content objects instead of naming hosts with ip addresses. ndn uses a hierarchical naming scheme for content objects, such as the uniform resource identifier (uri). communication in ndn is achieved using two types of packets: (1) interest packets, and (2) data packets. a user issues a request for some content by sending an interest packet. in return, a data packet containing the requested content is sent back to the user. in ndn, a router implements many interfaces that represent a generalization of those provided by ip networks and include three main components that enable the forwarding process. first, the content store (cs) that is a local cache intended for improving content delivery by storing recently requested or popular content. secondly, the forwarding information base (fib) that contains routing information related to the name of interest packets. finally, the pending interest table (pit) that contains the state of emitted interests with the purpose to route back data packets and to aggregate requests. more precisely, for each forwarded interest, the incoming interface is added to the corresponding pit entry if not already present, so that the corresponding data can be sent back to the user. for each data received, the corresponding pit entry is removed. consequently, ndn defines a stateful data-plane which enables efficient routing of interest and data packets."
"we first designed a virtualized node to be able to deploy multiple network services as software instances or virtualized network functions over a single physical host. each deployed vnf thus run on one or several virtual machines, depending on the design. as such, the doctor virtualized node can be structured into three layers. the application layer contains the vnfs, deployed as virtual machines over a virtualization layer which provides an abstraction for the underlying hardware resources offered by the physical hosts. a virtual network based on programmable virtual switches is then implemented to ensure end-to-end network connectivity between the virtualized machines, but also to enable network automation at the control plane."
"the complete attack graph for a company network is very large (potentially millions of edges for a few hundred machines), so that it is not relevant to present it to an operator. due to the complexity of many information systems, focusing interest on particular subgraphs of the attack graph is necessary. a noticeable subgraph category is attack paths."
"regarding the control-plane, the following two topology poisoning attacks are unique to sdn and affect major sdn controllers such as floodlight [cit] and opendaylight [cit] . they aim at deceiving the controller regarding the topology."
its leaf nodes are all facts of the topology that can be used to attack a particular target. attack paths consequently show the subset of facts that can be changed in order to thwart the attack. (erreur ! source du renvoi introuvable.).
"vnfs are software components providing network functions, so they are likely to be vulnerable to classic software flaws, such as: denial of service dos), bypass of isolation, and arbitrary code execution using, e.g., buffer overflows. denial of service is not a new threat, but in a virtualized environment, its scope changes since dos attacks can have side effects and affect other services collocated with the tar-get. arbitrary code execution allows an attacker to take over a vm or a vnf component, potentially compromising the whole vnf and providing a machine to continue or launch attacks. the principle is the same as for classical software, and such vulnerabilities are widely described in the cve (common vulnerabilities and exposures) database."
"in order to measure the risks faced by nfv and sdn, we adopt a practical point of view and survey the attacks. by attacks we mean any kind of malicious activity trying to collect, disrupt, deny, degrade, or destroy information or resources [cit] ] particularly targeting nvf and sdn. for this we identify the components that are likely targets, the possible attacks against them, and propose ways to detect and mitigate attack occurrences. although sdn and nfv are distinct technologies, they are complementary to form the infrastructure layer on top of which services are built. therefore, threats on them can be assessed following the same taxonomy, i.e., the separation between control and data planes, which leads to a similar separation of threats."
"increasing the number of attacks on smart surveillance systems presents more concerns on security. in this paper, we discussed a visual-data layer attack on video surveillance systems and introduced a novel detection method leveraging enf signals. enf fluctuations were inferred to be similar at different locations at the same time instantly, and these enf traces were embedded in media recordings through various factors. the enf estimations from power and audio recordings were estimated simultaneously, and a correlation coefficient was used to evaluate the signal similarity. a low correlation coefficient indicated that the signals were not similar, which in turn implied the potential existence of maliciously injected duplicated frames. a sliding window-based approach was proposed for online detection and different parameter values were investigated to obtain the best setting."
"in order to successfully perform the ifa in a combination of ndn and ip domains, an attack must go beyond the basic ifa mechanism. a possible attack scenario consists in stretching the responding delay of any http answers with the help of a malicious website (fig. 3 ifa setup in an ip/ndn environment). the consequence of this scenario is that ip and ndn do not protect themselves, as before, but rather make the phenomenon harder to mitigate. from an ip perspective, the symmetric nature of the traffic, as well as its rate-limited nature, makes it an ideal candidate for the definition of detection rules that an intrusion detection system can implement. in ip domains, the attack traffic cannot be separated from the legitimate one. by contrast, in the ndn domain, the delay spent by interest to get data packets unavoidably fills the pit and prevents the nack from removing these illegitimate entries. decupling this pattern by endorsing a sufficient amount of partner websites can easily lead to pit collapses in the ndn nodes."
"communications between the vnfs and nfv mano are subject to classical network eavesdropping and tampering though man-in-the-middle (mitm) attacks. however, authentication, encryption (tls) of the communication and the use of a dedicated control network can prevent this type of attacks."
"the link fabrication attack could be detected by authenticating lldp packets, introducing the same large overhead and pki issues as for host location hijacking. another detection mechanism proposed is in the hypothesis that the attacker is not on an sdn switch but on a host linked to the network. in this case, sdn switches could tag all their ports as host or switch, depending on whether they are connected to a host or another switch. such identification is possible by detecting host-specific traffic (e.g. dns, arp) on the links. since lldp packets are only exchanged with other switches and the controller, any lldp packet coming from a host-tagged port would be detected as an attack and dropped. to evade this detection an attacker would have to stop all host-specific traffic on his machine. while this is possible on the attacker's own machine, it would disrupt normal service on a compromised host, leading to detection."
"inspired by the characteristics of enf signals, this work explores the feasibility of applying it to detect malicious frame injection attacks at the edge. in order to obtain a reliable enf signal from the surveillance systems, we opted to use audio records as the source, which is insensitive to light conditions. a reliable database for authenticating the extracted enf was created utilizing robust extraction techniques like the spectral combination of multiple harmonics. a correlation coefficient threshold based method was introduced to detect the existence of duplicated frames inserted by the attacker."
"enf signals have been adopted in digital forensics to authenticate digital media recordings [cit] . the use of the enf technique was first demonstrated to authenticate media recordings as proof for legal jurisdiction purposes so as to verify whether or not evidence was tampered with. the enf authentication technique was introduced and multiple extraction processes have been discussed [cit] . many forgeries as false evidences were detected using the instantaneous enf signal. robust extraction of the enf signals has been an active research topic and multiple signal extraction and tracking algorithms have been proposed [11, [cit] . the signal extraction experiments on alternating-current (ac) powered recording devices and battery-powered devices reveal the source of enf in a battery-powered device is the acoustic hum generated by the electrical devices connected to a main power source [cit] . these experiments show that the main power noise source in the proximity of the recording devices can result in capturing enf traces."
"the deployment of mmt probes inside vnfs or as nids and the collaboration between distributed probes, directly using p2p communications or through the centralized application, allow to dynamically build the network topology and even to detect at runtime any change that may occur during the network operation (e.g., adding or removing network nodes and functions)."
"our main objective in the doctor project is to design a flexible and secure service-aware network architecture. the doctor virtualized network architecture is designed with the nfv concept in mind to efficiently host network functions and services which can be performed at high throughput. based on the sdn principles, the network control is separated from the data plane and is delegated to a controller. this controller allows configuring data routing, managing and orchestrating network services. these services include network monitoring that makes it possible to secure the overall virtualized architecture for the detection of network anomalies and attacks. fig. 1 shows an overview of the doctor virtualized network infrastructure, including the functional blocks and their interactions. note that the interactions or interfaces are numbered in the figure with different two colors (green and purple) to separate the sdn control plane for virtual network configuration from the nfv management plane, which concerns the virtualized functions."
"these previous studies demonstrate the usefulness of enf, so we adopted this technique to extract enf signals from our surveillance recordings. various environmental factors and device-related scenarios, like wave interference, the doppler effect, and movement of the recording device with respect to the noise source could affect enf capture in the audio [cit] . for instance, due to the different types of microphones used, the enf signals were embedded in multiple harmonics. figures 3 and 4 represent two enf instances recorded at the same time in two different rooms and different buildings. the enf signals were very similar throughout a power grid and the slight shift might be due to the oscillator error in two different device recorders."
"for instance, if we consider the serial combination of ip and ndn networking domains, deployed into a virtualized infrastructure, one can easily understand that the stateful nature of ndn combined with in-network caching will exhibit different security properties as compared to the stateless nature of ip. to further understand the impact of this coupling on the overall security, we consider the ifa usecase previously described but now implemented in a scenario in which ndn and ip are coupled to forward web traffic. in this case, an attacker, located in an ip domain who wants to reproduce an ifa in an intermediate ndn island by leveraging http traffic, may try to flood the network with http requests for nonexisting web content. however, as illustrated in fig. 2, users are not directly connected to the ndn network but to the ingress gateway, thus moving the problem to this entry point that should be able to detect flooding attacks with regular dos mitigation strategies for ip networks."
"before introducing our enf-based detection mechanism, this section investigates the feasibility of an automated real-time frame duplication attack at the network edge by an experimental case study. the constructed attacking system also serves as the testbed for detection scheme validation."
"deploying mmt as a nids allows monitoring the ndn network traffic to obtain a global view of the network comprised of metrics related to qos (e.g., response times) and detections of attacks targeting different ndn nodes. however, nidss are used to monitor ndn network traffic and alert on suspicious activity that violates network security policy. typically, one network node is tapped from which the nids then gains its input. what network node should actually be tapped for the nids depends on the network structure in use. however, ids systems in general function best in environments with limited amounts of noise. in very noisy environments the systems typically produce large amounts of alerts including a number of false positives. thus, nids need to be placed at strategic points to monitor traffic to and from the different devices and virtual machines and network policy will be enforced by the security rules defined and activated."
"where f p enf and f a enf are the frequency estimation of the enf signal from power and audio recordings, respectively. µ and var are the mean and variance of the frequency signal. l is the lag between the two signals. even though the recordings are made at the same time, due to the oscillator error between the two devices the signals are not in sync. the lag is used to match the signals and a threshold decides the similarity between the two signals. if the difference between the reference and the current detection goes beyond a certain threshold, the system considers that a false frame injection attack is detected."
"the doctor virtualized network infrastructure also includes a framework providing dynamic configuration and management, as well as real-time security enforcement in the virtualized network. the proposed control and management plane (as represented on the right side in fig. 1 ) consists in two function blocks:"
later research extended the operator to use neighbourhood of different sizes [cit] . using circular neighbourhood and bilinearly interpolating the pixel values allow any radius and number of pixels in the neighbourhood.
"1. the virtualized infrastructure manager (vim), responsible for provisioning hardware resources to vms (computing, storage, networking, including vm (re)configuration or migration, etc.) when necessary, based on the mmt operator decisions (interface 3 in fig. 1 ). to this end, the vim controls the hypervisors of the doctor virtualized node by using the interface 7. 2. monitoring and securing the vnfs, to secure the whole virtualized networking infrastructure. this is implemented by the doctor security orchestrator. the mmt operator is responsible for coordinating traffic monitoring provided by the mmt probes distributed in each virtualized network service deployed in the project (interface 4). the mmt operator interacts with the cybercaptor manager (interface 1) for network security analysis (attack path detection and remediation). 3. management and configuration of the network functions implemented with the vnfs. the mmt operator obtains information from the cybercaptor manager related to network security policies. it is thus able to apply remediations or corrections on the virtualized network functions in response to network misuses (interface 2), through the vnf manager using the interface 5. if needed, the vnf manager can ask the vim, via interface 6, to orchestrate (or allocate new) hardware resources for the vnfs."
"where v belongs to the coding anchor points and c v (x) is the related coefficient. imagine we select m anchor points, then w t would be an m by n matrix and each row w (v) t is the corresponding anchor vector."
an attack path is a subgraph of an attack graph corresponding to all graph nodes an attacker can cross to reach a certain objective (generally execute code on a given machine). it is a directed acyclic graph (dag) rooted on the target machine.
"enf signals can be extracted using various techniques from both audio and video recordings. the collection of enf signals is also affected by many factors including the environment of recording and the recording device itself. initially, enf traces were found in recorders that were directly connected to a power grid, and other researchers showed that enf signals are also present in battery-powered devices [cit] . the source of enf in such battery-powered devices is the audible hum from any electrical device running on power from the main grid and generating noise, where the noise carries the time-varying nature of enf traces [cit] . for battery-powered devices, a device in motion can have high noise and interference caused due to air friction in the enf frequency zone, hence making enf extraction more difficult [cit] ."
"an attack step needs all its preconditions to be true to satisfy its postcondition. for this, \"and\" logical nodes are used. on the other hand, \"or\" nodes represent different ways for an attacker to gain some level of privileges on the network (e.g., different attack steps that lead to the same postcondition). leaf nodes are nodes without preconditions. they correspond to elementary preconditions, or \"facts\", i.e. information given as input. these facts are the conditions that can further be remediated. in the example shown by 5, there are 4 leaves, 2 and nodes and 2 or nodes."
"physical infrastructure security and human safety rely on surveillance systems to monitor activities with minimal human intervention. a common example is audio-video systems for detecting human trespassing [cit] . some methods also provide safety by alerting first responders with emergent events to improve safety [cit] . on the other hand, the proliferation of smart surveillance systems has made them attractive to physical-layer, network-based visual data attacks [cit] . these attacks are primarily designed to compromise of audio-video feed to disguise malicious activities or prevent"
"among the features of pixel value intensity, lbp, gabor and hog features, hog feature can best represent a smiling face. in all the cases tested, it always gains the highest accuracy. all the other three features perform better than pixel value intensity, but one point deserves mentioning is that when the size of face images is small, lbp and gabor features tend to be inferior to pixel value intensity. as for the classifiers, in terms of accuracy, occ and rbf svm classifiers perform almost equally well and both outperform the linear svm with pixel value intensity. and when it comes to time consumption, occ svm classifiers are ten times faster than the rbf kernel. compared with linear svm and rbf svm, it seems that occ svm balances well between accuracy and efficiency."
"the caching technique also helps reduce the impact of denial of service attacks. in this type of attack, a targeted machine is flooded with superfluous requests in an attempt to overload systems and prevent legitimate requests from being ful-filled, but the caching mechanism intrinsically protects content servers from flooding attacks."
"the trigger detection in the \"deploying attack module\" is responsible for detecting a pre-determined event and using the audio-video replay recording as pseudo-live feed. in this paper, face detection (of the attacker) module was used as a triggering event. for modern surveillance cameras, a high-quality video stream was captured with decent frames per second (fps) compared to the surveillance cameras a decade ago. for the face detection module, the fps processed was lower, but a single frame with the required face model detected was enough to trigger the attack and make the processing speed irrelevant. in the face detection module, we used histogram of oriented gradients (hog) for fast human/face detection [cit] . the gradients of human faces were trained using a machine learning algorithm, where each face has a unique encoding. the perpetrator's face encoding was generated beforehand and embedded in the algorithm. when the perpetrator showed up in the camera view, the encoding vector was detected, and this event was used as a trigger mechanism for the replay attack. to avoid suspicions by deploying the attack as soon as the face was detected, the attack was instead placed on hold until a static scene appeared again, and then the frames were replayed to mask the live feed."
"it is worth noting that the doctor virtualized network infrastructure is designed respecting the recommendations from the etsi nfv group, while leveraging the sdn principles for decoupling the control functions from the data plane. thus, the application layer of the doctor virtualized node consists of different vnfs which provide the suite of network services needed to deploy ndn; the virtualization and infrastructure layers of the node represent the nfv infrastructure (nfvi); and, the northbound interface of the control and management plane in the doctor virtualized infrastructure implements the nfv management and orchestration. the doctor controller in the southbound interface is intended for making the virtualized network services programmable, allowing them to be managed and controlled by a central element. the sdn principles are thus implemented by the controller, enabling: a clear separation between the control and forwarding planes; and, the centralization of network control to dynamically configure the network functions through well-defined interfaces."
the enf signal estimated from both the power recording and audio recording for a small duration were compared to check for similarity using a correlation coefficient between the two signals [cit] . the enf signal from power p enf and audio a enf is given as:
"while the proposed system was focused on audio recording to detect frame duplication attacks using enf fluctuations at edge devices at a low computational cost, it was also possible that the enf harmonics were contaminated due to other electromagnetic interference and affected the enf signal estimation. to establish a secondary reliable system, our ongoing work includes developing lightweight estimating method using the enf from the video recordings and using the proposed technique to achieve a more robust real-time authentication method for smart surveillance."
"although cybercaptor does not depend on specific methods to gather the necessary knowledge (e.g., network scans, static configuration file analysis, vulnerability scans), sdn and nfv offer ways to obtain the required information. for instance, the monitoring tool can retrieve the network topology from the sdn controller, and the orchestration relations and vms placement from the nfv orchestrator."
"on the data-plane, a known distributed dos (ddos) attack against sdn consists in flooding a switch by sending crafted packets with many different source addresses/ports. each different source address leads to a flow miss and the packet is forwarded to the controller. this results in the saturation of the link between the controller and the switch, and of the controller's computing capacities."
"the host-location hijacking attack could be tackled by adding an authentication mechanism on the packets, making sure received packets are issued by the legitimate host. however, this would require signature verification for each packet (with large overhead) and an additional public key infrastructure (pki) for the hosts. another proposed defence is to monitor pre-conditions and post-conditions surrounding a host migration. for instance, the pre-condition for a legitimate host migration is that the former location of the host and the corresponding switch port are not used anymore, and that the controller has received a port_down message. similarly, a post-condition is that the host is unreachable at the previous location after migration. as a detection mechanism, the controller / switch could check these conditions when a migration occurs, and any migration that violates them (spoofed message from an attacker) could be detected and ignored in the host profile."
"this information, as well as the detection of network incidents including functional or non-functional incidents, allows providing valuable input to the cybercap-tor tool to assess the risk of such adaptive virtual network and propose relevant remediation to mitigate the impact of a vulnerability or stop an ongoing attack (e.g., malicious data exfiltration or scans). the remediation action to be taken needs to be selected at runtime (preferably in an automated way) and then orchestrated by the vnf manager and/or sdn controller to ensure the security of the nfv/sdn-based environment."
"mmt and cybercaptor are therefore fully complementary: the first can provide from its deep monitoring the detailed states of the virtualized architecture to cybercaptor, which can in turn give back the critical attack paths to be monitored and the remediations to perform, leading to a very efficient architecture to secure the deployment of ndn as virtual network functions."
"the orchestrator is subject to classical software vulnerabilities, so detection methods include hardening the machine on which it runs, logging all events and syscalls, or running the orchestrator inside a vm to benefit from virtual machine introspection. since it is a single point of failure, redundancy is required to avoid dos attacks."
"network softwarization, in both control and data planes, generate a new attack surface that can be expressed as a set of vulnerabilities. these vulnerabilities target sdn control and data planes, nfv control plane, the virtualization layer, the accounting system, etc. in this section we propose a classification of the vulnerabilities related to sdn and nfv that need to be taken into account in a virtualized infrastructure."
"with normalized pixel values, we calculate the gradient of each pixel point. then we divide the face image into small cells, for example, 5*5, and calculate the histogram of each cell to form the cell descriptor. and we combine several cells to a block, and concatenate each cell descriptor into a block descriptor. finally, block descriptors are concatenated to build a global description of the face."
"nowadays, video surveillance systems are arguably the most popular measure for the safety and security of physical facilities and residents of communities. the emergence of more sophisticated attack tools and methods has brought deep concerns to researchers and stakeholders. network-based attacks like cross-site scripting, buffer overflow, sql injection, and boot loader or firmware attacks give privileged access to unauthorized people. gaining root access allows attackers to impair the normal function of a surveillance system by conducting more attacks, such as blinding cameras, disabling video sensors, eavesdropping, as well as data exfiltration and infiltration oriented visual-data layer attacks. these suspicious activities could escape from detection and the attacker may even gain command and control over the surveillance network [cit] ."
a raspberry pi model b was used as an edge device where the surveillance system was operating. an additional module with a sound card was added to record the power recording at the same time as the audio recordings. a python based code was used for the implementation and estimation algorithm of the enf signal. the python's parallel threading enables capturing and estimating the power enf and the audio enf simultaneously. the recordings were stored as a file in the common database.
"however the added complexity, both in sdn/nfv and in the ndn stack, brings a large attack surface, which we tried to assess, in order to thwart the most likely attacks. for each technology, we presented the main known attacks and ways to detect and mitigate them."
"cybercaptor is a security monitoring tool based on an attack graph model. initially developed for physical networks, it was later adapted to virtualized networks and eventually ndn in the particular context of the doctor project. it is composed of four main modules forming a data pipeline, and a graphical visualization interface. these modules are attack graph generation, attack paths extraction, attack path scoring and remediation. the first three modules are automatically chained (with parameters given by the operator), while the remediation module requires manual validation to commit a remediation proposal."
"a gabor filter can be seen as a sinusoidal plane of a particular frequency and orientation, modulated by a gaussian envelop [cit] . a 2-d gabor function g(x, y) and its fourier transform g(u, v) are defined as"
"in practice, a responsive surveillance system has to provide alerts instantly rather than help discern the problem from a delayed forensic analysis. therefore, a sliding window-based approach was introduced to extract and estimate the enf from online records. a thorough study was conducted for a better understanding of the different setup and overlap times between each enf estimates. comparisons were made with the correlation coefficient between those estimated enf signals. figures 10 and 11 show different window sizes used at the initial process. based on the comparison between different shifting step lengths, it is clear that a window size of 25-30 s is the minimum to obtain a constant correlation coefficient of 0.8 and this value can be used as a threshold to detect dissimilar enf signal estimations. figure 10 is the correlation coefficient between the power signal and original audio signal. figure 11 is estimated between the power and attacked audio signal. it is clear that the correlation was higher for the original audio signal compared to the attacked audio. figure 12 is a detailed comparison between different window sliding step sizes. it is clear that with a smaller step size, a higher correlation coefficient value was obtained compared to larger step sizes. however, the experimental study also shows that the computational overhead was higher with the smaller sliding window step sizes. a balanced point is that a window step size of 5 s allows a real-time response in case of mismatching signals. taking multiple factors into consideration, our experimental results suggest a threshold for a correlation coefficient between two signals to be 0.8. a correlation coefficient above the threshold value of 0.8 means the video/audio stream is normal, while below 0.8 implies the possible existence of injected false frames. the lower the value is, the higher probability of attack. figure 13 is the comparison between different window sizes with a sliding window shift step size of 5 s. even though the window size of 10 s has smaller initialization delay, it is susceptible to a high false positive rate. the fluctuations in the correlation can be seen for original recording where some windows are not similar. in case of a 30 s or 60 s window size, the detection of frame duplication attack is similar. the 60 s window has less fluctuations between adjacent windows and the threshold of 0.8 clearly separates the distribution of duplication attacking scenarios with the actual normal recording. comparing with figure 14, which represents a window shift step size of 10 s, it is clear that the shift step size has a lower impact compared to the window sizes."
"in this paper, enf signals are extracted from audio recordings made by surveillance cameras connected to the power grid. audio signals are recorded at a sampling rate of 8 khz. this sampling rate provides room to capture the enf traces in multiple harmonics including the nominal frequency of 50/60 hz and consumes less storage. meanwhile, the high video frame rate of surveillance cameras makes it difficult to capture the enf that varies with high time resolution. some earlier research has extracted the enf signal by capturing changes in light intensity using optical sensors, aliasing frequency, rolling shutter, and a super-pixel based approach [cit] . however, these techniques are computation intensive, which makes them impractical for edge devices."
"the face detection model was used as an example to demonstrate the remote triggering capabilities of malicious algorithms. the trigger mechanism could also be performed manually using a command and control server to communicate with all the compromised surveillance cameras or by using a naturally occurring event to leave no traces of the attacker appearing in the frames. other examples of a triggering event could be a specially designed qr-code on a t-shirt, a unique hand gesture, or even a voice-activated trigger. figure 6 shows the frames observed by the camera (i.e., \"live feed\") and frames captured or delivered by the camera (i.e., \"duplicated feed\") when the attack is launched. in figure 6a, the face encoding of a user (i.e., perpetrator) has been stored in the algorithm. when the perpetrator enters the scene, the camera detects the face along with other faces in the scenario. the perpetrator could walk into the scene with a group or individually, as long as the camera can detect the face and match it with the embedded face encoding. the hog encoding is unique for different face structures, and hence it is faster to deploy a facial recognition algorithm at the edge. once a static scene is detected, the duplicated frames are replayed. here, we opted to deploy the attack once a static scene appeared again instead of immediately launching the attack. deploying the attack with static scene avoids suspicious artifacts like the sudden disappearance of a person from frame, and detecting duplicated frames in a static scene is harder than frames with objects in motion [cit] . in figure 6b, the periodic changes in the environment is reflected in the replay recording. the algorithm checks for changes in the environment every two s and updates the stored recording accordingly. the second column represents the recording stored for future deployment of attack. the duration of the recording made is also modified based on the indoor-outdoor requirements. the capability of the attack algorithm to adapt to changes in real-time shows the reliability of the algorithm in fooling human perception and reducing suspicious behavior when a camera does not reflect the changes according to the environment. for example, a replay recording made at noon is used at night time; this can easily raise suspicion and alarm the authorities. along with the video frame duplication, the audio samples are also masked. figure 7 represents masking noise made during the replay attack with its pre-recorded audio samples with no or less background noise. the allowed noise depends on the threshold used to compare the frequencies in fft. for an indoor application, the noise level is assumed to be minimal, so higher frequency noise is eliminated from the replay recordings."
"in this paper, we focus on data infiltration based visual-data layer attacks. frame duplication attacks are one of the most frequently encountered forgery attacks on a live video feed. once the attacker has gained access over the surveillance cameras through network attacks, the attack code can control the surveillance output. by inserting previously recorded video and audio frames with normal scenarios, the on-going suspicious activities, personnel, or objects may go undetected. many methods have been proposed to detect the replay attacks using spatial and temporal domain similarities by extracting features from the video frames and analyzing these features to detect frame forgery [cit] . these algorithms mostly extract features from a video sub-sequence and compare them with other sub-sequences for similarity [cit] . a number of correlation techniques [cit] have also been adopted to identify frame duplication and region duplication in a video. all these similarity detection techniques require a stored surveillance recording database, and hence they require much computation time to process each video frame. in the case of surveillance systems, the late discovery of such forgery after the event does not afford intervention, incident capture, or property anti-theft. real-time detection and alarm indication is a top priority."
"mmt can be deployed as a network based intrusion detection system (nids) in a separate virtual machine. this nids can be placed at strategic points within the network to monitor traffic to and from the different virtualized network functions (e.g., ndn nodes, http/ndn gateway, firewall). the chaining of the virtual machine is configured by the virtualization layer component (e.g., open vswitch) to place the mmt nids just after the http/ndn for intercepting the ndn based network traffic. in this way, mmt can passively analyze traffic on the entire subnet, and match the traffic passed on the subnets to the library of known attacks. once an attack or abnormal behavior is identified, an alert will be sent to the administrator via the mmt-operator."
"risk scores model the likelihood of the realization of an attack path. it is computed from the leaf nodes of the attack path to its root: each leaf represents a fact, with a default risk (depending on the fact), and each and and or nodes has a risk depending on the corresponding fact or rule and the number of ingoing and outgoing vertices of the node."
"for our testbed, we considered indoor environments where the changes in pixel values were more stable in comparison to outdoor environments. the changes which occur indoors are people walking, gradual changes in natural light intensities, and artificial light changes. the algorithm was tuned to detect these changes in the frames by using a gaussian blur on incoming frames. the gaussian blur performs convolution on the image, acting as a low pass filter and therefore attenuating high-frequency components more than the lower-frequency components. since human movement in the camera view appears as a low-frequency change while noise is a high-frequency change, removing the noise helps the algorithm better distinguish human motions from noise. below is the gaussian function for calculating the transformation to apply to each pixel in the image:"
"enf traces appear in different harmonics with increasing frequency variations at different spectral bands. figure 8 shows similar enf fluctuations at odd/even harmonics. the power recordings were not affected by any noise since it was directly extracted from the power outlet, but in case of audio recordings, external noise could be captured and interfere with the enf frequency ranges. the noise could lead to an inaccurate estimate of the enf signal. a more robust technique was proposed to combine the spectral frequency bins from different harmonic bins based on the snr [cit] . the snr is represented as the weight of spectral band, computed as the ratio of the mean of the psd in the enf frequency range to the mean of spectral bin of that harmonic frequency."
"face images from fig. 1 and fig. 2 witness a wide range of illumination conditions. to alleviate the impact of illumination conditions, we adopt several illumination normalization methods proposed in previous research for our experiment: 1) gaussian disposition : it first filters out the uncontrolled illumination changes by dividing the intensity value of each pixel in the original image by the one after performing gaussian smooth [cit] ."
"to conclude, we have shown how the combination of networking domains can be easily deployed in a virtualized infrastructure. we have also shown that in the case of denial of service attacks, for instance, novel security mitigations are possible but new threats also exist and need to be addressed. the normal behavior in one domain may be considered as an abnormal in another due to the different protocols and network functions running. furthermore, the security mechanisms are divided and network operators in charge of a particular domain lack a global view of the threats that would allow them to better understand what is occurring in the network to be able to detect and mitigate attacks and malfunctions. the next section presents how practical tools can be used to defend against the aforementioned security threats."
"a lightweight version of mmt probes can be co-located with each vnf. this allows the analysis of metrics and security indicators related to the vnf. in this scenario, only parts of the parsing plugins in mmt-dpi are needed to fulfill the list of protocols used by the vnf. besides, the security analysis and intrusion detection needs only to target the risks and vulnerabilities identified for the given vnf application and differentiate abnormal activity from allowed activity. the security analysis methodology and properties of an ndn node are indeed different from the ones for a firewall or a http/ndn gateway."
"sdn and nfv facilitate security management but also introduce new threats. the flexibility they provide to network infrastructures allows their in-depth monitor-ing, with a central point gathering all needed information (the sdn controller and nfv orchestrator). however, these new features come with many new software elements and protocols, which increase the attack surface of the infrastructure. moreover, some attacks specific to sdn and nfv have emerged. consequently sdn and nfv have a bidirectional relationship with security: they both are security enablers and introduce vulnerabilities."
"the southbound interface of the doctor control and management plane implements the doctor sdn control plane which consists of a sdn controller interacting with virtual networks for dynamic configuration (interface 9). following the sdn principles, the doctor controller is mainly designed to acquire a global view of the network and enable centralized, intelligence-based network control. it actually interfaces with the doctor security orchestrator (via the vim using interface 8) to be notified of attacks or anomalies detected with the assistance of cybercaptor, so as to correctly configure virtual networks to mitigate attacks. its role includes, e.g., setting up the http/ndn gateway to deliver traffic between heterogeneous network domains (i.e., ip and ndn), traffic load balancing, deploying rules in a firewall or an intrusion detection system / intrusion prevention system (ids/ips) service, adding/removing routes in ndn or ip router's forwarding tables, etc."
"ndn is designed to intrinsically prevent some types of threats that ip needs to solve using external mechanisms. in ip networks, an attacker can send altered data to end-users, thus causing damage when content is delivered. to avoid this, ipsec or transport layer security (tls) needs to be used to prevent any data alteration and avoid other security issues. on the other hand, ndn signatures are intrinsically computed and included in each ndn data packet. the user receiving the data packet can use the information to verify the signature, hence ensuring the authenticity of the content and avoid tampered data."
"this section introduces three well-known local features for image description, which have been evaluated in previous research on face recognition [cit] . we would like to evaluate and compare their performance on smile detection tasks."
"the monitoring audio-video replay module discussed earlier consists of two parallel processes, for video and audio, running independently to collect replay recording. the term replay recording represents a pre-recorded video frames or audio samples to be used later by the algorithm when the attack is triggered. the motion detection algorithm in the video process is used to detect an occurrence of a static scene by comparing the pixels in consecutive frames. the changes in pixel intensity are compared with a threshold, where different environments have a different sensitivity to pixel changes and hence different threshold values."
"it can also be observed that, regardless of the image size and the feature chosen, the occ and rbf svm classifiers perform almost equally well, much better than the linear svm classifier does."
"from the obtained spectral band, the instantaneous frequency for each frame window used is estimated by the maximum value in each power density vector obtained for that time instant. the period of signal duration represents the number of vectors obtained from psd and instantaneous enf values. quadratic interpolation is used to obtain its dominant frequency from the maximum value in each vector. in quadratic interpolation of the spectral peak, the peak location is given as:"
"this attack exploits the host tracking service of the controller that maintains a profile for each host in the network, and updates it as the host migrates to impersonate a specific web server and phish users. to do so, the attacker first retrieves the target's identifier used by the controller to identify the host (here: the mac address), then injects fake packets in the name of the target host. as a result, users trying to access the genuine server are redirected to the malicious server."
"additionally, the proposed detection method was been implemented and fully validated on a real video surveillance system using a cheaper foscam camera. the foscam surveillance camera tested was powered directly from the electrical grid. as proof of concept, the foscam api was used to control the camera recordings through python scripts. the resulting parameters were tested using the forged and original audio recordings. figure 15 shows the enf extracted from the audio recording along with parallel power recording. as the estimated enf shows, while the original audio/video was masked by the forged audio/video, the drop in the correlation coefficient was observed in figure 16, which clearly indicates the detection of an anomalous activity. in summary, the collected data and experimental results conclude that it is worthy to have a higher initialization setup delay with a better performance with an average shifting length of 5 s. in order to reduce the false alarm rate, a consecutive lower correlation coefficient detected by the system can be treated as an immediate alert to a challenging situation. in addition, a second pass performed by the fog layer can also be used as a reassurance for the alert."
"to launch a real-time frame duplication attack, we assume that the edge based surveillance systems have been compromised through network attacks. this allows the attacker to gain complete access to the live video feed along with the manipulation of the output stream as required. the algorithm devised includes two modules, monitoring for audio-video replay and deploying an attack. figure 5 represents the algorithm flow diagram. in the first module \"monitoring audio-video replay\" consists of collecting a duplicate recording in two parallel processes where video and audio streams are monitored independently. the video monitoring process constantly checks for any motion in the frame and when a static scene is detected, an automated recording of the static scene is started in the background process. the motion detection algorithm in the video process performs a gaussian blur on the frames to smooth out the edges and minimize errors due to noise, and then changes in pixel intensities are compared with a threshold to detect any motion. the audio monitoring process detects noise in the environment and records audio samples when there is no background noise. with the \"monitoring audio-video replay\", a recent recording of the video and audio are collected and stored. the second module \"deploying an attack\" represents detecting a trigger and launching the attack. the mechanism used as a cue is the face recognition algorithm. when the trigger event is detected, the video frames and audio samples are combined and deployed to mask the live video feed."
"the attack graph approach allows a defender to enumerate all possible attack paths for an attacker, given a network topology (i.e., network and software configuration, vms placement and domain dependencies). it relies on an up-to-date vulnerability database and a global knowledge of the network. cybercaptor depends on the mulval attack graph engine [cit] . it is an engine that uses generic rules and vulnerability information from the system to produce attack graphs. a few dozen rules are enough to model most attack steps. system topology and vulnerability information are used as parameters for the generic rules, thus forming attack steps. these attack steps have several inputs, called preconditions, and an output, called postcondition. mulval then produces an and-or graph, composed of 3 types of nodes: and nodes, or nodes and leaf nodes."
"in a distributed framework based on consensus constraints, agents will share variables with their neighbors in the process of iteration, like the w-subproblem and p-subproblem of our algorithm. take the l-th agent for example, sharing variables w l and p l needs communication complexity with o(n l ck) and o(n l qk), respectively. then general communication complexity of single agent is o (n l ck + n l qk) ."
"the whole flow of the proposed method distributed fast supervised discrete hashing is described in algorithm 1. it is noteworthy that although our approach is demonstrated in a certain order in the process of derivation, the three subproblems are updated in parallel in real application scenarios, i.e., local b l of b-subproblem, w l and l of w-subproblem, p l and l of p-subproblem are updated in parallel on each agent."
"in practice, the induced voltages in a real array (ulesda or patch array) are contaminated by the effects of the mutual coupling between the elements of the array which will undermine the performance of a conventional adaptive signal processing algorithm [cit] . by using a coupling transformation matrix, one can compensate for all the undesired electromagnetic effects. the voltages induced in an actual array are then transformed to a set of voltages that would be induced in an ulva consisting of omnidirectional isotropic point radiators. it is based on transforming the real array into an ulva operating in the absence of mutual coupling and other undesired electromagnetic effects. hence, we are going to select"
"to increase the number of degrees of freedom of the system, one need to use the forward-backward method that can be generated by putting both of the cancellation matrices from forward and backward directions together when calculating the weight vector. and the target signal complex amplitude can be estimated by either forward or backward direction. this increases the number of degrees of freedom and the accuracy of the system [cit] . in the next section, the performance of the three methods will be shown via numerical simulations."
"in this section, we introduce distributed fast supervised discrete hashing model in detail (e.g., fig. 2 ). suppose n training samples are distributed across p agents over a randomly generated network. let"
a d 3 ls stap approach applied on a wideband signals received by ulesda and patch array. we use only single snapshot in order to null the interference signal while maintaining the main beam directed towards soi. the main beam is directed towards the target signal and the jammer is nulled correctly with deep null. the results of patch array is better than that of ulesda since it provides more deep null in the direction of the jammer and it will affect on the accuracy of the complex amplitude estimation.
"in this paper, we have proposed a distributed supervised hashing learning method called ''distributed fast supervised discrete hashing'' (dfsdh) for distributed sites. dfsdh employs a joint objective learning in a distributed manner and integrates hash function learning, hash code generation, and linear classifier learning in the process of objective optimization. then the global objective optimization is split into three subproblems. for the solution to hash code problem, label information is regressed to compact code on a single agent and the proposed method both significantly decreases computational complexity and makes the learned compact code to be with better classification attributes. for two projection matrices subproblems, auxiliary variables and consistency constraints are introduced and extended to separable equivalent problems. besides, an alternate iterative procedure is employed to optimize three subproblems in parallel. experimental results indicate that dfsdh algorithm is competitive to most centralized supervised hashing methods and existing distributed hashing methods."
"it is unrealistic to compute and update the objective variables of joint optimization problem (4) at the same time. thus an alternating procedure is employed to update three variables iteratively, i.e., converting the whole problem into three subproblems and updating one variable while the other two variables are keeping fixed. b-subproblem: if w l and p l are fixed, local objective of equation (4) can be rewritten as:"
"where the superscript h represents the conjugate transpose of a complex matrix. the frequency transformation matrix ] [ f  needs to be computed only once a priori for the defined sector and this computation can be done off-line. so, now the actual steering vector at the wideband frequency is transformed to another one at the reference frequency."
"from fig. 6, it can be observed that most methods show an upward trend in average precision as the code length increases. supdish and dfsdh show similar performance in average precision. more importantly, the proposed dfsdh still performs well compared with other centralized hashing and unsupervised distributed hashing dish."
"2. our method adopts a closed-form solution at the step of learning binary code rather than a bit-by-bit solution. meanwhile, at the step of updating local projection matrix, the method of regressing label information to learn binary code has decreased the computation complexity. based on theoretical analyses and experimental result verification, both the solutions can greatly reduce the training time."
"where µ is penalty parameter and the last term of (4) is also called discrete loss error that measures the deviation between continuous real-value embedding and binary codes. the equation (4) is the final target of our method, by which not only ideal hash function can be obtained, but also binary hash code can be directly learned in the process of objective optimization. it is also worth noticing that the training process is to learn over multiple nodes and when the number of nodes is one, the dfsdh is equivalent to fsdh [cit], which is centralized hashing learning."
"the performance of d 3 ls stap on the induced signals from ulesda and patch array will be illustrated through the next two examples. for the first example, consider the soi to be arriving from fig. 7, fig.8, and fig.9 . as seen from the figures; the performance of d 3 ls stap is very good since the jammer is nulled correctly for the three cases while the main beam is constrained in the soi direction. the results of patch array have the advantage of providing deep null than the results of ulesda. this advantage will affect directly on the accuracy of complex amplitude estimation."
"the weight vector can also calculated in a backward direction; where the equations (18)- (20) are generated in the reverse order with complex conjugate starting from a p n n x, [cit] . the cancellation equations for backward method are as follows. where * denotes the complex conjugate of the data. we, then, simply obtain a similar cancellation matrix as in (21)"
"the map results of all methods conducted on minst dataset are shown in fig. 7 . overall, we can find that our method keeps considerable map with various code lengths and performance of which is no inferior to other methods. table 2 reports the training time consumption. compared with other supervised hashing methods, the dfsdh still performs high time efficiency based on a different dataset, which is identical with the results reflected on table 1 ."
"the beamforming of adaptive antenna is a computationally intensive process at which each users signal is multiplied by complex weight vectors that adjust the magnitude and phase of the signal from each antenna element. the principle advantage of an adaptive array is the ability to electronically steer the main lobe of the antenna to any desired direction while also automatically placing deep pattern nulls in the specific direction of interference sources and the direct data domain algorithms used to overcome these drawbacks of statistical techniques which are adaptively minimize the interference power while maintaining array gain in the direction of the target signal [cit] . until recently, most of the works on smart antennas have concerned narrowband communication systems that are characterized by a fractional bandwidth in the order of one to a few percents. the beamforming techniques used in narrowband systems are inadequate for wideband systems because they are unable to track a desired user or form nulls or low sidelobes towards interfering sources over a large frequency band. in order to overcome the problem of narrowband beamformers, several wideband beamforming techniques have been proposed recently [cit] . in order to deal with wideband signals we have two solutions either using wideband antennas or using narrowband antennas with a frequency transformation technique. this transformation technique is done by transforming the steering vector at any frequency within the design frequency band into a new steering vector at a specified reference frequency. in the practical case, if we are dealing with real elements. the elements spatially sample and reradiate the incident fields. the reradiated fields interact with the other elements resulting in mutual coupling between elements. em principles are applied to compensate for the effects of mutual coupling between the antenna elements. this em processing technique transforms the voltages that are induced in the uniformly spaced real elements due to all incoming signals to an equivalent set of voltages that will be produced in a ulva containing isotropic point radiators by the same set of incident signals [cit] . once the corrected voltages are obtained, we can apply stap based on d 3 ls approach. stap is carried out by performing two-dimensional (2-d) filtering on signals that are collected by simultaneously combining signals from the elements of an antenna array (the spatial domain) as well as from the multiple pulses from coherent radar (the temporal domain) [cit] . this algorithm is used for suppressing highly dynamic interference and enables the system to detect potentially weak target returns."
"next, we measure the performance of the proposed method and other algorithms on another dataset minst. the method adopted for comparison is same as experiment a. minst consists of 70k gray images with handwritten digits from '0' to '9' and each image has 784 dimensions. as for the experiment on minst, the whole dataset are randomly splited into a training set and a testing set. for all the methods compared, 69k samples are selected randomly for training, which include 10 classes (6,900 samples per class), and the remaining samples for testing. particularly, for dish, supdish and the proposed method, 69k training samples are distributed across 10 agents (6,900 samples per agent). we evaluate the performance based on hamming ranking of hashing codes and report the average precision (presion of hamming radius 2), map and training time with different code lengths (16 bits, 32 bits, 64 bits, 96 bits, 128 bits)."
"+ qn l k), respectively. due to the facts that the whole training data are distributed on several agents and subproblems compute on each agent in parallel, the proposed method performs more efficient than centralized setting based on same training samples. in terms of computational time complexity, with the number of data on single agent increasing, the time complexity will increase linearly."
then we measure the steering vectors associated with the set ] [ q  . the measured steering matrix at the frequency f w is defined by
"moreover, fig. 4 shows the map of the compared methods. it can be observed that dfsdh, sdh, and fsdh have similar performance. compared with supdish, agh, ksh, and sdh, dfsdh is much better in map."
". so in addition to soi there are u undesired signals. we will deal with two arrays of dipole and patch antennas. fig.1 and fig.2 show the configurations of the uniform linear equal spaced dipole array (ulesda) and patch array respectively. each configuration consists of n elements located along the x-axis, and the distance between any two adjacent elements is d."
a wideband received signal is incident on a uniform linear array of n elements and d the spacing between elements. the procedure is based on transforming the steering vector at the wideband frequency f w into a prespecified reference frequency f n using the transformation matrix for u+1 uniformly spaced directions covering the angular region [cit] . this technique is used for the case of first configuration as shown in fig.1 because this antenna is narrowband antenna and we will transform the wideband steering vector to another steering vector operating in the frequency range of this ulesda. the array steering matrix whose columns are the array steering vectors at the frequency f w corresponding to each of the u+1 directions is defined by:
"in this paper, to fully utilize the superiority of fsdh [cit] to sdh [cit], distributed fast supervised discrete hashing (dfsdh), based on fsdh, is proposed. the proposed method regresses label information to hash code on a single agent and adds nonlinear embedding on local objective functions to ensure that the binary codes belonging to the same class are different. then, local objective function is resolved into three associated sub-problems and update in an iterative manner with three steps. from a global perspective, the consistency of global variables is considered in a distributed framework to ensure that local variables of each agent update in parallel. thus consistency constraints are further introduced to the global objective function. it is shown that our method has enhanced both retrieval precision and training efficiency."
"since hashing learning is converting high-dimensional data into compact binary hash codes, the primary issue to consider is learning k bits hash function h (·). then, a nonlinear embedding learning method is employed to get hash codes of the training data and compute query by hash function, which is defined as follows:"
"in this section, we give the complexity analyses of the proposed dfsdh in communication and computation. firstly, we recall related data of work mentioned above: the size of the whole training examples is n. the number of agents is p and the size of training examples on l-th agent is n l . the dimension of training examples is d and the number of label categories is c. the number of anchor points, which are selected from training examples randomly, is q and the code length of binary hash code is k."
"the rest paper is organized as follows: in section ii, the notations, hash function and distributed network model are introduced briefly. section iii proposes a distributed fast supervised discrete hashing model and gives the formula deduction. besides, the complexity analyses are reported in section iv. in section v, the performances of dfsdh and other methods are evaluated on two benchmark datasets. finally, section vi makes the concluding remarks."
"combining the results of the two experiments above, dfsdh shows similar performance to supdish in average precision and map. while in terms of training time, dfsdh is obviously better than supdish, and the advantage of which will be more remarkable with the increasing of code lengths. compared with fsdh, although the training time is slightly increasing, dfsdh both liberates the limitation that fsdh can merely be suitable for single agent learning and has better retrieval accuracy than fsdh. besides, dfsdh has great advantages in terms of retrieval accuracy and training speed, compared with other centralized and distributed hash learning methods."
"since the antenna platform is moving, there is a doppler shift, f d, in the received signal with m pulses received by a single antenna element [cit] . so, the system processes m coherent pulses at a constant pulse repetition frequency, f r,"
"similarly, the cancellation matrix for (19) and (20) can be generated in the same manner. we have to know that (18) corresponds to spatial difference, (19) corresponds to temporal difference, and (20) corresponds to spatial-temporal difference of the received signal. once three different cancellation matrices have been generated, we will arrange the elements of each cancellation matrix as a row vector of dimension a n p n  1 by putting each row side by side. we call this row as a cancellation row. now, we have generated three cancellation rows. to find a weight vector, we need to generate the total of 1  a n p n row vectors. to preserve the soi from being canceled by the weight vector, we left 1 row for gain constraints along target direction as follows [cit] ."
"1. in order to embody the higher classification properties of binary discrete compact codes in a distributed framework, i.e., the hamming distance of hash codes within different classes is as large as possible. we regress label information to binary code in distributed hashing learning. to some extent, our method can significantly improve the performance in retrieval accuracy."
we shows that how to enable ipv6 networking over c2cnet which is specified in car2car communication consortium as a geographic routing protocol. then the system is divided into three functionalities and implemented as three modules in linux.
the c2cnet functions are divided into three main modules that cooperate each other. the three modules are implemented in userland for ease of implementation and modification. remind that one of objective of the project is to brush the specification up by feedback from the implementation. the three modules are responsible of particular function on obu. these modules cooperate via inter-process communication socket.
"positionsenser module is to create a stable interface for acquiring geographic data by the c2cnet modules. it is implemented as a stand-alone program connected to a positioning service available for a particular platform. it sends the position information over a udp socket to c2cnet modules. lowerlayer module is the interface between c2cnet (geonet internal modules) and the phy/mac layer. this is needed to support the platform independency of geonet. it allows geonet to support different platforms with different network-interfaces without holding platform specific parameters within the c2cnet modules. c2cnet module controls the position information and keeps transmitting a periodic packet to inform its neighbors about its presence. it also transmits received data with c2cnet header to lowerlayer module via udp socket. ip-c2c sap, which has geoipv6 function described in section 2.2, is integrated into c2cnet module."
we focused on vehicle-to-vehicle scenarios in the reported evaluation but we also intend to continue the evaluation with vehicle-to-infrastructure scenarios (roadside-based and/or internet-based. geonet obus comprise nemo [cit] and mcoa [cit] functionalities. we are motivated to measure the network performance using these functionalities over c2cnet.
au1 sends ipv6 packets to obu1 that is the default router of in-vehicle network. obu1 receives the packets on the ingress interface (eth0 in figure 2 ) and removes mac header of the packets. then ip header and payload part are transmitted into the tun0 virtual interface by the preconfigured rules of ip filter 1 . the c2cnet module reads the data from tun0 and parses the information of the ip header.
"humans spend considerable time in the vehicle these days. its is going to be more and more important technologies in our life, that enhance safety, driving efficiency and amusing by allowing various service such as fleet management, navigation, billing multimedia application and game. ipv6 is considered as the most appropriate technologies to support communication in its thanks to its extended address space, embedded security, enhanced mobility support and ease of configuration. future vehicles will embed a number of sensors and other devices that could be ipv6 enabled [cit] ."
"in the evaluation, we only tested udp, tcp, and icmpv6 with fixed sending rate. the performance of an actual traffic hazard application such as the one used in the final geonet workshop is still not evaluated. in the future, the performance of ipv6 geonetworking should be evaluated under more realistic scenario such as this one."
"anavanet is a tool developed internally at inria to analyze vehicular networks. it has originally been used to evaluate olsr-based ad-hoc vehicular networks [cit] . for the purpose of evaluating the performance of ipv6 geonetworking, anavanet is extended in order to analyze ipv6 packets transmitted with a c2cnet header in the geonet domain. figure 7 provides an overview of the experimental evaluation process carried out in the tests. the sender (au) is in charge of generating data traffic, and both the sender and the receiver save a high level log, according to the application used to generate network traffic. all obus record information about forwarded data packets by means of the tcpdump software, and log the vehicle position continuously. all this data is post-processed by the anavanet software and then analyzed. a java application traces all the data packets transmitted from the sender node. this way, it is possible to detect packet losses and calculate statistics for each link and end-to-end, and merge all these per-hop information with transport level statistics of the traffic generator. as a result, anavanet outputs an xml file with statistics of one-second periods, and a packet trace file with the path followed by each data packet."
"the xml file is compatible with google maps api and web based analysis is produced. the experiments carried out are available on the geonet 2 web site and can be replayed to see the momentary performance of the network during the tests. all the experiments can be selected and main performance metrics can be monitored at any time. from the two types of output file from anavanet,the gnuplot software generate the graphs that also appear in rest of the paper."
"the test was performed with one parked vehicle and one moving vehicle, when vehicles were in normal urban conditions and isolated from obstacles and interferences. the aim of this scenario was to check the maximum distance the wireless range can reach."
"network performances were, first, evaluated in the indoor testbed to avoid interferences due to unexpected radio perturbations and difficulties to trace the movements of the obus. the following experiments were performed outside of any vehicles. both the obus and aus did not actually move during the experiments. the network configuration is same as in figure 2 with single hop and multiple hop environment. we tested the round trip time (rtt) using icmpv6 and the packet delivery ratio and the bandwidth using udp with various parameters."
"in this section, we first describe the c2cnet and objective of the geonet project. then, the how to interact between ipv6 and c2cnet is described. the specification of the interface between ipv6 and c2cnet is called the ip-c2c service access point (sap) in the geonet project."
"in the test, udp packets are sent from au1 to au2 during 20 seconds by iperf command. the sender sent from 1mbits/sec to 6mbits/sec with various size of packet from 100 [cit] bytes. there was no traffic other than iperf traffic. figure 4 shows the result. figure 4 shows the packet delivery ratio on single hop. packet delivery ratio is low while packet size is small. there is no packet loss with a packet size between 700 bytes and 1300 bytes with 1m of sending rate, between 900 bytes and 1300 bytes of packet size with 2m sending rate and between 1100 bytes and 1300 bytes of packet size with 3m sending rate. figure 5 shows the throughput for the same tests as reported on figure 4 . the throughput is maximized with a 1300 bytes packet size for all sending rates. it shows that the most efficient configuration to send maximum data is realised with a 1300 bytes packet size and 5m sending rate. maximum throughput is around 4500 kbits/second."
"the service access point (sap) between ipv6 and c2cnet is the interface to transmit the packet up from the c2cnet layer to ip layer and down from ip layer to c2cnet layer. four types of communication are defined in c2cnet architecture: geounicast, geobroadcast, geoanycast and topobroadcast. first three are the type of communication which based on geographic information and the last one is based on network topology information. geounicast routes data from a source node to a destination node for which the exact geographical location is known. geobroadcast delivers data from a source node to all nodes located within a specific geographical area. and geoanycast routes data from a source node to any node located within a specific geographical area. topobroadcast routes data from a source node to all nodes located up to a specific distance in terms of hops."
"the destination ipv6 address is used to distinguish communication type whether unicast or multicast by the first 8 bits which are correspondent to geounicast and geobroadcast, respectively. in unicast case, the next hop ipv6 address is resolved from the routing table via netlink library by the destination ipv6 address. the last 64-bits of the next hop ipv6 address is correspondent to the destination c2cnet id. in multicast case, destination c2cnet information are pre-configured depending on the destination ipv6 address (i.e. if the destination address is link-local all node multicast address (ff02::1), the latitude and longitude are as well as those of obu1 and the radius is 500 meter)."
the organization of the paper is as follows: section 2 describes the geonet approach to make ipv6 work over c2cnet that specified in c2c-cc. section 3 shows the overview of prototype implementation on linux system. section 4 evaluates network performance using the implementation with indoor testbed and section 5 shows evaluation with vehicles. section 6 concludes the paper and shows the future works.
"in linux system, ipv6 packet forwarding is processed in the kernel space. however the packet has to be brought to the user land from kernel, because the c2cnet module is implemented in userland. then the packet is encapsulated with c2cnet header and then sent back to the kernel again. we decide to use tun virtual interface to bring the packet to the user land. overall process of ipv6 over c2cnet is illustrated in figure 2 ."
"throughput using tcp and considering the same scenario is given in figure 11 . the maximum throughput is around 1000 kbits/sec when the vehicles are parked next to one another. when the distance is from 50 meters to 200 meters, the average throughput is around 500 kbits/sec and figure 12 shows hop count, packet delivery ratio and jitter on dynamic tests under 30 km/h. the upper plot shows the number of hops used in the paths followed by udp packets, whereas the lower graphs show the packet delivery ratio, computed end to end and per link. the packet delivery ratio is calculated per second, while the hop-count is plotted for each packet transmitted from the sender node. when no hops are drawn, the route to the destination vehicle is not zero hops means that the packet was sent by the first obu but was not received by any other. negative values represent those packets which did not arrive to the destination vehicle, but some hops were reached. as can be seen, a direct relation exists between packet delivery ratio and number of hops. when this last value is equal or lower than zero, the packet delivery ratio decreases. when the vehicles are in the same street, some direct paths (one-hop) appear; however, when the distance between the sender and the receiver vehicles is large enough, the two-hop route is used. these different types of paths can be also seen if the perlink packet delivery ratio is observed. whereas the direct link (obu3-obu1) gives intermediate packet delivery ratio values, the packet delivery ratio between consecutive vehicles is almost identical and near 100 % when the two-hop link is used, due to the lower distance between nodes."
we have set up an experimental indoor testbed and outdoor testbed to investigate the network performance on ipv6 over c2cnet. the indoor test environment is designed to evaluate the pure performance of ipv6 over c2cnet avoiding interferences due to unexpected radio perturbations. we measured the network performance with udp and icmpv6 traffic using iperf and ping6. the test results show that ipv6 over c2cnet does not have too much delay (less than 4ms with a single hop) and is feasible for vehicle communication.
"according to table 1, only one function, named geoipv6, is defined to transmit the packet from ip layer to c2cnet layer. in this function three parameters could be considered: scope, destination and payload."
"the combination of ipv6 multicast and geobroadcast was implemented, however we could not evaluate the performance with such a scenario. one of the reasons is that a sufficiently high number of receivers is necessary to properly evaluate multicast but experimental evaluation is limited in the number of vehicles (4 in our case). so evaluation of ipv6 geonetworking with multicast capabilities by means of simulation or emulation is thus left for future work."
"in vehicular networks, vehicles equip with on-board units (obus) to enable the communication with other vehicles. vehicle-to-vehicle ad hoc networks are multihop communication using geographic position, which has been investigated on geonet project [cit] . on the other hand, roadside units (rsus) are installed around the road. ieee802.11 is used to connect between obus, and between obu and rsu. application unit (au) is a portable or built-in device connected temporarily or permanently to the vehicle's obu. obu also can be connected to the internet with cellular networks, wimax, etc. these terminologies are proposed in car2car communication consortium (c2c-cc [cit] )."
"the topology of the network dynamically changes during the test depending on the location of the vehicles. the performance of ipv6 geonetworking depends on the radio propagation which is influenced by obstacles. network performance also depends on other factors such as the distance, movement of vehicles. we have therefore developed the ana-vanet evaluation tool to perform the evaluation taking into account all of these factors."
"the packet delivery ratio using udp with this scenario is shown in figure 8 . the packet delivery ratio is almost 100 % from beginning to 200 meters. from 200 meters, the packets are starting to be dropped and the packet transmission finally ends at a distance of 420 meters. the packets are not delivered until the vehicle comes back to a distance of 400 meters 50 seconds after the communication ends."
"the data with c2cnet header, ipv6 header and payload are sent to lowerlayer module via local udp socket. lowerlayer module adds mac header over c2cnet header and transmits the frame into the air. the intermediate node (obu3) receives the frame and re-transmits the frame when c2cnet modules find that the frame should be retransmitted to reach the destination with multihop manner."
"c2c-cc is designing a separate network protocol (c2cnet) different from internet protocol (ip) to ensure car-to-car communication for both safety and non-safety and with taking into consideration both availability and non-availability of infrastructure. c2cnet protocol is tailored for vehicular environments and would rely on position-based routing. this protocol would define a separate c2cnet header with a separate c2cnet identifier, tentatively 64-bit length, identifying c2cnet node. c2cnet header is planned to carry source c2cnet identifier, destination c2cnet identifier, source geographic location and destination geographic location."
"the jitter of in the same test is illustrated on figure 9 . when the sender car leaves the receiver one, at a distance between 250 and 420 meters, the jitter is higher, due to layer two retransmissions caused by the increase of the distance. when the sender approaches the receiver again, this effect, but higher, is again visible at distances between 400 and 200 meters. this is due to packet flood of buffered packet during the disconnected period. figure 10 shows the rtt with icmpv6 transmission. the rtt is within 5 ms to 10 ms until 420 meters. after this point, no packets are delivered, until the sender vehicle comes back and reaches 100 meters of distance. since periodical c2cnet beacon messages are lost when the distance is around 420 meters, the destination c2cnet id is removed from the location table and the transmission ends at this point."
"finally, obu2 receives the frame and on the egress interface. then lowerlayer module removes the mac header. and c2cnet module finds that the destination of the c2cnet packet is obu2. the ipv6 header and payload are sent to the tun0 virtual interface. the packet is routed to egress interface (eth0). and au2 receives the ipv6 packet that sent from au1."
"to measure round trip time (rtt) between aus, au1 sent icmpv6 packet 100 times in 10 seconds (interval 0.1 seconds). the packet size is increased by 20 bytes each 10 seconds from 20 bytes until 1500 bytes. there was no traffic other than ping6. figure 3 shows the result. it shows the rtt on single hop without c2cnet (red line), single hop with c2cnet (green line) and multi-hop with c2cnet (blue line). in the single hop case, the rtt with c2cnet is 3 ms higher than one without c2cnet. in addition, packets with size exceeding 1300 bytes cannot be delivered with c2cnet because of the mtu, while the packet without c2cnet is delivered until 1500 bytes."
"to evaluate the performance in more realistic scenarios, we setup an outdoor field test environment with three vehicles equipped with an obu, an au, gps receiver and wifi antenna as shown in figure 6 . obus are alix3d3 embedded pcs on which ubuntu 9.0.4 is installed with a linux 2.6.29.6 kernel. each obus has one built-in ethernet port (ingress interface) which is connected to the ethernet hub connecting other pcs, and a mini-pci wireless card (atheros ar5414 802.11 a/b/g rev 01) used as wireless connection to other obus. the obus are also connected, via serial port, with a trimble aggps 323 gps receiver, whose external antenna is visible in the photo."
"the higher education admission center is offering many sms services for students. the students have to use the same gsm phones, as registered in the center's database. the students should send an sms to 90190 with acceptable format for the information required as shown in table 8 . the student will receive a reply after a few minutes for each message sent regardless in correct or incorrect format. if student do not receive any reply then it means that the system did not received message and it needs to be resend."
"in this paper the scalars are denoted by italic letters (a, b, a, b, α, β), vectors by lowercase bold letters (a, b), matrices by uppercase bold letters (a, b), and a i,j denotes the (i, j) elements of the matrix a. the superscripts t and -1 are used for matrix transposition and matrix inversion, respectively."
"oman is one of the gulf cooperation council (gcc) member country located on the southeast part of the world. [cit] ) . beside the economic development, oman has made an enormous growth in deploying technology in the country. oman has witnessed a wide spread initiative to promote e-oman as a state-of-the-art solution in implementing the country's digital strategy. the government's growing investment and commitments in electronic services to educating users by offering training programs and awareness campaigns to promote the use of mobile technologies. it has embarked on several projects using mobile technologies to offer varieties of mobile services to citizens, residents and businesses too."
"the reduced log data is divided into q time slots of n samples, where each sample is collected in a certain time period. each element represents the number of times that the port m appears at the n-th time period, at the q-th time slot."
"bahrain has launched several smart phone applications for android and itunes platform, which grant users easy access to these eservices at any place and time. some of these mobile applications include egov sms services that generates sms message requesting the users to use egov eservices through smart devices. oman has started recently to utilize mobile channels to offer m-application services to its citizens and clients. some of the major services include: sms-parking service which is developed by muscat municipality enables motorists to pay parking fees via sms; higher secondary school students can now retrieve their end of semester grades (via sms) by messaging their student seat numbers to a phone number designated by the ministry of education. the royal oman police (rop) initiated an m-service allowing drivers to enquire and receive information about their traffic offences through sending sms messages. similarly, many other organizations are offering mservices through sms in oman, which are needed to be explored further and to see their influence on the users."
"as shown in the section vi, by using the getv in the model order selection schemes, it's possible to detect the presence of malicious traffic even if applications are running."
"where is a diagonal matrix with the eigenvalues, and the matrix has the eigenvectors corresponding to each eigenvalue. however, for our model order selection schemes, only the eigenvalues are necessary."
"basically, the model order of a dataset is estimated as the number of main correlated components with energy significantly higher than the rest of uncorrelated components. in other words, the model order can be characterized by a power gap between main components and the noise components. in the context of network traffic, the principal components are represented by outstanding network activities, such as highly correlated network connections which have, for example, the same destination port [cit] . the efficacy and efficiency of methods based on principal component analysis (pca) depend on the mos scheme adopted, since each scheme has different probabilities of detection for different kinds of data [cit] ."
"beside the technological resources and government commitment, it is extremely necessary to create awareness among the potential users of the available m-services in the country. as we know the users may not be excited if there is a lack of awareness and unfamiliarity with the procedure for using these services. for a greater success, the service providers should invest in marketing, educating and creating more awareness on the availability of the service to consumers and what it offers and how it can be beneficial to them. this could be done by distributing brochures, which describe to the users the benefits of these services such as convenience, time saving and easy payment with other advantages. though the organizations are creating awareness among the users, but still there is a room for improvement to offer better client services, the service provider needs to explore the opportunities to create greater awareness among the potential users of these services."
"this research entirely focus on omani initiatives, but a brief overview of mobile usage and applications in the neighbouring countries such as uae, qatar, saudi arabia, and bahrain are also presented. in these countries, 7 out of 10 smart phone users use an application at least once every day and 35% of smart phone users who download applications regularly have purchased at least one application in the last 30 days. this makes it very evident that customer engagement and organizational communication will both soon be occurring extensively through varieties of mobile applications accessed through smart phone and tablet devices in the very near future [cit] ."
"most shopping malls and outlets are now sending their ads and promotional incentives to the local clients via the smss. other organizations have also started to send bulk messages to citizens informing them about their activities and events. for example, the public authority for social insurance has currently begun a public campaign to broadcast its services and their apparent benefits to clients. one of the means used was to send advertising sms to all residents in oman. another example include oman tender board and ministry of manpower where they now send notification messages via smss to clients, about their transactions and/or other issues such as new tenders and job vacancies etc."
"this new mobile service introduced recently for motorists to pay the parking fee in mbd (muscat business district) and its surrounding areas. the service is simple to use where a user should have a mobile phone with either a pre or post-paid option. the parking fee deducted instantly or sent through the user's monthly mobile phone bills. the user can park the car and send an sms to \"90091\" with the details of the car plate number and the minutes for which the parking is needed. the users will get a confirmation message for a successful transaction. the user will then gets a reminder message five minutes before the allocated time expires to either move the vehicle or renew his/her parking time [cit] . this flow of information from the user goes to the oman mobile and passes to the muscat municipality data bank. the inspector receives the information on his pda and uses it while inspecting the parking sites for violations. tariff: subscription of six months is one rial, but all messages sent to 90226 will cost 50 baiza each. the m-application service offered by the ministry of manpower is highly valuable for jobseekers by giving all information pertaining to job opportunities available in the private sector. it also helps companies to implement their omanization process smoothly by acknowledging the current status of omanis in their organisations. the user can request for information, notifications and also to track the status of services provided by visiting the ministry's website (www.manpower.gov.om) and follow the directions to benefit from this service [cit] ."
"we consider two cases, one case normalizing, and the other one nonnormalizing it. the purpose of this was to adapt the solutions to the characteristics of portscan and synflood attacks. thus, we built two correlation matrices. one from the normalized case, and other from the nonnormalized case ."
"in a typical household in qatar owns three mobile phones, two computers, and one smart phone, and people are using this technology to access the internet in ever-greater numbers. many government agencies in qatar have capitalized the opportunity and offered varieties of mapplications to citizens such as hukoomi service through which the users can pay utility bills using their mobile devices. ministry of municipality and urban planning (mmup) has been proactively collaborating with various municipalities and other relevant agencies in the country to automate processes and ensure more and more services are on mobile phones. the ministry of justice (moj) is also offering a mobile hotline application service to keep records of users' inquiries, and sends them sms notifications when their complaints are resolved. there are many other mobile applications are underway to be offered very soon [cit] ."
"in conventional computing environment, to do computing, a person has to sit alongside with computer to complete the task. a computer can be a standalone pc, or connected to network or to a server, and so on via cable. this condition limits the use of computer and created hardship for individuals and workforce on the move. in today's era, mobile phones have become essential tools for voice calling, text messages, and accessing internet applications. impact of mobile phones is humongous in our daily lives."
"material published as part of this publication, either on-line or in print, is copyrighted by the informing science institute. permission to make digital or paper copy of part or all of these works for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage and that copies 1) bear this notice in full and 2) give the full citation on the first page. it is permissible to abstract these works so long as credit is given. to copy in all other cases or to republish or to post on a server or to redistribute to lists requires specific permission and payment of a fee. contact publisher@informingscience.org to request redistribution permission."
"nowadays one of the greatest challenges in internet is security assurance, obtained by integrity, availability and confidentiality of data. there are several ways to provide security, taking into account both technical aspects, through the use of safety equipments or systems, as administrative and personal, related to establishment of a security policy and awareness campaigns. regarding safety equipments or systems, we can use for instance firewall, intrusion detection systems (ids) and intrusion prevention systems (ips)."
"each model order selection scheme has different characteristics. we used the following method in our simulations: aic [cit], mdl [cit], edc [cit], radoi [cit], eft [cit] and sure [cit] ."
"several methods have been proposed for identifying and characterizing malicious activities. classical methods typically employ data mining [cit] and regular file parsing [cit] for detecting patterns which indicate the presence of specific attacks in the analyzed traffic. recently, automatic blind malicious traffic detection techniques have been developed for honeypots [cit] . however, the honeypot traffic is simpler since there are no legitimate applications running."
"this service provides information about job opportunities available in the private sector of oman. the user can submit his/her cv online and receive an sms whenever any job opportunity matches the qualifications and interests. the smss are sent, about new job vacancies, to job seekers with the \"vacant job numbers\" as announced by the ministry of manpower. the job seekers then have two options: 1. accept or 2. reject the job offer by sending sms to 90017 along with the vacancy identification number sent earlier."
"as we know that omantel is the leading telecom operator in oman, providing both fixed and mobile communication services in the country. the strategy of the company remains centred on expanding the promising segments of the market by developing attractive m-application services with a particular focus on broadband solutions in order to fulfil increasing demand created by the rise of smart devices and social networks. the company has the most extensive fixed and mobile network coverage in the country, which, coupled with a customer-centric strategy, is being leveraged to create and sustain the competitive edge. besides, with a commitment towards maintaining its leadership position, the company is continuously expanding its footprint in the developing areas. furthermore, omantel is continuously expanding its international connectivity to enhance its position as an international hub. in order to achieve this vision, the company's strategy remains focused on continue delivering enhanced customer services to use m-services conveniently."
"al shabiba -mobile app brings daily news in the users' mobile devices. they are published by the muscat press and publishing house who also run print media -daily arabic newspaper 'al shabiba'. you can get the latest local news, as well as top headlines, prayer timings, weather, viewpoint the popular column written by times of oman editor-in-chief, city guide, phone directory, feature article, and more. this service is free of charge and can be used through smart phones or similar mobile devices. this is a very useful news sourcing service for citizens and residents of muscat, sultanate of oman."
"in oman, the total mobile users increased from 3. mobile technology with the introduction of internet enabled mobile phones, pda's, wi-fi and wireless networks have allowed their users to enjoy all the benefits of telephones, information accessing, and text messaging such as sms [cit] ."
"once obtained the vectors, we can construct the matrix, and then determine the correlation matrix in order to find the eigenvalues. (3) where n is the number of the sample time."
the advancement of mobile technology is paving the way for people to incorporate these devices into their daily lives effective and efficiently. this progression of mobile technology has created opportunities for government or private organizations of developed and developing nations including oman and its neighbouring countries to transition their services to mobile platform.
"it has developed a paid service that enables investors to receive regular updates on market and stock alerts via sms [cit] . the service also enables users to get a sms every 30 minutes on market movers -top winners, losers and most active companies [cit] )."
"it is a smart phones application used to know the prayer times and provided by the ministry of awqaf and religious affairs. it includes many services such as finding qiblah direction, knowing prayer exact times according to the area where the user is at the time of request. this mobile application includes alarms facility for any prayer timings as well as you can also find out the time left for athan (call for the prayer) of the next prayer. this mobile application also provides a service of finding the nearest masjid based on user location and guide user with navigation map to reach there in shortest time. furthermore, the application has a cal-endar of the islamic events during the hijri year, as well as to the newsletters issued by the ministry. this service is free of charge and could be used through smart phones or similar mobile devices."
"once we have the matrices for each period, it is now possible to obtain the correlation and matrices, related to each matrix . with that it was possible to obtain the set of eigenvalues for that correlation matrices, generating a total of 2q vectors of eigenvalues: 6 vectors related to, built from the normalization of (fig. 6 ), and 6 vectors related to, built from the nonnormalizing of (fig. 7) . figure 6 . eigenvalues of the normalized case over each time slot. in this figure is the greatest eigenvalue related to the portscan is much greater than the others. calculating the eigenvalues of each and matrices, we can reduce the size of our matrix, and get some interesting conclusions, derived from the eigenvalue decomposition properties of the correlation matrices, such as: the eigenvectors associated with each eigenvalue are orthogonal to each other, and also linearly independent; and the eigenvalues are real and nonnegative."
"in the fig. 2, the portscan transmits only two packets for each tcp port and one packet for each udp port. note that there is a high correlation since the traffic is equal. in the fig. 3, the synflood attack consists of sending hundreds of packets with the syn flag active in a short period of time. in our case, considering this attack, if a server with port 80 open, the server is overloaded and may cause the unavailability of the service rendered by it. in a time interval of ten minutes, there were more than two hundred ten thousand packages related to the attack, an unusual traffic in a data network, especially by the fact of being concentrated in a short period of time."
"currently, almost 30 government mobile applications are available in the uae. some of these included ministry of health (moh) m-application service, which offers easy communication with the ministry of health's call center nearest to the users' current location. general authority of islamic affairs and endowments' services are offered via sms such as fatwas, prayer times, qibla and location of the nearest masjid. seha application offers information on ambulance services and an interactive map service that enables the users to find the nearest seha facility. abu dhabi cityguard service offers service to report incidents and submit complaints related to the abu dhabi directly to the government. with dubai electric & water authority (dewa) mapplication service, the user can view and pay utility bills online, get information about dewa office location, contact details, working hours, and access many other services [cit] ."
"oman telecommunications company (omantel) is the pioneer telecom service provider in oman, offering fixed-line phone, mobile phone and internet-related connectivity and hosting services. nawras is a secondary telecom service provider in oman, offering fixed-line phone, mobile phone and wimax and 3g-based fixed line internet-related services. there are several other companies such as friendi mobile, samatel and renna are in the business, providing mobile phones and value added services to their clients."
most banks in oman are now offering m-application services where the clients can receive updates on their bank accounts activities for amount coming in or going out of their accounts.
"in detecting synflood we have a huge uncorrelation traffic. thus, differently of portscan we have in this case only one traffic, but with a high value. so, we cannot normalizing this traffic since the normalization would cause an abrupt reduction of the high value associated with this attack, causing it to disappear. thus, in order to detect the synflood, we cannot normalize the matrix . however, except by normalization, the whole procedure to find the getv is equal to the one shown is subsection v.a."
"oman observer -mobile app brings daily news in english to your mobile device. they are published by the oman establishment for press, publication and advertising who also runs other print media-daily newspaper \"oman observer\" in english. you can get the latest local, sports news and news-videos on your mobile device. this service is also free of charge and can be used through smart phones or similar devices. this is a very valuable news sourcing service for residents of muscat, sultanate of oman."
"times of oman -mobile app provides daily news in english to your mobile device. the news is published by muscat press and publishing house, which also runs other print mediathe daily newspaper \"times of oman\" in english. however, this service, you can get the latest local news, headlines, business and technology news in to your mobile device. this service is also free of charge and can be used through smart phones or similar devices."
"government commitment and initiatives are important in introducing m-services in the country. the government eoman initiative has the vision of transforming oman into an advanced, worldclass e-government by integrating all ministries and government entities to provide faster and more effective public services online. by facilitating better interaction between citizens, businesses and government, eoman will take oman forward to a new age of progress and prosperity. in line with eoman initiative, several public and private organizations started utilizing the available mobile technology resources and started offering m-application services to provide better and efficient services to their clients."
"in order to exemplify these collected data, we consider the following tcp traffic log: in this paper, we consider only the following information from the log data timestamp, port type and port number."
"the log information of a computer connected to the network is formed by timestamp, protocol, source ip address, source port, destination ip address, destination port and additional information, depending on the type of transport protocol used."
initiated an m-application service allowing drivers to inquire and receive information about their traffic offences. motorists are required to send a message of their ids and vehicle details to '90085' and will receive information on the number of traffic offences and amount payable. the rop is also offering an sms service to enquiry on vehicle registration status.
"the noise traffic (log below) is formed by udp port 67 and udp port 68, associated with the dynamic host configuration protocol (dhcp). it can be seen in fig. 5"
"in this paper we propose the greatest eigenvalue time vector (getv) approach for detecting portscan and synflood in a network traffic flow data collected at a computer. first we showed the data log used, and the propose of the model for network flow data, in order to verify the validity of our approach through simulation results with real log files collected at a computer. several model order selection methods were experimented with the simulation data, showing that edc and eft yields the best results for this type of data."
"in cooperation with oman mobile, the civil aviation and meteorology has introduced a weather forecast service for most towns in oman that allows users to receive weather reports on their mobiles [cit] ."
"for this simulation we used a computer (based on linux operational system) performing common tasks (web access mainly) during an interval of three hours. the application tcpdump was used to capture the network traffic, as shown in the fig. 4 shows the signal traffic consisting of requests and responses on tcp port 80, tcp port 443 and udp port 53. the tcp port 80 is associated with unencrypted web access, the tcp port 443 to encrypted web access, and udp port 53 is associated with name resolution, dns."
"oman is an emerging country and adapting to the new technologies. these technologies play a pivotal role in oman and helping significantly in moving forward its national development process. oman has just started to utilize mobile channels to offer m-application services to its citizens and clients. some of the examples include; muscat municipality's sms-parking service system that enables motorists to pay parking fee via sms. higher secondary school students can now retrieve their end of semester grades (via sms), by messaging their student seat numbers to a phone number designated by the ministry of education. muscat securities market sends updates on market and stocks alerts to investors via sms. similarly, many other organizations are also offering varieties of such services through sms, messaging clients to inform them about their different activities. due to continuous economic and technological advancement and the current initiatives taken by the government and private organizations, it is expected that in coming years, more m-application services will be offered in oman using the advanced mobile resources. this exploratory research has presented a synopsis of m-application services offered in oman, which paves the way for a comprehensive future studies including their successes and failures. many experts have doubts on the bright future of mobile application services. some say as websites across the globe develop mobile compatible interfaces and as mobiles get smarter many tasks, which require mobile applications, may be shifted to web again."
"economic condition is also widely recognized as a major driver for the adoption of m-services in the country. the gdp and income per capital are common indicators for the economic condition of a country. since m-application services rely on technology infrastructures, which are relatively oman's economy is mainly based on oil and gas export, continues excellent performance in recent years, accounting for approximately 47% of gdp and constituting around 81% [cit] budget shows an increase of 20.8% on yearly basis in total estimated revenue at ro 8.8bn (the largest ever in terms of estimations) [cit] budget at ro 7.28bn [cit] ."
"to update, on the issue of m-application services initiatives in oman, interviews were conducted with key staff in several government organizations. these organizations were chosen according to their respective experiences in offering m-application services and only the active adopters were chosen and their key staff members were interviewed [cit] ). the information collected is being summarized in the following four major aspects conducive for adapting to these in oman including economic conditions, technology resources, government commitment and creating awareness."
"in the above equations, k d is the single-site rate constant associated with the mass-action rate law of m) is a function that gives the number of distinct reaction centers within a common molecule m in a species instance s matched by l d, and"
"the simulation method described above has been implemented in software called rulemonkey. rulemonkey is available as a stand-alone application, which is freely available via the rulemonkey web site [cit], and as a simulation engine within the getbonnie environment [cit] . the input for a simulation is a bngl-encoded model and simulation specification. in other words, rulemonkey reads and interprets standard bionetgen input files, like dynstoc [cit] and bionetgen [cit] . the conventions of bngl are described in detail elsewhere [cit] . the novel details of algorithm implementation, which are related to the calculation of rule rates, are described below. usage considerations are also discussed."
"as another simplification, we will assume that internal-state dynamics are described by two-state trajectories. thus, a component c ij of molecule p i will be associated with an internal state s ij (t) 0 [cit] or none at all. (recall that a component need not be associated with an internal state.) if a component is a tyrosine residue subject to phosphorylation and dephosphorylation, then an internal state could be introduced to represent its phosphorylation status [cit], with one state value representing the unphosphorylated form and the other state value representing the phosphorylated form. software tools for rule-based modeling, including rulemonkey, actually allow for greater than two internal states to be associated with a component."
"we will use r d to denote a rule that defines unimolecular monogamous ring-closure reactions, such as that illustrated in fig. 1(d) . we assume that r d has the following form: l l are not separated by a plus sign, these pattern graphs can be understood to identify two reactive components within the same chemical species, consistent with the conventions of bngl [cit] . here, we further assume that these reactive components lie within the same molecule. the pair of components identified by l d and ′ l d forms the reaction center identified by r d, which defines a graph-rewriting operation that connects the two vertices in the reaction center."
"as noted above for rates of the form of r a, rates of the form of r b are updated on the fly as reactions occur during the course of a rulemonkey simulation. these updates are straightforward and follow from eq. 8."
"the advantage of the rule-based modeling formalism is that a modeler can use a rule to concisely and comprehensivley account for the site-specific details and consequences of a molecular interaction, no matter how many distinct reactions might arise from the interaction. however, this expressiveness comes at a cost. each individual reaction defined by a rule is assigned the same rate law up to a statistical factor, which can vary from reaction to reaction within a reaction class [cit] . (statistical factors are discussed further below.) in reality, each reaction in a reaction class may be characterized by a unique rate law. thus, the rate law associated with a rule provides only a coarse-grained description of the kinetics of the reactions within the rule-defined reaction class. however, because a rule can specify the molecular context that is permissive for reaction, the coarseness of a rule can be adjusted by tuning the contextual elements of the rule. at the finest level, the contextual elements are highly specific and a rule defines a unique reaction. at the coarsest level, the rule is free of contextual constraints, meaning that the rule indicates that a reaction center can undergo a reaction regardless of the molecular context in which that reaction center is found."
"(odes) for mass-action kinetics. although bionetgen has been used to study a number of systems [cit] 45, 50], the reaction network implied by a set of rules is often so large as to make the simulation approaches implemented in bionetgen impractical. network generation is expensive, as is simulation of a large-scale reaction network using a conventional method. such methods have a computational cost that depends on the size of the network being simulated. in the generate-first and on-the-fly approaches to simulating rule-based models [cit], which are implemented in bionetgen, there is both a network generation cost and a network simulation cost, because network generation is a prerequisite for network simulation via these approaches. in contrast, in network-free simulation approaches [14, 15, [cit], such as the method presented here and implemented in rulemonkey, there is only a network simulation cost. however, it should be noted that network-free simulation procedures have a relatively high cost per reaction event, as is evident in table 1 . thus, one should choose such a simulation procedure only when the cost of an alternative procedure that relies on network-generation is prohibitive."
"having presented an overview of the graphical formalism underlying bngl, we can now introduce the assumptions upon which the algorithm implemented in rulemonkey is based."
"in the actions block of a bionetgen input file [cit], a rulemonkey-specific command, simulate -rm, is used to initiate a simulation. this command takes the same arguments as analogous commands that invoke other simulation engines, such as dynstoc [cit] . the output of a rulemonkey simulation is a .info file and a .gdat file. the .info file reports metadata about a simulation run, such as the execution time. the .gdat file reports a time course for each observable quantity defined in an input file (a file with a .bngl extension). observables are sums or weighted sums of population levels of chemical species (eq. 3), and they are defined according to the conventions of bngl [cit] . the time points at which results are reported in the .gdat file are specified by a user using the simulate -rm command. these results are generated through evaluation of the system state at each of the time points of interest."
"update sets and quantities that depend on the system state increment time t +τ and update the sets of reactants associated with rules. likewise, update the match numbers associated with observables. the steps outlined above that follow initialization are iterated until a stopping criterion is satisfied."
"is a function that gives the number of distinct reaction centers within a common molecule m in a species instance s matched by both l d and ′ l d . in bngl, the graphs that represent chemical species are composed of graphs that represent types of molecules [cit] . thus, m in eq. 12 references a particular subgraph of a species instance graph s that corresponds to a type of molecule. updates of r d necessitated by system state changes follow from eq. 12."
"for purposes of discussion, we will focus on rules specifying reactions that result in the association or dissociation of two components or that change the internal state of a component (i.e., reactions that can be modeled as graph transformations involving the addition or removal of an edge in a graph or the change of a vertex attribute in a graph). note that a component is taken to have at most one binding partner at a time, as usual [cit] . extension of the discussion that follows to account for other types of rules, such as rules for synthesis, degradation and trafficking between compartments, is straightforward. rulemonkey is capable of handling these types of reactions, which can be specified in accordance with the conventions of bngl [cit] . for a system in which only association, dissociation, and state-change reactions are possible, counts of molecules and components are conserved quantities. moreover, for such a system, the lhs pattern graph(s) of a rule will serve to identify either one or two reactants for each reaction defined by the rule."
"where the vector field f(x) is composed of mass-action terms derived from the rate laws associated with the rule set r. for a rule set that implies m unidirectional reactions, there are m mass-action terms."
"on-the-fly updates of rates of the form of r c are fairly complicated but follow from eq. 9. basically, when the system state changes, r c is incremented or decremented by the terms in eq. 9 that are added or removed as a result of the system state change."
"the rulemonkey algorithm, like other network-free simulation procedures, is particle based, meaning that each and every material component of a system is tracked, regardless of whether the material components are chemically distinguishable. in other words, given a set of rules r and a set of instances of chemical species graphs, each potentially present in multiple copies, these graphs and their constituent parts are tracked individually as the graph transformations defined by r are applied to them (as part of a simulation procedure) and the constituent parts move through the space of possible chemical species. let us use s t ∧ ( ) to denote the set of instances of chemical species in a system of volume v at time t. these instances of chemical species can, in principle, be partitioned into disjoint sets, the set of n populated chemical species: is the jth copy of species s i and x i (t) is the population level of (or equivalently, the number of copies of) species s i at time t in v . in a network-free simulation procedure, the species instances s ∧ are tracked but they are not partitioned as indicated in eq. 2, which is essentially equivalent to determining x(t), the population levels of the chemical species in v. to determine x(t) from s t ∧ ( ), one must identify and count the graphs in s t ∧ ( ) corresponding to each species in s(t), which is an expensive procedure because it necessarily involves repeated solution of the graph isomorphism problem [cit] ."
"where r 2 (0,1) is a second uniform deviate. the next reaction event will be in the class defined by rule r i, where"
"given a set of species instances s t ∧ ( ) at time t, the instantaneous rate r a (t) associated with r a is given by the following expression:"
"in principle, eq. 1 can be derived from r [cit] . however, derivation and numerical integration of eq. 1 is impractical when a rule set implies a large-scale chemical reaction network (i.e., large m and n), which is often the case [cit] . reduced forms of eq. 1, involving transformations of the variables x, can sometimes be found through analysis of r [cit], and onthe-fly stochastic simulation procedures [cit] can sometimes be used to limit enumeration of the reactions implied by r, which is expensive. nevertheless, these methods fail to be practical for many rule sets."
"where k b is the single-site rate constant associated with the mass-action rate law of r b and v b (l b, s) is a function that gives the number of distinct reaction centers in species instance s matched by l b (i.e., the number of pairs of connected components in species instance s identified by l b ). note that a species instance s is considered to be matched by l b only if application of the graph-rewriting operation of r b to s produces the correct number of reaction products, i.e., the number of products consistent with the rhs of r b . the function v b in eq. 8 is analogous to the function v a in eq. 7."
"is the population level of the molecule or molecular complex s i described by graph g i and n is the number of chemical species that are populated over the time interval of interest. in the limit of continuous population sizes, x(t) evolves deterministically according to a system of coupled ordinary differential equations (odes):"
"rules are typically associated with application conditions that impose constraints on the molecularity of reactions and the number of reaction products [cit] . in bngl, separation of two lhs pattern graphs by a \"+\" symbol indicates a molecularity of 2, and a rule with a single lhs pattern graph indicates a molecularity of 1. similarly, separation of two rhs pattern graphs by a \"+\" symbol indicates two reaction products, and a single rhs pattern graph indicates a single reaction product. application conditions can be introduced for a variety of other reasons. for example, [cit] recently modeled steric effects on multivalent ligandreceptor binding using rules with application conditions that place constraints on the allowable geometric configurations of ligand-induced receptor aggregates. here, we will only consider application conditions related to molecularity and number of reaction products."
"below, equations are given for exactly calculating the rates associated with five types of reaction rules, which generate reactions of the types illustrated in fig. 1 . the five rule types are sufficient to specify a wide range of models. for efficiency, the equations given below are used only in the initialization step of the simulation algorithm. after initialization, as described in detail for rejection-free simulation of multivalent ligand-receptor interactions [cit], the rates given by these equations are updated on the fly as reactions occur, which requires that matches of lhs pattern graphs in rules be tracked during a simulation."
"given a set of species instances s t ∧ ( ) at time t, the instantaneous rate r c (t) associated with r c is given by"
"to overcome this problem, kinetic monte carlo (kmc) procedures have been developed in which r is used directly to generate stochastic trajectories consistent with the chemical master equation that corresponds to eq. 1 [14, [cit] . these procedures avoid enumerating the reaction network implied by r. thus, the term \"network-free\" is apt for these simulation procedures. the algorithm presented below is a network-free procedure."
"given a set of species instances s t ∧ ( ) at time t, the instantaneous rate r d (t) associated with r d is given by"
"instead of relying on a partition of s ∧ as indicated in eq. 2 or knowledge of x(t) to advance a simulation, a network-free algorithm relies on knowledge of the complete component state of a system, i.e., the state of each component in the system. the state of an individual component c is the information required to uniquely identify 1) the component, including its label and the label of the molecule of which it is a part; 2) the internal state of the component (if any); and 3) the bound state of the component, including its binding partner (if any). let us use ∑ c (t) to denote the complete component state of a system, which we will also refer to as simply the system state. note that ∑ c (t) fully defines s t ∧ ( ) (and vice versa). because the graph-rewriting operations defined by rules directly alter ∑ c, the system state as expressed in terms of component states can be easily tracked (with a memory requirement that depends on the number of material components in v ) and ∑ c (t) can be dynamically updated as rules in r are applied to graphs in s ∧ in a simulation procedure."
"typical validation results are shown in fig. 2 . in all cases, rulemonkey was found to produce results consistent with those obtained using the other methods/codes considered."
"a great deal of knowledge about signal transduction, which is mediated largely by the interactions of proteins, has been built up over the years [cit], in part because molecular changes that affect signal-transduction systems play a role in many diseases, such as cancer [cit] . signal-transduction systems are exceedingly complex, and as a result, our ability to manipulate the behaviors of these systems (e.g., through therapies based on molecularly targeted drugs [cit] ) is limited. to extend our understanding beyond that reachable through intuition alone, researchers are attempting to develop predictive mathematical models for signal-transduction systems [cit] . these systems are difficult to model for a variety of reasons [cit], and new modeling approaches, such as rule-based modeling [cit], are needed. rule-based modeling is notable because it provides a solution to the problem of combinatorial complexity [cit] ."
"we will use r a to denote a rule that defines unimolecular state-change reactions, such as that illustrated in fig. 1(a) . we will assume that r a has the following form: l a r a, where l a is a lhs pattern graph and r a is a rhs pattern graph. thus, r a defines unimolecular reactions that each have a single reactant and a single reaction product. we will assume that the pattern graph l a identifies a reaction center composed of a single molecular component in a particular internal state."
"the algorithm presented below is intended for simulation of rule-based models that can be specified using bngl [cit], particularly models for the system-level dynamics of protein-protein interactions in signal-transduction systems. in such a model (for examples, see [cit] ), molecules and molecular complexes are represented as graphs, and molecular interactions are represented as graph-rewriting rules. it will be useful to briefly review the graphical formalism underlying bngl [21, [cit] ."
"in the course of a rulemonkey simulation, rates of the form of r a are updated on the fly as reactions occur and are never recalculated de novo after initialization, which would be inefficient. thus, for example, if a species instance s matched by l a is removed from s ∧, then r a is decremented by k a v a (l a, s) . similarly, if a species instance s matched by l a is added to s ∧, then r a is incremented by k a v a (l a, s) . updates of rule rates are facilitated by tracking matches of lhs pattern graphs of rules to subgraphs of the graphs representing the species instances in a system. as part of this bookkeeping procedure, when a new species instance appears in a system (as a result of reaction), it is checked against all the lhs pattern graphs of rules to determine the types of reactions in which it can potentially participate and to calculate updates of the rates associated with rules. when a species instance disappears from a system (as a result of reaction), the rates associated with rules are appropriately adjusted as well. the cost of updating a rate such as that given by eq. 7, or any of the equations given below, depends on the number of rules in a model and the number of chemical species instances created and destroyed in a reaction. however, the cost per time step is more or less fixed over the course of a simulation. the overall efficiency of simulation depends on the efficiency of the procedures used to update rates and to store and update other information needed to implement the simulation algorithm described above."
"given the background presented above, we can now present an outline of the simulation procedure implemented in rulemonkey. this procedure has more or less already been described in earlier work [cit], although key implementation details, described below, are novel. the procedure can be used to produce stochastic trajectories consistent with the chemical master equation corresponding to a system of odes of the form of eq. 1. a comparison of methods is included below."
"we will use r b to denote a rule that defines unimolecular dissociation reactions, such as those illustrated in fig. 1(b) . we will assume that r b has one of two forms: in the latter case, r b defines unimolecular reactions that each have two reaction products, as when a bond opens and constituent parts of the reactant species dissociate from each other. we will assume that the pattern graph l b identifies a reaction center composed of two molecular components that are directly connected by a (non-covalent) bond. given a set of species instances s t ∧ ( ) at time t, the instantaneous rate r b (t) associated with r b is given by the following expression:"
"a set of rules for the protein interactions in a signaltransduction system typically implies a much larger set of reactions. the reason is that processes such as multivalent binding and multisite phosphorylation can yield a combinatorial number of chemically distinct protein complexes and phosphoforms [cit] . the sheer size of such networks, as well as the cost of simulating largescale chemical reaction networks using conventional methods, has posed a formidable barrier to the development and analysis of models for signal-transduction systems that account for site-specific details of protein interactions (in terms of rules)."
"the cost of network generation is often prohibitive, which has motivated the development of several network-free simulation methods [14, 15, [cit] . like these methods, the method presented here and implemented in rulemonkey avoids network generation and, consequently, has a computational cost independent of the number of reactions implied by a set of rules (figs. 2 and 3 ). we note that yang and hlavacek [cit] have described a method that is essentially the same as that implemented in rulemonkey but without providing details about how the method could be implemented for general use (i.e., beyond problem-specific demonstrations of the method) and without providing general-purpose software."
"there is a one-to-one relationship between ∑ c (t) (or s t ∧ ( ) ) and x(t), but as indicated above, the rulemonkey algorithm, like other network-free simulation procedures, avoids determining x(t). how then are system properties of interest determined as a function of time?"
"and w(g i, s) is a weight that is specified to be either 1 or the number of times that s is matched by g i, i.e., the number of distinct images of g i in s or the number of distinct mappings (injections) of the elements of g i to a subset of the elements of s. in a bionetgen input file, the former case is indicated by the bngl keyword species, whereas the latter case is indicated by the bngl keyword molecules [cit] . the match number associated with an observable of the species type is a count of the number of chemical species in v containing at least one moiety matched by the pattern graph of the observable. the match number associated with an observable of the molecules type is a count of the number of moieties in v matched by the pattern graph of the observable. because the match numbers associated with observables are functions of the system state (eqs. 3), these quantities can be easily tracked and dynamically updated as the system state changes in a simulation procedure."
"a key feature of dynstoc is its ability to simulate models encoded in the bionetgen language (bngl), a general-purpose model-specification language that has been described in detail and that can be interpreted by a number of software tools [cit] . thus, dynstoc can be used to simulate a wide array of rule sets that imply large-scale chemical reaction networks. unfortunately, the network-free simulation method implemented in dynstoc is a null-event stochastic simulation method, and as a result, simulation of models with fast reactions is inefficient, because the size of the fixed time step used in the simulation procedure is determined by the rate of the fastest reaction."
"here, we present a software tool, rulemonkey, which implements a network-free stochastic simulation method for determining the system-level dynamics of molecular interactions represented by rules. in this method, unlike in the null-event method of stochsim/dynstoc, the size of the time step is variable, and each time step results in a change of the state of the system being simulated. like dynstoc, rulemonkey is capable of simulating models defined using bngl."
"the reason we only consider generate-first simulations using bionetgen is that simulation cost and networkgeneration cost can be easily separated when this simulation approach is taken, as it consists of two steps. the first step is network generation. the second step is simulation. in the on-the-fly approach, execution switches back and forth between network generation and simulation. it has been observed that stochastic simulation via the on-the-fly approach tends to be faster than stochastic simulation via the generate-first approach [cit] . thus, if we find that generate-first simulation is faster than network-free simulation, we can assume with some confidence that on-the-fly simulation is also faster than network-free simulation."
"specify v, t, r, including the single-site rate constants in rate laws associated with rules, and a seed set of species instances s t ∧ ( ), which determines ∑ c (t)"
"finally, we compared the performances of rulemonkey and dynstoc for problems with reactions occurring on different time scales (fig. 4) . as can be seen, as the rate of the fastest reaction in a system increases, the efficiency of dynstoc degrades relative to that of rulemonkey."
"given a set of species instances s t ∧ ( ) at time t, the instantaneous rate r e (t) associated with r e is given by"
"rulemonkey and dynstoc [cit] are similar in that they can both simulate bngl-encoded models while avoiding network generation. however the method implemented in rulemonkey is a rejection-free method, whereas the method implemented in dyn-stoc is a null-event method. both types of methods are capable of producing correct simulation results but null-event methods tend to have a higher cost for simulation of systems with fast reactions [cit] . for cases that we have examined, rulemonkey typically performs better than dynstoc (table 1 ) and has a decided advantage for systems with fast reactions (fig. 4) . this advantage is perhaps significant because practical problems tend to involve reactions occurring on disparate time scales."
"indicates that the class of the next reaction event is randomly selected with probability in proportion to the cumulative rate of reactions within that reaction class. it should be noted that the cost of finding r i using eq. 6 is proportional to n. a more efficient procedure, with cost proportional to logn, could be implemented [cit], but eq. 6 is used in rulemonkey for simplicity. for simulation problems that we have considered, performance profiling indicates that this simplification does not limit the efficiency of rulemonkey."
"part (4) requires, for each p, that a can generate a few algorithmicallyobtainable (see ''rigorous analogues for both check and checkmate'') members of t and that a can at least obtain results related to s that a system using relatively modest mechanical deductions could deduce."
"the use of algorithms known to be fallible pervades artificial intelligence, often because of the intractibility of many problems that arises from either the unsolvability of the halting problem or np-completeness. one measure of the pervasiveness of such algorithms is that the index of russell & norvig's ai textbook lists a dozen places in the book where the word ''heuristic'' plays an important (i.e., boldface) role. also, the philosophical foundations chapter of that book states:"
"knuth's thesis might be similar to an assertion by richard feynman that has been interpreted in terms of derivations; i.e., in terms of deductions. [cit] : ''computational science has as its goal to understand phenomena sufficiently well to reconstruct them ... [cit] was this statement: 'what i cannot create, i do not understand'. '' [emphasis added.] quora (www.quora.com) asked readers to resolve the apparent contradiction between that feynman assertion and the fact that he was a theoretical physicist. more than two years later the highest-ranked explanation was: ''when feynman said 'create', he ... meant that, starting with a blank piece of paper and the knowledge already in his mind, he could'' take a ''theoretical result and re-derive it ' in addition to requiring the strong infallibility assumption of consistency, gödel's second incompleteness theorem-which is stronger than his incompleteness theorem-is not mathematically relevant to the mind, according to specialists on that theorem. in gödel, escher, bach, douglas hofstadter points out that the assertion ''we cannot understand our own minds/brains''-which is a central theme of hofstadter's book-cannot be obtained as an application of that theorem and is merely a ''metaphorical analogue to gödel [cit], p. 697. the accuracy of hofstadter's observation is praised in a book pointing out many misuses of gödel's theorem, whose author torkel franzén applauds this specific passage of hofstadter for ''the virtue of making it explicit that the role of the incompleteness theorem [to obtain an assertion about the human mind/brain] is a matter of inspiration rather than implication'' (franzén 2005 p. 124) ."
"in this context, we view limit our analysis to convergence in lexical category choices, which can be the consequence of both social and cognitive processes. we call this specific quantification of accommodation \"linguistic alignment\", but it is closely related to general concepts such as priming and entrainment. this alignment behavior may be the result of social or cognitive processes, or both, though we focus on the social influences here."
"the answer to the above question appears to be ''yes'', if one is limited to the comprehensibility theorem. similarly, if one is limited to the discovery of gödel [cit], then the only known restriction on a formal logical system would be a restriction on the provability of a formal statement that merely expresses a property of that logical system itself. [cit] perhaps the vast majority of research mathematicians saw no relevance of the incompleteness theorem to the mathematical conjectures they sought to settle via proofs or counterexamples. [cit], however, restrictions on the provability of numerous mainstream mathematical-as distinct from logically self-referent-conjectures within pa or zf or zfc have been discovered. here are two examples. goldstein's theorem, which is provable in zfc and relates to expressing natural numbers using exponents and bases for exponents (distinct from the use of such notations within formal systems), is expressible in pa but not provable within pa; [cit] . the continuum hypothesis, which is related to the possible existence of a set whose cardinality is strictly between that of the set of natural numbers and that of the set of real numbers (see ''hilbert's thesis implies each rigorous math conjecture is a halting problem''), is expressible in zf but neither provable nor disprovable in either zf or zfc; gödel (1939), [cit] . the proofs of such results can be altogether different from gödel's proof of the incompleteness theorem. for instance, cohen's proof used his invention of the entirely new methodology related to zf known as ''forcing''."
"testing our hypotheses in different settings can help to resolve this issue. one possibility is to separate the election debate into small sets of conversations with different topics, and then compare the alignment patterns between sets. because of the lexical coherence that each topic of conversations have, we will be able to better separate the effect of context-and content-oriented words from the linguistic alignment result. as a result, we might be able to see negative alignment on rhetorical category between subset of conversations. we can also test our hypotheses with different languages. investigating alignment in languages that do not use pronouns heavily for reference can be useful to see how the group dynamics are expressed through different word categories. particles in some languages, such as japanese and korean, can mark specific argument roles, and this linguistic structure can allow us to detect syntactic alignment without looking much into the contextand content-oriented function words. lastly, the swam model is an adaptation of the wham model, and while the basic patterns look similar to those found by wham, a more precise comparison of the models' estimates with a larger dataset is an important step to ensure that the swam estimates are accurate."
"the comprehensibility theorem, published in a journal for specialists in formal mathematical logic [cit], appears to be such a theorem. the present article examines the extent to which this impression withstands additional study, addressing many questions about the theorem."
"as we have seen, the ''proof'' in ''a conjecture and a nonrigorous ''proof'''' uses a self-reference argument similar to an argument that produces paradoxical nonsense. the purpose of that section is to provide intuitive motivation for the mathematical model presented in this section. this section reviews turing machines and explains conventions used in this article. the section then presents a lemma that permits the model to avoid strong infallibility assumptions on agents, and describes how that lemma leads to a particular set of true formulas in peano arithmetic, the formal system reviewed in ''gödel's second incompleteness theorem is a halting problem result''. the section then explains how that set of formulas is used in the definition of ''correctly deduce correctness-related properties'' in a way that permits the proof of the comprehensibility theorem to avoid the kind of dangerously non-rigorous reasoning identified in ''the inherent lack of rigor in the preceding ''proof''''."
"let a denote a single agent, in the real-world sense of ''agent''; that is, let a be a real-world system that consists of one or more robots, supercomputers, humans, humans with surgically-implanted silicon memory chips, and so forth."
"one can use the same counterexample-seeking approach to see that some wellknown unsolved problems-such as the conjecture that there are no odd perfect numbers and goldbach's conjecture-are also halting problems. the point here is to gain insight into the significance of halting problems, rather than to suggest that counterexample-seeking programs provide an effective way to settle such conjectures. (see the end of section ''hilbert's thesis implies each rigorous math conjecture is a halting problem''.)"
"this question arises because any standard formal system extending pa that is inconsistent is trivial, since such a system permits the deduction of each sentence of pa, regardless of whether the sentence is true or false (see ''how does the comprehensibility theorem differ from gödel's theorem and the unsolvability of the halting problem?''). but the way the mathematical model underlying the theorem distinguishes between the agent a and the formal system f implies that the answer to the above question is ''no''. any natural-number valued function defined on a subset of the natural numbers is an agent; an agent need have no other property. in particular, in order for ''a can correctly deduce correctness-related properties of s'' to hold, a need only satisfy the requirements specifically related to s in the definition of that concept; there are no restrictions on the values (if any) of the function a for other inputs. thus, if such an a makes an incorrect decision (that counts in the sense of section ''where does the mathematical model avoid infallibility assumptions about agents?'')-including making opposite decisions about two turing machines that differ in an utterly insignificant way so that a's decisions would be inconsistent in the intuitive sense-that does not imply that any of a's additional decisions about other turing machines must be incorrect."
"they alignment patterns were comparable across the conversation types, except that trump supporters showed divergence when responding to clinton supporters. the ct conversation in table 2 reflects this divergence, with mexico being repeated rather than being replaced by they, suggesting trump supporters reject the elements clinton supporters attempt to put into common ground."
"in addition, our section ''is there only a restriction on an agent's deductive self-comprehensibility?'' observes that, if one is willing to assume a thesis due to donald knuth, the comprehensibility theorem appears to be the first mathematical theorem implying the impossibility of any ai agent or natural agent-including a not-necessarily infallible human agent-satisfying a rigorous and deductive interpretation of the self-comprehensibility challenge. some have pointed out the difficulty of self-comprehensibility, even according to presumably a less rigorous interpretation. this includes socrates, who considered that challenge to be among the most important of intellectual tasks. moreover, the impossibility just mentioned is a result in applied mathematics, in the sense that it is an application of a mathematical theorem."
"the description of applying the theorem given in our section ''applying the comprehensibility theorem to real-world agents'' attempts to be precise. we now employ less-precise terminology customarily used by computer scientists, placing such terminology inside double quotes. one can apply the theorem to real-world situations in which the ''proofs'' of programmers or of any agents are not actually presented within a formal axiomatic system as long as it is possible-in principleto do so, and in which the texts of programs are typically not in terms of turing machines and the texts of programs and ''proofs'' are typically not represented by numerical codes. recent practical software systems that discover proofs of termination and non-termination are discussed in es3; those proofs are not within a formal system, but one can consider them as the kind of ''proofs'' mentioned in this paragraph. as another kind of example, consider any ai agent that produces non-formal deductions-via a potentially unending search through any search space-where the agent also produces the path to any solution that it finds; one can view such a path as a ''proof'' of the corresponding deduction. furthermore, one can relax restrictions of section ''applying the comprehensibility theorem to realworld agents'' to permit a real-world agent to receive any input at any time, as long the comprehensibility theorem 471"
"it is possible for the set a of humans to correctly deduce the facts in the preceding paragraph (since we have just done so), so the italicized claim follows."
"recalling the game in that conjecture, the names d, b, and r can suggest ''decision system'', ''break-tie'', and ''result'', respectively. the notation c can suggest the ''partial correctness'' terminology used in formal verification, which means ''not producing incorrect output''. there are four formulas of pa denoted as just mentioned such that these properties hold [cit] lemma 8.1):"
"... if anything, humans are known to be inconsistent. this is certainly true for everyday reasoning, but it is also true for careful mathematical thought. a famous example is the four-color map problem. alfred kempe published a proof in 1879 that was widely accepted and contributed to his election as a fellow of the royal society. in 1890, [cit], p. 1023)."
"in the passage indicated by the ellipsis (...) in that quotation, knuth cites the following comment of george forsythe: '' [cit], emphasis in original."
"the set of formal theorems of pa is computably enumerable. this follows from a well-known dovetailing argument based on two facts: (1) there is an algorithm to generate the axioms of pa and (2) for each rule of inference of pa there is an algorithm to carry out the effect of that rule. thus, there is a turing machine s that takes as input the code for any sentence a of pa and exhaustively seeks a formal proof of a, halting if and only if it finds such a formal proof. gödel proved there is a sentence con pa in pa whose meaning under n is that pa is consistent, whose truth we assume as explained above. when applied to pa, gödel's second incompleteness theorem states that con pa is not provable in pa. that is equivalent to saying: s, when given as input the code for con pa, fails to halt. it is this sense in which one can view gödel's second theorem as a halting problem result."
"two requirements on a player that attempts to break a tie in an instance of the chess game are that the player make the correct decision in that instance of the game and that the player justify that particular decision with a proof of its correctness. to avoid one kind of infallibility assumption on agents, the model binds that player's attempted proof with the decision made by that player. this binding of proof with decision simplifies the model, by removing the otherwise possible situation-since infallibility assumptions are to be avoided-that the player makes one decision and yet that player's attempted proof of correctness of that decision is actually an attempt to prove something different, perhaps even the opposite decision! let a be a function from a subset of n to n. the function a models an agent for answering halting problem related questions, in the following way. the value of a at hp; halt?i is a decision about hp; halt?i iff aðhp; halt?iþ is a number that (a) can be computationally decoded in one way to produce exactly one of hðpþ and :hðpþ, for which let us say a with input hp; halt?i decides that p halts (respectively, does not halt) and (b) can be computationally decoded in another way to produce a list of codes of formulas, in a situation where a gives an attempted proof of the correctness of that decision; if such an attempted proof is indeed a (correct) formal proof in f, then let us say that aðhp; halt?iþ is logically correct in f. the model supports this two-fold computational decoding in a straightforward way; the details are relatively unimportant."
"here is how a is modeled as a function a a . the set a receives from the input queue a natural number n, where n is a code for a halting problem. for simplicity, this paragraph assumes this is the first input equal to n received by a. until a subsequently produces two official outputs in the required form that correspond to n (additional details are given later in this paragraph), the set a is not permitted to receive any additional input. after receiving n, the set a can attempt to decode n to obtain the statement of the problem to solve; it is not necessary to assume a accomplishes such decodings flawlessly. when-if ever -a is ready to indicate a result, it produces two official outputs: the value of n immediately followed by the value of a's attempted code for an attempted formal proof to settle the particular problem."
"the standard way to avoid the kind of incoherence similar to that of the liar paradox is via the use of a hierarchy, such as that obtained by using formal proofs as objects within an appropriate mathematical model. it is natural for such formaldeductions. the gist of the theorem is as follows, where a is an agent and s is software:"
suppose a and s satisfy the hypothesis of the conjecture. we show that s loses to a in the instance of chess played with a particular computer program that we construct.
"note that there is no effect on the value a a ðnþ if, between receiving its first input of n and producing the two official corresponding outputs in the required form, the set a produces other official outputs related to other work accomplished by a, as long as no such additional output affects directly or indirectly-such as by causing the interleaving of digits of two official outputs-either the first official output of a equal to n itself or the official output that immediately follows the first official output of a equal to n."
"the rest of this section presents a ''proof'' of the conjecture that is a modification of the proof of the unsolvability of the halting problem. for simplicity, we modify a concise proof of that result based on the use of a pointer to a subprogram [cit] . the ada programming language is used here, since it provides a highlevel of support for pointers to subprograms, via its access reserved word."
"interestingly, i showed the least variability, both in baseline and alignment, across the groups. however, i is also the only one of these pronoun groups that does not refer to someone else, and thus should be least affected by group dynamics. [cit] 's general finding of solid i-alignment, even in crossgroup communication. overall, we see effects both in the baseline and alignment values that are consistent with a strong group-identity construction process. furthermore, we see strong negative alignment in cross-group communication on pronouns tied to group identity and grounding, showing that cross-group animosity can overrule the general pattern of positive alignment in certain dimensions. however, the overall alignment is still positive; even the rejection of certain aspects of the conversation do not lead to across-the-board divergence."
"the description in this section places a priority on clarity, through the use of a highly restrictive rule that prohibits a from receiving any input during the time interval between when it first receives the input n and when it subsequently produces its two official outputs corresponding to n. this rule prevents a from ''solving'' the problem related to n by simply using the content of an additional input during the time it is trying to solve the given problem. of course, such an act would not be compatible with the intuitive meaning of ''correctly deduce''. to notice the kind of difficulty avoided here, observe that the natural number code for a formal proof that solves the problem related to n could appear as a number used within a particular turing machine related to another problem that is coded in an additional natural number input."
"rather than being about incompleteness or unsolvability, the comprehensibility theorem is about an agent's ability to make deductions about software. it identifies a relationship between an agent's ability to deduce a correctness property of software and that agent's ability to deduce an intelligence-related limitation of that same software."
"the just-given summary suggests ways in which the comprehensibility theorem is a general result. however the comprehensibility theorem also appears to be limited in scope, being only a restriction on an agent's deductive self-comprehensibility, as pointed out in our section ''is there only a restriction on an agent's deductive self-comprehensibility?''. that section mentions that gödel [cit] appeared to have a similar kind of limited scope, but the kind of incompleteness result revealed by gödel's theorem stimulated the subsequent discovery of restrictions on the provability of numerous mainstream mathematical conjectures. our section ''is there only a restriction on an agent's deductive selfcomprehensibility?'' also suggests that self-comprehensibility in some form might be essential for a kind of self-reflection useful for self-improvement that enables some agents, such as some human agents, to increase their success at problem solving."
"the value of a a ðnþ-if it exists-is the official natural number output-if there is such an output-produced by a that immediately follows the first output of n produced by a, after the first input equal to n received by a. because of the requirements related to ''first'', even if a ''changes its mind'' (whatever that might mean) and produces-after its first input equal to n-numerous official outputs paired with n at different times, a a as just defined is a mathematical function. this application strategy defines a function a a from a (possibly empty) subset of the natural numbers to the natural numbers without requiring the computability or infallibility of a, and one can apply the theorem to a a . (for other possible application strategies, see es1.)"
"because of the way check is constructed from s, check(checkmate'access) either returns the correct boolean value or does not halt. thus check(checkmate'access) does not return true, since that would imply that checkmate halts and also, due to the way checkmate is defined, directly cause checkmate not to halt. it is also the case that check(checkmate'access) does not return false, since that would imply that checkmate does not halt and directly cause checkmate to halt. thus check(checkmate'access) cannot halt, so the definition of checkmate makes it clear that checkmate itself cannot halt and thus the correct decision in the instance checkmate of chess is ''input does not halt''. because of the way check is constructed from s, it is also clear that s would give no response in the instance checkmate of chess."
"first of all, baseline usage of you shows that you was used more often among repliers in the crossgroup conversations. however, the alignment pattern for you was much stronger in within-group conversations. that is, repliers are generally more likely to use you in cross-group settings to refer to out-group members overall, but within the group, one member using you encourages the other to use it as well."
the definitions of ''correctly deduce'' used in the comprehensibility theorem relate to the ability an agent might have of deducing properties of a computer program. the notion of scientific deductions seems much broader than deducing properties of computer programs.
"this requirement on f does not imply the infallibility of an agent. that is because the model distinguishes between f and the agent. even if f has perfect properties, an agent a that uses f can fall far short of being infallible. (two examples of fallible agents are in our section ''rigorous analogues for both check and checkmate'' and an additional example is discussed near the end of es3.) that is analogous to saying the rules of the usual board game of chess have infallibility properties-such as implying that after each legal move no square contains two pieces-yet also saying a player of chess can be fallible and try to violate those rules, with such an attempt simply being called ''illegal'' (i.e., incorrect) ."
"1. s with input hp; halt?i halts iff hltðs; pþ is true according to n and 2. s with input hp; halt?i halts iff hltðs; pþ is a formal theorem of pa, where hltðs; pþ denotes the sentence of pa obtained by substituting the numerals for the codes of turing machines s and p into the slots in hlt where the free occurrences of the two respective free variables occur. [cit] this article uses notation that suppresses explicit use of numerical codes: notation such as fðá á á p á á áþ, where f represents a formula in a formal system and p represents a turing machine, will denote substituting the numeral for the numerical code for p into the corresponding slots in formula f where the free occurrences of the respective free variable occur."
"the game in fig. 1 conveys some intuitive ideas behind the model. let us call the game chess, an acronym for checking the stopping of software, using all capital letters to distinguish it from the board game of chess. the tie-breaker rule in the last sentence of that figure can be used when one player does not respond after a long period of time; a proof that that player will not make the correct decision is conceivable, for instance, if that player is software that the other player can examine. the chess game requires no infallibility assumption on the players: a player can make an incorrect decision and a player can also give a proof attempt that is incorrect. whether a player has indeed won an instance of chess depends on the correctness of the comprehensibility theorem 447 decisions and sometimes also on the correctness of a proof attempt by a player. perfect judgments about correctness may lie beyond human capabilities; the mathematical model described in ''the mathematical model'' is independent of that issue. [cit], but that article lacks the motivation for the proof of the comprehensibility theorem provided by the non-rigorous argument in our next section."
"to construct check from s, first change a procedure into a function, and then replace each occurrence of put(''input halts''); return; in s by return(true); replace each occurrence of put(''input does not halt''); return; by return(false); and replace each remaining return statement by the explicit infinite loop loop null; end loop; now define the procedure checkmate in fig. 2 . we complete the ''proof'' by ''proving'' the claim checkmate is an instance of chess that s loses when playing a."
"1. for each specific input for s: a can correctly deduce whether or not s makes an incorrect decision and a can provide a correct proof of that deduction. \"input halts\" if the player has decided that p halts \"input does not halt\" if the player has decided that p does not halt no response if the player has not decided whether or not p halts a player is permitted to make at most one decision corresponding to the input p. one way a player can win the game involving p is to make the correct decision and for the other player to make the incorrect decision. the game has no time limits, but a player can also win by making the correct decision, justifying that decision with a (correct) proof of its correctness, and also giving a (correct) proof that the other player will not make the correct decision. fig. 1 rules for the game of checking the stopping of software 2. for each specific input for s: if s produces output, then a can correctly deduce that output and a can provide a correct proof of that deduction. 3. a can correctly deduce (and correctly justify its deduction) that for each input of s that yields output, that output attempts to settle the halting problem for the corresponding input."
thus we can focus in the rest of the ''proof'' on the case in which s either prints the correct output or gives no response when presented with the data checkmate.
"adequate formal systems can differ sharply from one another. the simplest is obtained by adding the formulas of t to the axioms of pa; its set of theorems is computably enumerable, because the lemma in ''rigorous analogues for both check and checkmate'' guarantees an algorithm that takes any turing machine p as input and constructs the formulas of t corresponding to p. since the formulas of t are true, our assumption that pa is sound implies the truth of each sentence of pa that is a formal theorem of that system. one example of an adequate system whose set of theorems is not computably enumerable results from adding all true formulas of pa to the set of axioms of pa. another example whose set of theorems is not computably enumerable results from extending pa in the natural way by adding the symbol 2 and letting the axioms be those of zermelo-fraenkel set theory (zf) together with all true formulas of pa; the requirement about the truth of each sentence of pa that is a formal theorem of this system goes beyond the assumptions needed to prove the comprehensibility theorem."
"if one is willing to assume knuth's thesis, one can say the following. the impossibility of deductive self-comprehensibility by an artificial or natural agent is a result in applied mathematics; that is, it is an application of a mathematical theorem, rather than a philosophical assertion. moreover, it is independent of the controversial issue of whether an accurate computer simulation of that agent is possible: if such a simulation is not possible, then the impossibility of deductive selfcomprehensibility follows from knuth's thesis directly; otherwise, the impossibility of deductive self-comprehensibility follows from an application of the comprehensibility theorem. furthermore, since the agent mentioned in the comprehensibility theorem is not-necessarily infallible, the impossibility of deductive self-comprehensibility by the agent is independent of the controversial issue of whether one can view that agent as infallible."
"despite our hypothesis that the rhetorical categories of words could indicate different groups' preferred style of argumentation, these categories showed limited variation compared to the pronouns. the baseline values only varied a small amount between groups, with clinton supporters having slightly elevated baseline use of social and cognitive words, and slightly less positive emotion."
"the typical mathematician assumes hilbert's thesis as a working hypothesis, pursuing mathematical results while accepting the current standard criterion for rigor as the requirement on all rigorous proofs, and also assumes as a working hypothesis that zf is consistent. the typical mathematician is not actually concerned about the rigorous formal foundations of that discipline. but it is possible for one to view the research of the typical mathematician as a process that proceeds by adopting the working hypothesis that zf is consistent and then seeks proofs of conjectures, where the proofs are formalizable in principle within zf. of course, a proof of a counterexample is itself a proof of a conjecture."
"there are strong similarities between the ''proof'' of the claim in the preceding paragraph and the ''proof'' given in the preceding section. attempts to apply variants of gödel's theorem and/or the unsolvability of the halting problem to an agent's ability to deduce properties of software have failed similarly, due to the logical fallacy in the idea that it is actually meaningful to talk about ''the method mathematicians use to correctly decide problems''. like ''truth'', this phrase simply cannot be given an exact meaning which mirrors in every way its informal use, and assuming it can leads one directly into the kind of incoherence familiar from the liar paradox [cit] pp. 276-277) ."
"in our subsequent description of the use of the two queues, we do not assume that the one-dimensional real-world paper tape described by turing is infinite in extent. there will be no problem with our subsequent description, for instance, if there are single numbers too large to appear as inputs, and there will be no problem if there is inadequate space for some or all of the output."
"we would add that the computer forces such precise and thorough thinking, not just because of the rigorous language used in computer programs, but because of a bare computer's lack of background, common sense knowledge. we refer to the following assertion as knuth's thesis: fully understanding a phenomenon in the physical universe requires, among other things, being able to make deductions about a computer program that exists and accurately simulates that phenomenon. if one is willing to assume knuth's thesis, the answer to the question in this section's title is ''yes''."
"historical note: sect. 3 of gödel (1931) showed that the kind of unprovable formal statement identified in that article is equivalent to a statement about the existence of solutions to polynomial equations over the integers; also see [cit] p. 279) . the resulting statement about polynomials (merely) expresses a property of the logical system in question, since it is equivalent to the kind of unprovable formal statement identified in that article, which itself has that property."
"the above summary is vague. to avoid well-known pitfalls of paradoxical reasoning, crucially needed are mathematical definitions of such terminology as ''agent'', ''correctly deduce'', and ''adequate''. the next two sections provide what we hope is substantially more-helpful motivation for the definitions (and proof of the theorem), [cit] . those definitions then appear in ''the mathematical model'' section."
"the notation t denotes the infinite set of true formulas (1) through (5), where p ranges over all turing machines. formulas of t are written using nested implications to support a particular role of modus ponens mentioned in ''how the comprehensibility theorem fits into the model'', which is the trick that enables the proof of the comprehensibility theorem to avoid the kind of dangerously nonrigorous reasoning identified in our section ''the inherent lack of rigor in the preceding ''proof''''."
"a standard methodology for avoiding paradoxical nonsense in arguments that involve self-reference is to provide fully mathematical definitions of all concepts, and to use a hierarchy of languages. that is the methodology used within the mathematical model. for instance, it is essential that the model require any ''proof'' given by a player in the chess game of fig. 1 to be a mathematically-defined (rather than intuitively-defined) object, namely a formal proof within a formal axiomatic system. also, since there is no known rigorous definition of ''human'', of necessity the theorem is presented at a higher level of abstraction, so that the theorem is applicable to agents of any kind."
"this article assumes all axioms of pa are true (according to n ) and all rules of inference of pa preserve truth, and hence that pa is sound; that is, all the formal theorems of pa are true. as explained in ''how can one assume that a system used by an agent is sound without assuming agent infallibility?'', this widely-accepted soundness assumption is altogether different from assumptions about the infallibility of agents. to say pa is consistent means there is no sentence a in the language of pa such that both a and :a are formal theorems of pa. it follows from the soundness of pa that pa is consistent."
"the comprehensibility theorem is an implication relating two kinds of deducing: roughly, if an agent a deduces some correctness-related properties of s, then a also deduces an intelligence-related limitation of s. the theorem requires correctness of a as that relates just to the specific s. all other formal proof attempts by a can be incorrect, and there can be infinitely many such attempts. but a's deductions about s must be correct regardless of how many inputs to s affect the output of s, and for some s there are infinitely many such inputs. [es10 explains how the correctness requirement on a could be greatly reduced.]"
"from our results, we see that social context affected pronoun use and alignment, which fits into the communication accommodation theory account [cit] . meanwhile, rhetorical word use and alignment was independent of social context between speakers, though it is unclear whether this reflects a perception of equal footing in their power dynamics or is driven primarily by automatic alignment influences rather than social factors [cit] . to expand the scope of this argument, we can further test if the negative alignment can be found in other liwc categories as well, which have no clear group-dynamic predictions."
"a seminal innovation of gödel was the analogous arithmetization of metamathematics, in which properties relating to formal systems are translated into naturalnumber properties that, in turn, are formalizable in systems such as pa. this permitted gödel to treat mathematically the idea expressed by the assertion ''this statement is not provable'', using methodology avoiding the non-rigorous reasoning that results from examining the truth of the assertion ''this statement is false''. that enabled gödel to prove his incompleteness theorem in an acceptable mathematical way."
our exposition will not require readers to master the next section's details; whenever we use the notation of that section-such as in our section ''definition of ''correctly deduce''''-we accompany its use with a corresponding intuitive explanation. to have replaced all mention of the notation in the next section with sketchy exposition would have resulted in a vague and thus inadequate explanation of the model.
"2 in order to be a decision about hp; halt?i, the model requires aðhp; halt?iþ to be the code of a list of natural numbers of finite length (the length can be 1, such as when a does not attempt a proof) such that the last member of that list is the code either of hðpþ or of :hðpþ, and a's proof attempt is logically correct in f iff aðhp; halt?iþ is the code of a formal proof of f. to avoid a circular definition of ''adequate'', the construction of the model does not use the part of this definition depending on the hp; halt?i 2 dðaþ implies ½aðhp; halt?iþ is a decision about hp; halt?i."
"as an input does not provide ''a hint or complete solution'' to a halting problem that agent is currently trying to solve. eliminating formal proofs in applications of the comprehensibility theorem does not undermine the use of formal proofs in our section ''hilbert's thesis implies each rigorous math conjecture is a halting problem'' in showing that hilbert's thesis implies broad importance to halting problems, since one can translate a non-formal conjecture into a formal conjecture and obtain related software, such that answering the halting problem for that software is equivalent to answering the non-formal conjecture. also, [cit] ."
"linguistic alignment, the use of similar words to one's conversational partner, is one prominent and robust form of this accommodation, and has been detected in a variety of linguistic interactions, ranging from speed dates to the supreme court [cit] . in particular, this alignment is usually positive, reflecting a widespread willingness to accept and build off of the linguistic structure provided by one's interlocutor; the differences in alignment have generally been of degree, not direction, subtly reflecting group differences in power and interest."
"the introduction mentions three long-standing shortcomings of earlier treatments: their assumptions about the infallibility of agents, use of inherently fallacious reasoning, and non-applicability to artificial agents. this article supports the conclusion that the comprehensibility theorem overcomes those shortcomings; see ''where does the mathematical model avoid infallibility assumptions about agents?'', the mathematical model, and ''applying the comprehensibility theorem to real-world agents'', respectively."
"proof in mathematics is both a means to understand why some result holds and a way to achieve precision. as to precision, we have now stated an absolute standard of rigor: a mathematical proof is rigorous when it is (or could be) written out in the first-order predicate language lð2þ as a sequence of inferences from the axioms zfc, each inference made according to one of the stated rules. [cit] p. 377) hilbert's thesis is the assertion that each provable mathematical statement can be expressed in the first-order language of set theory and has a formal proof in zf (or in zfc); for example, see [cit] p. 263), which also observes:"
"also note that the comprehensibility theorem, like any theorem, can be written in the form (p implies q). each application of such a theorem can be written as:"
"isolating the axioms of zermelo-frankel set theory is a remarkable and outstanding achievement. the formal system zfc is a clear, precise answer to this question: what logical axioms, rules of inference and nonlogical axioms suffice to derive the theorems of modern mathematics?"
perhaps the two most highly publicized mathematical results of the last half of the twentieth century were the proofs of the four color theorem and fermat's last theorem. this section shows how a simple counterexample-seeking approach easily enables one to see that these two theorems concern halting problems.
"in total, four sets of twitter data were gathered. the first two datasets (tt, cc) consisted of conversations between members of the same group (within-group conversation). the other two datasets (tc, ct) consisted of conversations across the groups (cross-group conversation). in the dataset references, trump supporters' message is represented with t, and clinton supporters' message is represented with c. the first letter indicates the initiator's group; the second indicates the replier's group. there is an average of 266 unique repliers in each group."
"the preceding paragraph also provides an example that helps us emphasize the difference between truth and the concept of provability in a formal system: assuming the consistency of pa, con pa is true according to n but, by gödel's second incompleteness theorem, con pa is not provable in pa. the comprehensibility theorem is about the concept of provability."
"alan turing went so far as to assert ''if a machine is expected to be infallible, it cannot also be intelligent'' [cit] (turing p. 124, [cit] . more recently, marvin minsky [cit] ) has asserted ''there's no reason to assume ... that either human minds or computing machines need to be perfectly and flawlessly logical''."
"also, the comprehensibility theorem can be applied to much broader kinds of systems than can gödel's and turing's results. an agent need only be a function from natural numbers to natural numbers, rather than having to satisfy morerestrictive properties. (see ''applying the comprehensibility theorem to realworld agents''.) there are uncountably-many functions from natural numbers to natural numbers, far more than the countably-many such functions defined by turing machines. likewise, gödel's theorem is about formal axiomatic systems whose axioms and rules of inference are computable. the comprehensibility theorem can be applied to human agents, without assuming anything about either the computability or the infallibility of humans."
"comprehensibility theorem: let a any agent, s any tm decision system, and f any adequate formal system within which all formal proofs are to be given."
"if a can correctly deduce correctness-related properties of s, then a can correctly deduce s does not have a 's capability for solving halting problems."
"3 as explained in footnote 1, one can choose to use prime-power coding, so that hs; p; correct?i denotes 2 #s 3 #p 5 2 . that last exponent is 2 since correct? represents the constant 2. similarly, one can code the three other halting-related problems, mentioned shortly, by letting result? represent 3, decision? represent 4, and breaktie? represent 5."
"we assume without loss of generality that, for any input program p, procedure s either generates no output or prints exactly one of ''input halts'' and ''input does not halt '' and: prints ''input halts'' and halts if s decides that p halts prints ''input does not halt'' and halts if s decides that p does not halt no response if s cannot decide whether or not p halts this does not imply that all-or even some particular small fraction-of s 's decisions about the halting of programs are correct, which sharply distinguishes the approach of this game from the approach of standard gödel-turing arguments. we assume without loss of generality that the final statement in s is return; that the first of the above three actions results from executing put(''input halts''); return; the second results from executing put(''input does not halt''); return; and the third either results from executing a return statement without having generated output or from s's own failure to halt. define a function check with declarations type accesstoprocedure is access procedure; function check (p: accesstoprocedure) return boolean;"
"the purpose of this section is two-fold: to explain some intuitive ideas related to the comprehensibility theorem, and to provide an opportunity for pointing out the need for much greater rigor in this context. the section assumes that one of the two players in the chess game of fig. 1 is human in nature, but the mathematical model presented after this section adopts a much more general viewpoint. the phrase ''actual human'' appearing in the following conjecture emphasizes the lack of an assumption that humans are infallible."
"like any other real-world application of mathematics, such as applying a theorem about differential equations to the flight of a rocket, the application strategy we describe cannot be fully mathematical, since no one has ever given a fully mathematical definition of any real-world object. similarly, in explaining how the kind of abstract mathematical machine alan turing defined captures essential aspects of the real-world concept of computation, he made the following nonmathematical assumption, where by ''computing'' he meant computing by realworld (in particular, human) agents:"
"the preceding paragraph presents reasons for a ''yes'' answer to the question addressed in this section. [cit], p.1399. but that assumption is not essential here, since section ''is it necessary to expect an agent to give formal proofs?'' explains that in applying (as distinct from proving) the comprehensibility theorem it is not essential to require an agent to give formal proofs."
"we also point out that there are many important historical examples where an effective framework into which to translate a mathematical problem is quite different from that of the initial problem itself. [cit], which solves a halting problem, even in the non-formal sense of section ''some results and conjectures that are halting problems''. thus, investigating the software p c directly need not be an effective way to settle the conjecture c, and an exhaustive search for a proof in zf seems an unreasonable way to prove a result, even though the latter is guaranteed to succeed any time such a proof exists, assuming one could carry out a huge number of steps flawlessly (an assumption avoided in this article)."
"a possible reason for the lack of differences in argumentation style may be uncertainty about the setting of the cross-group communication. elevated causation word usage has been argued to be employed by the minority position within a debate, to provide convincing evidence against the status quo [cit] . the datasets consist of conversations from the middle of the election campaign, when it was uncertain which group was in the majority or minority (as seen in the first tt conversation in table 2 ). this uncertainty may have led both groups to adopt more similar argumentation styles than if they believed themselves to occupy different points in the power continuum."
"in principle, one can state the comprehensibility theorem and carry out its proof within zf, thereby satisfying the current standard criterion for rigor, without the need for assuming the consistency of zf or the need for defining within zf the details of each specific f that satisfies the requirements for being adequate."
"computing is normally done by writing certain symbols on paper. we may suppose this paper is divided into squares like a child's arithmetic book. in elementary arithmetic the two-dimensional character of the paper is sometimes used. but such a use is always avoidable, and i think that it will be agreed that the two-dimensional character of paper is no essential of computation. i assume then that the computation is carried out on onethe comprehensibility theorem 461 dimensional paper, i.e., on a tape divided into squares. i shall also suppose that the number of symbols which may be printed is finite. [cit] for convenience, we adopt turing's assumption that there exists such a onedimensional paper tape in the real-world. without loss of generality we further assume there are just two kinds of symbols that may be printed on the paper. to the extent that space on the paper tape permits, the unary code for a natural number can be printed on the paper, with multiple such codes separated by single blank squares. also, without loss of generality, we may further assume that one of the squares of the paper is designated as the origin, so that (to the extent that space on the paper tape permits) the list of input numbers appears in order left-to-right as a fifo queue to the right of that square, and the list of output numbers appears in order right-toleft as a fifo queue to the left of that square."
"the comprehensibility theorem can be paraphrased as saying that if an agent can correctly deduce correctness-related properties of software s, then that agent can correctly deduce that s does not have that agent's capability for solving halting problems."
"the present work proposes a new model of alignment, swam, which adapts the wham alignment model . we examine alignment behaviors in a setting with clear group identities and enmity between the groups but with uncertainty on which group is majority or minority: [cit] u.s. presidential election. unlike previous alignment work, we find some cases of substantial negative alignment, especially on personal pronouns that play a key role in assigning group identity and establishing common ground in the discourse. in addition, withinversus cross-group conversations show divergent patterns of both overall frequency and alignment behaviors on pronouns even when the alignment is positive. these differences contrast with the relatively stable (though still occasionally negative) alignment on word categories that reflect possible rhetorical approaches within the discussions, suggesting that group dynamics within the argument are, in a sense, more contentious than the argument itself."
"it is well-known that arguments attempting to apply gödel-turing like resultsby that phrase we mean attempts to apply variants of gödel's theorem and/or the unsolvability of the halting problem to the processing of software by agents-have failed. for instance, this is made clear in an ai journal article [cit], whose conclusions include the following, where we indicate in italics shortcomings of all arguments that have attempted to apply gödel-turing like results:"
"the four color theorem asserts that each flat (''planar'') map is colorable using four colors so adjacent countries have different colors; equivalently, each planar graph is colorable using four colors so adjacent vertices have different colors. it is straightforward to construct software that generates the planar graphs using kuratowski's theorem [cit], tries all possible combinations of a particular set of four colors on each so as to satisfy the stated constraint, and halts if and only if it finds a graph that cannot be so colored. the four color theorem is equivalent to saying that the software just described fails to halt."
"additionally, four rhetorical word categories are considered. in liwc, psychological processes are categorized into social processes, cognitive processes, and affective processes, the last of which covers positive and negative emotions. social and affective process categories are, as their names indicate, the markers of social behavior and emotions. cognitive process markers include words that reflect causation (because, hence), discrepancy (should, would), certainty (always, never), and inclusion (and, with), to name a few. a speaker's baseline usage of rhetorical categories will present the group-specific speech styles that may be dependent on group identity, reflecting preferred styles of argument. the degree of alignment on rhetorical categories indicates whether speakers maintain their group's discussion style or adapt to the other group."
"if a can correctly deduce correctness-related properties of s, then a can correctly deduce s does not have a 's capability for solving halting problems."
"let a be a function from natural numbers to natural numbers, so that a models an agent. let f be adequate. let s be a tm decision system; thus s has the property: for each halting problem input of s for which s produces output, that output attempts to settle the halting problem for the corresponding input. the definition of ''a can correctly deduce correctness-related properties of s'' has four parts, where a gives its correct proofs within f:"
"the nonrigorous definition of ''correctly deduce'' in our section ''a conjecture and a nonrigorous ''proof'''' has three parts. the model provides not only a mathematical way to restate those three parts, it supports a fourth part that enables the proof of the theorem to avoid the kind of dangerously non-rigorous reasoning identified in our section ''the inherent lack of rigor in the preceding ''proof''''."
returns true if s decides that p halts returns false if s decides that p does not halt does not halt if s cannot decide whether or not p halts
"you alignment in within-group conversation could reflect rapport-building, a sense that speakers understand each other well enough to talk about each other, and an acceptance of the other's common ground (as in the example for cc in table 2 ). on the other hand, you alignment in between-group conversations should be interpreted as the result of disagreement to each other (see examples for tc in table 2 ). you alignment in this case is the action of pointing fingers at each other, which happens at an overall elevated level, regardless of whether the other person has already done so."
"system that is not provable in that system) in attempts to apply gödel-like results to the comprehensibility of software by an agent, but such an analogy overlooks the above italicized point. such attempts are based on investigating the consequence of assuming that an agent reasons using (correct) formal proofs and that the agent's reasoning system itself is consistent. thus, such attempts assume the agent's reasoning is infallible in a strong way, since they assume the agent is never inconsistent. (see es6.) to see how the lemma avoids infallibility assumptions, notice that for each of parts (a) and (b) there exists a highly-fallible turing machine that satisfies the hypothesis of that part; in fact, that makes infinitely many incorrect decisions about halting questions. a simple such p for which the hypothesis of (a) holds is the turing machine that, for each turing machine q, takes input hq; halt?i and produces as output the code for the list whose only member is the code for hðqþ. changing h to :h in such an example yields a simple such p for which the hypothesis of (b) holds. it is also easy to give an example for which the hypothesis of (c) holds: just consider a turing machine p that, for each turing machine q, takes input hq; halt?i and fails to halt."
"saying a player a ''can correctly deduce correctness-related properties'' of software s (whose output, if any, is an attempt to settle the halting problem for the corresponding input), intuitively means that, given the source code for s:"
"before leaving the question addressed in this section, we suggest that selfcomprehensibility in some form might be essential for a kind of self-reflection useful for self-improvement that enables some agents, such as some human agents, to increase their success. that may be the reason socrates ranked self-comprehensibility ''among the most important of intellectual tasks''."
"pronoun usage and alignment reflect the group dynamics between trump supporters and clinton supporters, and observations of negative alignment are consistent with a battle over who defines the groups and common ground. however, the use and alignment of rhetorical words were not substantially affected by the group dynamics but rather reflected that there was an uncertainty about who belongs to the majority or minority group. in a political debate or conversation between opponents, speakers are likely to project their group identity with the usage of pronouns but are likely to maintain their rhetorical style as a way to maintain their group identity."
the lack of infallibility assumptions on agents is discussed near the end of es3 in the context of sketching a particular real-world application of the theorem.
this section presents a nonrigorous argument related to a particular conjecture. we then explain why the treatment in this section lacks rigor and does not yield any theorem.
"the theorem requires an agent to give its problem-solving proof attempts within an ''adequate'' formal system f. in order for f to be adequate, the model requires that t be a subset of the axioms of f, and also that f satisfy the straightforward requirements in the rest of this paragraph. it must be an extension of pa; that means that each formula (respectively, axiom, rule of inference) of pa is a formula (respectively, axiom, rule of inference) of f. each sentence of pa that is a formal theorem in f must be true. finally, each formula a of f must have a gödel code #a so that the function that maps a to #a and its inverse are computable, and when a is also in the language of pa, the code for a must be the same as pa's code for a."
"where hypothesis h is the conjunction of some necessarily non-mathematical assumptions about the real-world situation, and where \" p and \" q are the interpretations of p and q in the real-world situation. to apply the comprehensibility theorem, one states within h the real-world assumptions specific to the constituents of a and the functioning of the input and output queues. one obtains \" p and \" q by replacing agent a in each of p and q by a a . the effect is to rephrase statements about values of a as statements about a and the input and output queues."
"it is important to interpret technical results-such as about ai ''belief'' nets, ''expert'' systems, and ''knowledge'' bases investigated in other articles, and about ''agent'', ''correctly deduce'', ''adequate'', etc., in this article related to the comprehensibility theorem-using their accompanying definitions and constructions rather than other possible interpretations of the same words. it is also important to distinguish between the concepts of truth and provability; see the end of section ''gödel's second incompleteness theorem is a halting problem result''."
"we first consider the case in which s prints an incorrect output when presented with checkmate. in this case, the correct deduction assumption implies it is possible for a to both correctly deduce that fact and to correctly deduce what s's output is (as well as to correctly prove those deductions). knowing which of the two outputs is incorrect enables a to deduce the other output is correct. the italicized claim in the preceding paragraph thus follows in this case."
"however, scientific deductions related to a natural phenomenon typically involve a reductionist process that attempts to analyze the phenomenon into primitive basic principles, explaining the phenomenon via a systematic, algorithmic procedure based on the resulting primitive principles. one can explain that reductionist process as an application of a viewpoint expressed by donald knuth-when that viewpoint is taken literally. knuth asserts that we don't fully understand something until we teach it to a computer, by writing an algorithm to do it. in knuth's words:"
"the results of baseline frequency and alignment values for the four conversation types are presented in figure 2 and 3, respectively. we analyze each pronoun set in turn."
let us refer to this three-part assumption as the ''correct deduction assumption''. by saying a player a can ''correctly deduce that s does not have a's capability in the chess game'' we intuitively mean that there exists an instance of the chess game such that a can correctly deduce that s loses to a in that instance of the game.
"the risk of using undefined capabilities of human reasoning within an argument is easy to see. here is a concise example of a fallacious ''proof'' in the context of human deductions, based on the vague concept of ''correctly prove'' and the nonrigorous use of self-reference in which humans participate inside the ''proof''. let s be the sentence ''no human can correctly prove that this sentence is true'', so that the meaning of s is ''no human can correctly prove that s is true''."
"a person well-trained in computer science knows how to deal with algorithms: how to construct them, manipulate them, understand them, analyze them. this knowledge is preparation for much more than writing good computer programs; it is a general-purpose mental tool that will be a definite aid to the understanding of other subjects, whether they be chemistry, linguistics, or music, etc. the reason for this may be understood in the following way: it has often been said that a person does not really understand something until after teaching it to someone else. actually a person does not really understand something until after teaching it to a computer, i.e. expressing it as an algorithm ... an attempt to formalize things as algorithms leads to a much deeper understanding than if we simply try to comprehend things in the traditional way. [cit] pp. 9-10), [cit] ."
"in the present work, we consider cross-group alignment on personal pronouns, which can express group identity, as well as on word categories that may indicate different rhetorical approaches to the argument [cit] suggests that it is reasonable to expect the different word usage from different groups. in fact, although we find mostly positive alignment, we do see negative alignment in some cross-group uses, suggesting strong group identities can overrule the general desire to align."
"the comprehensibility theorem is based on natural-number valued functions, turing machines, formal systems, and halting problems, which provide a convenient level of abstraction. yet one can see a connection between the model and practical, not-necessarily infallible, problem-solving systems, as discussed in ''where does the mathematical model avoid infallibility assumptions about agents?'' and ''is it necessary to expect an agent to give formal proofs?'', and in (electronic supplement) es3 and es4. like the definition of the limit concept and the definite integral concept in calculus, the mathematical definitions used to support the comprehensibility theorem are technical and somewhat complicated, yet arise naturally from a simple context; see ''a game based on solving halting problems''."
"the comprehensibility theorem appears to be the first mathematical theorem (see ''is gödel's second incompleteness theorem mathematically relevant to the mind?'') implying the impossibility of any ai agent or natural agent (including a not-necessarily infallible and not-necessarily computable human agent) satisfying a rigorous and deductive interpretation of the delphic challenge ''know thyself'', if one assumes knuth' s thesis concerning what is necessary for fully understanding any natural phenomenon (see the section ''are deductions about scientific phenomena related to deductions about computer programs?''). even according to presumably a less rigorous interpretation of that self-comprehensibility challenge, socrates-according to plato-admitted not being able to satisfy the challenge, yet considered that challenge to be among the most important of intellectual tasks; see plato's phaedrus 230 [cit] p. 478) . likewise, america's first widely-known scientist-benjamin franklin-considered that challenge ''extremely hard'' to satisfy; see passage 521 [cit] ."
"since they are quite different from standard uses of formal logic, the model and theorem raise many questions. the preceding part of this article is essential to facilitate this section and the electronic supplement, which address over twenty such questions. to the best of the author's knowledge, all sections address previously-unaddressed questions, except for sections ''how important is eliminating infallibility assumptions about intelligent agents?'', ''where does the mathematical model avoid infallibility assumptions about agents?'', ''how can one assume that a system used by an agent is sound without assuming agent infallibility?'', ''is an agent that makes mistakes necessarily trivial?'', and es3 (of the electronic supplement), [cit] . some new explanations appear in each section. occasionally a discussion here briefly repeats something elsewhere in this section, to facilitate skipping to questions of most interest to the individual reader."
"for convenience, whenever this article refers to the meaning of a formula of pa, we assume the use of the standard interpretation n of the language of pa (see ''gödel's second incompleteness theorem is a halting problem result''). also, the more concise notation hðpþ is used for hltðp; pþ, and for any turing machine p the phrase ''p halts'' is used as an abbreviation for the phrase ''p with input hp; halt?i halts''. for instance, it follows from these conventions and (1) that the meaning of the formula hðpþ is that p halts, and the meaning of :hðpþ is that p does not halt, where : denotes negation."
"this study examines alignment and baseline word use on 8 word categories from linguistic inquiry and word count (liwc; ), a common categorization method in alignment research. details on word categories and example words for each category can be found in table 1 the i pronoun, such as i, me, mine, myself, i'm, and i'd. we choose four pronoun categories (i, you, we, they) to investigate the relationship between group dynamics and linguistic alignment. we expect that in a conversation between in-group members, i, we, they will be observed often. when these pronouns are initially spoken by a speaker, repliers can express their in-group membership while aligning to their usage of the words at the same time. in the conversation with out-group members, you usage will be observed more often because it will allow repliers to refer to the speaker while excluding themselves as a part of the speaker's group. in the cross-group conversation, alignment on inclusive we indicates that repliers acknowledged and expressed themselves as a member of speakers' in-group. however, alignment on exclusive they in cross-group conversation should be interpreted with much more attention. when a replier is aligning their usage of they to their out-group member, it likely indicates that both groups are referring to a shared referent, implying enough cooperation to enter an object into common ground [cit] ."
"there is some evidence and an intuition that alignment can cross group boundaries, but it has not been measured using such models of adaptive linguistic alignment. [cit] pointed out that speakers with negative feelings are likely to coordinate their linguistic style to each other, while speakers who are not engaged to each other at all are less likely to align their linguistic style. speakers also might actively coordinate their speech to their opponents' in order to persuade them more effectively [cit] . if two people with different opinions are talking to each other, they may also align their speech style as a good-faith effort to understand the other's position [cit] ."
"like standard proofs of the unsolvability of the halting problem, the proof of the lemma uses a diagonal argument. unlike the lemma, standard proofs of the unsolvability of the halting problem conclude with a contradiction having a form such as: p 0 halts iff p 0 does not halt. also, whereas such standard proofs assume that 100 % of the decisions about halting questions made by a particular turing machine p are correct, the proof of the lemma does not assume that even 1 % of p 's decisions about halting questions are correct. at first glance, p 0 might seem analogous to a gödel sentence (a true sentence in a consistent formal axiomatic footnote 2 continued particular system f until after a set of formulas t -that do not depend on f-is defined in ''a set of formulas supporting a technical trick''. using a list of natural numbers as described in this footnote is analogous to late binding within the implementation of dynamic functions in object-oriented languages [cit] sect. 14.2)."
"wham infers two key parameters: η align and η base, the logit-space alignment and baseline values, conditioned on a hierarchy of gaussian priors. swam estimates the two parameters directly as:"
"baseline usage of they shows the opposite pattern from you usage, with higher they usage in the in-group conversations. this type of they usage can be a reference to out-group members (see the second example for tt in table 2 ). by using they, repliers can express their membership as a part of the in-group and make assertions about the outgroup. it also can reflect acceptance of the interlocutor placing objects in common ground, which can be referred to by pronouns."
"the wham framework uses a hierarchy of normal distributions to tie together observations from related messages (e.g., multiple repliers with similar demographics) to improve its robustness when data is sparse or the sociological factors are subtle. this requires the researcher to make statistical assumptions about the structure's effect on alignment behaviors, but can improve signal detection when group dynamics are subtle or group membership is difficult to determine ."
"more precisely, for each p, consider the set containing the axioms in t and the (at most three) formal theorems related to s and p resulting from parts (1) through (3). whenever it is possible to obtain a formal theorem about s and/or p from that set using at most three additional applications of modus ponens, a can yield a proof of that formal theorem. by a formal theorem ''about s and/or p'' is meant a formal theorem hðpþ or :hðpþ or a formal theorem that results from substituting s and p, in that order, into b, :b, hlt, or :hlt."
"the most fundamental of concepts are those arising in a variety of contexts. that is the case for halting problems. as just explained, hilbert's thesis-assumed as a working hypothesis by nearly all mathematicians-implies that each rigorous mathematical conjecture is equivalent to a halting problem. without using formal logic, ''some results and conjectures that are halting problems'' explains how it is easy for one to view as halting problems two highly publicized mathematical results of the latter half of the twentieth century. [cit] asserts that ''the ability to break out of loops of all types seems the antithesis of the mechanical''. [cit] speculates on the usefulness of giving a robot a primitive consciousness-like ability via a heuristic infinite loop detector: ''if you have a maze-running robot, if it has any way of discovering that it's just repeating its actions, then you have some way of getting out of that loop.'' moreover, solving halting problems can enhance the success of commonly used operating systems and other practical software; a team coordinated by microsoft research, cambridge, has recently developed software that discovers halting proofs for some practical operating system programs [cit] ) and, in the opposite direction, another team has recently developed software that discovers non-halting proofs for some practical programs including a permissions-controller for memory-protection in an operating system [cit] . (see es3 and es4; the proofs mentioned in the preceding sentence are not formal proofs, but see ''is it necessary to expect an agent to give formal proofs? ''.) due to the fundamental nature of halting problems, it is reasonable when constructing a mathematical model related to intelligence to consider a facet of intelligence related to solving them. intuitively speaking, if an agent possesses general intelligence, then the agent should perform well in this facet of intelligence; of course the converse need not hold. the mathematical model underlying the comprehensibility theorem is motivated by focusing on the fundamental capabilities of agents, rather than the relative speeds of agents."
"problem-solving software that is not-necessarily infallible is central to ai. such software whose correctness and incorrectness properties are deducible by agentsboth artificial and natural agents, and where the agents themselves are notnecessarily infallible-is an issue at the foundations of ai. a related theorem would help to define the boundaries of ai research related to agents and software, whether the software is written by humans or generated artificially. if such a theorem existed, having a working knowledge of it could be helpful for ai researchers, just as it is helpful for computer scientists to have a working knowledge of limitative results such as np-completeness and the unsolvability of the halting problem."
"claiming in-group membership by using ingroup identity marker can be one way of claiming common ground, which indicates that speakers belong to the group who shares specific goals and values [cit] . therefore, trump supporters' baseline use and alignment of we and they suggest that they were accepting and reinforcing common ground with ingroup members by using we, but rejecting common ground with out-group members by not aligning to they. clinton supporters showed a different way of reflecting their acceptance and rejection. they chose to reject common ground by not aligning to their out-group members' in-group marker we, but seemed to accept the common ground within the conversation built by out-group members' use of they."
"the rest of the article uses the prime notation on a tm decision system, and applies the properties of that notation given by the lemma."
"one thing to point out is that even though pronouns and some rhetorical words are categorized as function words, which have been hypothesized to reflect structural rather than semantic alignment [cit], these category words are still somewhat context-and contentoriented. that is, use and alignment of some function words is inevitable for speakers to stay within the topic of conversation or to mention the entity whose referential term is already set in the common ground. from trump supporters' negative alignment on they, we could see that speakers were in fact able to actively reject the reference method by not using the content-oriented function words. in the future work, it will be meaningful to separate the alignment motivated by active acceptance and agreement from the alignment that must have occurred in order to stay within the conversation."
"the mathematical community as a whole has … abandoned any attempts to solve by mainstream methods a question mathematical logicians have inferred to be undecidable. the work of the few who are exceptions to this generalization bears all the signs of being the work of crackpots. [cit] the present author prefers avoiding the word ''crackpots'' to describe people, but the above quote helps indicate the very strong acceptance of hilbert's thesis. (as mentioned at the end of the preceding section, the current article aims to be as broadly applicable as possible, and does not assume hilbert's thesis is the only reasonable approach.) before leaving this discussion of hilbert's thesis, we should point out that this article does not confuse the formalization of a conjecture with the formalization of the provability of a conjecture; see es9."
the end of the above ''proof'' depends on the participation of humans inside the ''proof''. we clarify this point by indicating the sense in which it is possible for a to construct its proof attempt in the game.
"formulas d, b, r, and c concern truth according to n, [cit] avoids conflict with tarski's theorem on the undefinability of truth. that theorem, whose proof is a rigorous reformulation of the liar paradox,"
"recent models of linguistic alignment have attempted to separate homophily, an inherent similarity in speakers' language use, from adaptive alignment in response to a partner's recent word use [cit] . if homophily is not separated from alignment, it is impossible to compare withinand cross-group alignment, since the groups themselves are likely to have different overall word distributions. both alignment and homophily can be meaningful; [cit] combine the two to estimate employees' level of inclusion in the workplace."
"a turing machine a is a tm decision system iff a is a decision system. [cit] and the proofs in this article, although the choice of terminology is different here. instead of defining ''decision system'', that article defined ''deductive system'', in a way that would have the effect of requiring a formal proof by a player regardless of how the player would win the chess game introduced in this article. also, for greater clarity, this article's statement of theorem and proof uses phrases involving ''deduce'' rather than phrases involving ''understand''.] rigorous analogues for both check and checkmate a lemma guarantees a computational way to obtain from any tm decision system p a turing machine p 0 such that, roughly speaking, the following holds. if p halts when asked to determine whether or not p 0 halts, then p's decision about that halting problem is incorrect; furthermore, if p goes into an infinite loop when asked to determine whether or not p 0 halts, then p 0 itself does not halt. the relationship between p and p 0 is analogous to the relationship between check and checkmate in the inherently-flawed ''proof'' of the conjecture in ''a conjecture and a nonrigorous ''proof''''."
"this section examines the application of the comprehensibility theorem to realworld agents, without requiring that such agents satisfy computability or infallibility properties. the description in this section is unconventionally precise. our later section ''is it necessary to expect an agent to give formal proofs?'' employs the kind of less-precise terminology commonly used in real-world situations."
"the current standard criterion for a proof to be considered mathematically ''rigorous'' is that it can be carried out in principle within the constraints of zermelo-fraenkel set theory, abbreviated zf. many working mathematicians extend this to include the axiom of choice, to get the system zfc, whose consistency is implied by the consistency of zf, according to a theorem of gödel."
"as explained in our ''conventions'', halting problems are formalizable in pa. our section ''a set of formulas supporting a technical trick'' will explain how additional aspects of the halting-problem chess game of fig. 1 are also formalizable in pa. many ai researchers are more familiar with other technical mathematics than with formal peano arithmetic. thus, prior to explaining the model's additional use of formal logic shortly, we provide a well-known analogy. ancient geometers might find surprising the investigation of geometrical figures since the time of descartes, due to its use of algebraic expressions. the use of such analytic geometry is called the ''arithmetization of geometry'', because a key step of analytic geometry is translating geometrical properties into real-number arithmetical properties."
"the corpus data was built by a snowball method from seed accounts. seed accounts spanned major media channels (@cnnbrk; @foxnews; @nbc-news; @abc) and the candidates' twitter accounts (@realdonaldtrump; @hillaryclinton). the original twitter messages from the seed accounts were not considered as a part of the data, but replies and replies to replies were. the minimal unit of the data was a paired conversation extracted from the comment section. an initial message a (single twitter message, known as a tweet) and the following reply b created a pair of the conversation."
"continue to assume hilbert's thesis through the current paragraph. it is possible that neither a c nor its negation can be proven within zf; that is, that a c is ''undecidable'' in zf. even that possibility itself is also equivalent to a halting problem. to see this, consider the modification of s zf that halts if and only if it finds a formal proof either of a c or of :a c . an example of such an undecidable sentence in zf-and hence an undecidable conjecture in rigorous mathematics-is the continuum hypothesis ch, which asserts that for every infinite subset of the set of real numbers there exists either a one-to-one correspondence between that subset and the set of natural numbers or else there exists a one-to-one correspondence between that subset and the set of real numbers."
"if the set a of all actual humans can correctly deduce correctnessrelated properties of software s, then a can correctly deduce s does not have a 's capability in the chess game."
"it is easy to see that this ''proof'' uses fallacious reasoning. if the ''proof'' supporting the claim is correct, then the claim is true and, in addition, some human (namely the writer) correctly proves that s is true, from which it follows that the claim is false, a contradiction."
"gödel's incompleteness theorem is about the incompleteness of any sufficientlypowerful, consistent, formal axiomatic system whose axioms are computably enumerable and whose rules of inference are computably applied. turing's unsolvability of the halting problem is about a limitation that any turing machine has for solving all instances of the halting problem infallibly."
"next note that in any situation for which (h and \" p) is true, the conclusion \" q is also true, regardless of whether any entity is aware that (h and \" p) is in fact true. the nebulous concept of ''awareness'' is factored out of the comprehensibility theorem. this is, of course, the case for customary applications of mathematics. one can apply a theorem about differential equations to the astronomy of an unobservable region of the universe: the (h and \" p) in such an application can be true, and consequently the \" q can be true, regardless of whether any entity is aware of the truth value of (h and \" p). we now examine more closely the restrictive rule that prohibits a from receiving any input during the time interval between when it first receives the input n and when it subsequently produces its two official outputs corresponding to n. although some real-world situations might permit this rule to be enforced in a computable way, that is not actually required. outputs that violate the restrictive rule simply fail to be official outputs, and this is the case regardless of whether any entity is aware whether or not any particular output is official. as an example, here is a paraphrase of the application of part (1) of the definition of ''correctly deduce'' to a specific a, s, and p: if, upon receiving its first input of hs; p; correct?i and before receiving any further inputs the real-world agent a produces the output hs; p; correct?i and its first such output is immediately followed by an output that is the code for the particular kind of list of codes specified in part (1), then in performing that action aðhs; p; correct?iþ ''yields a formal proof in f either of cðs; pþ or of :cðs; pþ''."
"it was essential to use formal proofs as objects within the model, since one must use self-reference within proofs with extreme care as explained in our section ''the inherent lack of rigor in the preceding ''proof''''. now that there is a proof of the comprehensibility theorem satisfying the current standard criterion for rigor, one can carefully apply it non-formally without risking paradoxical results."
"it is important here to mention that, although the proof of the comprehensibility theorem itself is formalizable in principle in zf, it does not assume that all proofs must be formalizable in zf. this article aims to be as broadly applicable and as independent of points-of-view as feasible: we do not assert that assuming hilbert's thesis is the only reasonable approach, that the only reasonable formalization of a problem must be within zf, or even that all acceptable mathematics must be formalizable in principle within a system whose formal theorems are computably enumerable; see es9."
"notice that here we are not using such conventions as they are routinely used when employing abstract mathematical turing machines to prove theorems in the theory of computation. instead, here we are pointing out that a particular generallyaccepted assumption of turing (which we adopt for convenience) justifies the existence in the real-world of a physical system that receives a given natural number from a fifo input queue and permits the recording of a corresponding official output so there is a fifo output queue."
"regardless of whether one thinks a highly intelligent, infallible agent can or cannot exist, it is advantageous-when possible-for a theorem to be independent of that issue, since such a theorem is a more general result. the comprehensibility theorem achieves that kind of independence."
"this study adapts the word-based hierarchical alignment model (wham; ) to estimate alignment on different word categories in the twitter conversations. wham defines two key quantities: baseline word use, the rate at which someone uses a given word category w when it has not been used in the preceding message, and alignment, the relative increase in the probability of words from w being used when the preceding message used a word from w . both quantities have been argued to be psychologically meaningful, with baseline usage reflecting internalization of in-group identity, homophily, and enculturation, and alignment reflecting a willingness to adjust one's own behavior to fit another's expectations and framing [cit] ."
"the s mentioned in the statement of the comprehensibility theorem is a turing machine. that is appropriate since the turing machine concept has a simple mathematical definition, yet captures the input/output capability of partial recursive functions of natural numbers. as a result, the turing machine concept captures the full input/output functional capability-for natural numbers, which as pointed out in our next paragraph is no fundamental restriction-of any current concept of software. the domain, dðpþ, of turing machine p is the set of natural numbers n such that p halts when executed using n as its input; that is, when the contents of the initial tape for p is the unary code for n and the read head of p is over the leftmost position in that unary code. corresponding to each turing machine p is a natural number code #p, such that the description of p can be computationally determined by decoding #p."
"obtaining a proof of a result like the comprehensibility theorem, regarding the ability of agents to make certain deductions about software, requires constructing an acceptable mathematical model. even if the goal is to convey just the central ideas to a broad audience, it is not possible to sketch the above intuitive idea behind such a proof and leave it at that, since that intuitive idea relies on fallacious logic."
"accommodation in communication happens at many levels, from mimicking a conversation partner's paralinguistic features to choosing which language to use in multilingual societies [cit] . one established approach to assess accommodation in linguistic representation is to look at the usage of function word categories, such as pronouns, prepositions, and articles [cit] . this approach argues that function words provide the syntactic structure, which can vary somewhat independently of the content words being used. speakers can express the same thought through different speech styles and reflect their own personality, identity, and emotions [cit] ."
"one important property shared by the comprehensibility theorem and those results of gödel and turing is that each is a mathematical theorem satisfying the current criterion for rigor: each is provable in principle in zermelo-frankel set theory, rather than merely being the result of a non-rigorous argument. that is unusually important because the proof of each uses some form of self-reference. non-rigorous reasoning using self-reference can easily produce self-contradictory results. (see ''the inherent lack of rigor in the preceding ''proof''''.)"
"the alignment values were mostly small positive values, much as has been observed in stylistic alignment in previous work. however, crossgroup trump-clinton conversations did have negative alignment on cognitive processes. this category spans markers of certainty, discrepancy, and inclusion, and has been argued to reflect argumentation framing that appeals to rationality. this may be a sign of rejecting or dismissing their interlocutors' argument framing. but overall, there is no strong evidence of differences in alignment in argumentative style in this data, and the bulk of the effect remains on group identification."
"roughly speaking, saying ''a can correctly deduce correctness-related properties of s'' means a can yield certain proofs in a formal system f, concerning s's ability to solve halting problems, and saying ''s does not have a's capability for solving halting problems'' means there is a halting problem that a settles correctly and deductively but s does not settle correctly. the formal system f is required to be ''adequate''. questions related to halting problems as well as attempts at formal proofs are coded as natural numbers, and an agent in this context is represented by a function a from a set of natural numbers to the set of natural numbers."
"it is not possible to use the simple counterexample-seeking approach of this section to view all mathematical conjectures as halting problems. consider, for instance, the twin prime conjecture: there are infinitely many pairs of primes, such as 3 and 5, that are successive odd integers. (the simple approach does not suffice because the twin prime conjecture is not a p 0 1 problem; see es9.)"
"we now examine more closely what it means for some of the assumptions mentioned above to be satisfied in real-world situations. first note that the definition of a mathematical agent used in the theorem is an abstract function, so it is only necessary to show how to obtain a mathematical function-rather than a computable function-from a real-world situation."
"before leaving the above question, we should point out that the assertions of turing and minsky above were made in the context of their strong criticism of some arguments whose goal was to show that software-based human-level ai is impossible. that goal is not the purpose, and is not achieved by, the comprehensibility theorem. halting problem without requiring the correctness of that decision; that is, without requiring the truth (according to n ) of the formula that indicates that decision. the model does not require that a proof attempt by an agent must be logically correct. the model does not assume that an agent infallibly knows when it has given a correct decision or made a logically correct proof attempt. the model's definition of ''correctly deduce'' and the proof of the comprehensibility theorem are based on the lemma in ''rigorous analogues for both check and checkmate'', which does not assume that even 1 % of a turing machine's decisions about halting questions are correct. the model's definition of ''s has a's capability for solving halting problems'' could be satisfied even if s makes incorrect decisions about infinitely many halting problems. the fact that a real-world agent's only response that counts is its first official response (see section ''applying the comprehensibility theorem to real-world agents'' and es1) permits a real-world agent to make blatantly contradictory decisions about the halting of a specific turing machine, as is also the case for other questions given to the agent. our section ''applying the comprehensibility theorem to real-world agents'' also explains that the model does not assume an agent decodes an input number flawlessly; there is an algorithm for the decoding, but the model does not assume the agent carries out that algorithm flawlessly."
"proof: because of the meaning of s, to prove the claim it suffices to show that s is true. but that is clear, since if s were false, then ''some human can correctly prove that s is true'' would hold, which would imply that s is true and that would result in a contradiction. h"
"1. s prints ''input halts'', given input checkmate, and that is an incorrect decision. by the correct deduction assumption, it is possible for a to write a correct proof of that deduction. it is then possible for a to follow up that correct proof with the decision ''input does not halt'', which is correct. 2. s prints ''input does not halt'', given input checkmate, and that is an incorrect decision. (is similar to 1.) 3. s does not print incorrect output, given input checkmate. by the correct deduction assumption, it is possible for a to give a correct proof of exactly that fact. it is then possible for a to extend that proof with the next to last paragraph of section ''a conjecture and a nonrigorous ''proof'''', to obtain the required proof mentioned in the tie-breaking rule of fig. 1 ."
"in the halting-problem chess game of fig. 1, each player is given the text of a program p that contains no input statements. the mathematical model of this game uses a turing machine, rather than a program, for p. also, rather than having a blank input tape, the turing machine p has an input tape containing a specific pattern-denoted as hp; halt?i-which can be computationally decoded to yield p as well as an indication that the relevant problem is that of determining whether or not p halts with input hp; halt?i. the model supports presenting to an agent five different kinds of halting-related problems to solve. let us use the phrase ''the halting problem for p'' to mean the problem of determining whether or not p halts with input hp; halt?i. there are many ways the coding of hp; halt?i could have been achieved, so the choice is relatively unimportant. as explained at the beginning of ''turing machines and codes'', any correct proof provided by a player in the chess game must be modeled as a formal proof. in particular, the model requires such formal proofs to be given within a system f that extends pa and satisfies additional properties described in an upcoming section that defines the concept of an ''adequate'' formal system. halting problems are representable within pa, as now explained. it follows from standard techniques of logic [cit] lemma 7.1) that there is a formula of pa-denoted by hlt-with two free variables and these properties:"
"also, the results of gödel and turing apply only to certain kinds of infallible systems, whereas the comprehensibility theorem can be applied to systems (i.e., to agents) that are far from infallible. (see ''where does the mathematical model avoid infallibility assumptions about agents?''.) the halting problem limitation is a limitation on a turing machine that is assumed to be infallible. concerning gödel's incompleteness theorem, there is typically extreme brittleness associated with the concept of the consistency of a formal system. a standard formal system extending pa is consistent only if there is not even a single sentence a in the system such that both a and its negation :a are formal theorems. moreover, such a system is trivial in a typical formal system if that system is inconsistent, since in a typical system if a and :a are formal theorems, then a _ b is a formal theorem where b is any sentence (regardless of whether the sentence is false) of pa and _ is the logical ''or'' operator, and it then follows from the theoremhood of :a that b is a formal theorem."
"moving on to baseline usage of we, trump supporters were most likely to use this pronoun, especially in their in-group conversations, suggesting a strong awareness of and desire for group identity. contrary to the alignment patterns of they, clin- ton supporters were actively diverging their usage of we from trump supporters. meanwhile, trump supporters were not actively diverging on we as they did for the they usage."
"it is the role that t plays in part (4) that permits the proof of the comprehensibility theorem to avoid the fallacious kind of self-reference identified in our section ''the inherent lack of rigor in the preceding ''proof''''. also, when an agent a satisfying the above definition makes a deduction, the axioms in t are there to help. thus a need not derive-from the axioms of pa-the formulas of f contained in t, which are true according to n regardless of the specific s. this is analogous to the fact that a need not derive-from more primitive axioms than those of pa-the first-order induction formulas, which are axioms of pa and which are true according to n . the fact that the members of t fail to be axioms of pa merely reflects the needs of the usual applications of pa."
"as indicated above, the theorem is specifically related to solving halting problems. how significant are halting problems? one can obtain insight by considering halting problems in mathematics and in logic."
"however, it is also reasonable to expect that speakers with enmity would diverge their speech style as a way to express their disagreement to each other, especially if they feel disrespected or slighted [cit] . at the same time, if the function word usage can reflect speakers' psychological state [cit], then negative alignment to opponents would be observed as a fair representation of the disagreement between speakers. [cit] showed that accommodation in word usage could be a feature to improve their model detecting agreement and disagreement between speakers."
"one can apply the theorem to a, regardless of the real-world systems constituting a. if a fails to satisfy the \" p for a theorem, that theorem holds by default for a. (some practical agents that satisfy the hypothesis of the theorem are considered in es3.) but one must interpret the theorem with care. for example, let h [ c consist of a specific set of humans and a specific computer, where c could assist in the-not necessarily infallible-decoding and encoding, it could facilitate developing formal proofs from nonformal proofs, and it could check cases that arise within a proof, like the extensive computerized case-checking in the first proof of the four color theorem [cit], which solves a halting problem as pointed out in ''some results and conjectures that are halting problems''. if a h[c is used in the hypothesis of the theorem, it (and not a h ) must be used in the conclusion of that application as well. the comprehensibility theorem gives: ''if a h[c can correctly deduce correctness-related properties of s, then a h[c can correctly deduce that s does not have a h[c 's capability for solving halting problems''."
"the model assumes that peano arithmetic, pa, is sound under the standard interpretation n of the symbols of pa, an assumption widely-accepted among mathematicians and computer scientists. (the relevant definition of sound is in our section ''gödel's second incompleteness theorem is a halting problem result''.) moreover, the comprehensibility theorem requires an agent to provide its deductions using an ''adequate'' formal system f, and one required property of such a system is that each sentence of pa that is a formal theorem in f must be true under the interpretation n ."
"this section further explains the importance of halting problems, by indicating how one can view as a halting problem result gödel's second incompleteness theorem, which implies gödel's incompleteness theorem and is thus perhaps the most significant theorem of logic. we begin with a brief review of peano arithmetic, which is used throughout the rest of the article."
"the theorem's statement uses the preceding mathematical definitions plus the phrase ''a can correctly deduce that s does not have a's capability for solving halting problems''. that phrase is roughly defined to mean there is an instance of the chess game for which a can make (and prove the correctness of) a correct decision and also can give a correct proof that s cannot make a correct decision. more precisely, that phrase is defined mathematically to mean there exists a turing machine p such that aðhp; halt?iþ yields a formal proof of the true one of hðpþ and :hðpþ, and in addition, such that aðhs; p; breaktie?iþ yields a formal proof of bðs; pþ."
"however, when the group identities are strong and unambiguous, this inference can be excessive, and may even lead to inaccurate estimates, as the more complex optimization process may create a non-convex learning problem. the bayesian hierarchy in wham also aggregates information across groups to improve alignment estimates; in cases where the groups are opposed, one group's behavior may not be predictive of the other's. we propose the simplified word-based alignment model (swam) for such cases, where group dynamics are expected to provide robust and possibly distinct signals."
"the conditions and are logical expressions over program variables and constants, which are constructed by using comparison operators, logical and arithmetic operators and the pseudo-operator \"_\". by definition, the pseudo-operator can be applied to variables only. the expression describes situations, when changing the value of the variable v is needed (if it is allowed by the condition ). the expression is built using variables and constants, comparison, logical and arithmetic operators and the pseudo-operator \"_\"."
"cf un(g, x) takes two parameters, of which g is a function receiving two 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 parameters and x is a variable with the same type as the first parameter of g."
"in the above description, the stopping criterion is the total number of iterations (nbiter), but it is easy to adapt the algorithm so as to use an alternative stopping criterion, such as the total computing time."
"because an acquaintance link between two peers may not follow the interaction graph of variables, we need to adapt definition 1 for the distributed context."
"since the intel xeon phi technology featuring many integrated cores (mic) is very new, the repository of scientific 1536-1241 © 2015 ieee. translations and content mining are permitted for academic research only. personal use is also permitted, but republication/ redistribution requires ieee permission. see http://www.ieee.org/publications_standards/publications/rights/index.html for more information."
the ltl-formula for deactivation of the variable (leaving a state ) with a transition rule without loops (only transitions to another states) has a simple form:
"for the description of all possible increasing value situations, formula (1) can have several sets of considered conjunctive parts combined in a disjunction, after operator . expressions, and have the similar meaning. more simple ltl formulas are proposed to use in case of a logical (binary) data type variable:"
"the set of protein-ligand complexes used in this study is a subset of the original findsite benchmarking dataset [cit] . it consists of proteins of different sizes and a varying number of associated ligand-bound templates, selected to mimic real proteomic data. as shown intable i, we first defined 12 bins depending on the number of amino acids in the target protein (200-500) and the number of templates (50-250). next, we populated each bin with up to 50 structures by randomly selecting proteins from the original findsite dataset. for each protein, its crystal structure is used as the target; weakly homologous templates sharing less than 40% sequence identity are identified by meta-threading using thread [cit] . the benchmarking dataset used in this study comprises 501 protein targets."
this article continues a series of papers [cit] dedicated to developing an approach to construction and verification of discrete plc-programs by ltl-specification. this approach provides a possibility of using model checking method for correctness analysis [cit] .we use the linear temporal logic for specification of program behavior and smv tool for verification [cit] .
"final stage: re-connecting all sub-jointrees when the above algorithm finishes, it ends with a forest of sub-jointrees, distributed on the network. each node has its own main cluster, fixed when the node was removed, but all the sub-jointrees have to be merged in one jointree, by reconnecting them while respecting the running intersection. the first peer that detects the termination broadcasts the reconnection message in the network (line 20). then, (not shown here), each non-leaf peer p that receives this message initiates the flooding algorithm 2: each p broadcasts a new message reqcon to the network with the set of its children. when receiving a message reqcon for the first time, a peer p records the sender p as a father link and broadcasts the message to all its neighbourhood (excluding p ). of course, due to the flooding mechanism, the message reqcon visits all links of the network (lines 4 to 6). now, when a peer has received the reqcon message from all its neighbours (line 10), it initiates the answer by sending back the message anscon with the list of children of the initial peer p (noted w anted in the algorithm, lines 12 to 14). the message then goes bottom-up in the tree, following all the fathers links created above. now, each"
"where the logical constant \"true\" means that the variable must be turned to zero on the next working cycle after its activation. despite that the implication in ltl-formula is the tautology, this formula construction makes sense. firing condition in the form of \"true\" constant is involved in an implicit ltl-formula describing situations in which a variable keeps its value after one execution of the program. the formula is shown below. it forbids the variable to have value 1 more than one working cycle of plc program:"
"thus far, we demonstrated that the parallel versions of findsite, including that accelerated by the intel xeon phi coprocessor, offer an improved performance, however, the results must also be technically correct. fig. 11 assesses the accuracy of pocket prediction by the code described in this paper, labeled as 1.2, compared to the original version of findsite [cit], labeled as 1.0. all three 1.2 versions, serial, parallel on the host, and parallel on the target, produce identical results demonstrating that the implementation of findsite using openmp and the accelerator maintains the functionality of the code providing shorter simulation times. version 1.2 is slightly more accurate than 1.0 because a couple of coding bugs in 1.0 have been identified and fixed during the porting process. for instance, for 65.7% and 84.3% of benchmarking proteins the best of top three ligand-binding sites is predicted within 4 å and 8 å, respectively. this high prediction accuracy is accompanied by high ranking capabilities; the best binding site is at rank 1 in 69.0% of the cases. we note that closely related templates with 40% sequence identity to the target are excluded in our benchmarking calculation, thus quantitatively similar results can be expected for real applications of findsite in function annotation at the level of entire proteomes."
"for languages il, fbd/cfc and ld we will note that the realization of the machine in these languages reduces mostly to rewriting if-elsif blocks and a pseudo-operator section."
"after the best move is chosen, it should be applied to the current configuration. applying the move involves making changes to the network and reassigning the routes for some traffic demands. as explained in section 3.1, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 the grooming solution is obtained by solving the mixed integer programming problem. although the model is tractable, it is still time consuming to solve it in each iteration."
the ts kndg procedure has two input parameters: the initial configuration x 0 and the number of iterations nbiter. we denote by k the number of lightpaths. 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 the goal of the procedure is to explore the set of configurations by applying a series of moves so as to discover a configuration of minimum cost.
"for a neighborhood search method, it is particularly important to be able to rapidly evaluate the neighborhood. as described in section 3.2.5, a relaxed linear programming model is used to estimate the effects of the twist operator. two experiments were carried out to analyze this evaluation technique on some representative instances, which are ndg20 t100.1, ndg20 t200.1, ndg20 t300.1, ndg40 t200.1, ndg40 t200.5and ndg40 t400. note that similar results can be observed on other instances. the time limit for each run was set to 4 hours. one can observe that for most cases, the linear programming model correctly evaluates the actual cost of the configuration. the plots almost coincide in most of the cases, especially for some easy instances like ndg20 t100.1 and ndg40 t200.5. for other instances we can see that the gap between the original problem and the relaxation is very narrow. these statistics indicate that our proposed evaluation technique works rather well with the twist operator."
"showed speedups ranging from 5 to 32 on intel xeon phi accelerators [cit] . thus, our results are fairly comparable to those reported by other developers."
"proof. consider an arbitrary minsky counter machine . match every counter and every state of the machine an unsigned integer variable and a boolean variable, respectively. in this case all state variables, except, are initialized by zero, and is set to 1 (true). counter variables are initialized by values of the corresponding counters of the machine . realization of the minsky machine is implemented as a program in the plc programming language. a transition from one configuration of the machine to another will correspond to an execution of the program during one plc working cycle."
"next, we benchmark the performance of findsite using the intel xeon phi coprocessor. we note that the pre-and post-processing steps are executed sequentially on the host, whereas only structure alignments are offloaded to the accelerator and processed in parallel using multiple target threads. moreover, we measure the performance up to 24 threads because there is not enough parallelism to fully utilize all target threads for one protein target, considering that one thread processes one template and that some proteins in the benchmarking dataset have only 50 templates. later on, to maximize the utilization of resources in production runs, we will launch multiple parallel tasks simultaneously processing multiple proteins. specifically, 10 tasks, each processing one target protein using 24 threads, will be launched in parallel to fully utilize 240 hardware threads on the accelerator."
"before start analyzing the performance of findsite, we need to develop an evaluation metric for speed measurements. intuitive metrics are the target protein size and the number of templates to be structurally aligned. indeed, as demonstrated for 501 benchmarking proteins in fig. 5(a), increasing both quantities results in a longer time required by findsite to predict ligand-binding sites. the execution time is measured for the total wall time (dark gray circles) and the time spent calculating structure alignments (light gray triangles). the corresponding pearson correlation coefficients are 0.809 and 0.781, respectively. to search for a better performance evaluation metric, in fig. 5(b), we plot the execution time against the number of single rmsd calculations realized by the subroutine, which is the most frequently called function in the process of computing structure alignments. the correlation with the total wall time and the alignment time improves to 0.963 and 0.960, respectively. this analysis suggests that the number of rmsd calculations per second provides the best performance measure for evaluating the speed of findsite. we note that the target protein size and the number of templates are known a priori, before the simulations start, whereas the number of rmsd calculations can be obtained only after the simulations are completed because it depends on the convergence of structure alignments (templates that are structurally similar to the target converge faster than those less similar)."
"in order to build a distributed jointree (djt) at the end of the algorithm, we consider the first removed peers as the leaves of the djt. the final djt is built bottom-up to the root. this is done when all the peers have been eliminated. the algorithm has to connect all the sub-jointrees together. because we do not want to allow any new link in the network, while ensuring the running intersection, this stage is a little bit tricky. each peer asks the network for its son by flooding its name and its projection, and when the son answers, all peers on the path between them create new \"connection\" clusters with all the variables needed for ensuring the running intersection property in the final djt. so, we have the following properties: first, during the whole algorithm, each cluster is only in one peer. second, before this last reconnection stage, each peer can only contains one cluster (called the \"main\" cluster). third, at the end of the algorithm, each peer contains exactly one \"main\" cluster, and an unbounded number of \"connection\" clusters."
"in this section, we explain how the exact algorithm for the grooming subproblem is embedded into a local search algorithm. the local search algorithm is considered as the master algorithm while the mip and lp procedures are used as slave procedures integrated into the local search."
"the idea of the algorithm is the following. each peer votes for its best neighbour (including itself). it has to decide, locally, which peer should be removed first. a peer that has been chosen by all its neighbours can be removed (it is a sink node in the graph of \"best\" choices). when removed, a peer builds a cluster of variables for the distributed jointree, and memorizes it locally. this peer will be the root of a sub-jointree in the final jointree. all the variables that are not local to this sub-tree are in its projection (exactly in the same sense as it is used in be, see 2.3). a removed peer remains active, and participates to the elections until all its neighbourhood has also been removed (after that, it will only be active for routing token messages)."
"we show the jointree built by distributed be with ddfs and the maximum cardinality set (msc) heuristic in figure 4. in the latter, privacy is preserved. one may notice here that variables are shared among peers if needed. the notion of projection (pjct pi in the above algorithm) will also appear in our algorithm. this notion is essential for ensuring the running intersection property in the final jointree."
"findsite predicted the total number of 22 pockets for this protein and assigned a confidence of 91.1% to the top-ranked binding site. fig. 12 shows the crystal structure of the target protein with the top three binding pockets represented by balls; the yellow ball corresponds to the top-ranked pocket, whereas the two red balls show the location of pockets at ranks 2 and 3. in addition to the pocket center, orange sticks and the transparent yellow surface depict residues predicted to bind a ligand. the structure displayed in fig. 12 also contains a -lactam antibiotic bound to this enzyme; we note that this compound is used only to assess the prediction accuracy and it was not included in findsite simulations. the distance between the predicted top-ranked binding site and the geometric center of the antibiotic is only 2.4 å with as many as 63% binding residues correctly identified. this case study illustrates that findsite is a reliable tool for ligand-binding prediction, which has a broad range of applications in protein function annotation, virtual screening and drug discovery."
"..] represents the vector of overloaded lightpaths and n (l o 1 ) represents the vector of lightpaths that is neighbored to l o 1 . here, move is a function that takes two parameters l 1 and l 2, and produces a twist move that will twist lightpath l 2 from one endpoint of l 1 to another endpoint of l 1 ."
"about concurrency, it is trivial to say that centralized methods are not concurrent. however, we think that the non-concurrency property is essential to obtain good jointrees. let us consider a simple path v 1 − ... − v n . one can check that the sequential elimination of one or two variables (i.e. (v 1, v n ) ) at ends of the path will lead to the creation of clusters with at most two variables. unfortunately, the concurrent eliminations of at least three variables at the same time will lead to the creation of a cluster of at least three variables. this example can easily be generalized for tree structured graph hal-00790112, version 1 -19 [cit] or cyclic graph, and clearly illustrates the fact that even few concurrent decisions can get any algorithm away from the optimal decomposition. in order to prevent those bad decisions to be taken, we propose to use a token to limit concurrent choices."
"to induce cellular and therapeutic effects, small ligand molecules such as metabolites and drugs bind to specific sites on the target protein surface, often referred to as binding pockets. however, for many pharmacologically relevant proteins only their ligand-free experimental structures or computer-generated models are available. therefore, at the outset of drug discovery, the identification of possible binding sites is typically the first step in inferring and modulating protein molecular function. currently, evolution/structure-based approaches to ligand-binding prediction are the most accurate and, consequently, the most widely used [cit] . these algorithms exploit structural information extracted from evolutionarily weakly related proteins, called templates, to predict ligand-binding sites and binding residues in target proteins. findsite is a recently developed evolution/structure-based approach [cit], which features a series of improvements over its predecessor, findsite [cit] . it employs a collection of new algorithms, such as highly sensitive meta-threading methods, advanced clustering techniques, and machine learning models to further increase the accuracy of pocket detection as well as to improve the tolerance to structural imperfections in protein models. consequently, it is especially applicable in genome-wide protein function annotation projects. for a given target protein, findsite uses meta-threading [cit] to identify closely and remotely homologous ligand-bound templates. subsequently, these templates are structurally aligned onto the target using fr-tm-align [cit], which is followed by the clustering of template-bound ligands using affinity propagation [cit] and a machine learning-based ranking of the detected pockets. the framework of findsite is written in with the protein structure alignment portion implemented in fortran77 and the affinity propagation clustering algorithm incorporated into the code as a library pre-compiled for linux systems. previous profiling of the findsite code revealed that, depending on the target protein length as well as the number of templates, the memory requirements could be fairly high [cit] . therefore, we decided to use the offload mode in porting findsite to the accelerator."
"the action of a step x1step corresponds to the program implementation of ltl-formulas of counter variable behavior specification. for instance, the action of this step is if-elsif construction in the st language. in a similar way, actions of steps x2step, …, xmstep correspond to program implementations of ltl-formulas of counter variables, …, behavior specification, respectively. the action of a step psvdstep is a pseudo-operator section, because this step will execute last within one pass of the working cycle."
"fortunately, the gap between the continuous grooming model and the one with binary variables is rather small (as we will see in section 4.4). thus it is possible to solve the linear programming relaxation at each iteration to get the estimated overload information when f is greater than zero, and solve the integer program when f equals zero to verify wether the network can satisfy all the traffic demands."
"the score of a peer is the estimated size of its main cluster (the algorithm tries to choose the next peer with minimal score). in addition, each peer maintains an array score[] of the \"cluster\" score of its neighbours. this \"cluster\" score takes into account the current projections of orphan sub-jointrees. more generally, this is where the heuristics take place."
"local elections each time a peer p token receives (or initially has) the token (line 2), it organizes local elections (line 4) and waits for the votes of all its neighbours (including removed ones). the reception of this message is treated in line 31. the computed score of p takes into account the size of the set of variables made up from current projections in the token (set of non local variables of sub-jointrees built so far), local variables and shared variables with ancestors. the underlying idea of this score is to estimate the size of the cluster if p, the peer that received the elec message (line 31), would be removed at this point. the computed score of the peer p is sent back to p token but also to all its other neighbours, line 33, and received line 37. so, if we get back to the initial peer p token that organized the local election, it has updated its own score[] values for all its neighbours, taking into account the current open projections in the token, and the local election is closed."
"when the problem is tree-decomposed, its complexity can be bounded by an exponent of its width, which is the size of the largest cluster in the tree (minus 1). many applications rely on good tree decompositions, and many polynomial classes are based on the existence of a good decomposition. because of its exponential impact on the bound, even a small improvement in the quality of the decomposition may lead to large improvements in practice."
"the remainder of the paper is organized as follows. a new mathematical formulation of the problem is presented in section 2. the framework of the proposed matheuristic is then presented in section 3, while the formulation used in the slave algorithms is introduced in section 3.1. the neighborhood structure of the master problem is described in section 3.2. the proposed algorithm is detailed in section 3.3. experimental results and the analysis are described in section 4, before concluding the paper in section 5. 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 2. problem definition and mathematical formulation"
"according to the approach to construction and verification of programs by ltl-specification, the proof of the theorem consists of two steps. the first step is the building of an ltl-specification of minsky counter machine the second step is the construction of a plc program by this specification in various programming languages that implements the behavior of the machine first step. we will write down the required behavior of each program variable with two ltl-formulas. we begin with describing counter variables behavior. the following ltl-formula is used for describing situations, leading to the increase of the value of the counter variable :"
"let us build the ltl-specification of 3-counter minsky machine of squaring . a counter variable is initialized by value in this specification. the state variable is initialized by 1, i. e. initially . other variables (including pseudo-operator variables) are set equal to zero."
"codes is still relatively limited. however, the availability of mic accelerators installed in many contemporary supercomputing systems, e.g., stampede at the texas advanced computing center (tacc), stimulates a broad interest in this new architecture. consequently, significant efforts are directed towards developing new and porting existing state-of-the-art codes to xeon phi as well as exploring the performance and functionality of the accelerator [cit] . in this spirit, we describe a new version of the findsite algorithm, a modeling tool used in structural bioinformatics and drug discovery, that can be deployed on computing nodes equipped with xeon phi cards. findsite was developed to accurately identify ligand-binding sites and binding residues across large datasets of protein targets using weakly homologous templates [cit] . devised to efficiently operate within the \"twilight zone\" of sequence similarity [cit], it is especially applicable to genome-wide protein function annotation, drug design, and systems biology in general [cit] . briefly, findsite extracts ligand-binding information from evolutionarily related proteins identified in the protein data bank (pdb) [cit] using sensitive protein meta-threading techniques [cit] . template proteins are structurally aligned onto the target protein, which is followed by the clustering of template-bound ligands to detect ligand-binding sites and residues. these predictions can be subsequently used e.g., in ligand docking and virtual screening [cit], molecular function inference [cit] as well as in the reconstruction and analysis of biological networks and pathways [cit] . in the original findsite algorithm, template-to-target structure alignments are executed sequentially, thus the identification of ligand-binding sites even for one target protein may require many cpu hours [cit] . this in turn complicates genome-wide applications, where the number of protein targets and the size of the template library can be very large. therefore, accelerating the findsite code shortens the simulation time resulting in faster genome-wide protein function annotation. in this article, we describe porting findsite to the intel xeon phi platform and demonstrate that its parallel execution on nodes equipped with accelerator cards significantly reduces computational time required for the identification of ligand-binding sites across large protein datasets."
"consider a realization of machine in a graphical language sfc. we will use a simplified sfc, available in the programming tool codesys [cit] . in the simplified sfc every step can be associated with three types of actions -step action, entry and exit. if the action of a step is implemented, a small triangle appears in the upper right corner of the step. while a step is active, its action will be executed once on every working cycle. deactivation of a step is carried out when its tansition conditions are true. the entry action is indicated by an \"e\" in the lower left corner and is executed only once, right after the step has become active. the exit action is indicated by an \"x\" in the lower right corner and is executed only once before the step is deactivated (but on the next working cycle before the activation of the next step). every action and transition condition can be implemented in any iec 61131-3 language."
"programming languages for logic controllers are defined by the iec 61131-3 standard. this standard includes the description of five languages: sfc, il, st, ld and fbd. il (instruction list) is an assembly language with an accumulator and jumps to labels. st (structured text) is a high-level programming language. its syntax is the adapted pascal syntax. ld (ladder diagram) represents a program by a graphical diagram based on circuit diagrams of relay logic hardware. fbd (function block diagram) is a graphical language of circuit diagrams of electronic devices on microcircuits. a variation of fbd is the cfc language (continuous function chart), that allows to place components and connections arbitrarily. sfc (sequential function chart) diagrams are a high-level graphic tool. they consist of steps and transitions between them that divide tasks into simple phases with a formally defined logic of a system operation."
"each accelerator card deploys a stripped down linux os called busybox and removes many power-hungry operations, yet it offers a wider vector unit and a larger hardware thread count compared to host processors. each 61-core coprocessor supports 4 hardware threads per core providing up to 244 threads, whereas the two host processors feature 8-cores each and 2 threads per core totaling up to 32 threads. however, to maximize the performance, only 16 host threads are available on stampede nodes (1 thread per physical core). intel xeon phi also has some other unique features; each computing core clocks at 1090 mhz and the device is equipped with 8 gb of gddr5 memory and four-way simultaneous multi-threading (smt). its 512-bit wide single instruction, multiple data (simd) vectors translate into 8 double-precision or 16 single-precision floating-point numbers."
"\" for a variable corresponds to an unconditional increment of a counter, and \" \" is used to denote the conditional decrement with the transition to another state on a right-handed arrow in case of zero value of the counter ."
"the result of a tree decomposition is also called a jointree. figure 2 represents the tree decomposition of the interaction graph of figure 1 . in this figure, one can check that each node labelled by a variable in the interaction graph belongs to at least one cluster of the tree-decomposition. in addition, we can notice that clusters ct 2, ct 5, ct 7, ct 6, ct 3 that contain l1 are connected in the tree. then, the running intersection property is satisfied for l1. it is easy to check that the running intersection is satisfied for all variables. the framework is made up of peers, where each peer knows a subset of formulae and can interact with its neighbourhood by messages in order to solve a global problem. in that case we need to consider at the same time the network interactions between peers defined by peers' acquaintances and the semantic interactions between variables defined by formulae (from a global point of view). in our approach, we model a distributed setting of peers and formulae by the notion of acquaintance graph."
"for the first type state of the machine and for the final state we have (for transition rules without loop, with loop and for the final state, respectively):"
this algorithm uses a breadth-first search strategy and a cutoff mechanism when there is no hope to find a better route. these components reduce redundant calculations when the candidate routes for a given traffic demand share common sub-paths.
"of course we can just calculate the cost function f (x ⊕ m) of the resulting 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 solution to see whether the move is good or not. however, computing time would become prohibitive. different mechanisms have been proposed in the literature for neighborhood evaluation in a local search algorithm. the fast incremental evaluation is one of them (see, e.g., [cit] ). it calculates the score of m, i.e. its impact on the cost of the configuration. then the move with the least score will be considered to be the best move in the case of a minimization problem. however, the fast incremental evaluation is usually effective only for problems with simple solution structures. in ndgp, the solution structure consists of the topology of the network and the routes for each traffic. it is hard to design a fast evaluation mechanism for the neighborhood introduced before."
"the above results must be nuanced by the cost of the distributed compilation itself. as shown in figure 8, if dfs does not produce very good jointrees, it can produce them very quickly. be needs more time, but produces the best jointrees. the total time needed by token elimination is clearly the worst on ba graph figure, and quickly increasing. however, let us point out that for ws graph te is faster than be. given the improvement in the quality of the obtained jointrees, we strongly believe token elimination is a good solution, particularly suited to a distributed compilation approach of the network."
"the configuration of a minsky machine is a set, where -a state of the machine, -natural numbers (including zero), which are values of the corresponding counters."
"after the move operation, t2 is much likely to change its route to avoid traversing ab. if that is the case the overload will decrease. but one can also observe that t1 is likely to change its route to traverse ab. if so, the overload may be worse. usually t3 is not affected by the move. therefore, if there are more traffic demands of the type t1 than t2 affected by the move, this move may reduce the overload of the corresponding lighpath. 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 proposition 1. if x is a feasible solution for k-ndgp, then any configuration x can be altered to x by performing a finite number of twist moves."
"in this section, we detail the tabu search procedure of the proposed algorithm. tabu search (ts) was first proposed by glover [cit] and has since been applied to numerous combinatorial optimization problems. a ts algorithm typically incorporates a tabu list as a recency-based memory structure to guarantee that solutions visited within a certain span of iterations will not be revisited. the algorithm then restricts its attention to moves not forbidden by the tabu list. additional techniques frequently used in tabu search are diversification and intensification. in particular, the goal of diversification is to escape from the region currently visited by the algorithm, so as to explore new regions in the search space. the intensification is usually achieved by a local search algorithm."
"the action of a step qstep is a nested sfc-diagram, which is built from the following fragments with connection to state variables. for the second type state of the machine we have fragments (for transition rules without loops, with one loop and with two loops, respectively):"
"in this paper, an estimation method is used to evaluate each move. here, the cost function of the configurations is evaluated rather than the score of the moves. in other words, we estimate f (x ⊕ m) to check whether m is good or not. the grooming subproblem formulation described in section 3.1 is used to evaluate each neighboring configuration. however, instead of solving the mip model we solve the linear programming relaxation of this model for each configuration in the neighborhood. the configuration with minimum f is chosen to be the next solution."
"for most of the configurations which are infeasible solutions of the problem, there are usually only a few overloaded lightpaths. thus, it makes sense that we can just do operations related to the overloaded lightpaths to save time."
"however, during the local search procedure, only the information of the overload on each lightpath is needed for the evaluation. the details of the traffic grooming only make sense when there is no overload on any lightpath."
"we now explain how the evaluation works in detail. for the following illustration, we denote by θ an empty vector, i.e., the vector with zero elements, and define two auxiliary functions described as follows:"
"in this section, we present experiments performed in order to evaluate the performance of our algorithm. the matheuristic algorithm was programmed in the java language. all experiments have been executed on a linux server with a 3 ghz cpu and 94 gb of ram. cplex 12.61 was used to solve the linear programming and the integer programming subproblems inside the algorithm. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65"
"if p was its own best choice, we are in a local minimum, and we also need to escape. observing the fact that at least one of the alive neighbours is not considering p as the best choice (if p was the best choice for all its neighbourhood, then it has just been eliminated, and has sent the elim messages to its neighbourhood forcing its neighbours to vote for another peer), we know that there is a peer with a better local score than p if we follow any best link from any neighbour of p that has not voted for it. so, in our solution, we propose to use this heuristic, as shown line 29."
"centralized methods often rely on equivalent problems for tree decomposition, such as graph triangulation. in this problem, one wants to add as few edges as possible to a given graph in order to obtain a chordal graph (all cycles of length four or more have a chord, which is an edge joining two non-adjacent nodes in the cycle). the triangulation algorithm iteratively eliminates all nodes. at each iteration, (1) a variable is chosen; (2) edges are added between pairs of nodes from its neighbours that are not already connected (clique-fill); (3) the node is removed. the quality of a triangulation can be trivially measured by the number of new edges. the node chosen at step (1) must add as few edges as possible, in a short and in a long-term point of view. for instance, a node in a clique with its entire neighbours is a very good candidate. a node with a poorly connected neighbourhood is a bad choice. the heuristic min-fill-in is based on this very simple observation. each node is evaluated w.r.t. the number of new edges one may have to add to fill its neighbourhood as a clique. in order to get a global elimination order from the above algorithm, we must add a fourth step: a cluster containing all the neighbourhood of the removed variable is built (and memorized) at the new step. in this context, the min-fill-in algorithm can be viewed as trying to reduce the size of the clusters. however, when adapting this idea to the distributed case, it may not be possible to add links to any pair of peers. in our formulation, the acquaintance graph is given, and no network link can be added \"on the fly\"."
"we implemented a java-based simulator of a distributed system for analysing the performance of our distributed token elimination algorithm. we used a generator of smallworld networks, following the model generation of barabassi and albert [cit] (called ba graphs) and watts and strogatz [cit] (called ws graphs). let us point out that ba graphs are extremely heterogeneous. the degree of nodes follows a power law, a strong characteristic of many real world problems (world wide web, email exchanges, scientific citations, ...) [cit] . ws graphs are more homogeneous and have been used for modelling diagnosis circuits [cit] or various csp instances [cit] . on this kind of networks, we were able to scale up to more than thousands of variables. before analyzing the results, let us also point out that we do not consider the privacy characteristics of our algorithms in this experiment: all variables are shared; and we consider that each peer has exactly one and only one variable for simplicity. however, as shown in the algorithm, line 12, private variables are treated apart. they are automatically removed from any projection the peer can send away. thus, our experiment does not necessarily have to take privacy into account. we report in figure 7 the quality of the jointrees produced by our algorithm, distributed token elimination, with a set of selected state of the art algorithms. bucket elimination with the minfill heuristic (be-minfill) is well known and represents the current state of the art for centralized methods. we also report two dfs variants (maximum cardinality set and no heuristic). the first one is certainly the most used in distributed systems. we report the token elimination algorithm with two variants. the first one is the mincluster (the one previously reported), and the second one minprojection (instead of scoring the peers with their potential cluster size, we score them according to the size of the set of projection variables). the reported results show a significant improvement of token elimination over dfs. the obtained results, with the mincluster variant, are now very close to be on ba graph. surprisingly, after 700 nodes te-minproj gives better decompositions than be on ws-graph. let us recall here that the jointrees built by token elimination are forced to follow the original links in the initial graph, which is not the case of be."
"all benchmarking results for different versions of findsite were obtained using stampede nodes at tacc. in fig. 6, we compare serial and multi-threaded versions executed exclusively on the host. the latter uses pragma-based openmp parallelization of the findsite code; we note that both preand post-processing steps are executed sequentially, while only structure alignments are processed in parallel using multiple host threads. increasing the number of threads for structure alignment calculations clearly improves the performance of findsite. using the total simulation time, 1 (serial) and 16 (parallel) threads give an average performance across the benchmarking dataset of and rmsd calculations per second, respectively. considering the time spent calculating structure alignments, the average performance is and rmsd calculations per second, respectively. for structure alignment calculations with 100% of the code parallelized, almost a perfect linear increase in the performance is achieved; the speedup of 16 host threads over the serial execution is 15.0. contrastingly, the performance reaches a plateau with 5.8 speedup using 16 threads when the total wall time is considered. these results demonstrate that findsite follows amdahl's law, which describes the relationship between the expected speedup of parallelized implementations of an algorithm relative to the serial algorithm [cit] . according to the profiling results showing that about 90% of the calculations are parallelized, the maximum expected improvement using 16 threads is 6.4. with the increasing number of computing threads, serial portions of the code dominate, which is represented by the plateau in fig. 6 ."
"in comparison, the host processor runs at a faster speed of 2700 mhz and has a larger memory of 32 gb ddr3, yet narrower 256-bit simd vectors."
"if the transition rule for has a loop or two loops, or is the final state, the program variable has the following \"deactivation\" ltl-formula:"
"an illustration of the final solution of our algorithm is shown in figure 5 . one can especially notice that privacy is preserved, and that clusters, thanks to connection clusters, are connected on the top of the initial peers network (see the p 2 clusters)."
"sending the token the idea of lines 24 to 29 is to let the token following the \"best\" links in the graph, and rely on the static dfs order for escaping. so, if p is not its best choice (not in a local minimum), then, if p still has alive neighbours, it sends the token to the best of its alive neighbours. otherwise, because neighbours + initially contains p itself, p has been removed with all its neighbours. in this case, we rely on the static cycle built on the top of dfs to send the token and escape."
"at this step, after line 5, the peer p token has now two possibilities, removing itself (see section 3.2), or giving the token to its best neighbour/cycle (see section 3.2)."
"in this paper, we have proposed a hybrid algorithm embedding lp and 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65"
"in the following, we first present the local search approach by defining the search space, the cost function, and the neighborhood. the procedure to tackle the k-ndgp will be described in section 3.3."
"the ultimate goal of this study is to take full advantage of the entire node equipped with the intel xeon phi card, . process the calculations using both the host and the target simultaneously. performance benchmarks carried out independently for the host and the target reveal several important results and constraints related to software, hardware and dataset characteristics. these should be taken into consideration when designing an optimized production code. first, because of amdahl's law, it is beneficial to execute multiple parallel versions of findsite on the host, each using relatively few threads. moreover, we demonstrated that using all available target threads with the compact affinity maximizes the performance of findsite on the coprocessor. therefore, our production scheme for predicting ligand-binding sites using findsite across large datasets of proteins comprises multiple tasks running simultaneously that utilize both the host and the target. specifically, we further modified the code to launch up to 4 parallel tasks on the host, each using 4 threads, and up to 10 parallel tasks on the target, each using 24 threads. fig. 8 compares the time-to-solution, defined as the total time required to predict ligand-binding sites for the entire dataset of 501 proteins, for the serial version as well as three parallel processing schemes. the time-to-solution for serial findsite, parallel findsite on the host only, the target only, and both the host and the target is 36.8, 3.1, 3.6, and 2.1 hours, respectively. fig. 9 shows that this corresponds to the speedup over the serial version of 11.8, 10.1, and 17.6 for the parallel processing using the host, the target, and both resources, respectively. therefore, using the coprocessor in addition to the main processor provides the desired acceleration in predicting binding sites for large protein datasets. it is noteworthy that at this point, the time-to-solution increases linearly with the dataset size, i.e., ligand-binding site prediction using findsite for 5000 proteins, which is a typical size of a bacterial proteome, would require 21 hours on a single stampede node equipped with the intel xeon phi card, compared to 31 hours without the accelerator and 368 hours using the serial version. in addition to the time-to-solution, we also monitor the utilization of computing resources. fig. 10 shows that the average usage of host and target cores during the production multi-task/ multi-threaded simulation is 99.9% and 82.2%, respectively. target threads periodically become idle while waiting for the host to finish pre-and post-processing steps, which results in a somewhat lower utilization of the coprocessor. on the other hand, the host remains fully utilized not only facilitating multiple tasks offloaded to the target, but also performing findsite calculations on its own. looking at the performance results presented in figs. 8 and 9, the computational capabilities of 240 coprocessor threads are fairly comparable to those of 16 host threads, thus we can expect an even partitioning of the computations between the host and the target when processing a large dataset. table ii shows that this is indeed the case; each parallel 4-thread task executed on the host performed about 12% of the total computations, whereas each parallel 24-thread task executed on the target performed about 5% of the total computations. adding up all computations performed by the host and the target gives a roughly even split of 49.2% and 50.8%, respectively. therefore, dividing the workload evenly between the target and the host will ensure the optimal utilization of computing resources in large production runs."
"in this paper, we proposed a new distributed method for building distributed jointrees. our method can handle privacy rules by keeping secrets secret (local variables stay local), and allow the jointree to be built only on the top of existing links between nodes. we show that this method can handle large structured instances and, even if the compilation cost is clearly above the simple distributed dfs algorithm, the quality of the obtained jointree clearly outperforms dfs, even with a clever heuristic, and can even surpass the quality of bucket elimination in its centralized version. we believe that our algorithm can improved previous results in many distributed applications."
"elected and removed when p token is a local minimum, i.e. all its neighbourhood, including itself, has elected it, then it has two steps to perform before being removed from the network. first, it needs to compute the new projection, and register itself as a new potential orphan sub-jointree with this projection. this is done from lines 9 to line 17 in the algorithm. p j p is the projection of p over its alive neighbourhood. children is the set of peers that p has chosen as sons: all the peers that are roots in the current orphan sub-jointrees with which p shares at least one variable (these allow one to preserve the running intersection). n ew χ is the set of variables in the projections of orphan sub-jointrees that p has to take into account when it will be removed, in addition to its own projection variables. so, for the new sub-jointree rooted in p, its projection χ p is computed in line 12. now, p has to update the set of sub-jointrees attached to the token. first, it removes its sons, then it adds itself as a new sub-jointree. at last, it is important that p tells its neighbourhood it has been removed (sending and acknowledging elim messages). during this stage, it is also important to notice that all neighbours of p update their best values (including p itself)."
"the network design and traffic grooming problem (ndgp) arises in the design of optical telecommunication networks. one of the main costs in a wavelength division multiplexing (wdm) network is the transceivers installed, which provide optical connections (edges) between pairs of nodes in the network [cit] . the optical connections in the network are called lightpaths and they are responsible for transferring traffic demands. in practice, the capacity of lightpaths is usually much larger than the bandwidth of one traffic demand. assigning an exclusive lightpath to each demand is thus costly and unjustified. since one lightpath can usually be shared by several demands, effective optimization algorithms can be developed to reduce the cost of the required transceivers."
"(1) (2) the leading underscore symbol \"_\" in the notation of the variable is taken as a pseudo-operator, allowing to refer to the previous state value of the variable v. this pseudo-operator can be used only under the scope of the temporal operator ."
"the flowchart of the findsite algorithm is shown in fig. 1 . the input includes a target protein structure and a ligand-bound template library. the output contains predicted ligand-binding sites and residues, as well as structure alignments between the target protein and the associated templates. the procedure of binding site prediction in findsite breaks down into three consecutive stages. during pre-processing, template information is extracted from the library and sequence alignments to the target are constructed. next, each template is structurally aligned onto the target; based on the code profiling results shown in section iii-a, we identify this portion of the code as a promising target for parallelization and porting to the accelerator. specifically, structure alignments of a target protein against the identified templates are implemented in parallel with each templateto-target alignment computed by a different hardware thread. pragma-based openmp is used to parallelize structure alignments and to perform the calculations within the host, offload them to the target, or use both resources simultaneously. as shown in fig. 1, each individual alignment is assigned to a different thread to carry out calculations in parallel. finally, template-to-target structure alignments are collected and used in the post-processing step to cluster the identified pockets, rank them using machine learning, and predict the corresponding binding residues."
"additionally, as reported section 2.4, we need to carefully handle concurrent decisions. for this, a single token is circulating on the network, following the current votes. because the directed graph of \"best\" choices is not guaranteed to be strongly connected, we must add an escape strategy. for this, we suppose a cycle that goes through all nodes exactly once. in our approach, we build this cycle by a dfs traversal of the peers."
"to conclude, we present a case study illustrating binding pocket prediction using findsite. our target selected from the benchmarking dataset is penicillin-binding protein from pseudomonas aeruginosa, pbp3 (pdb-id: 3pbr, chain a), a critical enzyme responsible for peptidoglycan synthesis [cit] ."
"second, programming models available for the intel xeon phi architecture are open-standard and portable between traditional processors and coprocessors; moreover, openmp makes it relatively easy to execute programs in parallel. findsite represents a piece of typical scientific software written mostly by domain scientists, who contributed different components to the code using different programming languages and styles. with minimal modifications, a complex, hybrid /fortran77 code was successfully ported to the coprocessor yielding satisfactory speedups. this illustrates that the process of modifying scientific software to benefit from the intel xeon phi architecture is relatively straightforward with quite short development times. nonetheless, since the coprocessor features wide simd vector instructions, a proper loop vectorization is particularly important for its full utilization. propitiously, vectorization reports collected for findsite show that the majority of loops taking the most execution time are indeed vectorized. still, there are other issues related to data dependency and alignment, which need to be addressed by rearranging loops, data structure padding, improving register utilization, and data caching. therefore, in addition to the parallelization of the remaining portions of the serial code, future directions of this project include thorough code optimizations to take a better advantage of the intel xeon and xeon phi architectures. the up-to-date versions of findsite are freely available to the academic community at www.brylinski.org/efindsite; this website also provides compilation and installation instructions, as well as a detailed tutorial on processing large datasets using heterogeneous computing platforms."
"give the algorithm more accuracy, however the computation is very time 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 here, we estimate how much time is saved by solving only the relaxed model during the search procedure. the following two strategies are considered: the proposed algorithm which only solves the lp during the search process except when the cost function value equals zero (pag); the algorithm used in the previous section which will solve an integer programming 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 problem at each iteration (eag). the evaluation of the average cpu time with the number of iterations is shown in figure 4 . one finds that the matheuristic with pag is faster than that with eag on difficult instances, but seems worse for simple instances. however, the difference on simple instances is negligible, for both algorithms can get the optimal solution in very short time. for difficult instances the two algorithms have similar performance at an early stage, but the gap gradually widens as k decreases. table 3 shows the detailed results of this experiment. one observes that pag gets better average and best results than eag. one also observes that pag uses more cpu time. however, that is because of the time limit of four 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 hours. pag happens to get better results in the time limit. for eag, it may need much more time to get the same result as pag. from this experiment we learn that the k-ndgp gets more and more difficult as k decreases, and the proposed evaluation technique is very helpful when k is small."
"as already mentioned in section 3.2, a configuration x corresponds to any network with k lightpaths. the cost of this configuration is the total overload amount on the network and the set of moves applicable to it is defined by the twist neighborhood."
"if the second type state has only one loop in the transition rule, an appropriate version of ltl-formula is chosen for the program variable or second step. we have a specification of every state variable behavoir and every counter variable behavior as a pair of ltl-formulas. now describe a method of constructing a program realization of the minsky counter machine in the st language. if a counter of the minsky machine is in both types transition rules, ltl-specification of counter variable behavior has the following form:"
"the intel xeon phi coprocessor provides a large number of computing cores, however, individual target cores in our benchmarks are notably slower than those of the host. a single host thread performs rmsd calculations per second, whereas a single target thread performs (balanced and scatter modes) and (compact mode) rmsd calculations per second. thus, depending on the affinity settings, target threads are about 10 (balanced and scatter) and 20 (compact) times slower than host threads. we note that if adjacent threads share common data, placing them on the same execution unit by using the balanced thread affinity may bring some improvement over the scatter mode (fig. 2) . nevertheless, individual threads in findsite are fully independent, therefore, the balanced and scatter thread affinities yield identical results; henceforth we report this performance as balanced/scatter."
"it can be seen that our approach is based on a reduction to the decision version of the problem, the k-ndgp. in order to solve a ndgp instance, i.e., algorithm 1 algorithm for the ndgp"
"in the above definition, the first three properties are similar to the classical tree decomposition. if the later preserves the initial set of nodes and their dependencies, in our definition, the distributed tree decomposition of an acquaintance graph preserves the set of peer variables and their dependencies. we also need to add the fourth one, to adapt our dtd definition to the distributed case. the compliance of acquaintances also forces dtds to have the following property: a cluster is hosted by one (and only one) peer, and a peer hosts at least one cluster (if its formula is not empty), and possibly many. in addition, all interactions between clusters are following the initial peer acquaintance. our privacy rule, in the context of dtd is expressed by the fifth property. all clusters hosted by one peer p can contain any shared variable from any peer, but only local variables of p itself. if we look at figure 4, we see that all variables l i stay at their initial peer. at the opposite, shared variables h and g are sent to p 4 for ensuring the running intersection."
"in this study, we developed a new version of findsite, ligand-binding site prediction software used in structural biology and drug design, for processing large datasets using modern heterogeneous high-performance systems, i.e., multicore processor platforms equipped with intel xeon phi the amount of computations is approximated by the product of the target protein length and the number of template structures. fig. 11 . evaluation of the accuracy of binding site prediction using different versions of findsite. the cumulative fraction of proteins is plotted against the distance between the center of the best top three predicted pockets and the geometric center of bound ligand in the native complex structure. inset shows the accuracy of pocket ranking when multiple pockets are detected."
"the execution of a minsky machine is a sequence of configurations, inductively defined according to transition rules. the counter machine has one execution from the initial configuration, because for each state there is one transition rule or none. the machine input is a set of counters. the machine starts from the state and stops in the state with an output set of counters or loops implementing a partial recursive function."
"accelerators. there are two major conclusions arising from this project. first, offloading parts of the computations to the coprocessor device indeed provides the desired speedups, which are considered significant when normalized by the cost of the hardware. compared to a serial version, findsite code runs 11.8, 10.1, and 17.6 times faster using the parallel processing on the host, the target, and both resources, respectively. of course, the acceleration strongly depends on the nature of the code. for example, the benchmarking of compute-intensive applications in finances reported speedups of 20--40 for a xeon phi code over a sequential reference implementation running on a single processor core [cit] . another study systematically compared a number of applications for microscopy image analysis and fig. 12 . pocket prediction for a penicillin-binding protein from p. aeruginosa using findsite. the crystal structure of the target protein and the binding ligand are displayed as a gray cartoon and sticks colored by atom type, respectively. the top-ranked predicted pocket is shown in yellow; the ball corresponds to the pocket center and the predicted binding residues are shown as sticks and a transparent surface. two red balls show the centers of binding pockets predicted at ranks 2 and 3."
"in this paper, we propose a matheuristic, i.e., an algorithm that combines local search with mathematical programming techniques, for the ndgp. the proposed algorithm implements tabu search as the master algorithm and em- 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 beds two slave algorithms, which are linear programming and mixed integer programming. this new algorithm has found improved solutions to each of 22 instances in a benchmark set."
"the t oken circulating in the network is also decorated with precious information. it contains, intuitively, the set of current orphan sub-jointrees that are being built, rooted in removed peers so far. to allow any peer in the network to smartly choose which subjointree to attach as a son, each sub-jointree also contains its projection, i.e. all the non local variables of the sub-jointree. neighbours * is the set of all the neighbours of a peer, including removed peers, and itself. neighbours+ is the set of all the neighbours of a peer, including itself, but not including removed peers. best is the \"best\" neighbour of a peer p, i.e. the peer having the smallest value in the score[] array, including p. children is the set of direct children a peer has registered so far."
"one may notice here that score[] arrays have been updated only in the neighbourhood of p token, and thus the votes of the neighbours of p token are based only on partial information: if p is a neighbour of p token, p only updates the scores of its neighbours that are also connected to p token . all other peers are ranked according to an old value of score[], that can be based on an old set of orphans sub-jointrees. at some point, we must accept this, because this is just a heuristic, and trying to have a better estimation will cost too much. however, one may also notice here that if no peer is elected, the token remains the same, and follows the \"best\" links or the global cycle, eventually touching all the peers at the end of the cycle. the algorithm will end because once a peer has updated its own score according to the token, if the token does not change (no peer is removed), then its score will remain the same. if the token touches all the peers, all score[] values will have been updated according to the same orphan sub-jointrees (this remark is important for termination, we have the guarantee to find a node to remove after at most one cycle, and in any case after visiting at most two times each edge)."
"the article demonstrates the consistency of the approach to constructing and verification of plc programs by ltl-specification from the point of view of turing power. it is proved that in accordance with this approach for any minsky counter machine an ltl-specification can be built, which is used for machine implementation in any plc programming language of standard iec 61131-3. minsky machines are equipollent to turing machines [cit], and the considered approach also has the turing power. the proof focuses on representation of the counter machine behavior in the form of a set of ltl-formulas and matching these formulas to constructions of st and sfc languages. sfc is interesting as a specific graphical language. st is considered as the basic language because the implementation of a counter machine in il, fbd/cfc and ld languages is reduced to rewriting blocks of an st-program. the idea of the proof is demonstrated by an example."
"however, when the system is intrinsically distributed, or subject to privacy settings, no peer can have a global view of the system, and new algorithms must be explored. for instance, adding a link between two peers may not be feasible (only already existing links can be allowed). in all previous approaches, the initial distributed system was supposed to fulfil an additional strong characteristic: two peers that share a common variable must be directly linked by the network."
"constraints (9) this mixed integer programming problem can be solved by a mip solver such as cplex in reasonable time, because the number of variables is not excessive. another good feature of this formulation is that it has a small integrality gap. this characteristic helps a lot when the model is used to evaluate the neighborhood structure. 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65"
each cluster is hosted by one peer. neighbouring clusters are hosted by the same peer or by neighbour peers in the acquaintance graph. (compliance of acquaintances)
"x l equals 1 if lightpath l is in use, 0 otherwise; s l i equals 1 if the origin of lightpath l is node i, 0 otherwise;"
"definition 2 (acquaintance graph). given a distributed problem setting defined on variables v ars, an acquaintance graph g((p, v ), acq) is a graph defined by:"
"in the specification development it is important to take into consideration the order of temporal formulas describing the behavior of the variables. a variable without pseudo-operator \"_\" may be involved in the specification of another variable behavior only if the specification of its behavior has already been completed and is in the text above."
"approaches to protein structure modeling and functional inference represent the most practical strategy to keep up with annotating the massive volume of dna sequences [cit] . the resulting knowledge facilitates a broad range of research in life sciences including systems biology and drug development and discovery. for instance, systems biology focuses on studying cellular mechanisms as a whole by constructing and analyzing the networks of complex molecular interactions and signaling pathways at the level of complete proteomes [cit] . such systems-level approaches hold a significant promise to develop new treatments for complex diseases, which often require a simultaneous modulation of multiple protein targets. this is the domain of polypharmacology, an emerging field that integrates systems biology and drug discovery [cit] . incorporating large biological datasets is critical for the success of many systems-level applications; however, considering the vast amounts of data to be processed, unprecedented computing power is required to achieve a reasonably short time-to-completion. significant challenges remain given that the currently available biological data may easily outbalance accessible computing resources."
"most computer programs follow the 80/20 rule and spend 80% of the wall time executing 20% of the code, thus the pareto principle can be applied to guide optimization efforts [cit] . before converting the serial version of findsite to a parallel version, we conducted a thorough profiling in order to identify portions of the code consuming the most cpu cycles. fig. 3 shows the results of code profiling using 12 proteins randomly selected from the benchmarking dataset (1 protein from each bin in table i ). in fig. 3(a), we measure the execution time for the three individual stages of findsite calculations, according to the flowchart presented in fig. 1 . pre-processing, structure alignments, and post-processing steps take 11%, 88% and 1% of the total simulation time, respectively. next, we use gprof, a performance analysis tool, to generate a function list ordered by computing time. fig. 3(b) shows that four functions, tmsearch_, cal_tmscore_, dp_ and get_score_, are the most time consuming taking up 29%, 27%, 21%, and 15% of the execution time, respectively. all these functions are involved in structure alignment calculations utilizing 92% of the entire computing time. based on these profiling results, we identify template-to-target structure alignments as the most computationally expensive, thus parallelizing this portion of the code and moving calculations to the accelerator should bring about the desired performance improvement in predicting ligand-binding sites using findsite. we also looked into the structure of the code for alignment calculations in findsite to estimate the difficulty of parallelization and porting it to the external accelerator card. each individual template-to-target structure alignment starts by calling the subroutine frtmalign in the main function. fig. 4 shows a detailed call graph generated by doxygen [cit] for functions associated with frtmalign. frtmalign calls a number of subroutines, which in turn call other functions and so forth. the subroutine at the bottom, the most frequently called by subroutines at higher levels, performs root-mean-square deviation (rmsd) calculations for structure comparisons. we note that all protein structure alignment functions are written in fortran77, consist of about 2100 lines of source code, and routinely access data stored in the memory using common blocks. porting this code to the gpu platform using cuda or opencl would require a great deal of effort; in contrast, using intel xeon phi combined with pragma-based openmp requires significantly shorter development time to yield satisfactory speedups."
"step action and exit action of qistep have the same program code (in any language -il, st, fbd/cfc and ld) obtained from ltl-formulas of state variable behavior specification. exit action \"x\" is necessary for deactivation of variable after triggering qistep transition condition. nested sfc-diagram matches with the transition graph of the counter machine thus, we showed, that for an arbitrary -counter minsky machine the ltl-specification of its behavior can be built. construction of program realization of the machine is carried out in any standard plc programming languages by this specification."
"the search space (set of configurations) explored by our local search procedure is denoted by s. in the proposed local search approach, a configuration x is any network of k edges (lightpaths) with the assignment for each traffic demand."
"for this experiment, a subset of seven instances was used: ndg20 t100.1, ndg20 t200.1, ndg20 t300.1, ndg40 t200.1, ndg40 t200.5, ndg40 t400 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 and ndg100 t500. the algorithm was run 10 times on each instance and for each parameter setting. the time limit was set to 4 hours for each run."
"gives the vertex that is shared by l 1 and l 2, and function dend(l 1, l 2 ) gives the vertex that is in l 2 but not in l 1 ."
"in the tabu algorithm, a perturbation is triggered when perturbp eriod iterations have been performed without improving the best configuration. in this case, a perturbation of strength strength is applied to x best ."
"in this paper, we propose to explore the general case, i.e. when links between peers are not forced to follow the above characteristic. in this more general case, we propose a new distributed algorithm for distributed tree decomposition, based on local elections with a token. using a token was necessary to prevent concurrent decisions, which was identified as one of the main issues for good jointree construction. we conclude this paper by an experimental analysis of our algorithm on families of small-world networks, and show that the produced jointrees are significantly better than state of the art distributed algorithms, while allowing more general networks."
is our second restriction for building a tree decomposition. we can notice that the tree decomposition shown in figure 4 does respect privacy while the one in figure 2 does not.
"encouragingly, fig. 7 shows that increasing the number of target threads leads to a linear scaling of the performance. for instance, increasing the number of threads from 4 to 24 with the affinity mode set to balanced/scatter improves the number of rmsd calculations per second from to for the total time, and from to for the alignment time. switching to the compact thread affinity mode improves the number of rmsd calculations per second from to for the total time, and from to for the alignment time. it is clear that the scatter/balanced thread affinity yields approximately 1.8 times higher performance per thread than the compact mode. this is because both scatter and balanced modes evenly spread threads across the target computing cores to maximize the hardware utilization, whereas the compact affinity mode places up to four threads on a core before moving to the next one (fig. 2) . thus, with the scatter/balanced setting, the best performance is achieved when 120 threads are spawned on the coprocessor, whereas 240 threads are required for the best performance in the compact mode. the advantage of using the scatter/balanced affinity starts diminishing when the number of threads is greater than 120. when the thread count reaches 240, different affinity settings yield the same performance corresponding to the compact mode. a core-to-core comparison shows rmsd calculations per second for the scatter/balanced and for the compact affinity. therefore, assuming that all available hardware threads are fully utilized by findsite, offloading the structure alignment portion of the code using the compact thread affinity gives the best performance."
"note that the traffic demands and the lightpaths are undirected. the network to be designed is a multi-graph, i.e., there may be several lightpaths in the graph with the same source and sink nodes. what makes the problem difficult is that the traffic demands are unsplittable in the sense that every traffic demand has to be assigned to a unique sequence of lightpaths forming a path from the origin to the destination of the demand. thus, we cannot consider the lightpath capacity to be additive on each edge."
"in this section we introduce an exact method to solve the grooming subproblem of the ndgp. we define the grooming subproblem as follows. given a fixed optical network and a set of traffic demands, the goal is to determine the routes for all the traffic demands ensuring that there is no overloaded lightpath in the network. from the previous definition of ndgp, it is clear that the grooming subproblem is exactly the ndgp with fixed lightpath decision variables (x l, s l i and d l i ). for the grooming subproblem, if the integrality constraints on the flow variables (v t l and w t l ) are relaxed, the problem becomes a multi-commodity flow problem with constraints but without an objective function. this multi-commodity flow problem with fractional flow variables can easily be solved by linear programming. however, an easy constraints satisfaction formulation does not help very much. in order to embed the exact method into the local search heuristic, it is desirable to understand the causes of infeasibility when the instance is infeasible. therefore, we change 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 this decision problem into another optimization problem as follows. for each lightpath l, we define a new constant λ l i equal to 1 if the lightpath originates from i, -1 if it terminates at i, and 0 otherwise. we also define constants δ l representing the overload for each lightpath l. the model for the grooming subproblem can be given as follows."
"distributed bucket elimination bucket elimination is a general method that can be used for decomposition. it builds a tree of clusters from the leaves to the root of the tree. when applied with the dfs order, be does not have to build the tree, it follows the dfs one: dfs induces a total order on viewed peers s.t., for each peer, each of its neighbours is an ancestor or a descendant. initially, an empty cluster χ ctp is associated to each peer; then, for each peer p i, if it is a descendant of some peer p j then the vocabulary common to the two peers p i, p j is added to the cluster χ ctp i . then, the algorithm typically proceeds bottom-up in two stages: (1) a peer p i sends to its father p j a projection pjct pi of the variables contained in χ ctp i and belonging to one of its ancestor; (2) when p j receives pjct pi from p i, it adds pjct pi in its cluster χ ctp i . when it has received a projection from all its children it projects to his father the variables of χ ctp j shared with one of its ancestors. for more details, the piece of work [cit] assumes a dtd based on cluster tree elimination [cit] applied to a decentralized context."
"a plc is a reactive system. it has a set of inputs connected to the control object by sensors and a set of outputs connected to actuators [cit] . the plc repeats the execution of a user program periodically. there are three main phases for program execution (a working cycle): (1) reading from inputs (sensors) and latching them in the memory, (2) program execution (with input variables remaining constant), (3) latching the values of the output variables to the environment. the application of programmable logic controllers (plcs) for systems controlling complex industrial processes makes exacting correctness demands to plc-programs."
"in our tabu procedure, a perturbation consists in a series of random moves and the strength of the perturbation is measured by the number of random moves to be performed. therefore, the perturbation can be seen as a jump in the search space, and the strength of the perturbation as the amplitude of the jump."
"the physical node is the component upon which all virtual machines reside. this component corresponds to the physical machine with a physical location and hardware specification. physical nodes are characterized in terms of cpu, memory, wireless and wired network i/o since we are focusing on network equipments. we used in our platform physical equipment with 4 gbyte ram, c2d-2.4 ghz cpu and six 1 gbps ethernet network interface. we adopted xen [cit] as hypervisor because it is open source and xen is increasingly popular among virtual network infrastructure researches. xen is also a high performance resource-managed virtual machine monitor (vmm) [cit] . the vmm provides isolation and safety for running virtual machines. despite the advances in virtualization technology and its applications [cit], the overhead of network i/o xen virtualization [cit] still have a negative impact on the performance of network intensive applications. however, the performance is close to the performance of non-virtualized software routers if the need for processing overhead is satisfied and allocated resources are oversized to support unfairness in resources sharing."
"the use of the framebuffer minimizes the latency and communication required to send the data back to the host and then sending it to the gpu at a later stage [cit] . therefore, the data is generally adapted so it can be stored on the framebuffer and used in all proceeding calculations."
"the gpu of computer 2 is significantly slower than that of computer 1, which should result in the gpu of computer 1 computing faster. however, the cpu in computer 2 was superior to that of computer 1 allowing the gpu in computer 2 to outperform that in computer 1."
"we evaluated the virtual wireless network access using three virtual wifi access points independently of the mesh virtual networks. a first ll gives us the results depicted in fig. 5 . in the use experiment, we begin with one virtual access point working then after 180 seconds a second one is working, then a third one after 360 seconds. after 720 seconds the first one is stopped and finally after 900 seconds the second one is stopped. the three flows are three different mpeg 2 flows to study the intrinsic performance of the access. on this picture, we see that with just one virtual access point working, we get between 22 and 24 mbps on the average. this depends on the movies going through the virtual access point. with two flows the total average rate decreases a little bit to a total of 21 mbps. finally with the three flows in the same time the total throughput is approximately 20 mbps. we can conclude nof'11 2011 international conference on the network of the future that the hypervisor takes a part of the power of the physical access point. a second experiment considers a unique application with different flows: the virtual networks are adapted to the characteristics of the flows. in this experiment, we were interested in the jitter associated to the different flows. indeed, the mpeg-2 movie could be reproduced at the receiver with some delay. however, we are interested in evaluating the jitter coming from the three virtual access points. the difficulty of the solution we proposed comes from the impossibility to address priority between the virtual access points. this is due to the isolation strategy we used in our solution. the results are described in fig. 6 concerning the three flows of the mpeg 2 application. we can observe that the jitter of the three flows are comparable since their respective virtual access points are scheduled with the same privileges. if we want to give some priorities to some flows we need to introduce in the privilege node (dom0) an algorithm allowing to prioritize the demands of access of the different flows. the drawback of this solution is the obligation to know the semantic of the flows to be treated in the access point. v. conclusion advances in virtualization technologies have created new opportunity for network operators to take advantage of network resources more efficiently. we try to adapt virtualization concepts to satisfy many of today's network telecommunication challenges."
"the physical wireless interface offers the ability of creating multiple virtual wireless interfaces as virtual acces point (master mode) with different ssid and mac addresses. in our case, the physical wireless interface is limited to four virtual wireless interface. each virtual wireless interface is connected to a virtual machine access point through a dedicated bridge. the virtual access point is named the same as the ssid affected to its virtual wireless interface. the creation of virtual wireless interface and the configuration of the channel and the ssid are done in the privilege domain dom0. the virtual access points have their virtual access interface isolated by the wireless interface hardware capabilities. the virtual machine access point are isolated down to the hypervisor isolation. the global architecture of the virtual access point is shown in fig. 3 . fig. 3 . the architecture of the virtual access points."
"the extent of parallelism used on a gpu depends on the ability of the programmer to exploit the instruction-level parallelism which is available through the typically four element vector structure within the gpu and ensuring that the numerous processors on the gpu are utilized for the program [cit] . in order to take full advantage of the gpu's capabilities, it is important that shader programs are highly arithmetically intensive, i.e. has a large ratio of mathematical operations to memory accesses [cit] . if it is not, the overheads added by the initialization will take longer and slow down the algorithm [cit] ."
"however, the conjugate gradient method is not affected by this weak convergence and is comparable to the number of operations required by both the jacobi or gauss-seidel methods [cit] . importantly, the conjugate gradient method calculates inverse matrix coefficients independent of each other, and was thus considered a suitable alternative to the cholesky method for stream processing application."
"gpus are not likely to threaten the prominence of cpus since they are developed for very specific applications. stream processing on the gpu should rather be seen to complement the cpu as a co-processor for certain types of applications. nonetheless, it is likely that gpus will play an important role in engineering calculations in the future. [cit] suggests that non-graphical computation on gpus could be the most important computing trend over the next 10 years."
"to validate the performance of the considered compression schemes, we use the mit-bih arrhythmia database [cit] that is the most commonly used database for the comparative study of ecg compression algorithms. this database contains 48 half-hour excerpts of two-channel ambulatory ecg recordings, obtained from 47 subjects studied by the bih arrhythmia laboratory. the recordings were digitized at 360 samples per second per channel with 11-bit resolution over a 10 mv range."
"in this study the c++ programming language was used for the main host program and glsl for the gpu shader programs. the opengl 2.1 library was used to link the c++ and the glsl code, initialize and store the textures in the framebuffer, and initialize all the shader programs. glut was used to interact between the window system and opengl. the fragment shader programs contain the algorithms that were used to perform the conjugate gradient method. as the program loads all of the data onto the gpu memory through the use of textures, the cpu was not used for any of the calculations performed in the stream processing implementation of the conjugate gradient method."
"the implication of the results for sparse systems is that the gpu would outperform the cpu for sparse matrices larger than about 270,000x 270,000 on computer 1 and 145,000 x 145,000 on computer 2. these represent exceptionally large water distribution models, and thus it is unlikely that stream processing would find practical application to hydraulic network solvers using current technology and the conjugate gradient method. however, the study clearly showed the potential for substantial improvements in computational efficiency for engineering problems that are better suited to stream processing."
"the rest of this paper is organized as follows. section ii introduces our proposed ecg compression and reconstruction scheme. section iii details the performance metrics and ecg database used for its evaluation. section iv details our embedded implementations. then, section v presents a discussion of our results and main conclusions."
"in our approach, virtual machine could be any kind of network equipment used within real network: routers, label switch router, firewall, access point, sip router, ip pbx, etc. a virtual machine could implement any stack protocol such as: ipv4, ipv6, mpls, etc. a virtual network is created by the instantiation of each virtual machine that composes its topology. these virtual machines are linked through virtual links. a virtual network router could use any protocol stack associated with a routing protocol such as: ospfv2, ospfv3, rip, ripng, bgp, etc."
gpus have advanced drastically in recent years and have opened up the possibility of applying them to problems outside of the graphics field. the implementation of nongraphical programming on the gpu through stream processing has great potential for speeding up certain types of engineering calculations. 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 even though there is a strong movement in programming from conventional linear or parallel programming on the cpu to mass-parallel programming on the gpu. further advances in gpu architecture and software are likely to introduce the gpu to a much greater extent as a standard technology in the scientific and engineering fields.
"the multi-billion dollar gaming industry has created a continuous and strong demand for more realistic computer graphics and faster animations in pc games [cit] . as a result, graphical processing units (gpus) have seen vast performance increases in processing capacity. at the time of writing, commercially available amd gpus were 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 due to the overwhelming demand for gpus in the gaming industry, gpus have become considerably cheaper than cpus with similar processing power. this has created an incentive for programmers to exploit the processing power of gpus for non-graphical problems, such as those found in engineering disciplines. this movement was strengthened by certain gpu developments such as increased speed, increased parallel processors, ieee 32-bit floating point precision and the unlocking of former fixed pipelines in graphics hardware [cit] . large it corporations, such as nvidia, amd, intel, microsoft, ibm, apple and toshiba recognized the potential for gpus, and have all made significant advances in the field in recent years [cit] )."
"after describing our virtual wireless network architecture, we have done some measurement on throughput, and jitter to have an idea about the behavior of such deployment. it's important to have an observation about the isolation between the instantiated virtual access points and how this approach supports several virtual networks transmissions. it indicates us how many virtual operator would probably be accepted to share wireless physical resources."
"since these internal dependences in the data stream are not allowed in stream processing, the sequential cholesky method could not be used, and an alternative linear equation solver had to be found."
"as aforementioned, the original ecg signal x has a sparse approximation, i.e., it can be represented by a linear superposition of s elements of an orthonormal wavelet basis"
"we have integrated wireless network virtualization within our virtual environment. each user, depending on their wireless interface capabilities, could be connected to several virtual networks simultaneously. those virtual networks are extended with virtual access points. the virtual access points implements the same stack protocol as the virtual network that they belong to. each virtual network could be allocated by a independent virtual wireless operator and extended as possible by other virtual network services within virtual environment or from cloud services."
"while the resting electrocardiogram (ecg) monitoring is standard practice in hospitals, its ambulatory counterpart is this research has been partially funded by the nano-tera.ch ntf project biocs-node, which is financed by the swiss confederation. 978-3-9810801-7-9/date11/ [cit] edaa still facing many technical challenges. for instance, the 3-lead ecg is still nowadays recorded on data-logging (holter) devices during 1 to 5 days of normal daily activities of a patient. these systems, currently commercialized by ge health care, sorin group, mortara and philips health care, suffer from important limitations: limited autonomy, bulkiness and no or limited wireless connectivity. recently, the realization of wireless-enabled ultra-low-power ecg monitors for ambulatory use has been receiving significant industrial and academic interest [cit] . yet, these state-of-the-art ecg monitors fall short in terms of either clinical relevance and/or autonomy figure, primarily because they transmit uncompressed ecg data over power-hungry wireless links. it is today widely acknowledged that the achievement of truly wbsn-enabled personal ecg monitoring systems requires more breakthroughs not only in terms of ultra-low-power readout electronics and radios, but also and increasingly so, in terms of embedded ecg compression and feature extraction algorithms, and assorted ultra-low-power dedicated digital processors."
"our target platform is the iphone 3gs mobile phone, which includes an arm cortex tm -a8 processor [cit] . this architecture supports multi-thread execution and single instruction multiple data (simd) instructions, which we exploit to achieve real-time operation of the cs-based ecg reconstruction."
"important gpu hardware limitations are the amount of memory allocated to a single texture, and the total amount of data that can be stored on the gpu's onboard memory. once the total onboard memory limit is reached on a windows xp-based system, the gpu cannot process the data stream. windows vista and windows 7 accommodate for the limit in onboard graphics memory by introducing the ability to use global memory for the gpu, and thus the total memory required to process the algorithm would not be a limiting factor [cit] ). the use of this virtualized memory, however, causes losses in processing power, because the gpu has to send and receive data from the cpu through the bus."
"as we show later on, many efficient sensing matrices can be constructed with simple pseudorandom design that can be implemented using a surprisingly small amount of on-board memory and computation."
"we have proposed virtual network environment based on virtual access points. we have presented the design of the mechanisms underlying the virtual network platform. we have implemented this approach in a real prototype. using experiments with realistic virtual router network applications and benchmarks, we demonstrated the performance/cost benefits and the effortless applicability of wireless network virtualization within virtual environment. we have measured the performance of this architecture with a real testbed."
"the classic graphics pipeline is shown in figure 2 . this pipeline has been used in graphics cards for the last 20 years, with improvements emerging in the functionality and programmability within each of the stages. data from memory is made available to the gpu through the use of textures, which consist of arrays of pixels [cit] . textures are processed by the shader programs and are output as fragments. fragments contains pixels, as well as extended data like depth, normal, and texture coordinates [cit] fragments become pixels, they still undergo many different operations [cit] . each pixel typically consists of four channels of data, three for the primary colors and the fourth for transparency. thus, in a non-graphical context a pixel can be viewed as a one-, two-, three-, or four-dimensional vector-array of numbers [cit] . the output from the pipeline is a texture, which is stored in the framebuffer [cit] . the output from the framebuffer can either be sent back to the cpu or reused within the graphics pipeline."
"the techniques used to optimize the cs reconstruction algorithm are based on exploiting code vectorization and simd extensions, typical of a large set of embedded microprocessors nowadays. in the case of the arm cortex-a8 of the iphone 3gs, the simd support is implemented using intrinsics in a general-purpose simd engine called neon [cit] . these intrinsics provide similar functionality to in-line assembly and appear as function calls in c that are replaced at compile-time by a sequence of low-level instructions specific to the cortex-a8. vectorizing the cs reconstruction algorithm involves operating on vectors of data lengths between 2 and l elements, according to the underlying simd hardware support, where l is the maximum allowed float elements per vector. regarding our iphone implementation, we vectorized the reconstruction algorithm to operate on vectors of 4 float data, as this is the largest value of l supported on the iphone 3gs. these vectors are processed inside loops, which are defined by the matrices of the cs reconstruction process. thus, the loops size of the cs reconstruction algorithm is the only hard limitation on the vectorization process. hence, we perform two different optimizations related to array padding, loop unrolling and loop peeling [cit] in the hierarchy of loops to achieve real-time operation, namely: a) single-loop optimization: when the number of iteration in a loop is a multiple of l, it can be then vectorized into vectors of l elements. each iteration is then independent from all previous ones and all vectorization pointers can operate on l successive addresses. however, if the iteration is not processing l successive addresses, then the vectors need to be loaded lane by lane. the following example shows an implementation of a simple multiply-accumulate in the code of the cs reconstruction algorithm. in particular, in the iphone the vector floating point (vfp) implementation takes 18 − 21 cycles for a single-precision multiply-accumulate, but by exploiting neon intrinsics, we can execute two multiplyaccumulate in 1 cycle, achieving large performance saving."
"in this section we describe the used embedded platforms, and report the platform-dependent optimizations we perform on the two algorithms of section ii to achieve real-time execution and optimized memory footprint."
"the processing capacity of gpus has been nearly doubling every year. this rate is 1.4 times greater than moore's law, which states that the cpu processor's performance doubles every 18 months [cit] . figure 1 [cit] . the primary driver of the gpu performance increase has not only been clock rate, but also increases in the number of processors on the gpu, allowing for a higher level of parallelization."
"the dense matrixes were constructed with diagonal elements that are equal to the sum of the absolute values of all of the off-diagonal elements in the row, making the system weakly diagonally dominant. this is equivalent to the coefficient matrix used in the global gradient"
"thus, it is evident that the results using stream processing on the gpu are dependent on the capabilities of the cpu. this effect has been illustrated in a test in which a cpu processor was overclocked to different frequencies and the performance of the gpu was recorded [cit] ."
"the gpu is a dependent, auxiliary component of a computer designed for the mathematical manipulation of computer graphics, and thus does not possess the more wide-ranging capabilities of the cpu [cit] . even though the primary purpose of the gpu is in the area of graphics, it is capable of running more general instructions and is considered a programmable floating-point processor."
"algorithm. the computation time of these matrices can then be used to assume the computational time required for a water distribution system, in which each of the pipes are connected to the next two pipes in the system.. the dense system consists of n 2 elements, while the sparse system would have 5n elements, with n being the size of the system. it is understood that there are extra computation required for the sparse implementations, like memory handling. the size of the linear system was increased starting with a 2x2 matrix (size 4), and increasing the numbers of rows and columns in powers of two."
"according to the world health organization, cardiovascular diseases are the number one cause of death worldwide, responsible for an estimated 17.1 [cit] (i.e., 29% of all deaths worldwide) and economic fallout in billions [cit] . these increasingly prevalent cardiac diseases are requiring escalating levels of supervision and medical management, which are contributing to skyrocketing health care costs and, more importantly, are unsustainable for traditional health care infrastructures. wireless body sensor networks (wbsn) promise to allow inexpensive, continuous and remote health monitoring for next-generation of ambulatory personal telecardiology or e-cardiology systems. outfitting patients with wearable, miniaturized and wireless sensors able to measure and wirelessly report cardiac signals to tele-health providers would enable the required personalized, real-time and longterm ambulatory monitoring of chronic patients, its seamless integration with the patient's medical record and its coordination with nursing/medical support."
"data parallelism is performed on the gpu by assigning a shader program to each processor on the graphics card. this allows each processor to perform the same set of instructions (or shader program) at the same time on different data streams. a data stream is made up of arrays of data stored in textures, and includes both the input to and output from the shader programs [cit] ."
"we have developed a multi-thread application using the producer-consumer paradigm to display and decode the ecg. the first thread manages the bluetooth connection using btstack [cit], decodes the data and stores 2 sec. of ecg (i.e., 512 sampled values) into a shared buffer. the second thread reads data from the shared buffer and draws it on the screen. then, to avoid the decoder to be paused, the second thread is called each 15 ms to draw 4 new pixels. finally, the buffer needs to store 6 sec. of ecg: 2 sec. for reading, 2 sec. for writing and 2 additional sec. due to the delay on the iphone drawing hardware."
the virtual access point allocates at least one virtual wireless interface allowing it to reach connected users in its coverage area. each virtual access point is detected by user transparently as a physical access point using native system tools. the virtual access point implements a dhcp server which provides ip addresses (ipv4 or ipv6) for users asking for a connection. this virtual machine includes also the access router to the rest of the virtual network. the physical node could instantiate simultaneously different type of access points. in fact each physical wifi interface could be shared between several virtual machines including access point functionalities. we developed mesh virtual networks over a physical mesh network. a mobile device can be connected simultaneously to the different mesh network using virtual access point: a virtual access point possesses on a hypervisor permitting to support different stacks of protocol. virtual access points deployement is described in fig. 4 . we developed this architecture aiming the following advantages:
"the network is distributed by nature. due to virtualization level capability, the network becomes also dynamic. a new operator can allocate available virtual resources and can instantiate a new virtual network with a new kind of service. another operator could delete a virtual network that is not any more in use to free the allocated resources. the virtual machine migration could be used for different purposes in this kind of platform to ensure high availability and loadbalancing. in the next section, we describe our virtual wireless network architecture which creates multiple virtual access points implementing different network protocol and providing heterogeneous services."
"while there are small differences between the languages, they all follow the same basic framework. the main programming languages available for stream processing on the gpu include: the opengl shading language was used in this study as it is very stable and has been in use for over 20 years, undergoing many revisions in this time. (glsl 4.10 [cit] ). thus, opengl and glsl continually stay on top of latest hardware developments and are trusted and stable languages. although glsl is not a direct gpgpu language, glsl is used to program real-time graphics and thus is a highly optimized api. opengl is also manufacturer independent."
memory handling becomes less pronounced as the linear system becomes larger. this is because the amount of arithmetic operations performed increases with an increase in system size.
"an instantiation of a virtual machine needs the following items: kernel, file system, network application and the configuration description of resources that will be allocated. we used small size virtual machines that can support different network applications. we have integrated some available open source projects inside virtual machines such as: xorp [cit], quagga [cit], asterisk [cit], mpls-linux [cit], opensip [cit] . we elaborated a virtual machine list that covers different kind of network equipment. we have adapted the virtual machine operating system to support several types of protocol stacks. we deleted unused operating system modules to eliminate their effect on network traffic. using light weight virtual machines increases routing performance and allows actions such as instantiation, migration, and backup."
"computational time was first estimated from the time it took to perform a single iteration of the conjugate gradient method for the various cpu and gpu implementations on a dense linear system. this provided a general comparison of the algorithm on the different 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 11 architecture. however, since the number of iterations required to solve different networks vary, the total run times were also compared."
"finally, we transform the loops of the cs reconstruction containing any if statement to be correctly vectorized. consider the optimization of the following code from the cs matrix projection:"
"the virtual access points are managed the same as for the other virtual machine. this means that all actions are supported such as: creation, migration, destruction, pausing, unpausing, etc. however, for virtual wireless network urbanization, we have to take in consideration that the allocated channel, ssid and mac address must be different to avoid packets transfer error and access conflict."
"while the matrix inversion is only one part of the method to solve hydraulic networks, it is done iteratively and is the main contributor to the computational cost of the solver. since the aim of this study was to explore the implementation and potential benefits of stream 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 processing for hydraulic network solvers, only the matrix solution process was implemented on the gpu. the standard conjugate gradient method was implemented on both the cpu and gpu and the computational times compared. no measures, such as preconditioning, were implemented to optimize the efficiency of the conjugate gradient method, since these methods are likely to show similar improvements on the cpu and gpu. for the same reason, sparse matrix techniques were not implemented (the jacobian matrix of a water distribution network is highly sparse), but the methods were applied to dense matrices representing highly connected networks. the implications for sparse network matrixes were interpreted from the results of the dense matrixes."
"unified gpus were first introduced in the nvidia geforce 8800, [cit], and have since become the standard for gpus. prior to unified processors, gpus consisted of a fixed number of fragment and vertex processors. unified processors consist of many stream processors which manage gpu processing resources more efficiently as stream processors dynamically switch between performing fragment or vertex processes. this eliminates the need for programmers to consider load balancing of the different processors. thus, unified gpus have become ideal general-purpose parallel floating-point processors [cit] ."
"in addition, the loops (or the number of the iterations of the loop) of the cs reconstruction algorithm use arrays not multiple of l elements for the filtering functions (cf. section ii-b). therefore, the remaining elements of the loop after the vectorization are processed separately, which implies three different cases, as shown in figure 3 ."
"the virtual network environment is formed by the amount of bounded virtual resources provided by physical network equipment. the physical nodes are connected through physical network link. each physical node offers its virtualized resources via a hypervisor. the hypervisor guarantees concurrent virtual machines running in isolation conforming to their allocated resources. the virtual machines share physical host resources in term of cpu, memory and i/o network interface."
the virtual network environment as shown in fig. 2 consists of several virtual networks. each virtual network can guaranty different kinds of service to satisfy the requested service level agreement (sla). the virtual network creation is achieved by the instantiation of each virtual machine. the virtual machines must correspond to the offered network service in terms of protocol stacks and network application.
"the hydraulic network equations have weak convergence characteristics, and thus neither the jacobi nor gauss-seidel methods could be used in the global gradient algorithm as both methods require excessive numbers of iterations to converge [cit] ."
"we have implemented the proposed optimizations (cf. section iv) for the 16-bit cs encoder and 32-bit reconstruction algorithm. figure 6 shows that the real-time implementation of the cs-based ecg system for wbsns provides the same accuracy as the original 64-bit matlab design. moreover, after applying the low-level optimizations of the cs reconstruction algorithm (cf. section iv-b), the algorithm runs 2.43 times faster for a compression ratio of 50%, which guarantees an accurate ecg reconstruction. without these low-level optimizations, the maximum number of iterations of the cs reconstruction process to respect the real-time operation of the decoding part (i.e., 1 sec. of total time spent in ecg reconstruction every 2 sec.) reaches 800 [cit] iterations. figure 7 shows the average number of iterations and the average execution time per packet while testing the mit-bih arrhythmia database [cit] on the iphone for different ratios. as figure 8 illustrates, the optimized system accurately receives and reconstructs in real-time the ecg signal on the iphone 3gs (as a wbsn coordinator), only taking a 17.7% of total cpu usage on average for a compression ratio of 50%. at the same time, the shimmer tm node is able to sense, compress and transmit the ecg signal to the wbsn coordinator in realtime, while having an average cpu usage of less than 5%. these results translate into a 12.9% extension in the node lifetime, with respect to streaming uncompressed data, which suggests the energy efficiency of cs. moreover, the processing energy can be further reduced using a more power-efficient microcontroller than the ti msp430 of the shimmer tm ."
"in this study, the conjugate gradient method used in hydraulic network solvers was investigated on the gpu and the cpu. the implementation of the conjugate gradient method in the stream processing model on the gpus proved to be both feasible and faster than the cpu implementations, but only for large linear systems. in large linear systems, the processing performance on the gpu was much greater than the cpu's performance due to the ability of utilizing multiple processors at the same time. however, the sparse nature of hydraulic networks means that gpus will outperform cpus only for exceptionally large networks. as a result, it is unlikely that stream processing will find practical application to hydraulic network solvers using current technology. however, the study clearly showed the potential for substantial improvements in computational efficiency for engineering problems that are better suited to stream processing finally, although opencl could not be used in this study, the development of opencl could enable the next generation of parallel processing applications and should be considered in future engineering applications. it already has the support of many large corporations, such as apple, amd, intel, nvidia and ibm. as it bridges the gap between the different architecture on the computer, namely the cpu and the gpu, allowing for both multiple task parallelization, which is most efficient on the cpu, and multiple data parallelization, which is most efficient on the gpu . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 view publication stats view publication stats"
"hydraulic modeling is an indispensible tool for the design of a water distribution system due to the size and complexity of many networks, and the non-linear nature of the governing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 9 hydraulic equations. the most common method used to solve the hydraulic network equations is known as the global gradient algorithm [cit] . this method uses mass and energy balance to set up the network equations under the assumption of incompressible flow. the application of a newton-raphson based method yields a set of linearized equations that are solved simultaneously for the link flow rates, and then repeated until convergence is achieved. the jacobian matrix in the set of linear equations is symmetrical, positive-definite, and highly sparse, and the equations are solved with the cholesky method [cit] ."
"the aim of this study was to investigate the application of stream processing on the gpu to water distribution network solvers. the paper starts off by introducing gpus and their potential in general purpose programming. it introduces necessary concepts and terminology, and an overview of the gpu programming paradigm. it then describes how stream processing on the gpu was applied within hydraulic network solvers using the conjugate gradient solver. finally, the results and conclusions are presented."
"solving the set of linearized equations in the global gradient algorithm is computationally intensive, and thus has potential for application to stream processing. however, the sequential cholesky method does not calculate inverse matrix coefficients independently of each other."
"the use of a fixed binary sensing matrix, combined with the quasi-periodic nature of the ecg signal, yields to very similar consecutive measurement vectors y. so, a large inter-packet redundancy exists, which must be removed prior to encoding and wirelessly transmit. hence, the redundancy removal module computes the difference between consecutive vectors, and only this difference is further processed. at the end, a state-ofthe-art entropy coding module is used for further compression. since the range of the difference signal just before encoding is between [−256 : 255], a complete huffman codebook of size 512 is needed with a maximum codeword length of 16 bits, for a given compression ratio. the storage of the offlinegenerated codebook requires 1 kb for the codebook itself and 512 b for its corresponding codeword lengths. the complete cs implementation requires 6.5 kb of ram and 7.5 kb of flash, 1.5 kb of which are for huffman codebook storage."
"to do stream processing, it is important to understand how the host (cpu) directs the gpu to perform calculations, and how the gpu itself operates. the process is graphically illustrated in figure 2 . the host is responsible for managing the computer's resources, interacting with the outside world and sending commands to the gpu [cit] . communication between the host and the gpu is performed through a bus, which is simply a set of wires that carry data (dokken el al. 2007) . the capacity of the bus is limited, but the need to send data between the host and gpu can be reduced by accessing local memory on the gpu in the form of caches (vertex buffer or framebuffer) and texture memory [cit] . the most common bus used in modern graphics cards is the pci (peripheral component interconnect) express bus."
"our experimental testbed consists of 4 physical machines. the physical node provides the necessary virtual resources for three instantiated virtual access points. three client laptops are connected to their virtual access point respectively. each one has c2d 1.6 ghz cpu, 2 gbyte ram and 54 mbps wireless network interface. the physical node that will host all virtual access points has 4 gbyte ram, c2d-2.4 ghz cpu, six 1 gbps network interface and d-link dwl-g520 wifi card. all instantiated virtual access points have this configuration: one x86 virtual cpu, two 100 mbps virtual interfaces, 20 mbyte image disk size, 80 mbyte ram, dhcp server and quagga router as network application."
"there are a number of important differences between the operation of the cpu and gpu. the gpu stores memory in a specialized and rigid memory structure, called texture memory, which acts like the cpu's random access memory [cit] . in addition, loops used in the conventional programming model on a cpu are replaced with shader programs, or kernels, on the gpu. shader programs are instruction sets that continually operate on data streams sent to the processor until all the data have been processed. the gpu processors can all run the same shader program simultaneously, and thus process the data in a highly parallel manner [cit] . finally, computations performed in functions on the cpu are also performed in shader programs on the gpu. a program can be made up of several shader programs as several functions combine to produce a program."
"as described in fig. 2, we have different protocol stacks ipv4, ipv6 and mpls network running on the same physical nodes and isolated down to hypervisor isolation. the ipv4 network is extended by ip pbx functionalities to support video conference service. mpls network is used for video streaming service due to its minimal delay guarantee. the ipv6 network guarantees data exchange service between its users. the clients of each virtual network use the same protocol stack and are able to self-configure to get simultaneous connections to their virtual networks. they can be connected to several virtual networks at the same time to take advantages of the different proposed services."
"where b orig and b comp represent the number of bits required for the original and compressed signals, respectively. the p rd, and associated signal-to-noise ratio (sn r), quantifies the percent error between the original signal vector x and the reconstructedx:"
"notice that the sensing matrix φ does not depend on the signal. to guarantee robust and efficient recovery of the s-sparse signal α s, the sensing matrix φ must obey the key restricted isometry property (rip) [cit] :"
"wbsn can offer cost-effective solutions to enable nextgeneration patient-centric tele-cardiology or e-cardiology solutions. in this regard, we have presented a novel real-time fig. 8 . ecg on the iphone energy-aware ecg monitoring system based on the emerging compressed sensing (cs) signal acquisition/compression paradigm for wbsn applications. our system demonstrated the feasibility of using cs in real-time for ecg compression, by implementing a light ecg encoder on the shimmer tm wearable sensor node and a real-time decoder running on an iphone 3gs, which acts as a wbsn coordinator."
"this work presents a complete real-time implementation of, on the one hand, a cs-based ecg compression/encoding on the embedded shimmer tm wireless mote, and on the other hand, the corresponding ecg reconstruction on an iphone. on the encoder side, the compression algorithm consists of the three processing stages depicted in figure 1 : a linear transformation is first applied to the original ecg signal, followed by an inter-packet \"redundancy removal\" stage, and an entropy encoding algorithm (i.e., huffman coding) is finally applied which outputs the compressed signal to be wirelessly transmitted. correspondingly, the decoder consists of three stages as seen in figure 1 : the input codes are decoded using the same codebook used in the encoder side, followed by a packet reconstruction stage which re-inserts the interpacket redundancy removed during the encoding, and finally the fista cs reconstruction algorithm is applied to find the best estimation of the original ecg."
"moreover, to quantify the compression performance while assessing the diagnostic quality of the compressed ecg records, we employ the two most widely used performance metrics, namely the compression ratio (cr) and percentage root-mean-square difference (p rd). cr is defined as:"
"a stream can be simple or complex, for instance it can consist of a stream of floating-point numbers or a stream of transformation matrices. a single stream can also be of any length, although longer streams are more efficient than shorter ones. a shader program works on an entire stream and cannot perform different tasks on individual elements in the stream [cit] . therefore the elements within the stream must be independent of other elements within the same stream [cit] . this allows shader programs to be highly efficient if the input data and the computed data are either stored locally or through carefully controlled global references. the same shader program can run on different gpu processors, each simultaneously manipulating a different data stream. [cit] ."
"a typical opengl program consists of creating a window and rendering a scene to the framebuffer [cit] . opengl is only concerned with rendering data into and reading data from the framebuffer [cit] . the library does not provide any support for any other peripherals such as a mouse or a keyboard. however, the opengl utility toolkit (glut), a freely distributed, cross-platform windowing api used to interact between the operating system and opengl, can perform these tasks [cit] . glut consists of a simple but useful interface and was designed for use with simple to moderately 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 8 complex programs, which deal mainly with opengl rendering [cit] . glut has support for c, c++, fortran, and ada; and is also platform independent [cit] )."
"it is important to note that parallel processing is not unique to gpus, but can also be implemented on cpus through threads that run on multiple processors on the same computer or on a cluster of computers. however, generally cpu parallelization is limited by the number of processors on the computer's motherboard (at the time of writing the commercially available limit is eight processors), and cost. consider, for instance, a typical 4-core cpu, the intel core i7-975, costing $999 upon release [cit] ) which can process up to 55.36 gflops [cit] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 [cit] in the field of hydroinformatics have used cuda for hydraulic network modeling."
"for all s-sparse vectors α. δ s is the isometry constant of matrix φ, which must be not too close to one. a universal good choice for the sensing matrix φ are random matrices, such as random matrices with independent identically distributed (i.i.d.) entries formed by sampling (1) a gaussian distribution"
"tm embedded platform: our target platform is the shimmer wireless sensor mote [cit] . from the hardware viewpoint, the shimmer tm mainboard includes the low-power texas instrument 16-bit msp430f1611 microcontroller and a bluetooth module. the msp430 microcontroller runs at 8 mhz, has 10 kb of ram, 48 kb of flash and includes a fast hardware multiplier, but does not include a floatingpoint unit. the mainboard also includes a micro sd slot supporting up to 2 gb of flash memory for data storage, and is powered by a rechargeable li-polymer battery. since our aim is to comparatively study the two compression algorithms on the standard mit-bih arrhythmia database, we used the shimmer tm serial port to read in the database records resampled at 256 hz, and to readout the encoded ecg data. from the software viewpoint, we used the open-source gcc 3.2.3 tool chain for the msp430 [cit] to generate the binaries."
"(2) we also circumvented the on-board generation of the normal random numbers by storing them on the platform; the large dense matrix multiplication still remained a main bottleneck. (3) we introduced an innovative approach to cs implementation using a sparse binary sensing matrix φ, i.e., sparse binary sensing."
"this paper describes the integration of wireless virtualization concepts to facilitate the management of a high evolutive virtual network environment. each device could be connected to several virtual providers using its multiple physical or virtual network interfaces. although many applications and protocols have been developed to offer specific features and tools for virtual wireless access point interface [cit], few works has been reported on providing virtualization support for wireless environment. we have developed that kind of software [cit] . this software permits the instantiation of virtual networks within a virtualized environment and the ability to configure each part. we have improved our previous work by taking into account the wireless network behaviour."
"in this paper, we use the fast iterative shrinkage-thresholding algorithm (fista) [cit], a popular method for cs recovery. the main contributions of this work are: (1) a novel cs approach that precludes large and dense matrix operations both at compression and recovery, and (2) several platform-dependent optimizations and parallelization techniques to achieve realtime cs recovery. to the best of our knowledge, this work is the first to demonstrate cs as an advantageous realtime and energy-efficient ecg compression technique, with a computationally light and energy-efficient ecg encoder on the state-of-the-art shimmer tm wearable sensor node and a real-time decoder running on an iphone (acting as a wbsn coordinator)."
"this platform could be used by several operators. they can share the deployed physical infrastructure by allocating the virtual available resources. the amount of allocated resources could be used to create different instantiation of virtual networks. depending on resources needed by each machine that compose the network, virtual machines have to be placed appropriately to an available resource location that satisfies the virtual network topology."
"for small data streams (less than 10) the time required to set up, handle and store the data becomes important and thus these points move away from the exponential trend line. other data points for the analysis of the data provide good fits for experiential curves as shown."
", m i is a nonzero scalar multiple of i . in other words, nonzero scalar multiples of each i are in the range of m and the dimension of the range of m is n, which is equivalent to m having full rank (c.f., [cit] ) ."
assumption 1: the damper force f d and spring force f s are continuous on a compact set ω and their first derivatives are bounded on the compact set.
"in this section, we describe eulerian and lagrangian representations of flow and present typical eulerian and lagrangian data sources that can be used for data-driven flow models."
"t, which is plugged into the system of equations in (22). note that matrix in (22) is also a nonlinear mapping for input . now that we have a system of equations with two nonlinear mappings and, solving the system of equations is challenging. as a workaround, we alternatingly fix either spatial or temporal parameters using their latest estimates and solve the system of equations for the other parameters."
"the rest of the paper is organized as follows. section 2 presents data-driven flow modeling with emphasis on its application to the gem for the guidance of auvs. section 3 provides an overview of mt to describe how an eulerian spatial map is constructed from lagrangian data collected by auvs. section 4 introduces a data assimilation method for the gem in which assimilation of both eulerian and lagrangian data is achieved in a unified framework to estimate the spatial and temporal parameters of the gem. section 5 validates our method through simulations and analyzes the results. lastly, sect. 6 concludes the paper with future directions."
"this paper is focused on the application of the lid technology in vehicles where the response time of the system is crucial for user acceptance. four languages were chosen for the experiments: english, finnish, german, and spanish. the databases were recorded within the scope of the eu project speechdat and for each language, the speakers were chosen from five dialectal regions. four high quality audio channels were recorded simultaneously. for this project, the close-talk microphone is selected. signals were stored as sequences of 16bit, 16 khz uncompressed."
"are the variables corresponding to the input and output, respectively. a ji and b jo are the fuzzy sets, and γ is the number of the rules. iii) fuzzy inference engine and defuzzifier: the fls adopted here with the gaussian fuzzifier, sum-product inference and centre-average defuzzifier is defined as:"
"extending our previous work [cit] b), this paper focuses on an estimation problem for data-driven computational flow models described as generic environmental models (gems) [cit] . after initialized by using prior information of the environment, possibly obtained from geophysical ocean models, the gem constructs a map of the environment to provide immediate navigational support for auvs. when a flow model is involved in auv navigation, the performance of auv navigation may, to a large extent, rely on the accuracy of the model. [cit], the authors analyze the influence of the geophysical ocean model on auv navigation and demonstrate that the error growth rate of the vehicle position depends on the accuracy and resolution of the model. this concept can also be extended to data-driven models. to improve the model accuracy, observation data can be incorporated into the estimation of the ocean state or parameters in the model, which is generally known as data assimilation [cit] ."
"remark 1 assimilation of eulerian and lagrangian data into the gem helps to reduce uncertainties in the noisy data. for such information fusion to be achieved in a unified framework of data assimilation, we should have a clear understanding of the covariance of noise in both data. since eulerian data are collected at a fixed location, the covariance of noise in eulerian data can be obtained from historic data or geophysical ocean models. however, since lagrangian data are collected from mobile sensors, the covariance of noise in lagrangian data is difficult to identify. we assume that the noise in the field is spatially uniform with respect to covariance and the noise in lagrangian data is ergodic with respect to covariance. then, the covariance of noise in lagrangian data can be obtained by computing the covariance of noise in eulerian data averaged over time at a fixed location. we also consider local correlation of noise covariance in the field. we assume that each grid cell in the eulerian spatial map converted from lagrangian data is identically correlated with its neighboring grid cells with respect to noise covariance. that is, noise covariance matrix k is const r u c t e d s u ch t h a t k, (p,p)"
"inspired by the previous discussions, we will present a new adaptive fuzzy control design for active suspension systems with input time delay and unknown nonlinearities. we first introduce an input time delay compensation strategy in the adaptive control design, such that the effect of input delay on the closed-loop control system can be remedied. then, a fls is adopted to accommodate the uncertainties and nonlinearities. to guarantee better estimation of unknown fls weights and thus improve the control response, a new parameter estimation algorithm containing information of the derived parameter error is developed to online update the fls weights. this can be obtained by designing a set of auxiliary filtered matrices on the measurable system dynamics to derive a new leakage term, which can be superimposed on the gradient algorithm. in this new parameter estimation framework, accurate and finite-time convergence properties of the estimated fls weights can be guaranteed. a lyapunovkrasovskii functional is constructed to prove the stability and convergence of the controlled system. a dynamic simulator is built by using a professional vehicle simulation software, carsim 8.1, together with matlab/simulink, and comparative simulations are given to show the efficacy of the proposed approach."
"to solve the equations in (19) for flow, we first need to determine ( ) based on the knowledge of vehicle trajectories. however, because of limited localization capabilities of auvs, their underwater trajectories are often unknown and thus must be estimated before solving (19). therefore, flow field mapping through mt (see algorithm 1) is achieved through an iterative process consisting of two key steps: trajectory tracing and flow field estimation. [cit] for details about trajectory tracing."
"autonomous underwater vehicles (auvs) are proven versatile instruments for ocean sampling and monitoring [cit] . however, environmental changes such as ocean currents often hinder the survey performance of auvs. to cope with environmental changes, an auv can employ environmental models (e.g., [cit] for localization). for geophysical flow, typical regional ocean models (e.g., [cit] numerically solve partial differential equations (pdes) for a large spatial area with long-term prediction. however, their high complexity makes solving these pdes computationally expensive. to reduce the computational burden, regional ocean models typically solve these pdes with large spatial and temporal resolutions that may not be suitable for auv planning and control. data-driven flow models [cit] can provide high spatial and temporal resolutions computationally fast for a small spatial area the research work is supported in part by onr grant n00014-16-1-2667 and nsf grants oce-1559475 and oce-1032285. with short-term prediction. hence, data-driven flow models are suitable for local guidance of auvs."
"the motion-integration error has been used to estimate a flow field in the vicinity of an auv that does not measure ambient flow directly. for example, the underwater glider, a buoyancy-driven auv [cit], computes a spatially and temporally averaged flow estimate from the motion-integration error along the vehicle trajectory traveled over one subsurface interval [cit] ). this method is very efficient in computation, and the glider incorporates this estimate into navigation to reduce the motionintegration error for the next subsurface interval. [cit] presents a similar way of estimating a flow velocity to identify model parameters for a time-invariant flow field. however, the effectiveness of this method"
"the implementation of the above control (23) is straightforward as shown in (24) . the first term from the fls,ŵ t φ, is used to address the lumped unknown dynamics, and the second term is the error feedback calculated based on (15) . to achieve satisfactory suspension performance and estimation convergence, the filter coefficients κ, should be set as small positive constants. moreover, the feedback gain k s, filter coefficients k, λ, and adaptive learning gains γ, l should be selected properly. in general, large gains k s, k, λ contribute to improving the suspension error rate, while too large gains may make the system sensitive to noise and disturbances. moreover, large learning gains γ and l can improve the parameter estimation convergence speed, but can result in oscillations of control actions."
"a quarter-car active suspension system with uncertain dynamics and control input time delay is studied in this paper, whose schematic is given in fig.1 . the variables in fig.1 are defined as: m s and m u represent the sprung mass and unsprung mass, which denote the vehicle chassis and the mass of tire assembly. f d is the damper force and f s is the spring force. f t and f b refer to the elasticity and damping forces of the tire. z s and z u are the vertical displacements of the sprung and unsprung masses, respectively. z r is the road excitation displacement and u is the control input of the active suspension system. τ is a bounded positive constant input delay, which is induced by the adopted actuator. the differential equations for describing the studied quartercar active suspension system's motion behavior can be established based on the newton's law as:"
"in this section, we provide numerical simulation results to exemplify the proposed control method in terms of suspension displacement response and ride comfort. to cover more realistic applications, a professional simulation software for vehicle systems, carsim (version 8.1), is employed to generate realistic driving road conditions collected from the realistic experimental data, which are then used to build a combined dynamic simulator together with matlab/simulink."
"as described in the previous section, eulerian and lagrangian data are provided on different time scales, leading to different time scales for eulerian and lagrangian data assimilation (i.e., every time step for eulerian data and every th time step for lagrangian data). suppose eulerian data are available at time step k and the latest estimates of the spatial parameters were computed at time step ⌊k∕ ⌋ . we fix all the spatial parameters k using their latest estimates ⌊k∕ ⌋ and estimate the temporal parameters k . now, in state vector, is the only unknown, formulating the filtering problem that is linear in ."
"to facilitate assimilation of lagrangian data collected by auvs into the gem, we convert the data into an eulerian spatial map through mt so that lagrangian data assimilation can be achieved together with eulerian data assimilation in a unified framework. in this section, we first study how the flow field affects the horizontal motion of auvs. then, by exploiting multiple auvs traveling in a domain, mt is formulated with spatio-temporal discretization, allowing to account for both spatial and temporal variability of flow. lastly, the flow field estimation problem through mt and its parameterization are introduced."
"the aim of this paper is to develop a new adaptive control scheme for active suspension system (3) with unknown forces and input delay, such that the following suspension performance requirements can be addressed:"
the differences in the performance of the three gems show that the gem combined with the proposed unified framework can favorably account for the spatial and temporal variations of flow by assimilating both eulerian and lagrangian data.
"and k and k are the vectors of the process and observation noise, which are zero mean gaussian with known covariance k and k . for the system in (34) and (35), we derive a kalman filter as follows."
"of the ith vehicle within (d j, t ) is constant, this flow setting constructs the linear trajectory in (d j, t ), leading to the piecewise linear trajectory over (d, t) . we assume that vehicle heading"
"the spatial characteristics of flow is modeled using gaussian rbfs as discussed in the previous section. to model the temporal characteristics, we decompose coastal ocean flow into tidal and non-tidal components. tidal flow is forced by a superposition of known tidal constituents, each of which has a specific frequency related to astronomical phenomena. therefore, to model temporal variation of tidal flow, we define the following temporal basis functions by using a series of sinusoidal functions:"
"the gem. note that flow field estimation (algorithm 2) in mt is deterministic. hence, by incorporating lagrangian data into the data assimilation framework, we can reduce the uncertainties in lagrangian observation data."
"instead of multiple vehicles, we can also use a single vehicle for mt. in this case, we may not have sufficient information of vehicle trajectories for mt. therefore, estimation of the spatial and temporal parameters of the nonlinear flow model (1) through mt can be challenging. as a workaround, we can fix the temporal parameters of mt using the temporal parameters of the gem and use mt to estimate the spatial parameters of mt only. then, an eulerian spatial map can be constructed by using the temporal parameters of the gem and the spatial parameters of mt."
"remark 7: the main aim of the current study is to propose a new compensator to address the input delay in the active suspension systems, which can be incorporated into adaptive fuzzy control design with guaranteed convergence and thus enhanced suspension response. moreover, it is noted that different actuators (e.g. electrical motors, hydraulic cylinder) can be used in the active suspension systems. hence, the dynamics of actuator used to generate the required force are not considered explicitly in this paper."
"recent successful techniques in both acoustic-phonetic and phonotactic levels are typically based on i-vectors [cit] . an ivector is a compact representation of characteristics of a speech signal, which was originally developed for speaker recognition [cit] and has also shown promising performance for lid (e.g., [cit] ). some post-processing techniques are usually required to compensate undesired session variabilities in the ivector space. linear discriminant analysis (lda), withinclass covariance normalization (wccn), and probabilistic linear discriminant analysis (plda) are the most commonly used techniques in speaker recognition [cit] . however, some of these techniques may not be such effective for lid due to the limited number of language classes [cit] ."
"the vertical displacement and acceleration of vehicle body should be minimized under external road disturbances, since these two variables are widely regarded as the key performance indices to evaluate the vehicle ride comfort. hence, the proposed control should have capability of attenuating vibrations and/or shocks as much as possible and maintaining the stabilization of vehicle body. this requirement can be described by:"
"two forms of i-vectors are considered as inputs to dnns, raw i-vectors and session-compensated i-vectors. lda and wccn are two commonly used techniques for session variability compensation among i-vectors. although lda performs better than wccn for the lid application when cosine scoring is used, we will use only wccn session-compensated i-vectors as the inputs to dnns. this is because the number of the language classes is very few in this application and, therefore, the maximum number of meaningful eigenvectors will be also few (number of classes minus one). we implemented different dnn architectures with lda-projected i-vectors as inputs but no gain was observed. the use of raw i-vectors is advantageous as no language-labeled background data is required."
"v. conclusion in this paper, we proposed a new adaptive fuzzy control approach for uncertain nonlinear active suspension systems subject to input time delay and unknown nonlinear dynamics. an appropriate input delay compensation strategy is first introduced to obtain a delay-free term to eliminate the effect of input delay in the control system. then an fls was used as the function approximation, and incorporated into the adaptive control design, such that the uncertain dynamics and unknown nonlinearities can be addressed. unlike existing conventional adaptive control designs, a novel finite-time parameter estimation algorithm driven by the parameter estimation error was developed to online update the fls weights, such that the estimated fls weights converge to a small neighbourhood around their ideal values in finite-time. the closed-loop system stability analysis was carried out based on the lyapunovkrasovskii functional. simulation results in terms of a dynamic simulator consisting of a commercial software, carsim, and matlab illustrated that the proposed control method can handle both the input delay and uncertainties, and the new learning method suggested in this paper also obtained superior performance in terms of both control and parameter estimation responses. future work will focus on exploiting the proposed adaptive control for active suspension systems subject to timevarying input delay and unknown actuator dynamics."
"for lagrangian data, the two vehicles always attempt to criss-cross around the mooring by heading towards the mooring, but to make a wide spatial coverage to capture the spatial variability of flow, the actual heading is adjusted by adding 18"
"the basis functions of the gem can be constructed by using prior information of the environment obtained from existing ocean models or historic observation data and its parameters can be trained on nowcast or forecast data from the ocean models. ocean models are typically updated on a daily basis. however, to support auv navigation in dynamic environments, the model parameters should be updated more frequently. during the time between the ocean model updates, one may use other observations such as data from auvs or moorings to update the parameters of the gem. this process can be considered as simplified data assimilation for data-driven flow models. details of this data assimilation process for (1) is discussed in sect. 4."
"a deep neural network (dnn) architecture has been proposed in this paper for i-vector language identification (lid) of short utterances recorded in cars. the computational complexity and the response time of the lid system is important in this application. in order to have the highest accuracy with the minimum response time of the system, signals with different durations from less than 2s to higher than 3s with the average duration of 3.8s have been analyzed. it has been shown that for test signals with durations between 2 and 3s the proposed dnn architecture with raw i-vectors as inputs outperforms gmm-ubm and i-vector/lda baseline systems by 37% and 28%, respectively."
"for geophysical ocean models, lagrangian data assimilation mostly employs passive drifters or floats. [cit], a method for lagrangian data assimilation is developed by using the augmented state that combines the eulerian ocean state with the positions of lagrangian observations traced by using advection equations. [cit] computes lagrangian velocity as the difference between the positions of a drifter over the position update interval. then, this velocity is interpolated to be assimilated into a flow model. however, since auvs are not passive but 'controlled', these methods using the positions of passive drifters may not be easily applicable to data-driven flow models combined with observations from auvs. in addition, data assimilation for geophysical ocean models with high complexity is computationally expensive. [cit], flow field estimation and data assimilation have been studied for auv path planning using a computational fluid dynamics model and flow measurements from auvs on the fly. however, auvs typically have a limited capability of flow measurement. in this paper, we present a framework of computationally efficient data assimilation for data-driven flow models using auvs with no direct flow measurement and limited underwater localization capabilities."
"this paper has presented a unified framework for assimilating both lagrangian and eulerian data into data-driven computational flow models described as generic environmental models (gems). in the framework, lagrangian data collected by autonomous underwater vehicles (auvs) are converted into an eulerian spatial map through the motion tomography (mt) method. this conversion has allowed for lagrangian data assimilation to be achieved together with eulerian data assimilation in the unified framework. leveraging different spatial and temporal scales of eulerian and lagrangian data, eulerian data are assimilated to update the temporal parameters of the gem and lagrangian data to update the spatial parameters. assimilation of both eulerian and lagrangian data in the unified framework leads estimation of the spatial and temporal parameters in the gem to a nonlinear filtering problem. to solve this nonlinear problem, two linear sub-filters are derived for spatial and temporal parameter estimation, respectively and to verify the convergence of the filters, the observability is analyzed. lastly, the paper has demonstrated that the proposed data assimilation framework combined with the gem can improve auv navigation through simulations using a double-gyre flow field and a flow field constructed from real ocean surface flow observed by hf radar. since geophysical ocean models incorporate lagrangian data collected by auvs, the proposed framework may be a helpful analysis tool for geophysical ocean models. future work will study the design of vehicle trajectories so that we can minimize the uncertainty in the flow field constructed from lagrangian data and maximize the accuracy of the flow model."
"three kinds of data have been selected: spontaneous sentences spoken as answers to specific questions, phonetically rich sentences which are read sentences from a phonetically balanced corpus, and phonetically rich words which are a set of words used to enrich the phonetic balance of the corpus. table 1 shows the number of utterances and the total signal duration in hours for each set. the training and development data are around 11 hours for finnish and german and 6 hours for english and spanish. figure 1 shows the histograms of the duration of the utterances. it can be observed that there is a maximum around the first second, corresponding to the phonetically rich words. there is another maximum in the third second, related to phonetically rich sentences. finally, the tail of the histogram is due to the longer spontaneous utterances."
"dnns are feed-forward neural networks with multiple hidden layers. they are trained using a discriminative backpropagation algorithm given class labels of input vectors. the training algorithm tries to minimize a loss function between the class labels and the outputs. for classification tasks, cross entropy is often used as the loss function and the softmax is commonly used as the activation function at the output layer [cit] . typically, the parameters of dnns are initialized with small random numbers. however, it has been shown that there are more efficient techniques for parameter initialization, wellknown as pre-training, like unsupervised auto-encoders or restricted boltzmann machines (rbm) and supervised layer by layer training [cit] . the effectiveness of the pre-training technique depends typically on the amount of available data. it is possible to update the parameters of the network after processing each training example, but it is often more efficient to divide the whole input data (batch) into smaller size batches (minibatch) and to update the parameters by averaging the gradients over each minibatch. the parameter updating procedure is repeated when the whole available input data are processed. each iteration is called an epoch."
"where pmiss(li) is the probability that an utterance spoken with the target language li is misclassified, and n is the total number of languages. table 2 summarizes the results for all the techniques in four categories based on the test signal durations: less than 2s, between 2 and 3s, more than 3s, and all durations. the first two categories are more interesting because the decision should be made fast in this application. the dnn results are reported based on the proposed architecture of fig. 2 with 3 hidden layers (400-512-256-128-4). the network is trained with training signals of all durations. as it can be seen in this table, among i-vector baseline systems, i-vector + lda outperforms the two others with a big difference in all categories. both i-vector-dnn systems show superior performance compared to i-vector + lda baseline system. however, except for the test signals with longer duration than 3s, dnns with raw i-vectors perform better than with wccn session-compensated i-vectors. this is an advantage where no language-labeled background data is available, e.g., [cit] . the frame-based gmm-ubm baseline system works better than other systems only for test signals shorter than 2s. however, the accuracy is still high in comparison to other categories. furthermore, table 2 implies that the shortest test signals for which all the techniques achieve adequate performance are between 2 and 3s. for these test signals, the proposed dnn architecture with raw input i-vectors achieves 37% and 27% relative improvements compared to gmm-ubm and ivector + lda baseline systems, respectively. in fact, the combination of i-vectors and the proposed dnn architecture meets the goal of this application, that is high accuracy and fast decision. for test signals longer than 3s, i-vector+wccn+dnn system works the best and for signals with any duration, the ivector+dnn system outperforms all other individual systems with 25% and 10% relative improvements comparing to gmm-ubm and i-vector+lda baseline systems, respectively. finally, the combination of the lid systems in the score level shows that a 16% relative improvement can be achieved for the signals with longer duration than 3s when the i-vector + lda baseline and the i-vector + dnn systems are fused. additionally, a slightly improvement is observed for the test signals with all durations when the i-vector + dnn system is combined with both gmm-ubm and i-vector + lda baseline systems. for score fusion, the scores of different systems are simply summed."
"suppose lagrangian data are available at the kth step. since lagrangian data are collected over relatively a long time period, the temporal scale of lagrangian data does not match that of eulerian data. to resolve this issue, we construct an eulerian spatial map for time step k by using the estimated parameters through mt using algorithm 2 and assimilate the constructed map into the gem. suppose that the latest estimates for the temporal parameters were computed at the (k − 1) th step. in contrast to temporal parameter estimation, we fix all the temporal parameters using their latest estimates k−1 . then, the subsequent filtering problem is linear in ."
"in this subsection, we will focus on the stability analysis of the proposed control system with control action (24) and finitetime parameter estimation algorithm (31) . we first present the following lemma:"
"in this paper, the gem is constructed by using spatial and temporal basis functions along with their corresponding parameters. assimilation of both eulerian and lagrangian data into the gem through the unified framework involves a nonlinear filtering problem to estimate the spatial and temporal parameters of the gem. we design a filtering method in which the spatial and temporal parameters of the gem are individually estimated through assimilation of lagrangian and eulerian data, respectively. we analyze the observability to check the convergence of the proposed filtering method. the gem combined with the proposed data assimilation framework can provide effective navigational support for auvs in real time. to demonstrate its capability for auv guidance, we compare three gems assimilating (i) both eulerian and lagrangian data, (ii) only eulerian data, and (iii) only lagrangian data, respectively. the three gems are used to guide simulated station-keeping vehicles in the following two flow fields with different characteristics: (i) a double-gyre flow field and (ii) a flow field constructed by using real ocean surface flow observations from high-frequency (hf) radar."
"open access this article is distributed under the terms of the creative commons attribution 4.0 international license (http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the creative commons license, and indicate if changes were made."
"because of the nonlinear observation equation, parameter estimation for the system (29) and (31) becomes a nonlinear filtering problem. to deal with this problem, we decompose it into two linear sub-filtering problems for (i) spatial parameter estimation and (ii) temporal parameter estimation. for each sub-filtering problem, we fix either set of spatial or temporal parameters and estimate the other set of parameters. because of their different spatial and temporal sampling scales, eulerian data better account for temporal variability of a flow field while lagrangian data better for spatial variability. therefore, we use eulerian data to update temporal parameters and lagrangian data to update spatial parameters. this decomposition of the nonlinear filtering problem is valid provided that the spatial and temporal characteristics of flow can be individually estimated from lagrangian and eulerian data, respectively. in the following sections, we present our filtering method and analyze the observability to verify the convergence of the derived filters."
"then, coastal ocean flow is modeled as the summation of the tidal and non-tidal flow such as are chosen to construct the temporal basis functions for the tidal and non-tidal flow, respectively. only a single rbf is used as the spatial basis function for the fixed coordinates. in this example, each product of the spatial and temporal parameters is considered as one spatio-temporal parameter and the parameters are estimated using linear least squares. after the flow model is constructed, predictions are generated from the model for the next 48 h. please note that for the rest of the paper, we discuss how spatial and temporal parameters in (1) can be estimated separately through mt and data assimilation."
"very recently, fischer and dixon developed a saturated tracking controller [cit] for uncertain nonlinear systems with input delay. this idea has been further explored for nonlinear systems with time-varying delays in the subsequent work [cit] . in this framework, a constructive predictor is proposed to address the effect of input delay, such that it can guarantee the uniform ultimate boundedness of the tracking error even in the presence of input time delay and uncertain dynamics. hence, this paper will further study the possibility of incorporating this new technique into adaptive control for uncertain active suspension systems with input time delay and nonlinearities."
"this section presents an example of the gem in the form of (1) that represents coastal ocean flow [see our previous work [cit] for details]. note that by expanding (1), the equations become let us define x, (m,n) and y, (m,n) as the product of the mth spatial and nth temporal parameters in the x and y components, x,m x,n and y,m y,n, respectively. then, by defining"
"language identification (lid) is the automatic process of identifying a language spoken in a speech utterance. lid systems use typically one of these two levels of information: acoustic-phonetic or phonotactic [cit] . the acoustic-phonetic level statistically represents the characteristic phonemes of each language by a set of acoustic parameters, while the lexicalphonological rules of each language are taken into account in the phonotactic level to connect phonemes and form words."
"this section validates the proposed method by simulating one stationary and multiple mobile sensors that collect data in (i) a double-gyre flow field and (ii) a flow field constructed by real ocean surface flow observations from hf radar. compared to a double-gyre flow field which is relatively smooth and has a periodic spatio-temporal pattern,"
"the successful use of dnns for discriminating between target and impostor i-vectors in speaker verification [cit], motivated the authors to make use of dnns for the lid multi classification task. as few i-vectors are available for each target class in speaker recognition and, therefore, the amount of target and impostor i-vectors are highly unbalanced, dnns need some tricks for training to be efficient. in this application, however, we do not have this problem. figure 2 shows the architecture of dnns we have proposed in this work. the inputs are i-vectors and the outputs are the language class posteriors. the softmax and sigmoid are used as the activation functions of the internal and the output layers, respectively. in order to gaussianize the output posterior distributions, we have proposed to compute the output scores in log likelihood ratio (llr) forms as,"
"in this subsection, we will further prove the finite-time convergence of the estimated fls weightsŵ with adaptive law given in (31) and (32) . the main result of this subsection is given as follows:"
"t is the vector of spatial and temporal parameters, which is the input to the nonlinear function in (26). by stacking f (j, ), we con-"
"to investigate the rms errors of predicted flow by the gems during the validation period, we show the timeseries of the magnitude of the spatial rms errors and the spatial distribution of the rms errors in figs. 12 and 13, respectively. for gem-el and gem-e, the mean rms errors shown in the figures are very close. however, when a flow model is used to guide a vehicle, this small difference may cause a significant drift of a vehicle as shown in fig. 11 . as a result, with lower prediction error, gem-el can guide a station-keeping vehicle to stay within the domain. even though the real flow observed by hf radar is complex, the results still show the promising performance of the proposed method and demonstrate the improved guidance of auvs. the operational conditions of auvs used in the simulation are motivated by underwater gliders which are robust sensor platforms widely used in the oceanographic community. typical horizontal through-water speed of underwater gliders is 0.25-0.35 m/s. as observed in this section, real flow is very complex and dynamic, and can significantly affect the navigation of auvs, especially slowly-moving auvs such as gliders. therefore, data collected by gliders are not necessarily synoptic and can thus convolve spatial and temporal representations of ocean features as they are sampled. outside of its capability for the guidance of auvs, the data assimilation framework presented in this paper could serve as a useful tool for analysis and interpretation of data from auvs through rapidly evolving features in oceanographic studies."
"a mooring is a stationary oceanographic instrumentation anchored on the sea floor with a collection of sensors. a mooring with the current meter can measure ocean currents typically on an hourly basis at different depths from near the sea surface to near the sea bottom. although a mooring provides eulerian data on fast time scales that are useful for time series analysis and interpretation, since it is fixed at one location, its flow data provide insufficient spatial variability for auvs unless a set of moorings are installed in a dense network or array."
"since the flow in this region is very dynamic and complex, we have 3 auvs criss-crossing around the mooring to collect data. the heading of the vehicles is always towards the mooring with 18"
"the structure of this paper is organized as follow: modeling of vehicle active suspension system and preliminary knowledge are given in section ii. the proposed input time delay compensation strategy, adaptive fuzzy control and the stability analysis are presented in section iii. section iv provides comparative simulation results. conclusions are given in v."
"it is shown in (30) that the online calculated vector o based on the derived variables r, φ, s, u,ñ with filter operations given in (26) and (27) contains information of the estimation errorw . therefore, the use of this derived variable o in the adaptive law can enhance the performance of parameter estimation. now, the following finite-time parameter estimation algorithm p roj(·) with projection operation can be designed to online update the fls weights:"
"in this section, we demonstrate the proposed data assimilation framework using real ocean surface flow data observed by hf radar off the coast of georgia (see fig. 9 ). compared to the double-gyre flow in the previous section, the real flow observed by hf radar has non-smooth and erratic behaviors with stronger flow, and the features in real oceanic conditions could vary much more in the spatial scale (e.g., 10-500 km for mesoscale eddies and 100-200 km for the gulf stream current)."
"an ocean surface flow field observed by hf radar is nonsmooth and erratic. for the construction of the gem and the parameterization of mt, we use the flow model in (1)."
"the rest of the paper is organized as follows. section 2 describes the scenario used for experiments. section 3 gives a brief overview of the background. section 4 presents the proposed dnn architecture for language i-vector classification. section 5 discusses the experimental results, and section 6 concludes the paper."
"the collected eulerian and lagrangian data are assimilated into the gem through the proposed unified data assimilation framework. to show that the performance of the proposed method combined with the gem, we compare three implementations of the gem through auv guidance: (i) gem-el, assimilating both eulerian and lagrangian data, (ii) gem-e, assimilating eulerian data only, and (iii) gem-l, assimilating lagrangian data only."
"for dnn experiments, the proposed architecture in section 4 is implemented with both raw and wccn sessioncompensated input i-vectors. in both cases no normalization is applied on i-vectors prior to feeding to dnns. the proposed dnn architectures are trained with the learning rates of 0.07 and 0.04 and the number of epochs of 500 and 200 for raw and wccn compensated i-vectors, respectively. momentum and weight decay are set, respectively, to 0.9 and 0.001 for all dnns."
"observation data incorporated in data assimilation can be categorized into eulerian and lagrangian. eulerian data are framed with a fixed grid in space and time, and lagrangian data are obtained while moving with the flow over time. since most flow models are eulerian, assimilation of eulerian data into the model is relatively straightforward. however, lagrangian observations such as data collected by auvs carry time-integrated information along the trajectory and have both direct and indirect connections with the state variables and observation noise (e.g., observation model error and sensor noise) of flow models [cit] . therefore, assimilation of lagrangian data raises difficulties and requires an additional framework in comparison to eulerian data assimilation."
"the gem is a class of data-driven computational models that estimate a map of the environment using the data collected by auvs to support their navigation [cit] ). the gem is named \"generic\" in the sense that it is independent of boundary conditions that are required to compute the states of typical pde-based models. this section introduces the structure of the gem and the modeling of coastal ocean flow as an example of the gem."
"remark 2: in practical operation scenarios of vehicle suspension systems, the vertical displacement x 1, velocity x 2 and accelerationẋ 2 can be measured by the configured signal acquisition devices, e.g. displacement/acceleration sensor. hence, the system state variables x 1, x 2 andẋ 2 in (3) are assumed to be measurable and will be used in the control implementation."
"step 2: measuring the system state x i, choosing positive constants k, λ and feedback gain k s to design controller u based on (23);"
"in the baseline system proposed by the national institute of standards and technology (nist) [cit], all the language ivectors are centered and whitened. afterwards, for each language the average of the training i-vectors is considered as the average-language i-vector. then the cosine distance between the average-language i-vector and test i-vectors is computed as final scores. if a language-labeled background dataset is available, lda has been shown to be effective for session variability compensation before scoring [cit] ."
"the eulerian representation describes flow as vectors at fixed coordinates in space and is used as a general method for ocean circulation modeling. this concept can be visualized by a spatial map of a flow field, in which flow vectors are assigned to fixed spatial locations. in the lagrangian representation, the position of a particle is tracked as it moves along flow vectors in space and time, and the path traced out by this particle is called a pathline. while a pathline describes the trajectory of a single particle along the flow vectors in both space and time, the flow vectors represent instantaneous lines of motion of particles at each of the assigned spatial locations at a fixed time (see fig. 1 )."
"based on the results reported on table 2, we can recommend the following lid systems for test signals of different durations: gmm-ubm for shorter than 2s, i-vector + dnn for longer than 2s and shorter than 3s, fusion of i-vector + lda and i-vector + dnn for longer than 3s, and fusion of ivector + lda, i-vector + dnn, and gmm-ubm for all durations. table 3 compares the performance and the size of the proposed dnn architecture with some other dnn architectures. as it can be seen, the proposed architecture with 3 hidden layers achieves the best accuracy in the first two categories. among these dnn architectures, the 2-hidden-layer dnn with hidden layer size of 512 works slightly better than the proposed architecture for signals longer than 3s, but with the cost of bigger size and, consequently, higher computational complexity. figure 3 compares the proposed i-vector + dnn system with the i-vector + lda baseline system in terms of detection error tradeoff (det) curves for test signals of all durations. det curves are obtained for each language versus all other languages. in other words, each language is considered as the target class and all other languages as the non-target one. each det curve shows how well the target and non-target languages are distinguished by the lid system. as it can be seen in this figure, the proposed i-vector + dnn system outperforms the ivector + lda baseline system for all languages in all operating points, resulting in a 7-38% relative improvements in terms of equal error rate (eer)."
"flow field modeling and estimation have been studied in oceanography, fluid dynamics, and marine robotics. typical regional ocean models for geophysical flow have high computational costs and coarse spatial and temporal resolutions (e.g., on the order of kilometers and output on the order of hours to days), which are not suitable for auv planning and control. by compromising detailed physical insights, datadriven flow models obtain flow estimates computationally faster and in higher resolution than pde-based geophysical ocean models. this section presents a data-driven approach for flow modeling for the guidance of auvs."
"even though the system in (32) and (33) and the system in (34) and (35) are uniformly completely observable, the convergence rate and accuracy of the derived filters may depend on how observable the systems are. [cit], that evaluate the degree of observability or unobservability."
"typical spectral features for lid are shifted-delta cepstral (sdc) features [cit] which capture the speech dynamics over a wider range of speech frames than the first-and second-order derivatives of mel-frequency cepstral coefficients (mfcc). the recent so-called dnn bottleneck features have shown superior performance compared to sdc features (e.g., [cit] ), but they are discarded in this task because of the high computational complexity. sdc features can be modeled with different techniques. the i-vector representation is one of the successful techniques which will be described briefly in the following. nevertheless, since we cope with very short utterances in this task, we will also consider the conventional gmm-ubm as a potential approach which is well-known to perform better than i-vectors dealing with very short signals. at the end, dnns can be used to model discriminatively the i-vector space of languages which is one of the goals of this paper."
"several recording conditions were defined: car stopped by motor running, car in town traffic, car moving at a low speed with rough road conditions, and car moving at a high speed with good road conditions. for the experiments, the signals were taken from all these conditions with windows closed, roof window closed and radio off."
", and k and k are the vectors of the process and observation noise, which are zero mean gaussian with known covariance k and k . for the system in (32) and (33), a kalman filter can be derived as where ̂ k is the optimal estimate of k ."
can be measured by a compass or estimated with small bounded error. the influence of the vehicle heading error on mt is analyzed in our previous work [cit] . intervals from lagrangian data boils down to solving (19) for .
"for each language, three sets of speakers were defined by selecting randomly from gender, age group and recording conditions: 150 speakers in the training set, 75 speakers in the development set and 75 speakers in the test set."
"as the response time of the lid system is important in the car, the computational complexity of the classifier should also be taken into account. therefore, we have proposed to choose the size of the first hidden layer as the lowest power of 2 greater than the input layer size. from the second hidden layer towards the output, the size of each layer will be half of the previous layer. for example, the configuration of a 3-hidden-layer dnn will be as 400-512-256-128-4, where 400 is the size of the input i-vectors and 4 is the number of language classes. it will be shown in section 5 that, in this way, we can decrease the computational complexity to a great extent while keeping the performance."
"this section reviews the following three typical data sources: moorings, hf radar systems, and auvs. these data sources have different complexities and ranges, and their observation data can be complementary to understand the spatial and temporal characteristics of flow."
we consider 2-dimensional flow fields for simplicity of presentation. let subscripts x and y denote the x and y components in ℝ 2 and define a flow vector as
"t he rapid development of automotive industry imposes more stringent requirements on the ride comfort and driving safety. in modern vehicle systems, suspension systems, as one of the most important vehicle chassis components, mainly affect the ride comfort, vehicle manoeuvrability and safety of drivers and occupants. typically, suspension systems can be grouped into three categories: passive suspension systems, semi-active suspension systems and active suspension systems. unlike passive and semi-active suspension systems, active suspension systems are equipped with extra actuation devices to provide or dissipate energy induced into the systems, thereby eliminating the vibrations and/or shocks transmitted to the vehicle body from irregular road roughness. in this respect, active suspension systems have been recognized as a promising pathway to achieve better suspension response and thus greatly improve the ride comfort and driving safety. therefore, active suspension system construction and the associated control designs have attracted significant attentions from both industrial and academic communities [cit] ."
"where s j (k) is the fitness of particle p j (k), d(i) is the initial value of particle p j (k), andd (p j,k) (i) are the predicted values of trained particle p j (k). the fitness of p jbest (k) is expressed as"
"the associate editor coordinating the review of this manuscript and approving it for publication was luca chiaraviglio. method (rbf) [cit], and the element-free galerkin method (efg) [cit], have been investigated. as one of the most popular methods for solving electromagnetic equations, efg has been demonstrated to have good robustness and stability compared to other element-free methods [cit] . however, the basis functions cannot satisfy the kronecker delta property, and the dirichlet boundary conditions (also referred to as essential boundary conditions) are not directly imposed [cit] . the computational complexity of efg is not obviously less than that of fem. moreover, improved radial basis function collocation methods for solving electromagnetic problems are sometimes implemented [cit] . by these methods, shape functions based on the radial basis function are established, and accurate solutions are obtained. however, the coefficient matrix of the rbf collocation method is full and asymmetric, which restricts its application for solving complex electromagnetic problems. rbf mixed with fem (rbf-fem) [cit] and natural boundary conditions are the simple treatment as fem, but the coefficient matrix of basis functions is not positive definite, and the obtained solutions are not always unique [cit] ."
"the two major components of fem are galerkin fem and variational fem. natural boundary conditions are directly satisfied, so it is easy to handle the complex boundary conditions with galerkin fem and variational fem."
"the minimization procedure is implemented by dividing into m small subdomains called elements, and representingφ in each element by means of basis functions defined over the element,φ"
"where l is a differential operator, f is the applied source or forcing function, and φ is the unknown quantity. in finite element analysis, the variational formulation to determine the unknown φ is expressed as:"
"the radial basis function (such as a gaussian or mq function) has the gibbs phenomenon on the discontinuous boundary conditions of electromagnetic equation [cit] . therefore, the method in this paper cannot solve the following electromagnetic equation with discontinuous boundary conditions:"
"where y is the output vector of rbf networks, ω is the weight matrix, and is the matrix of rbf nonlinear mappings performed by the hidden layer. fig. 1 shows the architecture of rbf networks."
"natural boundary conditions can be directly satisfied. moreover, dirichlet boundary conditions ∂ 1 ∩ ∂ and ∂ 2 ∩ ∂ are imposed by deleting the rows and columns of the matrix k corresponding to the nodes on the dirichlet boundaries and modifying b in (8)."
"first, the unknown function of the integrand function is approximately expanded by the weighted functions, and a system of equations is constructed. second, the system of equations is solved, and the undetermined coefficients u i are obtained. finally, the solved u i is inserted in the expanded formula (4), and the approximate solutions are obtained. the weighted function is known as a basis function. the method, which is galerkin fem, deduces the weak integral form in the computational process, and the natural boundary conditions are directly satisfied. therefore, it is easy to handle complex boundary conditions using galerkin fem. as an example, consider the typical 2-d electrostatic problem that is combined with natural and essential boundary conditions (also referred to as dirichlet boundary conditions):"
"t . therefore, the weights ω can be obtained by solving the linear system of equations (28) . the procedure of the proposed method for linear electromagnetic equations is shown in algorithm 1."
"four examples will prove the efficiency of the proposed method for solving the electromagnetic equations. they are a linear electromagnetic equation with continuous boundary conditions, a linear electromagnetic equation with discontinuous boundary conditions, a nonlinear electromagnetic equation, and a multimedia electromagnetic equation with complex boundary conditions. these examples have known analytical solutions. we tested the efficiency of the proposed method by computing the maximum error, the absolute error, and the root mean squared error."
"obviously, the electromagnetic equation (29) with discontinuous boundary conditions is transformed to an electromagnetic equation with continuous boundary conditions, so it is directly solvable by the method proposed in this paper."
"the key point of rbf networks is to estimate the weights ω, after which the solutions of the electromagnetic equation can be obtained. first, to easily deal with the complex boundary conditions, the governing equation of the electromagnetic problem is transformed to a weak integral form in the fem framework. then, in the ls-svm model, the weak integral form and dirichlet boundary conditions are considered in the quadratic programming problem. the different radial basis functions [such as the multiquadrics (mq) function] can be used to solve the electromagnetic equations. considering (22) and transforming (21) to the weak integral form, we obtain the following equation:"
"all electromagnetic problems are expressed as maxwell's equations with different boundary conditions and media materials. numerical methods are widely used for solving complex electromagnetic problems. to clearly illustrate the method proposed in this paper, we briefly describe the formulations of rbf networks, fem, and ls-svm. the expansion of approximate solutions is written with the same structure as the rbf network method. in the fem model, the governing equations are transformed to weak integral forms and variational formulations. then the weak integral forms, variational formulations, and essential boundary conditions are considered as quadratic programming problems in the ls-svm framework."
"in this paper, the expression for approximate solutions has the same structure as the rbf network method. the expansion of approximate solutions can be represented as:φ"
"where g are typically selected as gaussian kernel functions in the hidden layer, ω k are the weights from the hidden layer to the output layer, and q is the total number of training pairs. then eq. (1) can be written as:"
"boundary conditions of these electromagnetic equations have two types, which are natural boundary conditions and essential boundary conditions. essential boundary conditions impose constraints on the value of the unknown φ at several nodes. natural boundary conditions impose constraints on the change in φ across a boundary. dirichlet boundary conditions are imposed on the functional minimization (17) by deleting the rows and columns of the matrix k corresponding to the nodes on the dirichlet boundary and modifying b in (17)."
"this section describes the proposed method for solving 2-d electromagnetic problems (including 2-d linear electromagnetic problems with continuous boundary conditions, 2-d linear electromagnetic problems with discontinuous boundary conditions, and 2-d nonlinear electromagnetic problems)."
"the solutions of most electromagnetic problems rely on numerical methods. a number of numerical approaches, such as the finite difference method (fdm) [cit], finite element method (fem) [cit], and boundary element method [cit], have been developed, of which fem is the most popular. fem easily accommodates the complex boundary conditions of electromagnetic problems. during its computational process, fem requires a series of meshing processes in the entire computational domain, and only provides solutions at mesh points. additional interpolation for an arbitrary point is thus required, which increases computational complexity. this problem becomes more acute when solving complicated electromagnetic problems. to overcome the drawbacks of fem, element free methods (efms), such as smoothed particle hydrodynamics (sph) [cit], the radial basis function"
"in the fem framework, the governing equation of an electromagnetic problem is transformed to a variational formulation. in the ls-svm model, the variational formulation and dirichlet boundary conditions are considered in the following quadratic programming problem. considering (33) and transforming (32) to the variational formulation, we obtain the following equation:"
"whereφ e is the unknown solution in each element e, n e j is the basis function associated with node j in element e, φ e j is the value of the unknown quantity at node j, and n is the total number of nodes associated with element e. the linear, quadratic, or higher-order functions can be used as the basis functions (also known as interpolation functions or shape functions). in the more general case, finite element methods use either linear or polynomial spline basis functions. the functional within an element is expressed as:"
"where e i are the bias terms and γ is the regularization parameter. formula (25) is a quadratic minimization problem under linear equality constraints, which can be solved by the lagrange multiplier method."
"this paper uses rbf network-based fe-lssvm to solve 2-d electromagnetic equations, including a linear electromagnetic equation with continuous boundary conditions, linear electromagnetic equation with discontinuous boundary conditions, nonlinear electromagnetic equation, and multimedia electromagnetic equation. examples of electromagnetic equations in this paper were solved by fem, the analytical method, and standard ls-svm for comparison. from tables 1, 2, 4, 5, 7, and 8, the results show that the accuracy of the proposed method is superior to that of ordinary fem because the proposed method can obtain the closed-form approximate solutions of electromagnetic equations."
"particle swarm optimization (pso) has become one of the most popular forms of swarm intelligence, and it has been used for solving path-planning and multi-objective problems [cit] . assume an m -dimensional search space. the velocity and location of a particle are expressed by:"
"to solve eq. (29), we use a cubic spline to handle the discontinuous boundary conditions by the asymptotic approximation method. the electromagnetic equation with discontinuous boundary conditions is changed to an electromagnetic equation with continuous boundary conditions by constructing a cubic spline function on the abrupt boundary. thus, we have:"
hence the weights ω can be obtained by solving the nonlinear system of equations (42). the procedure of the proposed method for solving the nonlinear electromagnetic equation is shown in algorithm 2.
"in this paper a multi-mode parallel interleaver architecture targeting parallel siso decoding for different standards has been presented. the design is low cost and supports high frequency at low power. the conflicts, occurring while generating parallel addresses, have been managed by incorporating different schemes. the interleaver address generation hardware for different standards has been modified in the way that efficient hardware multiplexing can be achieved. the functionally of parallel turbo decoder for different standards is more or less same, but parallel interleaving appears to be the bottleneck to reach to a unified version of parallel turbo decoding. the proposed architecture plays a vital role to achieve unified parallel turbo decoding with multi-standard support by using the existing architectures."
"latest trends in radio communication systems always demand for a flexible and general purpose solution for data processing including symbol processing and forward error correction (fec). turbo codes [cit] are being widely used as a forward error correction system in the consumer electronics in order to detect and correct the errors during transmission over noisy channels. interleaving plays a vital role in improving the performance of turbo codes in terms of bit error rate. the primary function of the interleaver is to improve the distance properties of the coding schemes and to disperse the sequence of bits in a bit stream so as to minimize the effect of burst errors. at present a number of standards specifications like hspa evolution [cit], dvb-sh [cit], umts-lte [cit] and wimax [cit] have already included the turbo codes. the scheme of turbo code adapted is the parallel concatenated code with two 8-state constituent encoders and an internal interleaver. one turbo code internal interleaver is used in the turbo encoder while the turbo decoder uses multiple instances of interleaver and de-interleaver to decode the received bits iteratively (fig. 1) ."
"the proposed solution approach groups the goal into two subgoals, the capacitated clustering problem (ccp) and the location-allocation problem (lap), to solve. thus, we propose three metrics to evaluate methods for subgoals."
the channel coding in lte involves turbo code with an internal interleaver which is based on quadratic permutation polynomial (qpp). qpp interleavers have very compact representation methodology and also inhibit a structure that allows the easy analysis for its properties. the internal interleaver for turbo code is specified by the following quadratic permutation polynomial:
"the interleaver parallelism for individual standards has been explored in detail in the earlier sections and the number of parallel siso processors to be supported is summarized in table 4 . the main focus of the work has been to adopt a methodology which results in common computing elements, thus preparing grounds for efficient hardware multiplexing. as a result we reach to the conclusion that an accumulation followed by modulo logic (acc_mod) is the common computing element. therefore forming an array of acc_mod with re-configurability for different combinations can serve as a main part in the complete computing core along with an auxiliary part consisting of a multiplication and comparator. as a mandatory part for some of the applications a circular buffer of size 24 is also needed to be incorporated with acc_mod_array. the address computation follows the conflict resolution which mainly comprises of multiplexers and shift registers. the complete hardware block diagram for the re-configurable parallel address generation is shown in fig. 18 ."
"the two addresses generated simultaneously are used to write two data values in two memory locations. in case of a conflict while writing into same memory, it needs to be resolved on-the-fly. the following sub-section provides a comprehensive analysis to reduce the memory conflicts associated with whole range of block sizes in hspa+."
"the throughput requirements for wcdma based systems have been raised in a series of specifications. after addition of mimo and 64 qam in release 8 [cit] the throughput requirements have reached to 43.2 mbps. the interleaver associated with hspa+ is not inherently designed to support parallel siso processing, but higher throughput requirements and requirement to change the block size in each transmission time interval motivates to have the provision of more than one siso processing in parallel."
"all computational experiments were performed in a lenovo thinkstation p318 personal computer with intel core i7-7700 cpu that is processor of 3.60ghz, 2400mhz with 8.00g of ram memory on microsoft windows 10 version 1803. the whole implementation were developed in the java language and has been complied using the java se runtime environment version 1.8.0."
"generally, people are sensitive to price, time, and convenience of trip. therefore, we analyze the effectiveness of the bus pooling by comparing currently accepted price, time, and passenger mileage. further, we evaluate the scalability of the bus pooling by examining how the price, time, and passenger mileage change as the mileage of ride requests increases. to distinguish, trips are classified into four categories by mileage: extra-short (0-5km), short (5-10km), medium (10-15km), or long (15km or more)."
"another approach might be to split the memory banks into relatively smaller sub-memories to reduce the number of conflicts. up to 8 memories are already needed for 8 parallel turbo interleavers in this design, thus we can divide each memory bank required for hspa+ into 3 submemories. by applying this configuration to the interleaver address generation and data writing the amount of conflicts for most of the block sizes reduced to zero (fig. 3 ), but still many block sizes face some conflicts. however, the amount of conflicts is very small and it can be handled by using buffer registers. the number of fifo buffers can further be reduced by applying the progressive write during the conflict occurrences in the other memory bank. the total number of fifo buffers needed for each memory after memory division is plotted in fig. 5 with maximum of 12 fifo registers required for memory m3."
"in this paper, we proposed a large-scale bus ridesharing service: it allows riders to customise the bus route on demand and pick up and drop off at any desired location. this study focused on the ride-matching optimization problem, which deals with how to find suitable ridesharing matches. the proposed solution approach groups the goal into two subgoals, the capacitated clustering problem (ccp) and the location-allocation problem (lap), to solve. the former is the clustering of travel demand. due to the limited number of seats on a bus, each cluster has a certain capacity limit. the latter is to allocate a location for a set of riders be picked up and dropped off with the assurance of maintains the minimum cost. the experimental results show that our proposed service is economical, energy-efficient, higher in cost performance than driving, taxi, taxi ridesharing, electric-bike sharing, and bike sharing. it provides decision support for government authorities to manage urban bus planning and commercial operation of public transportation corporations."
"carpooling is an economical ride-share service for passengers who share transportation to the same direction of travel in a private vehicle with other travel companions. as a collaborative model of ridesharing, the hardest part is finding consistent travel companions in a short time. to address this issue, the majority of current studies in the field of carpooling is mainly on requirement mining, e.g., [cit] ."
"in this section, we propose the roadmap of bus pooling research. generally, a new service needs to undergoes several stages of evolution, such as basic service, service support, value-added service and knowledge discovery. in the study of ridesharing, whether it is car ridesharing or bus ridesharing, ridesharing service is to assume the role of the media. the core is to optimize a ride-matching problem, that is, to match the most suitable supplier for the demander and to match the most suitable demander for the supplier. the new challenge of bus ridesharing service calls for i) route (or trip) generation for each bus to perform multiple transports, multiple pickup/delivery points selection, vehicle and crew scheduling, crew rostering, depot location, emergency processing and decision that can provide support and guarantee for business management and operations. ii) demands for response to services (real-time or non-real-time) and combination optimization of constraints (such as walking distance, driving time, waiting time, price, route, etc.) that can provide more diverse services to satisfy the personal demands of different target markets. iii) route prediction and mining base on historical data that can discover potentially knowledge and open up scenarios."
"in addition, we assume a homogeneous fleet of vehicles with a 30-seat capacity (or even more) because our goal is to solve the problem of high capacity."
"each rider has a specific tolerance for passenger mileage. the maximum passenger mileage per rider must be less than a given value mileage, i.e., the first step of this study is to solve the matching agency problem [cit] . matching agencies use ridesharing offers and requests received from drivers and riders, respectively, to find suitable ridesharing matches. the primary search criteria refer to what information is used by the system to form driverrider matches. the criteria that can be applied to the bus ridesharing problem are as follows:"
"empirically, the pickup/delivery point is the central point of the point set; even if it is not the central point, it also may be near the central point. if we start with the neighborhood search in the central point, it should be accurate and efficient. consequently, we further propose a center-based search method. to be precise, a random location is allocated from a circle centered on the central point with the radius increasing at a given step size, and stop when a random location is located on the road network that meets the conditions."
"the second part of the conflict management is to apply fifo register bank dedicated for each memory. the size of the fifo register bank could have been very large but we reduced it using the progressive writes during a situation of conflict in other memories. covering all the standards, the fifo size requirement after applying progressive writes for different memories is given in table 5 . the role of controller is very important to achieve this goal as it checks continuously the empty slots for the corresponding mem- figure 19 layout snapshot of proposed unified interleaver."
"existing ridesharing systems (e.g., uber, lyft, didi, olacabs) are only focused on car ridesharing, but they have the problems of low capacity and high cost and cannot satisfy demands for recurring, long-distance, and low-cost trips. assume that millions of commuters living in a metropolis go to work from one district to another every weekday. they have to go back and forth at least once a day and spend a lot of"
"given a set of weighted individuals is to be partitioned into clusters such that, the total weight of the individuals in each cluster does not exceed a given cluster capacity. the objective is to find a set of centers that minimises the total scatter of individuals allocated to these centers [cit] ."
"at last, the social cost and social benefit caused by applying bus pooling are assessed from vehicles used, oil consumption and time consumption. the total number of vehicles for driving and taxi ridesharing is 22.75 times and 14.27 times that of bus pooling, respectively [ fig. 12(g) ]. assume that the oil consumption of a bus is 17.1 xl/100km and a car 9.12 xl/100km, then the total oil consumption of driving and taxi ridesharing is 12.84 times and 7.49 times that of bus pooling respectively [ fig. 12(h)] . to be precise, bus pooling reduces the number of vehicles used by 92% and 96% and the amount of oil used by 87% and 92% compared with taxi ridesharing and no-ridesharing, respectively. the numerical result indicates that bus pooling can make full use of resources and is very energy-efficient and economical. on the contrary, driving achieves at most 49% savings of all passengers' time [ fig. 12(i) ]. in addition, from the trend of the ratio of bus pooling and driving, the increase of the distance of rider requests leads to the gradual decline of ridesharings success rate."
"(2) the driver won't have breakdowns or traffic accidents en route. (3) if the driver arrives at the pickup point earlier than the upper bound of the time window, he or she needs to wait until the upper bound of the time window unless the last rider be picked up. (4) the capacity of all drivers can meet the maximum service requirements. (5) not every ride request is satisfied since this is subject to the number of riders on a bus."
"g ← g ∪ cell; given set) with minimum cost, the point is defined as the pickup/delivery point (fig. 5) . assume that there are at least two such points in the rectangle."
"in this section, we review previous studies of ridesharing services, including taxi ridesharing, carpooling, and slugging, and analyze the differences between them and bus pooling."
"ieee-802.16e standard [cit] called wimax provides the mobility features to broadband technology with higher transmission speed i.e. upto 70 mb/sec. convolutional turbo coding (ctc) is adopted in the fec block, which requires an internal interleaver."
"the contraction-based method can quickly solve the problem but will lead to the problem of dissimilarity imbalance between clusters because it is easy to fall into the trap of local optima. the candidate may be clustered to the local optimal first and miss the global optimum later. to resolve this problem, we further propose an insertion-based method to fix the problem of ''unfairness''. it is called insertion-based because it is similar to insertion sort. in each iteration, the first remaining entry of the input is removed and inserted into the result in the correct position, thus extending the result. the first stage of the method is to construct the elite solution. we first examine the maximum number of clusters k (line 1) and insert the top-k pair candidates as the elite solutions into k clusters respectively (line 5) after working out the dissimilarity measures between all candidates (line 2-4). the rest of r are regarded as candidate solutions (line 6-8). the second stage of the method is insertion, in which we insert an unvisited candidate into each cluster one by one and calculate the value added to the average dissimilarity measure of each cluster (line 9-15). the value added is the standard for evaluating the correspondence between a candidate and clusters, and it may be positive or negative. using the calculation result, we find the optimal one with the smallest value added and add a corresponding candidate to the corresponding cluster (line [cit] . finally, stop when we reach the maximum capacity of clusters."
"the rest of the sections in this paper are organized as follows. section 2 provides a picture of previous work done and challenges involved while implementing parallel interleaver support for parallel turbo decoding. sections 3-6 cover the parallel interleaving in hspa evolution, dvb-sh, 3gpp-lte and wimax standards respectively. following the hardware multiplexing methodology, the unified architecture of the re-configurable parallel interleaver is presented in section 7. section 8 provides the implementation results followed by a conclusion."
"as shown in fig. 9 (c), the efficiency of co is obviously better than that of the others. in is not scalable when k is large. this is because the number of candidates grows exponentially with k, resulting in high computational cost and memory consumption. on the contrary, co has the simplest steps and the fewest computations, which results in the shortest time. dg can achieve a better tradeoff between efficiency and effectiveness. the computational time of dg is not linearly dependent on k or the number of grid cells but on the solution. the earlier the solution appears, the lower the computational cost. in terms of k, that is as plain as the nose on your face: when k increases, it means that the workload is larger, and the computational cost is higher. in terms of capacity, the greater the capacity, the smaller the number of clusters, and the less the computational cost."
"the average passenger mileage is only 1.2 km [ fig. 11(c) ]. the changes of passenger mileage were not significant for extra-short, short, medium, and long distances. if the speed of walking and cycling are as in table 3, it only takes 7.2-17 min to reach the destination. we believe that this time is acceptable to most people."
"the system design of bus pooling adopts service-oriented architecture (soa). fig. 6 shows the framework of the bus ridesharing service. the main components include two business modules (i.e., matching module and terminal module). the matching module implements the function of efficient ride-matching and integration of trip demands. the terminal module implements the function of allocating the optimal pickup/delivery point for riders."
the complete hardware for the generation of parallel interleaved addresses and data handling is shown in fig. 9 . a fifo buffer of size 12 is needed as discussed in previous sub-section to handle the conflicts. the generation of a1 a2 a3 a4 a5 a6 a7 a8 b1 b2 b3 b4 b5 b6 b7 b8 c1 c2 c3 c4 c5 c6 c7 c8 d1 d2 d3 d4 d5 d6 d7 d8 e1 e2 e3 ? ? ? ? ? data reading data writing figure 7 column by column recursive address computation.
"-y j is 1, if driver j is used and 0 otherwise. the objective (4) minimizes total passenger mileage, hence increases valid ride requests, with the assurance of maintains the maximum ridesharing's success rate (i.e., the percentage of successful ride requests). constraint (7) ensures that every rider is assigned to exactly one driver and constraint (8) restricts a rider to driver j, denoted by a binary variable y j equal to one, to be selected only among the riders assigned to the driver j. restrictions (9) and (10) ensure that exactly p drivers will be created and that every driver does not exceed its capacity limit respectively, constraints (11) specifies the decision variables."
"the complication in the implementation is evident due to the presence of complex functions like modulo computation, intra-row and inter-row permutations, multiplications, finding least prime integers, and computing greatest common divisor. after applying the intra-row and interrow permutations a block of random addresses denoted by y k appears as shown below: the output from the interleaver is the sequence read column-wise from the permuted matrix. the output address y k within the matrix can be expressed as:"
"the parallel generated interleaved addresses for dvb-sh are not conflict free. figure 11 shows the number of conflicts for the two blocks sizes. applying the method of misaligning on the address and data streams, it is observed that the conflict percentage reduces significantly."
"in this paper, we study the bus ridesharing problem in a practical setting and design a bus ridesharing service system to resolve this problem. to facilitate a better comprehension of our new problem, we start with a demonstration depicted in fig. 7 . the rider can use an online bus-hailing service to upload his or her trip demand, and wait to be picked up by a public bus when it gathers enough people. the provider assigns drivers to riders after integrating the matched ride requests. in addition, the driver's trip must follow the established route with its departure, time-window, capacity, and cost constraints."
"the third approach is completely based on segmentation based modulo computation and it is a non-recursive way to compute the address ra. it can directly be applied to the term j â rðiþ% p à 1 ð þ to get the address for register file. here we know that the parameter r(i) can have only 22 values of prime numbers up to 89, thus sbmc approach can be used after applying some optimizations. the hardware required to compute the function j â rðiþ% p à 1 ð þ using sbmc approach is shown in fig. 8 . it needs more additions then the recursive approach; however, it can directly be used to compute intra-row permutation patterns in the preprocessing phase, thus providing single clock cycle support for computing each permutation pattern s(j). this approach cannot be better for very high clock frequency due to longer critical path. pipelining can improve the performance of this scheme but at the same time introduces higher control complexity."
"given a homogeneous fleet of drivers, where each driver is providing ride-share service with an associated capacity limit, and a ride request from a rider, located at an origin to go to a destination with a pair of corresponding expected times. the provider assigns drivers to riders after integrating the matched ride requests. the rider is prompted to walk to the driver's origin, board at the driver's departure time in the time-window, alight at the driver's destination, then walk from there to the rider's own destination (fig. 1) . the objective is to maximize ridesharing's success rate."
candidates have been visited (line 4); submit the rest as a cluster (line 14) and remove them from r (line 15). continue in this way; stop when all the candidates stored in r have been clustered (line 2).
"the traffic impedance model reflects various factors that the traffic users consider when choosing transportation and their importance. considering the double standard of time and price, it is not accurate to evaluate the bus ridesharing service by using the normal traffic impedance model. thus, we propose a time-price traffic impedance model as follows:"
"to realize this idea, we need to solve the combinatorial optimization. the work of this study consists of three steps as follows: firstly, select a search criterion as needed and solve the capacitated clustering problem [cit] of trip demands. secondly, solve the location-allocation problem [cit] of the pickup/delivery point for passengers in each vehicle. thirdly, prune by using constraints."
"at present, the speed of urban traffic infrastructure construction cannot keep up with the rapid growth of traffic demand, especially the traffic jams that occur in cities during rush hours. the traffic problem is gradually being exposed as a weakness of the city. in solving urban traffic problems, ridesharing [cit], a shared service that utilizes information and knowledge matching, can effectively use scattered social resources to reduce the demand for vehicles in the urban road network. it is an effective approach to solving the difficulty of taxi-hailing and ease traffic congestion."
"according to a survey conducted by the social survey center of china youth daily, 43.4% of respondents spend half an hour to an hour commuting every day. more than 90% of respondents would only accept a maximum commute of less than two hours. as shown in fig. 11(a), the time taken for extra-short, short, medium, and long distances is 34.97 min, 44.43 min, 64.29 min, and 109.53 min, respectively. all types of commuting time are less than two hours. meanwhile, it is convenient for passengers not to transfer and to spend no time waiting for and picking up/droping off passengers at bus stops."
"as shown in fig. 9(a), dg is the best, in is the secondbest, and co is the worst. co is a greed-based exact method that may calculate the optimal solution in a feasible amount of time, but it is easy to fall into the trap of local optima. as shown in fig. 9(b), co is better than the other algorithms at first, but eventually, it ends up worse. in is better than co because each individual has the opportunity to calculate all clusters. dg is an approximation method based on the divide and conquer principle, which narrows the potential search scope, neglects the bad solutions, and improves the accuracy of the solutions. the advantages of dg could allow it to escape the trap of local optima and finally achieve the global optimum. in terms of k, when k increases, the more combinatorial optimizations are available, and the better the effectiveness. in terms of capacity, the more riders on a bus, the greater the dissimilarity, and the worse the compactness."
"the associate editor coordinating the review of this manuscript and approving it for publication was roberto sacile. time and money on transportation. taxi ridesharing to work is convenient and fast, but it is not sustainable for salaried people because it is uneconomical. by contrast, taking a bus is economical, but the time to wait and to pick up/drop off passengers at bus stops is unpredictable, and transfers are a big problem. carpooling maybe a compromise option, but it's hard to find consistent travel companions."
"as shown in fig. 11(b), the fare is only ¥6 for 86.72% of riders. the rest are ¥6 to ¥8 (7.78%), ¥8 to ¥10 (3.28%), ¥10 to ¥12 (0.7%), and more than ¥12 (1.51%)."
"to the best of our knowledge, our work is the first to consider bus ridesharing for the problems of recurring, long distance and low-cost trip demands. we place our problem in a practical setting by exploiting a real city's road network and an enormous set of historical taxi trajectory data. the contributions of this paper are multi-dimensional:"
"2) time-window constraint each rider shall arrive at the pickup point earlier than the departure time and arrive at the destination no later than the deadline time, i.e.,"
"different modes of transport were used to complete all trip demands in the experiment, including bus pooling, driving, taxi, taxi ridesharing, electric-bike sharing, and bike sharing. the purpose was to analyze the efficiency of different modes of transport by comparing time, price, physical exertion, and cost performance. table 3 gives all the pricing and speeds of transportation in the experiment. notice that i) all speeds is the average after taking into account traffic jams and road conditions. ii) the speed of driving is faster than taxi and taxi ridesharing because it saves time waiting for a ride. iii) using a prebooked ride-hailing service saves more time than taking a taxi in practice. iv) electric-bike sharing cannot satisfy demands for long distance trip because of the limitation of battery capacity. in terms of time consumption, bike sharing is the most time-consuming, followed by bus pooling, electric-bike sharing, taxi, taxi ridesharing and driving [ fig. 12(a) ]. to be precise, driving, taxi ridesharing, and taxi take 55%, 63%, and 69% of the time of bus pooling respectively in extra-short, short, medium, and long distances. on the opposite, prices of taxi, driving, and taxi ridesharing are 4.93 times, 4.49 times, and 4.15 times that of bus pooling respectively for extrashort, short, medium and long distances. on price alone, bike sharing, electric-bike sharing, and bus pooling are cheaper [ fig. 12(b) ]. the numerical result indicates that driving, taxi, and taxi ridesharing are convenient and fast but cannot satisfy demands for recurring, long-distance, and low-cost trips. in terms of physical exertion, bike sharing consumes more strength. on average, bike sharing consumes 9.69 times as much energy per trip as bus pooling [ fig. 12(c) ]. obviously, cycling all the way from the origin to the destination is more tiring."
"(1) compare the effectiveness and efficiency of capacitated clustering algorithms; (2) compare the effectiveness and efficiency of locationallocation algorithms; (3) evaluate the effectiveness and scalability of bus pooling; (4) evaluate the efficiency of bus pooling. this includes two parts: (i) compare the time, price, physical exertion, and cost performance between bus pooling, driving, taxi, taxi ridesharing, electric-bike sharing, and bike sharing; (ii) compare total vehicles, total oil consumption, and total time consumption between bus pooling, driving, and taxi ridesharing."
"other studies on carpooling focuses on the effectiveness of ridesharing services, such as minimizing the travel costs of vehicles, maximizing the ridesharing's success rate, and improving riders' satisfaction, e.g., [cit] . in contrast, bus pooling is a ride-share service based on demand integration, and its target population are commuters and people taking long-distance trips. the trip demand uploaded by riders are relatively constant and regular."
"however, there may be errors in the real world, e.g., if there is a river or a valley between the two points, even though they are close together, two routes may be completely opposite."
"novel approach of using recursive computation for computation of m. after getting the parameter m for the first siso, the rest of the m values for parallel addresses can be computed recursively over the whole column size as shown in fig. 8 . the lookup table entries required to produce the correct m values for parallel addresses at each row instant are named as t aux and are given in the table 2 . the modified algorithm supporting generation of 8 parallel addresses (i m ) is given as algorithm 1 and the hardware for computing parallel addresses is shown in fig. 14."
the average of scatter between each cluster n individual in p clusters for the capacitated clustering problem. this represents the dissimilarity of ride requests on a bus.
"in subsequent work, we cluster the demands with capacity constraints by using the primary search criteria. generally, this particular clustering problem is called a capacitated clustering problem."
"in conclusion, the capacitated clustering problem is solved by the proposed methods. consequently, we can provide bus ridesharing services to these riders. the second step of this study is to solve the locationallocation problem. after capacitated clustering, we prepare to allocate a location for riders be picked up and dropped off with the assurance of maintains the minimum cost. that is, find the pickup/delivery point closest to all the origins/destinations of each cluster. at present, a plethora of existing studies have calculated the shortest distance between two locations in a road network. the state-of-theart approaches can be classified into two categories: spatial coherence based methods and vertex importance based approaches [cit] . however, the above-mentioned approaches are both based on a dataset containing an undirected graph that represents a part of the road network. this is different from our situation. to be more universal, we only consider the trajectory data, but do not consider the road network data-set within the study area. in this study, we propose two approximate methods to calculate the global shortest path distances between multiple locations in the road network without the road network dataset. then, further than that, we still have to consider allowed parking locations in the road network for practical application. for example, urban roads are divided into expressways, arterial roads, secondary trunk roads and branch roads by road classification and function division in china. among them, only arterial roads, secondary trunk roads and branch roads can support bus stops such as bus bays and request stops. therefore, the following principles should be met: 1) on an arterial road, secondary trunk road, or branch road; 2) roadside parking or short-term parking is allowed."
"in the experiment, the sum of global shortest path distances is calculated. the smaller the sum, the better. first of all, we try to figure out the number of iterations in which the algorithm converges to the local optimal solution. as shown in fig. 10(a), rs converges in four iterations, and cs converges in five iterations. consequently, the experimental evaluation score was calculated no less than five iterations. on the one hand, the effectiveness of rs is obviously better than that of cs, especially if the distance between the two locations is larger. not only that, the effectiveness of rs has better stability and less fluctuation [ fig. 10(b) ]. on the other hand, rs sacrifices efficiency to improve effectiveness and has a higher computational cost than cs [ fig. 10(c) ]. the advantage of rs is randomized adaptive search, which optimizes computation through probability. although both of them are approximation algorithms, the difference is that rs adopts the method of random sampling and probability gradual approximation, whereas cs infers the location distribution of the optimal solution by taking advantage of prior knowledge, and it gradually approaches the optimal solution from optimal to inferior."
"keyword/list searches a request and an offer by keywords on predefined lists. in practical applications, shuttle buses usually travel from one urban landmark to another, such as a station, park, cinema, square, and so on [ fig. 2(c) ]. we put the names of landmarks into keyword lists and measure the distances between these landmarks as the criteria."
"our contraction-based method starts by initializing p and q to contain the first θ and the remaining individuals in r, respectively (line 3) and initializing the scatter upper bound ub and the cursor variable cursor to facilitate pruning in the following enumerations of new candidates (line 5). then, we insert an unvisited candidate from q to p (line 6), and iteratively examine the scatter of the individuals in p (line [cit] . the purpose is to measure the dissimilarity between two individuals in p. we also maintain a set l to contain the complete list of the sum of the scatter of the enumerated candidates. after each iteration of p is completed, the one who has the maximum scatter in p is removed (line 13). scan all the candidates stored in q until all the"
"in order to solve the comparability between different standards, we finish the linear transformation of data with min-max normalization. the normalized value is defined as:"
"the address (ra) to the ram i.e. j â rðiþ% p à 1 ð þ involves modulo function. we present here three alternates to compute the ram address. the first method involves recursive computation of addresses as shown in fig. 7 . the data is written into memory row wise but when reading back column wise, the irp_ram address of previous column is used to find next address. the recursive function is given by:"
"the interleaver used in lte is special in the sense that it is conflict free inherently when parallel interleaving is required, thus providing ease of implementation for parallel turbo decoding. generation of parallel interleaver addresses can be achieved with the replication of the hardware shown in fig. 15 and getting support from a lut providing the starting values. in this case a total of 32 additions are needed; however, to achieve a low cost solution we present here the hardware with first part being reused for multiple stages. this optimized hardware uses 18 additions in total to generate 8 parallel interleaver addresses, thus saving 14 additions."
"along with multi-standard support, requirements of higher throughput are also emerging in connection with customer needs. on the other hand turbo codes in general exhibit latency (further reducing throughput) due to bigger block sizes and multiple iterations over soft bits needed to reach to a reliable hard decision. the latency can increase further if interleaver block size is varying in every transmission time interval (tti) with the requirement of some pre-processing while changing the block size. the technique of parallel turbo decoding is well established to meet the high throughput requirements [cit], but it also requires implementation of parallel interleavers. if there are p parallel siso processors then p sub-interleavers has to be implemented in parallel and each sub-interleaver will be responsible to generate n/p interleaver patterns independently where n is the total block size. the main schemes of parallel turbo decoding can be applied to different standards without any modifications; however, interleaving varies among different standards. many challenges are involved including unified address generation and conflict management which restrict the use of same parallel turbo decoding architecture for multiple standards. this paper paves special focus on handling these challenges and presents an implementation of a re-configurable interleaver architecture targeting unified parallel turbo decoding."
"on the one hand, each driver can only afford a limited trip demand at any time. on the other hand, in order to reduce operation cost, the driver needs to exceed the threshold of the number of riders to provide services, i.e.,"
"taxi ridesharing is a typical ride-share service that accepts taxi passengers' real-time ride requests sent from smartphones and schedules proper taxis to pick them up via ridesharing. the focuses of current studies are mainly on realtime systems and options for different constraints. for the former, the core is to devise a real-time matching algorithm that can quickly determine the best vehicle to satisfy incoming service requests, e.g., [cit] . for the latter, the studies are reflected in the combination optimization by considering different constraints, such as waiting time, price, route, scheduling, and payment, e.g., [cit] . in contrast, bus pooling offers an affordable mass service rather than personalized and efficient services. hence, we pay more attention to the effectiveness and scalability of bus pooling."
"in practice, the pickup/delivery points allocated by abovementioned methods may not allow parking on the map. therefore, a location-based service (e.g., google maps, bing maps, baidu map, amap) is used to search the recommended stops near the potential location, whereas this way may return multiple locations. we continue to pick the optimum by calculating the shortest path distances."
"to satisfy demands of riders, we prune by using constraints after above-mentioned two steps of this study. the steps are as follows: step1 calculate t 1"
the total of the shortest path distance from each cluster's pickup/delivery point in p clusters to corresponding cluster's n origins/destinations for the location-allocation problem. the shortest path distance between any two points is used as the cost indicator.
"in order to make the interleaver architecture fully autonomous, the parameters like r, c, q(i), t(i), s(j), p and v are needed to be computed in hardware. some of these parameters can be computed using lookup tables while the others need some close loop or recursive computations. the most critical parameter consuming more clock cycles and more hardware to compute is the intra-row permutation pattern s(j). the function to be computed for all the permutations is:"
"routing and time matches the route and the time from the origin to the destination. essentially, each route is a spatial trajectory represented by a sequence of timestamped geocoordinates [cit] . in order to obtain more accurate results, we calculate the similarity of trajectories to find similar routes. many existing studies have focused on defining trajectory similarity measures, such as dynamic time warping (dtw) [cit], longest common sub-sequence (lcss) [cit], edit distance on real sequence (edr) [cit], etc. in this study, we try to take dynamic time warping as a method to measuring dissimilarity that calculates an optimal match between two given time series with certain restrictions and rules because no noise points exist in this study."
"the parallel processing of turbo decoder with parallel siso blocks requires the interleaver to generate more than one address every clock cycle and at the same time resolve the memory conflicts. the computation of final interleaved address requires that the intra-row permutation data must be obtained in correct order from irp_ram. the data output from the ram is denoted as u(i,j) and it is given by:"
"we conducted experiments using a taxi gps trajectory data set. the dataset contains 65,065-trip instances from 10,585 shanghai taxis from one day (apr 1, 2018). we take the records of taxi passengers picked up and dropped off that were generated by gps receivers as the trip demand of riders in a period of time in the whole city. to show an overview, statistics are classified by time steps, total mileage, and demand distribution (fig. 8) . not only that, we conducted experiments using 100 groups of coordinate sets containing three coordinates as experimental samples to compare the effectiveness and efficiency of location-allocation algorithms. the above-mentioned datasets in the experiment are available at the web page http://dx.doi.org/10.21227/2877-mk46."
"the rider sends a ride request to the system by using a smart device; the request is queued and sorted by the scheduled time, then partitioned by time steps. the system batches trip demands of requests based on time steps. firstly, the matching module analyzes the spatial trajectory of trips and computes the similarity of spatial trajectories to find the similar trips. secondly, the terminal module allocates the optimal pickup/delivery point for riders by using algorithms and location-based service (to query the duration and distance data of the route). the results are pushed to riders and drivers are notified by a system prompt, and drivers and riders to wait at the designated location in the time window. in addition, any rider can modify or cancel the service before it is completed."
"looking at the implementation aspects recursive systematic convolutional encoders are very simple to implement as compared to siso decoding in turbo decoder, but the interleavers usually tend to have same complexity on both sides because of its variability. the implementation of interleaver on decoder side becomes more challenging when parallel interleaver implementation is needed in order to support parallel siso processing. other then the architectural complexity for generation of parallel interleaver addresses one of the big challenges is to deal with memory conflicts associated with multiple writes and reads at the same time. if the multiple addresses generated do map to different memories i.e. one address for each memory then there is no conflict, but on the other hand if two or more addresses are mapped to same memory then a situation of conflict occurs (fig. 2) and it needs to be resolved on-the-fly."
"the cumulative distance d(i, j) as the distance d(i, j) found in the current cell and the minimum of the cumulative distances of the adjacent elements, equation (19) can be used to derive the distance between q and c based on dynamic programming [cit] :"
"the interleaving algorithm for 3gpp-wcdma is mentioned below. here n is the block size, r is the row size and c is the column size in bits."
"different optimization algorithms have different principles and are applicable to different scenarios. after many investigations, we select the rmsprop algorithms as the optimizers for testing the optimization algorithms. the learning rate, one of the hyperparameters, is the speed at which the neural network reaches the optimal status. if the learning rate is too small, then the loss of the network declines very slowly. if the learning rate is too large, then the range of the updated parameter is too large, resulting in the network convergence to a local optimum. we select values experimentally and observe that the performance is less extreme with the change in hyperparameters."
"social bot detection can also be achieved without feature extraction. [cit] made use of a clickstream model to detect the real identity of social accounts on the server side. these authors input the clickstream sequence and then calculated the sequence distance to accurately classify social accounts. [cit] presented a novel method of detecting malicious social bots, including both feature selection based on the transition probability of clickstream sequences and semi-supervised clustering. this method not only analyzed the transition probability of user behavior clickstreams but also considered the temporal features of behavior."
"3) how many samples are generated for the most obvious improvement in bot detection? 4) when the degree of data imbalance in the original data set is different, what is the degree of difference in the improvement of the detection effect by different oversampling methods? experiment 1: to verify the effectiveness of the improved cgan method in generating minority samples, the original algorithm and three popular oversampling algorithms are compared. the random oversampling, adasyn, and smote algorithms are considered in the experiment. minority class samples are generated with the oversampling algorithms until the quantity of minority and majority samples is the same, and then the neural network is adopted to classify the social bots. the detection results are shown in fig. 6 . the roc curves are shown in fig. 7 ."
"density peak clustering is a new clustering algorithm that is different from the traditional clustering method. first, it needs to calculate the local density of each data point and the shortest distance from the data points with a higher density to each data point. second, the cluster center is manually determined through a sorting graph. finally, the remaining data points are assigned to the category in which the data points are higher in density and shorter in distance."
"more importantly perhaps, star can thus provide the power to automatically balance the clinical treatment, based on clinical goals for each patient. it thus avoids forcing a clinic-wide target or approach onto patients who have requirements outside the norm, or subjecting them to ad-hoc decisions to handle their particular cases. thus, for example, different bg target ranges could be readily specified for different clinical conditions based on diagnosis and this target range could be updated as treatment progresses."
"measurement or treatment interval is specifically limited when 1) the current measured bg is outside the specified target range of 80-145 mg/dl (one-hourly limits); or 2) the patient is unable to be fed (two-hourly maximum interval). otherwise, when the current measured bg is within the clinically specified target range, star calculates intervention options (insulin and nutrition) for one-, two-, and three-hourly measurement intervals. in clinical application, nursing staff chose between the available intervention options, allowing workload to be selfmanaged. for the virtual trials, two star versions were separately tested, \"star-max\" (longest available option always selected) and \"star two-hourly\" (two-hourly options selected whenever available). these two result sets give an indication of the tradeoff between clinical workload and performance/safety, and the two-hourly version allows for direct comparison between star and the precursor protocol, sprint."
"insulin is administered in bolus form for safety from unintended delays [cit] . however, infusions may also be used, if desired. robustness to glucometer measurement error limits increases in insulin rate to +2 u/h, with upper limits on the total bolus dose each hour (6 u) and any added infusion rate for highly resistant patients (3 u/h). thus, total insulin is limited to 9 u/h. no limit is placed on insulin reductions. enteral nutrition is controlled between 30% and 100% of accp goal [cit], and changes are limited to ±30% per intervention cycle. however, nutrition administration can be set to a fixed constant rate or zero if clinically specified. for clinical applicability, insulin and nutrition are discretized to 0.5 u and 5% increments, respectively."
"b. overall process fig. 1 shows the social bot detection framework proposed in this paper. by collecting tweet and user-profile information, we generate the available data set from the original data set through feature extraction and data normalization. then, we formulate a data-augmentation approach by adopting generative adversarial network method. to overcome the shortcomings of the original cgan-it easily causes mode collapse, and it is unable to effectively control the category of generated samples-we create an improved cgan by introducing the wasserstein distance with a gradient penalty. we also improve the conditional model of cgan to control the generated samples, and we use the modified density peak clustering algorithm to generate conditions as a part of the input."
"accurate glycemic control (agc) can mitigate these outcomes [cit], but has proven difficult to achieve safely and consistently [cit] . only one study [cit] reduced both mortality and hypoglycemia. however, the higher nursing workloads due to high-density glucose readings are impractical in many units [cit] . hand-held glucometers are easier for measurement, but their larger errors can add additional difficulty for some agc protocols. finally, clinical compliance determines much of the efficacy of any agc method, with quality of glycemic control thus also limited by the confidence and compliance of nursing staff [cit] . all of these issues interact with the inherent inter-and intra-patient metabolic variability [cit] to exacerbate the difficulty of achieving good control. hence, glycemic control targets have been raised to mitigate these factors and avoid hypoglycemia [cit], as a best outcome compromise, despite the physiological and clinical evidence on the negative impact of even moderate hyperglycemia [cit] ."
"in the original gan model, we cannot control the specific types of samples being generated by generator g. sometimes, we are interested in the ''conditional on'' class of social volume 8, 2020 bots. conditional generation is performed by incorporating more information into the training procedure of gans so that the generated samples conform to the same attributes as certain training sample categories [cit] . considering the inherent association of social bots, if the social bot samples are clustered first, then the clustering results can be used as the conditional model of the cgan, and the augmented social bot data set can achieve a more balanced class distribution and effectively avoid the generation of noisy samples. the use of clustering enables the cgan model to more accurately identify and target areas of the input social bot space where the generation of social bot data is most effective. however, there are some problems with the traditional cgan. the better the discriminator is trained, the more easily the vanishing gradient problem occurs in the generator of the traditional cgan. the lack of diversity in the data generated by the generator leads to the collapse of the traditional cgan. the wasserstein distance with a gradient penalty can solve these problems perfectly."
"in this paper, we propose an improved cgan model to increase the detection accuracy of malicious social bots. since the number of normal human beings and social bots on twitter is imbalanced, we improve the traditional cgan by incorporating the wasserstein distance with a gradient penalty and a clustering algorithm as a new dataaugmentation approach. specifically, the wasserstein distance with a gradient penalty is introduced to the cgan loss function to solve the problem of model collapse and gradient disappearance in the traditional cgan. in addition, we propose the gkdpca as a conditional model to label the social bot data in the cgan model, which improves the euclidean distance calculation method of the density peak clustering algorithm. furthermore, we use gaussian kernel function to project the original features into a high-dimensional kernel space. the findings indicate that the data-augmentation approach-improved cgan based on the gkdpca and wasserstein distance with a gradient penalty can rebalance skewed social bot data sets, which avoids the generation of data-augmentation noise and effectively overcomes imbalances between and within class distributions."
"there are two main problems with the traditional cgan algorithm. when the discriminator is too good, generator training may fail as the gradient disappears. moreover, if the sample diversity generated by the generator during training is insufficient, the generator fails to learn to represent the complex real data distribution and becomes stuck in a small space with extremely low variety, causing the model to collapse."
"in an unconditioned generative model, there is no control of the modes of the data being generated. however, by conditioning the model with additional information, it is possible to direct the data-generation process. such conditioning may be based on class labels, on some part of the data for inpainting, or even on data from different modalities [cit] . thus, the gan can be extended into a conditional model if both the generator and discriminator are conditioned with some extra information, y, which can be any kind of auxiliary information, such as class labels or data from other modalities. we perform conditioning by feeding y into both the discriminator and generator as an additional input layer."
"once integral-based parameter fitting is used to identify insulin sensitivity over the last hour, simulation of the range of bg outcomes for each insulin/nutrition combination begins. starting from the target nutrition rate and stepping down, fifth percentile (see fig. 2, points a-c) and 95th percentile bg outcomes (see fig. 2, points d-f) are simulated for every allowable insulin rate. for the current implemented form, typical runtime is approximately 15 s for up to 180 predictions. importance is placed on maximizing nutrition rates [cit], particularly for longer stay patients, but not at the risk of exacerbating hyperglycemia."
"virtual trials were carried out to verify performance before clinical testing. results were compared to clinical sprint data to demonstrate improvements in performance and safety over the currently utilized sprint protocol that successfully reduced mortality [cit] and organ failure [cit] . three other published protocols: unc [cit], yale [cit] and glucontrol a [cit] were simulated in virtual trials for comparison and context."
"the svd reduces the dimension of the matrix while maintaining as much column information as possible. then, the similarity of each pair of words can be quantified by the cosine similarity of the two row vectors a and b as follows:"
"the main purpose of this experiment is to examine the sensitivity of the four oversampling methods in relation to the expansion multiple. in this experiment, the number of minority samples accounts for 18% of the total number of samples. minority samples are generated with four different ϕ ratios: 1:1, 2:1, 3:1, and 4:1. thus, four different training sets are generated, and then the classification of the data is performed. the results are shown in table 3 . observing the accuracy trend graph of the classifier, as ϕ changes, the accuracy of the classifier generated by smote and adasyn fluctuate significantly. the accuracy of all of the algorithms is higher than that of the original data set. the accuracy of the improved cgan methods is stable, and its fluctuations are gentle, which indicates that the improved cgan method is less affected by changes in the sampling proportion."
the false positive rate (fpr) is the ratio of the number of negative predictions that are classified as negative to the number of all negative samples. the fpr is defined as follows:
"we input the bot accounts from the training set and clustering labels into the improved cgan and then train the improved cgan until it is stable. then, we incorporate random noise and random labels into the stable improved cgan to generate fake bot accounts that are difficult for the discriminator to distinguish. next, we mix them with the normal human data sets to form an augmented training data set. finally, we adopt the augmented training data set to train a classifier that will be used to classify an unlabeled account in the test set."
"a similar procedure occurs at 2 and 3 h, with admissibility determined by the heuristic rules described in fig. 3 . if a treatment is admissible under fig. 3 : condition a, the treatment is saved, whereas if it meets condition b, it is saved as a possible treatment. possible treatments are not overridden, instead saved/cleared after all insulin rates are tested for a given nutrition rate. if no allowable treatments exist, these longer time periods cannot be offered. if an allowable treatment is found, the selected nutrition rate is used as a lower limit for shorter treatment intervals. this approach ensures treatment consistency across all intervention and measurement intervals to maximize transparency and clinical acceptance, and thus compliance [cit] . specifically, it ensures an intuitive combination of treatment options, where longer measurement intervals generally yield wider stochastic forecasting bounds and thus more conservative (lower insulin) treatment choices."
"a data point used as a clustering center needs to have two characteristics: 1) the point should be surrounded by other points with a relatively low local density and 2) the distance between this point and other data points with higher densities should be great. thus, the algorithm needs to calculate the local density ρ i of each data point i and the shortest distance δ i from that point i to a point with a higher local density. the distance can be quantified by the cutoff distance [cit] or the gaussian kernel distance [cit] . the cut-off kernel distance is:"
"2. we improve the cgan model convergence judgment condition by incorporating the wasserstein distance with a gradient penalty, which has achieved a better performance that minimizes the different distribution divergence."
the category of the bot is selected by using the density peak algorithm as an input. we choose to classify bot users into two categories through the decision diagram of γ .
"as malicious social bots have increasingly used various social engineering methods to distribute unsolicited spam, advertise events and products of dubious legality, promote public figures, and steal sensitive personal information [cit], many studies have examined how to detect fake accounts generated by social bots. social bot detection aims to distinguish between bots and normal humans in social networks [cit] . machine learning has been used to detect social bots on social networks through various approaches, including by detecting fake content linked to the account, investigating the account profile itself, and using nonverbal indicators [cit] ."
"equally importantly, the ability of nursing staff to choose between measurement interval options means that an informed decision can be made at each bg reading. thus, nurses selfmanage this workload. this choice or feature is expected to also have a positive impact on compliance and acceptance."
"when the generator g generates fake bot samples that are not distinguished by discriminator d, the fake samples will be mixed with the real samples as input for the network classifier. the label for real bot samples is obtained through gaussian kernel density peak clustering, and this label is used as the auxiliary information for the improved cgan."
"the simulation results are supported by the initial pilot trial results presented in table vi and fig. 6 for the first ten patients with the target band of 80 (1486 h, 836 measurements). the clinical bg results are very similar to simulation results in table iv with"
"experiment 3: considering the lack of proportion between minority and majority samples in the original data, we make the number of minority samples equal to the number of majority samples to influence the performance of the oversampling methods. therefore, this experiment is designed to check the sensitivity of the four methods relative to the imbalance degree r, which is defined as:"
"during the process of training the cgan, problems such as pattern collapse and overfitting may occur. similarly, the wcgan fluctuates greatly during the training process. the loss of the improved cgan remains largely stable in the later period. the change trend of the improved cgan is significantly different from that of the wcgan. at the same time, the improved cgan also solves the possible problems of the original gan. the generated social bot samples are more consistent with the spatial distribution of the existing bots."
"the g-mean (geometric mean) is the square root of the product of the recall and precision and is used to evaluate model performance for imbalanced data. the best value for the g-mean is 1, and the worst value is 0. the g-mean is defined as follows:"
"in order to evaluate the performance of oversampling methods, we used stratified sampling to randomly divide 80% of the original data set into the training set and the remaining 20% into the testing set, so as to ensure that the training and testing sets have approximately the same percentage of samples of each target class as the original data set, as shown in table 1 . all oversampling methods are used to synthesize the training samples and balance the training set. the testing set is used to test the performance of the classifier trained by the oversampled training set. each experiment was run independently for 10 times to reduce the impact of randomly partitioning the training and testing sets."
"experiment 2: we also have to determine how many generated examples are suitable for training the classifier. for this purpose, the expansion multiple is used. it can be defined as follows:"
"the oversampling method based on the improved cgan model works best among all of the oversampling methods in fig. 6 . the f1-score of the improved cgan reaches 97.56%, which is 7 percentage points higher than that of the classifier trained by the original data set. the accuracy of smote and adasyn are relatively similar, and the accuracy of the classifier generated by the improved cgan is nine percentage points higher than that of the classifier generated by the original data set. the accuracy of the classifier generated by the improved cgan is the highest among all of the methods. the recall of the improved cgan methods is also the highest among all methods. the higher recall indicates that the classifier generated by the improved cgan can detect more social bots than the others and that the generator of the improved cgan can generate more samples fitting the law of the minority sample. the random oversampling randomly selects existing samples as new samples, which causes overfitting. the smote algorithm randomly selects a new minority sample from the connection line between the nearest neighbor samples and a specific sample. additionally, adasyn automatically determines how many minority samples are synthesized rather than synthesizing the same number of samples for each minority sample as the smote algorithm does. when the original data set is imbalanced, the g-mean value is relatively low. when the original data set is oversampled to a balanced data set, the g-mean value noticeably improves. the g-mean value of the improved cgan is noticeably larger than that of the other three oversampling algorithms. in fig. 7, the trends of the five roc curves are approximately the same. however, the auc value of the improved cgan is the highest. the g-mean and the auc value of the improved cgan indicate that the data generated by the oversampling method fits the existing data well. the results also indicate that the improved cgan is superior to other oversampling methods because the improved cgan can learn the spatial distribution characteristics of social bots in the process of iterative training."
"the fifth percentile target is prioritized for control (see fig. 3 ) due to the skewed nature of the bg outcome distribution, as depicted in fig. 2, which ensures bg outcomes best overlap the lower (80-125 mg/dl) desired portion of the 80-145 mg/dl range. this 80-125 mg/dl range is associated with better outcomes [cit] and is also associated with reduced rate and severity of organ failure [cit] . tightness of the agc provided by star is further ensured by the treatment of the 95th percentile forecast outcomes. monitoring the likelihood of bg rising above the target range allows for more explicit direct control over intrapatient variability and therefore directly limits the risk and occurrence of mild hyperglycemia."
"in the actual training process, the gan with wasserstein distance uses weight clipping to ensure lipschitz continuity in the whole sample space. after updating the discriminator parameters, it checks whether the absolute value of all of the parameters of the discriminator exceeds a threshold. to ensure that all parameters of the discriminator are bounded during the training process, we ensure that the discriminator can not distinguish two slightly different samples from each other, thereby indirectly meeting the lipschitz limit. the optimal strategy in this case is to make all of the parameters as extreme as possible. the specific approach we take is to increase the gradient penalty term on the discriminator's loss function because there is no need to have lipschitz continuity in the whole sample space, only to focus on generated samples, real samples, and the middle area [cit] ."
"having too few layers of the neural network can cause the network to fail to satisfactorily learn the features of the data. too many layers, however, may result in the phenomenon of overfitting. in many experiments, networks with two, three, and four layers have been tested in generators. both the discriminator and classifier are used to find the best number of layers. one of the hyperparameters is the type of activation function in each layer. common activation functions include sigmoid, relu, tanh, and leakyrelu. the leakyrelu function is defined as follows:"
"for each insulin rate at a specific nutrition rate, if the onehourly fifth percentile bg outcome is below the target range, the combination is discarded and the algorithm moves on. otherwise, if the prediction is in tolerance (see fig. 3, panel a), the combination is saved (but does not overwrite if a previous combination is already saved). finally, if the prediction is not in tolerance, but is closer to the lower bound than any previous combinations, it is saved as a possible treatment. the treatment for the first hour does not use the full set of admissibility rules in fig. 3, as a treatment is required to exist at all times. if no insulin/nutrition combination corresponds to a fifth percentile bg outcome above the lower bound of the target range for 1 h, the controller defaults to maximum nutrition and zero insulin."
"the f1-score is the harmonic mean of precision and recall. in imbalanced data sets, the f1-score is much more effective than accuracy in determining the performance of the model. the f1-score is defined as follows:"
"tweets and features of social bots are objective and immutable and are therefore not appropriate to modify and supplement. however, there are many similar factors in a social bot's characteristics, such as occupations, hobbies, and behaviors. in the existing research data on social bots, there is no recognized typical classification standard with a functional meaning division. therefore, clustering social bots by their features is a feasible method that can result in a more reasonable classification label. the sample data of a bot may be generated by different bots, and the spatial distribution rules are different. therefore, the category of the bot is used as auxiliary information to help the cgan generate more realistic samples. we cluster the bot samples to obtain the category information through algorithms."
". finally, assign each remaining point to the same cluster as its nearest neighbor of higher density; the training data set s is divided into k subsets dc 1, dc 2, · · ·, dc k return the k subsets dc 1, dc 2, · · ·, dc k ."
"where x is the number of accounts in the original data set and x r is the number of social bots in the original data set. we let r take the values of 10%, 15%, 20%, 25%, and 30%. in each case, specific numbers of minority samples are sampled randomly and placed into data set p together with all of the normal samples. thus, we obtain four different table 3. the trend diagram of the evaluation indexes pertaining to the classifiers generated by different oversampling methods with changes in ϕ. data sets with different r values. then, p is oversampled using the improved cgan and the compared algorithms previously mentioned, and then classifications are performed. the experimental results are shown in fig. 8 . as r increases, the accuracy of smote, adasyn, and the improved cgan methods gradually increase, and the accuracy of the random algorithm fluctuates in all evaluation indexes. the improved cgan is found performing better than all the other oversampling methods. moreover, the detection effect of the classifiers produced by all of the oversampling methods decreases slightly with an increase in the imbalance degree when r equals 30%. this indicates that when the data set tends toward balance, it is somewhat redundant to augment the data set using oversampling methods. only when the number of the normal human and social bots in the data set differs greatly the effect of the oversampling method becomes obvious."
"when the distribution of the positive and negative samples in the test set changes, the receiver operating characteristic (roc) curve can remain unchanged. therefore, the roc curve is also an important classification index. the area under the curve (auc) is defined as the area under the roc curve. we use the auc as the evaluation index because the roc curve often does not clearly indicate which classifier performs better, and with the auc as a value, the larger the value, the better the classifier performance."
this paper investigated the performance of lte mobility management. we considered the central policy for location update and three paging schemes based on the concept of ta and tal. our study indicates the following results:
"the comparison between star-max and star two-hourly illustrates a known tradeoff between measurement rate (nursing workload) and patient safety. however, both provide quality agc. thus, the main impact of measurement interval in the star framework is on safety from intrapatient variability."
"the relatively higher enteral nutrition rate for star compared to sprint is likely to increase clinical acceptance. higher feed is generally preferred in many cases [cit], despite some recent contradictory evidence [cit] . however, star can be easily adjusted by setting nutrition administration goals to match any emerging evidence, and insulin rates will automatically modify to maintain glycemic balance."
"the proposed improved cgan algorithm is also compared with three more common oversampling algorithms. experimental results show that the improved cgan outperformed the three common oversampling algorithms, with an f1 score of 97.56%. this finding indicates that it is an effective oversampling method in the field of social bot generation."
"the rest of this paper is organized as follows: section ii briefly reviews related research. section iii presents the method for the detection algorithms for malicious social bots, followed by the experiment and result analysis in section iv. section v concludes this paper."
"the true positive rate (tpr) is the ratio of the number of positive predictions that are actually classified as positive to the number of all positive samples, that is, the recall rate. the tpr is defined as follows:"
"the hyperparameters in a neural network affect the performance of the entire network, making it necessary to continuously adjust the hyperparameters of the neural network classifier until the classifier performance is optimal."
"basically, the central policy and the three paging schemes we described for lte mobility management partially implement the movement-based and the distance-based location updates [cit] with the shortest-distance-first (sdf) paging [cit] . although these schemes have been intensively studied in the literature, they have not been exercised in any commercial mobile telecom network because their implementations are not feasible. specifically, in the distance-based location update, the ue is required to have the cell topology information (i.e., the distance relationship between cells) [cit], which can not be practically implemented in a real network. in the sdf paging, it is difficult to dynamically define the neighboring cells (when the radio coverage changes, the \"adjacent cells\" may also change). through the predefined ta configuration, lte can partially implement the distance-based scheme with the sdf paging for commercial operation. in this paper, we show that lte mobility management outperforms third-generation (3g) mobility management by capturing the advantages of the distance-based scheme with the sdf paging. we propose an analytic model to evaluate the performance of the talbased location update with the above three paging schemes. this paper is organized as follows: section 2 introduces the tal-based location update. section 3 proposes an analytic model for modeling the tal-based scheme. section 4 investigates the performance of the tal-based scheme by numerical examples, and the conclusions are given in section 5."
"characteristics of agc varied between protocols. unc showed improved performance over sprint, while providing a relatively light nursing workload. safety was also improved with regards to mild hypoglycemia, although the incidence of severe hypoglycemia was increased. although yale showed improved safety, the number of required measurements was high."
"in recent years, online social networks (osns), in which people can conveniently share and promote news, information, opinions, links, and products, have grown widely. notably, the increase in the number of mobile devices has contributed to an increase in the frequency of user interaction via online social networks. [cit], facebook had 197 million monthly active users. [cit], the number of monthly active users had grown to 2.38 billion [cit], twitter had 30 million monthly active users [cit], the number of monthly active twitter users had grown to 330 million [cit] . however, the typical features of openness and sharing in online social networks have also promoted malicious activity by attackers, the associate editor coordinating the review of this manuscript and approving it for publication was jihwan p. choi . spammers, and fraudsters. social bots are one of the most high-level security threats in osns, as they make osns vulnerable to adversaries. a social bot is computer software that automatically produces content and interacts with humans on social media to emulate and possibly alter their behavior [cit] . the primary objectives of these social bots are to create the illusion that a social network actively influences public opinion [cit], to cause political penetration [cit], and to spread malicious content. on popular social networks, these malicious social bots have had a negative impact on human users. [cit], twitter identified more than 9.9 [cit] . another study found that social bots were responsible for generating 35% of the content that is posted on twitter [cit] ."
"when an incoming call to the ue arrives, it may incur large paging traffic if all cells in the tal page the ue simultaneously. to resolve this issue, we implement three paging schemes in lte. in this paper, an \"interacted cell\" refers to a cell where the ue is paged, makes calls, or performs location update. in other words, the interacted cell is the cell through which the ue had the interaction with the network."
"δ i is defined as the shortest distance between the data point i and the other data point with a higher local density, and this definition is as follows:"
"the clinically validated intensive care insulin-nutritionglucose (icing) metabolic model was used to simulate the fundamental metabolic dynamics [cit] . table i lists the population constants of the model defined in (1)- (6), and a graphical overview is presented in fig. 1"
"now we derive c p;ct, c p;t t, and c p;ct t . fig. 4 illustrates the state-transition diagram for the tal-based location update. in this figure, state j represents that the ue resides in cell j of the tal, where 1 j n c n t . for 2 j n c n t à 1, the ue moves from state j to state j þ 1 with probability p, and moves from state j to state j à 1 with probability 1 à p."
"a notable enhancement of the model-based approach of star compared to the fixed approach in sprint is the ability to change the desired target range and other factors or limits. this ability has implications for the balance between workload, safety, and nutrition. for example, raising the target bg range could provide increased nutrition intake at the expense of higher bg. providing a wider target band may allow reduced workload with fewer bg measurements under the target-torange scheme presented, but may result in increased glycemic variability within that band for which the clinical outcomes are not fully known. these decisions can be made by the attending clinical team based on their goals for a particular patient and assessment of current evidence. hence, the framework can be directly and easily adopted and generalized to any clinical culture and practice, unlike previously published protocols."
"i n a mobile telecom network, the locations of the user equipments (ues) are tracked so that incoming calls can be delivered to the ues. typical mobility management procedures include location update and paging. when a ue moves from one location to another location, the ue reports its new location to the network through the location update procedure. when an incoming call to the ue arrives, the network identifies the location of the ue via the paging procedure."
"the insulin sensitivity parameter used here broadly represents the body's glycemic response to exogenous insulin under the icing model assumptions. as such, mismodeled dynamics and/or sensor errors influence the fitted s i profile in an unphysiological manner. this is true for both virtual trials and real-time control. however, previous validation studies have shown that these profiles are essentially treatment independent [cit], and are therefore useful for conservative prediction of future outcomes. validation of the virtual trial approach was carried out by testing the methodology on independent matched cohorts, where crossvalidation virtual trials accurately repeated clinical results from sliding scale protocols. equally, the results here show that subsequent clinical trials are a good match for predicted response offering further validation."
"the explanation of (18) is similar to that of (13), and the details are omitted. let t be the probability that the ue resides in the ta of the last interacted cell when an incoming call arrives. similar to (17), from (6)- (9), (16) and (18), t is expressed as"
the main contributions of this paper are as follows: 1. we propose a data-augmentation approach by adopting improved cgan generative methods to extend social bot samples.
"to overcome these difficulties, in this work, we propose a data-augmentation approach to address the imbalance in twitter bot detection by adopting conditional generative adversarial networks (cgans). specifically, we propose using the power of the recently discovered density peak clustering to fulfill the task of the condition set of the cgan as well as using the wasserstein distance with a gradient penalty, which minimizes a different distribution divergence than the original cgan, to achieve better performance in terms of convergence."
"3. based on the clustering algorithm, in conjunction with using a cgan to rebalance skewed data sets, we propose an effective imbalanced data oversampling method that avoids the generation of data-augmentation noise and effectively overcomes the imbalances between and within class distribution. 4 . we propose gaussian kernel density peak clustering (gkdpca) as a condition model to label the social bot data in the cgan model. the gkdpca improves the euclidean distance calculation method of the density peak clustering algorithm. we use the gaussian kernel function to project the original features into a high-dimensional kernel space."
"according to the previous cluster center selection criterion, the larger the γ i value of a point, the more likely it is to be a cluster center. after the cluster center is manually selected according to the number of the clusters k, the category of the remaining points is the same as that of the data point with higher density and closer distance than those of points. the gkdpca algorithm is described below."
"on the other hand, for a fixed n c n t value, if p is large, e½m increases as n c increases. in the extreme case, when"
"the original dpca adopts the default euclidean distance to compute the distance between two data points. however, when the data set is complex and linearly inseparable, the euclidean distance can cause severe misclassification. we improve the dpca with the gkdpca, which measures the distance by implicitly mapping the raw data into a high-dimensional feature space. furthermore, to improve the performance of the proposed method, we introduce the gaussian kernel instead of the cut-off kernel to calculate the local density of the data points. the gkdpca can detect non-spherical clustering, which cannot be detected by traditional distance-clustering methods, such as k-means and k-medoids clustering."
"to analyse the effectiveness of the improved cgan in detecting social bots, we use three types of oversampling methods for comparison: the synthetic minority oversampling technique (smote) algorithms, the adaptive synthetic (adasyn) algorithm, and the random oversampling method. these three algorithms are common in the field of data augmentation, and the improvement of the effect on the classifier is obvious. 1) random oversampling [cit] : random oversampling is a simple method of copying minority samples that makes the rules learned by the classifier too specific and causes overfitting problems. 2) smote [cit] : the smote algorithm randomly selects positions on the line of two minority class samples as a new minority class sample. this method improves the accuracy of classifiers for minority classes by increasing the number of minority classes. 3) adasyn [cit] : the adasyn algorithm can adaptively generate bias by reducing the data imbalance for synthetic data samples of the minority classes. at the same time, the adasyn algorithm can be extended to handle imbalances in different scenarios. the implementation of these three algorithms is provided by the scikit-learn library [cit] . we executed these algorithms by calling the appropriate modules in the library. all of the above algorithms need to use the training data set as data input, and the algorithms have parameters that achieve the best performance. the use of these three oversampling algorithms is similar to the improved cgan in the experiment. the three oversampling algorithms use the training set as input data to generate more samples. we use the augmented data set to train the neural network classifier and use the test set to verify the performance of the classifier."
"it should be noted that the target range of 80-145 mg/dl is based on the hypoglycemic risk (5%) being set on 80 mg/dl and the skewed outcome bg distribution that results from controlling hyperglycemia against a counter-regulatory response. as a result, controlled bg clusters in this range with outliers skewed toward higher values. hence, while the range is 80-145 mg/dl, most bg values in table iv are clustered in the 80-125 mg/dl range reflecting this skew. thus, the lower target to 80 mg/dl helps ensure a tight range under control."
"the output of the generator is the input sample for the discriminator. when the input enters the neuron, it is multiplied by the weight. we randomly initialize the weights and update them during model training. in addition to the weights, another linear component applied to the input is the bias. it is added to the result of the weight multiplied by the input. for the initialization of the weights and biases, weight values are drawn from the normal distribution n(0, 1), and biases are drawn from the uniform distribution u (-1, 1) . a neural network generally consists of an input layer, hidden layer, and output layer. since the data dimension and the amount of data used in the experiment are small, we choose three network layers for the default implementation of the neural network."
"similar to the imbalanced data in social bot detection, data imbalance problem imposes challenges in performing data classification in realistic machine learning applications, such as fraud detection, text classification, face and image recognition, and medical diagnosis [cit] . the performance of classifiers leans to be partial toward majority classes in imbalanced data sets [cit] . various approaches have been proposed to overcome the problem of imbalanced data classification in the past decade [cit] . these approaches can be broadly divided into two main categories: data-driven approaches and algorithm-driven approaches [cit] ."
"we first derive c u . let e½m be the expected number of cell crossings before the ue leaves the current tal (i.e., the expected number of cell crossings between two consecutive location updates). then, c u can be computed as"
the experiments are conducted with the aim of answering the following questions: 1) does the improved cgan oversampling method improve the accuracy of bot detection?
"therefore, in this study, we improve the cgan by introducing the wasserstein distance with a gradient penalty and a condition-generation method through the modified density peak clustering algorithm. the structure of the improved cgan is shown in fig.2 . the discriminator has to correctly label real samples that come from the training data set as ''real,'' and it has to correctly label generated samples from the generator as ''fake.'' the generator is fed with random noise, z, and the labels from the gkdpca. the discriminator determines whether each input sample comes from the generator or the real environment."
"bin wu received the ph.d. degree in signal and information processing from the beijing university of posts and telecommunications. he is currently a lecturer with the national disaster recovery technology engineering laboratory, beijing university of posts and telecommunications. his research interests include network security, intrusion detection, social engineering, and artificial intelligence security. le liu received the bachelor's degree in cyberspace security from the beijing university of posts and telecommunications (bupt), where he is currently pursuing the master's degree with the school of cyberspace security. his research interests include network security, social engineering, and artificial intelligence security. xiujuan wang received the ph.d. [cit] . she is currently an instructor lecturer with the college of computer sciences, beijing university of technology. her research interests include information and signal processing, network security, and network coding."
"in long term evolution (lte), the mobility management entity (mme; fig. 1a ) is responsible for the mobility management function [cit], which is connected to a group of evolved node bs (enbs; the lte term for base stations; see fig. 1b ). the radio coverage of an enb (or a sector of the enb) is called a cell (see the dashed squares; fig. 1c) . every cell has a unique cell identity. the cells are grouped into the tracking areas (tas; e.g., ta 1 contains cell 1 and cell 2 in fig. 1d ). every ta has a unique ta identity (tai). the tas are further grouped into ta lists (tals) [cit] . in fig. 1, tal 1 consists of ta 2, ta 3, and ta 4 ( fig. 1e) ."
"iii. results table iv shows that both versions of star reduce the number of cases of severe hypoglycemia and more than halve the measures of mild hypoglycemia compared to sprint. there is a 79% reduction in severe hypoglycemia between star twohourly and sprint showing the importance of the star approach independent of measurement interval, as both protocols have a 2-h maximum interval and similar measurements per day. these safety gains are introduced with reduced clinical effort of 11.8 and 14.9 measures/day compared to 16.1 for sprint, and with equivalent or higher time in desired glycemic bands. importantly, median nutrition rates are raised by 32% (absolute) of accp goal rate. thus, all indications show that star will provide global improvements over sprint in performance, safety, and effort when applied clinically."
"malicious social bots affect information security and network environments by performing harmful operations and disseminating false information with malicious intent. therefore, it is a crucial and urgent task to detect and remove malicious social bots in online social networks. however, in the real world the number of social bots is far less than the number of normal humans, leading to a serious sample imbalance problem in social network bot detection methods based on machine learning, and this conclusion has been verified in experimental environments [cit] . in social bot classification detection using machine-learning methods, imbalances in training data can cause a difference in the proportion of positive and negative samples, and the final result may be misjudged to some extent [cit] . this data imbalance can be addressed by both undersampling and oversampling techniques. in a social-bot-detection scenario, undersampling is not suitable because of the loss of data information in most categories. the data augmentation algorithm is an important method for solving the problem of data-set imbalance faced by the oversampling technique, and it has been applied to fields including computer vision, scene reconstruction, voice data augmentation, and natural language processing [cit] ."
"future work may focus on malicious social bot detection. additional behavioral patterns and feature sequences of malicious social bots will be further considered. we also want to extend this work to other social networks, such as facebook and instagram, with the aim of creating a model for bot detection on social networks, network processing, network security, and network coding."
"the undersampling removes some set of data samples from the majority class, which has the possibility of losing informative samples. in contrast, the oversampling approach tends to increase the size of the minority class to obtain balanced classes, which results in overfitting and increases the time of the training phase [cit] ."
"in the following equation,x represents samples that are linearly interpolated by the real data x r and the fake data x g generated from the generator g(x):"
"six types of features are selected for the social-bot-detection process [cit] : user meta-data, sentiment, friends, content, network, and timing. we select the following 11 types of features with better classification abilities for social bot detection: 1) average number of topic tags: the average of the topic tags in the tweets. the ''#'' and other forms indicate that the tweet is highly correlated with a specific topic, and social bots want to expand influence quickly by publishing tweets on popular topics. 2) average number of user mentions: the average number of users mentioned by the user in the tweet. social accounts can be notified via ''@username'' for some tweets, and a social bot speeds up information diffusion by mentioning users more frequently than normal users do. 3) number of links: the average number of urls in the tweet. the tweet content in social networks can include urls, images, and other files. the social bots are more likely to add more links than normal users do, which induces normal users to click and redirects them to malware websites for social-engineering attacks. 4) number of retweets: the ratio of the number of tweets that belong to retweets (i.e., forwards of tweets) of other users to the total number of tweets. normal users only retweet tweets in which they are interested, whereas social bots always retweet other users' tweets to stay active in the social network. 5) number of favorites: the total number of the users' favorites. normal users can express their concern by favoriting some tweets about topics of interest, but social bots also use this tactic to increase their influence. generally the tweet content of social bots is favorited less frequently than that of normal human users. 6) ratio of followers to the number followed: online relationships for normal humans are with people they know in real life, whereas social bots have a small number of fans and always follow a large number of strangers to improve their influence in social networks. 7) tweet source: the number of tweet sources that are official. normal human users send tweets through a variety of platforms recommended by twitter. the source of tweets sent by social bots is unofficial because social bots are controlled by automatic programs. 8) similarity of content: the latent semantic text content similarity of the original tweet. latent semantic analysis (lsa) [cit] analyzes potential connections between documents and words by extracting the words from documents into a vector-semantic space. if different words appear in the same document multiple times, the words are semantically similar. lsa extracts the text-collection matrix from the data set. the rows of this matrix represent words, the columns represent documents, and the specific values of the matrix elements represent the number of times that a word appears in the document; then the matrix is subjected to singular value decomposition (svd). we convert a data text into matrix a as follows:"
"let c be the probability that the ue resides in the last interacted cell when an incoming call arrives. from (6)- (9), (13) and (16), c is expressed as"
"directly quantifying and managing hyperglycemic and hypoglycemic risk as a function of inter-and intra-patient metabolic variability can leverage agc benefits and minimize risk of unintended harm. stochastic targeted (star) is a model-based agc framework that can be implemented across a range of clinical scenarios and approaches, and uses dynamic and stochastic models to regulate blood glucose (bg) levels, workload, and patient safety within a predefined risk management approach. star uses stochastic forecasting of a patient's potential metabolic variability [cit] in conjunction with a clinically validated mathematical model [cit] to determine optimal insulin and nutrition treatment combinations with specified risks of moderate hyper-and hypo-glycemia. in essence, it is a patientspecific approach to manage inter-and intra-patient variability that overlaps the glycemic outcome range for a given intervention with a clinically specified desired glycemic range. it thus provides both control and a clinically specified risk of mild hypoglycemia. hence, star provides a framework of models and methods to manage intra-and inter-patient variability to mitigate the significant difficulty and risk seen in current glycemic control approaches [cit] ."
"finally, the virtual trial approach is further validated in comparison to results for the first ten patients (1486 h, demographic shown in table iii ) in initial star pilot clinical trials. all patients were treated for length of stay after informed consent was obtained. approval for this study and use of the data was given by the upper south a ethics committee (ura/10/09/069)."
"to analyze the effectiveness of the improved cgan method in detecting social bots, we performed three experiments with the original algorithm and three traditional oversampling algorithms. we use the same dataset and different oversampling methods to generate a balanced data set. the same hyperparameters and balanced data sets from different sources are used in the training process of the classifier, and the performance of the classifier is detected through the same test set."
"we consider the gamma distribution because it has been shown that the distribution of any positive random variable can be approximated by a mixture of gamma distributions [cit] . the gamma distribution was used to model ue movement in many studies [cit] and is used in this paper to investigate the impact of variance for cell residence times. from (15), (14) is rewritten as"
"we use t-distributed stochastic neighbor embedding (t-sne) to map the original data set and the augmented data set from the high-dimensional space to 2d space. the t-sne is a machine-learning algorithm for nonlinear dimensionality reduction. it is suitable for reducing high-dimensional data to 2d or 3d forms for visualization. fig. 4 shows, the relationship between the data generated by the improved cgan and the original data. the generated bot data and the existing bots satisfy certain correlations and differ from the normal human samples. the difference between normal humans and social bots on a 2d plane is shown in fig. 5(a) . the yellow dots represent normal human samples, the purple dots represent social bot samples, and a small number of social bots overlap with normal human samples. some interference samples affect the accuracy of classification. as fig. 5(b) shows, the social bot samples generated by the improved cgan are somewhat different from the normal human samples and original social bots. the spatial distribution of social bots only includes the two purple areas in the fig. 5(a) . in contrast, the samples generated by the improved cgan in the fig. 5(b) also conform to the spatial distribution of existing social bots, which can improve the detection of the social bots."
"achieving agc requires accounting for patient metabolic variability while balancing safety and workload requirements. in-silico results indicate that the developed star algorithm provides a safe and effective method for management of glycemia. both overall cohort and per-patient findings support the implementation of star in pilot clinical trials whose initial results have further validated the in-silico approach in this research. the model-based nature of star allows easy adjustment of bg and nutrition to match emerging clinical evidence, and permits individualization of treatment goals to particular patient outcomes. finally, the overall framework presented is unique in its stochastic, risk-based approach, as well as completely generalizable."
"our lab study also illustrated the tradeoffs between explicit and implicit control of motion. explicit control showed the intent of the satellite more clearly, and was preferred by the satellite confederate, but incurred a delay in operating the interface to aim the display. some evidence for this delay is found in the longer speech segment measure for the explicit condition compared to both implicit and stationary conditions. this measure may reflect protracted speech while the satellite is simultaneously aiming the direction of the proxy. or, explicitly aiming the display may leave the satellite's \"gaze\" aimed at a conversation partner longer than natural, protracting the partner's speech turn until the display turns away."
"having found an optimal vector n, a centralized optimal algorithm can easily assign the copies to different nodes (e.g., picking nodes sequentially and filling their buffers up with any nonduplicate copy, starting from the messages with highest assigned n i -due to uniform mobility the choice of specific nodes does not matter). it is important to note that, given this assignment, no further message replication or drop is needed. this is the optimal resource allocation averaged over all possible future node movements. the optimal algorithm must perform the same process at every subsequent time step in order to account for new messages, messages delivered, and the smaller remaining times of undelivered messages."
"specifically, it tests different initial parameters (fig a1b) in lieu of the true prior parameters used 850 (fig a1a, left) to see whether these provide better explanations for the observed data (fig a1a, right) . to computed the bma-ed posterior e concentration, which is used as the prior for the next day. 865"
"we then asked how the specialist and generalist mice perform when transported to different 408 environments. we constructed three testing environments (fig 6a) : the specialized environment, 409 similar to the environment the specialized agent is trained in; namely, with rewards that only appear 410 on the left side of the starting location (low volatility); the general environment containing rewards 411 that may appear in any of the four final locations (high volatility); additionally, the novel environment 412 has reward only on the right side of the starting location (low volatility)."
"we next turn our attention to minimizing the average delivery delay. we now assume that all messages generated have infinite t t l or at least a t t l value large enough to ensure a delivery probability close to one. the following theorem derives the optimal per-message utility, for the same setting and assumptions as theorem 3.1."
"the final questionnaire included three questions that asked which of the three conditions the participant felt the group communicated best with the satellite participant, as well as which condition was most preferred and least preferred."
"to explore how kinetic proxy movement may address the newscaster and skip-over effects, we implemented several forms of motion control, and assessed hub participants' perceptions of the differences. we performed a lab study of the prototype, comparing a stationary proxy, a kinetic proxy under the satellite's explicit control by mouse cursor, and a kinetic proxy controlled implicitly by the satellite's head motion. this is the first study to directly compare alternative ways to project attention and evaluate people's responses."
"implicit control tried to lessen the delay in moving the screen and reduce the cognitive burden on the satellite in aiming the display. however, it also added more ambiguity in the intention of the satellite, and the increased amount of motion exposes the hub to more of the negatives of motion (i.e., distraction, noise). head motion in the physical world can be communicative (e.g., turning toward someone to elicit their response) or incidental (e.g., a side effect of not being able to remain completely still). but the kinetic display motion generated by the turntable was largely interpreted as communicative. consequently, implicit control caused incidental head movement to be perceived as communicative movement, leading to more of a sense of distraction. in this way, implicit control transferred cognitive effort from the satellite to the hub."
"graph g construction. when we construct the graph g from the linked web apis dataset, for every predicate we create a bidirectional edge. a graph with bidirectional edges provides a richer dataset for maximum activation evaluation. a large graph with unidirectional edges may contain many dead end paths that may limit the number of improving paths that the algorithm would be able to find from the source to the target nodes. evaluation of maximum activation on such graph would not provide many interesting results."
"implicit control of the proxy results in more natural interactions, with lower cognitive effort, and greater sense of the satellite participant's reactions, compared to explicit control."
"note that there are other information in the programmableweb that we could use to make the linked web apis richer such as various technical information about protocols and data formats. in addition, we could better associate the data with other datasets in the link data cloud and publish it to the linked data community. although we plan to do this in our future work, the linked web apis that we present here already provide the sufficient dataset for our web api selection method."
"a node can only update its knowledge about the state of a message a at a node b when it either meets b directly, or it meets a node that has more recent information about the (a; b) tuple. the goal of the statistics collection method is that, through such message exchanges, nodes converge to a unified view about the state of a given message at any buffer in the network, during that message's lifetime."
"to test this, in tables 6 and 7, we compare the performance of the hbsd policy against a simple combination of \"drop oldest message\" (for buffer management) and \"transmit youngest message first\" (for scheduling during a contact). we observe, that in the low congestion regime, the two policies indeed have similar performance (4 and 5 percent difference in delivery rate and delivery delay, respectively). however, in the case of a congested network, hbsd clearly outperforms the simple policy combination."
"we ran the study as a within-subject design to encourage participants to make comparisons that primarily reflect the absence or presence of motion affordances, and their form of user control. each group of participants experienced all of the following three conditions:"
"our study shows that despite not achieving true eye contact, the kinetic proxy does project the satellite's attention focus so that hub participants could have engaging conversations and correctly respond to deictic requests. our experiences with the lab study have led us to explore teasing apart attention awareness from gaze awareness."
"interestingly, we note that specialist formation requiring a conservative training environment 503 adheres to the requirements specified by k. anders ericsson in his theory of deliberate practice -a 504 framework for any individual to continuously improve until achieving mastery in a particular field 505 (34,38,39). ericsson establishes that deliberate practice requires a well-defined goal with clear 506 feedback (low volatility learning environment) and ample opportunity for repetition and refinement 507 of one's performance (training, repetition and, potentially, bayesian model reduction during sleep). 508"
"for our experiments we use the full linked web apis dataset. the dataset contains all user profiles for users that created at least one mashup. we also extracted profiles on all categories, tags, mashups and web apis. [cit], till may 18th, 2012. the snapshot includes 5 988 apis, 6 628 mashups and 2 335 user profiles. in the experiments we addressed following questions:"
"in this section we present a case study for personalised api selection to illustrate capabilities of our maximum activation method. we have a developer who wants to improve tourists' experience in new york, usa by creating the visitor mashup. the visitor mashup should aggregate information about different events and information about restaurants in the city and in the area of new york (north america has a better geographical coverage by existing web apis then europe hence we decided to locate our case study in new york). information about various points of events and restaurants should be layered on the map and dynamically updated when tourists change their locations and new events and restaurants become available."
"a popularity and number of web apis and mashups utilising them is steadily increasing. however, current approaches to web apis search and selection use rankings based on a single criteria -web api popularity-based ranking. the popularity-based rankings works well with the large, widely-known and wellestablished apis, however, impedes adoption of more recent, newly created apis. in order to address this problem we proposed a novel activation-based web api selection method which takes into account user profile and user's preferences, temporal aspects (the creation time of web apis and mashups) and social links between users. existing popularity-based rankings use a single-dimensional ranking criteria (e.g., a number of apis used by mashups) whereas our selection approach is based on multi-dimensional ranking criteria using graph analysis and therefore providing richer results. by using our method, a user can specify that he would like to use apis that he has some previous experience with in similar contexts or by the people that he knows. our activation-based api selection approach gives its users much more control over their search preferences and results."
"by distinguishing between gaze awareness and attention awareness, what we learned in our lab study generalizes beyond the particular turntable proxy that we examined. the motion of our turntable proxy did provide a stronger sense of attention projection and awareness, even though it did not offer true eye contact."
"we also need to take into a consideration what has happened in the network since the message generation, in the absence of an explicit delivery notification (this part is not considered in rapid [cit], making the utility function derived there suboptimal). given that all nodes including the destination have the same chance to see the message, the probability that a message i has been already delivered is equal to"
where f l ðr i þ is the laplace transform of the above normal distribution. 5 after some algebraic manipulations we can get the new marginal utility for message i
"we first describe our problem setting and the assumptions for our theoretical framework. we then use this framework to identify the optimal policy, gbsd. this policy uses global knowledge about the state of each message in the network (number of replicas). hence, it is difficult to implement it in a real world scenario, and will only serve as reference. in section 3.1, we will propose a policy based on statistical learning that can approximate this optimal policy."
"buffer management and scheduling. let us consider a time instant when a new contact occurs between nodes i and j. the following resource allocation problem arises when nodes are confronted with limited resources (i.e., contact bandwidth and buffer space). 2 scheduling problem. if i has x messages in its local buffer that it should forward to j (chosen by the routing algorithm), but does not know if the contact will last long enough to forward all messages, which ones should it send first, so as to maximize the global delivery probability for all messages currently in the network?"
explicit control: the proxy screen swiveled in response to the satellite participant explicitly selecting the location she wanted to aim her proxy towards. the position of the proxy was directly linked to the position of the mouse cursor over the panorama view (there was no need to click the mouse button).
"interspersed through each session, the confederate satellite participant would directly address one of the hub group members with a question of the form \"what do you think?\" or \"what type of food do you like?\" often, this first question was followed with similar second or third questions to the other two members about their opinions. responses to these questions were tallied during subsequent video analysis, paying attention to whether it was a first, second, or third question in a series. this distinction is important, because there are three potential respondents to a first question, while there are two for a second question, and only one for a third. because of this increasing likelihood of responding correctly, we only analyzed responses for the first question according to the following protocol:"
"in a large enough network, even if the actual distribution of meeting rates is not known, a node could still derive good utility approximations, by measuring and maintaining an estimate for the first and second moments of observed or reported meeting rates (e.g., with techniques similar to the ones discussed in section 3.5). furthermore, the homogeneous assumption could be considered as a useful approximation for large networks where the common rate is taken as \""
"this likely resulted from the \"two-step\" maze being a relatively simple task. as agents are 586 incentivized to go to the very end of the maze to receive a reward, the naïve agents are not at a 587 disadvantage compared to generalists (since both have equal prior beliefs about the final locations). 588"
two other codes are possible: 1) someone other than the intended person checked or confirmed whether he or she was being addressed before responding and 2) more than one person responded immediately but this did not include the intended person. neither of these categories was present in our data.
"our lab study results showed that motion in the kinetic conditions performed better than the stationary condition in terms of conversational engagement, accurate responses to deictic prompts, and trends in user ranking, confirming h1. a participant illustrated these results with the comment, \"the motorized action brought the remote person to life.\" hub participants were able to perceive the satellite's attention in motion through the swiveling of the display."
"there have been only a few works in the past that have attempted to generate protocols automatically while extremizing an objective function. first, there is the work on the automatic generation of security protocols [cit] . the second body of related work is layering via optimization decomposition [cit], which does not incorporate control into optimization, and produces optimal results for the regime where data is much longer than control. however, for wireless networks, in particular for mac protocols for wireless sensor networks, the control information exchanges and data take up comparable resources. in contrast, the framework in this paper incorporates the impact of control into optimization."
. cc-by-nc-nd 4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint
"the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint overall, we find that becoming a specialist versus a generalist has sensible trade-offs. the benefit of 449 specialization is substantial when operating within the same environment, consistent with data on 450 this topic in a healthcare setting (28,29). however, if the underlying environment is different, then 451 performances can decrease to one which is poorer than the performance without specialization. 452"
"we develop a method called the maximum activation and show how it can be used for web api selection. the method evaluates a maximum activation from initial nodes of the graph (defined by user's profile), to each node from a set where a node in a set represents a web api candidate. we adopt the term activation from the spreading activation method [cit] and we use it as a measure of a connectivity between source nodes (initial nodes defined by user profile) and a target node (a web api candidate). we use flow networks as an underlying concept for evaluation of the maximum activation in the graph. we implement the method as a gephi plugin 4, and we evaluate it on several experiments and show that the method gives better results over traditional popularity-based recommendations."
"since the satellite in the lab study was a confederate, she was able to reflect on her experiences across all groups and conditions. she found that in the explicit condition, she became more consciously aware of who she was directing her attention to than in the implicit or stationary conditions. each movement of the mouse was made in order to either demonstrate listening to a particular person or direct speech toward someone. but over time, intentionally using the mouse in this way became more like a natural extension of her nonverbal communication and movements became more automatic."
"the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint people trained in a perceptual learning task perform well in the same task, but perform worse than 493 naïve subjects when the distractor and target set are reversed -and take much longer to re-learn the 494 optimal response than new subjects who were untrained (37). 495"
"in what follows, we will briefly review the tenets of active inference, describe our simulation set up 101 and then review the behavioural phenomenology in light of the questions posed above. 102"
"we will henceforth refer to the agent trained in the non-volatile environment as the specialist agent, 378 and the agent trained in the volatile environment as the generalist agent. anthropomorphically, the 379 specialist agent is, a priori, more confident about what to do: since the reward has appeared in the 380 leftward location its entire life, it is confident that it will continue to appear in the left, thus it has 381 predilections for left-going policies (policies 1 and 2 of fig 3b) . conversely, the generalist agent has 382 seen reward appear in multiple locations, thus it experiences a greater level of uncertainty and 383 considers more policies as being useful, even the ones it never uses. we can think of these as being 384"
"the second method, which strikes a balance between bma and bayesian model selection with respect 555 to the consideration of uncertainty, is bma with occam's window (50). in short, a threshold is 556 established, z, and if the log evidence of any reduced model is not within z, we simply do not 557 consider that reduced model. neurobiologically, this would correspond to the effective silencing of a 558 synapse if it falls below a certain strength (51). this way, multiple reduced models and relative 559 . cc-by-nc-nd 4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint uncertainties are still considered, but a great degree of computational cost is saved since less reduced 560 models are considered overall. 561"
"one limitation of our simulations was that our agents did not learn about cues at the same time they 576 were learning about policies; in fact, the agents were constructed with priors on which actions were 577 likely to lead to rewards, given specific cues (that is, a correctly perceived cue-left was believed by 578 the agents to -and actually did -always lead to a reward on the left). as such, we did not model the 579 learning of cue-outcome associations and how these may interact with habit formation. we argue 580 this is a reasonable approximation to real behaviour; where an animal or human first learns how cues 581 . cc-by-nc-nd 4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint are related to outcomes, and, once they have correctly derived a model of environmental 582 contingencies, can then proceed to optimising policy selection. 583 additionally, while we were able to see a significant performance difference between specialist and 584 generalist agents, there was little distinction between the performance of generalist and naïve agents. 585"
"a.1 meeting times are exponentially distributed or have at least an exponential tail. a.2 nodes move independently of each other. a.3 mobility is homogeneous, that is, all node pairs have the same meeting rate . regarding, the first assumption, it has been shown that many simple synthetic mobility models like random walk, random waypoint and random direction [cit] have such a property. furthermore, it is a known result in the theory of random walks on graphs that hitting times on subsets of vertices usually have an exponential tail [cit] . finally, it has recently been argued that meeting and intermeeting times observed in many traces also exhibit an exponential tail [cit] . as we will see in section 3.2, in our framework, we sample the remaining meeting time only when a drop or scheduling decision needs to be taken. in a sparse network (as in our case), it can be shown that, at this time, the two nodes in question have already mixed with high probability. thus, the quantity sampled can be approximated by the meeting time from stationarity, or the tail of the intermeeting time distribution, which, as explained, is often exponential [cit] . in other words, it is not required to make the stronger assumption of poisson distributed intermeeting times, as often done in related literature."
"multiview [cit] preserved gaze and gesture spatial relationships for groups of participants in a two-site (extensible to three) conference. multiple cameras at each site-one facing each participant-sent multiple video streams to a directional viewscreen at the other site. positioning was carefully arranged so that every participant at one site had a correct, angle-adjusted view of every participant at the other site, relative to his or her seating locations."
"there is an important connection between these model optimisation procedures, and those 329 processes thought to occur during sleep. this is because a variational free energy minimising 330 creature tries to optimise a generative model that is both accurate and simple -i.e. that uses the least 331 complicated explanation to describe the greatest number of observations. mathematically, this 332 follows from the fact that surprise can be expressed mathematically as model evidence -and model 333 evidence is the difference between accuracy and complexity. during wakefulness, an organism 334 constantly receives perceptual information, and forms accurate yet potentially complex models to 335 explain this (neurobiologically, via increases in the number and strength of synaptic connections 336 through associative plasticity). during sleep, which lacks any precise sensory input, creatures can 337 optimise their models post hoc with the goal of reducing complexity (24). this can be achieved by 338 considering reduced (simpler) models and seeing how well they explain the data collected during 339 waking hours (22). this is sometimes called bayesian model reduction and is analogous to the 340 ."
"distribution of the hbsd delivery rate and delivery delay utilities, respectively, for this low congestion scenario. surprisingly, our hbsd policy behaves very differently now, with both utility functions decaying monotonically as a function of time (albeit not at constant rate). this suggests that the optimal policy in low congestion regimes could be approximated by the simpler \"drop oldest message\" (or schedule younger messages first) policy, which does not require any signaling and statistics collection between nodes."
"as is evident from the above description, the gbsd policy is a greedy, locally optimal policy. however, greedy 3 . we say that a node a has \"seen\" a message i, when a had received a copy of message i in the past, regardless of whether it still has the copy or has already removed it from its buffer. policies in general, are not guaranteed to converge to globally optimal outcomes. we will investigate the optimality properties of gbsd further in section 3.5."
"the average throughput (reward) per slot on a markov chain is calculated as follows: define a \"cycle\" of a recurrent markov chain as the pair of events from the time that the chain starts in a recurrent state to the time that the chain returns to the same state (for the first time). let t denote the duration of this cycle. (note that t is a random variable.) let f denote the reward that the chain collects (on its arcs) in that cycle. that, independent of the choice of the recurrent state, the average reward per slot is computed as e[f ]/e[t ]. hence, our first task is to automatically generate the expression for this objective function."
"maximum activation and edges in c. the edges in c are constraining the maximum activation which means that if capacities of such edges increase, the maximum activation can be increased. note, however, that we assign capacities based on semantics of egdes thus by changing a capacity on an edge in c, we also change capacities on other edges not in c. running the algorithm again on the graph with new capacities will lead to a different set c and different maximum activation. in other words, it does not hold that increasing a capacity on any edge in c will lead to a higher maximum activation. this also means that maximum activation that our algorithm evaluates has a global meaning while activations on individual edges do not have any meaning. defining capacities for individual edges is the subject of our future work."
"i. introduction 1 the automated design of networking protocols is a new approach to protocol design, with substantial potential benefits to reduce the protocol design cycle, labor costs, and design errors while delivering high-performance protocols in a wide range of application-specific contexts [cit] 's, design automation of protocols is poised to bring similar benefits to the design of networks. medium access control (mac) is one of the critical layers in wireless networks, with direct impact on protocol performance (throughput, energy consumption, or delay), and one that can benefit immensely from protocol automation. our aim in this paper is to demonstrate, through a case study, a methodology by which the design of wireless mac protocols can be automated."
"as an agent acts to minimize their free energy, they must also look forward in time and pursue the 125 policy which they expect would best minimize their free energy. the contribution to the expected 126 free energy from a given time, (, ), is the free energy associated with that time, conditioned on 127 the policy, and averaged with respect to a posterior predictive distribution (15)"
-what is the impact of user preference function on results of the maximum activation? -how does the ageing factor influence the maximum activation? -how can the popularity of an api evolve over a time? -how to make the process of building a mashup more personalised and contextualised?
"mobility model. another important element in our analytical framework is the impact of mobility. in the dtn context, message transmissions occur only when nodes encounter each other. thus, the time elapsed between node meetings is the basic delay component. the meeting time distribution is a basic property of the mobility model assumed [cit] . 1 to formulate the optimal policy problem, we will first assume a class of mobility models that has the following properties."
"however, we also discovered some tradeoffs with motion. the swivel motion of the display could clearly communicate focus of attention (\"rotating lcd made it more clear who remote participant was talking to.\"). but, the more general locus of attention suffered, especially because swiveling toward one hub participant often meant that another hub participant was left looking at the edge of the display screen (\"…when the remote person was talking to other people, i couldn't see her and i felt excluded.\"). given the flat surface of the display screen, swiveling the display was more narrowly directional than the physical affordance of head and body orientation. we expect that these tradeoffs are the main reason why participants rated the motion conditions as lower in a sense of 'being there.' thus there was a tension in motion as it directed attention towards some hub participants but excluded others."
"we have collected the measurements of the computational complexity by measuring the execution time of the entire procedure in matlab, when the process was run on a dell studio 540 mini-tower, with intel core 2 quad processor q9550 (2.83ghz, 1333mhz fsb) and 12mb cache. fig.6 (b) displays the plots for both the time needed for the symbolic generation of the optimization program, and the process of solving the optimization program, using the tictoc feature of matlab, when the program was run on a dell studio 540, in the absence of any other active computationally intensive processes. the plot shows that the generation of the symbolic expression of the optimization program dominates the total computation time. (this will be reduced by performing the symbolic generation in a native language such as c, rather than in an interpreter such as matlab.) for this case study, because this is a broadcast channel, the number of states in the reduced global state space remains the same as n grows; hence, the complexity of symbolic generation is roughly constant in n . if control information were unicast, computational complexity would grow with n ."
"the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint locations and staying there, and policy 7 has the agent not moving from its initial location for the entire duration of prior is placed over these distributions. since the dirichlet distribution is the conjugate prior for 287 categorical distributions, we can update our dirichlet prior with categorical data and arrive at a 288 posterior that is still dirichlet (20) . 289"
attention is fundamental to the flow of face-to-face conversations. each participant projects cues of what he is paying attention to and other participants interpret these cues to maintain awareness of his locus of attention. this awareness helps them understand his deictic references. both production and consumption of awareness cues occur at conscious and subconscious levels [cit] .
"-restaurant api. the developer searches restaurants api by adding \"'food\", \"restaurants\" and \"menus\" categories to his profile. this time the developer decides to use his social links and to look for apis used by his friends developers that he adds to his profile. table 6 shows the highest ranked apis singleplatform, menu mania and boorah. the developer selects singleplatform api for restaurant information and recommendations."
"we set out to explore using motion to improve interaction with the satellite participant. we discovered that motion helps, but has some tradeoffs. swiveling a flat display screen toward one hub participant often excludes other participants, which can diminish the sense of presence of the satellite. plus, rotating the visual mass of a display incurs lag and some found it to be distracting. furthermore, swiveling the display did not succeed in improving eye contact."
"the results of the previous section clearly show that our distributed policy (hbsd), using estimators of global message state, successfully approximates the performance of the optimal policy (gbsd). nevertheless, in order to derive good estimators in a distributed manner, nodes need to exchange (a possibly large amount of) metadata during every node meeting. potentially, each node needs to know the history of all messages having passed through a node's buffer, for all network nodes. in large networks, this method quickly becomes unscalable with control data interfering with data transmissions, if statistics maintenance and collection is naively done."
"a number of embodied telepresence systems have focused on kinetic proxies for huband-satellite interactions. prop [cit] was a series of explorations of mobile, robotic personal stand-ins, composed of a video camera and lcd panel (and later, a small pointer) mounted atop a vertical pole and connected to a drivable base. due to mobility constraints, prop's primary means of directed gaze was through a pan-tiltzoom camera head, which served as a partial indicator of the operator's focus of attention, much like our proxies. but as we have found, this can be an ambiguous cue, as the camera may not always follow the operator's attention, or agree with his or her gaze. this overall form and interaction experience has recently appeared in commercial telepresence robots, including willow garage's texai [cit], anybots' qb [cit], and intouch health's remote presence [cit] ."
"once the agent gets to the final location, it will receive either a reward (if it is at the reward location) 246 or be punished. 247 ."
"a different, more robust approach is to find estimators for the unknown quantities involved in the calculation of message utilities, namely m and n. we do this by designing and implementing a learning process that permits a dtn node to gather knowledge about the global network state at different times in the past, by making in-band exchanges with other nodes. each node maintains a list of encountered nodes and the state of each message carried by them as a function of time. specifically, it logs whether a given message was present at a given time t in a node's buffer (counting toward n) or whether it was encountered earlier but is not anymore stored, e.g., it was dropped (counting toward m). in section 6, we describe our statistics maintenance and collection method, in more detail, along with various optimizations to considerably reduce the signaling overhead."
"this view gives the satellite a good sense of the spatial relationships among the people and objects in the meeting room. he can maintain awareness of the locus of attention for each of the hub participants. because the camera the satellite views is positioned near the screen representing him, he has a good sense of when a hub participant looks directly at him or gestures toward him."
"despite a large amount of effort invested in the design of efficient routing algorithms for dtns, there has not been a similar focus on queue management and message scheduling. yet, the combination of long-term storage and the, often expensive, message replication performed by many dtn routing protocols [cit] impose a high bandwidth and storage overhead on wireless nodes [cit] . moreover, the data units disseminated in this context, called bundles, are selfcontained, application-level data units, which can often be large [cit] . as a result, it is expected that nodes' buffers, in this context, will often operate at full capacity. similarly, the available bandwidth during a contact could be insufficient to communicate all intended messages. consequently, regardless of the specific routing algorithm used, it is important to have: 1) efficient drop policies to decide which message(s) should be discarded when a node's buffer is full, and 2) efficient scheduling policies to decide which message(s) should be chosen to exchange with another encountered node when bandwidth is limited and in which order."
"7. in future work, we intend to evaluate the effect of variable message size and its implications for our optimization framework. in general, utilitybased scheduling problems with variable sized messages can often be mapped to knapsack problems (see, e.g., [cit] )."
"-maps api. developer builds his profile adding \"maps\" and \"location\" categories to it. he assigns a high capacity to \"api-category\". table 4 shows the highest ranked results: google maps, microsoft bing maps and yahoo maps. the developer decides to select the google maps api. -events api. the developer further searches for events api by updating his profile with \"events\" category, adding \"google maps api\" and preserving \"maps\" and \"location\" categories. further, he increases the capacity of \"mashup-api\". table 5 shows highest ranked results: seatwave, eventful and upcoming.rg. the developer selects seatwave api."
"this leads it to the correct reward location. the nature of the maze is such that the agent cannot move 244 backward; i.e., once it reaches the intermediate location it can no longer return to the initial location. 245"
"a partially observable markov decision process (pomdp, or mdp for short) is a generative model for 133 modelling discrete hidden states with probabilistic transitions that depend upon a policy. this 134 framework is useful for formalizing planning and decision making problems and has various 135 applications in artificial intelligence and robotics (16). an mdp comprises two types of hidden 136 variables which the agent must infer: hidden states () and policies ( ). an mdp agent must then 137 navigate its environment, armed with a generative model that specifies the joint probability 138 distribution of observed outcomes and their hidden causes, and the imperative of minimizing free 139 energy. the states, outcomes and policies are defined more concretely in the following sections. 140"
"moreover, the hub participants do not have a visceral sense of mutual eye contact with the satellite participant. when he looks straight at the camera, all of the hubs perceive him to be looking straight at each one of them. we call this the newscaster effect 1 [cit] . when he looks to the left, none of the hubs perceive him to be looking directly at any of them, but instead experience him to be looking over their right shoulder. this disruption of eye gaze awareness adds to making it difficult for the hubs to maintain awareness of the satellite's focus of attention."
"during each condition, the group performed a decision-making task with no right answer [cit] that was intended to evoke discussion and interaction within the group. the following three tasks were always performed in the same order."
"developer starts the process of building the visitor mashup by identifying groups of relevant apis. as he progresses and selects apis, the ranking process becomes more personalised and contextualised. the process of creating the visitor mashup is described by following steps when in each step the developer selects one api:"
"we found that the kinetic conditions were generally better than the stationary condition, with interesting caveats. for example, screen motion toward one person is more akin to turning one's back (rather than one's head) toward someone else. we also found unexpected and previously undescribed benefits and drawbacks to both means of control of the kinetic proxy. among these is that implicit control generates more incidental proxy motion, which increases the cognitive effort experience by hub participants. these findings suggest designs for future experimentation in kinetic proxies."
"complementary to the approaches of re-creating eye contact in videoconferencing systems, we should also explore ways of providing attention projection and awareness which may not depend on gaze awareness. this approach may open up options that are mechanically simpler, more abstract, and perhaps more diverse than previous approaches for creating engagement through videoconferencing solutions."
"the current study focused on ameliorating the social asymmetries particular to hub-and-satellite teams. we have had meetings with multiple satellites in attendance by static proxy. in our experience, they have proceeded in much the same way-with comparable improvements in social integration but shortcomings in newscaster and skip-over effects-as meetings with single satellites. as we construct further prototypes, we hope to explore how interaction quality may differ (such as proxy-toproxy conversations) through the use of multiple kinetic proxies."
"we ran six groups of subjects through the experiment. we counterbalanced the ordering of conditions across groups. all of the groups were composed of three collocated hub participants who were recruited, plus a confederate acting as the satellite. participants were led to believe that the confederate was an untrained recruit like them. the same confederate participated in all of the groups, so that the kinetic proxy would be operated in a consistent way throughout the experiment."
"taking our \"two-step\" maze task for example, let us imagine an agent that repeatedly pursues policy 844 1 (fig 3b) throughout the day. at the end of the day, having completed 8 trials, its e parameter for 845 policy 1 has increased from a prior concentration of 1 to a posterior concentration of 9 (fig a1a) . the 846 . cc-by-nc-nd 4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint agent then goes to sleep, where it entertains possible combinations of reduced models for prior e 847 parameters (fig a1b) and computes the model evidence for each reduced model using the 848 derivations shown in appendix section a.1 (the resulting model evidence is shown in fig a1c) . 849"
"in this paper, we have investigated the problems of scheduling and buffer management in dtns. we have proposed an optimal joint scheduling and buffer management policy and introduced an approximation scheme for the required global knowledge of the optimal algorithm. using simulations based on a synthetic and real mobility traces we showed that our policy based on statistical learning successfully approximates the performance of the optimal algorithm. both policies (gbsd and hbsd) plugged into the epidemic routing protocol outperform current state-of-the-art protocols like rapid [cit] with respect to both delivery rate and delivery delay, in all considered scenarios. moreover, we discussed how to implement our hbsd policy in practice, by using a distributed statistics collection method, illustrating that our approach is realistic and effective. we showed also that, [cit], our statistics collection method scales well, not increasing the amount of signaling overhead during high congestion. finally, we have studied the distributions of hbsd' utilities under different congestion levels and showed that the optimal policy heavily depends on the congestion level. the above findings suggest that methods to signal the congestion level could allow nodes to switch off the more sophisticated but \"heavier-duty\" hbsd policy and use simpler local policies, when congestion is below some threshold. this framework also paves the way for an end-to-end congestion scheme that we intend to look into in future work."
"the questionnaire after each condition included nine likert-style statements and a single brief written response (see table 1 ). each had a 7-point scale with \"strongly disagree\" or \"strongly agree\" as their endpoints. the written response question invited participants to, \"please comment on your experience interacting with the remote person in this session.\""
"in this paper, we try to solve this problem in its foundation. we develop a theoretical framework based on epidemic message dissemination [cit], and propose an optimal joint scheduling and drop policy, global knowledge-based scheduling and drop (gbsd) that can maximize the average delivery rate or minimize the average delivery delay. gbsd derives a per-message utility by taking into account all information that are relevant for message delivery, and manages messages accordingly. yet, to derive these utilities, it requires global network information, making its implementation difficult in practice, especially given the intermittently connected nature of the targeted networks. in order to amend this, we propose a second policy, historybased scheduling and drop (hbsd), a distributed (local) algorithm based on statistical learning. hbsd uses network history to estimate the current state of required (global) network parameters and uses these estimates, rather than actual values (as in gbsd), to calculate message utilities for each performance target metric."
"we first look into a scenario, where each message has a finite t t l value. the source of the message keeps a copy of it during the whole t t l duration, while intermediate nodes are not obliged to do so. to maximize the average 1 . by meeting time we refer to the time until two nodes starting from the stationary distribution come within range (\"first meeting-time\"). if some of the nodes in the network are static, then one needs to use hitting times between mobile and static nodes. our theory can be easily modified to account for static nodes by considering, for example, two classes of nodes with different meeting rates (see, e.g., [cit] ). delivery probability among all messages in the network, the optimal policy must use the per-message utility derived in the following theorem, in order to perform scheduling and buffer management. to maximize the average delivery rate of all messages, a dtn node should apply the gbsd policy using the following utility per message i:"
"sun's porta-person [cit] prototype also addressed the social presence of remote participants through motion, but specifically within \"hybrid meetings,\" which include a mix of conference rooms, remote and local participants. the device included a video camera and display-replaced by a laptop computer in a later design-stereo speakers and microphones, all mounted atop a turntable and positioned on, or alongside, a conference table. porta-person and its turntable represent a direct lineage influence on the physical design of our kinetic proxy."
"in order to keep track of the statistics about past messages necessary to take the appropriate transmission or dropping decision, we propose that each node maintains the data structure depicted in fig. 8 . each node maintains a list of messages whose history in the network it keeps track of. for each message, it maintains its id, its t t l and the list of nodes that have seen it before. then, for each of the nodes in the list, it maintains a data structure with the following data: 1) the node's id, 2) a boolean array copies bin array, and 3) the version stat v ersion associated to this array."
. cc-by-nc-nd 4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint
"compared to our maximum activation method, the spreading activation does not guarantee activation of particular node while our method always assigns an activation if in the graph exists an improving path between source and target nodes. although there have been proposed constrained spreading activation which utilise the semantics of the edges [cit], no version of the spreading activation takes into account the \"age\" of edges while passing the activation to the connected nodes, as our method does. the maximum activation is better suited for web api selection mainly due to following reasons: 1) it is not known at which nodes the spreading activation terminates while web api selection problem uses web api candidates as input (the target nodes), 2) the spreading activation has a local meaning of activation that indicates a measure that can be used for recommendations on data whereas maximum activation uses the value as a global measure of connectivity from source to target nodes."
"between two consecutive optimal copy vectors, resulting from solving the optimal centralized version offline. these optimal vectors are calculated every 3 s, corresponding to the average time between any two consecutive contacts among the network. as is evident in the figure, this distance is very small, implying that our distributed gradient-ascent implementation of this policy (gbsd/ hbsd) has enough time to converge to the optimal vector, before this changes significantly. in order to further validate the optimality of our policy, we compare in fig. 7 the absolute difference between the number of copies assigned to a message by our gbsd policy and the number of copies allocated to the same message by the optimal algorithm 3.5. we have picked some messages randomly and plot this difference along a time window in their lifetime. these results show that the gbsd policy is able to follow the optimal one with an average error of 1-2 copies allocated at most (note that the number of copies for the messages depicted are about 10-12 copies so it's an error of 10 percent). we believe this result consolidates the optimality properties of our proposed distributed implementation of the optimal policy."
"in both cases, proxy motion was calibrated so that the satellite's mouse or head motions mapped directly to the intended positions around the conference table."
"we use a breadth-first recursive algorithm, shown in alg.1, to accumulate and generate the symbolic expressions for e [f ] and e[t ], using the null knowledge state as the initial state of the cycle. because the cumulative probability of a path diminishes with the number of hops in the state space, we use a maximum search depth n sd . in this way, we trade off optimization accuracy with computation complexity. fig.3 provides a picture of the state space through which the recursive algorithm branches out in its calculation. the key point is that many of the states, even though they reside in the state space, are not reachable states. since the transitions to those states are never computed in the breadth-first search, this approach obviates any a priori pruning of unreachable states from the state space. the algorithm starts from s0 (the null knowledge state), and calls the recursive function accumulatemetrics(). in the base case, the algorithm checks the stop condition (line 11 of the algorithm): (1) it has (2) it has arrived at the search depth n sd . if the stop condition is satisfied, the algorithm returns the average branch probability, average branch cycle length, and the average branch reward. otherwise, the algorithm calls the function neighbor() to calculate the set of valid states to which it can transition, with associated transition probabilities and rewards, then accumulates the rewards and multiplies the probabilities for each next state; after that, accumulatemetrics() function is called recursively for each next state. the total reward and total cycle length are calculated on lines 23, 24, and 25."
"in order to compare relative model evidence, we look at the log ratio of the reduced and full model 819 evidence, which is the same as the difference in their free energy (free energy of the full model minus 820 the reduced): 821"
"figs. 2 and 3 show the delivery rate for the random waypoint and kaist trace scenarios, respectively. from this plot, it can be seen that: the gbsd policy plugged into epidemic routing gives the best performance for all numbers of sources. when congestion level decreases, so does the difference between gbsd and other protocols, as expected. moreover, the hbsd policy also outperforms existing protocols (rapid and epidemic based on fifo/ drop tail) and performs very close to the optimal gbsd. specifically, for 70 sources, hbsd offers an almost 60 percent improvement in delivery rate compared to rapid and is only 14 percent worse than gbsd. similar conclusions can be also drawn for the case of the real taxi trace, zebranet trace, and the hcmm model and 70 sources. results for these cases are respectively summarized in tables 3, 4, and 5. 6 . we have also performed simulations without any antipacket mechanism, from which similar conclusions can be drawn."
"the e vector can now be thought of as an empirical prior that accumulates the experience of policies 307 that are carried over from previous trials. in short, it enables the agent to learn about the sorts of 308 things that it does. this experience dependent prior policy enters inference via equation 4. before 309 demonstrating this experience dependent learning, we look at another form of learning known 310 variously as bayesian model selection or structure learning. 311"
"the proxy also included a fixed-position axis 212 wide-angle camera. the view from this camera was displayed across the entire width of the satellite's 30\" monitor (see fig. 3 ). this configuration allowed the satellite to see all of the hub participants and their positions around the table, as well as the top edge of the tablet pc, to confirm that it was oriented as she expected. the satellite's screen also displayed the video directly from the tablet's integrated webcam, but the confederate preferred to focus on the larger, wide-screen image, both to more directly engage the hub individuals and because it provided sufficient feedback of the screen's orientation. all audio and video communication was over wired and wireless lan, while the position control stream for controlling the motion of the proxy was carried by usb cable between the two rooms."
"the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint synaptic homeostasis hypothesis of sleep (25). for an excellent review on sleep and model 341 optimisation, see (26), and for a review of bayesian model reduction, see (27) . 342 returning to our maze task, our artificial agents traverse through the maze each day and aggregate e 343 parameters (equation 6) to form its daily posterior -that will serve as tomorrow's empirical prior. 344"
"participants were instructed at the beginning of the experiment that the members of the group with the best solution to task 3, as judged by the experimenter, would receive a $20 gift card. (in fact all participants received the gift card.)"
"the rapid growth of web apis and a popularity of service-centric architectures promote a web api as a core feature of any web application. according to programmableweb 3, [cit] . while it took eight years to reach 1,000 [cit], and two years to reach 3,000 [cit], it took only 10 months to reach 5,000 [cit] . in spite of this increase, several problems are starting to arise. old and new not-yet-popular web apis usually suffer from the preferential attachment problem [cit], developers can only run a keyword-based search in a service directory or they run a google search to find web pages that reference or describe web apis. although there exist a number of sophisticated mechanisms for service discovery, selection and ranking, there is still a lack of methods that would in particular take into account a wider web apis' and developers' contexts including developers' profiles, information who developed web apis or used them in a mashup, web apis' or mashups' categories as well as the time when an api or a mashup was developed or published. with the popularity of web apis and directory services like programmableweb, it is now possible to utilize all such information in more sophisticated service selection methods."
"the generative model (fig 1) assumes that outcomes depend upon states, and that current states 147 depend upon states at the previous timepoint and the action taken (as a result of the policy pursued). 148"
"researchers of interpersonal interaction recognize that the nuances of body orientation and non-verbal behavior-some consciously controlled, others not-are important parts of the messages that people send and receive when communicating in person. goffman highlighted the difference between the expressions that people give and those that they give off [cit] . the former are verbal signs of the content they want to communicate; the latter are nonverbal and contextual. the former are more conscious; the latter are more subconscious. hall's study of proxemics formalized that, with cultural differences, the way people orient their bodies toward or away from one another indicates their degree of intent to engage in conversation [cit] . face-to-face is more direct, a 90-degree angle is more casual, and a 180-degree angle is more transitory and disengaged. the design and motion of the kinetic proxy, with its explicit and implicit forms of control, is inspired by these insights. 1 what we call the newscaster effect is more commonly referred to as the mona lisa effect."
. cc-by-nc-nd 4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint
"we are now ready to explore the tradeoff between the signaling overhead, its impact on performance, and the dynamicity of a given scenario. our goal is to identify operation points, where the amount of signaling overhead is such that it interferes minimally with data transmission, while at the same time it suffices to ensure timely convergence of the required utility metrics per message. we will consider throughout the random waypoint scenario described in section 5.2. we have observed similar behavior for the trace-based scenarios."
"the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint to see the effect of training under different environments, we set up two different maze conditions: 248 a volatile environment, in which the reward can appear in any one of the 4 final locations with equal 249 frequencies, and a non-volatile environment, where the reward only appears on the two left final 250 locations ( fig 3a) . crucially, this volatility is between-trial, because these contingencies do not 251 change during the course of a trial. the mouse has no explicit beliefs about changes over multiple 252 trials. two mice with identical initial parameters are trained in these two distinct environments. with 253 our set-up, each mouse can entertain 7 possible policies ( fig 3b) . four of the policies allow the mouse 254 to get to one of the final four locations, whereas three additional policies result in the mouse staying"
"handling converged messages. once the node collects an entire history of a given message, it removes it from the mum buffer and pushes it to the buffer of messages with a complete history (mch). a node considers that it has the complete history of a given message only when it gets the last version of the statistics entries related to all the nodes the message goes through during its t t l, 10 finally, note that, once a node decides to move a message to the mch buffer, it only needs to maintain a short summary rather than the per node state as in fig. 8 ."
"next, we consider a scenario with low congestion. we reduce the number of sources to 15, keep the buffer size of 20 messages, but we also decrease the cbr rate of sources from 10 to 2 messages/ttl. in figs. 18 and 19, we plot the 11. we speculate that this remaining error might be due to slightly underestimating m and n, as explained earlier."
"likewise, as the policy spaces are reduced and made more efficient, we also observe a corresponding 518 adolescent decline in brain glucose usage (43). this is consistent with the idea that informational 519 complexity is metabolically more expensive (44). 520"
"overall, the satellite had a slight preference for the explicit condition. each movement was meaningful, and participants seemed to pay attention to each movement and interpret its intent correctly. the proxy movement also added communicative value compared to the stationary condition. she felt that her \"voice\" was amplified by the motion, and having explicit control over this was the most comfortable for her."
"the rest of this paper is organized as follows. section 2 describes the current state of the art for resource allocation in dtns. in section 3, we describe the \"reference,\" optimal joint scheduling and drop policy that uses global knowledge about the network. then, we present in section 4, a learning process that enables us to approximate the global network state required by the reference policy. section 5 discusses, our evaluation setup and presents performance results for both policies (gbsd and hbsd) using synthetic and real mobility traces. in section 6, we examine in detail our mechanism to collect and maintain network history statistics, and evaluate the signaling-performance tradeoff. section 7 studies the behavior of our hbsd policy in different congestion regimes. finally, we conclude this paper in section 8."
"we also expected that hubs would perceive implicit control as more natural than explicit, as more of the satellite's gestures would be available to them, but we found mixed indications in the questionnaire responses. while our results are equivocal about h2, we have a richer understanding of the tradeoffs between explicit and implicit control."
"meaning of maximum activation value. as we noted earlier, we interpret the maximum activation of the graph as a measure that indicates how well the source nodes are connected with the target. in general, the more improving paths exist between the source and the target, the higher maximum activation we can get. however, the value of the maximum activation is also dependent on constraints and the creation time of web apis and mashups along the improving paths when the ageing function is applied."
"buffer management problem. if one (or more) of these messages arrive at j's buffer and find it full, what is the best message j should drop among the ones already in its buffer (locally) and the newly arrived one, in order to maximize, let's say, the average delivery rate among all messages in the network (globally)?"
"proof. let us denote the delivery delay for message i with random variable x i . this delay is set to 0 (or any other constant value) if the message has been already delivered. then, the total expected delivery delay (dd) for all messages for which copies still exist in the network is given by"
"note that, the per-message utility with respect to delivery delay is different than the one for the delivery rate. this implies (naturally) that both metrics cannot be optimized concurrently."
"other research prototypes expressly examine gaze direction in human-robot interaction. [cit] developed robots with movable heads to support turn-taking in their communications. these robots engaged humans in one-on-one monologue or simple dialog while orienting their heads toward people or objects of interest. the studies emphasized the coincident timing of robotic gestures with transitional words. our work also explores how orientation cues can influence interaction, but in a highly collaborative context. such robots also act as agents rather than avatars. by representing themselves in an interaction rather than a human other, and by not simultaneously presenting live video of that remote other, they avoid the potential to both complement and contradict an operator's actions. kinetic proxies take this hybrid approach to providing physical motion as well as onscreen video."
"in both cases, the goal is to more closely reflect the way that people interact during a collocated discussion, or at least, a reported improvement over the non-motion conference experience. we used a combination of behavioral and perceptual measures that are described in more detail below."
"and reducing the number of behaviours that an agent may engage in. if an agent can account for its 89 behaviour without calling on a given policy, it can be pruned, resulting in a reduced policy space, 90 allowing agents to infer which policy it is pursuing more efficiently. note that in active inference, 91 agents have to infer the policy they are pursuing, where this inference is heavily biased by prior 92 beliefs and preferences about the ultimate outcomes. we argue that pruning of redundant 93 behavioural options can account for the phenomenon of specialization (behaviour highly adapted to 94 specific environments), and the accompanying loss of flexibility. in addition to introducing bayesian 95 model reduction for prior beliefs about policies, we consider its biological plausibility, and its 96 relationship with processes like sleep that have been associated with structure learning (i.e., the 97 removal of redundant model parameters). finally, through the use of illustrative simulations, we 98 show how optimising model structure leads to useful policies, the adaption of an agent to its 99 environment, the effect of the environment on learning and the costs and benefits of specialization. 100"
"we can look more carefully at figs. 16 and 17, to understand what is happening in high congestion regimes. the number of copies per message created at steady state depends on the total number of messages coexisting at any time instant, and the aggregate buffer capacity. when too many messages exist in the network, uniformly assigning the available messages to the existing buffers, would imply that every message can have only a few copies created. specifically, for congestion higher than some level, the average number of copies per message allowed is so low that most messages cannot reach their destination during their ttl. uniformly assigning resources between nodes is no more optimal. instead, to ensure that at least some messages can be delivered on time, the optimal policy gives higher priority to older messages that have managed to survive long enough (and have probably created enough copies), and \"kills\" some of the new ones being generated. this is evident by the values assigned at different bins (especially in the delivery delay case)."
"it is interesting that measures of behavior (sociometric turn-taking, deictic prompt responses) were more demonstrative than perceptual questionnaire responses. some participants reported not even noticing that the display remained stationary during that condition. the behavioral measures showed that participants reacted to the motion conditions even though their perceptual rankings do not show statistically significant differences. taken together, these results show that people's mechanisms of attention awareness may operate at a subconscious level, as has also been seen in other research [cit] ."
"returning the two practical problems with our proxy that prompted this exploration, do we believe that the kinetic proxy will address the skip-over and newscaster effects? regarding the skip-over effect, we certainly believe the attention projection provided by motion will give the hub participants enough awareness of the satellite's attention to include her in the conversation. we look forward to deploying kinetic proxies into everyday usage to gain more experience with that. regarding the newscaster effect, we set out to improve gaze awareness, in the tradition of the research prototypes reviewed earlier that have attempted to do so. kinetic motion did provide a physical sense of gaze direction. however, it did not achieve eye contact, as the satellite's turning head turned the kinetic proxy display, but also caused her gaze to be directed off angle from the camera."
"reserve future data slots with high probability. in contrast, in protocol generation, since the impact of control information is incorporated into the optimization program, the \"optimal\" protocol is generated, taking the rac and rts-cts protocols as special cases in the larger protocol space."
"we note also that the message seen bin array, indicating if a node a had seen (rather than stored) a message b at time t, in order to estimate mðt þ, can be deduced directly from the copies bin array, and thus no extra storage is required. summarizing, based on this lists maintained by all nodes, any node can retrieve the vectors nðt þ and mðt þ and can calculate the hbsd per-message utilities described in section 4 without a need for an oracle."
"videoconferencing systems disrupt the link between attention projection and attention awareness. they do this in part because they do not faithfully reproduce the spatial characteristics of gaze, body orientation, and pointing gestures. this disruption is one of the reasons why video-mediated communication is less effective than faceto-face interaction. the lack of a shared physical environment further hampers participants' abilities to use spatial cues to support conversation and direct attention [cit] . videoconferencing configurations that involve multiple people at one site offer multiple plausible loci of attention, increasing the potential for confusion. we are particularly interested in videoconferencing systems to support hub-andsatellite meetings, where most participants are collocated except for one participant at a satellite location. this satellite is represented in the collocated space by a proxy device consisting of a display screen, camera, speaker, and microphone ( fig. 1) . the satellite perceives the hub location through streams of audio and video displayed on his computer display."
"under active inference, agents act to minimize their variational free energy (13) and select actions 106 that minimises variational free energy expected following the action. this imperative formalises the 107 notion that an adaptive agent should act to avoid being in surprising states, should they wish to 108 continue their existence. in this setting, free energy acts as an upper bound on surprise and expected 109 free energy stands in for expected surprise or uncertainty. as an intuitive example, a human sitting 110 comfortably at home should not expect to see an intruder in her kitchen, as this represents a 111 challenge to her continued existence; as such, she will act to ensure that outcomes (i.e. whether or 112 not an intruder is present) match her prior preferences (not being in the presence of an intruder); 113 for example, by locking the door. 114"
"there is a long series of research prototypes that have tried to improve gaze awareness in videoconferencing. hydra [cit] was a 4-way distributed meeting system that packaged a video camera, small display, microphone, and speaker in selfcontained, table-top surrogates dedicated to represent each participant at the meeting. participants could look toward any of the other participants, and everyone would have an appropriate indication of his or her gaze. a study that compared mediated with same-room conversations found very similar patterns in overall speaking time, speech segment duration, and the distribution of turns taken. our current study draws upon these same three conversational measures (among others)."
"for the explicit control condition, the satellite participant moved her mouse to place the cursor at a particular spot on the client's widescreen view of the hub's workspace. doing so sent a command which rotated the proxy's screen to face that location in the room. the client program updated the desired 'go-to' position approximately 30 times per second. since the proxy's turntable was only capable of rotation in the horizontal plane, we only tracked the horizontal component of the cursor's position."
"m obile ad hoc networks (manets) had been treated, until recently, as a connected graph over which endto-end paths need to be established. this legacy view might no longer be appropriate for modeling existing and emerging wireless networks [cit] . wireless propagation phenomena, node mobility, power management, etc., often result in intermittent connectivity with end-to-end paths either lacking or rapidly changing. to allow some services to operate even under these challenging conditions, researchers have proposed a new networking paradigm, often referred to as delay tolerant networking (dtn [cit] ), based on the store-carry-and-forward routing principle [cit] . nodes there, rather than dropping a session when no forwarding opportunity is available, store and carry messages until new communication opportunities arise."
"in this paper we develop a novel web api selection method that provides personalized recommendations. as an underlying dataset we create so called linked web apis, an rdf representation of the data from the programmableweb service directory, that utilizes several well-known rdf vocabularies. the method has the following characteristics: 1) social and linked -it exploits relationships among web apis, mashups, categories, and social relationships among developers such as who knows who in the programmableweb directory, 2) personalized -it takes into account user's preferences such as developers the user knows and preferences that define importances of predicates, and 3) temporal -it takes into account a time when web apis and mashups appeared in the graph for the first time."
"regarding the second assumption, although it might not always hold in some scenarios, it turns out to be a useful approximation. in fact, one could use a mean-field analysis argument to show that independence is not required, in the limit of large number of nodes, for the analytical formulas derived to hold (see, e.g., [cit] )."
"gestureman's [cit] goal was to support a remote operator in projecting his or her intentions in a workspace shared with a human collaborator. unlike other proxies, it did not support live video of the operator. instead, it had the ability to orient its own robot head, body and a pointing arm, which were controlled by tracking the operator's head movements, screen touches and joystick use."
"in simulations, one readily observes that policy learning occurs and is progressive, evident by the 363 increase in e concentration for frequently pursued policies (fig 4), which rapidly reach stable points 364 within 10 days (fig 4b, see fig 3c for the concept of \"training days\"). interestingly, the relative policy 365 strengths attain stable points at different levels, depending on the environment in which the agent is 366 trained. in a conservative environment, the two useful policies stabilize at high levels ( ≈ 32), 367 whereas in a volatile environment, these four useful policies do not reach the same accumulated 368 strengths ( ≈ 25). furthermore, the policies that were infrequently used are maintained at lower 369 levels when trained in a non-volatile environment ( ≈ 7 ), while they are more likely to be 370 considered for the agent trained in the volatile environment ( ≈ 11). 371 372 ."
"as considered for deep temporal models (56). this likely leads to a more accurate account of 598 expertise-formation, as familiarity with a domain-specific task should occur at multiple-levels of the 599 neural-computation hierarchy (e.g. from lower level \"muscle memory\" to higher level planning). 600"
"the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint environments in which the specialist and generalist agents were trained, respectively. the novel environment is a"
"where f l ðr i þ is the laplace transform of distribution fðxþ evaluated at r i . continuing as in the proof of theorem 3.2, we get the unconditional probability of delivery p i"
"we consider again the simulation scenario used in sections 5.2 and 6.3. first, we fix the number of sources to 50, corresponding to a high congestion regime. in figs. 16 and 17, we plot the distribution of the hbsd delivery rate and delivery delay utilities described in sections 4.1 and 4.2. it is evident there that the optimal utility distribution has a nontrivial shape for both optimization metrics, resulting in a complex optimal scheduling and drop policy."
"in contrast, the specialist agent performs poorly in a general environment (46% reward-acquisition), 441 and fails all but one out of its 512 attempts in a novel environment where it needs to go in the opposite 442 direction to that of its training ( figs 6b and 6c) . the generalist agent, being equally trained in all four 443 policies -that take it to one of the end locations -does not suffer from reduced reward-acquisition 444 when exposed to a new environment (the specialized environment or novel environment). however, 445 it does not perform better in a familiar, general environment either. the agent's reward-acquisition 446 remains around 60% across all testing environments, similar to that of a naïve agent ( figs 6b and 447"
"amalgamations of e parameters. for each configuration of these policy parameters, model evidence 346 is computed and bma performed to acquire the optimal posterior, which becomes the prior for the 347 subsequent day. in brief, we evaluated the evidence of models in which each policy's prior 348 concentration parameter was increased by eight, while the remainder were suppressed (by factor of 349 two and four). this creates a model space -over which we can average to obtain the bayesian model 350 average of concentration parameters in a fast and biologically plausible fashion. please see s1 351"
"the microphone recorded the wearer's speech amplitude and time codes [cit], and was the only sensor used in this study. badge data from each day's sessions were downloaded, combined and analyzed using scripts that had been developed earlier. these scripts measured speaking time, speech energy, speech-segment length, and turn-taking per participant and condition."
"that is, the sum of meeting rates with the destination of the n i relays for message i is (approximately) normally distributed. replacing this in (8), we get the (unconditional) delivery probability p i"
"to study delays, we increase messages' ttl (and simulation duration), to ensure almost every message gets delivered. for the random waypoint mobility scenario, figs. 4 and 5 depict the average delivery delay for the case of both limited buffer and bandwidth, for rand. waypoint and kaist, respectively. as in the case of delivery rate, gbsd gives the best performance for all considered scenarios. moreover, the hbsd policy outperforms the two routing protocols (epidemic based on fifo/drop tail, and rapid) and performs close to gbsd. specifically, for 70 sources and both limited buffer and bandwidth, hbsd average delivery delay is 48 percent better than rapid and only 9 percent worse than gbsd. table 3, table 4, fig. 5 and table 5 show that similar conclusions can be drawn for the delay under respectively the real taxi(s), zebranet trace, kaist trace and the hcmm model."
"in the past, the main approach to reconfigurable protocol design has been to \"choose\" a certain protocol or family of protocols for the application at hand. further, via analysis [cit] or simulation/emulation studies [cit], one can describe the protocol family that performs the best under different scenarios. for example, for wireless mac protocols, if the control overhead is large compared to data, a random access channel (rac) protocol would have better performance, and if the control overhead is small compared to data, a csma/ca protocol would have better performance due to its ability to 1 this work was supported in part by the national science foundation grant # 0917052."
"an organism which harbours alternative models of the world needs to consider its own uncertainty 320 about each model. the most obvious example of this is in the evaluation of different plausible courses 321 of action (policies), each entailing a different sequence of transitions. such models need to be learnt 322 and optimised (22,23) and, rejected, should they fall short. bayesian model averaging is used 323 implicitly in active inference when forming beliefs about hidden states of the world, where each 324 policy is regarded as a model and different posterior beliefs about the trajectory of hidden states 325 under each policy are combined using bayesian model averaging. however, here, we will be 326 concerned with the bayesian model averaging over the policies themselves. in other words, the 327 model in this instance becomes the repertoire of policies entertained by an agent. 328"
"figs. 14 and 15 plot the hbsd policy delivery rate and delay, respectively, as a function of mum buffer size. unlike the constant load case, it is easy to see there that, increasing the size of the mum buffer, results in considerable performance improvement. nevertheless, even in this rather dynamic scenario, nodes manage to keep up and produce good utility estimates, with only a modest increase on the amount of signaling overhead required."
"our focus in this paper has been on policy optimisation, where discrete policies are optimised 459 through learning and bayesian model reduction. by simulating the development of specialism and 460 generalism, we illustrated the capacity of a generalist to perform in a novel environment, but its 461 failure to reach the level of performance of a specialist in a specific environment. we now turn to a 462 discussion of the benefits and costs of expertise. principally, the drive towards specialization (or 463 expertise) is the result of the organism's imperative to minimize free energy. as free energy is an 464 upper bound on surprise (negative bayesian model evidence), minimizing free energy maximizes 465 model evidence (31). as model evidence takes into account both the accuracy and complexity of an 466 explanation (22), it is clear that having a parsimonious model that is well-suited to the environment 467 -a specialist model -will tend to minimize free energy over time, provided the environment does 468 not change. 469 ."
"a second observation is that, using our statistics collection method, a node can reduce the amount of signaling overhead per meeting up to an order of magnitude, compared to the unlimited mum case, even in this relatively small scenario of 70 nodes."
"we now turn to our question about the effect of the environment on policy learning. intuitively, useful 361 policies should acquire a higher e concentration, becoming more likely to be pursued in the future. 362"
. cc-by-nc-nd 4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint
"to measure the effects of the experimental conditions, we collected both objective and subjective data during each videoconference session. the objective measures included a tally of responses to deictic prompts and sociometric data that was captured by sensor badges that all of the participants wore around their necks [cit] . the subjective measures included the individual questionnaire after each session that focused on the quality of interaction with the satellite member during that condition, the final questionnaire that probed participants' perceptions and preferences about group communication across all three conditions, and the semi-structured group interview, where we discussed these issues in greater detail."
"the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint also corresponds with the neurobiological findings of childhood peaks in grey matter volume and 515 number of synapses, followed by adolescent decline (40) (41) (42) . in this conceptualization, as children 516 learn they prune away redundant connections, much as our agents triage away redundant policies. 517"
synthetic mobility models. we've considered both the random waypoint mobility model and the hcmm model [cit] . the latter is inspired from watts' caveman model and was shown to accurately reproduce intercontact time and contact duration statistics.
". additional complexity in the mobility model (e.g., correlated meeting rates) could still be handled in our framework, yet at the expense of ease of interpretation (and thus usefulness) of the respective utilities. we will therefore consider the simple case of homogeneous mobility for the remainder of our discussion, in order to better elucidate some additional key issues related to buffer management in dtns, and resort to a simulation-based validation under realistic mobility patterns. we stress, however, that the methodologies and results presented in the next sections are applicable to the case of heterogeneous mobility, as well."
"in bayesian model comparison, multiple competing hypotheses (i.e., models or the priors that defines 313 models) are evaluated in relation to existing data and the model evidence for each is compared (21). the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint parameters that are a weighted average under each model considered. this is especially important 318 in situations where there is no clear winning model (21) . 319"
"how should one choose bin size? clearly, the larger it is, the fewer the amount of data a node needs to maintain and to exchange during each meeting; however, the smaller is also the granularity of values the utility function can take and thus the higher the probability of an incorrect decision. as already described in section 3, message transmissions can occur only when nodes encounter each other. this is also the time granularity at which buffer state changes occur. hence, we believe that a good tradeoff is to monitor the evolution of each message's state at a bin granularity in the order of meeting times. 9 this results in a big reduction of the size of statistics to maintain locally (as opposed to tracking messages at seconds or milliseconds granularity), while still enabling us to infer the correct messages statistics."
"our future work directions are: (1) expanding the protocol space to (a) asynchronous mac protocols, (b) with acknowledgments, and (c) overlapping mac neighborhoods, while keeping the complexity of symbolic generation low. it is key to note that the complexity incurred is mostly is generating the compact formulation than in solving it. (2) in this larger protocol space, we expect that protocols such as csma/ca will turn out to be special cases in the feasible set of protocols."
"inference that equips an agent with the machinery to learn habitual policies via a prior probability 607 distribution over its policy space. in our simulations, we found that agents who specialize -608 employing a restricted set of policies because these were adaptive in their training environment -609 can perform well under ambiguity but only if the environment is similar to its training experiences. 610"
"convergence of utilities and performance of the hbsd policy. in this last part, we fix the number of sources to 50 and we look at the impact of the size of the mum buffer on 1) the time it takes the hbsd delivery rate utility to converge, and 2) its accuracy. we use the mean relative square error to measure the accuracy of the hbsd delivery rate utility, defined as for each bin, a is the estimated utility value of (18) (calculated using the 10. note that there is a chance that a node might \"miss\" some information about a message it pushes in its mch. this probability depends on the statistics of the meeting time (first and second moment) and the ttl value. nevertheless, for many scenarios of interest, this probability is small and it may only lead to slightly underestimating the m and n values. approximate values of m and n, collected with the method described previously) and b is the utility value calculated using the real values of m and n. fig. 13 plots the mean relative square errors for the hbsd delivery rate utility, as a function of time. we can observe that, increasing the size of the mum buffer results in faster reduction of the mean relative square error function. with a mum buffer of 80 messages, the delivery rate utility estimate converges 800 seconds faster than using an mum buffer of 20 messages. indeed, the more messages a node tracks in parallel, the faster it can collect a working history of past messages that it can use to calculate utilities for new messages considered for drop or transmission. we observe also that all plots converge to the same very small error value. 11 in practice, we are more interested in the end performance of our hbsd, as a function of how \"aggressively\" nodes collect message history. when traffic intensity is relatively stable (e.g., a fixed number of cbr sources), the size of the mum buffer sizes is not crucial. even a rather small mum size, nodes eventually gather enough past message history to ensure an accurate estimation of per-message utilities, and a close-to-optimal performance. we therefore omit here these simulation results. this is not necessarily the case when traffic load experiences significant fluctuations."
"each group worked in each condition for approximately 10-15 minutes. immediately after each condition, participants individually completed a questionnaire that asked them to rate their experience with that condition. after all three conditions were completed, participants individually rated their preference among the conditions on a questionnaire and then participated in a semi-structured group interview. all sessions lasted approximately one hour, and the entire session was recorded using two overhead cameras and microphones in the hub room that captured the team's activity for later analysis."
"amount of signaling overhead per contact. fig. 11 compares the average size of statistics exchanged during a meeting between two nodes for three different sizes of the mum buffer, as well as for the basic epidemic statistics exchange method (i.e., unlimited mum, albeit with all other optimizations \"on\"). we vary the number of sources in order to cover different congestions regimes."
"animatronic shader lamps avatars [cit] were another form of kinetic proxy: a life-scale styrofoam head mounted on a pan-tilt unit, onto which a video feed of the satellite operator's likeness is projected. the system tracked the operator's yaw and pitch head motions and mirrored them on the avatar. an advantage of this approach was that it presented correct focus of attention cues for all of the individuals interacting with the avatar and over a broad range of viewing angles. it has not been systematically studied from a human-factors perspective."
"the rest of the paper is organized as follows: in section ii, we carry out steps (1)-(4) of the above methodology for the case study of a synchronous broadcast mac with any number of nodes n . in section iii, we carry out steps (5)-(7) of the above methodology, continuing the same case study. in section iv, we display and discuss the results generated by this methodology, and validate them via mathematical analysis in their asymptotic regime. in section v, we present our conclusions and discuss directions for future work."
"this new per-message utility is only a function of the locally available history of old messages and is thus independent of the actual global network state. for large number of messages, it should lead to the same average delay as when the exact values for m and n are used."
"we introduce a novel feature to the active inference framework; namely, the ability to update one's 84 policy space. technically, a prior probability is specified over a set of plausible policies, each of which 85 represents a sequence of actions through time. policy learning is the optimisation of this probability 86 distribution, and optimising the structure of this distribution (i.e. 'structure learning') through 87"
"to address these two questions, we propose the following policy. given a routing metric to optimize, our policy, gbsd, derives a per-message utility that captures the marginal valueof a given message copy, with respect to the chosen optimization metric. based on this utility, two main functions are performed as follow:"
"fðþ is a probability distribution that models the heterogeneous meeting rates between nodes, and can be any function integrable in ½0; 1þ, capturing thus a very large range of conceivable mobility models."
"physical motion of the proxy results in greater conversational engagement, improved sense of directional attention, and preferred interactions by hub participants, compared to no motion at all."
"we may impose any metric of our choice on this markov state space. in this paper, we shall maximize the average network throughput per slot. the network scores a throughput of 1 unit whenever only a single node sends data (d) in a slot, and all of the other nodes are quiet. note thatd does not score any throughput in that slot. (it is easy to generalize this to a model where the control information consumes only a fraction α, e.g. the header of a packet, by assigning a throughput of 1 − α to every arc whered is transmitted.)"
"interestingly, the occam's window itself can also be thought of as a hyperprior. a wide window (high 562 z ) means more models are considered, which offers a more optimal averaged prior but at higher 563 computational costs; a narrow window (low z ) means only the models with high model evidence 564 are considered. this allows for efficient averaging over only the best models but comes at the cost of 565 strict pruning. likewise, both strategies offer different advantages, and the optimal balance may 566 depend on the nature of the agent's environment (i.e., is it an environment that provides definitive 567 evidence for a small number of policies -or is it an ambiguous environment?). a deviation from the 568 optimum may result in reduced fitness and suboptimal inference -a potentially useful perspective 569 on psychopathology in neuropsychiatric illnesses. for instance, an overly strict pruning rule -while 570 being highly efficient for policy optimisation -may result in useful policies being forever lost. this 571 sub-optimal form of structure learning may relate to the aberrant pruning which in schizophrenic 572 patients (52), leading to maladaptive policy spaces and policy-derived priors that could drive 573"
"based on our study results, we would like to explore designs that leverage the benefits of physical motion, but avoid the exclusion of turning away from participants. since swiveling the display still did not create true eye contact, perhaps there are ways to use a physical pointer, like a weather vane, to indicate attention projection while keeping the flat display stationary, so all hub participants maintain visual contact with the satellite. alternatively, it would be interesting to explore convex displays, rather than the flat display screen, which might afford a wider range of directing a satellite's gaze while not 'turning her back' on some participants."
"the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint ( 2 d, …, 2 e ) to be the number of times the agent observes itself performing policies 4, …, b, the 298 posterior distribution over the policy space is: 299"
"rotating the display also introduced delay in the conversation, especially when the satellite had to explicitly control the aiming of the display (\"…and then kind of like these awkward interruptions, every time she turned.\"). some also found the motion to be distracting (\"i felt like whenever she turned we all kind of like stopped talking for a second.\")."
"routing. each message has a single destination (unicast) and is assumed to be routed using a replication-based scheme [cit] . during a contact, the routing scheme used will create a list of messages to be replicated among the ones currently in the buffer. thus, different routing schemes might choose different messages. for example, epidemic routing will replicate all messages not already present in the encountered node's buffer [cit] . for the purposes of this paper, we will use epidemic routing as a case study, for the following reasons. first, its simplicity allows us to concentrate on the problem of resource allocation, which is the focus of this paper. second, it consumes the most resources per message compared to any other scheme. as a result, it can be easily driven to medium or high congestion regimes, where the efficient resource allocation problem is most critical. third, given the nature of random forwarding schemes, unless a buffer is found full or contact capacity is not enough to transfer all messages, epidemic forwarding is optimal in terms of delay and delivery probability. consequently, epidemic routing along with appropriate scheduling and message drop policies, can be viewed as a new routing scheme that optimally adapts to available resources [cit] . finally, we note that our framework could be used to treat other types of traffic (e.g., multicast), as well."
"finally, we plot in fig. 12 the average size of exchanged (nonsignaling) data per meeting. we can observe that increasing the size of the mum buffer results in a slight decrease of the data exchanged. this is due to the priority we give to statistics exchange during a contact. we note also that this effect becomes less pronounced when congestion increases (in line with fig. 11 ). finally, in the scenario considered, we can observe that, for mum sizes less than 50, signaling does not interfere with data transmissions (remember that packet size is 5 kb). this suggests that, in this scenario, a mum size of 50 messages represents a good choice with respect to the resulting signaling overhead. in practice, a node could find this value online, by dynamically adjusting its mum size and comparing the resulting signaling overhead with average data transfer."
"mebot [cit] was a small, desktop proxy with a three degree-of-freedom head that displayed cropped video of the operator's face, mounted to a mobile base with articulating arms. a study found that the proxy displaying motion was more engaging and likable than without motion. the role of motion as an indicator of attention was not evaluated, and since the participant's head motion was tracked (only), alternative forms of control were not compared."
"in our simulations, we optimised policy strengths through the process of bayesian model reduction 546 (to evaluate the free energy or model evidence of each reduced model), followed by model averaging 547 -in which we take the weighted average over all reduced models. however, bma is just one way of 548 using model evidences to form a new model. here, we discuss other approaches to model 549 comparison, their pros and cons, and biological implications. the first is bayesian model selection, in 550 which only the reduced model with the greatest evidence is selected to be the prior for the future, 551 without consideration of competing models. this offers the advantage of reduced computational cost 552 (no need to take the weighted sum during the averaging process) at the cost of a myopic selection -553 the uncertainty over reduced models is not taken into account. 554"
"each agent was tested for 512 trials in each test environment. note that the agents do not learn during 436 the testing phase -we simply reset the parameters in our synthetic agents after each testing trial to 437 generate perfect replications of our test settings. we observe that an untrained (naïve) agent has a 438 baseline reward-acquisition rate of ~60%. on the contrary, the specialist agent excels when the 439 environment is similar to that it trained in, performing at the highest level (89%) out all the agents. 440"
"in the implicit condition, she sometimes found herself intentionally \"driving\" the proxy with her head, rather than moving naturally and trusting the proxy to follow her actions (perhaps due to some technical difficulties with the prototype). she also noticed that, unintentional head movements sometimes distracted the hub participants, making her a bit self-conscious about the way she moved her head."
"the methodology that we have developed in this paper can be applied to any choice of an objective function defined on the global state space, and the same symbolic generator can be invoked to produce the program. for example, if the objective is to minimize the average energy consumption subject to a minimum throughput constraint for each node, then adding 1 line of code in the constraints, and changing a single line in the recursive function, quickly produces the new optimal protocol. this is in sharp contrast with hand-designed protocols that would design differently for each metric."
"in the case of bayesian model selection, the reduced model with the highest model evidence is selected 828 as the optimal model. that is to say, given a vector of the relative free energy for each reduced model, 829 δf, we pick the z which gives max (δf) . however, since we are interested in bayesian model 830 averaging, we need to compute the probability of each reduced model within the entire reduced 831 model space we defined: 832"
"unknown distribution in large networks. if the actual probability distribution of meeting rates is not known, the following approximation could be made in order to derive marginal utilities per message and use them for buffer management. let us assume that the meeting rates come from an unknown distribution with first and second moments \" and 2, respectively. let us further assume that there is a large number of nodes, such that n i, the number of copies of message i at steady state, is large. using the central limit theorem, we have"
"[ taking the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint 811 equation a5 tells us that the model evidence of any reduced model can be evaluated given the prior 812 of the reduced and full models, and the evidence of the full model. applying the above knowledge to 813 the e concentration parameters defined previously, we have the following: 814"
"the linked web api dataset uses several well-known ontologies. the concepts from the foaf 5 ontology represent mashup developers and their relations to other people, the concepts from the wsmo-lite [cit] 3 maximum activation method"
"we prefer the former in this context, as the affordances of live audio and video viewed on a digital screen are more directly comparable, and are closer to most people's experiences."
"on the contrary, a generalist agent can more easily adapt to changing, ambiguous environments, but 611 is never as successful as a specialist agent in a conservative environment. these findings cohere with 612 the previous literature on expertise formation -as well as with common human experience. finally, 613 these findings may be important in understanding aberrant inference and learning in 614"
"statistics exchanged. once a contact opportunity is present, both peers have to ask only for newer versions of the statistics entries (message id, node id) related to the set of messages buffered in their mum buffer. this ensures that, even for the sampled set of messages, only new information is exchanged and no bandwidth is wasted while not introducing any extra latency in the convergence of our approximation scheme."
"following the last of the three conditions, study administrators explained the nature of the study, revealed the confederate's role in the study, and discussed with the group reflections on the experience. the group interview provided an opportunity to interactively engage participants about their responses, and to follow-up on particular comments. the debrief sessions were video recorded and reviewed after the study."
we sought to mitigate these problems by physically moving the proxy in response to the satellite's actions. we began with one degree of motion by putting the display of the proxy on a motorized turntable that is controlled by the satellite participant. we called this a kinetic proxy.
"the number of deictic prompts varied from session to session, depending on the flow of the conversation or the personality of the participants, but when summed over the study, each condition had 12 first-question prompts and responses."
"in this section, we describe the type of statistics each node maintains toward calculating the hbsd utility for each message, and propose a number of mechanisms and optimizations to significantly reduce the amount of metadata exchanged during contacts. finally, we explore the impact of reducing the amount of collected statistics on the performance of our buffer management and scheduling policy."
"however, specialization does not come without its costs. the price of expertise is reduced flexibility 487 when adapting to new environments, especially when the new settings are contradictory to previous 488 settings (11, 36) . theoretically, the expert has a simplified model of their domain, and, throughout 489 their extensive training, has the minimum number of parameters necessary to maintain their model's 490 high accuracy. consequently, it becomes difficult to fit this model to data in a new, contradictory 491 environment that deviates significantly from the expert's experience. for instance, we observe that 492 ."
"no response: none of the participants responded to the question. an example is several seconds of silence, followed by a new thread in the conversation."
"note that the way in which we define our reduced model influences how learning of the e parameters 527 proceeds. recall that to explore a plausible model space of priors, we increased concentration 528 parameters by 8 and divided the others by either 2 and 4. these changes were hand-crafted and 529 somewhat arbitrary, and are basically used to assess the change in model evidence when prior beliefs 530 in a particular policy are strengthened, relative to others. the exact ways in which the repertoire of 531 reduced models could be specified in terms of as hyperparameters, and reasonably there would be 532 hyperpriors, which are prior distributions over hyperparameters. 533"
"theorem 3.2. to minimize the average delivery delay of all messages, a dtn node should apply the gbsd policy using the following utility for each message i:"
"when global information is unavailable, one can calculate the average delivery rate of a message over all possible values of mðt þ and nðt þ, and then try to maximize it. in the framework of the gbsd policy, this is equivalent to choosing the estimators n ðt þ and m ðt þ so that the calculation of the average delivery rate is unbiased"
"to address the above limitations, future work could involve more complex tasks to more clearly 590 differentiate between specialist, generalist and naïve agents. additional types of learning should also 591 be included, such as the learning of state-outcome mappings (optimising the model parameters of 592 the likelihood (a) matrix, as described in (4,6)), to understand how learning of different 593 contingencies influence one another. in addition, more complex tasks may afford the opportunity to 594 examine the generalisation of specialist knowledge to new domains (53). this topic has recently 595 attracted a great deal of attention from the artificial intelligence community (54, 55) . 596"
"to test the effectiveness of the kinetic proxy, we conducted a laboratory study to compare it to a typical stationary video display, and to compare explicit and implicit motion control mechanisms. the study sought to explore the satellite's ability to project gaze cues under these alternative conditions, and hub participants' resulting sense of gaze awareness and presence, by including the directional affordances that people enjoy in face-to-face conversation."
"similar to the case of delivery rate, we calculate the estimators n ðt þ and m ðt þ in such a way that the average delay is not affected by the estimation. this gives the following per-message utility specific to hbsd,"
"hp's halo and cisco's telepresence are conference room-scale installations that support individual as well as group meetings. because all of the participants at each site share the same views of the participants at the other site, correct gaze is not maintained as one moves to different positions within the room. the virtual window [cit] sought to adjust for motion parallax such as this by tracking participants' head motions and moving a remotely-operated camera to simulate the effect of looking through an opening directly into the remote space."
"in order to assess participants' level of engagement in the conversation, we measured their communication behavior using the sociometric badges. a withinsubject analysis of this data detected several significant differences between the conditions, which suggest that the different configurations did have an impact on participants' engagement."
"there are other works in the area of web service discovery and selection including qos selection [cit], collaborative and content-based filtering methods [cit] which are less relevant."
"in our future work we want to extend the method so that we can assign capacities to individual edges. in cooperation with programmableweb.com, we also plan to improve the linked web apis dataset and eventually make it available in the linked data cloud. we want to enrich this dataset with user profiles from traditional social networks. we also plan to incorporate to our method various social network analysis metrics evaluated on the linked web apis dataset. last but not least we want to evaluate the method on datasets from the linked data cloud."
"we establish the following methodology: (1) the impact of successful transmission of control information is represented by the \"knowledge state\" of a node, that encodes what control information the node has sent into the channel, and what control information it has received. note that because these are wireless links, a node can never be sure that the control it has sent has been perfectly received. ( 2) the knowledge states of a node form a markov chain, and the probability that the node takes action a upon knowledge state s is an optimization variable. it is key to note that these variables are defined only with respect to the local information available at that node, which is encoded in its knowledge state. (3) the cartesian product of the markov chains of all the nodes is the global knowledge state representation of the network. the optimizer uses this global state representation, even though no individual node has access to the global state representation. (4) based on the symmetries of the network (such as the case of identical nodes), the global state representation is mapped to an equivalent representation, called the \"reduced global state representation\". this is a significant step in reducing the total number of variables of the program, as well as the computation of the symbolic expression of the objective function. (5) an objective function is given exogenously (e.g. average network throughput) as a metric defined on each arc or state of the markov chain. it is essential to note that no symbolic expression of the objective function is given a priori (since its number of terms scales with the size of the markov chain). (6) the objective function is symbolically computed recursively by a breadth-first search down the reduced global state representation, and the optimization program is symbolically generated. (7) the generated optimization program is solved via state-of-the-art non-linear solvers."
"our local policies offer a distributed implementation of a gradient ascent algorithm for this problem. gradient ascent algorithms look at the current state, i.e., vector nðkþ at step k, and choose a neighboring vector nðk þ 1þ that improves the optimization function in (13), and provably converge to the optimal solution [cit] . in our case, a step corresponds to a contact between two nodes, and the neighboring states and permitted transitions depend on the messages in the buffers of the two nodes in contact. in other words, our gradient ascent algorithm is supposed to make enough steps to converge to the optimal copy vector n ã, before the state of the network (i.e., number and id of messages) changes enough for the optimal assignment to change significantly. this depends on the rate of update steps (%l 2 ) and the message ttl. if t t lããl 2 ) 1, then we expect the distributed, local policy to be able to closely follow the optimal solution at any time t. in section 5.4, we use simulation to prove that this is indeed the case for the scenarios considered. 5 . note that the laplace transform is not raised anymore to the n i th power, as the distribution already corresponds to the sum of all rates. global knowledge in practice it is clear from the above description that the optimal policy (gbsd) requires global information about the network and the \"spread\" of messages, in order to optimize a specific routing metric. in particular, for each message present in a node's buffer, we need to know the values of m i ðt i þ and n i ðt i þ. in related work [cit], it has been suggested that this global view could be obtained through a secondary, \"instantaneous\" channel, if available, or by flooding (\"in-band\") all necessary metadata. regarding the former option, cellular network connections are known to be low bandwidth and high cost in terms of power and actual monetary cost per bit. in networks of more than a few nodes, the amount of signaling data might make this option prohibitive. concerning flooding, our experiments show that the impact of the flooding delay on the performance of the algorithm is not negligible. in practice, intermittent connectivity and the long time to flood buffer status information across dtn nodes, make this approach inefficient."
"at the end of the session, all participants were asked to indicate \"in which of the three sessions did you and the group communicate best with the remote participant?\" four indicated the stationary condition, five indicated the explicit condition, and seven indicated the implicit condition (with two stating no preference). overall, participants felt that the kinetic conditions were more effective by 3:1 over stationary."
"furthermore, aiming the display in the direction of the satellite's gaze did not address the eye contact issue. especially in the implicit condition, where turning the head was used to aim the display, the combination of swiveling the screen and having the head aimed off center from the camera combined to disrupt a true sense of eye contact (\"seemed as if she was looking over my shoulder.\")."
"eye contact and gaze awareness are mechanisms used for attention projection and awareness when face-to-face. there has been a long series of research prototypes that have indicated how difficult it is to re-create eye contact and correctly convey gaze awareness in videoconferencing. but there are other mechanisms for conveying attention awareness without having to recreate eye contact. while fels and weiss [cit] have begun to explore this space, we see more opportunities to support attention awareness without relying strictly on eye contact and gaze awareness."
"proof. the probability that a copy of a message i will not be delivered by a node is given by the probability that the next meeting time with the destination is greater than r i, the remaining lifetime of a message"
". this is equal to expðàr i þ under our assumptions. knowing that message i has n i ðt i þ copies in the network, and assuming that the message has not yet been delivered, we can derive the probability that the message itself will not be delivered (i.e., none of the n i copies gets delivered):"
"1. scheduling. at each contact, a node should replicate messages in decreasing order of their utilities. 2. drop. when a new message arrives at a node with a full buffer, this node should drop the message with the smallest utility among the one just received and the buffered messages. we will derive next such a per-message utility for two popular metrics: maximizing the average delivery probability (rate), and minimizing the average delivery delay. table 1 contains some useful notation that we will use throughout the paper. finally, the gbsd optimization policy is summarized in fig. 1 ."
"the 18 participants (9 male, 9 female) were recruited from the local region and did not know each other prior to the study. they were given a gratuity for their participation. participants ranged in age from 20 to 55 years old. their prior experience with videoconferencing varied from this study being their first exposure, to participating in conferences on a weekly basis. while individual groups had an uneven makeup, every group had both genders (the confederate was female)."
"our goal was to measure the influence of the kinetic proxy on conversation effectiveness. for this comparison, we included sociometric measures such as speaking time and energy, segment length, and turn-taking. prior work demonstrates how these measures can characterize interaction [cit], as well as establishes desirable values and directionality for the measures in face-to-face interaction [cit] ."
"analogous to a general practitioner, who must entertain many possible treatment plans for each 385 patient, compared to a surgeon who is highly skilled at a specific operation. 386 ."
"likewise, more unique cases of learning can also be explored, such as the ability and flexibility to re-601 learn different tasks after specializing, the influence of sleep deprivation on policy learning, and 602 different ways of conducting model comparison (as discussed above). 603 ."
"the above findings suggest that it would be quite useful to find a generic way to signal the congestion level and identify the threshold based on which nodes can decide to either activate our hbsd scheme or just use a simple drop/ scheduling policy. suspending a complex drop/scheduling mechanism and its underlying statistics collection and maintenance methods, whenever not needed, can help nodes save an important amount of resources (e.g., energy), while maintaining the same end performance. finally, we believe that the indirect signaling provided by the behavior of the utility function during congestion, could provide the basis for an end-to-end congestion control mechanism, a problem remaining largely not addressed in the dtn context."
"the satellite's wide display meant that she had to turn her head to see the hub participants to her left and right. for the implicit control condition, we tracked this head motion using an in-house, webcam-based software head tracker. this mode also updated the proxy position approximately 30 times per second. similar to the explicit control condition, we used only the horizontal component of head rotation to orient the proxy's screen."
"to each source node, we have associated a constant bit rate (cbr) application, which chooses randomly from [0, t t l] the time to start generating messages of 5 kb for a randomly chosen destination. we have also considered other message sizes (see, e.g., [cit] ), but found no significant differences in the qualitative and quantitative conclusions drawn regarding the relative performance of different schemes. 7 unless otherwise stated, each node maintains a buffer with a capacity of 20 messages to be able to push the network toward a congested state without exceeding the processing and memory capabilities of our simulation cluster. we compare the performance of the various routing protocols using the following two metrics: the average delivery rate and average delivery delay of messages in the case of infinite t t l. 8 finally, the results presented here are averages from 20 simulation runs, which we found enough to ensure convergence."
"each participant wore a sociometric badge that consists of several sensors, including an internal microphone, accelerometer, and infrared emitter and detector, which store their data on a removable micro-sd card. the sensors are all housed in a lightweight black plastic case that is similar in shape, but slightly smaller in size, than a deck of playing cards. one badge was worn around each participant's neck on a lanyard that positioned it about mid-chest."
"the remainder of this paper is structured as follows. section 2 describes the underyling linked web apis dataset we use in this work and section 3 describes the maximum activation method, its definitions and algorithm. section 4 describes several experiments when we run the method on the linked web apis dataset and a case study. section 5 describes the related work including the method comparison with the spreading activation method, and finaly section 6 concludes the paper and describes the future work. over 300k rdf triples. to build this dataset we gathered data about web apis, mashups, their categories, developers' profiles, and relationships among web apis and mashups that use them, relationships among web apis, mashups and developers who developed them, and relationships among web apis, mashups and categories. moreover, we also capture the time of the web apis and mashups when they appeared in the programmableweb's directory for the first time."
"when the offered traffic load changes frequently, convergence speed becomes important. the bigger the mum buffer the faster our hbsd policy react to changing congestion levels. we illustrate this with the following experiment. we maintain the same simulation scenario, but we vary the number of cbr sources among each two consecutive ttl(s), from 10 to 70 sources (i.e., the first and second ttl window we have 10 sources, the third and fourth window 70 sources, etc.-this is close to a worst case scenario, as there is a sevenfold increase in traffic intensity within a time window barely higher than a ttl, which is the minimum required interval to collect any statistics). furthermore, to ensure nodes use nonobsolete statistics toward calculating utilities, we force nodes to apply a sliding window of one ttl to the messages with complete history stored in the mch buffer, and to delete messages out of this sliding window."
4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi:
"let's denote by n ðt þ and m ðt þ the estimators for n i ðt þ and m i ðt þ of message i. for the purpose of the analysis, we suppose that the variables m i ðt þ and n i ðt þ at elapsed time t are instances of the random variables nðt þ and mðt þ. we develop our estimators n ðt þ and m ðt þ so that when plugged into the gbsd's delivery rate and delay permessage utilities calculated in section 3, we get two new per-message utilities that can be used by a dtn node without any need for global information about messages. this results in a new scheduling and drop policy, called hbsd, a deployable variant of gbsd that uses the same algorithm, yet with per-message utility values calculated using estimates of m and n."
"intuitively, the higher δ is, the more evidence the reduced model has. we can evaluate δ for an 826 arbitrarily large number of reduced models. 827 . cc-by-nc-nd 4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint"
"a number of sophisticated solutions have been proposed to handle routing in dtns. yet, the impact of buffer management and scheduling policies on the performance of the system has been largely disregarded, in comparison, by the dtn community."
"since global information gathered thus about a specific message might take a long time to propagate and hence might be obsolete when we calculate the utility of the message, we follow a different route. rather than looking for the current value of m i ðt þ and n i ðt þ for a specific message i at an elapsed time t, we look at what happens, on average, for all messages after an elapsed time t . in other words, the m i ðt þ and n i ðt þ values for message i at elapsed time t are estimated using measurements of m and n for the same elapsed time t but measured for (and averaged over) all other older messages. these estimations are then used in the evaluation of the per-message utility."
"the second method of learning is learning via expansion. here, we start with a very simple model and 521 increase its complexity until a more optimal model is reached. concretely, this problem of increasing 522 a parameter space is one addressed by bayesian nonparametric modelling (45), and has been 523 theorized to be utilized biologically for structure learning to infer hidden states and the underlying 524 structures of particular situations (46, 47) . 525"
"similar to model parameters, hyperpriors can be optimised over time to reduce the path integral of 534 free energy. for example, in bayesian model reduction there can be different settings for how much 535 to increment concentration parameters, and different degrees of comprehensiveness when it comes 536 . biorxiv preprint to exploring the reduced model space (i.e. whether or not to iterate through all possible combinations 537 of policies). if one subscribes to the notion that this kind of structure learning occurs during sleep, 538 optimising hyperparameters becomes a behavioural scheduling problem. the organism can sleep 539 more frequently to compute empirical priors for the next period of waking, or it can spend more time 540 awake to gather empirical data. both periods (sleep and wake) offer different advantages, and the 541 balance between them is a delicate equilibrium -influenced by ecological pressures. one can imagine 542 that the species-specific circadian rhythms maintain this optimum and evolution helps to fine-tune 543 the hyperparameters facilitating this schedule (48, 49) . 544"
"sampling messages to keep track of. a first interesting question is: should a node maintain global statistics forevery message it has heard of or only a subset? we argue that monitoring a dynamic subset of these messages is sufficient to quickly converge to the correct expectations we need for our utility estimators. this dynamic subset is illustrated in fig. 10 as being the messages under monitoring, which are stored in the mum buffer; it is dynamic because its size is kept fixed while messages inside it change. when a node decides to store a message for the first time, if there is space in its mum buffer, it also inserts it there and will track its global state. the actual sampling rate depends on the size of the mum buffer and the offered traffic load, and results in significant further reduction in the amount of metadata exchanged. at the same time, a smaller mum buffer might result to slower convergence (or even lack 9. according to the nyquist-shannon [cit] sampling theorem, a good approximation of the size of a bin would be equal to intermeeting-time/2. a running average of the observed times between consecutive meetings could be maintained easily, in order to dynamically adjust the bin size [cit] . off). in section 6.3, we study the impact of mum buffer size on the performance of our algorithm."
"here, we use simulations results (based on the rw scenario) that our proposed policy (gbsd) can \"keep up\" with the optimal algorithm described in section 3.5. fig. 6 plots the normalized manhattan distance"
"the two quantities required to form posterior beliefs about the best policy (i.e., the free energy and 189 expected free energy of each policy) can be computed using the a, b, and c matrices (4,18). the 190 variable is an inverse temperature (precision) term capturing confidence in policy selection, and x 191 is the (expected log of the) intrinsic prior probabilities in the absence of any inference (this is covered 192 more in-depth in the \"policy learning and dirichlet parameters\" section below). the three quantities 193 are passed through a softmax function (which normalizes the exponential of the values to sum to 194 one). the result is the posterior expectation; namely, the most likely policy that the agent believes it 195 is in. this expectation enables the agent to select the action that it thinks is most likely."
"a more subtle problem is that the satellite is just not as \"present\" as his collocated counterparts. we have observed several instances where people are talking in order around the table, e.g., to introduce themselves or report status, and the satellite on his proxy is skipped. we call this the skip-over effect. despite the physical presence of the proxy, we believe there are many factors contributing to this deficit in presence."
"five badges were used in total: one worn by each hub participant, one worn by the satellite, and one placed on the proxy. the badge worn by the satellite was not used in the analysis, as the one on the proxy produced a duplicate but cleaner audio signal."
"conversely, a volatile environment precludes specialization. the agent cannot single-mindedly 496 pursue mastery in any particular subset of policies, as doing so would come at the cost of reduced 497 accuracy (and an increase in free energy). the generalist agent therefore never reaches the level of 498 performance that the specialist agent is capable of at its best. instead, the generalist performs barely 499 above the naïve average reward-acquisition rate, even when tested under a general environment. 500"
"there are two principal modes of (policy) learning. the first is learning via reduction, which entails a 510 naïve agent that starts with an over-complete repertoire of possible policies, who then learns to 511 discard the policies that are not useful. this is how we have tackled policy learning here; specifically, 512 via optimising a dirichlet distribution over policies, using bayesian model reduction. by starting with 513 an abundance of possible policies, we ensure that the best policy is likely to always be present. this 514 ."
"we assume there are l total nodes in the network. each of these nodes has a buffer, in which it can store up to b messages in transit, either messages belonging to other nodes or messages generated by itself. each message has a time-to-live (t t l) value, after which the message is no more useful to the application and should be dropped by its source and all intermediate nodes. the message can also be dropped when a notification of delivery is received, or if an \"antipacket\" mechanism is implemented [cit] ."
the best drop or forwarding decision will be the one that maximizes jáðddþj (or àáðddþ). this leads to the per-message utility in (4). t u
"these two terms can be rearranged into epistemic and pragmatic components which, as one might 186 guess, reduce uncertainty about hidden states of the world and maximise the probability of preferred 187"
"by physical motion, we mean the physical movement of the proxy within the meeting room (where the hub participants are located). this is in contrast to apparent motion, which might be represented by repositioning a projected image on a stationary screen. for this study, we focused on physical motion of the screen that displayed the satellite participant's video stream."
"contrary to this, when the offered load is low enough to ensure that all messages can on average create enough copies to ensure delivery, the optimal policy simply performs a fair (i.e., equal) distribution of resources."
"a study of proxies in everyday use has documented the benefits of a physical representation of the satellite in group interaction [cit] . our own use and studies of proxies in our day-to-day work has led to a design that includes a wide-field-of-view camera that shows most of the meeting room at the hub site (see fig. 3 ). the satellite views this panorama of the hub room displayed in a window that is full-screen width across his display. relative to this window, the satellite's camera is positioned horizontally centered and vertically as close as possible."
. cc-by-nc-nd 4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint
"the reverse, however, is not true. the hub participants have a general sense of whether the satellite is looking left, center or right, but nothing more fine-grained than that. the video mediation introduces too many invisible parameters, such as the field of view of the wide-angle camera and the size of the satellite's desktop monitor, preventing the hub participants from having a precise sense of the satellite's focus of attention. with the multiple potential foci of attention in a typical meeting (e.g., people, whiteboard drawings, artifacts), breakdowns occur when the hubs cannot determine the satellite's focus of attention."
"we return to our question of the effect of the environment on policy learning via setting up a 198 simulated environment in which our synthetic agent (visualized as a mouse) forages (figs 2a and 199 2c) . our environment takes the form of a two-step maze inspired by (19), which is similar to that 200 used in previous work on active inference (3, 15) . the maze allows for an array of possible policies, 201 and the challenge for our agent is to learn to prioritize these appropriately. the agent has two sets of 202 beliefs about the hidden states of the world: where it is in the maze, and where the reward is. the 203 agent also receives two outcomes modalities: where it is in the maze and feedback received at each 204 location in the maze (fig 2c, right) . the agent always knows exactly where it is in the maze (fig 2a), 205 and receives different \"feedback\" outcomes, depending on where it is in the maze and the location 206 of the reward (fig 2b) . where it thinks the reward is. the agent is able to make geographical observations to see where it is in the maze 229 (fig 2a), as well as receive a \"feedback\" outcome which gives it a cue to go a certain location, or to give it reward / 230 punishment ( fig 2b) . the small numbers beside each arrow illustrate the ambiguity of the cues. as am example, we"
"furthermore, we look deeper into our distributed statistics collection solution and attempt to identify the available tradeoffs between the collection overhead and the resulting performance. aggressively collecting statistics and exchanging them with every encountered node allows estimates to converge faster, but it can potentially result in high energy and bandwidth consumption, and also interfere with data transmissions. our results suggest that close to optimal performance can still be achieved even when the signaling overhead is forced (through sampling) to take only a small percentage of the contact bandwidth."
"in case of a full buffer or limited transfer opportunity, a dtn node should take respectively a drop or replication decision that leads to the best gain in the global delivery rate dr. to define this optimal decision, we differentiate dr with respect to n i ðt i þ"
"the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint trials, where the reward location / frequency in the testing environment is identical to the environment in which 402 the agent is trained (i.e. a specialist agent is tested in an environment with low volatility and the reward always 403 being on the left of the initial location). the frequency is computed from how many out of the 32 trials the agent is 404 able to get to the true reward location."
"in this last section, we turn our attention to the utility distributions themselves. first, we are interested whether the resulting distributions for hbsd delivery rate and delivery delay utilities react differently to different congestion levels, that is, if the priority given to messages of different ages shifts based on the offered load. furthermore, we are interested whether the resulting utility shape (and respective optimal policy) could be approximated by simple(r) policies, in some congestion regimes."
"any self-organizing system must adapt to its surroundings if it is to continue existing. on a broad 54 timescale, population characteristics change to better fit the ecological niche, resulting in evolution 55 and speciation (1) . on a shorter timescale, organisms adapt to better exploit their environment 56 through the process of learning. the degree or rate of adaptation is also important. depending on the 57 environment around the organism, specialization into a specific niche or favouring a more generalist 58 diminishes, likely because we have learned enough about the structure of the task to discern and 81 learn appropriate habits (12) . 82"
"drawing once again from healthcare, the benefits of generalising are numerous as it allows for the 453 practitioner to react more flexibly to changing demography and societal perspectives (30). 454"
"this is a constrained optimization problem, with k variables and 2k þ 1 inequality constraints. the optimization function in (13) is a concave function in n i . constraint in (14) says that the total number of copies (for all messages) should not exceed the available buffer space in all l nodes, and is linear. finally, the 2k constraints of (15) are also linear, and simply say that there is no point for any node to store two copies of the same message. consequently, if we assume that n i are real random variables (rather than integers), this is a convex optimization problem, which can be solved efficiently [cit] (but not easily analytically)."
4.0 international license is made available under a the copyright holder for this preprint (which was not peer-reviewed) is the author/funder. it . https://doi.org/10.1101/644807 doi: biorxiv preprint
"the study's setting was a distributed conference consisting of several collaborative tasks. we used measures of conversation effectiveness such as engagement, naturalness of interaction, cognitive effort and sense of presence."
"here, represents a vector of sufficient statistics of the posterior belief about policies: i.e., 179 expectations that each allowable policy is currently in play. f is the free energy for each policy based 180 on past time points and g is the expected free energy for future time points. the free energy scores 181 the evidence that each policy is being pursued, while the expected free energy represents the prior 182 belief that each policy will reduce expected surprise or uncertainty in the future. the expected free 183 energy comprises two parts -risk and ambiguity. risk is the difference between predicted and 184 preferred outcomes, while ambiguity ensures that policies are chosen to disclose salient information. 185"
"throughout our analysis, we have so far assumed homogeneous node mobility. recent measurement studies have revealed that, often, different node pairs might have different meeting rates. we extend here our analytical framework, in order to derive per-message utilities that maximize the global performance metric, in face of such heterogeneous mobility scenarios. we illustrate the extension with the delivery rate. 4 specifically, we assume that meetings between a given node pair are exponentially distributed with meeting rate, where is a random variable such that: 2 ½0; 1þ; distributed asfðþ:"
"extensive experiments are conducted to demonstrate the effectiveness and efficiency of tasselnetv2. first, we perform experiments on the wsc dataset to search optimal hyper parameters. after obtaining these, we verify the effect of adding context in tasselnetv2. next, tas-selnetv2 is further compared against other state-of-theart approaches on the wsc dataset. to demonstrate the generality of tasselnetv2, we also evaluate it on the mtc [cit] and shanghaitech datasets [cit] . mean absolute error (mae) and root mean squared error (rmse) are chosen to quantify the counting performance. they are defined as"
"the local visual context, in the framework of local regression, refers to the surrounding pixels of local sampling patches. in fig. 2, if the visible parts belong to local sampling patches, those invisible parts represent the context. unfortunately, since the context is not within local patches, it remains invisible to local regression networks like tasselnet. if a network can see the context, overlapping objects or part of objects may be inferred easily and counted accurately. the high-level idea is thus to enable the network to process both local patches along with the context, as shown in fig. 3 ."
"there are tens of thousands of wheat spikes in the wheat images, and they present a high degree of similarity when the time interval is short, which makes the annotations for all of the captured images costly and needless. this means only a subset of images is essential to build the dataset, but this subset should be large enough to cover wheat spikes in various scenarios. we pick out this subset with a two-stage selection strategy. at the first stage, we choose images according to the date, after the heading stage of wheat. before obvious emergence of spikes, the sampling interval is set to 3 days. after wheat spikes emerge, the number of wheat spikes changes rapidly, and thus the sampling interval is shortened to 2 days. at the second stage, 10 candidate images collected in each day (from 8 a.m. to 17 p.m.) are taken into account. considering the illumination characteristics in one day, three images are chosen from three time periods, i.e., morning (8 a.m. to 11 a.m.), noon (12 a.m. to 14p.m.), and afternoon (15 p.m. to 17 p.m.), to maintain the diversity of the dataset."
"we first highlight the concepts of \"input image\", \"input patch\" and \"input patch with context\" in fig. 7 . they are prerequisites for readers to better understand tasselnetv2."
"albeit successful, we found that tasselnet cannot predict correct counts when spikes partially present in local image patches. as shown in fig. 2, it is not clear whether there are two wheat spikes or not when only looking at those visible regions. this situation is even more serious when spikes are occluded. in fact, wheats are planted far denser than maize plants, and the density of spikes typically varies between 200/m 2 and 600/m 2, which means partial spikes would occur frequently in cropped local image patches and thus seriously limits the applicability of tasselnet. to address this, our intuition tells that we need the help of visual contextual information. this is in consistent with the fact that, when one cannot infer the exact number of partially occluded objects within a local area, he may look further until supporting information, such as the border or other object parts, is identified. this kind of supporting information in real world refers to the visual context in images, and it is a kind of \"weak context\" for it only contains the local surroundings rather than all of remaining images. therefore, a simple way to tackle above problem is to enable tasselnet to receive both local images and their surrounding pixels, as shown in fig. 3 . this raises a subsequent question: how to integrate the context into cnns in a principled way? one way is to use large convolutional kernels but at the cost of introducing extra parameters. in this paper, we show that a much clever way is to include the context as part of the receptive field so that the model can keep the same number of parameters. this idea is particularly useful for local counting models, such as tasselnet, that do not make full use of their receptive field. as a consequence, we make a simple yet effective extension to tasselnet so that contextual information could be received, leading to an extended version of tasselnet-contextual tasselnet (tasselnetv2 for short)."
"we first compare tasselnet trained with/without the context to highlight the pure effect of adding the context. then, tasselnetv2 is evaluated to show its efficiency and accuracy beyond tasselnet."
"since tasselnet is the direct baseline of tasselnetv2, we set the hyper parameters of tasselnetv2 same as the tas-selnet, in order to demonstrate the superiority of tas-selnetv2 w.r.t. tasselnet and the benefit of embedding context information. hence, we first search the optimal parameters on the wsc dataset using tasselnet so that tasselnet can report the optimal performance, and we then apply the same parameters to tasselnetv2."
"in this work, we addressed an important and practical problem of counting wheat spikes in the field-based environment using computer vision. we observe that, some existing cnn-based local regression models, such as tasselnet, suffer from the problem of lacking contextual information, so they usually cannot predict correct counts when objects partially present in local image patches. by integrating the context into the framework of the tasselnet, we proposed a simple but effective extension, i.e., tasselnetv2. a large-scale wsc dataset, with 1, 764 images and 675, 322 annotated wheat spikes, is also created. the dataset is very challenging due to intrinsic and extrinsic variations not only in spikes per se but also in environment, which makes it appropriate to be used as a benchmark for counting non-rigid objects."
"on both the part a and part b subsets, the benefit of adding the context can be reflected when comparing tasselnetv2 with tasselnet, but the improvement is marginal. when using a pre-trained vgg-16 model, tas-selnetv2 † outperforms csrnet and reaches the state-ofthe-art performance. this suggests pre-trained models is necessary to fully exploit the benefit of context on the shanghaitech dataset. figure 10 shows some qualitative results of tasselnetv2 on the wsc dataset. in most cases, tasselnetv2 predicts accurate counts (the first four rows). however, it exposes prominent under-estimate phenomena in some cases, particularly when severe overlapping and heavy blurring occur. these visual patterns raise a huge challenge to discriminate spikes even for a human expert. efforts still should be paid to overcome these challenges. we leave this for future explorations."
"counting wheat spikes is a typical object counting problem in computer vision, and currently convolutional neural network (cnn)-based local regression models have shown remarkable performance in counting crowd [cit], vehicles [cit], cells [cit], animals [cit], and plants [cit] . however, when turning to the scenario of counting wheat spikes in the wild, things are much difficult due to the non-rigid nature of spikes and substantial visual challenges. as shown in fig. 1, these challenges are:"
"with the rapid development of recent deep learning technologies, large-scale visual databases and cost-effective graphical processing units, image-based approaches appear to be promising alternatives to automate the task of wheat spikes counting."
"where n denotes the number of images, c pre i denotes the predicted count of the i-th image, and c gt i denotes the corresponding ground-truth count. mae measures the accuracy of counting, and rmse measures the stability. lower mae and rmse imply better counting performance."
"another limitation of tasselnet is its low efficiency due to the need of densely sampling local image patches. this introduces many redundant computations. we wonder whether these redundant computations could be avoided in tasselnetv2. inspired by fast r-cnn [cit], we show that one actually can first extract the features maps of the whole image and then densely sample the feature maps to obtain local features, rather than processing local patches individually. based on this idea, we implement a fully convolutional form of tasselnetv2, which is proven to be an order of magnitude faster than tasselnet. in particular, we created a large-scale wheat spikes counting (wsc) dataset to validate the effectiveness of tasselnetv2."
"we further analyze the error distributions in fig. 9, and find that patch-based and image-based errors are more likely to shift towards zero with the help of context. so far, it can be concluded that lacking the context is the main drawback of tasselnet, and it is important to add the context during training."
"we implement tasselnetv2 based on matconvnet [cit] . during training, we use 1359 images in the training and validation sequences of the wsc dataset. 90% images are randomly chosen for training, while the rest 2x2 max pool1, 0, /2 3x3 conv1, 16 for validation. before learning, mean subtraction is preprocessed (the mean is computed from the training set). it is worth mentioning that, no data augmentation is performed because the wsc dataset already contains wheat spikes under various scenarios."
"the main idea of tasselnetv2 is to process local patches with the context. notice that there is a massive waste of the receptive field in tasselnet. it is natural to think how to reduce such a waste. in this paper, we show that one can cancel zero paddings to enable the network receiving extra context and to make full use of the receptive field. the way to achieve this is simply to delete paddings in all of convolutional layers, as shown in fig. 6 ."
"in agricultural production, crop yield is one of the key factors when monitoring crop growth status. wheat is one of the top three cereal crops in the world. its"
"some wheats may be perpendicular to the lens and only occupy a small number of pixels in the image, which renders difficulties to distinguish wheat spikes from background. this also leads to large size variations of wheat spikes (fig. 1f )."
"extensive experiments show that, tasselnetv2 reaches 91.01% relative counting accuracy and achieves the stateof-the-art performance on the wsc dataset, and notably, can process images 13.21 times faster than tasselnet (13.82 fps for tasselnetv2 vs. 1.05 fps for tasselnet). further experiments demonstrate that tasselnetv2 also reports state-of-the-art counting performance on the maize tassels counting (mtc) and shanghaitech crowd counting datasets [cit], which confirms a good generality of tasselnetv2. several interesting ablative studies are conducted to justify the effectiveness and necessity to include the context for better counting performance."
"extensive experiments illustrate that, tasselnetv2 achieves state-of-the-art performance on the wsc dataset with 91.01% relative counting accuracy, and is also more than an order of magnitude faster than tas-selnet. further evaluations on the mtc and shangha-itech datasets demonstrate that tasselnetv2 can also push forward the state of the art. sufficient analyses of potential issues effecting the practical application of tasselnetv2 are also described, including emphasizing the role of the context in object counting, searching optimal parameters for local counts regression, and analyzing potential errors. we believe tasselnetv2 shows great potentials to be applied to other object counting domains. albeit empirically effective, the reason why the context can improve the counting performance only stays at an intuitive level, and it remains unclear how the context interacts with the central receptive field as auxiliary information. we hope such empirical findings in this paper could inspire others to uncover the mystery of the receptive field. fig. 10 some ground truth density maps overlaid on original images on the test set of the wsc dataset and count maps generated by tasselnetv2 (finetuned with pre-trained vgg16). the number above each original image denotes the ground truth count number of wheat spikes, while that above each density map denotes prediction count number. the last line shows some unsuccessful predictions, and error maps of these images are also presented. an error map denotes the difference of the ground truth and predicted density map. over-estimate is denoted by red, under-estimate by blue, and minor difference by gray. the darker the color is, the greater the errors are. we also zoom in some local areas with high counting errors. 'gt' denotes ground-truth counts and 'error' denotes the difference compared to the ground truth. further visualizations can be found in additional file 1."
"local patches from an image may have severe overlaps due to dense sampling, but tasselnet requires extracting the local feature from each patch first and then mapping it to the local count. in this paradigm, many redundant calculations appear during feature extraction. inspired by fast r-cnn [cit], redundant calculations can be avoided by first extracting the feature maps of the whole image, then densely sampling the feature maps to obtain local features and finally mapping them to local counts in a light-weight manner."
"inspired by the idea of fully convolutional networks (fcns) [cit], we implement tasselnetv2 into a fully convolutional form, which speeds up both training and inference significantly, as shown in fig. 6 . in what follows, we further explain in detail how tasselnetv2 works and improves efficiency."
"through extensive experiments, the optimal setting of hyper parameters for tasselnet on the wsc dataset is summarized in table 4 . detailed procedures of searching optimal parameters are provided in additional file 1."
"with seven sequences in the wsc dataset, the training set, validation set and test set are divided, as shown in table 2 . images from the shandong taian (2012 [cit] camera 1) sequence exhibit a relatively clear distinction between spikes and background. spikes in this sequence also appear to have a high density and are with dramatic changes caused by illumination. in the henan zhengzhou (2012 [cit] ) sequence, it is hard to distinguish the spikes from the background. the presence of severe occlusions makes this task even more challenging. evaluations on these sequences can sufficiently show the adaptability and robustness of the counting method. local visual context may be helpful for identifying overlapped objects, as shown in fig. 2 . we embed local visual context in tas-selnetv2 to alleviate such a problem. following [cit], dotted annotation is adopted where a point is marked at the location of each wheat spike. figure 5 shows an example of annotated image. six colleagues in our laboratory first participated in the annotation process. after the dataset is annotated, we double-checked the annotations and corrected some missing and wrong annotations. especially for the second round checking, we trained a tasselnet to predict counts and identified the areas with high counting errors. with this kind of auxiliary information, particular attentions are paid to these areas for careful checking further, and other areas are also checked again."
"the above experiments justify that it is better to use a portion of the receptive field as the context, instead of counting all objects within the whole receptive field [cit] ."
"plant methods *correspondence: poppinace@hust.edu.cn 1 national key laboratory of science and technology on multi-spectral information processing, school of artificial intelligence and automation, huazhong university of science and technology, wuhan 430074, people's republic of china full list of author information is available at the end of the article grain yield is mainly associated to spike number m −2, grain number m −2 and thousand grain weight [cit] . among these traits, spike number m −2 is the most important index [cit] . conventional manual approaches to counting wheat spikes are tedious and labor-intensive. the counting results are also errorprone and unrepresentative due to small sampling areas used. to meet the need of large-scale analyses in the era of intelligent agriculture and to obtain the index of spike number m −2 accurately in real time, counting wheat spikes must be automated in a reliable way, and possibly with low cost."
"according to the above evaluations, the optimal setting on the wsc dataset is shown in table 4 . next, to compare tasselnetv2 with other state-of-the-art methods, several well-established baselines are chosen:"
"quantitative results are presented in table 5 . we observe that, when forcibly adding the context into tas-selnet during only inference (trained without context), the counting error increases notably, which suggests that tasselnet cannot utilize contextual information when trained without the context. this is the problem table 5 the effect of context on the test set of the wsc dataset. \"train\" denotes adding context into tasselnet since training phase as fig. 7b, while \" we call information asymmetry. however, after embedding contextual information since the training phase, the mae decreases more than 10 without increasing model parameters (compared to tasselnet). adding the context is effective. it is worth noting that this significant performance improvement comes almost at no cost. it also can be observed that tasselnetv2 exhibits the same degree of improvement of adding the context. meanwhile, tasselnetv2 is more than 10 times faster than tasselnet during the training stage. this is achieved by processing input images in a fcn manner rather than densely sampling image patches, thus avoiding redundant computations in feature extraction, as analysed in table 3 . now we can say that tasselnetv2 is a much more efficient implementation of adding the context into tasselnet."
"with a pre-trained model, tasselnetv2 † only performs slightly better than tasselnetv2 but increases more than an order of magnitude of parameters. we conjecture the main reason is the lack of training samples in the mtc dataset (only 186 training images). the potential of pre-trained models may not be fully exploited with such a small dataset, while a small network, such as tasselnetv2, can already produce satisfactory results. in this case, tasselnetv2 is effective and efficient, which seems to be a better choice than tasselnetv2 † ."
"in this section, the sea_ml-based mas development approach is discussed. although this new development methodology also considers the adoption of sea_ml, it differentiates from the previous development approach [cit] as being a complete development methodology covering the analysis, design, and implementation of the mas. analysis phase, which does not exist previously, is now included in the methodology and both analysis and design phases are improved with two types of iterations. such an iterative development process is not considered in the previous methodology. in addition to the modification of models, new methodology also supports the changes in auto-generated codes if required. the proposed sea_ml-based mas development methodology includes several steps following each other (see figure 1 ): mas analysis, mas modeling, model-tomodel (m2m) and model-to-code (m2c) transformations, and finally code generation for exact mas implementation. following subsections discuss the methodology's phases covering those steps."
"we believe that our work on the development of e-barter systems contributes to the abovementioned efforts by first utilizing the semantic web services and capabilities of bdi agents for e-bartering instead of reactive agents which is preferred in most of the previous work. in addition, the remaining studies (e.g., [cit] ) which utilizing bdi model for e-bartering do not benefit from internal reasoning mechanism of bdi agents which can bring extra intelligence in the procedure of matchmaking for the e-bartering. also, these studies do not use swss as the automatic selection mechanism for the categories. finally, while the methodologies of the other studies are generic for agent development, our study uses sea_ml and its tool inside a domain-specific methodology. this makes the design and development of the mas system easier and less-costly as the developer works with the domain concepts and benefits from generative capability of the provided tool."
"the rest of the paper is organized as follows: the next section presents the proposed mas development methodology based on sea_ml modeling language. the analysis, design, and implementation of the e-barter system, using the proposed methodology are discussed in section 3. section 4 gives a demonstration of the implemented system. in section 5, the related work is reported and compared with our study. finally, the paper is concluded in section 6."
"in the traditional e-barter systems [cit], customers and their products' information are stored in databases in a monolithic way. this approach has two major disadvantages. first, the system is not scalable. by adding different product categories, the maintenance effort and cost of the system will be increased. the second disadvantage is that a semantic approach cannot be achieved with the traditional methods. in this study, these two disadvantages have been overcome by using sws. the use of sws primarily ensures that the customer and the product information are stored categorically in the external services. this leads to a more scalable system. in addition, these services with the semantic structure allow a semantic logic to be implemented in the matching and gives the opportunity to the customers to communicate only with the appropriate services. this increases the likelihood that the barter process has successful result in a limited time."
"the semantic web services modeled in sea_ml are transformed into owl-s services to enable the implementation of these services. owl-s offers a high-level service ontology that can store three basic information about a service. the service profile tells you what the service is doing and provides information to discover a service. the service model describes how the service can be used and the composition of the service. finally, service grounding provides information on how to interact with the service. therefore, in our study, each sws modeled in sea_ml is transformed into an owl-s service element and the appropriate service profile, service model and service grounding documents are created for the related sws."
"therefore, when considering customer agent request for bartering foods, the agent should play the foodbartering role. while playing this role, the agent applies askmatchmaker plan for finding a suitable service interface of a food sws. this plan is realized by interacting with the matchmaker agent which applies registerrequest plan to register services. therefore, customer agent cooperates with matchmaker to receive some services for getting name of customer agents who are candidates to barter. finally, the customer agent applies its foodservicecall plan to collect candidate customer agents with whom it can negotiate."
"semantic service agreement plan (ss_agreementplan) is a concept that deals with negotiations on quality of service (qos) metrics (e.g., service execution cost, duration and position) and contract negotiation. semantic service executor plan (ss_executorplan) after service discovery and negotiation, the agent applies the semantic service executor plan (ss_executorplan) to invoke appropriate semantic web services."
"this document describes what the service provides for prospective clients. this is used to advertise the service, and to capture this perspective, each instance of the class service presents a service interface."
"at this point, the negotiation between agents starts. first, customer c sends a proposal to customer a. however, as seen in line (a) of figure 9, the customer a rejects this offer because the offer sent by the customer c is higher than the upper limit of the customer a. then, customer c sends the same proposal to the customer b. since the proposal is in the acceptable range of customer b, the negotiation between them begins and eventually, an agreement was reached by these two agents as shown in line (b) of figure 9 ."
"in this study, we also experienced that the proactive behavior of the bdi agents may help the fruitful application of e-barter systems by especially preventing bartering the goods ineffectively with undesired exchange ratios. implemented bdi agents in here aim at choosing the most appropriate plan to achieve the maximum gain out of the bargaining on behalf of their users. it is possible to develop a similar mas for the same purpose with agent models other than bdi which probably leads to provide desired efficiency in bargaining. however, in addition to the achieved fruitfulness, we also found modeling and implementation of the mas for e-bartering convenient by utilizing bdi constructs and their relations."
"a package of information to be send from an agent to another; possibly to deliver some information or instructions. two special types of actions, namely send and receive, are used to handle these messages."
"an agent-based e-barter system consists of agents that exchange goods or services of owners according to their preferences without using any currency. the system analysis phase is performed by considering the mas viewpoint of sea_ml language. in fact, this viewpoint provides an overview of the system which is shown in figure 2 . when considering the structure of the system, the ebartersystem constitutes of two semantic web organizations called customers organization which include customer agents, and management organization where the matchmaker agent and serviceagent agents reside. it is worth indicating that entities given in this overview can be considered as stereotypes and in the real system implementation, there can be many instances of these entities. for example, there can be many agents of type serviceagent working in this system. in this mas, a matchmaker agent, which is defined as a swa, handles the interaction between customer agents and serviceagents. this agent is responsible for registering sws provided by each serviceagent in the system and matching proper services with customers. to infer about semantic closeness between offered and purchased items based on some defined ontologies, matchmaker may use sws. conforming to its matchmaking definition, matchmaker needs to discover the proper sws, interact with the candidate service and realize the execution of sws after an agreement."
"based on sea_ml, the analysis and the design of the software system can be realized using the application domain's terms and notations. this helps the end users to work in a higher level of abstraction (independent of target platform) and close to expert domain. also, generative feature of sea_ml paves the way to produce the configured templates from the designed models for the software system in the underlying languages and technologies. currently, sea_ml can generate architectural code for jade [cit], jadex [cit], and jack [cit] agent programming languages and owl-s [cit] and wsmo [cit] sws documents. this is realized by model to model transformation of the designed platform independent instance models to the instance models of the target mas languages and sws technologies. then, these platform specific models are transformed to the platform specific codes by model to code transformations. this generation capability of sea_ml can increase the development performance of the software system considerably. finally, by constraints checking provided in sea_ml, the instance models are controlled considering domain-specific syntactic and semantic rules. these rules are applied in the abstract and the concrete syntaxes of the language. this feature helps to reduce the number of errors during the analysis and design of the software system and avoid postponing them to the development and the testing phases."
"in the system modeling step the agent developer can use the fully functional graphical editors of sea_ml to elaborate the design of the system, which includes 7 viewpoints of the sea_ml's syntax, in addition to the mas viewpoint used in the analysis phase. these viewpoints cover both multi-agent part of the system (using agent internal, plan, role, interaction, and environment viewpoints) and semantic web aspect of the system (using agent-sws interaction and ontology viewpoints). each viewpoint has its own palette which provides various controls leading the designers to provide more accurate models. by designing each of these models for viewpoints, additional details are added to the initial system model provided in the analysis phase. these modifications immediately are updated in the diagrams of all other viewpoints. as the other viewpoints may have some constraint checks to control some properties related to the newly added element, the developer will be directed to complete those other viewpoints to cover the errors and warnings (coming from the constraint checks). this can lead to several iterations in the design phase. the result of this phase is the development of a complete and accurate platform independent model for the designed mas."
"according to the system analysis realized in the previous phases (discussed in section 3.1.1), the system constitutes of two semantic web organizations: management and customers. each semantic web organization is a composition of swas having similar goals or duties. these organizations need access to some resources in the ebartersystem environment. for this reason, there are interactions with the ebartersystem environment to get access permission. in addition, the interactions of agents with each other are modeled. in this case, study, the mas-to-be-implemented consists of 3 types of semantic web agents: matchmaker, customer and serviceagent. all customers and serviceagents cooperate with matchmaker to access the e-barter system. in addition, customer agents interact with each other to negotiate for bartering. for instance, a customer agent cooperates with a matchmaker agent to get the list of customer agents who have the requested product(s)."
"the codes generated by sea_ml are architectural codes and the relations are established by the language considering the model which are controlled by the language at the semantic control stage that prevents most of the semantic errors in the code. also, the codes do not have any syntactical errors at this level. the delta codes should be added manually to establish behavioral logic. such code completion is need for both mas and sws parts of the system."
"the m2t rules are applied on platform-specific models (jack and owl-s models) for the generation of jack bdi agent codes and owl-s documents (including service, profile, process and grounding documents) corresponding to semantic web agents and semantic web services designed in the system. as an example of the generated code, an excerpt of owl-s service file (\"service.owl\") is shown in listing 2. this file consists of the definition of the other documents for the service."
"in this study, a model driven approach is adopted, and a model-based methodology is proposed for design and implementation of semantic web-enabled mass. to this end, the proposed methodology covers the use of a dsml. in this way, the complex systems including swss and mas components are modeled at a higher level of abstraction. in addition, these languages can model the interaction between swss and agents. as a result, the system can be analyzed, and the required elements can be designed using the terms and notations very close to the domain. these domainspecific elements and their relations to each other creates the domain-specific instance models which pave the way to implement the system. as these models are persisted in a structural and formal way, they can be transformed to other proper paradigms, such as mathematical logics. in this way, they can be formally analyzed and validated based on formal methods. this can decrease the number of semantic errors later in the developed system. furthermore, these models can be used to automatically generate the architectural codes for agents and artifacts of the complex systems which can end up with less syntactical errors and speed up the development procedure. according to the definition of artifacts in agents & artifacts (a&a) metamodel [cit], artifacts in our study are environmental components and entities providing services, such as owl-s documents (including process, grounding, and interface documents) for sws. faster development requires less efforts and it brings cost reduction in the projects. moreover, fewer syntactical and semantical errors mean less iterations in the development phase and less testing phase which also reduce the cost and effort. therefore, the system can be checked, and the errors can be partially found in the early phases of development, namely analysis and design phases, instead of finding them in the implementation and testing phases."
semantic service finder plan (ss_finderplan) is a plan in which automatic discovery of the candidate semantic web services take place with the help of the ssmatchmakeragent.
"it is a swa extension. this meta-element represents matchmaker agents which store the sws' capabilities list in a mas and compare it with the service capabilities required by the other agents, in order to match them."
"in this case, study, each service has an ontology to help matchmaking. as there are 2 semantic web services developed for the e-barter system, we have two ontologies called electronic devices and food. these ontologies are used to demonstrate affinity relations in directing appropriate customer agents to appropriate services for barter processing. in the process of matching between the customer needs with services, each semantic web service uses its own ontology."
"as the last step, the developer needs to add his/her complementary codes, aka delta codes, to the generated architectural code to have fully functional system. however, some agent development languages, such as jack, have their graphical editor in which the developer can edit the structure of mas code. the generated codes achieved from the previous step can be edited and customized to add more platform specific details which helps to reach more detailed agents and artifacts. then the delta code can be added to gain the final code."
"for the semantic proximity detection in the e-barter system, the methods that provide the main categories (the top-level concepts in the food ontology) are mapped to the profile documents. thus, the matchmaker agent can determine the appropriate method for the customer agent through the profile document to find the appropriate service. within this case study, only the profile.owl document was used for sws operations. the matchmaker agent can propose the appropriate service to the customer agent with the help of the profile:hasoutput tag in this document. the profile document contains categories at the method level, and these services correspond to the top-level concepts on the ontology. if the product category searched by the matchmaker cannot be found in the profile, the service's ontology is traversed to find out if there is an upper category containing this product and the search in the profile document is repeated. the matchmaker returns the wsdl addresses of the appropriate services to the customer agent after determining the appropriate services. when the customer agent sends a request to a service which is just found, its repository is used which is an xml file containing the names, requests, and offers of agents eligible for bartering. the service searches for those agents which can match with the customer agent on this xml file. if some agents are found, the service will return the names of these agents to the customer agent."
"in this paper, the design and the implementation of sws-enabled agent-based e-barter system were realized using jack agent language [cit] and owl-s sws technology [cit] ."
"in this study, we present three viewpoints of the e-barter system in sea_ml namely mas, agent internal and agentsws interaction viewpoints which represent both the mas and sws aspect of the system. the diagrams representing the models in these viewpoints are shown in figures 2-4 respectively."
an action to transmit a message from an agent to another. this can be based on some standard such as fipa_contract_net receive an action to collect a message from an agent. this can be based on some standard such as fipa_contract_net task tasks are groups of actions which are constructing a plan in an agent.
"when the system is started to run on the client side, the initial interface is shown. on this interface, the user can add a service agent that represents a semantic web service or a customer agent which represents himself/herself to propose a bargaining."
"the serviceagent agents represent the e-bartering swss in the system. they interact with the matchmaker agent to register, update, and un-register the swss used in the system."
"autonomous, reactive, and proactive agents have social ability and can interact with other agents and humans to solve their problems. to perform their tasks and interact with each other, intelligent agents constitute systems called multi-agent systems (mass) [cit] . in addition, autonomous agents can evaluate semantic data and collaborate with semantically defined entities of the semantic web, such as semantic web services (sws), by using content languages [cit] . the implementation of agent systems is naturally a complex task when considering their characteristics. the internal agent behavior model and any interaction within the agent organizations become even more complex and hard to implement when the requirements and the interactions for other agent environments such as the semantic web [cit] are considered. therefore, it is natural that methodologies are being applied to master the problem of defining such complex systems. one of the possible alternatives is represented by domain-specific modeling languages (dsmls) [cit] that have notations and constructs tailored towards an application domain (e.g., mas). dsmls raise the abstraction level, expressiveness, and ease of use."
"upon completion of model transformations, the developers have two options at this stage: (1) they may directly continue the development process with code generation for the achieved platform independent mas models or (2) if they need, they can visually modify the achieved target models to elaborate or customize them, which can lead to gain more accurate software codes in the next step, code generation. in either case, the achieved result of this step are platform-specific system models for jack platform, owl-s and owl instances."
jack is a bdi oriented mas development language providing a framework in java. it is a thirdgeneration agent platform building on the experiences of the procedural reasoning system (prs) [cit] and distributed multi-agent reasoning system (dmars) [cit] . jack is one of the mas platforms that uses the bdi software model and provides its own java-based plan language and graphical planning tools.
"semantic web agent in the sea_ml stands for each agent which is a member of semantic web-enabled mas. it is an autonomous entity which can interact with both the other agents and the semantic web services, within the environment."
"in the generation procedure, a structure is generated from the system model for each ontology. these structures are extended to develop the complete ontologies for the services. part of the ontology developed for the food service is depicted in figure 7 . in this ontology, there are 4 product categories under the basic food node. these are the fruit, diary, vegetable and meat categories. these categories are divided into subcategories within themselves to obtain a tree structure in which the closeness relation can be established semantically. the services used in e-barter system also need to have semantic web service documents to provide semantic web service functionality. for this purpose, the draft documents produced through sea_ml have been used. listing 4 shows the draft profile.owl document produced by sea_ml for the food service. the document generated by sea_ml basically provides a draft of the resources imported with the owl: ontology tag in the rdf:rdf tag (listing 4-lines 16-25). this draft document was modified by the system developer in accordance with the ontology and hence the complete profile.owl document (see listing 5) was obtained. in this document, the profile tag, which is the label we used to determine the semantic proximity, was added to the draft to obtain the document in listing 5 (lines 18-39). within the profile:profile tag, the methods provided by that service are addressed which are based on the top-level concepts of the food ontology."
"an agent's general interaction and bartering processes are modeled with using sea_ml based on the performed analysis discussed above. in this step, we evaluate the swa agent instances in the mas and then model the internal structure of each agent using agent internal viewpoint. it is worth noting that, the instance models and the instances of specific elements (such as swa) are related to example models conforming to the sea_ml meta-model within the model-driven approach. after that, we model the interactions of these agents with services, using the internal components of the semantic web services of agent-sws interaction viewpoint."
"in this document, it is described how an agent interact with the sws. a grounding provides the needed details about transport protocols. instances of the class service have a supports property referring to a service grounding."
"this concept refers to certain conditions in which agents are present at certain times. an agent can only have one state (agent state) at a time, e.g., waiting state in which the agent is passive and waiting for another agent or resource."
"it is important to note that, although all above mentioned steps are supported by sea_ml to be done automatically, at any stage the developer may intervene in this development process if he/she wishes to elaborate or customize the achieved agents and artifacts."
"as another future study, our aim is to provide the execution of sea_ml agents on jason platform [cit] . similar to operational semantics of sea_ml currently provided for jack, mas models prepared in sea_ml can be transformed into jason model instances and code generation can also be possible for this platform. recently, we derived a metamodel [cit] for jason agents and we plan to use this metamodel as another psmm and prepare a series of m2m transformations between sea_ml metamodel and this metamodel which will lead to generation of jason agents inside the mas development methodology proposed in this paper. table a1 . sea_ml concepts, their notations and descriptions."
"in accordance with the sea_ml-based mdd methodology, we start by creating system models based on different viewpoints. the information needed for designing these models is gathered with the appropriate requirements engineering within the domain of the e-barter system."
"the following subsections discuss the details of analysis, design, generation, and implementation of e-barter case study using the detailed methodology proposed in this study and benefiting from sea_ml platform."
"the codes containing the negotiation logic of the customer agents are critical to the system. these codes are located mostly in the plans of the agents, such as proposaleval, negotiationproposal, and performnegotiation, where most of the delta codes are added. in listing 3, the delta code for the reasoning method of proposaleval plan that allows an agent to evaluate proposals, is shown."
"in this study, the proposed multi-agent e-barter system is implemented using the jack platform [cit] . jack is selected as it is one of the widely accepted java-based bdi mas development platforms. also, it is as a mature and robust commercial product and meets the appropriate needs for industry adoption, such as scalability and integration."
"the work conducted in this study is mainly related with two research fields: mas development methodologies and e-barter systems. hence, in the following, we first discuss the efforts given on deriving mas development methodologies as similar to our proposal and then give some noteworthy studies on developing e-barter systems which especially consider employing agents."
"in this subsection, we discuss the analysis and the design of the agent-based e-barter system. system analysis is realized by specifying bartering elements using agents and their components in sea_ml, while system design is realized by providing diagrams of different viewpoints of the system using sea_ml."
"according to omg's well-known model-driven architecture (mda) [cit], sea_ml metamodel can be considered as a platform independent metamodel (pimm) and jack and owl-s metamodels can be considered as platform-specific metamodels (psmm). the model transformations between these pimms and psmms pave the way for the mdd of the semantic web-enabled mass. these transformations are implemented using atl language [cit] to produce the intermediate models which enable the generation of architecture code for the agents and sws documents. an agent developer does not need to know both the details of these transformations written in atl and the underlying model transformation mechanism. following the creation of models in the previous modeling steps, the only thing requested from a developer is to initiate the execution of these transformations via the interface provided by sea_ml's graphical user interface (gui)."
"although the codes generated for the mas can be executed directly in the jack environment, additional codes should be added into these generated codes, called delta code, by the developer to have the fully functional system. the generated codes in the target language environment according to the proposed mas development methodology, the generated code can be modified in the target language environment, jack editor in the case of our study. jack environment has a built-in graphical user interface that represents classes and their relations. after model-to-text transformations, jack java classes are produced for the customer agent from intermediate models. apart from creating a java class for the customer agent, separate java classes are generated for this agent's capabilities, plans, events, and beliefs. part of the generated codes demonstrated in the jack editor is depicted in figures 5 and 6 . in the generated codes, the customer agent has two capabilities, namely bartering and searching. there are separate java classes produced for these capabilities which are interlinked to the generated architecture code. also, the java classes that are generated for each capability of the agent, are linked to the plan, event, and belief classes that this capability requires. as it is shown in figure 5, the searching capability has two plans. one of them is asktomatchmaker plan. this plan handles seedrequest event and sends matchrequest event to the matchmaker agent for getting suitable services list. the handlematch plan handles match event which is sent from the matchmaker agent. the match event encapsulates a service list. if the service list is null, it means that there is no suitable service for matchrequest sent from customer agent. otherwise, the handlematch plan try to get appropriate agents from the services for bartering. if the customer agent finds suitable agents using the handlematch plan, it will start negotiation with these agents, if not, it will send a request to matchmaker agent to register itself in a suitable service."
"bartering capability (see figure 6 ) is responsible for the negotiations between the agents. when startnegotiating event is posted, the performnegotiation plan is executed to handle it. negotiationproposal event is created and sent to the relevant customer agent. proposaleval plan is responsible for evaluating incoming proposals and responding them. if the answer of the proposal is positive, the negotiationfinalize plan is used to finalize the negotiation between two agents."
"in this paper, a development methodology is proposed for development of mass working in semantic web environments. this methodology is based on a dsml, called sea_ml. the study is demonstrated using a case study for e-barter. to this end, the bdi agents and the swss for the ebarter system are analyzed, designed and developed using different viewpoints and features of sea_ml. also, a demonstration scenario is provided for the implemented system."
"wautelet and kolp [cit] investigate how a model-driven framework can be constructed to develop agent-oriented software by proposing strategic, tactical and operational views. within this context, they introduced a strategic services model in which strategic agent services can be modeled and then transformed into the dependencies modeled according to the well-known i* early phase system modeling language [cit] for a problem domain. in addition, generated i* dependencies can be converted to bdi agents to be executable on appropriate agent platforms such as jack. however, implementation of the required transformations and code generation are not included in the study."
"sea_ml's metamodel is divided into eight viewpoints, each of which represents a different aspect for developing semantic web-enabled mass. agent's internal viewpoint is related to the internal structures of semantic web agents (swas) and defines entities and their relations required for the construction of agents. interaction viewpoint expresses the interactions and the communications in a mas by taking messages and message sequences into account. mas viewpoint solely deals with the construction of a mas as a whole. it includes the main blocks which compose the complex system as an organization. role viewpoint delves into the complex controlling structure of the agents and addresses role types. environmental viewpoint describes the use of resources and interaction between agents with their surroundings. plan viewpoint deals with an agent plan's internal structure, which is composed of tasks and atomic elements such as actions. ontology viewpoint addresses the ontological concepts which constitute agent's knowledgebase (such as belief and fact). agent-sws interaction viewpoint defines the interaction of agents with sws including the definition of entities and relations for service discovery, agreement, and execution. a swa executes the semantic service finder plan (ss_finderplan) to discover the appropriate services with the help of a special type of agent called ssmatchmakeragent who executes the service registration plan (ss_registerplan) for registering the new sws for the agents. after finding the necessary service, one swa executes an agreement plan (ss_agreementplan) to negotiate with the service. after negotiation, a plan for service execution (ss_executorplan) is applied for invoking the service. table a lists the important sea_ml concepts (meta-entities) and their brief descriptions for the comprehension of the corresponding visual notations used in the diagrams throughout this paper."
"cavalli and maag [cit] have developed an approach and its supporting tool that generates test scenarios suitable for the e-barter system. these scenarios are intended to test the compatibility of the system with the intended functions. system specs were implemented using the specification and description language (sdl) [cit] . with this method, design mistakes are prevented in the early stages of development. however, test case generation is not the aim of our study."
"customer agents represent the end users in the e-barter system. this agent receives the user's offer and purchase items and interacts with matchmaker and other customers to realize bartering. at the first stage, this agent interacts with matchmaker to find out if there is a proper service containing candidate customers. in case of success, it receives the service addresses and interacts with those services to get the list of suitable customers. these services contain ontologically close customers with our customer needs. in case of failure, the matchmaker simply registers the customer into the proper service. a customer agent, having the list of candidate customers for bartering in hand, starts to negotiate with them one by one to make an agreement and realize bartering."
"on the other hand, there are some studies in literature addressing the development of e-barter systems with different approaches. generally, these studies aim at formalizing the domain and increasing its effectiveness."
"after the services are registered in the system, customer agents can be included in the system. for this purpose, \"adding customer agent\" interface (an example shown in figure 9 ) needs to be launched from the initial user interface. using this interface, the agent is created after the information for the customer agent is entered. to instantiate a customer agent, the product which is needed and offered as well as the lower and upper limits must be prescribed by the user for the bartering. in this scenario, three agents named customer a, customer b, and customer c, are included to the system with the details (such as name, need, offer, lower and upper limits) provided in table 1 . figure 9 shows, as an example, the instantiation of \"customer c\" agent in the system. customer a agent would like to barter with some other agents who offer netbooks. meanwhile, this agent offers coconuts for netbooks. to achieve this, customer a creates a barter request. this barter request is sent to the matchmaker agent which uses each service's own ontology to send the customer a list of the most appropriate semantic web services. in this way, the agent gets semantically close services that accommodate the agents providing the needed product. however, till this point of the scenario, no agent has been introduced to the system before, so if customer a agent finds appropriate services, these services will not return any agents that customer a can barter. therefore, customer a will be registered to an appropriate service and wait for the new agents to be added to the system."
it describes how the sws is used by defining a process model. instances of the sws use the process via described_by to refer to the service's servicemodel.
"the application of model-driven development (mdd) and use of dsmls for mas development emerged in agent-oriented software engineering (aose) research field especially for the last decade [cit] . researchers developed various metamodels (e.g., [cit] ) and dsmls (e.g., [cit] ) to cope with the challenges encountered on design and implementation of mass. moreover, some fully fledged dsmls (e.g., [cit] ) exist for developing software agents especially working in semantic web environments where agents can handle the semantic web content on behalf of their human users and interact with other semantic web environment entities, such as sws. one of these mas dsmls is semantic web enabled agent modeling language (sea_ml) [cit] which has a built-in support for the modeling interactions of agent and semantic web services by including several specialized viewpoints. sea_ml aims to enable domain experts to model their own mass on the semantic web without considering the limitations of using existing mas development frameworks (e.g., jade [cit], jadex [cit] or jack [cit] ). the evaluations [cit], conducted for the assessment of sea_ml, show promising results considering the generation performance and the development time reduction during mas design and implementation. according to the experiences gained from the multi-case study [cit] conducted by using sea_ml, the developers can benefit more from this dsml when they use different viewpoints of sea_ml in a proper way in the development of mas. therefore, in this study, we propose a model-driven mas development methodology which is based on an extended version of sea_ml and covers the whole process of analysis, modeling, code generation and fully implementation of a mas working in the semantic web according to the well-known belief-desireintention (bdi) [cit] agent principles. the use of the new sea_ml-based mas development methodology is exemplified with the development of a semantic web-enabled mas for electronic bartering (e-barter)."
"owl-s (semantic markup for web services) enables the discovery, invocation, interoperation, composition, and verification of services. it builds on the formerly developed daml-s [cit] and was the first submission for describing sws submitted to the w3c. each sws in owl-s consists of a service profile, a service model, and a grounding. the service profile describes what the service does and is used to advertise the service. the service model answers the question \"how it is used?\" and describes how the service works internally. finally, the service grounding specifies how to access the service. owl-s is based on the web ontology language owl [cit] and supplies web service providers with a core set of markup language constructs for describing the properties and capabilities of their web services in an unambiguous, computer-interpretable form."
"the next step in the mas development methodology based on sea_ml is the automatic model transformations. the models created in the previous step need to be transformed from platform independent level into the platform-specific level, e.g., to the jack and owl-s models as in the case of this study. these transformations are called m2m transformations."
"based on the proposed methodology, the development of a semantic web-enabled mas starts with the analysis of the system by considering the mas viewpoint of sea_ml (see figure 1 ). this viewpoint includes mas elements such as organizations, environments, agents, and their roles. the viewpoint provides the eagle-view of the system and shapes the high-level structure of the system. the result is a partial platform independent instance model of the system covering the analysis phase of the system development and providing a preliminary sketch of the system."
"when the system's agents are determined in mas viewpoint, the internal structure of each semantic web agent is modelled. the instance model should cover all the required roles, behaviors, plans, beliefs, and goals of an agent. figure 3, illustrates an instance model of the agent internal viewpoint for a customer agent. customer agent has two capabilities called searchcapability and barteringcapability. the searchcapability includes its goals (\"servicematching\" and \"agentregisteration\"), belief (\"barter_goods\"), and plans (\"askmatchmaker\" and \"handlematch\"). the barteringcapability includes its own goal (\"aggrementnegotiation\"), belief (\"barter_goods\"), and plans (\"performnegotiation\", \"proposaleval\" and \"negatiationfinalize\"). when considering the beliefs, the agent uses them to know which goods it has, and which goods it needs. therefore, the agent decides what to offer and what to require for in the bartering process. also, the agent could play searching and bartering roles. the searching role could realize its task over \"finding services\" behavior by calling the askmatchmaker plan. if this plan is executed successfully, \"getting agent names from services\" behavior is performed with the \"handlematch\" plan. otherwise, the agent performs \"sending register request to matchmaker\" behavior. the bartering role realizes all behaviors associated with the bartering transaction. the bartering transaction is carried out among the customer agents and the \"bartering role\" covers all these process behaviors which are realized by relevant plans. figure 4 shows the instance model which includes semantic services and the required plan instances of the agent-sws interaction viewpoint. the instance model contains all the plans for discovering, negotiating with and executing the candidate services. customer and matchmaker agents are modeled with relevant plan instances to find, make the agreement with, and execute the services which are the instances of the ss_finderplan, ss_agreementplan, and ss_executorplan, respectively. the services could also be modeled for interaction between the sws's internal components (such as process, grounding, and interface), and the swa's plans."
"in this scenario, after customer a, customer b and customer c agents are involved in the system. the requirements of the customer b are the same as the customer a, except for the lower and upper limits that the customer a determines for bartering. in this case, customer b would register to a suitable service because it will not find a suitable agent for bartering. the customer c agent offers \"netbook\" and requests \"coconut\", unlike customer a and customer b. therefore, a semantic web service, which is found by the matchmaker agent, sends suitable agents for bartering to customer c. in this case, customer a and customer b are the suitable agents for customer c."
"first, service agents, that represent semantic web services, should be added to the system. when the \"add service agent\" in the initial user interface is selected, the interface for adding a service agent appears (see figure 8 ) to get the information necessary for registering the semantic web service. then an agent is run to represent the relevant semantic web service and sends a message to matchmaker agent to register the semantic web service. in our demonstration scenario, there are two semantic web services called \"food service\" and \"electronic device\". for each one, a separate service agent must be established."
the scalar field defined by the function g has the same zero level set as the function f . it comes from the requirements to the function h:
the kernel function w should take its maximum at 0 and smoothly converge to 0 as its argument goes to infinity with any of its coordinates. there are various ways to define such functions; examples are the gaussian function and the bump function.
"one problem of the distance function is that it can lack c 1 continuity, which results in creases when further operations, such as blending, are applied to the distance field. gradient-based blending [gbc*13] also relies on c 1 continuity. most approximate methods provide c 1 continuity, but for some applications, such as sourcebased material interpolation, the exact surface needs to be preserved and simultaneously c 1 continuity away from the surface is desired. a smooth approximation of the distance function can be obtained by taking the convolution with some smooth function (see figure 1) . the convolution is then numerically evaluated. in order to keep the method efficient, an efficient evaluation of the signed distance field is used."
"where we use the unsigned distance in the integrand, and compute the sign of the function at the end. while this approach solves the problem of the extra zero level sets, it does not prevent additional extrema (with respect to the exact signed distance function)."
"in this paper, we deal with exact distance fields and therefore consider only methods for representing the input polygonal mesh exactly. these methods can be separated into two categories: r methods calculating the iso-value of the scalar field using prebuilt data structures over the initial geometric data."
"to efficiently and accurately evaluate (2) numerically the samples should be distributed inside some volume. equation (2) suggests sampling in the entire space, at least for the gaussian kernel, while for the bump function we only need to sample into a finite volume."
"evaluation of the sign can be done by different approaches. one of the fastest yet reliable methods is the angle-weighted pseudo normals method introduced in [ba05] . this method requires the input polygonal mesh to be watertight, i.e. free from holes and self-intersecting triangles. ray-casting is another common solution [req96], however it is at best o(log n) and requires robust raysurface intersection procedures to avoid numerical errors. slower, but more robust solutions exist for non-watertight meshes, for example, by computing the winding number [jksh13] . in our experiments, we only considered watertight meshes and therefore used only the angle-weighted pseudo normals approach. note that the sign computation generally does not depend on the distance computation and the distance can be evaluated for both watertight and non-watertight meshes, while the methods used for sign computation directly depends on the quality of the input mesh."
"alternatively, we can achieve a similar result when a bump function (with compact support) is used, by setting its parameters appropriately. we remark that the capping distance f c and the width of the bump function b are related if we want to prevent additional zero level sets. if b is set, then f c has a lower bound depending on b in order to prevent the filtering region to cross the surface boundary (where the sign of f changes), and therefore create additional zero level sets. similarly, a given f c implies an upper bound on b. since we want to avoid any non-zero weighted s to cross the surface, the following inequality must hold for all p:"
"the representation of polygonal meshes by discrete or continuous scalar fields has recently attracted a lot of attention in research as well as in application areas because of its properties. scalar fields can be used efficiently for such operations as controllable blending, metamorphosis between meshes with different topology, surface offsetting, robust mesh repair and others. applications include function-based heterogeneous object modelling and rendering, rapid prototyping and digital fabrication and simulation of different physical properties in medicine and geology (see [cosl98, fprj00, ju04, lw11, pk08, jbs06] and references therein)."
"while the iso-value of the scalar field is required to be zero on the polygonal mesh surface, it is not required to carry any geometric"
"methods of representing polygonal meshes with signed distance fields have been increasingly popular because of their numerous applications. these methods can be classified from different points of view into discrete and continuous distance fields, exact and approximate ones, signed or unsigned. in [jbs06], various methods are surveyed according to how the distance is evaluated and propagated across the discrete volume. below we discuss several methods for computing the signed distance to meshes as well as applications of these methods."
"in the general case, the integral defined in (2) cannot be evaluated analytically and therefore a numerical approximation is required. the convolution (2) can be approximated by the following finite summation:"
"information in the general case. however, we can distinguish cases where the (absolute) field value corresponds to the euclidean distance to the polygonal mesh. in this case, the scalar field is called a distance field. we distinguish between signed distance fields and unsigned distance fields. for unsigned distance fields, the iso-value is the distance (or sometimes squared distance) to the mesh and is positive everywhere but on the initial mesh surface. on the other hand, with signed distance fields, the sign is defined by the position of the evaluated point relatively to the interior or the exterior of the mesh object."
"metamorphosis also benefits from c 1 -continuous fields. c 1 discontinuities introduce unwanted creases during the transformation. the metamorphosis is a weighted sum of the two fields in its simplest case, but other methods also rely on some summation of the two fields. during intermediate frames, the surface will pass through the c 1 discontinuities present in each field. this causes creases on the surface. using filtered distance fields, the intermediate shapes look smoother and only have creases as they get closer to one of the interpolated shapes. figure 18 shows intermediate shapes of the metamorphosis between a fan disk and a mechanical part with sharp features (a and b), using signed distances (c and d) and our method based on filters (e and f). the metamorphosis here is achieved using a simple linear interpolation between the values of each field."
"methods for representing meshes using scalar fields can be distinguished as exact and approximate ones. for exact methods, the iso-value is guaranteed to be zero only on the initial mesh surface, meaning that all the features of the initial models are preserved by the scalar field representation. for approximate methods, such as radial-basis functions (rbfs) [yt02, myc*01], multi-level partition of unity (mpu) [oba*03] or moving least squares (mls) [sos04] some given approximation error is allowed. exact and approximate methods are both suitable for visualization, animation, reconstruction and other purposes. however, in some applications, such as medical simulations, approximation errors are not allowed and therefore approximate methods cannot be used."
"results obtained with these different approaches are illustrated in figure 10 . the regular uniform filter has visible c 1 discontinuities just like the signed distance field. the gaussian distributed samples provide visually smoother fields. in figure 11, we show how the number of samples affects the quality of the result. in practice, numerical filters provide good results while preserving the efficiency of the method."
"in figure 17, a gear model is subtracted from a sphere using a blending difference. the c 1 discontinuities of the signed distance fields create visible edges in the model as seen in figure 17(b) . using convolution filters on signed distance fields, the sharp edges are smoothed out."
"one needs to compute a scalar field to a polygonal mesh in order to apply some specific function-based operations to the scalar field or to use it within a general function-based modelling environment. possible methods, as it can be evaluated very efficiently for any polygonal mesh. the main concern with the euclidean signed distance field is that it is generally non-differentiable (c 1 -discontinuous). we proposed to smooth the distance field by taking its convolution product with a smooth kernel. the integral is numerically evaluated as a finite summation. the introduced filtering procedure for multiple coherent queries was used for smoothing the distance field in order to efficiently compute a smooth field. several experiments show that the resulting field is smoother and therefore is more suitable for operations such as blending, metamorphosis and other general modelling operations in the context of a function representation modelling system. the proposed approach relies on the efficient evaluation of the signed euclidean distance. in our approach, we used a bvh (bounded volume hierarchy) structure to achieve maximum performance. in addition, we benefit from packet sampling as most of the points where the distance should be evaluated lie in the neighbourhood of the query point. however, our approach would benefit from more efficient evaluations of the signed euclidean distance, which is still an open research as several methods have been introduced recently. we reviewed most of them in [sfp13], but a more detailed survey of the current state of the art has yet to be done. one of the ways to increase the efficiency is the acceleration by using graphics hardware (gpu) implementation. however, it relies on the size of the input mesh and does not handle large input meshes well."
"filters can be used to smooth the shape selectively. to achieve localized smoothing, two c 1 -continuous functions are needed. the function f o represents the original object, while the function f s represents the smoothing volume. the function h in (9) is replaced by a function which converts the values from f s into a filter size value. here, we use a smooth step function to remap the values from f s to a kernel size value. for any point outside the smoothing volume, we do not need any filtering and therefore keep the value f o (p). if the point is within the volume f s, then, convolution filtering can be applied by replacing h(p) in (9) by:"
"for practical reasons, when (9) is used for the approximation, we limit ourselves to a finite volume (called a unit volume) near the evaluation point. two obvious ways to define a unit volume are: a unit sphere centred at the query point and a unit cube. for efficiency purposes, we used a unit cube in our experiments. therefore, all the samples are distributed inside a unit cube and defined by their position and weight. it is clear that the more samples we have, the closer the approximation to the integral is, but at the same time the less efficient the method is, so a balance has to be found. to distribute the samples inside the unit cube, different approaches can be used. the most naive solution, which is largely used in discrete filters, is a regular pattern of a 3 3 grid where each sample point has a weight given by the function w. however, our experiments show that the resulting field still has creases even if we increase the number of samples in the regular grid. instead we experimentally found out that it is more efficient to draw samples following the function w. in this case, more samples are drawn where the weights are larger."
"as mentioned above, the number of samples influence the quality of the result as well as the efficiency. to achieve good quality without sacrificing computational efficiency, the number of samples is adaptively changed across space. from our experiments, a low number of samples is able to approximate the convolution reasonably well far from the surface or the medial axis. therefore, we suggest the number of samples to be adaptively increased only around the surface and the medial axis."
"c 1 -continuous fields are crucial for heterogeneous multi-material modelling to avoid stress concentrations. we apply transfinite interpolation as described in [rsvt00] using our filtered field to achieve better results than with exact signed distance functions, and with more control and faster computations than l p −dist fields [bfp13] . the transfinite interpolation uses two (or more) source features defining different materials. the material properties in-between the features are interpolated across space blending both properties based on the distances to the surface boundary of each feature. the formulation relies on distance properties, but the c 1 discontinuities cause stress concentrations and other issues due to the loss of differential properties as discussed in [bst04] . figure 19 shows the transfinite interpolation using different method for computing an approximation of the distance fields. the white and grey stripes represent the source feature shapes, and the colours define the distribution of each material. using exact distance fields, figure 19 . this results in additional zero level sets and bad behaviour of the field. overall, the filters succeeded to create c 1 -continuous material blending. the maximum filter region can be adjusted and the capping distance f c lets the user control how close to the surface the smoothing should happen."
"polygonal meshes are the most common representation of object geometry in computer graphics. the mesh allows to represent geometry in a format easy to understand and convenient to modify. on the other hand, this format restricts the number of operations on geometric objects or does not allow to perform some of these operations easily. for example, the metamorphosis between two meshes can be hard to implement if these meshes have different genus. therefore, an alternative representation of the mesh object is sometimes needed to provide greater flexibility and a larger spectrum of available operations such as, for example, implicit skinning and gradient-based blending [vbg*13, gbc*13]."
"another direction of future work is related to the convolution itself. convolution proved to be useful to smooth the field. convolution with varying kernel size across space have other potential applications which have yet to be investigated. in this paper, we used a numerical method based on a monte-carlo approach to approximate the integral. other numerical techniques can be investigated to evaluate this integration, such as for example the fast"
"naive computation of the distance from a point to the mesh is an expensive procedure. however, it can be accelerated by using spatial structures, pre-processing and sorting the mesh polygons, application of different traversal strategies for the selected spatial structure and using hardware acceleration. in [sfp13], different spatial structures, building and traversal strategies are discussed in details and compared to present the optimal way to calculate the distance function to a polygonal mesh. this work also discusses how to improve the performance of a single query of the distance by using packet sampling. often the function has to be sampled several times within a small volume. in ray-tracing, a similar problem occurs and is referred to as coherent rays processing. the related process is called packet sampling. each sample is likely to go through the same branches of the tree, and it is inefficient to iteratively do each query separately. instead, all the sample points are grouped and the tree is traversed once for all, which improves cache efficiency."
"the signed distance to a polygonal mesh can be computed from the heat flow [cww13] or by anisotropic diffusion [cdr00] . these methods use numerical solutions of partial differential equations and therefore require discretization with either a regular grid or a volumetric mesh resulting in inaccuracies. table 1 compares different methods for computing a distance field to a polygonal mesh. most of the methods have linear (time) complexity with respect to the number of triangles in the input mesh. in addition, most of these methods are performing expensive operations per triangle or per vertex, making them slow in practice for large meshes."
"it is possible to get better results by sampling, for example, from the distribution with density w (assuming that w is normalized). in this case, an approximation of (2) is obtained by"
"the parameter f c controls the distance to the boundary outside of which g is guaranteed to be smooth. it needs to be small enough to exclude points where f is not smooth, or taken as small as possible otherwise. trade-off between how close g approximates the distance function f and the shape of the level sets of g. it should be selected based on the type of applications."
where s d is the distance from the boundary of the smoothing object at which the kernel size will reach its maximum value and s r is the smoothing radius (i.e. the maximum kernel size). that a smooth handle can be made from a basic shape following the same procedure.
"approximate signed distance fields were presented in [wk03] where a piecewise linear approximation of the signed distance function was used. in [cosl98], the distance function was interpolated for a mesh with planar cross-sections. the signed distance field can be approximated from its values given at the nodes of a voxel grid, as first introduced in [rp66] . some discrete approximation methods sample the signed distance or an approximation at the nodes of an octree grid [fprj00, ju04] . an approximation of the distance field to a point cloud was presented in [ct11] . an approximation to the signed distance to noisy point cloud data is discussed in [mdgd*10]. a physics-based level set method can be used as well as in [zo02] . approximate methods can be very fast to evaluate the signed distance value, however they are inaccurate and cannot provide a continuous real function without a proper interpolation procedure and therefore cannot be used in a number of applications."
"the quality of the result depends on the number of samples, their distribution and weights. because most of the samples are likely to be in the same neighbourhood, packet sampling can be used to evaluate efficiently all the samples at once. we discuss below further details for the filter evaluation."
"a continuous approximation of the signed distance to a polygonal mesh can be obtained by representing the object with set-theoretic operations on the half-spaces bounded by the planes passing through the polygonal faces [fpa11] . this method provides a continuous function, but the distance query can be slow and numerically unstable especially for large input meshes."
"the quality of filtering can be evaluated visually by applying operations on these fields. in figure 13, we show how filtering affects the gradient field. figures 13(b) and (d) illustrate the results obtained with the exact signed distance function and our method when an edge detection filter is applied to the gradient field. the edge detection filter uses 3 3 samples with the weights all set to −1 except for the central sample which is set to n − 1, where n is the number of samples. we remark that the distance field contains several clear discontinuity of the gradient, while our method removes most of them. convolution filters have several applications in shape modelling. in figure 1, our method is applied to compute a smooth offset from the stanford bunny. an alternative approach for computing an offset could be to adapt the recent method for computing pointset morphology [cb14] . in the following, we describe additional applications of convolution filtering."
"simple blending with addition or subtraction of material for smooth transition between two objects was introduced in [pass95] . the main idea is to apply an r-function defining a set-theoretic operation between two objects and apply some displacement to the resulting value. the result of the blending operation is the smooth transition between the initial surfaces. c 1 continuity is crucial for blending operations. when applied to exact distance functions, the additive blending shows a sharp edge crossing the otherwise smooth additional material. figure 16 shows the effect of c 1 discontinuity on blending union (c), and the result obtained using our method (e)."
"for this purpose, the approximation in (10) is used, where s i are sampled directly from the distribution w. when a gaussian is selected, the box-muller transform is used to sample from it. for the bump function, we use rejection sampling (a uniform distribution over [−b, b] 3 can be used as a proposal distribution). alternative approaches such as adaptive rejection sampling or importance sampling could be used as well. but it would not result in a significant difference, since the samples are only computed once during initialization."
the second category of exact methods includes optimized evaluations of the signed distance field (as used for example in figure 2a ). these are discussed in the next section.
", 1). note that this function does not depend on the sign of f . reducing the kernel size to zero on the mesh surface preserves the exact surface representation by g."
"in this work, we stay within the class of methods dealing with a continuous real function representing the signed euclidean distance to the polygonal mesh, which was first introduced in [pt92] . these methods have been increasingly popular because of their numerous applications, such as collision detection [gbf03], rendering [har96], intersection free mesh offset or shell [lw11, pk08] and heterogeneous object modelling [bst04] . surveys of applications can be found in [jbs06] . in the following section, we discuss different techniques for the efficient evaluation of signed distance fields."
"there are no known \"gold standard\" approaches to analyze data for n-of-1 trials. several approaches have been proposed and are well argued with simulated or trial data [cit] . however, issues of missing data and autocorrelation persist. many techniques to handle missing data are being developed for both observational and interventional study designs [cit] . simulation studies can incorporate missingness but are difficult to model similarly to real-world data. autocorrelation presents an additional complication when analyzing n-of-1 data. autocorrelation implies a serial dependency within the data collected from the same individual over time-i.e., patient's stress level today may be highly correlated with their stress yesterday and tomorrow. thus, it is important to check for autocorrelation in data from n-of-1 trials and address it if detected. traditional tests of difference such as the t-test are not appropriate for analyzing data from n-of-1 trials because they violate key assumptions of statistical independence. two common methods to address autocorrelation include an autoregressive model or a dynamic model, where autocorrelation is modelled as part of the analysis procedure [cit] . random effect parameters can be included to account for autocorrelation in multi-level or hierarchical models."
"recently, intraneural and epineural electrodes [cit] have been used for stimulating the peripheral nerves of patients with amputation in order to restore the sense of touch during the control of prostheses. conversely, the use of these interfaces for a natural and physiological control of the prosthesis is still very preliminary."
"four levels of force plus rest were discriminated with an accuracy of 52.4% with respect to 25% chance level, while three levels of velocity plus rest were discriminated with 37% accuracy, versus 20% chance level (fig. 7a, b) ."
"carryover effects are present in n-of-1 trials when the effect of a treatment cycle impacts subsequent cycles beyond any assigned washout periods. washout periods are pre-defined blocks of time in which treated individuals do not receive the treatment of interest to allow time for the effect of a treatment delivered in a previous period to wash out. it is possible that including blocks with varying lengths might best address this issue. future work might evaluate the number of \"control days\" could be randomly picked in a set of numbers between 1 and 3 following an \"intervention day.\" rather than equal time spent in control and treatment cycles, this randomization of block length potentially addresses the possibility of confounding effects from treatment carryover. stronger carryover effects may result in more conservative differences in treatment effects due to inflation of type-ii error [cit] . thorough reviews of pilot data or relevant literature should be implemented prior to trial design in order to determine the potential length of a carryover effect and inform the design of the n-of-1 study periods [cit] ."
"(2) where t* are the spike times occurred before t, and τ 1 and τ 2 are chosen so to have a rise time of 60 ms and an half-relaxation time of 80 ms [cit] . the amplitude a spike is not fixed but is larger for spikes fired by stronger motor units. more in detail, since motor units are recruited in order of contractile strength [cit], it results that if they are n at time t and then become n + m at time t + 1, each of the new m active units elicits a stronger force than the strongest of the \"old\" n active ones. in other words, force grows superlinearly with the number of fired spikes, i.e. with the overall firing rate. starting from these considerations, the superlinear relationship between the amplitude of the global force exerted by the muscles and the overall motoneuron firing rate was defined as:"
"finally, starting from the seminal works of fuglevand and colleagues [cit], whose modeling studies provided many insights about peripheral motor control [cit], we used a spiking neuron network model of the spinal circuits to evaluate the influence of the lack of sensory feedback (as in amputees) on the neural features used in the decoding procedure."
"in the continuation we explain how this algorithm was developed. force and velocity levels were first identified as \"rest\" or \"activity\" according to two features (feat. 2 and 3, table 1 ) defined on the trespassing of two thresholds on the signal (one for rest and one for activity). in the \"activity\" state, our classifier predicted the activity level (from 1 to 4 in the case of force, from 1 to 3 in the case of velocity) according to the least euclidean distance between a feature, i.e. feat 1, and feat 1 centroids of the different classes. such centroids were computed on a training set folded by a leave-one-out strategy. feat. 1 was"
"in addition to expanding the analytical approaches to quantifying carryover effects (i.e., slow vs. long), n-of-1 studies offer a unique advantage: they can often provide their own \"pilot data\" through initial phases prior to data aggregation or meta-analysis. researchers with little prior information on expected carryover effects for a phenomenon could gather pilot data before conducting a full trial."
"motoneuron activity is peripherally modulated by proprioceptive feedback [cit] . to assess the transferability of our findings to amputees (i.e. to subjects without proprioceptive feedback), we analyzed to which extent our characterization of motoneuron activity holds in absence of proprioceptive feedbacks during voluntary hand movements, by means of a simplified spiking neuron model of a local neuromuscular circuit (fig. 3) . the model was calibrated on our experimental findings (\"results\")."
"the movement/grasp repetitions in which (i) the subjects had executed the task correctly and (ii) the mng presented a snr higher than 1 were included, in the motoneurons behavioral analysis. the snr was defined as it follows:"
"the n-of-1 trial design is increasingly popular among healthcare researchers and clinicians. n-of-1 trials are increasingly proposed as an alternative to randomized controlled trials (rcts) [cit] . n-of-1 trials are particularly useful for evaluating treatment of chronic stable conditions such as attention deficit hyperactivity disorder (adhd), chronic pain, and many other chronic stable conditions [cit] . such designs can be implemented for a variety of populations (from children to the elderly [cit] ), settings (developed and developing countries [cit] ), demographics (races and ethnicities [cit] ), and indications (e.g., targeted efforts to deprescribe excessive medication [cit] ). taken together, n-of-1 trials could represent the next advance in personalized medicine given their ease of integration with health technology data [cit] ."
"microneurography is a minimally invasive technique that records nerve signals through needles inserted percutaneously. it has been extensively used to characterize afferent signals [cit] . we hypothesized that it could be also a valuable tool to investigate motoneuron behavior, and to propose in the future novel decoding strategies to employ in prosthesis control driven by nerve recordings acquired through implanted intraneural electrodes. therefore, we performed ultrasound-guided microneurography [cit] to record the putative firing activity of motoneurons, generated during voluntary finger movements."
"we proved then that specific features of neural activity recorded by microneurography could be used to decode the velocity of motion and force of grasping, performing better than standard approaches. our results are direct evidence that more information, exploitable for the direct control of neuroprosthetic devices, is available in peripheral nerve recordings with respect to what previously exploited for neural signal decoding [cit] . in fact, starting from a single-channel recording, we achieved a novel neuralbased decoding of movement parameters such as force and velocity. we found that force prediction was more efficient than velocity one. we also found that the cumulative activity of neurons is more efficient in the prediction of the force (or velocity) than singleunits. in this light, this work made already an important step toward the development of decoding strategies based on peripheral efferent nerve recordings, which could contribute to improve in the future the controllability of bidirectional prosthetic devices [cit] ."
"we tested if velocity and force of finger voluntary movements could be predicted, by means of an algorithm that relied on the use of the features of motoneuron discharge, we extracted from our neural recordings."
"here, we described a protocol, which starting from microneurographic recordings of the peripheral efferent neural signals allowed to use these signals to potentially design decoding algorithms for the control of hand prostheses. unlike previous works that attempted to record efferent signals by microneurography [cit], we characterized the relationship between neural activity and movement features relevant for hand prosthesis applications. in particular, in isotonic force tasks in which the subject was asked to exert and hold a specific level of force over an object, we showed that both overshoot and plateau level of the firing rate were sublinearly correlated with the exerted level of force. this happened not only during force holding, in agreement with previous results [cit] but also during force application. moreover, we recorded, for the first time to the best of our knowledge, efferent firing rate during isokinetic movements of single fingers. in this case, peak firing rate was independent from velocity, which affected only the time-to-peak. this is due to the fact that the motoneuron firing rate correlates with the force generated by the correspondent motor units, as demonstrated by previous studies [cit] and confirmed by our spinal networks model (fig. 9a) ."
"translating this result to the control of artificial limbs would represent an important breakthrough for the neuroprosthetic field, but we needed to ensure that this translation is possible, in terms of difference of data in amputees w.r.t. healthy subjects, and in terms of recording ability of the chronic devices w.r.t. acute microneurography. hence, starting from our experimental data, we developed and calibrated computational models of the recording electrode-nerve interface and proprioceptive reflex pathway spiking neurons, which enabled us to assess whether our results could be translated to amputees for whom recording procedures are different and there is no muscular feedback."
"methodologists have also raised issues with trial design and analysis techniques [cit] . repeated observations of outcomes across multiple treatment and control periods is required from the participant in an n-of-1 trial, in order to provide sufficient statistical power to detect true treatment effects at the individual level. the need for repeated observations can lead to statistical issues which can be addressed through careful statistical analysis. selection bias, power, sample size, design and method of data analysis are important considerations for any clinical trial."
"we implemented two types of feedbacks acting on motoneuron activity: a movement feedback (mf) dependent on fingers movement and a force feedback (ff) depending on muscle force, to emulate those brought to motoneurons by ia, ii, ib afferent fibers [cit] . coherently with previous literature we set mf excitatory and ff inhibitory [cit] ."
"this whole geometry was built, meshed and solved in comsol 5.0.2 multiphysics. the validation of the model is described in additional file 1: methods and fig. s1 ."
"n-of-1 trial designs offer a rigorous method for advancing personalized medicine and healthcare while minimizing costs and resources, provided treatments are ethically designed for chronic stable conditions with sufficient washout periods. we performed a simulation study to examine the operating characteristics of n-of-1 designs compared to parallel rct and crossover designs. our results follow closely from previous theoretical results which demonstrated simulated sample sizes required to achieve a given power were lower for n-of-1 trials than for parallel rcts, as the number of cycles administered increased [cit] . we validated this theoretical result through simulations and showed that n-of-1 operating characteristics were superior to those of crossover designs. we also examined the effects of carryover and sample representativeness on the operating characteristics of each of the three trial designs. carryover effects can result in inflated type i error probabilities for both crossover and especially n-of-1 designs, but when treatment effects are present, n-of-1 designs have better power than both crossover and parallel rct designs, regardless of carryover effect size. when trial samples are not fully representative of the target population, all three trial designs make incorrect conclusions about the treatment effects in the target population, but n-of-1 designs make incorrect conclusions far more often due to increased numbers of observations on each patient. however, due to multiple observations taken on each patient under different treatment periods, n-of-1 designs provide the opportunity to identify patients who are not alike within the population of interest, suggesting the need for bayesian clustering methods to be further developed and implemented [cit] ."
"the aim of this paper is to compare n-of-1 trials to parallel and crossover rct designs and evaluate (1) effect sizes, (2) power and (3) sample size for each study design through simulation study. results from this study will provide information about the adequacy and utility of n-of-1 trials compared to traditional trial designs under a range of potential trial conditions."
"this allowed us to gather information about the features of motor neural signals, and use them to decode the velocity and force of movements. moreover, in order to assess the translatability of results obtained with chronically implanted intraneural electrodes to results achieved with microneurography, a hybrid computational electromagnetic-biophysics [cit] model of recording of intraneural electrodes (times [cit] ) in the median nerve was developed and then used, to verify whether the features of acute (microneurography) and chronic (implanted neural interface) recording device are similar."
"in addition to expanding the analytical approaches to quantifying carryover effects (i.e., slow vs. long), n-of-1 studies offer a unique advantage: they can often provide their own \"pilot data\" through initial phases prior to data aggregation or meta-analysis. researchers with little prior information on"
"where f coeff is the feedback amplitude, is the delay, w is the integration window and α a memory factor. the movement feedback depended on velocity and position [cit] as in the following equation:"
"n-of-1 trials demonstrate potential to significantly improve clinical decision-making. physicians can continue to treat individuals where improvement is demonstrable and stop treatment for those who have harmful or null effects. such trial designs may offer a vital complement to traditional research methods. our findings provide further evidence that n-of-1 trials can produce rigorous, evidence-based results to inform personalized healthcare."
"an issue when conducting aggregated n-of-1 trials (i.e., data aggregated from a series of n-of-1 studies) is the generalizability (external validity) of their results. selection bias in aggregated n-of-1 trials could result from not including a sufficiently sized, representative sample of participants. this concern is also relevant to rcts. the external and internal validity of aggregated n-of-1 trials can be limited by three primary forms of selection bias: lack of representativeness due to a small sample size, lack of random sequence allocation of intervention and control periods, and lack of allocation concealment [cit] . to address these, one can use block randomization or counterbalancing and single, double, or triple blinding treatment when feasible. other strategies to increase scientific rigor include repeated assessments within treatment periods, adaptive trial \"stopping rules\" to terminate trials as soon as negative or positive treatment effects are demonstrated, and applying appropriate design and methods of analysis that yield highest power and effect size without inflating type-i error [cit] . an important statistical consideration in planning an aggregated n-of-1 study (i.e., data aggregated from a series of n-of-1 studies) involves calculating internally valid power and sample size. aggregated n-of-1 trials require smaller overall sample sizes than traditional rcts, as individuals are serving as their own controls. one must account for power needed to achieve identification of individual-and group-level differences resulting from treatment."
"where act i and rest i are intervals of 100 ms in which the signal is fragmented, and n is the number of intervals. an isotonic task repetition was considered well performed if the subject had applied the requested pressure over the grasped sensor (10% tolerance). besides, we counted an isokinetic task repetition as successfully concluded when the subject's movement followed the timing imposed by the gui (10% tolerance). this was checked by comparing the occurrence of the maximum of the forearm muscles semg and the moment in which the gui asked for a velocity change (i.e. transition between flexion and extension of the movement/grasp). for the isotonic and isokinetic task analysis, we considered for each subject, only the semg of the muscle that was the most correlated with the mng (highest pearson's index) in both the exercises. the most correlated muscle for each subject can be seen in fig. 1f ."
"1. inclusion criterium: high correlation with active fingers' movements executed by the participant (prerogative of efferent but also proprioceptive fibers) 2. exclusion criterium: negligible activity when (a) a mechanical stimulus was applied over belly or tendons of the hand muscles innervated by the median nerve, (b) the fingers were passively moved, (c) a mechanical stimulus was applied on the hand skin."
"we have shown that n-of-1 trials have more power at lower sample sizes compared to parallel rct and crossover designs. this finding has also been shown in similar studies [cit] . n-of-1 studies should be considered high-grade evidence for informing subsequent research, e.g., proof-of-concept, or traditional randomized control trial designs. we posit that n-of-1 trials offer benefits not present in traditional designs."
"the procedure applied to record from human median nerve was approved by the ethical committee of the campus bio-medico university. all the experiments reported in this work were conducted in accordance with the approved guidelines and all the subjects signed the informed consent. six healthy volunteers (four males, two females) underwent the procedure."
"page 18:44 optimal boundary dimensions (the bound is shaped as a cylinder) were estimated using convergence calculation [cit] and applied to the model (140 mm of diameter and 90 mm of height). extruded anatomies formed three tissues: epineurium, perineurium and endoneurium. the values of electrical conductivity inside of these tissues were based on findings from multiple studies [cit] . the extraneural environment was assumed to be 1% saline at 38 °c [cit] . the nerve was implanted with a microneurographic needle or a time electrode. the tungsten microelectrode was replicated as a cylinder (40 mm of length) with a cone-like ending (250 μm of diameter). the time electrode was built as a rectangular structure (800 μm of length, 200 μm of width and 24 μm of thickness) where four circled active sites (80 μm of radius) were placed on each side of the structure. the nerve was randomly populated with myelinated fibers whose activity was individually controlled by a spike train based on a poisson process with a range of mean firing rate of 2.5-25 hz [cit] ."
"n-of-1 trials can be demonstrably effective in ascertaining treatment effects, both at individual and population levels. interventions can be tested with adequate power with far fewer patients than traditional rct and crossover designs. operating characteristics compare favorably to both traditional rct and crossover designs."
"consider an n-of-1 trial of a new therapy versus placebo targeting improvements of finally, we examined the extent of issues that n-of-1 trials can have when there is a non-representative sample for the population of interest (i.e., selection bias). we have already demonstrated that if samples are drawn from the population of interest, n-of-1 trials obtain desired power with fewer patients than parallel rcts or crossover designs and maintain type i error constraints when carryover effects are not present."
"we performed a simulation study to compare the operating characteristics of aggregated n-of-1 trials, rcts and crossover designs for varying sample sizes and effect sizes. we examined the differences in a proposed trial of a new therapy versus placebo over 3 cycles. these cycles were sampled representatively to include a proportion with cycle effects, without cycle effects and then disproportionally from a population with intentional lack of generalizability. in general, data was generated from:"
"all these works, though, did not strictly characterize the performance of the prosthesis controlled with such signals, or the signals themselves. in fact, the relationship between the intended grasping force, or the movement velocity of the fingers, and peripheral nerve recordings has not been yet characterized. this would allow the development of decoding strategies leading to an intuitive and effortless prosthesis control [cit] . another limitation in developing a robust (in terms of cross-subject reliability) method to drive a robotic hand by neural signals is the lack of a dataset including recordings of peripheral nerve electrical activity correlated to different hand grasp/finger movements [cit] . creating such dataset from healthy humans, whose validity could be extended also to the case of amputees, would boost efficient neuroprosthetic developments."
"each volunteer was comfortably positioned on a chair with the right arm placed on a support over a table. then, a neurologist identified the median nerve site using nerve ultrasound imaging (the echograph was an esaote mylab 70xvg, equipped with a 14-18 mhz probe) and guided the insertion of the microneurographic active electrode (fhc unp40gas, diameter 250 μm, length 40 mm), through the skin above the elbow, into the nerve itself (fig. 1a) . the correct and final placement for the electrode was individuated when the microneurogram (mng, i.e. the signal acquired through microneurography) satisfied two criteria:"
"limitations we acknowledge several limitations. first, testing the full range of possible design variants would exceed the scope of this paper. some designs were arbitrary and unlikely to occur in true trial conditions such as scenarios with very high or low variance or sample sizes in the 10's or 200's. these structures are necessary to consider, however, as they demonstrate the comparative effects on each trial design when scaling up or down from average sample sizes. thus, attempts to cover this range substantially, but also efficiently, were emphasized. indeed, not all statistical challenges present with n-of-1s could be evaluated with equal rigor-selection bias cannot be modelled as comprehensively or accurately as power, sample size, or even carryover effects. subsequent research is warranted which might further model and formalize selection bias hierarchically with other concerns."
"in fig. 5a an example (from subj. 2) of the neural firing pattern observed during the isotonic task is shown: the cell firing presented a steep rise (reaching phase), correspondent to the moment in which the participants were squeezing the sensor to reach the required force, followed by a stable discharge level (holding phase), when the subjects were maintaining the desired pressure. in both phases, the neural activity showed a quasi-linear increment with the lower values of applied force. instead, for higher levels of force, firing rates progressively reached saturation."
"the input to each neuron was the same [cit] and constituted by the sum of three components: the central drive (i stim ), the feedback current (i feedback ) and the noise. the noise was generated from a poisson distribution with the same time varying mean for all the cells [cit] ."
"individual treatment effects, these results suggest that future methodological advancements may be able to cluster patient treatment effects to determine existence of subgroups within patient cohorts."
"to test all the afore mentioned decoders, the movement/grasp in which the mng presented the highest snr average among repetitions (computed as described in motoneurons behavioral analysis) was selected."
"in order to compare data acquired by means of microneurography and time electrodes, 10 simulations per electrode, per value of spike train mean firing rate, were run. 10 values of mean firing rate were selected in the interval 2.5-25 hz sampled with a step of 2.5 hz."
consider an n-of-1 trial of a new therapy versus placebo targeting improvements of psychological health in adults age 25-50 years old. it is plausible that our population contains individuals with disproportionately high or low baseline risk of poor psychological health relative to our target population. if we sample from this sub-population our statistical conclusions about the population of interest may be incorrect.
"finally, we examined the extent of issues that n-of-1 trials can have when there is a nonrepresentative sample for the population of interest (i.e., selection bias). we have already demonstrated that if samples are drawn from the population of interest, n-of-1 trials obtain desired power with fewer patients than parallel rcts or crossover designs and maintain type i error constraints when carryover effects are not present."
"we simulated neural activities as an izhikevich regular spiking neuron model [cit] (see additional file 1: methods for details). we simulated five neurons for simplicity, but results are compatible with experimental observations for a number of neurons ranging from two to twenty."
"in order to transfer the observations from microneurography data to the chronic condition, we compared the microneurography data with the data recorded by long-term implantable devices, as time electrodes, by exploiting a computational model. a hybrid finite element method (fem)-neuron model of the human median nerve was developed to compare the recordings obtained by microneurography and time electrodes (fig. 4) . the anatomical structures were modeled according to histological data [cit] . the (6)"
"the null treatment effect hypothesis was tested similarly for the crossover design, while a standard linear regression model was used to test this hypothesis for the rct."
"n-of-1 trial designs offer a rigorous method for advancing personalized medicine and healthcare while minimizing costs and resources, provided treatments are ethically designed for chronic stable conditions with sufficient washout periods. we performed a simulation study to examine the operating characteristics of n-of-1 designs compared to parallel rct and crossover designs. our results follow closely from previous theoretical results which demonstrated simulated sample sizes required to achieve a given power were lower for n-of-1 trials than for parallel rcts, as the number of cycles administered increased [cit] . if n-of-1 trials are conducted within current guidelines we have evidence that they will provide robust effect estimates of treatment [cit] . we validated this theoretical result through simulations and showed that n-of-1 operating characteristics were superior to those of crossover designs. we also examined the effects of carryover and sample representativeness on the operating characteristics of each of the three rct designs. carryover effects can result in inflated type i error probabilities for both crossover and especially n-of-1 designs, but when treatment effects are present, n-of-1 designs have better power than both crossover and parallel rct designs, regardless of carryover effect size. when trial samples are not fully representative of the target population, all three trial designs make incorrect conclusions about the treatment effects in the target population, but n-of-1 designs make incorrect conclusions far more often due to increased numbers of observations on each patient. however, due to multiple observations taken on each patient under different treatment periods, n-of-1 designs provide the opportunity to identify patients who are not alike within the population of interest, suggesting the need for bayesian clustering methods to be further developed and implemented [cit] ."
"these conditions exclude the acquisition from proprioceptive (2a-b), tactile (2c), and autonomic fibers (1) [cit], which would produce a non-negligible signal, if their activity was present in the mng. another electrode, inserted percutaneously 2 cm away in the proximal direction, was used as reference. during the research of the best placement of the electrode for recording, the envelope of the microneurographic signal was showed in real-time while a thresholded version of the mng was sent to a speaker. the differential microneurographic signal was referred to a ground represented by a metal strip placed over the biceps (fig. 1a, left), amplified by a factor 10 5 and filtered in the band 300-3000 hz [cit] by a grass p511."
"microneurography and semg signals were recorded at a sampling rate of 10,000 hz with a 16-bits data acquisition board (national instruments pci-6251), installed on a personal computer (pc) [cit] that handled the recording and the real-time processing assisting the neurologist in seeking the fibers. the pressure sensor was custom-made [cit] and constituted by an airtight spherecatheter system connected to a programmable interface controller at its turn connected to the pc by a serial port. the recording of its data was performed at a sampling rate of 10 hz and controlled by the aforementioned custom program. the sensor readout was shown in the gui."
"part i attempts to establish causal mathematical logic (cml) as a new stable and self-consistent theory of physics, grounded on the greatest experimental base ever assembled and adequately represented by the fundamental principles of physics that apply to all things physical. argued in part i is that cml can represent the dynamics of a physical system in as much detail, or in as much generality as is possible or necessary, and seamlessly refine or expand the granularity as more is learned about the system, in sharp contrast with traditional statistical and differential approaches. the main purpose of part i is to introduce the causal theory in the full extent of its scope, which is universal for all physical systems, rather than being just a theory of agi or one of the brain. it is also the purpose of part i to place the specific argument of this paper within the general picture. part i is necessarily abstract. some readers, depending on their disciplines, may prefer to read part ii first. part ii contains many references to part i and may serve as an introduction to it for a second reading."
"pocket prediction and selection: detection of pockets was performed on a reference structure, chosen as the holo form structure if exists or an apo form with crystallographic structure with the best resolution. cavities are computed using fpocket software [cit], based on the theory of alpha-shapes, voronoi diagrams and delaunay triangulation. fpocket ranks cavities as the most probable active site. nevertheless, we have chosen to perform our asmc over all predicted pockets. we present results for the most conserved pockets, which turned out to be the enzyme active site. in majority of the cases, the first ranked cavity by fpocket software was identification of subfamily-specific sites the enzyme active site, except for families pf00278, pf01227, pf01583, pf02901 and pf03414."
"black-box experiments are central to addressing the issue of the external observation of life signals. they bridge the gap between internal function and externally observable signals. in simulation, the requirement for life signals to emerge rather than being explicitly programmed is the same requirement of self-programming."
"the same considerations can be applied to a computer, and the same results can be obtained, including massive parallelism and intelligent behavior. sp has proposed a causal virtual machine [cit] f, § 4.1), where the same principles are applied and the same results are obtained, including massive parallelism and intelligent behavior. \"neurons\" are represented as simple processors, and \"push each other\" is described as \"commutation\" between adjacent neurons."
"here, we propose to extend similar views to neuroscience. we propose a universal language for neuroscience and agi based on causality, detailed enough to describe the finest structures of the brain yet general enough to accommodate the versatility and wholeness of intelligence. we also propose a new fundamental theory based directly and exclusively on the fundamental principle of causality, of which that language is the expression, and we further propose that engineering practice in agi and neuroscience should be carried out in the context of this theory."
"after the discovery of the action functional, an experimental program was started to conduct more experiments of that kind, the main purpose being to understand the scope of the finding and write a corresponding theory. several sources of data for the experiments were identified, and are discussed in section 9.3. several experiments were indeed performed, as discussed in section 9.4. all experiments were designed subject to severe limitations in computational power and lack of resources. other possible types of brain experiments that were not tried yet are in section 9.5, and the project plan schematic of section 10 proposes a possible approach to continue the research."
"nucleotidyl cyclases: we selected 2201 sequences from pf00211. this set presented sequences of 199.80±70.13 residues and after selection by size we retained 1646 sequences of 187.25±14.44 residues. we used one template for guanylate cyclases (pdb id:chain, 3et6:a), one for adenylate cyclases (1ab8:a) and we analyzed 536 sequences with more than 30% identity to these templates. these 536 sequences have 41.18±7.61% sequence identity with their template sequences."
"in neuroscience, life signals such as electromagnetic brain waves and oscillations are used to detect brain function. computer simulations/emulations of the brain that have not been explicitly programmed to reproduce these signals are expected to reproduce them anyway. waves and oscillations are used because they are externally observable and hence serve to judge the quality of the simulations. here we propose that waves and oscillations are just one example of an enormous variety of life signals, the blocks in the block systems, such as hofstadter's 'mother'. these blocks are observables, but generally not amenable to easy external detection."
"for the purpose of a black-box brain experiment, the brain is considered as an implementation of a causal dynamical system that receives some input information, processes it, and generates some output information. only the input/output relationship is of concern. the details of the implementation, no matter how complicated they may be, are irrelevant. the output from the box is known as the \"measured\" information."
"a problem on object-oriented software analysis, design and refactoring was selected. the problem statement describes a local area network (lan) with 4 stations of different types interconnected in a token ring. the statement describes the actions that each type of station is expected to take, such as allow a user to enter a message, receive a message, retransmit it to the next station, print it, destroy it, or report the condition of the network. the statement is, of course, causal, and is converted to the form of a causal set, consisting of ordered pairs given in any arbitrary order. an experienced developer is in the black-box. the developer is asked to produce a design for object-oriented java code. the problem is used in european universities to teach refactoring and oo analysis and design. the final java code is public (demeyer, rysselberghe, and et. al., 2005) ."
"with the increasing number of genomes and meta-genomes sequenced, a critical challenge concerns the functional prediction of proteins encoded by novel predicted genes. genes and proteins are commonly classified in terms of families, subfamilies or superfamilies according to different molecular taxonomy [cit] ."
"in this work we treat information as a physical entity with physical properties of its own and try to follow its path through the brain as it follows causal links originating in the sensory organs and the brain's memory, is processed in the connectome, and exits the brain via motor nerves and other efferent nerves. our focus remains on the information, not on the brain anatomy or physiology, but we apply the general physical and mathematical properties of information to derive requirements that the anatomy and physiology must satisfy if information is to be processed the way it is. the requirements place effective constraints on our understanding of the brain, add to neuroscientific knowledge in general, and can be directly compared with experimentally observed features and help to explain intelligent behavior in humans and animals."
"there is no need for any separate bootstrap mechanism or for the presence of any loop that never ends and keeps the brain working. when the brain is represented as a causal set, there is an input and an output of information, as discussed above. the input is \"all the information that enters the brain,\" irrespective of the origin of that information or of the \"meaning\" we may want to attach to it. similarly, the output is all the information that the brain puts out, irrespective of it's destination. but these considerations also imply that external feedback loops automatically arise, as a consequence of the interaction of the brain with its environment. there can be a large numbers of such loops. for example, consider the sensory-motor loop. a baby shakes her arms and legs as directed by output from her brain. but the baby can also see and feel the arms and legs as they move, and generate input for the brain. this is an external feedback process. the brain stores initial input, then binds it with additional input coming from the external sensory-motor loop, and creates additional output for the limbs. the resulting feedback loop results in sensory-motor integration, where the baby starts with, perhaps, random movements and progressively learns how to control her limbs, how to grab, etc. another loop such as this may be responsible for eye-movement coordination. of course, there is no closed-form mathematical proof of our statements. they can only be verified by construction, and this task needs black-box experiments."
"a natural theory of information has to be successful in the translation across many substrates. e.g. from fundamental physics -dna -phenotype -simulation. the first issue that is put forward in cml is based on overtly simple physics, while a proposed application such as brain simulation project has considerable complexity. first of all the scope of cml is such that it does not miss in between stages across substrates, as the physical definitions are operational both in live physical actions (such as operation of cells, or a brain). they are also operating within the information space of discrete substrates such as dna, as well as the in-between stages of the transition between genotype and phenotype, such as epigenetics and principles of development. to support this perspective, pioneers in biology have proposed a growing body of work that such causal physical principles are to be found operational within the genetic information space [cit] . as noted previously, simrep type projects active in cellular biology are also forced to simplify their computational domains by deferring to natural physical principles."
"translation between two languages with slightly different subsets of observables is relatively easy, but it may be very difficult if the subsets of observables are very different. in either case, detailed translation can be achieved only by detailed reference to the underlying causality."
"residues are shuffled at all positions of the msa and a new log-likelihood is computed. we perform 100 simulations and consider the average value for the log-likelihood computation. thus,"
"there follows a corollary. the size of set c, or the total number of causal sets, is infinite countable. the size of h, or total number of finite fractal hierarchies, is also infinite countable. this means that a one-one correspondence must exist, that this correspondence is effected by function e, and most importantly, that nothing is left outside the central theorem. in other words, because there are no hierarchies outside of h and no causal sets outside of c, any causal set or any hierarchy that is found in nature is already accounted for by function e. if a hierarchy is found, there is a corresponding causal set. if a causal set is found, there is a corresponding hierarchy. this corollary allows procedures such as the conversion of software into causal format. it guarantees that the conversion is possible, which is a powerful conclusion."
"in engineering, when a goal is prescribed, a causal path is frequently manually assembled from existing fragments of causal paths, trying to find a complete path that reaches the prescribed goal. if a gap is found, as is frequently the case, then the goal changes and much of the engineering effort is concentrated into closing the gap by searching for the missing causal fragment. this search is not just a computational search, it may involve research and experiments. when the causal path is complete, then the goal is achievable, and the path is the algorithm that allows the prescribed goal to be reached. the algorithm may be a computational algorithm, but it also could be an industrial process."
"influence of sequence similarity on asmc: one of the factors that influences the quality of models predicted by homology modeling is the sequence identity between the target sequence to be modeled, and the reference one (associated to a structure). it has been shown that models obtained for a similarity greater that 30% have, on average, more than 60% of the backbone atoms correctly modeled and a root mean squared deviation (rmsd) less than 3.5 å [cit] . the efficiency decreases below the 30% threshold, which defines the upper bond of the twilight zone. supplementary figure s2 presents the evolution of se and sp depending on sequence similarity obtained for the 42 benchmark families. it spotlights that detection of sdps is more efficient with dataset composed of divergent sequences (increase of specificity). also, this analysis underlines that asmc is still efficient below the 30% threshold, a compromise to obtain good sp and se."
"the third source of interesting brain experiments, is image recognition. in the hofstadter's example, the causal input consists of the 100 million dots plus a collection of causal imagery in memory, the output is 'mother', and the device is the retina plus the brain of the observer. more in general, a television picture that consists of illuminated pixels can be the input. the eye and brain of the person who is watching the image provide the dynamics, and the output is the recognition of the objects present in the scene."
"the fitness of chromosome is given by using \" i w \" weight associated which varies as per the qos requirement to the performance objective i f ."
"but in the causal theory, goals are viewed very differently. if a goal is a state that can be reached from some initial state by following an existing causal path, then all states found in the causal space are goals, except the initial states, the ones that have no known predecessors. hence, the notion of goal-based agents is not necessary. any state in the causal space can be designated as \"the goal\", and decisions necessary to reach it can be found by goal-directed reasoning going backwards from the designated goal. if, however, the desired goal is not in the causal space, then the goal changes to that of expanding the causal space by learning more information. which may involve research, observation, and experiments to determine the missing causality. once the causal path is complete, then the original goal is recovered."
"in the brain, the causal set captured at the retina is transmitted to the brain and interpreted by causal logic, resulting in the identification of the objects present in the scene. this process is extremely simple, basic, and fast, yet such a large amount of effort has been invested for so many years in designing image recognition signals with only partial results."
"in some cases, for example when the human force consists entirely of software developers untrained in causal modeling, or when a computer model of the system already exists and needs to be integrated with other programs and causal models, it is possible to convert software directly to causal set format. this possibility is discussed in more detail in section 10.1, where a graphics user interface (gui) is proposed for general causal modeling work and visualization."
"one of the experiments below is for the vision system, others are for high function in the human brain. we don't have access to resources adequate to perform these experiments in selected parts of the brain. the issue of limitations is discussed some more in section 10. experimental results from several different classes of black-box experiments are discussed in section 9.4. table 1 lists the names of the experiments and their respective input and output. these experiments indicate that the notion of brain analysis is not just convenient, but necessary. the brain receives causal input from sensory organs and other afferent nerves in a form that is easy to formalize as a causal set. the observable output consists of objects that we can understand and use to make decisions and take actions. however, the input for the european example ( §9.4) consists of a computer program, the input for image recognition ( §9.4) is a table representing points on a plane, the input for theories of physics ( §9.4) is a set of equations, and the input for euler equation ( §9.4) is also a set of equations. all these tasks are considered as high brain function. (brain, computer) virtual machine measured output predicted output figure 1 : a black-box brain experiment. the \"device\" in the black box is any unknown dynamical system that receives some input information, processes it, and generates some output information. a causal virtual machine receives the same input information and generates it own output. predicted objects calculated by the virtual machine are compared with the natural objects and found to confirm the theory."
"the generation of millions of papers in neuroscience working on various parts or levels within the brain has helped us to define most of its circuits and many of the higher level functions and principles. due to the complexity of the system we have arrived at an impasse in regards to how far we can proceed to complete understanding without large scale integrative neuroscience. current projects now emerging for the ground work in whole brain simulation or aspects of it, are: human brain project (hbp), the us brain project, darpa synapse, brain corporation, neurogrid, allen brain atlas and the ibm ultrascalable tissue simulator. with current technology these will be restrained in regards to how much of the brain's detail can be simulated. currently whole cell models are able to derive a replication of the phenotype from genotype in entire genetic detail using physical principles (simtk framework). by contrast, whole brain emulation (wbe) aims at a more efficient route to acquire the minimal requirements to run a workable copy of our conscious operations with as few of these detailed constraints as possible."
"for the purpose of confronting the agi problem we have adopted the working hypothesis that intelligence is of this world and must be explained in the context of a theory of physics grounded on the supreme principle of nature, the fundamental principle of causality. we also proposed that other fundamental principles, symmetry and least-action, can be derived from the principle of causality [cit], and in section 2 we argued that information should be considered as a physical entity."
"with time, after a transition period and as cml becomes more powerful and causal research advances, natural languages, where words and sentences and books are also invariant observables of the same kind we discussed in section 3, will become part of the gui. it will then be possible for neuroscientists to communicate directly with the models. this is part of our vision, but we are not there yet. we have to get there first."
"in this work, we propose asmc, a novel methodology for unsupervised classification of protein subfamilies allowing functional and specificity determining positions (sdps) detection. [cit], sdps are related to fundamental regions that correspond to ligand binding and protein interaction sites. methods developed to detect sdps are based on multiple sequence alignments and we propose with asmc to improve the detection by using structural alignments of active site residues. while 3d structural information gives valuable insights to understand the molecular function of proteins, there is still a small fraction of experimentally solved structures in comparison with the amount of sequences. in this context, homology modeling provides structural information for the members of a protein family, using one or more templates [cit] . different methodologies have been developed to improve protein annotation of protein specificity using 3d information [cit] ."
"the phenomenon of binding, defined in section 4.3 as the generation and globalization of local associations of elements of information, is by itself not sufficient for intelligence. there are other conditions that need to be satisfied before a complex system exhibits intelligent or adaptive behavior."
"causal logic can too. discrete, finite causal logic can deal with continuous entities the same way we humans do. if it didn't, then our conjecture that causal logic is a theory of intelligence would be invalid."
"comparison of clusters with ec classification: in order to test the relevance of asmc clustering, we checked the coherence of clusters using experimentally proven ec numbers. seventy percent of the clusters are in agreement with the ec number classification, each ec number segregating the clusters. in supplementary figure s1, we give an example of classification obtained with asmc for pf02274 pfam family. asmc has separated two main clusters with very distinct activities (ec 2.1.4.; aminidotransferases and ec 3.5.3.6; fig. 2 . asmc results over mono-and multifunctional families. on top, asmc performance is described in terms of specificity and sensitivity (see section 2). below, average distance to bound ligand(s) is compared between cps + sdps categories, predicted to be functional residues, and ops categories, the remaining residues. for some families, two results (family-1/-2) are presented when two ligands are bound to the enzyme in different pockets."
"in computer engineering, the process of binding is known as refactoring. refactoring, and objectoriented design, are done by human developers. attempts at automating these tasks were many, but were not successful, and as of today only humans can do it. not a surprise, after reading what precedes."
"brain simulation. henry markrams blue brain or human brain project (hbp) is a current primary example of simulation. hbp does not have an agenda for wbe scanning. it is interested in biological detail and a general system model. to achieve aims with current technology there is a cut-off limit in regards to specific details of cellular replication. hbp also proposes to simulate the brain's lifespan from neurodevelopment to ageing. this is one step from a generic bootstrap intelligence model with a causal direction, and it is hoped we might see generic principles of intelligence emerge without explicitly programming them. replication. at its purest level, replication seeks to recreate the fundamental physical conditions of the brain in our natural environment to explore the possibility of discovering the deepest principles of intelligence. it is not currently feasible to build a physical copy of the brain so replication projects are minor in scale. a neuroscience example is colin hales aims to develop his work on ion channels [cit] into physical replication experiments. [cit] ."
"the mathematics of the theory seems to be complicated, but its application to practical problems is really very simple. the first step is to construct the causal set model of the system of interest from input. the input can be a statement describing the system, a stream of data coming from sensors, or any other source. many possible sources have been discussed in section 3 of pissanetzky (2012f) . if the system has a memory, then any information stored in the memory has to be considered as part of the input. the total input consists of all information that's available to cml, irrespective of its source. cml's function e takes as input a snapshot of the total input and returns the hierarchies corresponding to that snapshot. cml is ongoing. it functions as a movie camera. it constantly composes new snapshots and produces a movie of the evolution of the structures. this is how we see objects that move or break or change shape as they move."
"there may be many different trajectories in causal space, corresponding to many different total orders of set s. any of the trajectories can be followed by the causal execution, giving rise to an indeterminacy, or uncertainty, which corresponds to the physical notion of entropy. furthermore, the equivalence between causality and time, both of which are directional and both of which are related to the direction of entropy, has led sp to introduce the concept of action for causal pairs. in physics, there are various definitions of action, but action is always associated with \"traveling energy,\" energy that travels during a certain interval of time (not over a distance!), which is very much consistent with our definition, as given below in more detail."
"in the causal theory, causal information problems are solved in causal space, not in spacetime as is done in physics. causal space is possibly very similar or the same as information space, proposed by manfred eigen (at the time of this writing, the book had not yet been released). however, we believe that \"causal space\" is a more appropriate name and will stick to it at least for now. in causal space, there is only one dynamical system -information, only one dynamical representation -the causal set, and only one dynamical equation -causal mathematical logic [cit] f, and references thereof), which gives origin to our familiar notions of meaning, semantics, and understanding, including our understanding of the laws of physics, life, and thought. causal space and spacetime map easily from one another, although maps have not yet been fully explored for some types of physical phenomena. the example \"euler equations\" in the supplementary material [cit] d) illustrates one use of the causal-space/spacetime map in classical physics. the notion of causal space is unifying for all sciences."
"the major question of ai is this:\"what in the world is going on to enable you to convert 100,000,000 retinal dots into one single word 'mother' in one tenth of a second? perception is where it's at.\""
"contrary to what it may appear, the experiment was not easy. the conclusion, is that the sp's brain was trying to minimize the total length of the flux lines in the matrix. the process consisted of a series of permutations among neighboring statements in the program, all of which were conducive to the minimization of that length. this conclusion represents the discovery of the action functional, now proposed in eq. (3), and led to the definition of cml as a thermodynamical process that minimizes the action functional. these are the conclusions as they stand today, but they were not easy to arrive at the time. it took several years of intense research to discover the significance and wide scope of what had been found, and to elaborate the corresponding theory as it stands today."
"mutation has been included by visiting every bit in each new child string which throwing a random number between 0 and 1 if this number is less than 1/64 flipping the value of the bit. best fitted individual is shown in figure 4 . above example is simulated for 100 iterations. and results are exhibited for first 60 iterations. best fitness and mean fitness shown in figure 3 against generations carried. the best fitness is in the range of 1.4881 and the mean fitness is in the range of 1.4882. figure 4 shows number of variables vs current best individual. the first variable specify expected data rate and second variable is channel date rate. fitness of the optimal individual is 1 hence after 1st iteration values of individuals shown in the range of 0 to 0.8. children populated by each individual are shown in figure 5 . average distance between individuals for number of generations is shown in figure 6 . it is used to estimate the distance between parent and child and minimum distance achieved suggest that the optimum value is reached which is 0, specifies the solution of the objective function. sample results for different modes of operation and value of relevant gene is presented in figures 7-10 . in ht mode which has the 65% weight. in this mode the data rate objective function reaches close to the optimal value of after 51 iterations. in pr mode, the objective function of minimizing power has maximum weight. in this mode the required power reduces in a few iterations. in this case 65% weight is given to the tps gene which achieves its maximum value after 25 iterations and the rest of parameters are also approaching to their respective optimal values. figure 11 shows users with only best effort service, the capacity of each su is evaluated over the different rate proportional. results verify that ga guarantees to obtain the expectation capacity for a particular su by adjusting the operating parameters. figure 12 shows the performance of the proposed subcarrier allocation scheme over the number of users. as the number of users, the proposed scheme approach to the optimal solution with perfect channel information. the simulation results confirms that the information is fed back and association amongst subcarrier play a vital role in order to improve system performance. as compared to random scheme ga appears attractive as the number of users increases. further it significantly reduces computational complexity and achieves higher capacities."
"all three conditions are necessary, but the third one is the most critical: it is the requirement that an external process must exist that will optimize action and bind the information being processed. a system such as this, where two different processes coexist and interact, is known as a host-guest system [cit] . the term substrate is sometimes used to designate the host [cit] . when information is learned by way of the system interacting with its environment and growing its memory, the acquired information carries energy and entropy with it. entropy is a measure of the degree of uncertainty in the information. as natural systems constantly learn and acquire more and more information, more and more energy, entropy and uncertainty accumulate in the system, making it \"hotter\" and more uncertain and disorganized, and the need arises for the external process to constantly operate and remove excess energy. this process represents the fundamental principle of least-action, well known in physics. when action is minimized, excess energy and entropy are removed from the information, the information becomes more certain and better organized. patterns arise when uncertainty is removed and the information is invariant. the process that minimizes action is known as causal logic [cit] f), and an implementation of causal logic on hardware or software is known as a causal processor. a causal processor is a thermodynamic device, not a computer, but it can be simulated on a computer or on specially designed hardware. once production starts, causal processors will be small and inexpensive devices. they will be rated by problem size, with execution time independent of the size of the problem, and will be stackable for larger problems without increasing the time interval needed for completion. more details are covered in section 10 of the present paper."
. for verification of operation the ga was modeled using rf parameters/genes. several simulations were run and results are verified. the output consisted of final optimized gene values.
"causal dynamics is not the same as causal logic. causal logic is also a dynamic process, but a different one. it happens to the algorithm, but is not the algorithm, and is of a thermodynamic nature, not algorithmic. causal logic selects trajectories according to their uncertainty, or entropy, re-organizes the algorithm, and finds a structure for it composed of invariant observables (section 3). the invariant observables are the mathematical blocks of section 4.1. because the observables are invariant under transformations, such as translation or rotations, they are also certain, no longer uncertain as the original unstructured algorithm used to be. and this certainty, originated from a purely thermodynamic process, is what we call semantics. it represents meaning, as proposed in pissanetzky (2012f) and previous publications. the elements of the original causal set are initially devoid of value and meaning, but the process of causal logic creates structure and is the base for semantics. and because algorithms represent behaviors, and causal logic makes the behaviors certain and invariant, these structured behaviors are intelligent behaviors."
"the task of binding is absolutely essential for intelligence. it is the core engine that generates a global response to the local actions of each one of the billions of neurons that work simultaneously, locally, and in complete isolation except for their immediate connections, but without any awareness of the global consequences of their actions. the globalization of the local action is formalized by eq. (3). this equation is the critical detail that has been missing in science so far. it expresses a global effect -the optimization of the global action functional -as the algebraic sum of an undetermined and possibly very large number of positive independent contributions, coming from independent parts that have only been programmed by evolution to perform a simple local association without any external director. eq. (3) is what allows the human brain to be so large."
"one of the most exceptional feats of the human intellect is its ability to recognize the objects in a scene in very little time and with so little effort, and to do so continuously, without interruption during the entire day. when light from the scene impinges upon a light sensitive element in a camera or retina, the element generates an electric signal that acknowledges the fact that causal information from the external world has just been acquired, where the light is the cause and the signal is the effect. hence, (light, signal) is an ordered cause-effect pair. all sensors operate this way. when many sensing elements are operating at the same time, the resulting output is a causal set. there is no need for any conversions."
"modeling: homology models using multiple templates are constructed using modeller version 9v6 [cit] . prior, for each family, a 'structure profile' was built with a multiple structure alignment using the salign module [cit] ."
"the history matrix is denoted by h for the pus on the k channels. it maintains history of each channel and based on this history the transmission possibility scale (tps) for each available channel is calculated.tps indicates upcoming accessibility of channel for cr users without interfering pu. tps is represented as,"
"contrary to what it may appear, plenty of detailed and well-documented information for experiments of this kind exists in the literature for the case where the black box is the entire brain. for experiments involving parts of the brain, it is obviously more difficult to obtain precise measurements of input/output, but it may still be possible to carry them out in small animals or in the context of the projects mentioned above, followed with an effort to develop better tools for this research. figure 1 illustrates the concept of a black-box experiment. the black box contains any physical device that can process causal information. it is called a black box because we do not need to know what's inside. the device receives a certain well-defined causal input, and generates a certain causal output, both of which can be observed and measured. the device can have an associative memory, but then the content of the memory must be considered as part of the input. this is the experimental part, and the output is the observed result."
"experiments in all these 3 areas were initiated. some were completed, but some run into computational limitations and were carried only partially. the limitations arise from the need for special hardware, as discussed in section 4.3 of pissanetzky (2012f) . the hardware is very simple, but was not available at the time and is still not available now. all the work that was completed, was also successful, no disagreements were observed. the following section reviews some of this effort."
structural 3d alignment: the alignment of pocket residues were obtained using multiprot software [cit] . results of the multiple structure alignment of modeled pockets were compiled to build a multiple sequence alignment (msa) where each residue of the modeled pockets is positioned relatively to the reference structure.
"this problem is known as the binding problem. it was started by bertrand russell a century ago. a more modern name is the mind-body problem. the retina, and the mouth that pronounces 'mother' are the body. the 100 million dots are the observation, 'mother' is the observable. the causal link between the 100 million dots and the myriad signals that the brain sends to muscles in the lungs, vocal chords, tongue, lips, face, to cause them to pronounce 'mother', are the mind. how does the mind work? how does perception work?"
"the hierarchy represents compression of knowledge all the way to the top of the hierarchy, and makes memory associative. shared hierarchies allow us to communicate. 'mother' in the hofstadter example is the top of a hierarchy. hierarchies tend to be compact, but they may also be distributed as networks all over the brain."
"several examples and case studies in causal modeling have been published, see for example pissanetzky (2012a,g,d) in the supplementary material. in pissanetzky (2012c), a study involving hundreds of small causal system has been published."
"if that kind of input is presented to a human analyst, the analyst's visual system would first convert the program, the table, the equations, or the tasks into action potentials, and a great deal of processing would have happened in the analyst's brain before the action potentials are again interpreted as a program, equations, a table, or tasks. this processing is addressed in the image recognition experiment, but excluded from the others. in all cases, the input is represented as a causal set."
"( 1) where a corresponds to the number of times that we find the most frequent residue in position i in a specific cluster, b is the frequency of this residue in the other clusters, c is the number of sequences in the specific cluster and d is the number of sequences in the other clusters."
"the second source of carefully documented experiments is provided by the theories of natural science, that is, by the theories themselves, not the measurements on which the theories are based. a theory of science also consists of objects. in this case, the input consists of the experimental measurements that support the theory, the scientist's brain is the device, and the output is the theory."
"the experiment was presented in section 5 [cit] and in section 4.5 of pissanetzky (2011a) . unfortunately, the experiment is still ongoing due to lack of resources. the causal set has 1433 elements, too many for our capabilities. still, one major feature of the image has been correctly detected: the edges of three objects composed of points, present in the scene, indicating that edge detection works without having been explicitly programmed. it is not currently possible to proceed to further details. recall that the causal processor is generic, and has not been programmed in any way, certainly not with any edge-detection algorithm, and yet it detects edges. the results are available as supplementary material [cit] g) ."
"practical issues such as developing special hardware, building a prototype, training scientists and technicians in the new discipline of causal analysis, building a development platform, interfacing with legacy software, and training causal machines have been considered. a complete program for the development of new causal systems has been proposed, including hardware, software and human training. also proposed is the design of massively parallel specialized hardware of a high efficiency, that works much faster than the human brain and is of a general nature, yet inexpensive, simple, small, light, portable, and stackable as the size of the models increase."
"to introduce our work, we must first explain the nature of information and how we follow its flow through any device where it is being processed, such as the brain, or a computer. we must then discuss causality, state that information in nature is always causal, introduce the representation of causal information by ordered (cause, effect) pairs in fundamental mathematics, and introduce causal mathematical logic (cml) as the corresponding quantitative formalization. this paper is divided into three parts."
"many hierarchies predicted by the theorem are trivial, meaning that the only blocks are singletons, and are the elements of the causal set themselves. this means that the elements are themselves invariant observables. the proportion of non-trivial to trivial hierarchies, or the total number of non-trivial hierarchies, are important questions, but the answers are not known. the central theorem is the constitutive theorem of the causal theory."
"one of such sources, is object-oriented (oo) code. in the course of development of oo code, a human analyst receives a problem statement that describes some physical system, such as a business, or a machine, or perhaps a problem of physics that needs to be solved, and is asked to create an object-oriented model of that system. the model is subsequently used to develop software and documentation. all the work is causal, and the process amounts to a carefully controlled experiment where the given problem is the causal input, the analyst's brain is the device in the black-box that provides the transformation dynamics, and the resulting oo code and documentation are the output."
"information in nature is discrete and finite, and is best represented as a collection of ordered cause-effect pairs. in mathematics and physics, such a collection of ordered pairs is known as a causal set, defined as a finite partially ordered set that satisfies some additional conditions. our work is entirely described in terms of causal sets. we use causal sets because they are essential for understanding information, they provide a universal representation and communications protocol for nature, and they are naturally recursive. when we learn something we acquire information that is naturally causal. being causal means that effects follow their causes [cit] ). it does not mean that all causes are necessarily known, it only means that they exist. information arriving in our brains via afferent nerves or biochemical signals is causal. every process that happens in the brain and affects information in any way is causal. even neurons that spike randomly are causal because random only means that the cause is not known, or that we don't care to know, not that it does not exist. signals sent from the brain to motor nerves and other efferent nerves are causal. the universe is causal, and all information that flows in it is causal. the origins of the universe have been traced all the way back to the big bang. many physicists believe that causality and time are one and the same. we know for a fact that time is the same everywhere in the universe, except in black holes, and we know for a fact that causality is a fundamental principle, also everywhere but in black holes. the two facts are one and the same. the universe is causal, and so also are information and brains."
"the additional degree of freedom introduced by name assignment gives rise to natural languages. the differences in observables caused by differences in information give rise to conflict. shared observables make communication fast an efficient. why are natural languages so difficult to translate into one another? the number of observables is infinite numerable. but languages are finite. each language selects a finite subset of observables and assigns names to them, but the subsets selected by different languages are different."
"so currently all these approaches are on the table and may support each other in various ways. first of all we need to justify cml for neuroscience in the first place, and the validity of simrep as a concept. this will be attempted by determining if cml is both general enough to provide a description of the natural world, and from there review if we can determine its consistency with the basic components in intelligence signals that are sought to emerge in contemporary brain simulations. i.e. self oscillation and spreading waves. we will propose it has explanatory power to describe well in natural terms, the integration of these components where we define perceptual markers of mammalian intelligence in terms of the most emergent and complex signals in neuroscience which are the spectrum of event related potentials (erp). [cit] for more details."
"the causal mathematical entities introduced in section 4.1 are abstract and fundamental, but they are profoundly meaningful for both physics and neuroscience. in this section, we establish a correspondence between the mathematical entities and more traditional concepts used in science. there appears to be an impressive one-one correspondence."
"there are three conditions that need to be satisfied in order for a dynamical system -any dynamical system -to exhibit intelligent and adaptive behavior. the first condition is that the causal space must be large, and the larger it is the more intelligent the system will be. the causal space is the collection of all state variables that describe the dynamics of the system. the state variables hold information and represent the memory of the system, and a state of the system is the state of the memory at any particular instant of time. the larger the causal space the larger the memory and the larger the complexity and degree of detail of the patterns it can create and hold. this condition appears to be satisfied quite well among known adaptive systems, with motor proteins at the bottom, followed with worms and other animals, apes, and humans. it is generally recognized that the bigger the brain the more intelligent the animal is, although of course differences in implementations cause fluctuations."
"however, it is easy to implement the virtual machine on a computer, in which case it becomes a simulation of the thermodynamics of cml, but only the thermodynamics, never the device. the computer implementation was also discussed in §4 of pissanetzky (2012f) . in this case, the machine does need a program, simply because any computer needs a program, otherwise it wouldn't work. the program performs only two tasks: the input and output of the supplied and resulting information, and the minimization of the functional in eq. (3). the program never changes, and never has any knowledge at all of the device in the black box. it is fixed, it has to be developed only once and then used in perpetuity for all experiments. the two outputs are, then, compared. if they are similar, then quantitative conclusions can be drawn regarding details of the internal implementation of the device in the black box. several black-box experiments of this type were performed, and agreement was found in all cases. the most notorious quantitative prediction is the optimally short length of dendritic trees, later independently confirmed by a team of neuroscientists. without cml, even with the power law now known, and even if a connection between the power law and intelligence were to be discovered, it would still take neuroscientists considerable effort and time to explain the connection."
"in order to measure precisely the influence of homology modeling on spds detection, we performed a similar asmc pipeline but changing the structural alignment step by a multiple sequence alignment for the 42 families of our benchmark. the multiple sequence alignment was performed using mafft [cit] and the following steps (identification of residues of active site and classification) were applied as described in asmc methodology."
"we assign names to observables. all names we use in speech designate observables. anything we know and we can recognize is an observable and we assign a name to it. 'mother' in hofstadter's example is the name of an observable. but the meaning is not in the name. the meaning is in the observable, or, more precisely, in the causality that supports the observable. shared observables with shared names are what makes communication possible. function e introduced in pissanetzky (2012f) and further discussed in this paper formalizes a unique, one-on-one bijective correspondence between any given body of causal information and the corresponding observables. function e is deterministic. if two persons possess exactly the same information they will derive exactly the same observables. if their information is slightly different, then their observables will be different too. because of extreme sensitivity of function e to initial conditions (butterfly effect), the observables can be very different even if the information is only slightly different."
"in mathematics and physics, causal sets are used to formalize causality. a causal set σ is a set with a partial order, defined as follows:"
the central theorem of causal theory summarizes this entire mathematics section as the definition of a function e that maps the collection c of all causal sets to the collection h of all finite fractal hierarchies:
"the critical question in this section is of course: can we do the same on a turing computer? if we have removed the human from the loop, but the turing machine requires a man-made transition function, what do we do about it? easy: cml is the transition function, as described by ω in the causal set, and s is the turing string. and cml is not man-made. this is a non-explicitly programmed turing computer, or virtual machine, as described in section 4.3 of pissanetzky (2012f), where the transition function is mathematical, and the data is directly acquired from senses, or sensors in the case of the machine. it can also be viewed as a zisc (zero instruction set architecture), as invented by guy paillet and used for pattern matching. like the cml virtual machine, the zisc architecture allows a constant execution time regardless of the number of processors connected in parallel."
"the eureka effect, also known as the aha! effect, refers to the common human experience of suddenly understanding a previously incomprehensible problem or concept. we say we \"have an idea\" when an algorithm is delivered from unconscious to conscious. we feel a sense of surprise, because we now \"know\" the solution that we didn't expect. we say this at the precise point where we feel \"ready\" to act on that idea, and we feel ready to act because the binding has finished and the connectome for our algorithm is complete. there is no \"transmission\" of bound information from unconscious to conscious. it happens in place. the associative memory itself serves both as a substrate for the unconscious binding and the conscious algorithm, including the neural cliques, and cliques of cliques. the notion of having an idea, or suddenly understanding something or discovering something occurs when the host finishes its work and feeds the results to the guest. that is, when the unconscious thermodynamic process is complete and its output is passed to conscious."
"causal sets are used in causal theory to represent information. information is considered here as a dynamical system by itself, with properties such as energy, and uncertainty and entropy. ordered causal pairs are considered as dynamic entities describing the flow of information, i.e. energy, from cause to effect. the cause-effect relationship itself is considered as time, hence giving rise to the notion of action as energy that travels from event to event during a certain period of time. causal pairs form chains in many different combinations, causing a combinatorial explosion. the elements of the set correspond to variables in the dynamical system. the search process that visits the elements of the set in a certain order describes the dynamics of the system. intermediate steps in the search where some elements have been visited and others have not, correspond to states in state space. the chains of causal pairs correspond to the trajectories of a dynamic system in state space and each pair corresponds to a transition from state to state. hence, a causal set alone provides a rich mathematical model of a dynamical system in causal space, that directly corresponds to a more traditional model of the same system in state space or phase space. in this causal model, we discuss the trajectories first."
"the result is a scale-free, detail-tolerant theory that describes observables as fractal hierarchies of algorithms, or behaviors, allows for their quantitative calculation, and seamlessly extends across scales and substrates from the fundamental mathematics of causal pairs to elementary systems to complex systems. this is a theory where lossless physical models of variable granularity and complexity can be constructed and integrated together, from the finest details, to high-level models that emphasize global features and observables. the theory is a unifying principle for information."
"subsequently the two point crossover process is carried out. mutation process of forming the new generation is performed. mutation is used to randomly flip the value of single bits within individual strings. after selection, crossover and mutation the string is applied to the initial population so that the new population can form the generational counter is incremented by one"
"as discussed above, blocks are algorithms that control the external phenomena that we observe. at the same time, we expect those algorithms to be somehow generated by a simulation where they have not been explicitly programmed. in computer science, this is known as self-programming. and self-programming has been explicitly covered in pissanetzky (2012f), where it has been argued that algorithms originate precisely from the same ordered (cause, effect) pairs that we are discussing here and that we receive via our senses, or the computer receives via its input, or a robot via its sensors. coincidence? no. this is a very strong confirmation that we are on the right path."
"the order in which the elements of s are visited or initialized by execution is described by a list of the elements of s in that order, i.e. by a legal total order of the elements of s -one that does not violate the partial order. this total order is known as a trajectory, and the dynamical space in which trajectories live is called the causal space. the causal space corresponds, exactly, to the mathematical set t of total orders in section 4.1."
"an issue that will arise in practice, is the use of heuristic assumptions made by developers and firmly planted in code. as models from different origins are integrated into a general causal model, incompatibility issues between various heuristics will arise, and the heuristics will have to be eliminated or made compatible. the only information that needs to be entered to the causal model is data, measured, observed data, not interpretations. however, as humans have created these heuristics, humans can recognize them and eliminate them, and so also can cml. with time, as more experience is gained, it is anticipated that a fully automatic cml-based model conversion capability will be within reach."
"let t be the set of legal total orders of s under partial order ω. set t is very important because it does several things. set t lists all possible trajectories of execution of the causal set in causal space. alternatively, and using a term familiar to software developers, it can also be said that t lists all possible refactorings of the equivalent algorithm."
"at that time, sp had already noticed the experimental fact that certain sparse \"canonical matrices\" that represent code had remarkable self-organizing properties. canonical matrices are equivalent to causal sets, so the finding was the same as saying that causal sets had remarkable self-organizing properties. this statement is itself remarkable. to investigate the matter, sp created a setup where a canonical matrix corresponding to the initial disorganized version of the program was constructed, and any refactoring decisions made by sp's mind were reflected as operations that caused changes on the canonical matrix. this setup made it possible for sp to directly observe the mathematical effects of his refactoring decisions on the matrix."
where η indicate tps and ψ represents the history pattern of the pu on the channel s t number of time slots and it is calculated using (5). the value of summation varies from 0 to 1 while the tps (η ) varies from 0.3697 to 1. the variables s t and s can be used interchangeably and these are used together to improve clarity. the tps purpose is to minimize the interference and switching overhead in order to get better qos. this can be used to select the best channel in terms of the cr qos requirement.
"nothing of this happens in causal modeling, because the objects, architectures and code are automatically created by the causal machine. the output consists of ready-to-use functionally specific algorithms. \"functionally specific\" means focused on a specific task. there is nothing to adjust at that level. given the causal set the process follows as a consequence of the causality. the iteration takes place at the level of input, and any new information or changes are simply added to the model for the purpose of refining it. the important part is that the input must be expressed in causal set format, no matter what."
"the theory of causality describes information as a multiple preset goal system. or, to say it differently, as a single goal system, defined as that of minimizing the action, that leads to the creation of multiple goals as all possible final states of the system's evolution. this feature is described in more in detail in pissanetzky (2012h) . the notion of goal, as used in ai or engineering, does not exist in the causal theory."
"corresponds to number of objective functions. the simplification process is performed by converting the multi objective optimization objective function in to single objective function by using the relationship specified. the formulation of objective function is given as,"
"an observable is the result of an observation, performed by a human with or without the help of instruments. we have just used the word \"result\" quite casually, because that's how it is commonly used. but how is this \"result\" obtained? what exactly is an observation? what is the difference between an observable and the observation of that observable? douglas hofstadter has summarized this question on page 633 of metamagical themas:"
"simrep will be proposed as a cml-type foundational basis to help determine whether wbe is solid or has a foundational problem. theoretically simrep asks us to be open to the conclusion we might require different software and hardware platforms, to our current tools, at least for wbe to be capable of operating as an intelligent entity it may have to be, but the conclusion to this question is as unknown as the current state of physics. in summary, simulation, replication, emulation projects (and in between categories) have different priorities and roadmaps, which crossover with each other. it is preferable for the efficiency of wbe if the requirement for a simrep platform, or even many facets of neuroscience are found to be unnecessary. a summary in regards to what we might be able to strip out from the biological substrate are covered elsewhere [cit] ."
"structural alignment is ideal, we computed the deviation between residue positions in an mseqa and the position obtained by structural alignment. we can note that the cumulative error rate in sequence-based comparison, along the sequence, is corrected by the structural alignment. the highest alignment quality for residues in the catalytic cavity illustrates the sensibility of sequence-based alignment to sequence conservation (residues in active site regions are usually more conserved). the curve presents the averages between all proteins of the family and shows errors in the mseqa due to insertions and/or deletions. this result highlights the impact of structural information on sequence alignment quality."
"a scaling problem is the classical symptom that diagnoses the accumulation of entropy in dynamical systems that do not incorporate a causal processor. the system appears to \"work\" for small problems but fails at a larger scale because the excess entropy and uncertainty are not being removed. scaling problems are very common in ai systems."
"for each sequence, we built 50 models and the best model was selected as the one with the lowest objective function. this function corresponds to the total energy of the system, based on charmm all atom potential, and indicates a model with the best fit with the input data (inter-residue distances and residue dihedral angles, i.e. stereochemical constraints)."
information is not processed in the brain the same way it is processed in a computer. there are three different processes going on in the brain. the three processes are concurrent and collocated. the first is the formation of the connectome from highly entropic information that arrives in the brain in the form of unrelated causal fragments from senses and other afferent nerves. we do not discuss this process here.
"finally, the metric for the causal theory is defined by assigning a measure to total order t itself, defined as the sum of the measures of all ordered pairs in t:"
"we hereby propose that a concerted research program is needed, based in black-box brain experiments, specifically intended to bridge the gap between internal life signals and external observables."
"testing will also be included in the gui. testing is very special in causal logic: in short, it works just like a human. what does a child do when information is incomplete? he asks for more. so also does cml. what does a child do when information is contradictory? he asks why. so also does cml. with time, testing will be more of a conversation than the traditional case-by-case testing used in engineering. these features have been discussed, with the help of examples, in section iv \"basics of mmc supervised learning\" [cit] ."
"this experiment is interesting because it illustrates the first use of the mathematical limit within a causal set, hence extending cml to applications with differential equations. we humans can deal with the infinite and the infinitesimal using 100% finite means. we can define the concept of mathematical limit and use it to \"invent\" real numbers, continuous functions and manifolds, integrals and derivatives, and all the powerful properties of continuous things without ever having to write, calculate, display, or draw those infinite things. we always prove the properties of those entities by way of the mathematical limit. we approximate real numbers as integers, we use integers in computers for real calculations, we solve differential equations in time steps, or with finite elements. our means are finite, yet we deal with the infinite."
"but we expect more of a physical entity. we say that a system is physical when it can be observed and its properties measured. we expect the physical system to interact with other systems, and to have states and experience transitions between them, and invariants so it can be observed. we then build a mathematical model of the system and try to find a law that explains the transitions and conforms with (or falsifies) the principles of physics. the causal theory confirms landauer's vision. in the causal theory, structure and algorithms do have an existence of their own. information can be observed, for example in a complex system, and its properties, the structures, are invariant and can be measured. information has two extreme states, causal set (high entropy, fragmented and disorganized) and hierarchy (low entropy and fully organized), and a plethora of intermediate states that correspond to local minima of the action. it can interact with the environment and exchange energy with it, and when it does that a transition happens and its state changes. furthermore, there is a mathematical model, the causal set. and there is a law that explains the transformations, cml, and this law conforms to the principles of causality and symmetry. it would not be possible to attribute all these facts to the complex system, or even to complexity in general, because they are valid for all causal systems, except that black holes are currently under controversy. and there is even a quantum for information: the cause-effect pair."
"such a gui will give neuroscientists unprecedented flexibility for programming language selection. new languages specific to brain simulation will be created and easily interfaced into central causal models without the usual design cost and overhead associated with man/machine communication. it will allow programmers to work in their languages of choice, with the only limitation that an automatic converter for that language to and from causal models must exist."
"the difference between this pure replication approach and a simulated replication as proposed here will be clarified further. as replication tends to be based on the fundamental principles of the natural world, it would be useful to develop a classification between simulation and replication which we will call simrep. simrep would be the neuroscience analogy to simbios and simtk currently used for cells, which rely heavily on physical principles [cit] . although we view simrep as theoretically foundational, we cannot be sure at this stage if it will be as physically foundational as replication experiments or pure science theories like cml. pure replication, does not have to represent every detail, but proposes we will benefit from staying on track with the most general laws of physics, as we can derive principles on intelligence directly from those which can translate across substrates like dna into fully complex systems."
"in the proposed work of subcarrier allocation the crn is considered, where each cr has capability of precise sensing and history management. the results are true for more generic cases. the ga parameter settings considered for implementation is as dr set to value of 265 kbps, pwr is 15 dbm and ber is 10 −5"
"the globalization of the local action propagates causal association to the entire brain. this phenomenon is known as binding. it has a place in history. the binding problem was started by bertrand russell over a century ago, but remained unsolved until now. the failure to solve the problem for so long is directly traceable to the omission of causality the solution to the problem of binding was proposed in the context of cml theory. it was also proposed that binding is a universal phenomenon, and is critically necessary for intelligence because it allows the globalization of associations that would otherwise remain local and the generation of global behaviors that respond to global conditions. but binding by itself is not sufficient to produce intelligent behavior. phenomena of emergence and self-organization observed in many complex systems that aren't intelligent are also due to binding. necessary and sufficient conditions for intelligence are discussed in section 7."
"in neuroscience, the theory is proposed as a universal, mathematical, non-explicitly programmed, computer-readable and human-interfaceable language, with a graphical user interface that will allow neuroscientists to express their observations and measurements and integrate them in all scales, where elements of information are automatically associated and compressed to create associative memories, and high brain function including signals of intelligence emerges from models."
"carlson also proposes that russell and whitehead solved the mind-body problem when they recognized the incompleteness of science and the need to include events of sentient perception in the causal patterns. but that solution is a qualitative one. here we seek a quantitative solution, one that can be mathematically described and implemented on a computer, as needed for a brain emulation or an agi machine. this, russell and whitehead did not do. attempts at solving the mind-body problem quantitatively continued during the better part of the 20th century. the various lines of thought that were attempted produced negative results, and were mostly abandoned after wittgenstein's work. in sections 1, 2 and 3 of his turing centennial paper, evolutionary biologist stuart kauffman [cit], describes this research in extensive detail. but the problem remained unsolved, and has recently acquired critical proportions because it stands in the way of critical technologies such as agi, the semantic web, the internet of things, natural language, image, voice and smell recognition, and many others known collectively as the guaps, or great unsolved automation problems of software engineering [cit] b) ."
"the fact that metric m(t) in eq. (3), where t is a total order of s, is positive definite, is critical, because it solves the famous binding problem. metric m(t) is the sum of many terms, each of which is a positive number. but m(t) is also the action functional. when the principle of least-action is applied and action is minimized, the only way to minimize m(t) is to minimize each term in eq. (3) independently of the others. what does this mean for the brain? it explains the proverbial \"massive parallelism\" of the brain, that has never been fully explained. it means that each neuron can locally minimize its own contribution to the action by making its own connections optimally short, using only local information available to the neuron, without any external direction, and by \"pushing\" its neighbors as needed to optimize its local morphology. it means that all neurons work together, simultaneously, guided by thermodynamic rules alone and not explicitly programmed to create any particular behavior, much less an intelligent or adaptive behavior. it means that intelligence is a side effect of thermodynamics, guided by forces known as causal entropic forces that originate in causality and entropy. these forces can act in both directions. they have been called \"pulls\" or \"net pulls\" [cit] f, §4.2 and 4.3). they have been more recently independently confirmed in cosmology [cit] ."
"discussed in this paper is a new, fundamental theory of physics grounded on the universal principle of causality as its only principle. the theory has only one postulate, the action functional, and one model, a collection of ordered (cause, effect) pairs known as information. it proposes that natural phenomena ranging from the elementary to the complex, and including life and thought, are expressions of the mathematical properties of ordered pairs and must be understood as such."
protein kinases: we selected 33 665 sequences from pfam families pf00069 and pf07714 (protein lengths: 219.36±81.09 residues). a total of 3403 sequences were selected by the length and identity different filters (templates used 2cpk:e for serine/threonine kinase and 1u46:a for tyrosine kinase). the average sequence identity with the templates was 35.96±7.49%.
the greatest strength of causal logic is its power to integrate fragments of knowledge or fact into a functional whole. an attempt to harness that power immediately spawns hundreds of projects in many different disciplines including agi and neuroscience.
"a graphical user interface (gui) for work in causal modeling is proposed. the features will include various forms of input of causal information, such as files and models in other programming languages, visualization methods, and testing."
"the simulation results are repeated 100 times with the 200 gas iterations and the best chromosome is selected for checking the performance of objective function. to demonstrate the effectiveness the, proposed sapr scheme is compared with random channel selection schemes."
"the second process, is the formation of short dendrites. it doesn't matter whether they are made short or somehow become short. the fact is that they are short. this is a mechanical process, but it has a thermodynamic effect on the information stored in the connectome: it minimizes the action in it by removing energy. with the energy, also go the entropy and uncertainty, and the net result is that both the connectome and the information it contains get self-organized. the connectome self-organizes physically, forming hierarchies of neural cliques [cit] . the information self-organizes logically, forming hierarchies of structures. this is, we propose, how cml works in the brain. critical to add credibility to our proposal is to verify that the dendrites are optimally short. they are, indeed (cuntz, mathy, and häusser, [cit] ) ."
"in reality it will be far more complex than this. there will be two steps forward one step back, a mixup of various directions as well as leftfield insights, which feed off and back into each other. we cannot depend on having the ideal sequence, but we propose that the sequence is still important enough there should be ongoing discussion on whether or not a fundamental theory of physical intelligence should be integrated a priority to the wbe roadmap."
"we do all this in the light of this new theory known as causal mathematical logic (cml), or, in older texts, as the theory of detailed dynamics. cml is a general theory of physics, not just agi or the brain, directly grounded on the fundamental principles of physics and nothing else. cml is distinguished from statistical or differential theories that disregard details in the information and may be considered as approximations. cml is also distinguished from the theory of computation, but can be emulated on a computer. cml is a discrete theory. it describes information in full detail, just as it is captured from the environment by a sensor or sensory organ, and carries that detail throughout all mathematical calculations without any approximations or simplifying assumptions. hence, it is not surprising that cml is more general than other theories and reaches detailed conclusions that other theories cannot."
"in the block systems, each block is composed of ordered pairs, taken from the original partial order ω. what happens with the ordered (cause, effect) pairs when the block system is constructed? when both cause and effect are in the same block, the pair is encapsulated into that block. it becomes hidden, \"invisible\" from outside the block. this notion corresponds, exactly, to the notion of encapsulation in object-oriented programming. when cause and effect are in two different blocks, then the ordered pair is induced in the block system itself, and the two blocks become one the cause and the other the effect. hence, the block system itself can be considered as a new, smaller causal set, where the blocks are the elements and the induced pairs the partial order. the entire procedure just discussed can be repeated for this new causal set, and a new, smaller block system is obtained. the process can be continued until exhaustion, and the result is a hierarchy of block systems. the hierarchy is determined by, and only by, the original causal set σ under the axiomatic metric. the hierarchy is a mathematical fractal."
"the second condition is that the memory must be autobiographical, where information acquired by interaction with the environment is stored and does not \"disappear\" by being erased or overwritten by new information. in a dynamical system such as a hot gas in a bottle, the causal space is enormously large, but the gas is not intelligent because states get overwritten: two molecules in a certain state collide and create a new state, but keep no memory whatsoever of the previous one. by contrast, the brain creates new variables for new states, rather than overwriting existing ones. the brain creates new state variables by constantly making new connections to store learned information, and keeping all memories for a very long time."
"the black-box concept is expanded in the next section. the first experiment of this kind was performed by one of us (sp) years ago. it was designed as a research tool at a point where extraordinary self-organization had already been observed in causal sets (by sp) but was not well understood, and it led to the discovery of the action functional given in eq. (3)."
"it has been proposed that, as global binding happens \"unintentionally,\" in the sense that evolution only intended to preserve resources without sacrificing memory and not to create intelligence, it would then be fair to say that intelligence is a thermodynamic side effect of resource preservation [cit] ) . of which, of course, evolution has taken after-the-fact advantage."
"a block system is a partition of the original set s into subsets known as blocks, which have the property of being invariant under all the total orders in t * . in other words, the same partition into compact blocks applies to each of the total orders in t * . the blocks in the block system are the invariants we are seeking. the theorem is proved by construction, as follows."
"black-box experiments are central to addressing the issue of external observation. in the causal theory, observables -that is, algorithms, including waves and oscillations, are generated by thermodynamics, not by computer program. the causal virtual machine we propose generates its algorithms thermodynamically. in a computer simulation of the virtual machine, there is no program, nothing needs to be programmed that is related to the generation of the observables. there is of course a program for input and output, visualization, and for minimizing the action functional. but the life signals emerge as a side effect when the functional is minimized, and not by program. black-box experiments bridge the gap between internal life signals and externally observable life signals."
"the original motivation for this work was sergio pissanetzky's (sp) interest in the automation of the guaps, the great unsolved automation problems of software engineering. the guaps are loosely defined as a class of problems that are easy for humans but difficult for computers to solve [cit] b) . refactoring [cit], or the organization of computer code so it becomes more \"understandable\" to the developer, is one of the guaps. there are many tools designed to help the developer to refactor code in different languages, but in the end, it is always the developer who makes the real decisions, the decisions that deal with the \"understandable\" part. a full solution of the problem of refactoring would entail a solution to the problem of meaning, but such solution was not available at the time."
"most importantly, once the hardware is ready and basic experience in causal analysis is gained, then cml will be ready as a new tool for brain research."
asmc is phylogeny independent and relies on (i) an improved alignment of specific active site cavities based on 3d protein structure alignment and (ii) a classification of these alignments based on information theory. each classified cluster is defined by a unique profile. comparison of these profiles pinpoints sdps correlated with functional intra-family diversification.
"the intent of the first experiment was to discover the secret of the intelligent behavior that a developer exhibits when refactoring code. the experiment had not been planned to be observer-independent or to have rigorously controlled parameters. after that goal had been achieved, the natural next step was a search in the literature for other possible sources of causal input and corresponding output obtained by a human that had been independently obtained and documented in the literature. luckily, there are several sources and plenty of carefully documented cases."
"with a causal machine, it works the same. the machine is trained with some initial information. the machine can ask questions, see for example section v.f \"supervised learning\" [cit] . if the machine has sensors, then it can acquire additional causal information directly from the environment. if the machine has actuators, then it can play with toys, and \"see\" what happens. one day, causal machines will be able to learn from each other, just like we humans do. there is one single reason why all this is possible with a causal machine but not with an ordinary computer: the machine has the capability to bind and integrate information from different sources and create the focused algorithms of adaptive behavior."
"euler equations are a set of three differential equations that describe the rotational motion of a rigid body. euler used differential calculus to derive them from newton's equations, who in turn derived his from experiment. for the purpose if this black-box brain experiment, we consider an imaginary scientist who lived before newton and performed some experimental measurements on rotating rigid bodies, but did not understand his measurements, didn't know newton's equations, and was not able to derive the equations of motion. the scientist expressed his measurements as simple formulas involving real numbers that he could measure, but whose meaning he couldn't understand. he took measurements at short time intervals, and knew that shorter intervals produced more accurate results. that's all the scientist knew. and that's all the causal machine was told, yet it correctly derived euler's equations."
"part iii is directly focused on a particular target: the black-box brain experiments. it addresses immediate actions needed to jump-start a cml project and expand it to a point where cml is sufficiently developed for application in brain research. the notion of black-box brain experiments is defined, the first brain experiment where the action functional was discovered is described, and several other experiments that are either successfully completed or under way are discussed. current limitations, and the steps necessary to overcome them, are near the focus of the discussion. a preliminary project plan is included, and an application for a patent is recommended."
"the causal processor's only function is to calculate values of function e, as defined in eq. (5), for a given causal set. the given causal set provides training for the processors, or one can also say that the processor learns or gains experience from the environment. the causal set is the only contact between the processor and the environment. the processor is fixed. once designed and tested, it will work for all types of problems and it never needs to be modified of re-programmed again."
"a different type of brain experiments involves in-vitro cultures of live neurons ground from animal brains and placed on a petri dish with multi-electrode arrays, also known as brain-on-a-dish experiments, where the behavior of individual neurons can be directly observed. the experiments are designed to observe and measure spontaneous network formation and neuronal behavior. the electrodes detect the action potentials of individual neurons as they spontaneously organize in the culture, create a network, and become active. this experiments were proposed for cml research [cit], § viii.d) . see also [cit] for cultured hippocampal networks."
"asmc, applied to 536 sequences (section 2), detects two clusters in the first tree ramifications: one is composed of 323 gc sequences and the other is composed of the 213 ac sequences (fig. 4) . it detects conserved residues (highest p-value) implied in activity or structure integrity: phe480, phe486, leu502, asp527 and gly595. the positions known to be involved in specificity are detected as sdps (lowest p-value): the glu523/lys938 position and the position cys592 which is substituted by a negative residue at position 1018 in the ac group."
"there follows that every causal physical system has a symmetry of the action, although the symmetry may be trivial in some cases, such as a total order or no order. set t formalizes the uncertainty/entropy value of the trajectories, giving rise to the laws of thermodynamics. set t also gives rise to a groupoid, and groupoids have invariant observables. the existence of set t proves the second fundamental principle of physics, the principle of symmetry, that states that any system with a symmetry of the action also has conserved quantities. hence:"
"part i attempts to establish causal mathematical logic (cml) as a new stable and self-consistent theory of physics, grounded on the greatest experimental base ever assembled and adequately represented by the fundamental principles of physics, and applies to all things physical. also argued in part i is that cml can represent the dynamics of a physical system in as much detail and with as much generality as is possible or necessary, and seamlessly refine the detail as more is learned about the system, in sharp contrast with other traditional statistical and differential approaches."
"where, k s ψ specifies channel history vector of a particular channel k for s number of time slot. the value of s considered is 100."
"while quantities of sequence and structural data continue to grow, only a few methods are using these two kinds of information to annotate proteins at a high-throughput rate. our method, asmc, proposes a new approach to the annotation of enzymes with a high level of precision. indeed, prediction of sdps in active site provides information for understanding protein functions and evolution. the main advantage of asmc over existing methods is the structural alignment of pockets that provides a high-quality comparison of residues located in the active site. comparison of the method with a multiple sequence alignment-based methods demonstrated the limitation of using only the sequence information, due to the low quality of sequence alignments. the conceptual clustering step separates the subfamilies and determines the residues responsible for the specificity. the methodology has been validated with families of known functions and predicted sdps are well confirmed by experimental data."
"it is easy, in relative terms, to convert legacy code or existing brain simulations in computer software to causal models. a conversion module has to be developed that can convert both ways between each programming language of interest and causal models, or alternatively between the corresponding virtual machines and the causal virtual machine. this task, writing the conversion module, has to be accomplished only once for each language. it is anticipated that, with time, readily available conversion modules will exist for many languages. as a by-product, the modules will also allow direct translation between the programming languages themselves."
"when we start up a wbe, the information for the neural components will need to have been compressed into the set of simulation objects and integrated frameworks deemed suitable for them to be run. as the machinery is the information, a temporary dualism between process and information will occur during the phase we derive the design of these objects and frameworks from general principles. hopefully we can resolve this and get our theory of natural information right to predict the foundational methods. if such a theory is solid, the wbe system should require less peripheral support to compensate the wrong foundation, and so transferring biology to a new substrate will have more freedom in the physical world to direct itself with a natural evolution. if we get it wrong, decades spent on development may be resting on a house of cards."
"one of us (sp) has proposed a virtual machine that can be implemented on hardware and has those features [cit] f, sec 4.1). here we review several computational experiments carried out on a personal computer implementation of the machine. the experiments put the theory, as well as its scope, to a test, and all of them demonstrate the emergence of intelligent behavior within obvious limitations."
"statistical significance: a log-likelihood analysis [cit] ) is computed for each msa position. this procedure is a test based on the ratio between the probabilities of two different hypotheses. for each position i of the msa, the frequency of the most frequent residue in a specific cluster is computed both for the individual cluster and all the others merged. the null hypothesis states that the most frequent residue is found with similar frequencies in the cluster and elsewhere so that it is not important for the specificity. the alternative hypothesis states the opposite. the log-likelihood analysis states on the specificity signification (the null hypothesis tends to be rejected with high values). the log-likehood log(l i ) is approximated as:"
"the brain is known to contain many internal feedback loops. we can't explain exactly what their function is, but it would be perfectly possible, as far as this theory is concerned, that they merely serve the purpose of reusing resources in a way similar to how computers reuse resources by \"calling\" subroutines or methods or instantiating objects of a common class to avoid the repetition of code."
"serine proteases: we selected 7256 sequences from pf00089 (193.82 ± 57.23 residues) and kept 6016 after filtering by size (202.85 ± 30.60 residues). the template structures are 1est:a for elastase, 5ptp:a for trypsin and 1ab9: (we joined chains a, b, c and d to form one chain) for chymotrypsin. the retained 1686 serine proteases with more than 30% identity with the templates have an average identity with them of 39.04 ± 10.08%."
"the overall integration of these extremes within erp was then derived as a larger scale thermodynamic \"information engine\" structure in terms of the glia which develop the mammalian brain. such a finding might have predictive power for the questions in glia that surround wbe [cit] . it is early days for such a broad and controversial approach, but in essence if agreement is found, an integrated theory of the brain is proposed to have been derived from the natural mathematics laid out in cml that is consistent with the generally accepted details of the mammalian brains input and output."
"the preceding overview of the causal theory may appear to be quite abstract. but in fact it maps very directly, we'd say surprisingly directly to familiar systems such as the brain or a computer, and actually to all dynamical systems. this map is what allows the central theorem of causal logic to be directly applied to any of those systems, and important conclusions, of the kind that cannot be arrived at by any other means, to be drawn immediately. an article explaining how the abstract causal space maps directly to dynamical systems traditionally understood in terms of state space or phase space, is found in pissanetzky (2012h)."
"whole brain emulation (wbe), has as its priority to take a snapshot of the mind in whatever detail is required and try to engineer a workable running system with translation of the system onto a substrate more robust than dna. the problem of acquiring the wbe data is hard so is a priority. wbe aims to get the copy to run as it was, i.e. remember its past, and have conscious input/output processes. for the copy to be able to proceed with evolving its own intelligence, it may be preferable to have the principles of intelligence to simplify the entire process. so for such an aim, there is often dialogue between wbe and agi."
"because simulation is approaching pure science, we expect it to help us understand the entire system more accurately. whole-brain emulation (wbe) places more intrinsic demands and seeks a system strip down. for example developmental space is not a priority. the wbe system is a copy of our information and we require to restart the processes of intelligence with the least running description. wbe will likely require a subset of the principles which will be derived from tissue simulators, and tissue simulators will benefit from predictions made from the principles of intelligence. it appears then that from a logical sequence point of view the progress for system simplification could be hindered without a natural theory of information and intelligence. the issues here for such a fundamental theory are how it should be derived: (1) from a pure science/replication perspective such as simrep; or (2) recruiting a basic theory like cml to assist with consistency checking simulation output."
"for the purpose of this theory, it is necessary to refine the meaning of the term observable. in physics, an observable is usually a number that can be measured or calculated in terms of a system's state, and that remains invariant (does not change) during the dynamical evolution of the system. this definition is too narrow for agi purposes. it is too narrow even in physics, where the role of the observer in an observation is subject to frequent controversy, revealing the fact that a scientist's sentient perception plays a role in the observation because perception is what provides science with its observational data. that meaning is exceedingly narrow for a work such as our's, where the purpose is precisely to explore sentience, perception, cognition, intelligence, emotions, and other phenomena such as emergence and self-organization that can be \"observed\" and described but are not easy to measure, and to eventually recognize them in the human brain or elsewhere, emulate them on a machine, and make them measurable."
"but the brain does not work like a turing machine. there is no \"human in the loop\" to take care of causality and write a transition function, and an intelligent machine with a human telling it how to be intelligent would not be intelligent. it is necessary to remove humans from the loop. completely, not some of them."
"a finite set s with n elements has no order, and can be represented mathematically in n! different ways, the n! permutations of the n elements of that set. in this case, there are no non-trivial invariant blocks, as no two elements can stick together in all permutations. all block systems are trivial and the invariant quantities are the elements of the set themselves."
"we propose all that as part of our working hypothesis. because invariant blocks solve the uncertainty problem. least-action removes entropy, and removes uncertainty, leaving the blocks that are certain and are needed to make predictions. block systems are well understood in group theory, and easy to compute from a given groupoid. these considerations explain the origin and abundance of fractal structures in nature."
"the required hardware is a causal processor. it consists on an implementation of the virtual machine proposed in section 4.3 of pissanetzky (2012f) on a programmable processor such as an fpga or gpgpu unit. there is still no design for it, but a crude estimate indicates that it may contain more than 100,000 neurons per unit. units are inexpensive and stackable with no penalty paid for time of execution, except that a slight latency associated with inter-unit communications is to be expected. this way, a stack with millions of neurons can be obtained at low cost. it will run an estimated 1 million times faster than the human brain. in time, much faster lsi processors with many more neurons per unit will be produced, and a stack of them will contain as many neurons as the human brain and will execute many millions of times faster."
"in addition to the predicted trypsin, chymotrypsin and elastase subfamilies, asmc identifies a 13 sequence cluster associated with a conserved sdp pattern (fig. 7) . this cluster is composed of sequences annotated as kallikreins, for which we did not use 3d structure information for protein modeling. this result illustrates that asmc is able to predict new subfamilies without the support of a 3d structure belonging to the subfamily."
the causal theory discussed in this paper proposes to represent causal information by means of ordered cause-effect pairs. this is in stark contrast with the use of strings of symbols to represent information in other theories such as the theory of computation and the theory of information.
"it would, of course, be of great interest to obtain the java code itself as the final product. this is simple engineering. it would involve training the virtual machine with the content of a java user's manual. we anticipate that such machines will soon be available, already pre-trained on various languages, and will be of enormous commercial interest. similar machines will also be developed for applications in high-performance computing, such as the one reported in [hon, liwen] . however, for the time being, training a causal virtual machine with a user's manual is beyond available resources. for the purpose of the experiment, the design, not the code itself, was the output to be considered."
"we begin by introducing the mathematics of causal theory. then, in the next sections, we address the profound physical meaning of the mathematical entities and their role in neuroscience."
"the third process is the execution of the algorithm. the causal set is the algorithm, there is no need to \"invent\" one. the causal information arriving from the senses in fragments is the algorithm, and cml does not create algorithms. cml associates like fragments together by forming the block systems. this association or binding happens because the mechanical process of shrinking the dendrites has removed entropy and uncertainty from the information, and has finally generated the invariants needed for semantics, i.e. for us to be able to comprehend and manipulate the information. these invariant algorithms are the behaviors we call adaptive. they execute when the neurons spike, and the chain of spikes sends signals to the muscles. it is all automated, no humans in the loop."
"the resulting oo design is compared with the output of the causal virtual machine, as schematically represented in fig. 1 . as usual, the machine is fixed. the machine is trained with the given causal set as input, and its only task is to minimize the action functional and output the resulting oo design. the machine knows nothing about java, or about the theories of object-orientation or refactoring. the purpose of the experiment is to demonstrate that the virtual machine accurately emulates the high brain function of the developer in the black-box."
protein sequences were selected from the pfam database [cit] and length and identity filters were applied to remove sequences with lengths differing by more than one sd from the family average length or sequences with less than 30% similarity to one of the template structures. pdb identifiers for the templates were extracted from the same database.
"in this study, target sequences were aligned with template structures using the salign module of modeler [cit] . structural alignment differs from sequence alignment methods as it takes into account structural information from the template. this information is used to introduce a gap penalty function that tends to place gaps in exposed solvent and curved regions, outside secondary structure segments, and between stericaly inconsistent positions. the alignment error rate is reduced by 1/3 relatively to standard sequence alignment and this improvement is enhanced as the similarity between the sequences decreases [cit] . to optimize the alignment process [cit], we computed 50 models, for each sequence, using all the structures available for a family [cit] ."
"nucleotidyl cyclases are enzymes that catalyze the formation of cyclic nucleotide monophosphate from nucleotide triphosphate. the guanylate cyclase (gc) group catalyzes the formation of cgmp from gtp and the adenylate cyclase (ac) group converts atp to camp. [cit], the specificity of gc can be modified to ac by two amino acid substitutions of guanylate cyclase (pdb id 3et6:a): glu523lys and cys592asp. the cys592asp mutation abolishes guanidine binding by creating an electrostatic repulsion between the aspartate and the guanine o6. in the ac group, it stabilizes adenine binding by adding a hydrogen bond with n6. the glu523lys mutation creates a hydrogen bond with adenine. other conserved amino acids have been described in these fig. 4 . analysis of the active site pocket residues of (a) guanylate cyclases and (b) adenylate cyclases. sequence logos were generated using weblogo [cit] . for each position of the catalytic pocket 3d alignment (section 2) supporting the classification method, representation of the observed frequency of residues is encoded by the height of the letter. the overall height of each stack is proportional to the sequence conservation at that position. the letters in each stack are ordered from the most to the least frequent so that one may read the consensus sequence from the tops of the stacks. p-values are computed as described in section 2. positions of amino acids in pdb ids (a) 3et6:a and (b) 1ab8:a."
"agi research attempts to confront the difficult issues of \"human-level intelligence\" by stressing on the versatility and wholeness of intelligence, rather than on the parts of which intelligence appears to be composed. agi also proposes that the engineering practice should be carried out according to an outline of a system comparable to the human mind. several current projects on brain mapping, emulation, simulation, or replication, seem to be headed in a very similar direction."
"the general areas of neuroscience where we focused on, were the set of the highest level \"intelligence signals\", the event related potentials (erps). erps are proposed as such signals as they are associated with both information and the complex processing of it. erp signals represent single sensory modalities associated with cortical processes, such as response inhibition, response conflict, error monitoring, novelty detection, object recognition and intentional deviation. multi-modal integration such as context updating, action selection, perception stabilization, maintenance in working memory, syntax processing and generation of expectancies that are associated with conscious awareness [cit] . so erps are our current biophysical markers for the general intelligence processes in terms of how we can measure intelligence from the systems input and outputs."
"where s is a finite set and ω is a partial order for set s. a set is a collection of elements with no order, no value, and no meaning. note also that s is a finite set, which makes causal theory a discrete theory. as we will see below, this is not a limitation and does not preclude the theory from handling continuous functions or smooth manifolds and differential equations."
"we will briefly summarize the difference between these projects, and why the area of neural replication needs to be clarified as having a validity to bridge the gap between simulation and agi. also we will attempt to clarify why agi is directly relevant to wbe or brain simulation at all. most importantly we aim to present in a more sequential method the way in which all these disciplines will be required for the success of wbe, and how a natural theory of information is both consistent with current neuroscience, and may posses a proposed explanatory power for neuroscience discoveries not attainable by other means."
"average distance of cps + sdps to cognate ligand onto related family members crystal structure: for 85% of the cases, average distance between cps + sdps and ligands is smaller than distance between odps and ligands, showing that asmc recovers most functional residues. these distances have been compared with results obtained by sdpsite [cit] for cps + sdps. for 80% of the cases, asmc performed better (see supplementary material for detailed results on 42 families)."
"the two classes are characterized by two consensus sequences, rdlkpen present in serine/threonine kinases and rdlaarn in tyrosine kinases. figure 5 shows the active site pocket residue composition in each subfamily found by our method. [cit], predicted cps (lys72, glu91, arg165, asp 166, gln171, asp184 and phe185) are all involved with catalytic activities of both the two families. the three discriminating amino acids in patterns are the three residues scored with the lowest p-value: ala254, ala255 and arg256 for the tyrosine kinases and lys168, pro169, glu170 for the serine/threonine kinases. these residues are known to be involved in the differentiation of substrate specificity. other residues have been described as determining specificity [cit] . the thr201 and tyr04 residues are close to the p + 1 [cit] . in this study, we focused the analysis on residues of the active site pocket and we did not extend the analysis to the p + 1 loop. a third group composed of 237 sequences, defined by a very well conserved pattern of residues, has also been identified by asmc. annotation of sequences identified them as epidermal growth factor receptors (egfr). in our classification, it appears as subfamily of the tyrosine kinases."
"just as the brain is considered as a black box with an input and an output, any functional part of the brain can as well be considered as a black box with an input and output. this idea gives rise to the notion of brain analysis, discussed in section 9.1. algorithms and a computer implementation are discussed in section 10."
"although we presented results for known protein families, the method can provide valuable insights in the study of families with unknown functions and can serve as an input to elucidate new enzymatic activities. this is a real challenge as 3000 above 11 912 families of the database of sequences families (pfam-a) are referred to domains of unknown functions (dufs). among them, about 250 families have at least one structure deposited in pdb. asmc can be used by screening and searching for homologous patterns in an active site profile database associated to an enzymatic activity. another interesting perspective of asmc patterns is the improvement of de novo function prediction through virtual screening approaches. sdps and cps can be used to improve selection of plausible configurations of substrates in the active sites by pinpointing important substrate/enzyme interactions."
"the task of refining a causal model is very much like teaching a child. first, a teacher provides some knowledge to the child. the knowledge usually comes as an algorithm, i.e. in causal form. then she asks the child to solve some problem where that knowledge is necessary. if the child fails, the teacher may detect what is missing and teach the child some more. the child can also do it on his own, by asking questions or playing with toys. given the means, the child can acquire the missing information on his own. all this is possible because the child has the capacity to integrate additional information automatically, without the need for a software developer."
"in part iii, the notion of black-box brain experiments is introduced. the first experiment of this kind and the discovery of the action functional are described, and several other experiments that are either successfully completed or under way are discussed. emphasis is placed on the variety of applications and the wide scope of the causal theory, because they attest to our conjecture that the causal theory is indeed a theory of intelligence and of the brain, and has as a consequence a scope and range of applications at least as great as our own cognition. current limitations, and the steps necessary to overcome them, are near the focus of the discussion. a preliminary project plan is included, and an application for a patent is recommended."
"set t also describes the symmetry of causal set σ. in physics, we say that a physical system has a symmetry of the action when it can be described in more than one way in the mathematical model. the total orders of s listed in t are, precisely, all the ways that causal set σ can be described. hence, we have an important conclusion:"
"a concerted program based on black-box experiments is proposed to make more life signals externally observable. the program should be directed, of course, to interests in neuroscience, although applications in robotics, in software engineering, such as solving the guaps, and some general-purpose applications can also be considered and may attract additional funding."
"where r d, represents data rate and c d indicates the data rate available on the channel. p, indicates transmission power on a give channel and max p maximum power available to the cr user. average ber is represented by minber p is the minimum value of ber. max i, indicates maximum interference limit to the pu. η is the full opportunity for the cr on the given channel and max η represents the maximum value of the opportunity index. the average power factor ( µ ) representing the power required to transmit the data in a given time slot."
"the same input information is separately supplied to a causal virtual machine, such as the one proposed in section 4.3 of pissanetzky (2012f) . the resulting output is considered to be the \"prediction\" and is expected to be identical to the measured output."
"we carry carlson's first argument further, beyond science and into everyday life involving intelligence and adaptive behavior, and propose that an explanation for intelligence and adaptive behavior will not be obtained until the sentient events that connect observations with observables are quantitatively reduced to causal patterns and included into our science. we propose that causal association of elements of knowledge binds them together and provides the causal link between observations and observables we are seeking. and we propose that causal association is formalized by the theory of cml discussed in this paper and other previous publications, and that, consequently cml is the quantitative solution to the binding problem and the mind-body problem, as well as the guaps."
"ga uses three different operators i.e. selection, crossover and mutation to direct the population towards convergence of the global optimum. naturally these initial guesses are held as binary strings of the true variables, although an increasing number of gas use real valued encoding. the subcarrier allocation with parameter reconfiguration (sapr) system is centered around dsm with a hypothesis that inputs are provided either by sensing information from the radio environment or by the cr user. the cr senses the information from the environment which serves as initial population for the genetic algorithm (ga). after that the receiver engages in decision making process in order to provide new spectrum allocation demanded by the user. a new tps gene inside the chromosome is added to facilitate the better qos in terms of data rate, service time (delay) and minimal interruptions (channel switching). the tps gene reduces number of retransmission because of collision with pu and accordingly saves the transmission power. the frequency band (fb), power (pwr), modulation method, (mod) bit error rate (ber), data rate (dr) interference with primary user (iwpu) and tps are the major genes considered, and the combination of these genes provide 30 bit solution (chromosomes)."
"the new theory described in this work is different from other existing theories, and the conclusions and predictions are also different and must not be judged in the context or by the tenets of other theories. they must be judged only by way of making predictions and comparing them with experimental observations. the theory itself is judged by lee smolin's three rules: a theory, in order to be a scientific theory, must be confirmable, falsifiable, and based on the simplest hypothesis that explains the phenomena of interest. the third rule is also known as occam's razor. this theory is confirmable because experiments that confirm it are possible, and some have already been completed. it is falsifiable: one single experiment that is proved to contradict the theory will falsify it or some part of it. and the hypotheses are the simplest, as we explain below."
"families: arg590, leu593 and phe594 in gc and their counterparts in ac (gln, ile and trp). mutagenesis experiments could not identify the roles of these three amino acids in substrate specificity."
"causal modeling is very different from software development. in software development, the need to provide all information is emphasized, the reason being that a usually large team of developers, analysts, and managers is awaiting the information to begin a long and complicated process where \"objects\" are designed by the human analysts, architectures are proposed by designers, and software developers convert them into code, which is then tested and adjusted for missing features through intense interaction with subject-matter engineers. any changes of the initial information are considered a major problem, and placed under change management to assess the additional cost, giving rise to a very costly iteration."
"this paper is of a multi-disciplinary nature. the reader will find some sections interesting and others that may not be, depending on that reader's particular interests and background. another reader will prefer a different selection of sections."
"there is no need to describe the system in full detail. quite on the contrary, it is recommended not to. it is better to use some coarse information to start the model and gain some experience. later, additional information can be entered to refine the model. after all, making changes in cml is very easy."
protein kinases are a family of enzymes that transfer phosphates from nucleotide triphosphates (usually atp) to proteins [cit] . two broad classes have been characterized with respect to substrate specificity: tyrosine kinases and serine/threonine kinases.
"following the success of the foundational black-box experiment and the discovery of the action functional, an effort was initiated to provide additional experimental evidence that is more advanced and observerindependent. software was developed for that purpose."
"contemporary mathematical philosopher carey carlson views observation as a sentient occasion of human perception. he argues that science delivers only the bare causal pattern of events, but that among these events are sentient occasions of human perception, which provide science with its observational data. he continues to argue that the events of human perception are routinely excluded from the causal patterns delivered by science for the simple reason that they are considered as mental events that cannot be explained, and that a coherent view of the world will be obtained only when those sentient events required for the causal patterns are included. we concur."
"in the causal theory causality is recognized as the supreme principle of nature, and the theory is derived directly from the fundamental principle of causality, that effects follow their causes."
the ideal sequence at its simplest form is : (1) agree on a starting basis for the natural principles of intelligence such as cml; (2) run experiments to determine their consistency with simrep; and (3) use derived principles for simulation objects and frameworks for wbe.
"the impact of alignment quality of asmc over classical sequence global multi-alignment is illustrated in figure 3 . assuming that the fig. 3 . divergence in residue positions in sequence alignment (mseqa) and structural alignment (msa). a total of 2201 sequences from family pfam pf00211 (nucleotidyl cyclases) were aligned using clustalw, and by the structural alignment used in asmc. the curve presents the deviation average between the methods for all the proteins of the family. dots show the positions of residues in the multiple sequence alignment that correspond to the active site residues according to fpocket prediction [cit] . for more clarity, they have been placed on top of the error bar. for instance, position 938 is mismatching in 200 sequences resulted of the mseqa. this result highlights the impact of structural information on alignment quality, correcting cumulative sequence alignment errors due to insertion/deletion misplacements (x-axis: mseqa positions; y-axis: number of sequences that present divergent alignment between the two alignments)."
"consequent with the wholeness of intelligence, and the universal scope of the proposed language and theory, topics found in this paper, as well as the examples selected for the experiments, touch on agi subjects of interest not only to neuroscience but also to other disciplines. this work is, in essence, strongly multidisciplinary."
"part ii attempts to establish cml as a theory of the brain, under the working hypothesis that the brain is physical and intelligence should, and can be explained by physics. the argument in part ii posits that the brain in its capacity as an information processing device is a causal thermodynamic device, fundamentally different from any turing computer, and cannot be studied or understood in the context of the turing theory of computation unless causality and thermodynamics are appropriately simulated. also argued in part ii is that any physico-mathematical description of the brain must be carried out in the context of a theory that supports full detail, rather than other theories of physics or of the brain where simplifying assumptions are made, such as statistical distributions for statistical methods or assumptions of smoothness and differentiability for differential methods."
"an even more extraordinary feat of human intelligence is its ability to abstract sentient evidence and configure it as a formal theory that allows complex predictions to be made. more precisely, the unparalleled ability of humans to construct theories of nature. however, as our working hypothesis describes the brain as a thermodynamical implementation of causal mathematical logic, with the ability to formalize its own abstractions and create meaning the same way intelligent humans do -even scientists -it is therefore appropriate and necessary to test whether such an ability does indeed pertain to cml as well."
"in physics, observables must satisfy certain transformation laws that relate observations made under different conditions or by different observers. for example, tensors are used in classical physics because they are invariant under rotations of the frame of reference. the generalized observables we are proposing in this section must also satisfy such laws. the observable 'mother' in hofstadter's example must remain the same even if the 100 million dots change due to light conditions, distance or angle of observation, body or eye movement, or the presence of other objects, as long as enough information is captured to make the recognition. this is a critical requirement for an observable, and it is met by cml, provided, once again, that enough geometric information is captured to make the assessment. in physics, the invariance of an observable must be proved by a closed mathematical argument. in the causal theory, it is proved by cml. the grouptheoretical argument used for the proof of the central theorem is the closed mathematical argument."
"we believe that any attempt to explain intelligence without first understanding the principles of causal information will necessarily revert and lead back to the need to understand that information first. it is just as impossible to understand the brain, or to understand agi, without first understanding information, as it would be to understand a lubrication system without understanding oil, or a steam engine without understanding steam. or understanding a computer's hardware without first understanding the transistor. hence, the attempt to explain intelligence would be much easier if carried in the correct order."
"programmers will still be needed to enter observational information into the causal models during their development. they will operate the gui, test and maintain the system, upgrade it when necessary or if better hardware becomes available, and provide feedback to neuroscientists. but programmers have preferences."
"the proposed scheme addresses the major functions for cr user from spectrum decision perspective in crn. the proposed scheme is based on ga and implemented considering all important issues counting channel characteristics, with assuring qos issue without hampering pu operations. the future accessibility of channel for cr users without interfering pu is formulated, for selecting the optimal channel. the solution is developed through ga which performs genetic operations and then cr users reconfigure their parameters according to the allocated subcarrier. in connection with the weights given to particular objective functions different transmission modes are identified and their effectiveness is discussed. results validate that, the fitness function of the individual parameters increases with increase in number of generations and reaches the optimum solution among the pool of available solution. all the parameters considered for fitness function are independent of each other. the major outcomes of the scheme are observed in the reduction in the number of channel switching, and the increase in channel capacity and data rate. the advantages make the sapr scheme suitable for power sensitive applications like cognitive radio sensor network."
"part ii attempts to overcome the abstractness of part i and expand the reader's vision in a practical direction. it's purpose is to constrain the wide scope of the causal theory to practical problems and place the reader into a practical mindset. this should help to better understand its place in today's technology and engineering, and its role in solving problems that can't be solved without it. part ii tries to establish cml as a theory of the brain, under the working hypothesis that the brain is physical and intelligence can and should be explained by physics. the argument in part ii proposes that the brain, in its capacity as an organ in an organism, is best understood as a causal analog system, specialized in processing causal information, and fundamentally different from any turing computer. that intelligence is not intended in the design of the brain but emerges as a side effect of resource preservation. that any digital or analog computer, or even an appropriately designed nanomaterial, that stores information in a causal format and adequately simulates the analog behaviors of neurons, but is not explicitly programmed to simulate the brain's corresponding cognitive response, will nevertheless cause that correct cognitive response to emerge. and that any physicomathematical description of the brain must be carried in the context of a universal theory that emphasizes the versatility and wholeness of intelligence, rather than other theories where simplifying assumptions are made, such as statistical distributions for statistical methods, or assumptions of smoothness and differentiability for differential methods, or the encoding of causality in a string that requires a human to deal with the encoding protocol."
"separately, the same causal input is supplied to a cml virtual machine. the machine is extensively discussed in section 4 of pissanetzky (2012f), but is not covered here. the machine knows only the input information, but knows nothing about the device in the black box, is not an emulation, simulation, or replication of the device, and contains no computer and no computer program. the machine is a causal processor that operates thermodynamically in accordance with the causal theory. the machine is equivalent to function e discussed in section 4.1."
"the tyr172trp position, which presents a low p-values in chymotrypsins and also elastases, has been described as determinant in the trypsin/chimotrypsin conversion [cit] .the treedet (clustalw) method predicts the following discriminating residues: trp51, his91 and leu155 which do not include the four found to be determinant (172, 189, 216 and 226) . [cit] detects the positions 189 and 226 but not 216 and 172, discriminating between trypsins and chymotrypsins. positions 121, 137 and 164 are also detected and we have no experimental data on these residues."
"arginine deiminase) while pfam classification has grouped them. in the remaining 30%, enzymes can manifest infidelity of molecular recognition. many enzymes can promiscuously catalyze reactions or act on substrates, other than those for which they evolved [cit] . this could be the case for enzymes belonging to families pf00128, pf01112, pf01135 and pf01712. for pf00128, [cit], the same enzymatic activity seems to have evolved independently on two separate branches of the phylogenetic tree. for pf01112, asmc was not able to separate β-aspartyl-peptidase (ec 3.5.1.26) and aspartylglucosylaminase (ec 3.4.19.5) activities. for pf01135, asmc has mixed n-acetylglucosamine-6-phosphate deacetylase (ec 3.5.1.25) and n4-(β-n-acetylglucosaminyl) -l -asparaginase (ec 3.5.1.26), which are very promiscuous reactions. in pf01712, asmc has been able to separate pyrimidine and purine kinases, but was enable to discriminate guanosine to adenosine, and cytidine to thymidine specificities ."
"we differ in part from the stated vision of agi. our vision is that very simple machines can be built, with a basic, repetitive structure, that are very different, much simpler, and much faster than the brain and yet can learn by training to a point where they exhibit the brain's full functionality. we treat the brain, or suitable parts of it, as an implementation of the causal theory, with the caveats that many different implementations may be possible and that they do not necessarily reflect all the capabilities of a theory. yet, we take the brain as our primary example and source of inspiration, because it is the only known physical system that is intelligent and we can observe. in a sense, the brain defines intelligence and provides a point of reference for our work."
"function e was introduced in pissanetzky (2012f) . hence, a unique correspondence exists from each causal set in c and each fractal hierarchy in h. to each causal set there corresponds a hierarchy. the notion of observable was generalized in section 3. based on this generalized definition, the central theorem of the theory of causality can be stated as follows:"
"the fact that causal logic acts on the algorithm has been interpreted as an entropic \"pull\" on the trajectories themselves [cit] f), the effect of which is to select trajectories and create intelligent behaviors. the net effect is that the thermodynamic force of an entropic nature acts on the trajectories and creates intelligent behavior. more recently, the same interpretation has been re-discovered and again associated with intelligent behavior, this time in a cosmological environment [cit], and renamed as causal entropic forces, all of this apparently independently and under the cover of a complex formalism that hides the essential simplicity and nature of these forces."
"we argued above that the brain is an organ and that its behavior is causal and analog, just like any other organ in the body. as an organ, it works continuously, and we are not aware of its doings any more than we are of what our liver is doing. the organ part corresponds to the unconscious, or to the host in a host-guest model [cit] . the organ part is of a causal thermodynamic nature. its task is to organize the guest information by associating causal pairs and making the memory associative. it encompasses the entire brain, every neuron is constantly trying to associate causal elements represented by its dendritic connections. neuron spikes are not involved. this task is carried out constantly, without effort, and without us being aware of it or being capable of controlling it. it corresponds to the legendary perception of the brain as a massively parallel \"computer,\" except that \"massively parallel organ\" would be more accurate. this task is not computation, it is not related to computation, and is fundamentally different from any turing machine. however, the connectome itself, when considered as a network, is an algorithm, mechanically segregated from the causal input by the binding process, where the neural cliques correspond to behaviors. when the neurons fire, the behaviors execute, for example the motor signals that pronounce \"mother\" in the example above are sent to the muscles."
"(1) that cml, as implemented on the causal virtual machine, can indeed duplicate high brain function in humans. (2) that cml indeed creates semantics and meaning in the same way we humans do. the whole purpose of techniques of objectorientation and refactoring is to improve the understandability of machine code for human developers. this task can only be achieved by creating meaning in the same way humans do. (3) that cml is the key for both agi and brain research. it is not about computational efficiency. the point is that, as confirmed by prediction and confirmed by cuntz, mathy, and häusser [cit], the human brain actually runs cml in the unconscious, and the structures and meaning arrived at by cml are the same as the structures and meaning arrived at by humans."
"serine proteases are proteolytic enzymes involving a catalytic triad composed of a nucleophile serine, an electrophile aspartate and an histidine as a base. trypsins hydrolyze peptides harboring arginine or lysine at p1 position, chymotrypsins act on large hydrophobic residues at this position and elastases act on small aliphatic residues. this substrate specificity is associated with structural changes in the s1 binding pockets: the trypsin asp189 residue accounts for the preference for positive residues. the hydrophobic property in chymotrypsin is due to a modification of the aspartate to a serine residue. in elastases, the pocket is occluded by val216 and thr226 (both positions are occupied by a conserved glycine residue in trypsins and in chymotrypsins), that accounts for the preference for small aliphatic residues. asmc separates trypsin, chymotrypsin and elastase subfamilies (fig. 6 ). the lowest p-value position in the trypsin subfamily is asp189 that is involved in arg/lys tropism at p1 position. this single position fully explains the trypsin specificity when compared with the other two families. this position is changed to serine for chymotrypsins and elastases with low p-values. the other known catalytic positions 216 (glycine) and 226 (glycine) are coherently scored with high p-values in the trypsin subfamily as they are determinant for chymotrypsin and elastase activity. val216 and thr216 residues may be substituted by glycine or serine in the elastase group. all sequences of this group are annotated as elastase by swissprot / uniprot and our method did not identify a subgroup specific for these variations; these observations suggest that these changes do not modify the elastase specificity."
"black-box brain experiments can also be carried in reverse, that is, start from the given output and calculate the input necessary to produce that output, which is equivalent to application of function e −1 . this approach provides additional information, helps with causal studies, and it may be the usual case when not all input causality is known."
"as every groupoid, groupoid g has a unique group-theoretical block system. a block system is a partition of s that remains invariant under all the permutations in π."
"a schematic preliminary project plan has been prepared in the supplementary material [cit] e) . the project only covers immediate actions needed to jump-start a project on cml and expand it to a point where cml is sufficiently developed for application in brain research and agi. the project includes requirements for hardware and software. the software can be developed separately if preferred, but it will be difficult to test or make it operational without the hardware. hence, the hardware needs to be developed first."
"the notions of interaction and transfer of information by way of ordered causal pairs has led one of us (sp) to consider causal pairs as dynamical entities [cit], thus giving them a physical meaning beyond mathematics. in this new context, information, hence energy, flows from the causes to their effects and is what makes the effects come into existence and become available to serve as causes for other effects. we refer to this dynamical process as the execution of a causal pair. hence, a collection of causal pairs is an algorithm, and we also say that the algorithm executes when all pairs have executed and all elements of s have been visited or initialized by the flow of information. the notion that a causal set when considered as a dynamical entity is an algorithm answers the fundamental question about the origin of algorithms [cit] f, sections 2.4 and 2.5). the algorithmic dynamics corresponds to the mathematical search process discussed in section 4.1 of this paper."
"once the hardware is functional, there will be many projects that can start simultaneously and run in parallel by different teams, depending on priorities. some ideas follow:"
"but communication between people and machines is difficult, and slow. the turing test has not been met, and technology is nowhere close to meeting it. people/machine communication is difficult because machines do not implement function e and can not calculate observables from information and preserve their meaning the way humans do. the turing test will be met only when function e is implemented on machines. then, machines and man will be able to communicate efficiently."
"here we will summarize current work [cit] ) that contains more neuroscientific detail about how it is possible to propose that cml theory has a solid consistency with neuroscience, and help to complete halted projects where the progress had, as is common in neuroscience, fragmented to a dead end. this is not a selective process, where we take parts of neuroscience to fit our theory. the cml theory and its principles appears to make sense of the most general aspects of neuroscience that are currently agreed on, but the integration of these to a complete theory for the mind had remained incoherent. its not proposed this is the final solution either. there may still be deeper general aspects of neuroscience to be found, so we can not be sure, but at this stage the mammalian system has been taken apart and studied repeatedly on so many levels it can be proposed we have a good general idea of its input and output processes. there are still macroscopic unknowns such as the glial system and principles of neuro-development for which a covering attempt is produced in light of current data [cit] . as the sub-neuron detail and genetic substrate are still not completely mapped there is less certainty and we have to remain open to concepts such as neural coding factors that could affect structural conformity in the neurons cytoskeleton. there may also be wider ranging genetic and hormonal aspects effect on the brains primary components not yet known."
"the greatest strength of a scientific theory is that it can be falsified, but has never been. a theory is scientific only when it satisfies lee smolin's three conditions: it must be confirmable, it must be falsifiable, and it must be parsimonious. the third condition is also known as occam's razor for competing hypothesis. causal mathematical logic (cml) is clearly confirmable. all the experiments of the previous section confirm the theory. the theory is parsimonious. it has only one fundamental principle, causality; only one hypothesis, the action functional; only one physical system, information; only one space, causal space; and only one law, the logic. and the theory can be falsified. any of the experiments could have falsified it. but the theory is weak. it is weak because of practical limitations that have permitted only small-scale experiments, and have limited the range and the scope of the experiments. this project plan offers the steps that need to be immediately taken in order to make the theory stronger by trying to falsify it. there are only two outcomes possible: either the theory is falsified and discarded, or a stronger theory will emerge with all the technical means to undertake large-scale developments in neuroscience."
"we have insisted on our views that the brain is the only intelligent system we know and we can observe, and that in some sense the brain is not only an example of intelligence but also a measure for it. in particular, the brain's memory has been recognized as necessary for prediction and intelligence and the subject for studies and imitation. however, these studies traditionally focus on just two types of memory, the species-wide dna, and the individual-specific learned memory of the connectome. we propose that this context is too narrow for intelligence. we propose this on the grounds that intelligence is wider than a species -who would say that a dog is not intelligent? -and even wider than life. in fact, phenomena of intelligence share many features in common with phenomena of self-organization and emergence, observed in inanimate systems and studied in complexity science. we propose that a third memory is necessary to account for such generality, that is universal and not inherited in dna or stored in connectomes, and that this third memory is matter itself. in fact, matter acts as a memory inherited by all things material, where all the properties of matter are stored, and used to determine its behaviors -the phenomena of nature. the causal theory incorporates the third memory explicitly at the precise point where the principle of causality is incorporated. at this time, the causal theory appears to be the only theory to recognize matter as playing a role in intelligence."
"programming languages constantly change. research demands come in a great variety but must remain flexible. this constant pressure for change is hard for programmers, and good programmers are hard to find. such misfortunes are alleviated by the proposed gui and conversion modules. programmers will not have to learn \"causal modeling\" as if it were yet another programming language. they will be allowed to work in their own preferred languages, and in a stable environment. the object models for their programs will be done automatically by cml via the conversion modules, specific to their preferred language. multilanguage teams of developers will become common in following years. with causal modeling, programmers will be relieved of much of their non-essential work. they will be able to better concentrate in their specific task, which is to supply new information to computers. they will become more efficient and productive as their work simplifies. there will have to be a transition period, however, these changes cannot happen immediately."
"for the purpose of the black-box experiment, we extracted the causality from that scientist's expressions and reduced them to a causal set with 21 elements and 18 ordered pairs. as usual, the elements have no meaning. after causal processing, three difference equations are obtained, where the time step appears as a factor. the three differential euler equations are obtained upon taking the limit for infinitesimally short time steps. the experiment is described in detail in the supplementary material (http://www.scicontrols.com/publications/euler.pdf)."
"average distance: sdps, cps and ops residues have been positioned onto the crystal structure bound to the ligand(s). for each category, the average distance between residues and the ligand(s) [cit] . the average distance is defined as the sum of the minimal distances between residues and ligands divided by the number of residues. the minimal distance is the distance between the closest atoms of one residue and the ligand(s). as ops residues are not supposed to be catalytic, functional or binding residues, it is expected to have longer distances for ops than for cps or sdps residues. sensitivity (se) and specificity (sp): the set of residues in contact with ligand(s) has been extracted from the holo structure and are defined as positives residues (i.e. functional residues). the remaining residues are defined as negatives. we considered that a residue is in contact with the ligand when its minimal distance is smaller than 5 å. se is the ratio of true positives (cps or sdps) to true positives plus false negatives. sp is the ratio of true negatives to true negatives plus false positives."
"results were compared with the treedet method that identifies six clusters. the main one is composed of 19% of the sequences (mainly but not only of gc), and the second one includes 12% of the sequences (mainly but not only ac). the remaining sequences (69%) are divided between four clusters. eighteen determinant positions are predicted (gln880, tyr882, ala890, leu912, ile919, val934, glu935, ile937, lys938, val1009, lys1014, tyr1017, asp1018, ile1019, trp1020, val1024, phe1074 and val1075). only one of the two experimentally validated residues (asp1018) was detected with a poor conservation rate. in this example, asmc performed better for subfamily division and sdp prediction. one should note that the treedet analysis combines both the mseqa building (clustalw slow/accurate alignment) and the method itself."
"we notice that senses in animals and sensors in machines always capture causal pairs, where the cause is in the environment (dots of light in the hofstadter's example) and the effect is in the system (spikes in the optical nerves). this process, when viewed as an interaction where information is acquired from the environmentthe cause -and affects the internal state of the system -the effect -is the essence of the notion of learning. hence we capture the pairs right there, at the sensory organs or the sensors, and we follow the causal chains they give rise to inside the system, be it the brain or in a machine."
"when a partial order ω is added to s, then the partial order partially breaks the original symmetry, leaving a lower symmetry. the lower symmetry can give rise to both trivial and non-trivial invariant blocks. the proportion of non-trivial to trivial is not known, and is an important question that will have to be theoretically addressed. however, judging by the abundance of invariant observables in life and thought, the proportion must favor non-trivial invariants very strongly. there must be a very large number of them, but whether that number is infinite is not known. this conclusion can be weighted in relation with a generally accepted \"law\" of physics: the discovery of one non-trivial and hence physically meaningful invariant equals one nobel prize."
"limitations discussed elsewhere and related with the need for simple but special hardware have prevented us from undertaking any large scale attempt at answering the question. in a very small scale, and only as a proof of concept, one of us (sp) has solved the euler equation as a black-box brain experiment with a human scientist in the black box, and a causal virtual machine trained with a causal set that describes the causality involved in a simple measurement, but knows nothing about physics or mathematics, or even differential equations. [cit] . the results are available as supplementary material [cit] d) ."
"this metric is introduced in the causal theory by axiom. the metric assigns a value to each total order in t . as order t conforms with ω, measure m is always a positive integer, and so also is measure m(t). hence, m(t) is a positive definite, monotone metric, and only the empty set has a measure of 0."
"when turing introduced the use of a string to represent data, he separated reality into two parts, the string with the data, and the causality that controls them, and left the causality part to be taken care of by human developers. later, the von neumann stored-program architecture put data and instructions in the same memory, but the human developers remained in charge of the instructions. this was good for efficient computation, but left the humans in charge and in the loop. shannon's information theory followed suit. data is stored in a string, humans deal with causality. a shared communications protocol is assumed to have been agreed upon by the humans, for example a tv set comes with a controller that tells it what to do with the incoming stream of data. the tv signal alone can not produce a picture. the shared protocol is good for efficient transmission, but humans were again left in the loop and the machine as their slave."
"nothing has been said yet about sensory-motor integration, an all-important topic critical for gaining understanding of the operation of the brain. integration is the key here. integration is precisely what cml does."
"we identified several applications of patterns proposed by asmc. cps and sdps positions outline residues of active site pockets that are under selective constraint, conserved or correlated to subfamily differentiation. these residues, used as a geometric pattern, can be used for screening structure databases to identify new candidates for specific functions. asmc can also be used to classify new sequences, belonging to families with at least one structure available, in order to annotate them into functional subfamilies."
"what are the erps composed of? this is a complex area of neuroscience. for more easy terms for cross discipline use here they contain a spectrum of two extremes of signal (spreading waves and oscillations) for which there are sets and coalitions of different frequencies of these, as well as signal types where the line is blurred between these two extremes. the more detailed analysis for this area of erp [cit] revealed the thermodynamic theory and the associated definitions with cml are surprisingly consistent across the translation from natural mathematical language to our full neuroscience complexity. [cit], who sought to define the entire system thermodynamically in terms of its resting state. the action functional is really now just the proposed current implementation attempt that tries to codify the principles of causality, least action, symmetry and thermodynamics to information for the entire system. it is very foundational, yet the same predictions for neuroscience to both authors arrived later. the power law for cuntz neurons (least action) was predicted as a result from the minimization of action within the block system. coupled oscillations in the brain are a function of entropy which are described by cyclic permutations, where the use of the cyclic permutation in the functional predated these conclusions [cit] ."
"an access policy can be expressed by an access tree, t, with and and or gates by using, respectively, 2-of-2 and 1-of-2 threshold gates. a user, u, is permitted to access the data, if and only if the attributes of u satisfies t which is attached to the encrypted data [cit] . the access tree used is converted into a boolean formula. we extend the boolean formula to include an attribute element, which is expressed as \"name: required value\". for example, if we allow a phd or a master student at the school of computer science, university of"
"assuming that md knows url of an accessed file about f id in cs, the data downloading and decryption for md consists of four steps as follows:"
"in this paper, we mainly focus on our proposed lightweight attribute based encryption scheme (labe) for mobile cloud assisted cps. based on labe, the mobile device can exploit the benefits of ciphertext polity provided by abe."
"the rest of paper is organized as follows. we review the related work on location and trajectory privacy in section ii. in section iii, we present the overview of our system. in section iv, we propose a novel attack model based on tensor voting theory. in section v, we give the formulation of the privacy maximization problem, derive an upper bound of it and illustrate a heuristic algorithm to feasibly solve the problem. we formulate the energy cost minimization problem in section vi. in section vii, we analyze the performance evaluation. finally, we draw conclusions in section viii."
"to allow data owners to enjoy fine-grained access control for their data stored on these semi-honest cloud servers, an access control mechanism with data confidentiality protection against unauthorized external as well as internal entities is needed. such symmetric encryption algorithms as advanced encryption standard (aes) can be used but they are difficult to distribute keys to the intended data users. identity-based public key algorithms also have their limitations in the mobile cloud for the multicasting requirement [cit] . these limitations are, firstly, prior to encrypting any data the identities of every data user to whom the message will be delivered must known. secondly, the data owner must know the public keys of his/her data users. once data are encrypted, the data cannot be accessed by any new users."
"respectively. we can see that the optimization improvement performance becomes more and more obvious with the attribute number increasing. when the attribute number equals to 50, the decryption time of labe is 177 ms, which still is a good accepted response time for md."
"illustration of token refinement. figure 3 illustrates an overview of a typical tensor voting analysis for the simple case of points in 2-d. analysis begins with no information at the input sites other than their locations. we create a token at each input site, according to the second order representation, initialized with a unit ball tensor indicating that no separation of the normal space from the tangent space is yet known. the first step of tensor voting, named as sparse voting, which is used to communicate information among token locations, refined tokens have encoded saliency and preferred directions of normal space at the input sites. major and minor axes of the ellipse in figure 3 align with the preferred normal and tangent directions, respectively. the difference between the major and minor axis lengths represents the degree to which structure at the token is curve-like. in addition, the outliers tend to have lower saliency and are less curve-like because they are unorganized and unlikely to conspire to form a false structure. the second step of tensor voting is dense voting, which means the tokens cast vote to every neighbor location regardless of the presence of tokens. after these two steps, we can get a dense saliency figure which shows the map of saliency."
"in labe, adversaries come from external or any of the internal entities, including m d, cs, as, and ep s. we assume that md is untrustworthy, and cs, as and ep s are semi-trusted, in that entities behave honestly according to the functional design, but, in certain situations, these entities may try to acquire private information from the users data for their profits. the semitrusted adversary model is weaker than the malicious model, but it is commonly used in related work designs [cit] . threats imposed by the four groups of entities in our model are as follows: 1)md is untrustworthy. md is the main source of threats normally. md may want to download and decrypt data for which they do not have access privileges, and multiple mds may collude with each other or with cs to break the encryptions to gain unauthorized access to data."
"using restful web services, cp-abe algorithms become network resources, and each resource can be identified via a uniform resource locator (url). table 5 summarizes the labe cp-abe service resources of as and ep s.if mds want to use any of the labe services, they can simply access the services via their corresponding urls using http protocol directly."
"in this section, we will demonstrate our trajectory privacy preservation scheme against tensor voting based inference attack. in subsection v-a, we will propose our formulation for trajectory privacy maximization problem. because the formulated problem is a mixed integer linear programming (milp) problem, we will first give the upper bound for the problem in subsection v-b. in order to solve the formulated problem efficiently and effectively, we will demonstrate a heuristic algorithm for the feasible solution and analyze the complexity of the algorithm in subsection v-c."
"we have designed the fitness units, controllers and assistant circuits using programming languages and tools specifically used for designing with reconfigurable hardware. the main cores were programmed using vhdl hardware description language [cit] . this is a is very efficient and known language, specially when we are programming at the register-transfer level, allowing to program algorithms abstracting away the hardware as far as it is possible."
"those salient properties make tensor voting based inference attacks superior other inference attacks [cit] because the adversary only needs partial/limited information to launch inference attacks via tensor voting. for example, as shown in figure 2, even without any timestamps, the adversary can still leverage the historical/known locations to infer the user's trajectory using tensor voting. in general, given the collected location data of the lbs user, the adversary can encode the normal space with tensor representation and mathematically infer the trajectory of the lbs user according to the tensor voting theory."
"the structure information of an input location site can be encoded with a tensor. according to the gestalt principles [cit], the exist of objects or shapes which are close enough indicates that these objects probably appear as a group. the strength of each type of visual structure, or saliency, and the preferred normal directions can be encoded within a second order symmetric non-negative definite tensor."
"considering the typical cps application scenes, outsourced data sizes are randomly between 10k to 10m bytes. the data are separated into two categories, one is small size (10-100 k bytes), and the other is media size (1-10 m bytes). the total time is the time for md to finish the whole procedures of encrypting data defined section 6.3, which includes cp-abe encryption of sk and symmetric encryption of original data using sk. 10 attributes are used in cp-abe and 8 attributes are specified in the access policy. figure 2 shows the total time of encrypting data with various sizes under labe,\"cp-abe service\", and \"cp-abe on md\". after utilizing the proxy based service architecture, the encryption speed of labe and \"cp-abe service\" is much faster than that of \"cp-abe on md\". especially, the total time of labe is about 25% of that of \"cp-abe on md\" in figure 2 .a, and the total time of \"cp-abe on md\" is about 3700 ms (million second). labe works always the best since the proxy architecture and the special lightweight cp-abe are used."
"in this subsection, we use figure 4 as an example to illustrate the tensor voting procedure in 2-d. stick vote is used in tensor voting to transmit information about the normal direction from a voter point o(x 1, y 1 ) to a votee point p(x 2, y 2 ). the tensors of them after encoding can be represented by"
2)cs provides storage services for the data owner. cs may sniff the data of md and try to gain plaintext from md uploaded data for the business profit.
"aiming to address those challenges, we novelly leverage tensor voting techniques [cit] to quantitatively model and analyze trajectory inference attacks. to thwart tensor voting based inference attacks, we propose a new trajectory privacy preserving tpp scheme, which satisfies the ''paradoxical'' requirements of lbs users with limited energy consumption of their smart devices. our salient contributions are summarized as follows:"
"as shown in figure 1, the blue dashed line indicates the true trajectory, the red solid line indicates the fake trajectory and the two black points along the line are the source and destination locations. in order to make the attacker trust that the user follows the fake trajectory, because, sometimes, before the user is trying to fake his/her trajectories, the location of the user may be already known by the attacker and also a person cannot go too far away in a short time period, the source and destination locations are assumed to be public known locations. in figure 1, the yellow parts are buildings and the green parts are grass or bushes. we assume that the user can go through a building and not cross through the grass or bush. we set the grey rhombi as the candidate locations, which are all along roads or close to the exit in the building. as we illustrate in section iii, we assume the set of candidate dummy locations is d. the dummy locations are chosen from a candidate set, which is c. the time set is t ."
"the top-level circuit to test the fitness function (additional file 1: figure s1 ) is composed of nf instances of the fitness circuit, nc instances of a floating-point comparator, and a controller that drives and parallelizes the operations involved in f. the value of nf depends on the fpga area."
"the controller and the fitness circuits have different implementations according to the parallelization model and the bicluster size. the implementation version is identified by one letter (f for the partially parallelized model, and a for the fully parallelized one) followed by the matrix size. in addition, the number of fitness and comparator units is specified for the controller. for example, controller-f16x8-nf6-nc3 denotes the circuit implementation for a bicluster of 16 experimental conditions and 8 genes driven by the partially parallelized model using 6 parallel fitness units; in this case, the fitness circuit associated with this controller is identified as msr-f16x8."
"during the voting procedure, votes are not cast equally from a token to another. the vote will attenuate with distance, in order to reduce the influence between unrelated tokens. additionally, the voter will not cast any vote to a receiver which is at an angle larger than π/4 with respect to the tangent of the osculating circle at the voter. the attenuation function can be given empirically,"
"to begin with, we need to mathematically model the structures. in a n − d space, there is a set of n orthonormal basis vectorsê 1"
"without the proxy service architecture, the basic cp-abe works on mobile device very slowly, and the encryption time of \"cp-abe on md\" reaches to about 5 seconds. the ratio of cp-abe encryption time in the total is over 98% when the encrypted data is smaller than 100k in \"cp-abe on md'. the cp-abe encryption time of \"cp-abe servic\" decreased 74% of that of \"cp-abe on md\", and labe is about 40% of \"cp-abe service\". for labe, the average cp-abe encryption time is only 0.5 ms which is just 10% of \"cp-abe on md\", and the speed up is 10 times. obviously, it is too expensive to run the cp-abe on mobile devices directly and labe is a good scheme which can help the resource constrained mobile device to do cp-abe."
"step 4: upon the receipt of ciphertext ct from ep s, md appends ct with the encrypted data en c(d, sk) and sends both to cs. of course, how to upload data to cs is private secret for md itself."
"step 1: md sends a request for the file by sending the file identity, f id, to cs, and the server responds to the requestor by sending a response message."
"which is also a stick tensor. finally, stick votes received at a votee p are the sum of votes cast by all the input tokens. we assume that there are k locations in a set k on the map. the votes received by a votee p can be represented as"
"in other words, a users attributes are not handed out to the user, rather they are kept by as, so different users cannot compromise a data decryption by combining the attributes of different md in labe. proof. considering that there is an adversary, adv, and a challenger, we discuss the ind-cpa attack for labe. at first, the challenger generates p k and msk, and publish p k to adv. adv submits two distinct plaintexts m 0 and m 1 to the challenger. adv tries to distinguish the input message based on the"
"a data owner may revoke a consumer's access privilege granted earlier, and it is essential to withdraw a user's privilege in the outsourced data protection of cps. the traditional cp-abe algorithm does not provide a revocation function, and some cp-abe algorithms revoke privilege by re-encrypting the file."
"3) multiple distributed roles are used to separate security into different parties, and each role is just responsible for part functions to balance loads.it is helpful for the data owner to avoid attacking hazard from the labe manager."
"in the mobile cloud assisted cps, users' data are outsourced and the outsourced data are managed by a third party which is not fully trustworthy. the issue of preserving data security and data owners privacy is among the most challenging issues and then has raised great concerns among the cloud users, particularly for those with sensitive data [cit] . how to preserve the confidentiality of data and privacy are essential requirements for cps users."
"both reasons represent two levels of parallelism: in the bottom, a fine-grained parallelization of the fitness equation; in the top, a fast computation of the fitness evaluation phase applying replicated fitness units in parallel to several individuals of the population. we focused our research mainly on the fine-grained parallelization of the fitness formulation, although on-chip concurrent fitness evaluation has been explored as well. figure 1 illustrates these considerations, comparing usual cpu sequential programming to custom on-chip parallel systems. we can accelerate the computation of the fitness phase making good use of parallelism: replicated fitness functions working in parallel at the top-level, and parallel computation of the fitness equation at the bottom-level. we can observe that cpu requires sequential steps not only for the evaluation of the fitness of each individual, but for the calculation of the fitness equation."
". we can find that the attenuation function is a normal distribution function which is corresponding to a real number. the stick vote cast from voter o to votee p is as the following,"
"the simulation setup is demonstrated in subsection vii-a. in this subsection, we will discuss the result of energy cost minimization problem and compare the performance of the tpp scheme with random and rotation schemes [cit] ."
"data stored in the cloud storage is managed by a third party, which means that the data owner and keeper are in a different domain."
"the security level of labe is dependent on the security levels of the building blocks used in the service design. these building blocks include a symmetrickey encryption algorithm, e.g. aes, channel security protocol, ssl, and our proposed cp-abe."
"a similar analysis can be done seeing fig. 5, that shows the speedups in the biclustering of gene expression data problem for experiments that use different matrix sizes and parallelizing strategies. here, we have considered the high and medium-performance fpga devices and other two different cpus. now, we obtain higher speedups than in the former bioinformatics problem (up to x14), and for all the cases, because of the higher parallelization degree in both, the fitness equations and the matrix operations. in addition, we can extract two interesting conclusions. on the one hand, the msr fully parallelized model provides better performance than the msr partially parallelized model for equal bicluster sizes, as the first one involves more parallel operations. nevertheless, the highest number of replicated floating-point arithmetic operators runs out first the fpga area available: this is the reason why we can not consider large matrix sizes in the fully parallelized model. on the other hand, when using the msr partially parallelized model, since it parallelizes mainly by rows, we should compare matrix sizes with the same number of rows, for example f16x8 with f16x16. in this case, we find that the performance is better with fewer columns, as the lower number of floating-point arithmetic operators allows more area to host more fitness units working in parallel, which has more weight in the performance than the bicluster size."
"identity-based encryption (ibe) was first introduced by shamir [cit], in which the sender of a message can specify an identity such that only a receiver with matching identity can decrypt the message. [cit], fuzzy identity-based encryption was proposed by sahai and waters [cit], which was also known as attribute-based encryption (abe). there are two abe schemes: kp-abe and cp-abe. in kp-abe [cit], a ciphertext is associated with a set of attributes, and a private key is associated with a monotonic access structure specified by using and, or and other threshold gates. a user can decrypt the ciphertext if and only if the access structure in his private key satisfies the attributes in the ciphertext. in cp-abe, the ciphertext is created combining the access structure, and the private key is generated according to users' attributes [cit] ."
"tensor voting is an unsupervised data-driven methodology to automatically infer and group geometric objects [cit], which systematically explains how to infer hidden structures like gaps and broken parts in the trace trajectory [cit] . it can be widely used in machine learning or computer vision as a perceptual organization method. as for trajectory inference volume 6, 2018 attacks, the dishonest lbss or eavesdropping attackers may exploit the tensor voting theory to infer a user's trajectory, because tensor voting has desired geometric properties such as smoothing continuous trajectories and bounding boxes with minimum registration errors."
"system initialization is necessary to set up and put system parameter values in as and ep s, which is run before mds make any request. new ep s components may be added dynamically after the initialization step is completed."
"3)as and ep s collectively provide reliable abe services to decrypt the ciphertext of sk. after getting sk, the data requester can decrypt the ciphertext in cs. as and ep s may collude with mds that have no access privileges for the corresponding cs."
"summarizing, our proposal presents the performance from a computational perspective. other performance features closer to the specific bioinformatics problems only can be tackled by the corresponding algorithmic methods and software packages, which are out of the scope of this work."
"1. the first step is the same as in the msr partially parallelized model: calculation of sum_bij i and sum_bij j . 2. now we increase the parallelism with regard to the first model, calculating in parallel sum_bij, bij i and bij j . 3. this step corresponds with the fourth step in the first model: calculation of bij. 4. now we can calculate r ij in a fully parallel way, because we have more parallel floating-point multipliers. 5. the last step calculates msr as the previous model does."
"since many optimization problems in bioinformatics define fitness functions as floating-point arithmetic operations, we have tested two of them in order to check specific implementation features: area occupation, response time and energy, mainly. from these values we can obtain finally, the very low power consumption of the fpga devices in comparison to cpus proves that fpgabased parallel computing environments are excellent low-cost computing solutions for intensive computing scenarios."
"the above considerations move us to implement the fitness functions in hardware to enhance the system performance. these functions have been accelerated by means of fpga devices in genetic programs for financial markets [cit], spatial image filters [cit], filtered image signals [cit], test cases [cit], and many other engineering applications."
"when we encrypt the plain text and transmit it through the wireless network, there is energy consumption. because our scheme is used in mobile devices and there is a limited power usage, we need to reduce the power usage when processing the our scheme. in our paper, the extra energy consumption is from encryption and transmission of the critical locations along the true trajectory. therefore, we assume that the power usage need to satisfy (28), where p e is the power cost to encrypt and transmit one location and p th is the limited power usage of the mobile device. in our paper, we use the aes scheme to encrypt the location information for example. the energy cost constraint can be represented as following"
"to make the encryption and decryption services of labe easily accessible by md, all the interfaces of as and ep s are encapsulated in restful web services."
"the total decryption time in figure 4 is stable for the small size, whereas, the total times increase nearly linearly with the data size for the medium. the total times of decryption are smaller than the corresponding time of encryption, exception for the \"cp-abe on md\" in decrypting data with small size. our labe works the best, \"cp-abe service\" works the second, and \"cp-abe on md\" works the worst. if the data size is less than 100 k, \"cp-abe on md\" needs 7 seconds to complete decryption. without the proxy based service architecture, the mobile device finishes cp-abe too poorly to be applied fluently in reality."
"in our work, the user can select several critical locations such as turning points to be encrypted with an encryption function e(·). the encrypted data set can only be decrypted by the trusted party. similarly, the set of locations on the real path is t r and the locations are chosen from an encrypted candidate set e, which are introduced in section iii. we give a function to choose the candidate location as following,"
"from table 4, it can be seen that the complexity of public parameter is a constant 4l in our special cp-abe, whereas, the basic cp-abe [cit] has constant encryption complex: 3τ e, but it's decryption complex is 2mτ p, worse than our scheme: τ p + mτ e . notice that the encryption algorithm is operated once, decryption is invoked by md in the accessing every time. so, the better decryption performance is more important than encryption in the real application systems. it is most important that our decryption computation complexity is fastest in all cp-abe algorithms."
". this architecture needs three floating-point arithmetic operators (adder, multiplier and divider) and an integer to float converter. the fitness controller supplies the operands to the arithmetic modules and receives the results. once the calculation of f has been completed, the fitness controller gives it back to the controller."
"in our paper, as described in section iii, the users are also able to communicate with the trustworthy party. we assume to encrypt users' true locations by traditional encryption algorithms like aes and 3des. in this case, the security of transmitting information to trustworthy party is preserved by the hardness of compromising those encryption algorithms."
"the average decreased total time of labe is about 59% and 13.8% of that of \"cp-abe service\" in the cases of small size and medium size respectively."
"the design methodology follows some steps, starting from the programming of the circuits using vhdl and core generator tool. in this step is mandatory to do the maximum parallelization effort in order to design an efficient architecture. once built the codes, the synthesis and implementation step allows obtaining the minimum clock frequency for a determined fpga device. using this information, a vhdl testbench customized with the corresponding clock period can simulate the top level design using isim, obtaining the time response of the circuit, which will be used to calculate the fpga speedup. table 1 shows the hardware used for the experiments: fpga devices for implementing the fitness circuits and general-purpose cpus for comparing the performance results."
"in this section, we demonstrate the problem statements and notations. under the non-interactive model of the privacy preservation framework, which uses the learning algorithm, we propose the trajectory privacy preservation solution tpp against tensor voting based trajectory inference attack. tpp is based on the fact that the location information is not directly sent from gps to the third-party server, but from the user's device. also, the user has the control location information reporting."
"the core of this work deals with the hardware-level parallelization of the fitness functions used in two bioinformatics problems: gene selection for cancer classification and biclustering of gene expression data. the reason for designing fitness hardware accelerators is twofold. on the one hand, every fitness function is applied to each individual of a population in many bio-inspired metaheuristics; this fact allows us to parallelize the computation of the fitness evaluation phase if we place several copies of the same fitness hardware implementation. on the other hand, fitness functions are usually formulated by means of floating-point arithmetic equations that can involve many operation steps; this way, parallelization of some of these steps using repeated units of the same floating-point operator increases the performance of the design."
"the ratio of the area occupied by just one fitness circuit to the maximum number of such circuits that the fpga can host can be seen in fig. 6, for the second bioinformatics problem: we can have more fitness units in larger fpgas or considering designs that use lower slice resources. summarizing, there is a strong relationship between the area required to implement a single fitness function and the bicluster size. furthermore, increasing the area required for the fitness function decreases the total number of parallel units that can be implemented on fpga. therefore, it is needed to establish a tradeoff for each experimental framework."
"this problem deals with numerical matrices that represent information extracted from microarray data. these matrices can be built using clustering or biclustering methods [cit] . clustering methods gather together genes with a similar behaviour under all the experimental conditions, using algorithms based on genes similarity, whereas biclustering methods find subsets of genes with the same behaviour under a subset of experimental conditions."
"this procedure is highly parallelizable. there are different ways to parallelize the calculation of msr, according to the experimental constraints: the more resources we have, the more parallelization we can achieve. since the parallelism comes basically from the use of replicated circuits of the floating-point arithmetic operators, the fpga device can host different number of these units depending on two factors: the specific fpga device (family and model) and the size of the bicluster. due to this reason, we have considered two different parallelization models to compute msr."
"the remainder of this paper is organized as follows. section 2 and 3 introduce related works and definitions about abe. section 4 provides design ideas, system architecture and adversary model. a lightweight and fast cp-abe algorithm are constructed for labe in section 5. such details as access policy construction, system initialization, data uploading and downloading, and restful interface for labe are given in section 6. section 7 analyzes the security of proposed abe algorithm and the whole system of labe. performances are evaluated in section 8 and section 9 conclude works finally."
"when t is a tuple the simulator b gives a perfect simulation so we have that αs . besides of the valid plaintext and ciphertext pairing is leak, we discuss two cases where the master key or the random security s is leak as follows."
"reconfigurable computing has been successfully applied to many bioinformatics problems, because they have a high parallelism degree. knowing how to make the most of this parallelism, we can obtain speedups and energy savings needed for intensive computing or real-time applications. in this area, we can find fpga implementations for dna matching based on the blast algorithm [cit], bowtie short-read mapping [cit], epistasis detection [cit], molecular modeling [cit], and many other algorithms involved in sequence comparison, multiple sequence alignment, rna and protein secondary structure prediction, gene prediction and phylogenetic tree computation [cit], among many others. nevertheless, these works are usually focused on solving specific problems, dealing with their special characteristics and constraints. contrary to these approaches, our work tries to get a wide insight into important aspects to take into account when designing accelerators."
"each synthesis was repeated several times following different strategies in order to obtain the highest clock frequency. on the one hand, we considered three optimization synthesis profiles: default, timing performance with physical synthesis, and timing performance without input/output blocks packing; other synthesis profiles were discarded because of their worse results. on the other hand, we have tested two possibilities when it comes to synthesizing the floating-point arithmetic operators by core generator: using internal dsps or logic blocks in the architecture optimization. if we consider dsps, the performance can be better, but the limited number of dsps forces us to consider digital logic if we want to have more parallel units, involving more area consumption; this tradeoff between number and performance of parallel operators must be evaluated in each case."
"the first bioinformatics problem in our study is gene selection for classification of high dimensional microarray data in cancer disease. this optimization problem has been studied using mainly gas and support vector machines (svms), where the ga is used to evolve gene subsets whose fitness is evaluated by a svm classifier. in this line, there are approaches based on single objective [cit] and multi-objective [cit] points of view. nevertheless, we have not found any fpga implementation of fitness functions associated to this problem. therefore, we offer novel insight into its hardware parallelization."
"normally, once a user has obtained a private key for an encrypted file, the key is valid until the file is re-encrypted by a new access policy. in labe, the revocation of a mdś privilege does not require the re-encryption for the outsouced data, rather the system only need to change the attributes assigned to the md in as."
"labe system with md, as and ep s is realized using java language. the protected data owned by md can be uploaded to any cs system such as dropbox, zip cloud, baidu cloud. cs environments will not be discussed in our experiments, since cs may not affect the encryption and decryption performance evaluation on md."
"the analysis of microarray-based gene expression allows us to compare between the gene expression levels of cancerous and normal cells, in order to select the genes under suspicion [cit] . these genes are useful for cancer classification, but hard to be selected when the number of genes (m) and samples (n) are very high, shaping a combinatorial optimization problem."
"in our work, we are trying to propose a mathematical way to quantify the trajectory privacy. we define the location along the true trajectory at timestamp t is l t i, and similarly the dummy location along the fake path at timestamp t is l t dj . the location l t i can be represented as a triple-tuple (x i, y i, t), where x i and y i are the coordinates of the location. consequently, we can get the euclidean distance between the two locations at the same timestamp as follows,"
"decryption is called more frequently than encryption and the performance of decryption is more important than that of encryption. we also evaluate the encryption performances under three cases: labe, \"cp-abe service\", and \"cp-abe on md\". figure 4 illustrates the total time of decrypting the ciphertext with various sizes including the small size and medium size."
"since biclustering is more complex than clustering, several evolutionary algorithms have been applied in order to find biclusters. these algorithms consider as fitness function a measure for assessing the quality of biclusters. one usual measure is the mean squared residue (msr), that provides lower values for better biclusters. the msr value is calculated following these steps:"
"the speedups for the second bioinformatics problem (biclustering) are good in all the cases and higher than for the first problem (gene selection). we find the reason mainly in the parallelism degree of the fitness circuit design, rather than in the number of such circuits working in parallel. the bottom level of the fine-grained parallelization is the fitness circuit, which is composed of some basic floating point operators: adders, dividers, multipliers and integer to float converters. this way, the more floating point operators running in parallel, the better performance we expect. we find 4 operators in the fitness circuit for gene selection, whereas the fitness implementations for the different bicluster sizes and architectures go from 8 to 32 operators. the number of floating-point operators running in parallel has great influence on the final performance, even more than the number of replicated fitness circuits. in fact, the number of parallel units is higher in the first problem: the performance speedup for the gene selection test with 256 fitness units is x9, whereas 20 units in a f8x8 bicluster gives x14. the reason is simple: a greater number of parallel fitness units in the same fpga device implies more circuit density in the top level architecture (more communication buses, interconnection blocks, logic cells, etc.), which produces smaller clock frequencies with the corresponding time response decrease."
"labe system can provide fine-grained access control services to md of cps based on cp-abe. the fine-grained features is inherited from the features of cp-abe, where each md is assigned with a set of attributes by as and the access policy is defined by the owner of outsourced data."
"the last decade has witnessed the exploding growth in the quantity and capability of consumer mobile devices such as smartphones, tablets, etc., and the proliferation of wireless services. with the advance and commercial use of global positioning system (gps) technology, smartphones and tablets feature sensors that can pinpoint users' locations, which can allow the location-based services (lbss) to use users' whereabouts in a variety of ways. actually, the lbss do more than just tell us about exactly where we are. they offer useful features based on our location from locationbased discovery tools and smart search (e.g., foursquare, yelp, glympse, detour, gowalla, shopkick, etc.) to games and exercise tracking (e.g., pokémon go, ingress, scvngr, etc.). for example, foursquare encourages users to check-in at locations in return for virtual badges and points. it also helps users keep up with friends, discover what is nearby, save money and unlock deals. according to the study by kantar tns, lbs users are increasingly using services to enrich their daily lives, with 22% using lbss to find their friends nearby, 26% to find restaurants and entertainment venues, 19% to check the public transport schedules, 8% to book a taxi, and 13% to find a deal or special offer."
"where v x,p is the vote point x cast to point p. because the vote is also a stick tensor, equation (12) can be decomposed by (3) as following"
"the selected xilinx fpga devices offer a representative range of features, including the low-cost spartan6 (xc6slx150), the high-performance virtex6 (xc6vlx550t) and the balanced virtex5 (xc5vlx330). these devices may be characterized by four important features that describe the process technology (complementary metal-oxidesemiconductor -cmos-depth in nanometers), the number of logic cells (as indicator of the area available to host the circuits), the number of internal digital signal processor (dsp) slices (related to the speed of the floating-point arithmetic operators) and the number of memory blocks (useful to handle the circuit data)."
"we measured the performance of our three fpga devices with the post-placement and routing simulation tool provided by the implementation environment. the validation of the results consisted in comparing the simulation times of the virtex5 device with those measured with custom circuits on a prototyping board that hosted the xc5vlx330 device: xilinx university program virtex5 development kit. since both times were almost equal, we can approve the simulation results corresponding to the other fpga devices."
"step 2: md generates a private symmetric key (sk) randomly, and encrypts (sk) using ssk: en c(sk, ssk). the encrypted data request message containing sid and en c(sk, ssk) is submitted to ep s. the original data (plaintext) d is encrypt by the symmetric encryption algorithm using sk, and the result en c(d, sk) is kept in md to be uploaded in step 4."
"the architecture of the fitness circuit (additional file 1: figure s4 ) may contain different number of adders, multipliers, dividers and integer-to-floating point converters, according to the implementation version. each implementation version takes into account specific parallelization and resource use. for example, the design"
"as shown in figure 5, we assume the black solid line and blue dashed line are two trajectories, and the red dashes lines between the two trajectories are the euclidean distances between two locations and the total length of all the red dashed line is considered as the defined trajectory privacy."
"the hardware implementation of fitness equations is made easier thanks to hardware description languages (hdls) and field programmable gate array (fpga) devices [cit] . the fpga technology favoured the rise of a computing domain that combines software flexibility with hardware performance exploiting the parallel paradigm: reconfigurable computing (rc) [cit] . this way, a fitness function carefully designed can surpass the cpu performance in similar experimental conditions, as rc has demonstrated in many applications [cit] . in addition, we decide on fpgas instead of other competitive technologies as graphical processing units (gpus) since fpgas usually provide better performance and lower power consumption than gpus [cit] ."
". now, we continue to analyze the computational complexity of our heuristic algorithm. as illustrated before, we relax and fix the w t j -variable by iterations. in order to determine all the w t jvariables, we repeat doing iteration. the complexity for the iteration procedure is o(c) and the complexity for the lp problem is o(c 6 · l), which results in the overall complexity is o(c · c 6 · l). obviously, the computational complexity is significantly reduced compared with the optimal solution with complexity o(2 c · c 6 · l)."
"in this paper, we have studied the trajectory privacy maximization problem via our proposed scheme against the tensor voting based inferring attack. we have introduced a novel trajectory inference attack model based on tensor voting theory. we have mathematically formulated the trajectory privacy maximization problem under several constraints such as saliency limitation based on tensor voting theory, and put it into an milp problem. because of the np-hardness of the milp problem, we have converted it into lp problem for an upper bound and developed a heuristic algorithm for feasible solutions. we also have illustrated the energy cost minimization problem. through extensive simulations, we have shown that the proposed tpp scheme can effectively preserve lbs users' trajectory privacy."
"as illustrated in section iv, we take tensor voting analysis to launch trajectory inference attacks. in order to hide the true path, after processing with tensor voting, the saliency of fake locations along the dummy trajectory should be larger than an upper bound threshold value s th h, and the saliency of locations except the destination and source locations along the true trajectory should be smaller than a lower bound threshold value s th l . likewise, after the encrypted critical points along the true trajectory has been decrypted by the trusted party and put back on the map, the saliency of locations along the true trajectory should also be larger than the upper bound threshold value s th h . from subsection iv-c, we can obtain the tensor of location l after voting procedure can be represented as following"
"after cs receives the uploaded data, the encrypted data are stored in cs as table 4 . where, l ct (4-bytes long) indicates the length of ct ."
"labe has two layers: mobile user layer and cloud service layer. in the mobile user layer, the resource constrained mobile devices (md) does work in cps, and the encryption and decryption algorithms of abe are invoked to protect their outsourced data to be stored in the cloud. in the cloud service layer, cloudstorage(cs), the authenticationservice(as), and the encryp- md represents a data owner or a data consumer. as a data owner, md defines an access control policy for the data to be uploaded onto a cs. to upload data, the owner first encrypted the data by using a symmetric encryption as and ep s are implemented in the form of restful web services. actually, as and ep s may be viewed as encryption/decryption cloud services. in order to separate the working function and enhance the security, the as and ep s are not deployed in the same security domain as the cs. so, though cs has the data uploaded by md, cs does't know the decryption way. on the other hand, as and ep s help to do the encryption and decryption operations, but the outsourced date are not need to be sent to as and ep s."
"step 2: md authenticates itself in as, and the step may be performed by using an existing protocol such as ssl. if md is a legitimate user, as will generate (sid) and (ssk). after checking requesters information of sid, u id, ssk stored in as's database, as sends a response message, (sid, ssk), to md."
"in addition, αs is difficult to be calculated from e(g, g) αs based on the discrete logarithm assumption. assuming s is obtained from the ct when a satisfied attribute is known, the security of α is assured as the exponent αs cannot be computed from e(g, g) αs ."
"step 1: the data owner specifies an access policy (policy), described by the monotonic boolean formula, and sends a data uploading request to as having uid (the users identifier), f id (a unique identifier for the data) and policy."
"with the increasing of the attribute numbers of the md, the cp-abe decryption time increases for labe, \"cp-abe service\", and \"cp-abe on md\". especially, without any optimization, the original \"cp-abe on md\" increases so quickly that the decryption time reaches to more than 40 seconds and 50 attributes, the ratio of labe on \"cp-abe on md\" is 1.5%, 0.8%, 0.6%, 0.48%, and 0.43%, and the ratio on \"cp-abe service\" is 8.2%, 5.5%, 4%, 3%, and 2.8%"
"in the rest of this section, we introduce the tensor voting framework in 2-d. as shown in figure 2 (a), attackers are able to collect history locations of a user. with the tensor voting process, the outlier locations are filtered out shown in figure 2 (b). after feature extraction, attackers can mathematically track the user's trajectory. next, we will illustrate the approach to representing a token, which is encoded with normal space. then, we introduce the tensor voting based inference attack procedure."
"the system overview is shown in figure 1, which is a partial map of university of houston from the college of technology building to the student center satellite. the two solid circles are source and destination locations, the hollow circles are the locations along the true trajectory and the rhombi are candidate dummy locations. we assume the user is walking along the dashes line which is the true path. the user does not report the true locations but choose candidate locations, rhombi, and the fake path can be generated which is solid line on the figure. at the meanwhile, some critical location, such as turning points along the true trajectory, will be encrypted in traditional ways like advanced encryption standard (aes) or triple data encryption algorithm (3des) and send to the trustworthy party. after the trusty party decrypted the data set and put the location back to the map, the true path of the user will be shown according to the tensor voting analysis. in order to quantify and increase the trajectory privacy, we are looking forward to maximizing the differences between the true and dummy trajectories. in this case, we can protect the trajectory privacy to the greatest degree. we will further illustrate the problem formulation of maximizing trajectory privacy in section v."
"the decryption algorithm outputs either the plaintext m, when the collection of attributes s satisfies the access structure a, or ⊥ when decryption fails."
"should be known. ep s makes the second request to as. lsss is queried according to f id, and then s is generated based on the corresponding mds."
"the interest of applying the reconfigurable computing technology based on fpgas to implement the fitness function lies in the possibility of accelerating the evaluation phase in many metaheuristics. this phase evaluates a population of solutions to a combinatorial optimization problem in the bioinformatics domain. the design of a custom circuit that implements the fitness equation allows its replication in several processing units that work in parallel and, thus, accelerate the evaluation phase."
"after the trustworthy party decrypts the encrypted location data, by applying the tensor voting framework, the true trajectory can be obtained by the reliable party."
step 3: md requests ep s to acquire sk and decrypts the encrypted data received in step 1. the request message contains sid and en c(f id +
"step 4: the outcome of this decryption is sk, and then sk is encrypted using ssk to be send to md. after decrypting en c(sk, ssk), md gets sk to be used to decrypt en c(d, sk) using symmetric algorithm, obtaining the plaintext d."
"the high performance cost of the fitness evaluation phase in relation to the overall computing time of the metaheuristic is a well-studied fact in the literature. fitness evaluation can take up to 95 % of the total execution time in genetic programming [cit] or 64 % in gas with evolutionary mapping [cit] . in general, many works have demonstrated that the execution time of the applications will mainly depend on the execution time of the fitness function [cit] ."
"although the user prefers to fake their trajectory in order to prevent from attacking from malicious parties, the user may also desire to tell his/her true trajectory to parents or friends who are considered as trustworthy party. as we illustrated in section iii, the user can send selected locations to the trustworthy parties. however, because of sending these locations, it will cause extra communication and computation cost. in this section, we will minimize the energy cost when sending encrypted locations to trustworthy parties for true trajectory reconstruction."
"in order to establish a valid fpga vs cpu comparison, and properly analyze the performances, we should consider the use of contemporary devices with similar technologies. this reason led us to use several processors of different cmos technologies and clock frequencies, as we can see in table 1 ."
"in this section, we will analyze the security and discuss about the simulation results about our proposed trajectory privacy maximization and energy cost minimization problem."
"(1) labe is encapsulated in restful web service [cit] . restful web services are developed following the rest principles without complex prototype standards like traditional soap based web services. labe can be invoked by the resource constrained mobile device fast and easily and then the confidentiality and personal privacy of cps users are greatly enhanced using abe based access policy. (3) labe has integrating security, fine grained access control, and user revocation. numerical results show that the lightweigh cp-abe algorihtm and proxy service architecture are able to greatly improve the speed of encrypting and decrypting outsourced data for the mobile cloud assisted cps."
"αs0 and m i e(g, g) αs1 may have the same value, even m 0 is not equals to m 1 . according theorem 1, the challenger should not predict what will happen to the cipher by changing the plaintext, and we cannot get any information about the input message from the ciphertext."
and ep s and the channel between as and ep s are based on ssl. the data encryption and uploading operation of md involves four steps as follows:
"the total time of the three cases is stable when the encrypted data are increased from 10k to 100k, whereas, the total times are nearly linear to the data size from 1m to 100m. the reason is that the ratio of symmetric encryption time in the total time in encrypting data with medium size is higher than that in encrypting data with small size. with the increasing of data size, the cost ratio of symmetric encryption become higher and higher. cp-abe just encrypts the symmetric key for the protected data. we can see that the encryption time of cp-abe is not affected by the data size, and the cost time depends on the architecture and cp-abe algorithms."
"we designed custom circuits to test the performance of the fitness function instead of using embedded processors because these ones take up an area that, otherwise, would be useful for hosting more parallel fitness circuits."
"(2) assume a current random security s is leak. using the leak plaintext and ciphertext pairing (m 1, c 1 ), the adversary just could calculate e(g, g)"
(2) as distributes p k and mk to all ep ss using the secure channel; (3) ep ss save p k and mk in the configuration file for the future encryption and decryption process.
"as future research line, we will tackle the connection of these accelerated fitness functions with evolutionary frameworks for solving the combinatorial optimization problems. the main idea is to implement an ea in software, leaving the intensive fitness computation to the hardware."
"on the other hand, we have used xilinx ise 14 software suite [cit] for the simulation, synthesis and implementation of the top-level circuits. this suite contains two important tools: on the one hand, core generator system tool was used for generating the circuits for the floating-point arithmetic operators; on the other hand, isim simulator was used for testing the top level circuit and measuring the time responses, very useful to calculate the speedups of the fpgas with regard to cpus."
"which is also the angle between vector v and vectorv o,t and α is the arc length from point o to p. geometrically, we can obtain normal vectorv p,n of votee p iŝ"
"finally, it is interesting to know the power consumption of the fitness circuits, since they have an important impact in the metaheuristics as we saw in the related work section. this impact involves high energy when the optimization problems demand intensive computations along the time."
"among the many data returned by the synthesis processes, we analyze mainly the timing reports, since they provide the speedup of fpga versus cpu (of course, we have checked the numerical results are the same in both fpga and cpu implementations). we understand by timing performance the reciprocal of the computing time t [cit] . to compare the performance of fpgas and processors, we say that the speedup of fpga versus cpu is t cpu /t fpga . hence, a speedup greater than one means that fpga is faster than cpu; otherwise the processor wins. it is important to realize that both values, t cpu and t fpga, measure the same number of fitness evaluations; in the first case, using a loop of sequential computations, whereas the second case considers a parallel computation of nf fitness circuits. according to this speedup definition, and taking into account the maximum number of parallel fitness circuits that can operate in parallel in the same fpga, fig. 4 shows that fpgas are much faster than cpus computing the fitness phase in the gene selection for the cancer classification problem, according to the different fpga devices, two processors, and a wide range of values for nf. the fpgas provide better speedups than cpus (up to x9), even for the highest performance processor. we can observe that, the more parallel fitness units we consider, the better speedup we obtain, although this increase is not linear, because of the more dense top level circuits that slow down the clock frequency. in addition, virtex5 provides better performance than virtex6 because of the memory constraints to handle the synthesis of large designs (this constraint impedes to consider 256 fitness circuits for the virtex6 device). finally, since the spartan6 device is a low-cost fpga, it provides much lesser area than the other devices, making it impossible to host more than 32 parallel fitness units."
"to reduce abe computational costs, the work [cit] introduced methods for online/offline encryption and key generation. the idea is to shift the computational task of encryption and key generation to an offline phase, thus spreading the the computational cost over a longer period of time. to allow cp-abe to be used in arm based mobile devices and speed up the executions of abe in the devices, the authors modified the original model of abe with outsourced decryption so that some of the computationally expensive tasks are moved from the mobile device to a proxy [cit] : let g and g t be two multiplicative cyclic groups of prime order p. let g be a generator of g and e be a bilinear map,e :"
"manchester to be an legitimate user in t, the boolean formula can be specified as follows. md specifies an access tree when it submits data to a cloud storage before the data are encrypted. as discussed earlier, the policy is presented by using the boolean formula. the corresponding lsss is generated by as. in the lsss matrix r, the number of rows will be the same as the number of leaf nodes in the access tree. pieces of r are a vector over the finite field, and the secret will be hidden in the access structure and can be reconstructed using a linear combination of the pieces. md having the corresponding attributes can decrypt the ciphertext correctly. in our labe, the corresponding attributes of md is generated by as automatically in the decryption process."
"we define s l i as the saliency of the encrypted locations along the true path after being decrypted and processed voting. the saliency should be larger than the upper bound threshold value s th h as demonstrated in subsubsection v-a.3, which can be represented as following"
"after the input sites have been encoded with tensors, the voting procedure is used to communicate information from each input site, or voter, to any output site, or receiver."
"in order to choose the candidate location, we denote (14) where t is the total number of time slots of the whole trajectory. like we illustrated before, we assume the source and destination locations are public known, the sum of the selected candidate location should be t −2. moreover, during one time slot, only one candidate location from the set can be chosen, which is shown as (11)."
"a general bicluster is represented by a matrix b of i rows (number of experimental conditions) and j columns (number of genes), where the element b ij is the expression level of the gen j under the experimental condition i."
"as illustrated in subsection v-b, we are able to get the upper bound for the proposed problem as the benchmark, nevertheless we still explore for an effective and feasible solution. in this subsection, we will describe our heuristic algorithm to solve this optimization problem."
"the data encryption service of labe is invoked when a data owner wants to outsource data to cs. each md has a valid user identifier (uid) registered in as,md knows the url of as, ep s and cs. the interfaces of as and"
"accordingly, the saliency of the locations along the dummy trajectory and the locations along the true trajectory can be represented as s l dj and s l i . hence, in order to hide the true trajectory and show the dummy trajectory, we need to satisfy"
"as shown in fig.4, the average total time of \"cp-abe service\" is fewer over 5400 ms than that of \"cp-abe on md\", and the average total time of labe is fewer over 1100 ms than that of \"cp-abe service\". according to the section 6.4, the whole data decryption is divided two procedures: symmetric decryption and cp-abe decryption. the symmetric decryption is to obtain the original data, whereas, the cp-abe decryption is to obtain the key for the symmetric decryption."
"the key materials required to compute the decryption key are managed by a third party, as. applying cryptographic algorithms to protect the outsourced data uploaded from md in cps, we separate duties of storing cs, as and ep s. the outsourced data are kept in cs, but the data are ciphertext. the entities including as,ep s may know the part information of decryption keys, but they do not have the access to cs. the plaintext is just decrypted by the md satisfying the access policy."
"there are many other bioinformatics problems involving metaheuristics with fitness functions similar to these two cases, specifically with regard to the floating-point arithmetic [cit] . this way, analyzing the fpga implementation of the two case studies can contribute to expect good computing speedups in other works."
"in order to provide a fast, secure, and fine grained protection for the outsourced data in the mobile cloud assisted cps, we propose a lightweight attribute based encryption scheme and presents its realization. using proxy based service architecture, the functions of encryption/decryption are separated into different components, and then the working load of the mobile is lowered. algorithms used in labe is optimized specially in the transmitting bandwidth and computation complexity, the encryption does not need pairing and decryption only needs one pairing. labe is secure with fine grained access control and user revocation capability. the experiment results show that the encryption and decryption performance of labe can be improved dozens of times for the traditional schemes and can be suitable for mobile cloud assisted cps."
"the mission of the controller is to handle the different steps of the test process, which follows this scheme: the fitness circuit implements the arithmetic operations involved in f, some of them in parallel. the architecture of the fitness unit (additional file 1: figure s2 ) is composed of several arithmetic modules and a fitness controller. the fitness controller drives the arithmetic operations according to (1), where three operations are performed in parallel:"
"in order to counter the tensor voting based inference attack, we proposed the tpp scheme shown as figure 1 . first, we let the user not report the location points along the actual trajectory, but intentionally choose candidate locations and report them which are along a dummy trajectory. based on the tensor voting analysis, we make sure that both the saliency of dummy tensors are sufficiently large to form a fake trajectory. so with tensor voting based inference attack, the attacker believes that the user follows the dummy trajectory instead of the actual one. at the meanwhile, the user will select several specific and critical locations along the true path to be encrypted and sent to the trustworthy party. after the trustworthy party decrypts and puts these locations back to the map, the true path will appear on the map by processing with the tensor voting framework."
"1. area occupation. several indicators (slice registers, slice look-up-tables and occupied slices) allow us to calculate the number of circuits that we can replicate in the same fpga device in order to work in parallel. depending on the values returned by these indicators and the fpga family and model, a different number of such circuits can be considered. 2. timing performance. the value of the maximum frequency (mhz) (that corresponds to the minimum clock period in nanoseconds) allows us to determine the time to process the fitness function; if we consider nf parallel units of the fitness circuit, the time to process the different solutions is equal to that time. 3. power consumption. nowadays, it is very important to design energy-aware circuits in order to minimize operation costs when solving problems that involve massive computations along the time. the synthesis process tells us the power (watts) consumed by the fitness circuits."
"this way, our main contribution in this paper is to demonstrate that the fine-grained parallelization of fitness functions based on floating-point arithmetic can surpass the performance given by cpus, in time and power terms, when they are massively used by metaheuristics for solving large combinatorial optimization problems in bioinformatics. the conclusions of our work can be applied in general to similar cases, because of the representativeness of the fitness functions we have chosen. for this purpose, we have selected two specific fitness functions used in the above mentioned optimization problems by two reasons: on the one hand, there is not enough information about their implementation in fpgas in the existing literature; on the other hand, they provide different computational workloads and parallelization levels because of their floating-point arithmetic formulations, being representative formulations of other similar functions widely used in bioinformatics."
"as we pointed out in the previous section, bio-inspired and evolutionary optimization algorithms are very appropriate to be parallelized, not only by applying repeated fitness hardware units in parallel on several individuals of the population, but parallelizing other important parts. for example, the intrinsic parallelism in popular genetic algorithms (gas) [cit] allows better speedups. in this line, fpgas have been successfully applied to parallelize many metaheuristics and optimization algorithms, like differential evolution (de) [cit], particle swarm optimization (pso) [cit], artificial neural networks (ann) [cit], and ant colony optimization (aco) [cit], among many others."
"all the communication channels linking any pair of entities in the labe architecture are authenticity and confidentiality protected using ssl. since the authentication procedure and the keys used in protecting the channel security are secure, the session symmetric encryption key ssk is distributed securely and then the communication between md and ep s will also be secure. when md wants to upload data to as, md will symmetrically encrypt the data by using a random private symmetric key, sk. sk is encrypted by cp-abe provided by as and ep s. at last md will get the corresponding cp-abe ciphertext ct from ep s."
"bioinformatics is an area where we can find many large combinatorial optimization problems [cit] . the high size of the space of solutions causes these problems can not be tackled by means of exact searching techniques, which require an excessive computational effort. in these cases, the usual way of obtaining optimal solutions is to consider metaheuristics [cit] and particularly evolutionary algorithms (eas) [cit] . nevertheless, even these algorithms can be slow for complex problems, demanding more hardware resources based on current general-purpose processors or central processing units (cpus). if we identify what part of the algorithm takes more time to be computed, a hardware coprocessor specifically designed to accelerate this function is a direct solution to further speed up the performance. in this sense, the fitness function is a simple but critical operation involved in the metaheuristics. most of the computing time of the algorithm that solves the optimization problem may be spent running the fitness function, although it could mean a small part of the code."
"in addition, based on tensor voting theory, we can also get the maximum distance d max from the voter at which the vote cast will have 1% of the voter's saliency, as e −(d max"
"to avoid those issues, it is worthy to study how the dishonest service provider analyzes the location data, and infer the users' trajectories. the emerging location data has provided opportunities for trajectory prediction [cit] . it is necessary to innovate a scheme to preserve the trajectory privacy of lbs users. on the other hand, users sometimes would like to intentionally disclose their trajectory to trustworthy parties (e.g., their parents, family members, close friends or even some well-known lbs providers). therefore, it is also important to satisfy those requirements of users with the proposed trajectory privacy-preserving scheme. moreover, as the scheme is applied on a mobile device, the energy consumption should be considered as a constraint. however, most existing trajectory privacy preservation works [cit] have limited concerns about this seemingly paradoxical but practical requirements of lbs users. besides, there is a lack of quantitative approaches to analyze either inference attacks or the countering privacy preservation measures."
"a common approach to face this challenge consists in selecting a subset of suspicious genes for cancer classification. this is the basis of many metaheuristics where the individuals of the population are gene subsets. we have considered a fitness function given by (1), where x is the subset, a(x) is the leave-one-out-cross-validation accuracy provided by a classifier, r(x) is the number of selected genes in the subset, and w 1 and w 2 are weights for the accuracy level and the number of selected genes, respectively [cit] . this fitness function must be maximised by the metaheuristics in order to find an optimal gene subset."
"in order to propose a grounded model for developing translational science project managers, we utilized both case analyses and interviews using faculty, researchers, and students at the university of texas medical branch at galveston (utmb). we focused on how junior researchers have actually achieved and experienced these roles, as well as identified specific competencies and needed training for postdoctoral trainees. the interviews and analysis followed the logic of discovery suggested by the grounded theory model of qualitative research [cit] . we also employed auto-ethnographic case histories [cit] ) utilizing two of the more experienced and successful team project managers discovered during the interview process. [cit], for an extensive discussion of the qualitative methods used in our research at utmb)."
"here, collaboration and network skills as expressed by dealing with other teams as well as leaning how to work with upper level administrators are illustrative. these emergent skills are also similar in nature to previously cited literature involving team science skills. specifically, bennett, gadlin, learning how to deal with ctsa administrators ctsa administrators can be differentiated from other traditional scientific administrators. they are more committee based and somewhat parallel to if not independent of traditional academic lines of authority."
"table iv compares the energy efficiency of different network deployments for two different target area throughput values. as it can be expected, macro base stations combined with wlan provide the highest energy efficiency for both area throughput targets, compared to the dense macro and the macro+pico case. another important conclusion is the difference between energy efficiency results for different target area throughputs, for the same deployment case. as it is shown in table iv, energy efficiency is decreasing with an increased area throughput for the dense macro case, while it increases when macro+wlan are used. these results can be explained as follows; with the dense macro case, the increased capacity can not compensate the additional power consumption of macro base stations. however, lower power consumption advantage of wlan reverse this conclusion for macro+wlan deployment scenario."
"thus, formal team training needs to be integrated as both an educational and experiential intervention in the preparation of future translational scientists. obviously, much work needs to be done in this regard."
"where a i, b i and p tx are defined as in sec. ii. additionally, performances in terms of energy efficiency are also analyzed. energy efficiency is defined as the ratio of the total network throughput over the energy consumption within a given period, t, where the unit is bits/joule [cit] . for an heterogeneous network using only two type of base stations, i.e., n 1 macro base station and n 2 pico base stations, the energy efficiency can be written as follows:"
"in this paper, the impact of mobile backhauling on the total power consumption of a mobile radio network is evaluated. to this end, a power consumption model for mobile radio networks including backhauling is proposed and used to compare the area power consumption of three different heterogeneous deployment schemes under the same area throughput and coverage target. we aim to answer the following questions;"
"research resources are yet another great resource to learn about and to utilize. the research resources at utmb are available to all ctsa funded teams, and consist of help with ethics, education, community engagement, bioinformatics, and so forth. as with the coordination committee, the team manager can serve as the liaison between the scientists and the resource."
"we have investigated the performance of root-check ldpc codes in mimo systems with idd schemes using mmse-sic [cit] . in particular, we have studied numerous scenarios where root-check ldpc codes lose in terms of bit error rate (ber) to the standard ldpc codes at a high snr. we have observed in simulations that the parity-check nodes from root-check ldpc codes do not converge. in particular, with root-check ldpc codes, the llrs exchanged between the decoder and the detector degrade the overall performance. to circumvent this, we have adopted the use of controlled doping via high-order root checks in graph codes [cit] . in our studies, the llr magnitude of the parity-check nodes connected to the deepest fading always presented a lower magnitude level than the other parity-check nodes. in contrast, for the case of standard ldpc codes, this magnitude difference has not been verified. for the case of root-check ldpc codes, the difference in llr magnitude (gaps) at the decoder output for the parity-check nodes has lead us to devise an llr compensation strategy to address these gaps. the gaps and the lower llr magnitude for the paritycheck nodes place the llr values close to the region associated with the nonreliable decision. in addition, in an idd process, such values can cause the detector to wrongly demap the received symbols. therefore, we have devised an llr processing strategy for idd schemes in block-fading channels (llr-ps-bf). first, the a posteriori llrs generated by the soft mimo detector are organized in the"
"for the cases where small low power base stations are used to increase the capacity of the network, the relative effect of backhaul power consumption is getting more influential. this indicates a tradeoff between the power saved by using low power base stations and the excess power that has to be spent to backhaul their traffic. however, it is shown that this impact is not enough to shift the energy efficient deployment toward larger cells. we may expect larger backhaul impact on the energy-optimized networks and cost-optimized networks which consider significant increase of energy cost. moreover, a sensitivity analysis of the proposed power consumption model is performed based on the switch power's traffic dependency. we also investigated different energy efficiency behavior for heterogeneous networks under different area throughput targets which is directly related to their required transmitter power-capacity enhancement relations. the results presented here are obtained for a specific backhaul solution, and may differ for alternative solutions, but the underlining message is that when assessing the benefits of a deployment strategies, the backhaul power consumption can not be simply ignored."
"fading is a major factor that deteriorates the quality of signal transmission in wireless communications. spatial diversity is a useful technique to alleviate the fading effect and can be achieved by installing multiple antennas at a transmitter. however, the transmitters may not be able to support multiple antennas due to the constraints of size, complexity, power, cost, etc. alternatively, cooperation between two single-antenna users can also yield spatial diversity to greatly increase the system reliability [cit] . the relay channel [cit], which consists of a source, a relay, and a destination, is the most elementary framework of cooperative communication systems. recently, the fundamental theoretical limits of the relay channel have been carefully studied in ergodic and quasi-static fading (qsf) channels [cit] ."
"a little more than a year ago, i transitioned from being a participant to leader and manager of an extraordinary team consisting of 20 researchers from various disciplines (structural biochemistry, molecular biology, immunology, physiology, engineering, epidemiology and clinical sciences). i've learned that there is no formal training that can fully prepare junior faculty for leading a team of incredibly intelligent and established researchers. to be a successful leader, you must be committed to the team. you need to invest time learning about what makes a great team and what is needed to maintain productivity. there are numerous resources to help make a great team, but one of the most valuable is finding a committed mentor with strong team leadership experience. as leader, it is your responsibility to keep everyone focused on the team's goals and provide direction. in order to do so, you need be knowledgeable about the work of each member, and willing to take risks to drive the science forward. these activities will help generate respect and trust in you as a leader and team's mission."
"where q is the list of residuals in descending order. we then proceed to the calculation of ξ as (17)- (21) . again, returning to the example given in [4, p. 4, fig. 10], the values of ϕ c i for rolbp throughout the iterations are"
"with respect to job skills of project managers, thematic analysis of these data indicated eight different cognitive or skill-based themes. these are depicted in table 1 . these eight categories represent not only complex integrative skills, but also traditional administrative skills, political skills, and continuous learning skills and capabilities."
"in contrast, in our work, two key elements of an idd system are considered. first, by properly manipulating the log-likelihood ratios (llrs) at the output of the decoder and exploiting the code structure, we can obtain significant gains over standard llr processing for idd schemes in block-fading channels. second, to improve the overall performance, we introduce a new scheduling strategy for block-fading channels in idd systems. the main contributions of our work are the development of a novel idd scheme that exploits the code structure and a novel strategy for manipulation of llrs that improves the performance of mimo idd systems in block-fading channels. in addition, we have also developed a method of sequential scheduling to further improve the performance of mimo idd systems in blockfading channels. the gains provided by the proposed idd scheme and algorithms do not require significant extra computational effort or any extra memory storage."
"at the same time, we recruited research fellows who were either m.d.'s or ph.d.'s to train in translational science at the multi-center level, and had them each lead a project under the direction of myself or one of our site clinicians. they would then attend the quarterly meetings to present their progress -gaining experience in disseminating information to audiences with diverse backgrounds, and to accept constructive criticism designed to help them improve as translational scientists. this project management paradigm developed throughout this time was the basis for our multi-disciplinary translational team (mtt) within the ctsa."
"the rest of this paper is organized as follows. in section ii, we describe the system model. in section iii, we discuss the proposed llr compensation strategy. in section iv, we introduce the proposed scheduling method. section v depicts and discusses the simulation results, while section vi concludes the paper."
"as an individual becomes a more mature scientist, and possibly be designated as principal investigator, they would assume full leadership and managerial responsibility for a scientific team. here, the developmental challenge is one of being a visionary leader, fully capable of collaboration, and the mentoring of young scientists. thus, the roles of liaison (e.g., reporting and coordinating with other teams as well as upper administration) and facilitator (e.g., meeting and discussion leader) seem to be most aligned at this stage."
"the auto-ethnographic case histories used two prominent team project managers. the first (sara dann, ph.d.) is currently an assistant professor of internal medicine -infectious disease, and the second (celeste finnerty, ph.d.) is an associate professor in the department of surgery, and is affiliated with the shriners hospital for children burns center. due to their first-hand knowledge and team science experience, both were asked to serve as collaborators and co-authors for the study."
"for the last ten years, i've served as a project manager for the large-scale, multidisciplinary collaborative efforts for a burns research group associated with a shriners hospital. the position did not exist at the beginning of my postdoctoral experience; the need to rapidly expand the scope of our research, however, made this leadership role a necessity. throughout this period, the requirements of the job, my view of my role in team and project management, the barriers to success, and team management styles have changed significantly."
"r ma,i and r s,i represent the average data rate provided of a macro base station and a small, low power base station, respectively. on the other hand, p ma,i and p s,i represent the power consumed by both types of base stations. these values are obtained using (1) and p bh is the backhaul power consumption of the network, expressed as in (3)."
"learning how to write letters of recommendation/support enlightened p.i.s mentor their team managers in the art of effective letter writing. this art is especially important today in a climate of extremely tight competition for grants, faculty positions, traineeships, and so forth learning how to deal with \"big guns\""
this section presents a power consumption model for cellular mobile networks including the backhauling part of the network. different architectures and technologies are available to implement backhauling: we have chosen the solution which seems the most readily available and which presents the lowest complexity: a fiber optic network based on point-to-point ethernet.
"what emerged from the data was a taxonomy of six essential tasks required of mtt project managers. these are: 1) maintain all reports and team documents, 2) call team meetings, 3) keep minutes of team meetings, 4) assemble meeting agenda, 5) assemble quarterly reports, and 6) accomplish billing. respondents indicated that approximately 10% of their overall work time was devoted to engaging the tasks involving project management. by themselves, these tasks can be fairly easily routinized, resulting in the ability to stay within the 10% limit. respondents seem to understand two types of management style: operations vs. leadership. which style is pursued depends on several factors, inclusive of team size, research topic, the principal investigator, and the background of the project manager."
"in my early postdoc years, i lucked into a project that was associated with a multi-institutional grant; the grant was divided into separate subgroups including patient oriented research, sample analyses, and data analysis. the pi that i worked with was the leader of one of the divisions based on clinical population. during our early quarterly meetings, i played a mostly secretarial role -taking minutes, keeping track of the deliverables that each person was responsible for, and reminding people of these deadlines during monthly phone calls. this role quickly morphed into one to facilitate communication between several teams -our clinical team and the statistical and analytical teams. at this point, my role expanded to include project management. connecting analytical folks -those running tests and those analyzing the data -with the clinical team was necessary to ensure that all projects moved ahead. from this stage, we quickly moved to developing and assigning new projects, and monitoring progress beginning with patient selection and data evaluation through manuscript production. with the transition to junior faculty, i became responsible for writing and presenting group progress reports, making sure that projects were progressing on a monthly basis, and running the group face-to-face meetings when the pi was absent."
"assuming that the systematic symbols for a root-check ldpc code always converge to an llr magnitude greater than zero, we proceed to the following calculations:"
"given the backhauling architecture presented in the previous section, the power consumed to backhaul the traffic from all the base stations to the aggregation network, can be expressed as follows. the average power consumption of a base station p i is modeled as a linear function of average radiated power, that is:"
"where is the hadamard product. the final step in the proposed llr-ps-bf algorithm is to generate the a posteriori llrs to be used by the idd scheme. therefore, the optimized vector of the a posteriori llrs is given bỹ"
"where, p dl is the power consumed by one downlink interface in the aggregation switch used to receive the backhauled traffic. it is assumed that: 1) all switches are identical, 2) each base station in the network, regardless of its type, uses a dedicated downlink interface, and 3) all downlink interfaces are identical and use the same transmission rate. n ul and p ul are the total number of uplink interfaces, and the power consumption of one uplink interface, respectively. it is also assumed that all uplink interfaces are identical. n ul is a function of total aggregate traffic collected at the switch(es) (ag tot ) and the maximum transmission rate of an uplink interface (u max ). more formally n ul can be expressed as"
"respondents note that the team managers can and should be responsible for generating and maintaining reciprocal relationships among teams. for example, one team manager noted how valuable it has been for her to schedule visit for the team to one of the more successful, large and established teams learning how to manage one's personal and professional time"
"in my role as the burns mtt manager, i was the point person for organizing and directing the meetings and projects along with the mtt director. the burns mtt included clinicians, basic scientists, fellows, students, biostatisticians, and advisors. projects were led by faculty or fellows and were reviewed intensively by the individuals and small groups during the two weeks between meetings and then presented to the entire group at the mtt meetings for review of the analyses and results, and discussion about how to better evaluate the data sets or what the next step should be. in order to prepare our trainees to lead translational science efforts, we are now transitioning to having the fellows and postdocs run the meetings. we anticipate that this change will provide leadership and communication experiences that will give these energetic team members an advantage in their future endeavors."
"with the development of new power saving techniques in ip/ethernet switches the traffic dependent portion of power consumption would gradually play a more important roles in the future. in fig. 3 we assumed five different α values which reflects the percentage of the switch power that is independent of network traffic for macro+wlan deployment scenario. as we can see, with an increasing energy efficiency of the switchs (decreasing of α values), the impact of backhaul on the overall network power consumption decreases."
"university of texas medical branch, supported in part by a clinical and translational science award (ut1tr000071) from the national center for advancing translational sciences, national institutes of health. we acknowledge the staff of the institute for translational sciences at the university of texas medical brach at galveston for their assistance in the coordination and scheduling of interviews obtained."
"a. backhauling solution figure 1 shows an example of a fiber-optic carrier ethernet backhauling solution for a heterogeneous mobile network. ip/ethernet interfaces for backhauling has been standardized in 3gpp beyond mobile data core elements and out towards the base stations and radio controllers [cit] . ethernet switches for backhauling can be flexibly located in the distributed cell sites, or in a centralized aggregation node, or at both locations to have several levels of aggregation. in this study, for the sake of simplicity, we assumed a backhaul system with one level of aggregation only, i.e., the traffic from all wireless nodes is collected at one or more aggregation switches (if the number of the traffic streams to be aggregated so requires) just before the edge service node or the aggregation network. it is assumed that all backhaul links from all the base stations (macro, pico and wlan) to the aggregation switch(es) are optical fibers. each cell site has an optical small-form factor pluggable (sfp) interface connected to an ethernet switch port at the aggregation site. an alternative solution would be to use copper cable connections for the downlink. however, even if this solution would be acceptable for 3g umts networks, copper cables would not provide enough bandwidth to support long term evaluation (lte) and lte beyond networks which offer 100 mbps to each user."
"overall, our analysis of available literature and from case and interview data suggest four key conclusions. first, team science leadership and project management skills are critical for developing future scientist. second, development of team leadership and project management skills should be conducted as part of an overall career life cycle process, starting with graduate students and culminating in mature principal investigator's engagement of multidisciplinary and transdisciplinary teams. third, both knowledge and skills of team science are required of future scientist, and therefore both formalized and experiential opportunities must be developed and provided to enable the development of complex skills and contextual learning. last, postdoctoral experiences to develop leadership and project management competencies should be sequential relative to roles and tasks required, in accordance to the maturity of each postdoctoral student."
"analysis of the narrative from the case reports and interviews indicates two clear implications for postdoctoral training and team project management. first, respondents made the point that tenure track researchers are generally best equipped to serve as team managers. assistant and associate professors are already aligned with on-going research groups. their assigned managerial tasks coincide well with their personal scientific interests. there was agreement that a managerial assignment can seriously interfere with a postdocs main priority: focusing on a research problem, possibly publishing follow-up studies from one's dissertation, learning how to write grants, and in general move along with one's career. the implication is that a tenure track scientist has the experience and, hopefully, maturity to know how to exercise formal leadership (e.g., dealing with team members from all ranks and generating constructive communication across disciplines and specialties as expected in team science). thus, perhaps assignment of formidable team project management responsibilities should be restricted to the most mature postdoctoral students."
"following the narrative case history methodology [cit] ), all four collaborators reviewed the case histories in conversation to resolve differences of view in order to generate the analysis presented below. the authors conferred on their analyses to arrive at consensus. given the relatively small number of interviews, we decided not to utilize any qualitative coding software. interview data was subsequently content analyzed and coded for purposes of exploring emergent themes."
"whereas the research base on team effectiveness and development has progressed [cit], so too has the identification of specific team-based competencies. general team competency models [cit] and virtual team competency models [cit] ) depict a broad range of complex social, behavioral, and cognitive skills required for effective team member performance. recently, literature has been emergent relative to team skills related to translational science. [cit] education and career development key functions committee has developed core competencies for graduate (master's) programs in translational research. among the 14 defined competency areas are two specific to team science, consisting of translational team work (i.e., managing interdisciplinary teams of scientists, group decision making, managing conflict, etc.) and translational team leadership (i.e., fiscal and personnel responsibilities, fostering innovation, etc.). [cit] note that communication skills and negotiation skills are paramount for functioning in translational teams, along with critical thinking skills, and the ability to work and lead collaboratively in team management systems. [cit] have provided a field guide for collaboration and team science with identification and recommendations involving team competencies. among these are building a team, fostering trust, developing a shared vision, communicating to others, sharing recognition and giving credit, handling conflict, strengthening team dynamics, and navigating and leveraging networks and systems. [cit] also note the need for translational scientists to receive training in teamwork and communications skills. [cit] have as well suggested a model for team science which involves specific attitudes (e.g., mutual trust), specific behaviors (e.g., communication), and cognitive attributes (e.g., shared mental models)."
"first this section briefly describes the network layout under consideration and the method used to obtain each deployment. then, the area power consumption for each heterogeneous network deployment is calculated using the power consumption model introduced in sec. ii. in the presented case study, the parameter m introduced in (2) is assumed to be equal to two. this means that only two types of base stations are considered for each heterogeneous network deployment, i.e., 1) macro base stations in combination with pico base stations, and 2) macro base stations in combination with wlan access points."
"and managing a team of transdisciplinary researchers is an amazing experience, but not an easy task. it is different from running your own lab, in which you have autonomous control over everything. leading and managing a team of strong individuals, trained to work independently, requires a unique skill set to overcome challenges and to successfully achieve goals."
"review of the case analysis narrative as well as the emergent task and skill themes from interviews suggest that they are likely context specific findings. previous reports of utmb's research culture [cit] provide evidence that changes in funding, research team participants, and research foci give rise to situation specific perceptions. thus, the data reported here are most genralizable to the approximately 60 ctsa funded sites. while it is questionable whether these results are applicable beyond the biomedical sciences, given the growth in team science, collaborative teams, and funding priorities for translational efforts, these results may be most useful in addressing future training needs."
"the graduate school level (i.e., master's and doctoral) would be the optimum time for acquisition of formalized knowledge covering the basics of team science and leadership skills. here, basic self awareness is of great importance, with the goal of acquiring the basic knowledge and skills. as a graduate student, performing the roles of observer (e.g., observing the roles and skills of team members and leaders) and evaluator (e.g., diagnosis of scientific issues and team dynamics) seem most advantageous as a developmental building block. during the postdoc phase, we recommend that the opportunity for expanded skill development be provided. in this postdoc phase, we suggest that leadership skill development can be accomplished by the limited roles suggested by this study in a project management position. thus, knowledge and skill development is the primary goal. here, engaging the roles of record keeper (e.g., keeping minutes, tracking progress) and scheduler (e.g., agenda communication) seem most appropriate. this provides opportunities for short term and specific assignments involving team and project management without the full responsibility of administrative authority. this of course would be contingent upon the maturity of a given postdoc, the stage of the project, and the size of the team. the postdoc then can test out and refine their leadership capacities without conflict with publication and funding requirements or career consequences."
"to address the issue of when one should assume team project management responsibilities, figure 1 proposes a life cycle model for the overall development of translational team skills. rather than rely on serendipitous acquisition of team and leadership skills, we propose that such acquisition be specifically phase related. as shown, we propose that a lifecycle continuum for the development of translational team skills be considered. this continuum is initiated as a graduate student, progressing to the postdoc status, to assistant professor, and culminating in the status of mature scientist. at each stage, we propose that there is a different training purpose, differing developmental challenges, and different roles and tasks that might be appropriate."
"the aim of calculating l p t is to ensure that the llrs of the paritycheck nodes do not get close to the region associated with nonreliable decisions. as a consequence, the llrs fed back to the detector will not deteriorate the performance of the demapping operation. in the appendix, we detail how the proposed llr-ps-bf compensation scheme works."
"in this paper, we proposed a new power consumption model for mobile radio network where the backhaul power is taken into account. the impact of backhaul on the total power consumption was shown, by means of numerical simulations, to be significant for three heterogeneous network deployments (i.e using only macro base stations, macro and pico base stations, macro base stations and wlan access points)."
"what then can be concluded from the studies articulating the need for team related training? it is clear that training in team skills is important, and changes in translational science require competence in multidisciplinary and transdisciplinary environments. there is strong evidence from general team training research and meta-analysis reports [cit] suggesting that team training not only improves team skills, but also improves objective performance."
"we propose that after the postdoc phase, it is appropriate in the assistant professor phase to then make project and team management responsibilities more prominent. at this stage, more formal responsibility would allow for skill enactment and refinement. it is at this stage that the roles of presenter (e.g., active participation in presenting research results) and coordinator (e.g., arranging for participants and external disciplines to collaborate, prioritizing agendas) might be most legitimate."
the doctoral thesis by klas johansson [cit] presents a series of cost effective deployment strategies for heterogeneous wireless networks. we have used this basic method to study the power consumption of the following heterogeneous wireless networks;
"each case history and interview was audiorecorded and transcribed. a total of twelve interview questions were asked, and were generated from the existing literature on translational research teams. these involved: 1) history with the institution, 2) team involvement, 3) perceptions of translational science, 4) promotional experience, 5) managerial selfperception, 6) prior leadership/managerial experience, 7) mentoring received, 8) skills acquired, 9) managerial responsibilities, 10) experiential reflection, and 11) best practices/advice. the two senior authors reviewed, content analyzed, and coded the interview data for purposes of exploring emergent themes."
"in this section, the total power consumption of the three heterogeneous deployments scenarios presented previously are compared. all three deployments have the same characteristics in terms of coverage and capacity, and they just differ in the number and in the type of base stations used. in order to compute the power consumption of the backhauling segment the values in table ii are used, while the power consumption parameters of each type of base station are summarized in table iii . the transmission speed for the transmitters and the receivers at the downlink interface is assumed to be 1 gb/s, while the maximum transmission speed for the uplink interface (u max ) is 10 gb/s. the parameter α and max dl are equal to 0.9 and 24 respectively, while ag max is equal to 24 gb/s. in fig.2 the impact of backhaul on the overall power consumption of the network is shown for three different network deployment schemes. the power is computed in terms of area power consumption and is presented as a function of the area throughput. as it can be seen, the relative effect of the backhaul power consumption is increasing as the more small, low power base stations are used. especially for the macro+wlan deployment scenario, the total network power consumption is almost doubled and with a rate that increases with higher area throughput values. however the effect of backhaul is not as significant when it comes to the case of dense macro deployment where too many additional base stations are not needed to satisfy the required area throughput, as it was for the macro+wlan case. this result indicates the trade-off between the power saved using low power base stations and the excess power that spent for backhaul. on the other hand, even with the inclusion of the extra power consumed by the backhaul segment, the use of low power wlan access points and pico base station still remains the best choice to have a network with an overall reduced power consumption. it should be noted that the presented network deployment results are not optimized in terms of power consumption as explained in the previous section. the moment power consumption (including backhaul) is considered as part of energy based optimization process, it would be expected that overall power figure for all three deployments will be lower as well as the gap between each deployment strategy."
"where m is the number of base station types used in the network, n i is the total number of base stations of a specific type i-th (e.g., macro base stations), and p i is the power consumption of a base station of type i, which is calculated using (1). the backhaul power p bh includes not only the downlink and the uplink power consumption (i.e., from a base station to the aggregation switch(es) and from the switch(es) to the aggregation network, respectively) but also the power consumed at the aggregation switch(es), which is proportional to the total traffic backhauled from the mobile network. a detailed expression for p bh is given by"
"the team research concept, especially at centers like utmb that pay special attention to team structure and function, result in high powered, veteran researchers sharing the status spotlight on inter-disciplinary teams. respondents note that a successful team manager must learn how to support egos while simultaneously stoking creativity and innovation."
"which results in a scheduling method that decreases the prioritization, as seen in (15). by adopting this strategy, we ensure that rolbp outperforms both the standard bp and rlbp algorithms. the reason is that we give enough information to the root connections and avoid the values for ϕ c i, as in (15), which cause a degradation in performance of root-check-based ldpc codes. the pseudocode is described in algorithm 1."
"team managers learn a range of skills that will be valuable in the future, and time management is one of the more practical skills. in general, this is an illustration of how the team manager experience is a great resource for learning how to be a p.i. [cit] identification of targeted communication skills are noteworthy. [cit] identification of situation awareness, adaptability, and interpersonal relations as related to our findings of dealing with administrators from many constituents that change over time. the political nature of many of the skills identified relate directly to the popular notions and literature of influencing without authority [cit], and to upward influence tactics [cit] . the skill area of time management relates to much popular literature as well as empirical findings [cit] suggesting that effective time management is important to managerial success and well being."
"findings from this study suggest that, at least for postdoctoral students involved with managing multidisciplinary and transdisciplinary translational teams, that training in project management and team science should be customized to the needs in the biomedical academic environment. in addition to the teamwork and communication basics, results from this study suggest that a developmental emphasis should be placed on upward influence and time management. these findings may be generalizable to other similar type of research teams (e.g., engineering and information technology), particularly if it requires changing membership, innovation, management of multiple disciplines, and numerous constituencies with institutional power."
"besides the environmental concerns, there is a strong economical motivation for network operators to decrease the power consumption of the network. the main consumers are data servers, backhaul routers and base stations (bs) which constitutes between 60 to 80% of the overall network power consumption [cit] . energy efficiency can be improved in two main ways. the first consists of reducing the power consumption of the main consumer, i.e., the bs (either by using more power-efficient hardware or by using more advanced software to adapt power consumption to the traffic situation). the second is intelligent network deployment strategies where using high density deployment of low power, small base stations is believed to decrease the power consumption compared to low density deployment of high power macro base stations. the idea being that a bs closer to mobile users lowers the required transmit power due to advantageous path loss conditions [cit] ."
"the coordination committee is the top group of ctsa administrators at utmb. it is composed of esteemed scientists and grants persons, clinical administrators, and university leaders. the well-trained team manager learns not only how to interact with the coordinating committee, but also learns to see it as a great role model for managers."
"finally, the discrete signal l c is processed by g[l c ] to generate the compensated version of l c calledl c . therefore, g[l c ] is defined as"
"recently, ctsas have been purported as exemplary frameworks for development of team science [cit] . specifically, multidisciplinary translational teams, or mtts, are utilized not only to generate collaborative science, but also provide fertile training arenas for emerging translational scientists. in the mtt model, graduate students, postdoctoral students, and junior faculty are recruited and deployed along with mature scientists from many disciplines. frequently, postdoctoral students and trainees operate as team project managers, under the directive and mentorship of the principal investigator. however, recent literature [cit] involving the need for team leadership training and preparation for postdocs suggest there is much to be gained from more formalized efforts."
"over the last decade global warming has become an important issue on the political agenda and efforts for reducing the power consumption and, consequently, the carbon emissions is currently attracting a lot of attention [cit] . in this regard the information and communication technology (ict) industry is seen as an increasingly important contributor with an 8% [cit] . mobile communication networks alone consume 0.5% of the global energy consumption [cit] . with the increase in the demand for broadband services that we are experiencing nowadays, there is a need for denser networks and in this context increased energy prices are expected to constitute a significant challenge in the near future [cit] ."
"the increased emphasis upon translational science has placed tremendous pressure upon graduate schools in clinical and biomedical education programs to recalibrate their educational competencies and educational processes. the national institutes of health (nih), through the clinical and translational science awards (ctsa), is reshaping how biomedical science is conducted. the nih has articulated a road map [cit] calling for a redefinition of the ways in which medical research is conducted, which is intended to produce significant improvements in health. at the center of this movement is the use of research teams."
"we focused our interview investigation upon project managers for the twelve multidisciplinary translational teams currently operating at utmb. a total of eighteen individuals were interviewed, inclusive of nine assistant professors who have served as project managers, but who began this job while serving as postdoctoral trainees; five postdoctoral trainees currently serving as mtt project managers; and four associate and full professors who work extensively with postdoctoral teams, and have experience in mentoring project managers."
"p max represents the maximum power consumption of the switch, i.e., when all the downlink interfaces are in use, while ag max is the maximum amount of traffic a switch can handle. the model presented above is then used to compare the power consumption of three heterogeneous networks with and without considering the backhaul power consumption. details on this case study are presented next."
"there is some limited research [cit] suggesting that graduate training in specific team competences (e.g. data dissemination) is helpful in developing team based behavior in research groups. [cit] have recently examined training and education practices in ctsa institutions related to team science. they report that 86% of the respondents suggested training in team science was important, but unfortunately only 52% of the institutions sampled offered such training."
"the purpose of this paper is threefold. first, we examine the growth of team science and the importance of developing team skills for those engaging and preparing for careers in the translational sciences. second, the importance and contextual significance of team skills will be explored through first-hand case experiences of translational team project managers. through cases and interviews we shall explore the essential tasks and needed skills for team project leaders and managers. these shall be explored in relation to traditional project management and team science literature. last, we shall propose a life cycle model for the development and enrichment of translational team competencies, isolating how this would impact the postdoctoral experience and education. building on this, specific roles and tasks appropriate for postdocs assuming project manager responsibilities will be explored."
"the case study results, which are based on two highly successful project managers who were previously postdocs, suggest that serving as a project manager in a multidisciplinary translational team is a very helpful career development opportunity. [cit], young scientists need a shorter pathway to success. perhaps serving as a project manager while a postdoc may facilitate early career achievement, as it has been suggested that early exposure to collaborative projects can be helpful to career success [cit], and more importantly to later stages of one's career [cit] . given the longer developmental trajectories due to the complexity of collaborative science, exposure to translational projects has become all the more critical [cit] has suggested that the metrics for successful scientific careers should involve team leadership. thus, as evidenced by the two case reports in this study. participation as a project manager may play a big role in later career advancement."
"proposed llr-sp-bf scheduling idd scheme must be calculated where the fading happens at index δ f . this is unique for block-fading channels; other types of channels do not require these additional steps. then, the extrinsic llr is obtained as"
"can be obtained by time averages of the corresponding samples over the transmitted packet. after the first iteration, the mmse soft cancelation performs sic by subtracting the soft replica of multiple-access interference components from the received vector asr"
"to become an effective leader you do not need to be the most intelligent person in the room, however you must be able to inspire and nurture talent and empower others to succeed. the best way to learn this is through observation and communication. as a graduate student and postdoc, i took the opportunity to observe leaders in action. i also discussed with my peers their experiences with their mentors and took time to reflect on my own experiences as a mentor and mentee. it was through these activities that i learned about different leadership and management styles and determined what styles and characteristics promote success. the strongest leaders, managers, and mentors are actively, but not overly, involved in projects and provide nurturing environments through encouragement and support. during my training, i actively sought out numerous mentors who provided me with many views on management styles. i gleaned what i thought were the best qualities from each and try to incorporate them in my everyday life. becoming a great leader and manager is an evolving and continual process. you learn from your mistakes, failures and successes, and use that knowledge to fine-tune your style and use in future endeavors. learn everything you can from every style that you can and in the words of my greatest mentor: \"take heed, take notice, go forth and conquer!\" case history: celeste finnerty, ph.d."
"as future work, we plan to improve the power consumption model of ethernet switches by considering the scalability (i.e., with the possibility to switch off individual ports rather than whole board which may be expected from future switches) and use different backhaul network architectures with less general assumptions (e.g., using nonidentical switches, decentralized aggregation node)."
"). an idd scheme with a soft mimo detector and ldpc decoding is used to assess the performance of the system. the soft mimo detector incorporates extrinsic information provided by the ldpc decoder, and the ldpc decoder incorporates soft information provided by the mimo detector. we consider inner iterations as the iterations done by the ldpc decoder and outer iterations as those between the decoder and the detector. in addition, in the decoder, a novel scheduling method is used for block-fading channels. the proposed scheduling method combines the benefits of the layered bp (lbp) and the residual bp (rbp) [cit] algorithms, as will be discussed in section iv. in the idd scheme, for the jth code bit x j of the transmitted vector x of each antenna, the extrinsic llr of the estimated bit of the soft mimo detector is given by"
"then, we can solve the block-fading problem by generating a queue q of all ϕ c i in a descending order from the largest to the smallest to obtain the corresponding indexes of the check nodes as"
"with high-penetration power electronics in the future grid, the power system dynamics are moving towards millisecond time scale electromagnetics and being control mode dominated. accordingly, harmonic resonance could be introduced at a range of a few hundred hertz to a few kilohertz, because of the control bandwidth, harmonic generation, and impedance interactions of inverter output filter and line impedance. normally it is hard to design a dedicated pss to suppress such resonance. however, each converter in the system can be designed to share the system stabilization function in a distributed way while providing their primary functions. such techniques include various active damping, impedance reshaping, feedforward control, passivity-based control, and other robust control techniques."
"for smart grid interoperation, the concept of \"energy services interface (esi)\" was proposed in the nist framework and roadmap for smart grid interoperability standards. an esi is a bidirectional logical, abstract interface that supports secure communications between internal entities, for example, a facility with ders and loads, and external entities (e.g., the utilities). the esi adopts service-oriented architecture and serves as a platform that provides energy services to both the facility side and the grid side. even though ieee 1547 is well established as an interconnection standard for ders, there is a trend to push the te and esi concepts from building loads towards generic ders."
"where is the transfer efficiency and is the signal-to-noise ratio of the receiver. the profit of the user, (, ), can be expressed as"
"th e i n c o m p l e t e g a m m a function 0, 1 : th e z e r o h y p o t h e s i s a n d t h e alternative hypothesis 1, 2 :"
"th e c o s t o f u s e r, : th e c o s t o f c h a n n e land the throughput requirement to access the channel *, * :"
"reputation systems are widely used to cope with liars holding false positive/negative opinions [cit] . the concept of reputation has been widely used in economics, ecology, anthropology, and other social sciences. a rich body of literature has been devoted to the investigation of different reputation systems for computer networks [cit] . recently, derived from the dempster-shafer theory [cit] and with the ability to explicitly represent and manage a node's uncertainty, subjective logic and uncertainty based reputation mechanism has emerged as an attractive tool for handling trust relationships and has attracted much attention in distributed crns."
5) rule five locates the element of minimum stress and switches the material property to the weak material property if the design material property was initially present.
"with the booming of the internet of things, digital communication capabilities are replacing the old-fashion analog control interfacing in power plants. for inverter-based distributed grid devices, standardized and nonproprietary communication protocols are required for interoperability and control purposes. on the contrary, power electronics and embedded control units are facing potential risks from cyberattacks, inadvertent control errors, and hardware failure. hence, conventional power electronics design needs to be reexamined from a cybersecurity perspective to identify its vulnerabilities and physical impacts, and a secure and resilient power electronics architecture will need to adopt a multilayered protection mechanism to integrate power hardware, sensors, control/communication hardware and software, and to offer hardware-reinforced cybersecurity functions."
a major issue of concern in the assignment of an alterable material property for each element in the defined mesh is the total number of elements considered.
typical stability problems in power grids are within low frequency (i.e. subsynchronous or subharmonic range) due to a large amount of rotational inertia and relatively slow electromechanical dynamics. subharmonic resonances happen in large power plants and inter control areas. dedicated power system stabilizers (psss) are normally equipped to damp the oscillation and maintain system stability.
"the final designs generated by the phase one search are \"dirty designs\" as they contain obvious design flaws. the convergence of the phase one search was found to be slow which made it difficult to assign the population size required for a true global search to be performed. as the mesh was refined to form smaller elements, the problem size increased to the point where a reliable solution could not be located through the phase one search alone."
"a multifunctional smart inverter demands optimized and flexible control structure to facilitate transition among operation modes or control functions. research trends are moving towards control modularity [cit] and/or universal control architecture [cit] driven by software switches or control parameters. normally an inverter is designed for a dedicated application, such as solar, wind, and energy storage, with specific control requirements and functions. the concept of universal inverters, more like general power amplifiers, has emerged in recent years. these inverters can be customized by software modification to adapt to specific applications; hence, they are referred to as software-defined inverters."
"cognitive radio networks (crns) are promising wireless communications systems that can resolve the spectrum scarcity problem arising from the escalating demand of wireless radio frequency and spectrum underutilization by license holders [cit] . the architecture of crns is depicted in figure 1, which consists of base stations and cognitive terminals. the base stations (i.e., primary users) constitute a primary network using the licensed spectrum and the cognitive terminals (i.e., secondary users) form a secondary network that makes use of the licensed spectrum when it is not occupied by the primary users [cit] . to avoid the potential interference with the primary users, secondary users firstly sense whether the spectrum of interest is being used by the primary users. if the spectrum is unoccupied, the secondary users apply certain spectrum allocation scheme to decide which of them may access the available spectrum [cit] ."
"the strategy establishes a distributed reputation database for nodes as a basis for the channel search sequence in spectrum sensing. next, we design a novel vickrey-clarke-groves (vcg) mechanism [cit] based on the reputation generated from exchanged sensing results and propose a novel cheat-proof spectrum resource allocation strategy to restrict the impact of the malicious behaviours. as an important mechanism design, the vcg mechanism studies how to design mechanisms to incent the players (i.e., users or nodes) to provide truthful information about their preferences over different outcomes [cit] . a vcg mechanism is a dominant strategy mechanism, which can achieve ex-post incentive compatibility (truth-telling is a dominant strategy for every player in the game) [cit] . this paper makes the following main contributions."
"in the rest of the paper, section 2 reviews related work. section 3 introduces the network and adversary models. section 4 presents the distributed secure cooperative sensing strategy. section 5 presents the vcg based distributed cheat-proof spectrum allocation strategy. section 6 presents the simulation results and performance analysis. section 7 concludes the paper."
"traditional power system control philosophy is to assign different system functions to different resources and devices based on their economics and dynamics, resulting in dedicated devices for dedicated functions (e.g., generations for baseloading, peak-loading, and regulation, respectively, and devices for harmonic and reactive power compensation, separately) and centralized system control structure. with the maturity of power electronics and continuous advance on controls, smart inverters offer the potential to serve multiple functions (e.g., with the primary function of energy feeding and with secondary or ancillary functions of grid support based on capability or shared-grid voltage/frequency control, which renders a distributed system control philosophy and flexible grid devices in future power grids). some of the ancillary control functions include harmonic compensation, unbalance compensation, damping injection, synthetic inertia or fast frequency response, and dynamic reactive power control."
"the details of the dcsa are described in algorithm 2. in dcsa, the application and allocation of channels are done in competing slots, which have two stages. at the first stage, the base station calculates the best allocation results and then allocates the channels based on the demands from primary users. primary users can apply for consecutive slots to complete its transmission. at the second stage, base station allocates the remaining vacant channels to secondary users, who can only apply for one slot in order to avoid the interference with primary users. after the allocation, base station waits for the users to complete transmission and then reallocate channels in the next competing slot."
five rules were developed for demonstrating the operation of the phase two search process. each rule works independently or in a synchronized manner with multiple rules in order to achieve an improvement in the objective function which is to reduce the overall volume of the current plate design. listed below are the five rules developed.
"in this section, we implement our strategies and conduct extensive simulation experiments using matlab to verify the efficacy of the proposed strategies. we assume that the malicious nodes can launch ssdf attacks in the sensing and allocation process by reporting falsified values. they can also implement the random attack strategy or the intermittent attack strategy which means that the attacks are launched intermittently in a random way. for the intermittent attack strategy, we simulate the scenario where the malicious nodes attack with a 50% intensity. the intensity stands for the probability that the malicious node launches an attack during an interaction."
"recommendations electric utilities have experienced operational challenges from high penetration solar power and frequency stability issues due to a significant amount of instant tripping of traditional pv inverters connected to low-voltage distribution networks. in germany, the study of the \"50.2 hz problem\" suggests technical retrofitting of existing pv inverters with frequency-dependent active power control and stochastic reconnection. in the united states, utilities have foreseen the phenomenon of \"duck curve,\" or \"nessie curve,\" which depicts potential solar power overgeneration resulting in a reduced net load profile during midday (10:00 a.m.-2:00 p.m.), followed by a sharp ramp-up in later afternoon (3:00 p.m.) to evening (9:00 p.m.) requiring expensive and fastresponding generators to react. therefore, the utilities have started to investigate new interconnection and operation requirements for inverter-based ders."
"with the maturity of power electronics topologies and recent advances in wide bandgap semiconductor devices and high-frequency magnetic components, power electronics are approaching performance limitations from the internal driving philosophy, and the future technological development of power electronics is primarily driven by emerging applications as recognized by wyk [cit] and kolar [cit] control workshops of the us office of naval research [cit] that power electronics are at the crossroads with the goals shifting from hardware performance metrics (i.e., smaller size, lighter weight, and lower cost) to more control, more functions, more integration, more flexibility, and more commonality. being one of the major emerging applications for power electronics, electric power grids are experiencing the vast inception of high-penetration power electronics from transmission backbones (e.g., hvdc and facts) to grid edges (i.e., distribution systems), as illustrated in fig. 1, and are transitioning into an integrated and power electronics-based grid."
"cyber-secured, grid-connected power converters will require development of hardware-based authentication mechanisms, analog ic device-based protection, and fail-safe design features for power converter controller and auxiliary circuits (interfacing and gate drive circuits). specifically, a multilayer hardware protection architecture will be need to be developed and implemented based on reconfigurable hardware and analog devices to survive potential threats from cyberattack and system fault or failure. in addition, hardwarebased authentication will be integrated to implement encryption algorithms and to protect firmware from malicious cloning and tampering by challenge-response strategies. mechanisms on runtime monitoring and secure bootstrapping will be devised to secure software integrity. fig. 2 shows a multilayer protection scheme of the controller architecture. for authentication, reconfigurable hardware (fpga or asic) or commercial cryptoprocessors will be used to provide immunity to any network attack. protection logics for overvoltage, overcurrent, overtemperature, and shoot-through faults will be implemented using analog operational amplifiers for physical errors or unknown glitches."
"the phase two optimization progress is plotted in figure 16 and figure 17 . the phase two process converged in approximately ten generations and while neither the stress nor displacement constraints were fully active, the design stress is approaching the specified limiting value. in this design exercise, the phase two search resulted in a more modest reduction of additional material, but the resulting design can be seen to be more refined in nature. the other interesting point of note is that the phase two result reduced the amount of material required for the design while increasing the safety margin of the design with respect to both displacement and stress criteria."
"where success is the number of successful channel accesses and select is the number of channel accesses. we can observe in figure 3 that all the three strategies can detect the channels from 1-4. but, since both the random and edso strategies do not adopt the cooperative sensing scheme, they cannot detect channels 5 and 6. the proposed dscs adopts the cooperative sensing scheme, which makes it possible to receive the sensing results recommended by the other cooperative users, so it can gain the status of channels 5 and 6 and use them to improve the channel utilization rate. moreover, it is seen that the sensing accuracy performance of the dscs is better than that of the random and edso scheme. in the random and edso scheme the channel status information is computed and obtained only from the local sensing results, while in dscs the secondary users gather channel status information from the cooperative users and compute the final sensing results through combining the local sensing result with these cooperative sensing results."
"based upon the results achieved with the twenty-four and eighty-eight element plate design examples, it seems like a reasonable assumption that additional mesh reduction may allow for a further increase in the volume of material removed from the original design region. a one hundred forty element plate was constructed to investigate this possibility. design topologies corresponding to the phases one and two results are illustrated in figure 18 and figure 19 . an american journal of computational mathematics examination of figure 18 reveals that the phase one global search solution had several disassociated elements in the final design as well as elements beyond the load application points which are both indicators of the level of difficulty this size problem presents for a traditional genetic algorithm in locating a refined solution. once again, the phase two solution remedies the design discrepancies and produces a fairly refined design."
"various stakeholders, such as industries, utilities, government, and standards bodies, have been working together to develop common, standardized control functionalities for interconnection and interoperability of ders with electric power systems. these efforts address various issues from standard control functions, information and data models, communication protocols, and grid codes to compliance and certification testing. an overview of us smart inverter activities is given in table i ."
"in this paper, the state of the art in smart inverter standard development is first reviewed. then potential integrated system functions are proposed for future smart inverters beyond those common functions defined in the standard. furthermore, research trends for smart inverters are projected in terms of desired features and control architectures, followed by the conclusion."
"(2) a distributed algorithm is designed to help secondary users compute the sensing result and allocate the spectrum. secondary users iteratively update their local values to arrive at consensus, without help from any central authority."
(3) simulation results demonstrate that the proposed strategies can provide an effective countermeasure against the internal ssdf attacks without relying on a central authority or a common control channel and are therefore applicable in distributed crns.
"power inverters exhibit physical hardware constraints and may require additional hardware and oversizing design, depending on the required control functions. new generations of power semiconductor devices based on wide bandgap materials have been developed that push the boundary of power electronics to higher voltage, higher frequency, and higher temperature [cit] . nonetheless, compared with conventional iron or copper-based components, semiconductor-based power electronics devices are still less efficient during normal full-power processing conditions and are less robust, or rather fragile, during faulty conditions, not to mention cost and reliability concerns. therefore, highefficiency, high-power, and overloading or short-circuit interrupting capabilities are key performance metrics of power devices to enhance and fortify power electronics for seriesconnected grid applications, such as solid-state transformers and circuit breakers."
"the plate design process investigated incorporates a genetic algorithm programmed in microsoft visual basic [cit], and caefem [cit], a finite element solver which performs a linear static analysis for each design topology considered."
"the fields of expert systems and neural networks may be utilized in order to capture elements of the manual design process. this may, however, limit the design to known techniques and procedures which removes the possibility of creating a novel or revolutionary design. the best of both worlds would be a process which exploits the benefits of a global optimization process, but is guided or influenced by domain specific knowledge. this is possible within the framework of a genetic algorithm. this process may well maintain the global nature of the search as well as to allow for an adaptive framework over time as rules are modified to improve performance. a simple framework is presented, but this framework provides a rich development platform for more sophisticated structural design optimization approaches."
"smart inverters with integrated protection functions, such as self-awareness functions, advanced prognostics, and health management algorithms, can also achieve internal condition monitoring, lifetime prediction, and improved operational reliability, effectively avoiding catastrophic accidents in safety critical systems using power electronics."
"grid interconnection standards require the inverter to be capable of detecting grid abnormal conditions and of responding with correct actions (e.g., either de-energizing or keeping online and injecting reactive power). there is a tendency to integrate protective relay functions, even fault current blocking and interrupting functions like a circuit breaker, into the converter system, given the fault current control capability of power electronics and the fast switching behavior of semiconductor devices, which can detect the fault at the very beginning and interrupt/isolate the fault current using lower rated devices because the fault current magnitude is relative smaller initially."
"input: wireless channel set, detectable channel set, output: three most trustworthy secondary users (trust 1, trust 2, trust 3 ) and an untrustworthy secondary user (notrust ) (1) begin (2) setup the spectrum detection collaborative group (cgsd) for secondary user ;"
"realizing that today's power grid is facing challenges due to more renewable energy and more distributed system structure, doe developed a grid modernization multiyear program plan with six defined tasks. the first task is to develop smart devices and an integrated system that interoperate with each other, with grid sensors and grid control systems, and that interconnect to maintain stable grid operations while providing valuable grid and local energy services. power electronics (i.e., the inverters associated with ders) [cit] to reduce the cost of integrating renewable energy by 50%, reserve margins by 33%, and reduce outages by 10%, together with energy storage and other grid devices."
(1) the reputation model and vickrey-clarke-groves (vcg) mechanism are introduced into the coopera-tive sensing and spectrum allocation strategies. this combination can better reflect the real world nature of communication networks and defend against spectrum sensing data falsification (ssdf) attacks from internal malicious nodes.
"existing spectrum sensing and allocation methods and security mechanisms are usually based on a centralized infrastructure, where a central authority plays an essential role in coordinating the defense against attacks and thus brings heavy communication overheads and the issue that central authority may be compromised by attackers. moreover, few works took into account the joint design of spectrum sensing and spectrum allocation, but they only considered individual spectrum sensing or allocation. in particular, they did not consider the internal attacks launched by an inside attacker that has the legal identity. consequently, it is still an open problem and a challenging task to design secure and distributed spectrum sensing and allocation schemes in crns to resist the internal attacks. the main notations and symbols used in this paper are summarized in notations and symbols."
"although various grid-hardening approaches can be adopted to strengthen the grid infrastructure's physical resistance to natural events, the existing centralized and widely synchronized us grid architecture poses risks on cascaded failure. ultimate grid resilience shall be approached from system architecture perspective. presently there is no established standard for the design of resilient power systems, and the concept of design-for-resiliency becomes crucial to achieve the long-term grid resiliency goal. to make the power grid more adaptable and elastic to continuously changing and dynamic conditions from either climate or cyber events, a stronger grid infrastructure is not sufficient. the grid needs to be smart and agile everywhere to react to changes, and smart power electronics play irreplaceable roles in this area."
"in this equation, f(x) represents the objective function value at the design point x, which specifies how much material was removed from the spatial design region. the penalty factor, r is a numeric value which continually increases from an initial value by a user specified amount as the optimization proceeds."
"ieee std 1547 [cit], which governs the interconnection and interoperability of ders with associated electric power systems, has recently been revised and published to reflect some of the advanced functions demanded by the utilities. these control functions [cit] include constant power factor mode; voltage-reactive power mode; active power-reactive power mode; constant reactive power mode; voltage-active power mode; frequency droop mode; new power quality requirements (e.g., limits of rapid voltage changes); and new voltage, frequency and the rate-of-change of frequency fault ride-through requirements for der during abnormal grid operating conditions. ieee 1547 specifies and harmonizes interconnection requirements for ders. the state public utilities commissions may adopt it in distribution legislation (e.g., california rule 21). the revised ieee 1547 is also aligned with ferc and nerc bulk system reliability standards that regulate transmission-level interconnection and commerce."
"current smart inverter development is focusing on standardized control and protective functions for grid compliance and interoperability. while these functions can be characterized as grid-supporting functions in today's centralized grid architecture and control environment, the trend of future smart inverters is evolving into more intelligent and autonomous entities with integrated system functions, which would demonstrate superiority when handling complex decentralized grid dynamics and uncertainties and flexibly adapting to appropriate control modes. furthermore, smart power electronics would be expected to serve as a fundamental grid architecture resilience tool."
"where we assume that the channel is awgn. denotes the transition probability of the channel state from 1 to 0 after has sent some sensing results. is the detection probability of the secondary user, is the false alarm rate, and is the false negatives rate. and are defined as"
"after spectrum sensing, how to ensure the rationality and reliability of spectrum resource allocation is a new challenge for distributed crns. in this section, a novel cheatproof spectrum allocation strategy based on vickrey-clarke-groves (vcg) mechanism is proposed. with the mechanism, we formulate the utility function of system and malicious users and then analyze and proof the efficiency of the strategy through the utility function."
"3) rule three locates the element of maximum stress. upon the location of the element of maximum stress, if applicable, the optimizer alters the weak material property of the element to the design material property."
"completely autonomous operation remains a technical challenge for smart inverters. performance of global frequency and voltage control may be compromised with distributed control methodologies. to tackle this challenge, cooperative control based on consensus algorithm and graph theory with neighboring communications have been applied on smart inverters for autonomous microgrid operation [cit] ."
"in this subsection, the performance of the proposed dscs is compared to the distributed random [cit] and edso [cit] schemes in terms of the sensing accuracy rate denoted by acc."
"the plate design mesh is constructed using femap [cit], a three dimensional cad package, which generates a neutral file that the finite element solver can read and evaluate. this neutral file [cit] documents the geometry as well as all of the material properties and other parameters of each plate design investigated."
"compared to synchronous generators, power electronicsbased generation exhibits different behaviors based on its inherent nature, despite the fact that the basic power generation function is quite similar. one observation is that a pure power converter does not have an internal energy buffer and always maintains instantaneous input-output power balance, although limited power buffering does exist mainly for high-frequency harmonics filtering (i.e., power quality purpose). this translates to a power system requirement as a low or zero-inertia system. another major characteristic is that power electronics use a digital feedback controller with high control bandwidth and regulate the output voltage or current very tightly. this translates into power system specifications as either a constant power load with negative incremental input impedance or a generator with dynamic output impedance. both cases are prone to oscillation and instability issues."
"subjective logic [cit] represents a specific belief calculus that uses a belief metric called opinion to express subjective reputation. since it is necessary to develop mechanisms to detect and manage malicious users in distributed crns, subjective logic with the ability to explicitly represent and manage a user's uncertainty has emerged as an attractive tool for handling trust relationships in distributed crns."
"the level of difficulty certainly increases with the number of elements considered and this may partially explain this phenomena. another possibility is that the topology is shifting from a global level to a microscopic level of detail. with large elements, the fundamental form of the structure is the major result. the approach clearly does not generate a final detailed design, but a clear topology is evident from which to form a final design. as the mesh is formed of smaller elements, the topology shifts to a finer grain which actually allows for more of a grain type structure to be formed such as that represented by a foam or matrix material containing small patterns of voids. additional experimentation will be required in this area."
"the final plate design exercise investigated is composed of two hundred thirty four plate elements which are subjected to the previously defined loading and american journal of computational mathematics constraint conditions. phase one genetic input parameters consisted of population size and number of generations of 486 and 420 respectively. figure 26 and figure 27 display both the phase one and phase two genetic optimization solu-tions. the phase one result is difficult to interpret as the final topology is not as well defined as for the previous plate examples considered. this fact is attributed to the substantial increase in the number of plate elements in this example which magnifies the difficulty in generating a solution through the use of a traditional genetic algorithm. conversely, the phase two design shown in figure 27 represents reasonable design solution, where elements are reconfigured and connected with surrounding elements and are no longer located beyond the defined nodal loading conditions. this is perhaps the best indicator of the power of the phase two approach as significant redesign was necessary in order to refine the phase one solution. displacement and stress values versus plate element number are presented graphically in figure 28 and figure 29 . both the displacement constraint and the stress constraint are seen to be active."
"as reviewed, the state of the art in smart inverter development is towards standardized functionalities for interoperability and centralized communications for control room integration for utility applications with high penetration renewable scenarios. although interoperability can provide a lot of benefits from the system control perspective, smart inverters with advanced autonomous capabilities will play major roles in distributed grid controls. there is a need to extend smart inverters into distributed deployment applications (e.g., microgrids) and to advance smart inverters with additional autonomous, cooperative control functions operating with distributed communications (e.g., openfmb), or without communications, and integrated system operation, control, and protection functions."
"power grids demand better observability and situational awareness for operation and control purposes. there is a need to develop low-cost sensors and phasor measurement units (pmus). incorporated grid sensor and pmu functionalities will substantially reduce the cost of implementing dedicated pmu systems, increase system observability (situational awareness) and controllability (optimality), and enable synchronization as well as accurate power sharing. special redundancy or other measures need to be taken to \"correct\" sensor data during hardware failure, system faults, and cybersecurity compromised situations."
"these insights to the rule development process and the ability to reduce or eliminate the phase one search process highlight the promise of the rule based approach. the implementation tested is one of a wide variety of similar algorithms which could have been constructed. further understanding of the rule development process is needed as well as additional testing on larger topological design problems. the post process review of the successful rule distribution could be expanded to include rule interactions which would also provide useful design information. based on the results achieved to date, however, the method promises to provide an effective design tool for structural design which provides insights into the solution process that are unavailable by most other approaches. the process represents a unique combination of an evolutionary search which is guided by domain specific knowledge."
"technical challenges exist for smart inverters embedded with system functions. although the power electronic community is addressing the challenges, there are emerging trends that offer solution approaches for smart inverters moving towards more software-oriented, artificially intelligent, and transactive operations."
"the future power grid will inevitably adopt more and more power electronics-based energy resources, controllable load, and other type of devices. these power electronics-based grid devices can be divided into three categories: generators with power converters interface decoupling the source from the grid, loads controlled by power converters with regulated output voltage, and various power electronics based power flow controllers (including hvdc, facts, and distributed grid edge technologies)."
"however, for the case that the belief, disbelief, uncertainty, and base rate change over time, the secondary users' reputation evaluation and the trust relationship between the secondary users also changes over time. therefore, the reputation evaluation and the trust relationship at present time depends not only on the values of the underlying parameters but also on the decayed values of the previous trust."
"the genetic algorithm generates designs through the alteration of the material property of each plate element within the selected neutral file. the alteration of material properties can generate a realm of diverse topological designs, where each design is processed by altering the neutral file and passing it on to the finite element solver. the element stresses and the nodal deflections are computed by the finite element solver and subsequently, this information is utilized to evaluate the objective function and constraints which are defined by the optimization formulation. the specific form of objective function in this application was chosen to be the minimization of the total volume of a plate based design subject to predefined loading and constraint conditions. stress and displacement limits served as structural constraint bounds."
"each of the rules has a unique assignment of the rule block positions which provides the information required to process the rules for a given topological plate design. in all cases, the first of the three block positions represents the rule number, which is an integer from one to five. the second block position is an integer representing the element number selected for modification (when required) and the third block position is only applicable for rule number two where it is an integer identifying the second element number selected for rule execution."
"the california public utilities commission (cpuc) [cit] to develop technical recommendations [cit] and steps to be taken for inverter-based ders to support distribution system operations. seven autonomous inverter functionalities, including anti-islanding protection, low/high voltage ride-through, low/high frequency ride-through, dynamic volt-var operations, ramp rates, fixed power factor, soft-start, communication for data monitoring and control, and cyber-security requirements have been recommended and approved to be included in california electric tariff rule 21, which governs der interconnection in california utilities. advanced functions by communications are currently under development to include the control of real power and reactive power, frequency support, emergency response, and scheduling functions."
"the total utility of all the other applicants when participates in the mechanism and the total utility of all the other applicants when withdraws from the mechanism * (, * ):"
"the two phase approach can be implemented in a host of different ways and the effectiveness of the resulting algorithm may improve in the process. certainly, however, the initial results are promising. completing the phase one search before implementing the phase two process may not be as efficient as a mixture of the phase one and phase two search where the phase two process operates on the entire phase one population. the development and representation of rules is also an area which requires further experimentation. the use of the rule history for improvement of the process is also an open area for further research. in any case, the basic methodology has been demonstrated and shown to be an effective approach to topological optimization. it certainly exceeds the performance of a traditional genetic based approach."
"when an opinion is used in a decision, it is projected onto the belief/disbelief axis through its expectation, ( : ), which is used to identify malicious nodes and can be computed as"
"the plate optimization procedure considered here operates through the assignment of different material properties throughout a predefined meshed design region. in order to simplify the concept, only two different material property values are introduced. for a plate design, the logical material property to consider is the modulus of elasticity. the first material property value represents the intended design material such as that for steel or aluminum. the second material property value corresponds to an extremely weak material which adds little to the structural integrity of the design. the utilization of a weak material allows for extreme topological change to occur without requiring a re-mesh for the elements and eliminates the problem of generating a singular stiffness matrix. the stiffness matrix for the finite element analysis of the design is assembled with material property information provided by the genetic encoding on an element by element basis. the final topology of the design may be identified by simply removing the elements formed of the weak material. this is not intended to provide a final, highly detailed design, but rather a topology which may be refined in order to generate such a design."
"1) rule one indicates that for a randomly selected element, the material property is switched from its current material property to its opposite (i.e. weak material property to the design material property or vice versa)."
"the key component is the power electronics \"joint\" inserted among the various zones to increase grid elastics. it can be applied to existing centralized grids granularly, for instance, at the point of interconnection for networked microgrids or microgrid-to-grid, where the interfacing device can be replaced by the proposed power electronics based device for asynchronized operation, seamless transition, and decoupled/isolated fault management to minimize cascaded system failure. other possible locations include the solid-state substation, transmission buses, distribution feeder segmentation, and so on. v. conclusion we envision the future power grid as a distributed power electronics-based grid, where all the individual devices, such as ders, power flow controllers, solid-state transformers, and medium-voltage and high-voltage dc converters are required to participate in grid services and share common system control goals. smart inverter functionalities provide a technical basis for the capabilities of grid services."
"the challenges of power grids with decentralized system structures can be approached at different hierarchical levels and different time scales. typically, the lower the level at which it is addressed, the faster the issue will be resolved and the lower the cost. for this purpose, smart grid devices, like smart inverters with integrated system functions, provide a wide range of grid services, alleviate system control and protection burden, and better integrate with overall grid operations."
the behavior of the genetic algorithm is presented graphically for the phase one solution in figure 22 and figure 23 . an objective function value of 88.59
"once the crossover of traits between parent designs has been completed, stresses and displacements for each of the offspring designs are calculated and evaluated in the stress and displacement constraint functions. element stresses and nodal displacements are immediately available once the finite element analysis algorithm, caefem, has completed the analysis. now the evolutionary process has commenced. this evolutionary process begins with the selection of new parent designs from the current generation. once all parent designs have bred new offspring, one generation has been completed. the total number of generations of offspring produced is a user specified variable, which is application specific. the effectiveness of a specified set of input parameter values may be observed through feedback from the graphical user interface."
"objective function and constraint values versus generation graphs are provided below which plot the progress of the phase one genetic optimization made during the phase one search process. additionally, the constraint versus generation graph illustrated in figure 33, is clearly indicates that both design constraints are active at the solution."
"two separate genetic optimization formulations are introduced. the first represents a traditional encoding, where each element in the defined structural region has its own binary variable in the encoding string. this formulation is developed in this section. the development of the rule based extension follows in the next section. the plate design examples investigated were all subjected to identical stress and displacement constraint conditions. each plate design was also restricted to the same spatial design volume, although the discretization level or element size was varied. the dimensions of the spatial design volume consisted of eighteen cm in the x direction, thirteen cm in the y direction, and a thickness of 0.5 cm in the z direction. directional forces consisting of 67 newtons in the x, y and z directions were applied at coordinate locations (15, 3 .25, 0.5) and (12, 9 .75, 0.5) where the units are in cm and the origin is at the lower left corner of the bottom surface of the specified plate design volume. the use of multidirectional forces allow for an optimal yet robust topological design to be acquired. the concept of robust design optimization has been effectively demonstrated by sandgren and cameron [cit] for truss structures as well as an automotive inner body panel. mesh sizes ranged from twenty four to two hundred thirty four elements. the maximum allowable stress constraint was set at 140"
"the progress of the genetic algorithm throughout the phase one search process is provided below through graphical representations of objective function value and constraint values versus generation, as shown in figure 14 and figure 15 . the search did not converge until over one hundred generations had been produced and the displacement constraint was active at the phase one design solution."
"distributed cooperative sensing strategy implements spectrum sensing through the distributed secondary users in a wide area. in distributed cooperative sensing, each secondary user obtains a local measurement in a time interval . after a sensing session, a series of value update sessions are executed by the secondary users. all secondary users exchange their local spectrum sensing results with their neighbors within its communication range and update their own values based on the received values."
"as the number of elements increases, the complexity as well as the time required for solution by the optimization algorithm increases significantly. on the other hand, the number of elements required to define the fundamental design topology will generally be considerably below that required for a detailed design analysis. another approach is to limit the optimization to a specific region of a design, where the topology of other regions is already fully defined. meshing at several levels of detail is considered in the examples which follow. this is somewhat equivalent to the refinement provided by a variable length encoding [cit] which could be implemented in order to automate the overall process. the fundamental question which must be addressed is whether a rule based genetic algorithm is capable of solving a realistic structural design problem with a reasonable amount of computational effort. the process is inherently parallel in nature which could lead to significant reduction in computational time but not in computational effort. extensions to problems involving three dimensional solid objects are straightforward, using the same design optimization strategy with an expanded genetic encoding."
"in this section, we propose dscs, a reputation-based sensing strategy that is a distributed cooperative strategy using subject logic based reputation mechanism to defend against internal malicious secondary users' attacks."
"graphical representations of displacement and stress values for each plate element in the final design after the phase two operation are provided in figure 20 and figure 21 . from these plots it can be seen that both constraints are satisfied, although both are approaching the design limits imposed on the optimization formulation."
"mutation is a random occurrence of an altered material property value within an arbitrarily selected parent design string which occurs during a crossover operation. this random alteration allows for the possibility of a solution to be generated which could not result from any combination of the encoding strings represented in the current population. although a random occurrence, the probability of mutation is user specified and normally set at a relatively low value. if the mutation probability is set too high, the search becomes more of a random search rather than an ordered search. toward the end of the search process, most progress is made through mutation as most of the design content in the original population of designs has been exploited or lost. in a computationally expensive problem environment, such as structural design, it is important to have a reasonable mutation rate as the population size must be limited."
"a number of different encodings are possible with a genetic algorithm for topological design. in order to be a useful design tool, however, the desire is to let the algorithm have complete control over the topology of the design. this means the algorithm must be capable of deleting as well as adding elements or features to the design. for a plate design, only the loading and ground locations should be specified initially along with a feasible region of space in which the design must fit. each analysis of a structure is time consuming which brings up the issue of efficiency. design population size must be limited, which in turn limits the effectiveness of the approach. the traditional genetic algorithm exploits successful design content, but this is not the same as exploiting design knowledge. a two phase genetic algorithm for robust topological design is developed herein. a series of design problems utilizing plate elements is presented and the results of the two phase algorithm are contrasted to those generated by a traditional genetic algorithm. only loading conditions, restraints and geometric boundaries are specified for each problem."
"4) rule four locates the element of maximum displacement, and alters the material property of the element to the design material property if the current material property was designated a weak material."
"the discretized nature of computational structural analysis provides a direct link to the encoding used in a genetic optimization algorithm. if each element is represented by an individual position in the chromosome, topological modification can be carried out by removing or adding individual elements. the removal of elements may easily be handled through the assignment of a weak material property which effectively eliminates the element, but does not require any re-meshing. using this approach, a region, or allowable design space, can be identified and meshed into elements. the genetic algorithm will then select the elements required for the design topology which best satisfies the design criteria and constraints. often, however, the result of an evolutionary structural optimization contains obvious flaws that an experienced designer would not allow. this is a result of the fact that a mathematical abstraction of the \"real\" design problem is being operated on by the optimization algorithm. in order to eliminate these design flaws, some knowledge of the structural design process must be embedded within the optimization process. overall, the encoding or chromosome string of a traditional genetic algorithm is proportional to the complexity of the problem, however knowledge based encoding approach is independent of problem size and does not proportionally grow in complexity. a smaller knowledge based encoding scheme eliminates traditional genetic algorithm solution flaws, increases solution convergence time, and provides feedback to the end user regarding which key features of information were the most essential when deriving the final solution."
"plots of both the objective function and constraint values versus generation graphs are provided in figure 6 and rithm behavior for the phase one search process. these graphical representations correspond to the final solution illustrated in figure 2 . figure 6 indicates that the solution was located by the eighth generation and figure 7 indicates that both the displacement and stress constraints were satisfied. the displacement constraint can be seen to be near its prescribed bound, while the stress constraint is far from being active. figure 8 and figure 9 provide the solution history for the phase two search process. figure 8 contains a plot of the objective function for each generation of designs produced and from this plot it can be seen that little improvement in the phase one design is produced after the tenth generation. the resulting objective function from the phase two search was enhanced from 68.25 cm 3 to 87.75 cm 3 which resulted in an additional thirty percent increase in the amount of material removed from the design. the overall stress and displacement values, shown in figure 9 document that the phase two solution lies closer to the stress constraint american journal of computational mathematics figure 9 . constraint versus generation."
"first, we compare the acc of the three strategies without ssdf attacks (i.e., without dishonest recommendations). suppose that there are 6 available channels and each secondary user can detect up to 4 channels. the acc can be computed as"
"the results presented to this point clearly point out the limitations of the phase one or conventional application of a genetic algorithm in setting the material properties for the topological optimization. the quality of the solution generated was reduced considerably at each mesh refinement stage and in the last case involving two hundred and thirty four elements, very little useful topology detail is available after the phase on e search process. the rule based, phase two implementation is remarkably more efficient and is capable of developing fairly refined topologies in within a very limited number of generations. this is due to several factors, the main two being the ability to infuse problem specific knowledge into the search and the reduction in the size of the solution space in the problem formulation. this brings up the issue of how much, if any, of a phase one search is required in the topological optimization. the other issue which deserves some additional consideration is the post evaluation of the rule usage during the phase two search process. both of these issues will be addressed briefly."
"new economic tools and processes have been introduced in a modernized grid. transactive energy (te) is one of these processes to enable market-based transactive exchange between energy producers and consumers or prosumers. as defined by the us doe gridwise architecture council, the te is \"a system of economic and control mechanisms that allows the dynamic balance of supply and demand across the entire electrical infrastructure using value as a key operational parameter.\" accordingly, various transactive control methods are emerging for ders and responsive loads to provide grid services."
"reliability, security, affordability, flexibility, sustainability, and resilience. this has offered tremendous opportunities for power electronics to expand in the utility space and moves across the boundary from high-voltage and low-voltage to medium-voltage applications, in addition, however, advanced system functionalities are required. as an enabling technology, power electronics have been used to support energy efficiency and grid integration of sustainable energy. with grid modernization, power electronics can also play roles in advanced grid architectures [cit] as a fundamental resilience tool. nevertheless, large-scale power electronics integration poses new challenges [cit] on stability and power quality of modern power grids, which need to be addressed. this manuscript has been authored by ut-battelle, llc, under contract de-ac05-00or22725 with the us department of energy (doe). the us government retains and the publisher, by accepting the article for publication, acknowledges that the us government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for us government purposes. doe will provide public access to these results of federally sponsored research in accordance with the doe public access plan (http://energy.gov/downloads/doe-public-access-plan)."
"the approval of california rule 21 has also led to ieee 1547a [cit], which was amended to ieee 1547 [cit] with changes on voltage regulation and abnormal voltage/frequency response requirements, and to ul 1741 sa, which provides testing and certification standards for smart inverters with grid support functions."
"to a large extent, the inverter function and operation mode depend on the interconnected grid or load conditions. the three basic inverter functions [cit] are grid-feeding, gridforming, and grid-supporting. typically, an inverter is designed and controlled to be either a voltage source to form a grid voltage with/without grid frequency support or a current source to feed energy into the grid with/without grid voltage support. with increasing grid dynamics and uncertainty, it is desired to design the inverter with flexible control modes on the fly based on in situ grid operating conditions and grid control architecture."
"the fundamental operation of the genetic algorithm remains identical to the phase one operation, with the exception of the development of rule strings which are mated and offspring rule sequences which are introduced to further refine the final solution(s) from the phase one search. each time the genetic algorithm has located a rule sequence for which both stress and displacement constraints are satisfied and an improvement in the objective function is achieved, the improved design replaces the current design. this rule based refinement is based upon the single row encoding scheme illustrated in figure 1 . this specific encoding example consists of ten columns, where each column or block position contains information that is necessary in order to execute one or more of the rules. this particular encoding method allows for up to three rules to be executed simultaneously. the first position in the encoding string determines how many rules are to be executed in order to modify the current design. this leaves three groups of three block positions to provide the information necessary to execute specific rules. the first of the three block position values determines which rule to execute. the second and third block position values provide any additional information necessary to execute the selected rule (i.e. specific element to manipulate). this encoding method is repeated throughout the remaining two block sets which compose the remainder of the encoded row. this encoding methodology provides the foundation for the development of design strings which are injected into the genetic algorithm to refine the best design(s) from phase one. phase two has the ultimate goal of determining the best rule sequence, from a global perspective, which refines a previously located design point from the phase one search or the current design(s) in the phase two search."
"this allows an early exploration phase where some constraint violation is allowed, but builds a penalty over time so that later designs are pulled toward the feasible design region. lastly, the array g i (x) represents the constraint value for each of the defined constraints. additional background on penalty function theory and operation is provided by gen and cheng [cit] . the penalty function or specific fitness value for each population member determines which members of the population are best suited for producing new design offspring, resulting in the term parent designs. an in depth discussion of parent selection as well as the genetic optimization process is given by goldberg [cit] and davis [cit] ."
"5.3. vcg based distributed cheat-proof mechanism. in the distributed spectrum allocation process, some secondary users behave maliciously to maximize their own performance by providing the false resource demand. to offer stronger incentives for secondary users to honestly participate in the spectrum allocation process, we connect spectrum allocation to the reputation through a vcg based distributed cheatproof mechanism. based on the abovementioned analysis in sections 5.1 and 5.2, we first propose a distributed cheating-proof mechanism detailed description as follows."
"compared to conventional machine-based generators, this limited energy buffering is also equivalent to the lack of rotational or mechanical inertia, which reduces the capability of primary frequency response as in a conventional grid but could be improved by inertia emulation in control and by integrating additional energy storage components in power inverters."
"as shown in figure 5, the acc of all the three strategies decreases when the number of the channels increases. since the ssdf is randomly launched and cannot be detected effectively, the acc of the random and edso scheme decreases at a percentage up to 70% and 73%, respectively. however, for the dscs, because the cooperative users can provide additional sensing information combined with the local sensing information to compute the final sensing result, its acc only decreases by 35%."
"we assume that, in spectrum sensing, malicious secondary users strategically report falsified sensing results, aiming at incurring interference between the primary users and legitimate secondary users. in spectrum allocation, malicious secondary users may launch collusion attacks or bad mouthing attacks to report falsified reputation values, aiming to keep the legitimate secondary users away from using the spectrum resource."
"in this paper, we have investigated the challenging problems of secure spectrum sensing and spectrum allocation in crns and have proposed a distributed cooperative sensing strategy (dscs) and a cheat-proof spectrum allocation strategy (dcsa). based on the combination of the proposed dynamic reputation model and vcg mechanism, the dscs and dcsa can effectively defend against the internal ssdf attacks. moreover, the dscs and dcsa do not rely on a central authority or a common control channel and are therefore applicable in distributed crns. nevertheless, the proposed algorithm imposes some communications and computing overhead caused by nodes interactions which is not calculated in this work, as we focus on the security and effectiveness of the cooperative spectrum sensing and allocation scheme in crns. we intend to investigate and analyze the overhead issue in our future work. the elaborated simulation tests and performance analysis have verified that the dscs and dcsa are secure and efficient. more specifically, in the presence of ssdf attacks, the sensing accuracy rate and the spectrum allocation accuracy rate of the proposed dscs and dcsa are much better than those of the existing strategies."
"it is commonly known that power electronic devices have negative incremental input impedances, which interacts with grid line impedance or other grid devices causing oscillations and potential instability. therefore, multi-inverter system dynamics and their stability analysis is among imminent research and development needs for increasing penetration of power electronics, since existing state-space modeling approaches and commercial software tools become incompetent because of complexity and computation requirements. various impedance-based stability approaches [cit] can be adopted but are still limited to small-scale power electronics systems. there is a lack of tools and approaches for integrated dynamic grid analysis, modeling, and simulations of an ultra-large-scale power electronics based grid."
"the plate design examples differ only in the number of elements involved in the meshing of the design space. the specific input parameters utilized for each of the design exercises including population size and number of generations of offspring utilized for both the phase one and phase two search processes are documented in table 1 . from this table, it is noted that both the population size as well as the number of generations required increased as the number of elements increased in the phase one search. these parameters for the phase two search remained constant, regardless of the number of plate elements contained in the design space mesh. this is a direct correlation to the length of the encoding string which increases for phase one as the number of elements increases, but remains fixed for the phase two search component. this once again shows the promise of the design process using a rule based approach. regardless of the size of the problem, the phase two search remains the same, reduced size. the results for each of the four mesh densities will be reviewed in detail. the phase one search is equivalent to a traditional genetic optimization, while the two phase search procedure represents the rule based, global search process."
"most existing cybersecurity studies and efforts are focusing on cybersecurity framework, attack modeling, and laboratory testbed,. little commercial work has been reported on the embedded cybersecurity functions within the power electronics hardware. in addition, existing cybersecurity approaches are mostly software based (e.g., network firewalls and testing sandboxes) and focus mainly on communication systems and industrial control systems. although softwarecentric solutions are easy to update and patch as new vulnerabilities are discovered, hardware-based solutions offer advantages such as fewer data vulnerabilities because the tampering of hardware circuits requires physical access, hardware root of trust, etc."
"in figure 4, we analyze the acc performance of dscs when malicious users exist. the results indicate that the sensing results are affected by the malicious users, and the acc of dscs decreases from 0.6 to less than 0.4 as the number of malicious users increases from 20 to 30. in dscs, the proposed reputation mechanism will identify the malicious users and then punish them by isolating them from the network. the punishment will decrease the number of cooperative users and the cooperative sensing information; as a result, the accuracy of the dscs drops."
"the first decrease in the mesh size for the prescribed design region resulted in an increase in the number of plate elements from twenty-four to eighty-eight. all other design parameters remained at identical levels as in the previous design example. solutions for both the phase one and phase two search are provided below in figure 10 and figure 11 . an examination of figure 10 compared to the phase one solution for the course mesh from figure 2 shows a significant change in the design topology. the objective function value was improved from the previous phase one solution of 68.25 cm 3 to a value of 93.07 cm 3 . a more refined design was located upon the conclusion of the phase two search which is illustrated in figure 11 . the phase two search solution improved the objective function value to 97.06 cm 3 and removed extraneous elements from the phase one solution. the element identification numbers are provided for the elements formed from the actual design material in figure 11, which correlate to the graphical representations for the element stress and displacement values. the displacement and stress values for each plate element upon the conclusion of the phase two search process are presented graphically in figure 12 and figure 13 . the nodal numbering scheme which is determined by the mesh generator makes it more difficult to identify specific element values, but it is clear from these graphs that both stress and displacement values are below the specified design limits. this once again points toward the possibility of a reduced mesh size allowing for even more material removal."
"other rules than those specified may be more effective in the phase two process, but the algorithm will adapt through the genetic process to locate the best rules and combination of rules. the rules are controlled by the genetic algorithm which in turn is utilized to modify a previously specified plate design. it should be noted that the rules are not required to be good design rules. for example, rule number four may be seen to be of limited value since the reduction of displacement at a nodal position will be influenced by all elements along the load path and particularly, those elements close to a ground position. the beauty of the rule based process is that good rules will be executed and bad rules will be avoided as the search progresses. at the end of the search, the user can see which rules or combination of rules lead to design improvements during the search. this can lead to rule refinement or even the discovery of a new design strategy. this links the process closely with learning and memory which are fundamental to any successful design process."
"the second phase of the optimization process is designed to refine the best design or designs from the phase one search through the implementation of domain specific knowledge provided by the user in the form of rules. these rules are created by the user and encoded into a design string similar to the element selection string within the phase one search. upon the conclusion of phase one, a global search has been performed which should result in a reasonably good design, which is at or close to a feasible design topology. since by nature, the genetic optimization process tends to make good progress early in the optimization, the location of a set of reasonable design points does not require many generations to be executed in the phase one search. alternatively, the phase one search can be done away with entirely and replaced by a set of randomly generated designs. while this process may lead to a local minimum, depending upon the diversity of the randomly selected designs generated, it allows for large problems to be solved using the greatly reduced encoding length of the rule based search. the alternatives supported by the two phase design optimization methodology lead to a rich set of alternatives which can be implemented to balance computational effort with the scope of the global search."
"crn can improve the efficiency of spectrum usage, but it also introduces new security threats including internal attacks during the spectrum sensing and allocation process, which can degrade the effectiveness of spectrum sensing and allocation dramatically. for example, an adversary may launch data or information falsification attacks during spectrum sensing and allocation process, where the adversary corrupts a subset of secondary users as illustrated in figure 2 to report falsified data or information, aiming to affect the final group decision [cit] ."
"in order to test the algorithm with a limited phase one search component, the 88 element plate problem was revisited. a phase one search was conducted with an extremely limited population size, 25, and for only 25 generations. the best final population member was then passed directly to the phase two search process which utilized a population size of 100 rule strings for a total of 40 generations. the final topology from the phase two search is presented in figure 34 . the final amount of material removed was 98.38 cm 3 . this is actually slightly higher than the final two phase solution presented in example 2, although the final topology is very similar to that presented in figure 11 . the objective function value and the constraint values versus generations conducted in the phase two search process are documented in figure 35 and figure 36 . thus by reducing the total phase one function and constraint evaluations by over a factor of 40, did not damage the value of the final result. the limit of the reduction of the phase one search would be to stop the phase one search after the first generation which would produce a set of randomly generated designs. utilizing this approach, the design space, or encoding length in the genetic algorithm, for a mesh containing tens of thousands of elements would be on the order of ten or twenty elements. solving this size problem repeatedly, is demonstrated to be and expected to be far more efficient with respect to the number of function and constraint evaluations. the issue of which rules are implemented and which of those rules were successfully utilized in the solution process provides an important look at how the general topological optimization process can be improved over time. the distribution of rule utilization for the solution of the 88 element problem with the reduced phase one search is shown in figure 37 . it was mentioned in the early discussion in the selection of the rules that rule number four was not constructed in a way that would improve the solution. this fact is documented in figure 37 where it is indicated that the rule was never utilized successfully in the search. this information can be used to replace the rule with another, more appropriate rule. the distribution of the remaining four rules shows a dominance of rule number five, and lesser dependence on rules one, two and three. this means that for this particular case, the search was improved most often through the removal of material in regions of low stress. this is not an unexpected result, but the usefulness of building some randomness into the rule base is documented by the high rate of use of rules one and two. the interesting part of the rule development process over time, is that a better understanding of the optimization and design process is developed. this is a rare feature among any topological optimization procedure."
"mpa and a maximum allowable displacement constraint was defined to be 0.635 cm. the objective function used to evaluate each design is based upon the amount of volume removed from the original mesh. this objective function value is equal to the total element volume of weak material present in each design as specified by the genetic algorithm encoding. the encoding value for each element has a value of zero if the element is assigned to the design material and one if it is assigned to the weak design material. this way, the summation of volume to be maximized in equation (1) consists of the total volume of weak material elements, or alternatively, the volume of material removed. in the constraint equations, s limit and d limit are the maximum allowable stress and displacement values while s max (calculated) and d max (calculated) are the maximum computed values for the design being analyzed over all of the elements considered. the formulation of equation (2) generates ratios, which allow for normalization of constraint violation in the disparate magnitudes of the displacement and stress values as well as to allow for a consistent graphical representation over the diverse set of designs within each genetic population. both constraint equations produce positive values for any design which does not exceed the design limits."
"for this purpose, a power electronics-based zonal grid architecture is proposed with inherent power flow control, noncascaded fault management, and distributed power system architecture with full controllability to maximize grid resiliency. the concept of a \"interconnected, but dynamicallydecoupled grid structure\" will be applied here, as shown in fig. 4, and the new grid architecture is established based on design-for-resiliency and controllability measures. compared to the machine-based grid, the power electronics-based grid dynamically decouples generation, load, and grid dynamics such that the grid frequency can be controlled in a fast way. the salient features are asynchronous zonal grid operation, controlled power flow, and nonpropagated fault management."
"the term \"smart inverters\" was initially used for solar photovoltaic inverters with control functions of fault ridethrough, grid voltage support, and reactive power compensation, which were either not required or not allowed by the standard and grid codes at the time. with power deregulation, there is a demand for distributed generations to provide ancillary services. nowadays this \"smart\" function has been extended from solar photovoltaic inverter to any type of distributed energy resources (ders) with power electronics-based interfaces in utility distributions systems. a common understanding is that a smart inverter has communication capabilities and can provide additional and advanced control functions, in many cases autonomous functions, beyond its basic power conversion and energy feeding functions. hence, it is also called a multifunctional inverter."
"in distributed crns, the secondary user network is vulnerable to both external and internal attacks. external attacks can be effectively solved by using the traditional cryptography theory and authentication method. the internal attacks are launched by an inside legal and certificated user, which makes the traditional encryption and authentication techniques no longer effective. in the internal attacks, the attackers may or may not participate in the cooperative sensing process and may report falsified values when participating."
"power electronics converters have limited energy buffering capabilities, except for very little power buffering from dc-link capacitors, and are typically designed to achieve instantaneous power balancing between input and output. for renewable energy applications, the nondispatchability due to intermittent energy resources causes high operating reserves and other issues such as voltage spikes and reserve ramping-up rate requirements. advanced forecasting techniques, firming approaches using energy storage, and spatial aggregation for smoothing effects are common operational practices to address this issue. hybrid pv/energy storage or multi-input and/or multiport converter systems are proposed to address this issue, in addition to advanced forecasting techniques and spatial aggregation for smoothing effects at the system operation level."
"the development of power electronics has been driven by internal semiconductor technology and converter circuit topology, approaching the limits of its internally set metrics (e.g., efficiency). although the original driving philosophy indicates internal maturity, the external constituent technologies of packaging, manufacturing, electromagnetic and physical impact, and converter control technology still present remarkable opportunities for development. as an enabling technology, power electronics is generally considered to provide only a supporting partial function to enable a main function to be realized. hence, the future development of power electronics, together with internal developments such as wide bandgap semiconductors, will be driven externally by emerging applications where the requirements cannot be satisfactorily fulfilled by existing concepts, as depicted in fig. 3 . with this observation, power electronics are experiencing the following emerging solution trends in an integrated power electronics-based grid."
"the inclusion of domain specific knowledge in the form of design rules was implemented within the framework of a genetic algorithm and tested on a series of structural design problems involving plates. each problem was subjected to identical boundary and loading conditions as well as the same original predefined design volume. the problems involved four different mesh sizes for the prescribed design region. the topology was defined by the assignment of material property values to each of the elements through the genetic encoding of the optimization algorithm. two material values were assigned, a normal value for the intended design material and a value which represents a significantly weaker material which represents a void."
"the aim of this initiative is to provide consistent functions and uniform communication protocols for a diversity of resources (e.g., solar pv and energy storage) in varying sizes and from different manufacturers to integrate with utility distribution management and supervisory control and data acquisition systems. the communication information models of these functions have been considered by iec tc57 wg17, which takes charge of developing the communication architecture for integrating der into the iec 61850 body of communication standards. iec technical report, iec/tr 61850-90-7, describes those advanced functions object modeling for power converters in der systems, which has been adopted in the new edition of iec 61850-7-420, the first standard for der object modeling."
"next, we compare the sensing accuracy performance of dscs to that of the random and edso scheme under the random ssdf and intermittent ssdf attacks. the results are shown in figures 4-6."
"as shown in figures 9 and 10, under the ssdf attack, the acr of both strategies (with or without dcsa) decreases when the time or the percentage of the malicious users increases, while the acr of strategy with dcsa decreases slower than that without dcsa. in this simulation, the malicious users are assumed to be rational, so the proposed dcsa can effectively incentivize secondary users to honestly participate in the spectrum allocation process by reducing its reputation when it provides dishonest information. since the reputation of a secondary user will be the reference for the next round of spectrum allocation and cooperative user selection, the reduction of reputation will keep the malicious users away from lying because they cannot get utility from lying. as a result, the acr of the strategy with dcsa is better than that without dcsa."
"the addition of the second phase, rule based genetic algorithm was found to be extremely effective in cleaning up the design resulting from the phase one search. this process was found to require a very small number of generations, even when the number of elements assigned to the design region grew. the result from the phase one search was improved by the phase two search in all design cases considered. the issue of the topology generated as the number of elements increased was interesting. when relatively few elements were assigned, the final design was easily defined and clearly related to the design generated by the phase one search. as the number of elements increased, the final design topology represented a better design (less weight), but as the number was increased farther, the final design actually decreased in the measure of quality."
"since distributed cooperative sensing can enhance sensing accuracy, while reducing the need for sensitive and expensive sensing technology, it is proposed to enhance the sensing performance [cit] . however, it is vulnerable to the internal attacks threats. the internal adversary may control some nodes to report false sensing results to degrade the final sensing decision, which will make the performance of cooperative sensing degrade significantly."
"next, we will prove that the proposed distributed cheating-proof mechanism is a vcg mechanism. theorem 1 (the mechanism is incentive compatible (ic)). a mechanism is incentive compatible (ic) if truth-telling is the best strategy for the users, which means that the users have no incentive to reveal false information."
