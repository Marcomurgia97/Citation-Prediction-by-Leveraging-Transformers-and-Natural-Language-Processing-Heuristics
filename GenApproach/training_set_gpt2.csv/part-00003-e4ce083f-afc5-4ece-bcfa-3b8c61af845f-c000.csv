text
"the selection of variables, also called attribute selection, is used in datasets for pattern formation in data mining (witten, i. h., frank, e., & [cit] ) ."
"subordination is a way of constructing new lévy processes from existing ones. more preciously, subordinator is a one-dimensional stochastic process that is non-decreasing almost surely. let be a lévy process with lévy exponent ( ) and let be a lévy process with lévy exponent ( ), then be a subordinated process which can be expressed as;"
"the data analyzed is a dataset obtained from the uci machine learning repository using the support vector machine and neural network based on particle swarm optimization. the results of the analysis will then be compared to get the selected model according to the best model selection criteria, namely the method or model that has the highest accuracy."
"the value of training cycles in the study is determined by conducting a trial run by entering c (1.0) and epsilon (0.0). next is the kernel model in the support vector machine training data. table. ii out of 116 data as many as 39 data that were predicted correctly included in the healthy control classification, and as many as 20 data predicted healthy control but in fact entered into the patients classification, 13 data were predicted by patients but included into the healthy control classification, 44 data were predicted to be exact that is included in the patients classification such as the confusion matrix table presented in table ii. the results obtained from roc processing for the support vector machine model using training data of 0,797 can be seen in figure iii table. iii out of 116 data as many as 43 data were correctly predicted namely included in the healthy control classification, and as many as 14 predicted data healthy control but apparently included in the patients classification, 9 data predicted by patients but included in the healthy control classification, 50 data were predicted to be exact that is included in the patients classification such as the confusion matrix table presented in table iii ."
"the path planning is performed pre-operatively using a 3d scan of the tissue, and the computation time is not critical. therefore, we have developed an algorithm which can be categorized as a sampling-based path planning. the path planner uses the needle-tissue interaction model to find all the feasible paths to reach the target with the pre-defined orientation and intersect these paths with the surface (skin). in order to generate the paths, the needle trajectory for a single insertion (no rotation) for a defined length is sampled. these points are called sample points. these sample points are generated using the needle-tissue interaction model. the sample points can be rotated in order to achieve all possible trajectories in 3d space. the resolution of samples and rotations are the key factors influencing the time needed to generate the final path. the paths from the target to the surface (skin) are divided into equally spaced section. the number of sections can be set by the user and it depends on the insertion length. fig 3 shows the algorithm for a simplified 2d case. the needle-tissue interaction model depends on several system parameters, such as tissue stiffness, needle bevel angle and needle young's modulus. these parameters are estimated pre-operatively."
"therefore, in figure 1, all calibrated models are compared with market gold options to get an overall impression for one maturity; that is to say one rage of strikes (2 [cit] options). according to figure 1, the nig model gives a very closer fit than other models. however, as we expected bs model is far from the market data from the other models (it illustrates the well-known bias of the bs model which is to underprice the itm calls and overprice the otm calls) (note 5). furthermore, the gh model is the second, the vg model is the third and the gig model is the fourth. still, it is difficult to identify which model gives better best. some are close to one and another. there are differences between in-the-money and out-the-money too. however, some models can work better on a dataset and worse on another. for clarification, the sum of squared errors (sse) is calculated and listed in table 4 . a similar result is generated as seen in the graphs. for more comparative purposes, in-sample pricing performance and out-of-sample performance are computed under several global measures of fit in the next section. figure 1 . comparison between market gold option prices and results of calibrated five models"
"in testing the breast cancer coimbra dataset using 10-fold cross validation. k-fold cross validation carried out experiments as much as k. one testing data is used in each trial and training data is obtained from part k-1, then one training data is an exchange of testing data."
"percutaneous needle insertion is a common minimally invasive surgical procedure used for diagnostic and therapeutic purposes. lung cancer-related diagnoses and therapies are among the important procedures in the field, due to the high mortality rate worldwide (1.59 million a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 [cit] ) [cit] . in the united states and europe lung cancer screening with computed tomography (ct) is recommended for people at high risk or within clinical trial settings [cit] . ct-guided lung biopsy is often performed for the nodules greater than 10mm, and for small fast-growing nodules. the ct images are used to locate the lung nodule and then the needle is advanced into the subcutaneous tissue incrementally. a ct scan is acquired after every needle manipulation. this procedure is commonly performed manually by clinicians, and can result in complications such as pneumothorax and pulmonary hemorrhage [cit] . core needle biopsy (cnb) or fine needle aspiration (fna) is used to cut a core for pathological analysis, or to aspirate cell clusters for cytological analysis, respectively. cnb is often reported to result in a higher diagnostic performance, but fna has a lower complication rate [cit] ."
neural networks tend to provide accurate predictions. one of the advantages of neural networks is that they can work effectively with data that are not normally distributed [cit] .
research has also been conducted [cit] in predicting breast cancer using a combination of neural network (nn) and association rules (ar) methods. both methods were tested using 10-fold cross validation. the results obtained that the neural network method with genetic algorithms get a high accuracy value of 97.00% compared to the naïve bayes method which produces an accuracy of 96.24% and the neural network method based on the rules of the rules with an accuracy value of 95.6%.
"in the experimental case i, the tip steering is compared with hybrid steering. each method is repeated 5 times and the radius of curvature is measured. the radius of curvature is registration fig 5. the work-flow of the experiments using the needle insertion device (nid) is as similar as possible to the clinical practice. the radiation symbol (☢) represents a single computed tomography (ct) scan performed. in clinic, aligning the needle with the lesion is challenging. this results in several needle manipulations and ct scans, which is visualized by the red area. this issue is solved using the nid, where the fiducials on the robot are used to calculate the robot pose which aligns the needle with the lesion. the pre-operative path planner provides the insertion point."
"also, the smallest relative entropy with respect to a given prior measure is used to take care of its lack of identification problem. however, in our paper, we check the relative errors with the infinity norm as the allowing tolerance 1.e-6, and the gradient norm as well. secondly, in order to reduce the market pricing noise, we use the common method of eliminating data points that are deep out of the money and deep in the money. now our calibration problem is set up as;"
"the needle steering algorithm, along with the path planner, are tested using 3 experimental cases which are discussed below. the results of the first experimental case shows that the needle deflection has increased using the hybrid control with respect to the tip-steering. this results in higher controlability of the needle trajectory. it is important to mention that the deflection depends on the amount of base rotation of the needle. therefore, the deflection can go even higher, but the disadvantage is higher tissue damage. in these experiments we have set a limit of 20˚on the rotation of the needle at the insertion point. in the experimental case ii, the needle is steered towards real target in a an anthropomorphic gelatin phantom of the thorax. the targeting accuracy is higher than the case with human cadaver, which is due to homogeneity of the gelatin phantom. the mechanical properties of the gelatin phantom is known, and therefore the model used for steering is accurate. in experimental case iii, the tip of the needle is tracked in real-time using the em tracker. therefore, the control loop is closed using the measurements, which in principle increase the targeting accuracy. however, the needle is steered in a human cadaver which is not homogeneous. to reach the target in the lung, the needle has to pierce the following layers of the thoracic wall: muscle fascia, external intercostal muscle, internal intercostal muscle, innermost intercostal muscle, endothoracic fascia, parietal pleura, visceral pleura. the mechanical properties of these layers are different and these are not accurately available. this results in a lower targeting accuracy with higher standard deviation. in the feasibility test, the robot was positioned over the left 2nd intercostal space. the space between the parietal and visceral pleural usually contains just a film of fluid, but with defects of the thoracic wall (e.g. by piercing or cutting it) air can fill this space and push the lung away (pneumothorax). indeed, the ct-experiment showed that the pleural space was wider than normal, possibly due to earlier experiments/punctures performed on the cadaver. this resulted in higher targeting error. the authors have showed in their previous study [cit] that fusing the real-time em tracking data with ct images can increase the targeting accuracy. however, in clinic, the procedures should be made as straight forward as possible for the clinicians, as far as the procedure could be performed with the required accuracy. in this study, it was shown that even in absence of data fusion of ct images and a real-time sensing device, an accuracy of around 3mm could be achieved in human tissue. the smallest lung nodules which physicians usually try to reach with a needle, assuming a spherical shape, is about 5mm diameter. this is very challenging even for experienced physicians and it involves several iterations of insertions, retraction and manipulation of the needle. if the center of the nodule is selected as the target, a robotic system such as the one presented here can reach targets of 6mm or larger in one step, without the need of retraction and re-insertion of the needle. furthermore, the measure of the radius of curvature for case ii and the feasibility study shows that the hybrid control is effective and is used during all the insertion, in the sense that higher deflection is realized. it demonstrates that even in the case of insertions into the lung (softer tissue), still the radius of curvature is smaller than bevel-tip steering in gelatin (stiffer tissue)."
is a point on the segment with l i being the segment arc length. boundary conditions are applied to the segments to find the polynomial curve coefficients. the second order continuity can be formulated as follows:
"herewith, the pricing of the european is focused on a given market model, when the payoff function is only a function of the terminal stock price. let be a strike price, be an asset price and be an expiration of a contingent claim. the value plain vanilla call option prices at time 0 are given by the expectation of the payoff under the martingale measure ℚ is given by;"
factors causing the death of most women in the world due to exposure to a fairly dangerous disease that is cancer (cancer) and one of them is a cancer that is very worried about most women is breast cancer.
this study aims to determine the highest accuracy value by comparing the support vector machine method and the neural network method based on particle swarm optimization (pso).
"at present, many researches regarding the modeling of financial assets are focused on the market model based on the generalized hyperbolic distribution (n. [cit] ). the pdf of the gh distribution can be defined by:"
"where k i 2 r is the virtual spring coefficient and depends on the tissue properties. the initial position and position at time (t) of the needle in 3d space are denoted by p 0;i 2 r 3�1 and p t;i 2 r 3�1, respectively. therefore, the shear force along the segment can be written as"
"ii. then introduce a parameter as an estimated parameters furthermore, can be considered as a risk-neutral (rn) drift of a given process and it is as ∆."
"this stage is the stage where the resulting model is actually applied to data that is not yet known for its class. rating algorithm classification is usually seen from the accuracy of the model. model accuracy is the accuracy of the model in predicting data classes. in addition to the accuracy of the speed of the model formation, the ability of the algorithm to deal with irrelevant or even incomplete data, and the ability of the algorithm when applied to large or small amounts of data."
"then, the risk-neutral stock price process can be formulated using equation (31) and equation (32) for the characteristic function of the risk-neutral stock price process."
"the needle deflection can be used to correct the initial orientation error during the insertion of the needle by rotating the base of the needle. this will not only decrease the amount of needle manipulations, but also enables the clinicians to target even small lung nodules. in order to apply such a solution for steering, robotic setups such the one presented in fig 1 can be used. in order to address the second issue, breathing instructions are often given to the patient prior to the procedure to minimize the movement. the patient is asked to hold breath in a consistent fashion if the nodule is close to the diaphragm, and therefore, the motion of the nodule will be minimal [cit] . researchers have developed several robotic setups in order to assist clinicians perform needle placement accurately. the literature suggests that such systems can be beneficial for clinical use which are discussed below."
"thus, a calibration problem can be expressed as a nonlinear least-squares problem in practice for a given call option prices c i model and find a parameter vector θ such that;"
the force at the end of each segment (f i 2 r 3�1 ) is directly proportional to the displacement of the needle at that point with respect to its initial position
"in general, typically to find an analytical solution in lévy process models like a black-scholes (bs) formula. [cit] showed how to use the fft to value options efficiently based on the characteristic function of the asset. the bs model with fourier transform pricing methodology is presented first to illustrate the idea clearly and simply. a brief derivation is given for this method is the simplest case which needs to be extended to allow for a fft to be implemented."
"https://doi.org/10.1371/journal.pone.0210052.g005 hybrid control algorithm for flexible needle steering: demonstration in phantom and human cadaver 289.6 ± 7.2mm for the tip steering and 155.8 ± 8.7mm for the hybrid steering. this means around 17mm of deflection in 10cm of insertion in comparison with approximately 39mm of deflection. this results show that hybrid steering can improve the amount of deflection and therefore the controlability. the results are presented in fig 6. in experimental case ii, the needle is steered towards 5 real targets. the targets are spheres with a radius ranging between 3mm to 8mm, and these are placed randomly in the phantom. the needle is steered towards the center of the targets. the error is calculated as the absolute distance between the target and needle tip position in 3d space, and also the final angular error. the error is calculated by performing a final ct scan. the mean targeting error is 1.35±0.49mm. the results are presented in table 2 and fig 7. in experimental case iii, the targets are virtual points in 3d space within the cadaver lung, and the locations are chosen randomly. the steering experiments are repeated 5 times. the mean targeting error is 1.97±0.89mm and the results are available in table 2 . the feasibility study is performed twice and the needle is steered towards virtual targets within the lung. a reconstructed 3d visualization of the second feasibiliy study is shown hybrid control algorithm for flexible needle steering: demonstration in phantom and human cadaver in fig 8(a) . the needle trajectory for the same experiments is shown in fig 8(b), where the needle is pointed by the green arrows. the mean targeting error for the feasibility study is 2.89 ±0.22mm. the radius of curvature for the two insertions are 223.74mm and 184.57mm, respectively. the radius of curvature is calculated based on the first 25mm section of the needle, where the effect of hybrid steering is better visible. it is worth mentioning that the radius of curvature is not defined for the needle as a whole, since the trajectory is in 3d space and curvature could only be calculated locally."
"it is well known that in-sample fit alone is not enough to guarantee the predictability problem for empirical data. also, we may not determine which model is the best fit for specific data. to check this reliability, we conduct the out-of-sample performance. [cit] procedure in the following manner."
"according to (maimon, oded& [cit] ), there are 3 (three) steps in the data mining process, including: 1. preparation data is selected, then cleaned and then preprocessed in accordance with the guidelines and knowledge of domain experts who take and integrate data both internal and external into the organization as a whole."
"the deflection of bevel-tipped needles using only tip steering methods is low, especially if thick needles are used. for instance, the radius of curvature for a 22g fna needle is around 292mm in the breast tissue. base-manipulation has the advantage of increasing the needle deflection. however, the current approach of the base-manipulation technique causes tissue damage [cit] . the shear forces are high at the insertion point because the base of the needle is manipulated and there is no mechanism to minimize the movement of the needle at the insertion point. we present in this section, a new hybrid control algorithm which combines the bevel-tipped needle steering with a modified version of the base-manipulation method. in order to apply the hybrid control, we have developed a needle steering robot with a remote-center-of-motion mechanism. the robot enables us to manipulate the orientation of the needle, while keeping the insertion point fixed. this results in an increase in the deflection of the bevel-tipped needle, while the tissue damage at the insertion point is reduced. the formulation of the new needle manipulation technique, which we have called local-manipulation is described below."
"the incidence of cancer in indonesia (136.2 / 100,000 population) ranks 8th in southeast asia, whereas in asia it ranks 23. the highest incidence rate for women is breast cancer, which is 42.1 per 100,000 population with an average death rate of 17 per 100,000 population [cit] ."
"three experimental cases and one feasibility study are conducted to evaluate the proposed algorithms. in all the experiments, the needle is inserted into the tissue with a fixed velocity of 1mm/s. case i. an fna needle is inserted into a gelatin phantom using bevel-tip steering and the hybrid steering. the target is placed in an unreachable distance (100mm away from the needle) on x-axis (fig 4,④) for both methods. this causes the control algorithms to produce the maximum deflection in order to minimize the error. the gelatin phantom is made by mixing 14.9% (by-weight) porcine gelatin powder (dr. oetker, ede, the netherlands) with 85.1% water. this mixture results in a phantom with a young's modulus of 35 kpa, which is similar to breat tissue [cit] . lung tissue is very soft and results in sub-millimeter deflection for the aforementioned needle if bevel-tip steering is used. this results in noisy and not robust curvature estimation. breast tissue is stiffer than the lung tissue, which could result in more deflection for both methods. this helps to compare the proposed hybrid algorithm with the conventional algorithm. the amount of needle deflection in the two cases are compared. the results are used to validate that hybrid steering can result in a higher deflection of the needle."
"the results obtained from roc processing for the support vector machine model using training data of 0,819 can be seen in figure iv . table. iv out of 116 data as many as 35 data were predicted correctly that is included in the healthy control classification, and as many as 18 data predicted healthy control but it turned out to be included into the patients classification, 17 data were predicted by patients but included in the healthy control classification, 46 data were predicted to be exact that is included in the patients classification such as the confusion matrix table presented in table iv ."
"as mentioned by tankov, this problem has some difficulties such that; it is hard to identify the optimal model due to a finite number of market data used to solve this, the equation (47) has many local minima due to function is non-convex. cont and tankov reformulate the least square problem (47) to overcome these technical difficulties. they use the regularization method to approximate the solution to remove pricing errors."
"different robotic setups, needles and steering algorithms have been developed in order to minimize the lesion targeting error and tissue damage. we discuss some of the relevant approaches below."
"an equivalent martingale measure or risk-neutral measure q is needed for valuation of the arbitrage-free price of the derivative product written on an asset with price process, i.e. a probability measure q is equivalent to under which the discounted price process exp (− ) evolves as a martingale. in this study, two ways of finding an emm were considered: the esscher transform martingale measure and a mean-correcting martingale measure."
"many known distributions are infinitely divisible however some are not. as examples, the normal, poisson, gamma and geometric distributions are infinitely divisible. this often follows from the closure under convolutions of the type:"
"here, our interest is in backing out parameters describing risk-neutral dynamics from observed option prices, ie. inverse problem. [cit] pointed out that it is not easy to find an exact solution and the inverse problem is ill-posed. because there may be many pricing models that generate the same prices for the benchmark options thus the solution of the inverse problem is not necessarily unique. another problem is, of course, the computation of a solution for the inverse problem, for which efficient and stable algorithms are needed."
"in this work, we have proposed a new hybrid steering algorithm in which we combine two methods in order to achieve high deflection. in this new algorithm a bevel-tipped needle is steered using the tip steering and a modified version of base-manipulation algorithm. we have used a previously developed ct-compatible robotic system, which has a remote-center-ofmotion (rcm). the rcm enables us to apply the developed algorithm. the experiments are performed in a ct scanner in the lungs of a human cadaver in order to validate our method in a completely realistic scenario. this is a key step in bringing needle steering into clinical practice. a pre-operative path planner is developed which provides us the feasible paths to the target, considering the final orientation of the needle and preferred insertion region. we have used clinical fna needles for the experiments, which show the developed algorithm can be used with common clinical needles, and thus there is no need for specific needle designs."
"data collection techniques used in this study using secondary data, where the dataset used was taken from the uci repository machine learning. this research was designed by conducting a support vector machine and particle swarm optimization (pso) based neural network method to determine the highest level of accuracy in the classification of breast cancer."
"for the empirical study, daily prices of physically settled gold futures and american-style options written on these futures contracts traded at the comex were considered. the price taken as the daily price is the last trading price of the day as quoted in u.s. dollars and cents per troy ounce. further, the price for gold futures is for the nearest expiration contract and the data was obtained from bloomberg. data on futures prices span the period from january 2, 2007, to december 27, 2017, whereas the available options data set spans the period january 3, 2017, to december 27, 2017, and comprises 251 trading days. there are several maturities are available in each and every trading day. call options and the corresponding futures contracts are available with maturities in each calendar month. while trading in the futures contract ceases three business days prior to the first day of the delivery month, trading in the options written on this futures contract ends on the business day before the last trading day of the futures."
"this section describes the experiments performed to assess the proposed needle steering and path planning algorithms. the experimental setup and plan are presented below, followed by the results at the end of the section."
"the rn characteristic functions are given in the following table 2 for the above discussed models. assume that dividend yield is ignored after that for derivation. (as an example, refer appendix a, for detail derivation for variance gamma rn process. in the same manner rest of the rn characteristics functions can be derived.)"
"today gold futures markets are volatile. therefore, option prices do not support the black-scholes world of constant volatility and the lognormal distribution of asset prices at options expiration. hence, it is necessary to add the jump process to the model. lévy's process fame-works can capture these difficulties in the real market."
"1.1.1 needle steering. the needle steering methods can be categorized by the type of needle that is used. the needles can be either passive (such as symmetric tip [cit], bevel-tipped [cit], pre-bent/curved tip [cit] ) or active (such as programmable-bevel [cit] and tendon-actuated tip [cit] ). the advantage of passive needles over active needles is that their design is rather simple. therefore, those can be used in clinical practice easier and with lower costs. the deflection of bevel-tipped needles (and therefore controllability) is lower than pre-bent/curved tip needles [cit] . however, the bevel-tipped needles are currently in use for clinical application, and therefore it is beneficial if the same needle can be used for steering."
"case ii. the path planning algorithm and the hybrid steering is first tested in an anthropomorphic gelatin phantom of the thorax. the experiments are performed using ct images. an sticker is attached to the phantom, which has 7 cylindrical shape fiducials on it. a ct scan is performed and the fiducials are extracted from the ct images and the skin surface is reconstructed. the target and the final angle is selected and the path planning is executed. the suitable insertion point, and therefore the path, is selected by the user from the feasible option that the path planner provides. considering the insertion depth, the number of intermediate section of the path planner is set to five. the insertion point is marked using the laser system of the ct scanner and the fiducial sticker. the robot is then placed at the insertion point accordingly, and a new ct scan is performed (fig 4a) . the fiducials on the robot are used to register the robot in the ct scanner reference frame. the insertion is then performed automatically and a final ct scan is taken to check the error. the work-flow of the experiment is depicted in fig 5. case iii. a needle equipped with an electromagnetic (em) sensor is steered towards a virtual target in fresh-frozen human cadaver. in this experiment, the needle is tracked in realtime using an em tracker, and the needle controlled using the hybrid steering method. in this case, the skin is perforated using a surgical scalpel blade, in order to minimize skin influence on the needle trajectory and to avoid damaging of the needle's tip. the insertion point is altered for each insertion. the experimental setup is presented in fig 4. experimental case ii and case iii are summarized in table 1 ."
this section presents our hybrid control algorithm which combines tip steering and localmanipulation algorithm. this is followed by describing the pre-operative 3d path planning algorithm.
"in applying the support vector machine method and the particle swarm optimization (pso) based neural network method, rapidminer software is used, where the software has a comprehensive system for analyzing data and has the ability and flexibility and ease of use. after being applied with rapidminer then the level of accuracy is compared using the confusion matrix and roc (receiver operating characteristic) curve to find out the method that has the highest level of accuracy so that the purpose of applying the method for the breast cancer coimbra dataset can be achieved."
"data mining has its own interest in the community and the world in the field of information systems, due to the need and willingness of large amounts of data to make useful information (witten, i. h., frank, e., & [cit] ) ."
"in the first part of this study, some exponential lévy processes used for option pricing were introduced with their foremost mathematical properties. then, option pricing with fourier transform methods was calculated based on the knowledge of the characteristic function, performing the fft algorithm. finally, the model parameters were calibrated to option market prices with the non-linear least-squares method. therefore, the rn characteristic function of the vgp is formulated as:"
"so it can be concluded that the pso-based neural network method has a higher level of accuracy than the pso-based vector support machine. thus, the pso-based neural network method can be used for classification in breast cancer prediction."
"the paper is organized as follows. section ii describes the developed steering and pre-operative path planning algorithms. the experimental setup, plan and results are presented in section iii. finally, in section iv, we conclude our work and suggest directions for future work."
"the procedure was performed on a normal-sized male cadaver (died at the age of 73 years) from the groningen human body donation program. the body had no lung pathology, and had not undergone surgical procedures on the thorax. after death, the body was shaved. the cadaver was fast frozen to -40˚c, then stored at -24˚c. two days prior to the study the body was defrosted to room temperature. the internal ethics committee of the groningen human body donation program approved the use of the body."
"1~( 1, 1 2 ) & 2~( 2, 2 2 ) ⇒ 1 + 2~( 1 + 2, 1 2 + 2 2 )"
"the results obtained from roc processing for the neural network model using training data of 0,757 can be seen in figure vii . the network obtained an accuracy of 84,55% according to the confusion matrix table presented in table. v out of 116 data, 41 of them were predicted correctly, namely included in the healthy control classification, and as many as 7 data predicted by healthy control but turned out to be included in the patients classification, 11 data patients predicted but entered into the healthy control classification, 57 data predicted exactly that is included in the patients classification such as the confusion matrix table presented in table v. the results obtained from roc processing for neural network models using training data of 0,885 can be seen in figure ix based on the measurement of the accuracy of the two models with pso-based, it is known that the pso-based neural network model has a higher accuracy value of 84.55% from the results of pso-based support vector machine with an accuracy value of 80.08%."
"in order to see the pricing performance of the chosen models, the following errors are performed: average pricing error (ape), average absolute error (aae), average relative percentage error (arpe) and root mean square error (rmse)."
"according to the data and procedure mentioned earlier, the parameters underlying the black-scholes (bs), gh, nig, gig and vg models were calibrated for gold options. each model is calibrated for each maturity separately. for the sake of simplicity and to focus on the essence of the stochastic behavior of the asset, the risk-free interest rate is fixed as 10% and dividend yield to zero."
"research conducted [cit] that breast cancer is one of the diseases that makes high mortality every year. in this study, a comparison of performance between different machine learning algorithms: support suppot vector machine (svm), decision tree (c4.5), naive bayes (nb) and k-nn in wisconsin breast cancer (original) dataset was done. the experimental results show that svm provides the highest accuracy (97.13%) with the lowest error rate."
"in this work, we took some steps in order to use needle steering in a clinically relevant situation. we have used a work-flow similar to the protocol which is used in clinical practice. however, further experiments are needed before the setup can be used for patient studies. in the future, we want to perform more human cadaver experiments using the ct scanner, not only to validate the accuracy of the system, but also to evaluate the design and user-friendliness of the setup. furthermore, we are planning to perform several experiments on live animals in order to check the effects of biological motions on the needle steering accuracy and trying to compensate for the motion."
the most commonly known examples of lévy processes are the poisson process and brownian motion (also named wiener process). [cit] introduced the notion of an infinitely divisible distribution and showed that they have an intimate relationship with lévy processes. this relationship gives a reasonably good impression of how varied the class of lévy processes really is.
"the method proposed in this study is to compare the support vector machine method and the psobased neural network method. in processing the initial dataset, the data set is in the form of testing and learning data, the dataset is transformed into range 0 and range 1, then the dataset is divided into 10-fold cross validation."
"to be able to predict a disease, especially breast cancer, the use of data mining techniques is very potential to be applied into health services [cit] ."
"research conducted [cit] in the diagnosis of benign (benign) and malignant (malignant) breast cancer patients based on the results of mammography and analyzing the causal factors using logistic regression methods and support vector machines (svm from previous studies related to the classification of breast cancer using a variety of methods used, this study compares 2 (two) methods, namely the support vector machine method and the neural network method based on particle swarm optimization (pso) which is intended to determine the accuracy value the highest."
"next test with the neural network method produced in figure vi by using three layers consisting of an input layer consisting of 9 nodes including age, bmi, glucose, insulin, homa, leptin, adiponectin, resistin, mcp.1 plus one bias node. the second layer is a hidden layer consisting of seven vertices and one bias node. the third layer is the output layer, there are two nodes which represent the attributes of the healthy control and patient classes. figure 6 is the result of a neural network experiment with one hidden layer consisting of nine vertices."
"it is a characteristic function if and only if ( ) [cit], petroni (2008 petroni (, p. 1884 . the stationary process and the self-similar process are related by using the first lamperti representation [cit] ."
"time-averaged ape, aae, arpe, and rmse are listed in table 5 . the time-averaged ape, aae, arpe, and rmse are reported in table 5 . nig model lists the lowest pricing error under ape and rmse and gh model reports the lowest pricing error under aae and arpe while the bs model gives the highest errors under all criteria. therefore, we are unable to decide the better-fitted model to gold options prices according to in-sample pricing performance. however, there are more parameters in the gh model (four parameters) than the nig model (three parameters). when a number of parameters increases, the fitted model is more closed with market data. it can caught-up more variability in the market."
"let denote today price. firstly, on yesterday ( − 1), the parameters of each lévy model are recovered using yesterday's call prices and variables. secondly, today's model prices are computed using yesterday's calibrated parameters ϑ t−1 and today's variables. thirdly, using today's model prices and today's call prices, the absolute pricing error is obtained following the equation:"
"under this section, pure jump lévy processes are presented in which the asset price dynamics are modeled by the pure jump lévy process. this means that the asset price moves only by jumps with zero gaussian variance in lévy-khintchine formula. the generalized hyperbolic process, normal inverse gaussian process, generalized inverse gaussian process and variance gamma process are discussed under characteristic function, lévy process and together with some more properties."
"ijef.ccsenet.org vol. 12, no. 2; 2020 where n is usually a power of 2. fft is a commonly employed discrete approximation technique of fourier transform used to reduce computational labor. suppose that we want to approximate the inverse fourier transform of a function c t (k) in (40) with discrete fft. then this integrand should be truncated and discretized by:"
"according to globocan data released by the [cit] there were 18.1 million new cases with a mortality rate of 9.6 million deaths, of which 1 in 5 men and 1 in 6 women in the world experience cancer. the data also states 1 in 8 men and 1 in 11 women, die of cancer."
"international journal of economics and finance vol. 12, no. 2; 2020, then the m parameter is changed in an appropriate way such that discounted asset price process becomes a martingale."
"in this study, we have presented a new algorithm to steer a flexible needle by combining tipbased control with the base-manipulation control. the algorithm is developed for a ct-compatible needle insertion setup, which has a remote-center-of-motion at the point of insertion. the combination of the robotic setup and the developed control algorithm results in an increase in the deflection of the needle. a pre-operative path planner is developed, in which we consider the location of the target, the required final pose of the needle and the preferred hybrid control algorithm for flexible needle steering: demonstration in phantom and human cadaver insertion region. the path planner provides the feasible paths and the surgeon can choose the preferred path."
"feasibility study. in this study, we try to resemble the clinical conditions to evaluate our system in a realistic setup. a fresh-frozen human cadaver is used to target two virtual lesions in the lungs with an fna needle. the work-flow is similar to case ii, which is usually used for lung/liver biopsies. the experimental setup for this study is shown in fig 1. the skin is perforated using a surgical scalpel blade in this case as well."
"another way to obtain an emm is hat considering mean correcting the exponential of lévy with reference special parameter (note 3). first, all the parameters have to be estimated which are involved in the asset price process ijef.ccsenet.org"
"the testing phase is the stage of applying the rules that have been formed at the learning stage into sample data that is not included in the learning data. in this stage, the rules of the model will be applied to each attribute in the test data and see the match between the predicted class and the actual data class."
"in table 3, reports the time-averaged parameters with their standard errors of each model, because each parameter is calibrated with each maturity separately for the sample period. note that, in nig's μ is set to zero due to redundant. the results of the five calibrated models for call options are visualized as the time to maturity (from one month to six-month maturity) for all the strikes (1500-2100) on 01.10.2012, in figure b .1 through figure b .5 for the bs, gh, nis, gig and vg respectively (see appendix b). in each figure, circles are the market prices and the plus signs are the model prices. it is difficult to say which model works better than others by looking at each and every figure."
"the calculation results, the accuracy of the auc performance obtained by the results of the study are, the two methods are pso-based neural network with auc value of 0.885 and pso-based support vector machine with a value of 0.819 included in the category of good classification."
"particle swarm optimization (pso) has a simple concept, easy implementation, converges quickly, and can be applied to various applications and fields to solve optimization problems (vieira, mendonça, [cit] ) ."
"needle-tissue interaction can be modeled locally using virtual springs placed along the needle shaft [cit] . the needle is divided into n segments, and a spring is attached to the end of each segment as depicted in fig 2. this segmentation is performed only for the part of the needle which is inside the tissue. each segment can be approximated by a 3d polynomial curve"
"this procedure is repeated every day across all the call strike prices. table 6 reports the out-of-sample absolute pricing errors per call option. also, we hold the same filter on out-of-sample observations and all parameters for the numerical schemes are kept. similarly, we investigate the pricing performance as we have done for in-sampling. overall, the nig process outperforms the other models. the b-s model's absolute error was, on average, 12% greater than those of the nig process. based on all the selection criteria, the nig model shows a better fit than other models for gold options."
"despite the high blocking rates shown in section 6.2, we notice a portion of peer ip addresses could not be blocked. these ip addresses often belong to newly joined peers. therefore, a potential solution is to use these peers as bridges for restricted users. since these peers are newly joined, they are less likely discovered and blocked immediately by the censor."
"although both tor and i2p provide similar features, there are some major differences between them. tor operates at the tcp stream level, while i2p traffic can use both tcp and udp. tor has a centralized architecture in which a set of directory authorities keep track of the network, while no entity has a complete view of the i2p network due to its decentralized nature. every i2p peer helps other peers to route traffic by default, while there are only 6.5k [cit] . as a result, while tor is mainly designed for latency-sensitive activities (e.g., web browsing) due to bandwidth scarcity [cit], i2p's capacity also enables bandwidth-intensive peerto-peer (p2p) applications (e.g., bittorrent) [cit] ."
"for instance, after blocking more than 95% of active peers in the network, the attacker can inject malicious routers. he then configures the local network firewall in a fashion that forces the victim to use the attacker's routers to connect with the rest of the i2p network. in this case, the victim is bootstrapped into the attacker's network. the attacker can facilitate this process by whitelisting the group of malicious routers under their control, while repeatedly blocking addresses of other active peers. by narrowing down the victim's view of the network, the attacker is a step closer to conducting several types of attacks, including the deanonymization attack mentioned above [cit] ."
"conclusively, in economic terms, within the design organizations and divisions, we consider that supporting methods and tools should be able to seamlessly integrate into already existing development processes. particularly, methods and tools which are least-disruptive regarding existing and wellestablished design processes will possess the potential to be accepted by product-developing companies in the long run."
"although figure 2 shows that the number of peers observed in nonfloodfill mode is slightly higher than in floodfill mode, it is possible that this difference is the result of a fluctuation in the number of daily peers during the study period. therefore, we operated another 14 routers in both floodfill and non-floodfill mode simultaneously to prevent any potential fluctuation in the number of daily peers from affecting our observations. these 14 routers are divided into two groups: non-floodfill and floodfill, with seven routers in each group. for the routers in each group, we gradually increase the shared bandwidth as follows: 128 kb/s, 256 kb/s, 1 mb/s, 2 mb/s, 3 mb/s, 4 mb/s, and 5 mb/s. we pick 128 kb/s as the lowest bandwidth because it is the minimum required value for a router to be able to gain the floodfill flag [cit], while the highest value is based on the highest bandwidth usage observed in our previous experiment (section 4.1). we run these routers on machines with hardware specifications described earlier. figure 3 shows that floodfill routers with shared bandwidth lower than 2 mb/s observe 1.5-2k more peers than non-floodfill routers that have the same shared bandwidth. on the other hand, non-floodfill routers with shared bandwidth greater than 2 mb/s observe about 1-1.5k more peers than floodfill routers of the same shared bandwidth. however, it is interesting that when combining data from each pair of routers with the same shared bandwidth, the total number of observed peers (upper line in the graph) stays at around 17-18k, regardless of the difference in shared bandwidth and the number of observed peers in each mode. to explain this behavior, we first identify the four primary ways i2p peers can learn about other peers in the network:"
"nonfunctional requirements, on the other hand, can be seen as quality criteria for a set of functional requirements, which can be applied to infer concrete procedures of the design method. when adopting a model-based character for the design method, generally valid requirements for reference models such as reusability, universality, adaptability, and recommending ability can serve as an orientation for defining nonfunctional requirements [cit] . likewise, a set of nonfunctional requirements, which at the same time correspond to the general properties of reference models, can be derived from the above-defined functional requirements. these are illustrated and interrelated in table 2 . concurrently the identified nonfunctional requirements additionally can be related to some of the major qualities of a software according to iso/iec 9126-1 (norm for product quality in software engineering). these are specifically functionality, reliability, usability, maintainability, and portability, which are mapped to the nonfunctional requirements for and functional requirements (fr) of the envisaged design method. emphasizing the relation of functional and nonfunctional requirements of the design method to quality attributes for evaluating software makes sense as it enables a standardised validation process through well-defined and recognised validation criteria."
"therefore, we choose an alternative method, and opt to conduct our experiments in a passive way by operating several routers that simply observe the network. the primary goal of our experiments is to investigate how many i2p routers one needs to operate and under what settings to effectively monitor a significant portion of the i2p network with the least effort. in order to avoid the bandwidth limitation of prior studies [cit], all of our experiments are conducted using dedicated private servers instead of research infrastructure shared with other researchers."
"conducting research on anonymity networks comprising thousands of users must be performed in a responsible manner that both respects user privacy, and does not disrupt the operation of the network. it also necessitates all collected data to be handled in a careful manner [cit] . although i2p routers are run by individuals who may actively use the i2p network for their own purposes, our study does not involve any human subjects research, as it focuses on studying the infrastructure provided by i2p. our measurements do not capture users' traffic or their online activities. we solely measure network-level characteristics of the i2p network."
"due to the need of enhanced capabilities of mobile interaction devices, the design methods for mobile interaction devices will have to consider a wide spectrum of possible features as early as in the conceptualisation phase [cit] . it becomes obvious from the industrial norm din en iso 9241-210:2011 (human-centred design processes for interactive systems/ergonomics of human-system interaction) as illustrated in figure 2 (left side) that the early phases of design processes during conceptualisation particularly include the first 2 phases of the user-centred design process. these formerly represent the task and user analysis, i.e., the analysis of user context and specification of requirements. concurrently these represent the first 2 phases of the product development guideline vdi 2221 [cit] as illustrated in figure 2 (right side). the use of vdi 2221 as a sector-independent procedural guideline is recognised and recommended for development and engineering tasks of technical systems and products. explicitly in these early design phases, there is a lack of methods and tools which allow a sufficient documentation, analysis, and communication of context of complex work situations [cit] . in relation to intelligent production environments, a major challenge for an appropriate design method is the sufficient incorporation of novel interaction concepts [cit] and user/usage requirements in the design process. additionally, it has to be kept in mind that contemporary interaction devices such as control panels and operating device do not fully support potential interaction opportunities provided in intelligent environments (e.g., wireless and multimodal interfaces). further on to guideline support, existing guidelines and standards for user-centred design as various iso guidelines such as the iso 9241-210:2011 or the iso tr 16982:2002 only provide a rough qualitative framework but do not consider the evolutionary steps of the product emergence [cit] . as a consequence, there is a need for appropriate methods and tools which not only support the efficient usage of the context in the conceptual design process but also facilitate the systematic description of the context in intelligent production environments [cit] . due to the high degree of complexity in intelligent production environments, it is not feasible for the designer to consider all contextual aspects of the working situation, without supportive methods and tools [cit] . the specification of the contextual aspects can elementarily be described through models as suggested in modelbased approaches for conceptual design of products and user interfaces [cit] . however, the essential and sufficient model elements that characterise an intelligent production environment are not adequately standardised. further, if models were to include the different contextual elements necessary for describing intelligent production environments, it is still a contemporary issue to determine effective criteria and rules for mapping the model elements with one another [cit] . mapping of model elements refers to establishing descriptive or logical relations between the model elements as a basis for constructing rules how contextual elements are dependent upon each other. this is particularly relevant when the model is to be used as a data basis for a tool that supports the design process, as this would justify a connection between context and design information. concurrently, the mapping of model elements is crucial in the course of a tool for suggesting design recommendations for mobile interaction devices. although the mapping of model elements has primarily been applied to challenges related to software user interface design (e.g., graphical user interfaces) it can be adopted to physical user interface design. this becomes obvious when logical relations between functionalities of user interfaces and the context of working situations are constructed. under these circumstances, a relation to the \"mapping problem\" within the model-based design of software user interfaces can be established [cit] . the mapping problem is described as the number of transformation rules which are required to transfer an abstract model to a concrete model [cit] . while an abstract model can relate to the representation of user tasks, a concrete model represents the target platform where the user interface is to be implemented. the mapping problem itself originates from the modality theory and was described by bernsen as \"the general mapping problem\" [cit] : \"for every information which is exchanged between the user and a system while performing a task, the input and output modalities are to be determined which offers the best solution for the representation and exchange of this information\". when transferred to the context of physical user interfaces it can be interpreted that the technical functionalities of a mobile interaction device (concrete model) have to be in line with the context of the working situation (abstract model). in conjunction with our area, this means that the design recommendations of the functionalities of a mobile interaction device should be in line with the context of the working situation, the environment, and respective user group."
"in combination with mobile interaction devices, humans are empowered with new interaction opportunities. new interaction opportunities include multimodal and situated interaction where an appropriate combination of specific interaction channels (e.g., acoustic, haptic, visual, and tactile) is activated according to the intermediate context of the user (see figure 1) ."
"the i2p community has come up with a set of guidelines [cit] for responsibly conducting research in the i2p network, to which we strictly adhered. according to these guidelines, we were in close contact with the i2p team regarding the purposes of our study and our measurements. adhering to the principle of minimizing the collected data to only the absolutely necessary, we collect from i2p's netdb only each node's ip address, hash value, and capacity information available in routerinfos. finally, we securely delete all collected data after statistically analyzing them. only aggregated statistics about the collected data are published."
"therefore, it is necessary to complement this information a priori to the design pattern generation in order to bring the design pattern in line with the hci design pattern structure. finally, the emerged design patterns are crosschecked and validated by an expert and subsequently incorporated in the initial model. in this manner, it is ensured that the data of the initial model is continuously updated and thus upgraded from a nonfunctional perspective with dynamical properties."
"we note that the challenges incorporated with the early design phases in user interface conceptualisation are not yet entirely exploited. the fact that intelligent production environments are characterised by cyberphysical systems will increase the need for specialised mobile interaction devices, which support the interaction with distributed information in the cyberphysical work environment. as a consequence, methods and tools, which consider and efficiently utilise the context within the product development process, will play a significant role. moreover, it will eventually be recognised that emotional awareness is an equally important context element, which cannot be neglected in the early design phases. examples include maintenance processes and collaborative working in intelligent production environments where emotions as fear, inconvenience, fatigue, boredom, and distraction can have a severe impact on safety conditions and quality of work of the human."
"distribution. next, we utilize the maxmind database to map addresses of i2p peers to their autonomous system number (asn) and country. since about half of the observed peers are associated with more than one ip address, as discussed in section 5.2.2, we need a proper way to count the number of peers residing in each asn/country. for each peer associated with many ip addresses, we resolve these ip addresses into asns and countries before counting them to avoid counting two different ip addresses belonging to one peer. if two ip addresses of the same peer reside in the same asn/country, we count the peer only once. otherwise, each different ip is counted. figure 10 shows the top 20 geographic locations of i2p peers. united states, russia, england, france, canada, and australia occupy more than 40% of peers in the network. the united states tops the list with roughly 28k peers. except for new zealand, all five eyes countries [cit] are in the top 10. this group of 20 countries makes up more than 60% of the total number of peers observed, while the rest is made up of peers from 205 other countries and regions. among 32 countries with poor press freedom scores (i.e. greater than 50) [cit], there are 30 countries with a combined total of 6k i2p peers. china leads the group with more than 2k peers. singapore and turkey follow with about 700 and 600 peers observed in the network, respectively."
"the implementation of the modelling tool as seen in figure 9 is technically regarded as an implementation of the system architecture of the required modelling tool. at the same time it is important to note that the implementation of the modelling tool can be considered as a major part of formalizing the design method. this is because the modelling tool incorporates and connects the context models on the basis of an ontology as discussed in the preceding sections. based on the requirements and guidelines elaborated in the last sections, figure 9 presents the system architecture of a prototypical modelling tool that is composed of a front end (user interface) and backend (reasoning engine and ontology model). as illustrated in the third and fourth step of the procedural model in figure 8, the modelling tool should allow the configuration, visualisation, and analysis of the work situation, as well as the output of design recommendations. in order to fulfil these requirements, the configuration of the work situation is accomplished with the ontology data of the partial model (initial model) in the backend sphere. thus, to process the ontology data, we have applied the apache jena framework. jena as such provides a collection of tools and java libraries to develop java applications. as a rule-based inference system, jena includes a reasoning engine. the reasoning engine is responsible for analysing the consistency of ontology data and assessing the rule framework. due to this inference process, the final model is created. finally, the created ontology data of the partial models are imported and exported through the jena interface. figure 10 provides an overview of the initial model, which is represented by the ontology data of the six partial models."
"i2p is a dynamic p2p network in which peers come and leave frequently. prior to this work, [cit] conducted the first churn study of i2p and reported the probability of an i2p peer going offline after 30 minutes to be around 15%. however, the experiment was conducted for only five days, and only eight floodfill routers were deployed. [cit] ran their experiment for around two weeks and reported that 19.03% of the collected peers survived for one day, while 48.66% of them survived more than seven days. overall, these works were conducted over a short period of time and on a small scale, providing an incomplete view of the churn rate of the i2p network. moreover, none of the previous studies mentioned the address changing phenomenon of peers in the network, which often happens due to the fact that most isps do not usually allocate a static ip address to residential internet connections. in this section, we analyze the collected routerinfos to fill these research gaps. figure 7 illustrates the churn rate of i2p peers during our three-month measurement. as shown in figure 7, the percentages of peers staying in the network for more than seven days are 56.36% (continuously) and 73.93% (intermittently). that percentages of peers online longer than 30 days are 20.03% (continuously) and 31.15% (intermittently). although i2p is a purely distributed and dynamic p2p network, these results imply that more than half of the peers stay stably in the network more than a week. compared with the churn rate of 48.66% [cit], our findings of both continuous and intermittent churn rates show that the network is becoming more stable."
"we attribute the observed behavior to the last two of the above mechanisms, as they are the main ways in which our routers learn about other peers in the network. since the two groups of routers used interact with the network in different ways, each group obtains a particular view of the network from a different angle, which the other group could not observe. as a result, aggregating their data together gives us a better view of the overall network. in summary, from this experiment we learn that it is important to operate routers in both non-floodfill and floodfill modes. by combining different viewpoints, we can gain a more complete view of the network."
"unlike tor, multiple messages can be bundled together in a single i2p garlic message. when they are revealed at the endpoint of the transmission tunnel, each message, called \"bulb\" [cit] (or \"clove\" in i2p's terminology [cit] ), has its own delivery instructions. another major difference between tor and i2p is that all i2p nodes (except hidden routers, discussed in section 5.1) also participate in the network as relays, routing traffic for other nodes. in figure 1, the hops (denoted by boxed onions) forming the tunnels for alice and bob correspond to actual i2p users. while routing messages for alice and bob, these hops can also communicate with their intended destinations in the same way alice and bob do. similarly, alice and bob can be chosen by other peers to participate in the tunnels these peers will form."
"afterwards, we have performed the evaluation of the design method in cooperation with design experts from several companies. among the participants were user interface designers, mobile technology developers, and technical consultants. the purpose was to test the method in practice in order to gather feedback from designers regarding the applicability and usability of the method. the evaluation involved the usage of the modelling tool while configuring a given mobile service process. subsequently, the designers answered an online questionnaire in limesurvey, an open source online survey tool. as a use case, we have selected an inspection process in a factory plant as this represents a typical process where large amounts of data have to be collected by interacting with the environment with the help of physical user interfaces. to support the participants, we have described the inspection process as a comprehensive text, while highlighting certain keywords (process steps and tasks), which can be easily identified in the modelling tool. a time slot of seven days with three days follow-up was predetermined for the participants. the whole evaluation process, which involved installing the modelling tool, configuring the use case, and answering the online questionnaire, took averagely 45-60 minutes per participant. although 22 companies confirmed their participation, only the representatives of 11 companies practically conducted the whole evaluation. the results of the evaluation were significantly constructive for the refinement of the modelling tool, which is described in the next chapter."
"taking the observations made in section 4 into consideration, we conducted our measurements by operating 20 routers using the machine specifications defined in section 4.1. these routers consist of 10 floodfill and 10 non-floodfill routers. we collected routerinfos observed by these routers for a period of three months [cit] . figure 5 shows the number of unique i2p peers and ip addresses, including both ipv4 and ipv6, observed during the three-month period. the number of daily peers remains stable at around 30.5k. note that an i2p peer is identified by a cryptographic identifier, which is a unique hash value encapsulated in its routerinfo. this identifier is generated the first time the i2p router software is installed, and never changes throughout its lifetime. for the number of unique ip addresses, we count all unique ipv4 and ipv6 addresses (if supported by an i2p router) on a daily basis. given that some peers frequently change their ip address, as we discuss in section 5.2.2, one would expect the total number of unique ip addresses to be higher than the number of peers. however, as shown in figure 5, the total number of ip addresses is noticeably lower than the number of peers. by analyzing the collected routerinfos, we identified a large number of i2p peers whose routerinfos do not have a valid ip address field. in other words, the public ip addresses of these peers are unknown. we then analyzed other fields in the routerinfo of these peers and discovered that there are two subgroups of peers within the group of unknown-ip peers. these are firewalled and hidden peers. firewalled peers are operated behind nat or strict firewall configurations. hidden peers only use other peers to route their traffic but do not help other peers to route traffic since they do not publish their ip address in the network database. by default, peers located in countries with poor press freedom scores (i.e., greater than 50) [cit] are set to hidden. however, this setting can be modified to expose the peer to the rest of the network to benefit a better integration, thus better performance. we classify these two groups by examining the ip address field of introducers in each routerinfo file."
"queries for the network database are answered by a group of special floodfill routers [cit], which play an essential role in maintaining the netdb. one of their main responsibilities is to store information about peers and hidden services in the network in a decentralized fashion using indexing keys (i.e. routing keys). these keys are calculated by a sha256 hash function of a 32-byte binary search key which is concatenated with a utc date string. as a result, these hash values change every day at utc 00:00 [cit] . in the current i2p design, there are two ways to become a floodfill router. the first option is to manually enable the floodfill mode from the i2p router console. the other possibility is that a high-bandwidth router could become a floodfill router automatically after passing several \"health\" tests, such as stability and uptime in the network, outbound message queue throughput, delay, and so on."
"conclusively, there is a need for highly customised mobile interaction devices, which go beyond the capabilities of contemporary mobile devices. thus, input, output, and communication devices have to fulfil a wide range of interaction requirements. the reason lies not only in the fact that they are applied in different locations and situations but also because they have to be able to seamlessly integrate with the working context of the user. as such, devices must not only enable a seamless integration in the technical environment but also integrate with the activity of the user. as illustrated in figure 1, mobile interaction devices are intermediary physical user interfaces acting as a means of interaction between the human and the environment [cit] . the mobile interaction device supports the users accomplishing their primary tasks through explicit and implicit interaction. typical tasks where a support through mobile interaction devices is possible are service processes. service processes represent preventive or troubleshooting tasks which are performed in order to support the production process [cit] . apart from this, service processes are characterised through activities with an increased extent of mobility and thus often are performed in varying locations [cit] ."
"to the best of our knowledge, there has been no prior work focusing on the blocking resistance of i2p. throughout this paper, we aim to shed some light on this aspect of the network. similar to tor or any other anonymous network, i2p is susceptible to blockage. prior to this study, there have been some commercial tools alleging to be able to block i2p. however, to the best of our knowledge, despite the range of techniques used by these tools, none are able to block i2p effectively, or at least not to the degree that would be required for a large-scale adoption (e.g., nationwide blocking). we briefly review some of these tools below."
"to conduct our measurements, we need to introduce and operate several additional routers into the live i2p network. this is a standard approach in the context of studying anonymity networks, as is evident by the many previous works that have followed it to study the tor network [cit] . the i2p team also operates an i2p router to gather network information for development purposes [cit] . in particular, the stats.i2p website provides network performance graphs to help the i2p developers with monitoring the network and assessing the effectiveness of software changes in each release."
"due to the centralized network architecture of tor, it is relatively easy for a censor to find and block all public tor routers. to cope with this blocking susceptibility, several studies have aimed to enhance the blocking resistance of tor [cit] . despite its decentralized design, i2p is also susceptible to censorship, but, to the best of our knowledge, its resistance to censorship has not been extensively studied-we focus on this aspect in this section."
"as part of our future work, we plan to expand our research by studying the feasibility of using newly joined peers in combination with firewalled peers as bridges for those peers that are blocked from accessing the network."
"how resilient is i2p against censorship, and what is the cost of blocking i2p? despite the existence of many pro-privacy and anticensorship tools, these are often easily blocked by local internet authorities, thus becoming inaccessible or difficult to access by nontech-savvy users [cit] . hence, it is important to not only develop censorship-resistant communication tools, but also to ensure that they are easily accessible to end users. due to the centralized nature of tor's network architecture, it is relatively easy for a censor to obtain a list of all public tor routers and block them [cit] . even hidden routers (also known as \"bridges\") are often discovered and blocked [cit] . despite its decentralized design, there have still been reported attempts to block i2p [cit] . however, to the best of our knowledge, no prior studies have analyzed how challenging (or not) it is for a censor to block i2p access. by analyzing the data we collected about the i2p the network, we examine the censorship resistance of i2p using a probabilistic model. we discover that a censor can block more than 95% of peer ip addresses known to a stable i2p client by injecting only 10 routers into the network."
"although this study focuses on the problem of blocking access to i2p, the probabilistic blocking model we introduced is not simply an effort to block access to the i2p network. if a censor cannot completely prevent a local user from accessing the network, it can conduct attacks such as traffic analysis to deanonymize that user (e.g., revealing which destination is being visited by the user)."
"the compilation of the procedures in the table provides further insight regarding scope and structure of necessary context elements. as such, we propose that the sufficient description of the context which, e.g., includes the working situation, the environment, and potential design recommendations can be described with specific partial models."
"in this final manual step, potential contradictions and inconsistencies are uncovered, which might lead to an adaptation of the initial model such as the introduction of new data interrelations. thus, an adaptation or extension of partial models is required in order to add new expert knowledge and can be practically performed through an ontology."
"sonicwall, a company specialized in content control and network security, suggests blocking i2p by filtering out both udp and tcp tunnel traffic to block proxy access with their app control [cit] . however, this approach is not feasible at a large scale either, as the company acknowledges that the approach may cause collateral damage by unintentionally blocking other legitimate traffic, such as encrypted udp, ipsec vpn, and other encrypted tcp traffic."
"beside creative techniques, design guidelines and standards are well-established supportive tools for the early product development phases such as within the sketch phase and are nowadays in use in industry. as we have mentioned in the previous chapter, some of the most notorious guidelines are the iso guidelines such as din en iso 9241-210:2011. this design standard is based on an iterative approach and runs through an unspecific number of design cycles until the final product is established. the decisions about which design techniques and tools are to be applied in the individual stages are among other aspects (e.g., company internal guidelines and checklists) dependent upon the preferences of the designer. the drawback of this kind of guidelines is however that they fail to consider all product variations and features [cit] . concerning mobile interaction devices, typical product features are represented through interaction features. this means that a guideline could be applicable to a mobile interaction device with conventional interaction possibilities. however, guidelines and standards are not sufficient in cases when more specific and advanced devices are required, such as devices supporting multimodal interaction. in this respect, iso guidelines are usually much generalised and rarely of quantitative nature and thus are only sufficient for a concrete technology design up to a certain extent. moreover, they are often described in descriptive texts and tables, which is not very much in line with preferences of product developers for the presentation or visualisation of design recommendations. often these guidelines are complemented by company internal guidelines, such as user interface style guides and reference lists [cit] . due to the close conceptional relevance of some mobile interaction devices to mobile smart phones, design and user studies of modern smartphones and interaction concepts can be complementarily considered [cit] ."
"a further aspect was the implication of the design recommendations upon the overall user interface concept when concurrently implementing multiple design recommendations. likely, the implementation of a quantitative design recommendation may have a negative implication (undesired effect) on other interaction components; i.e., design recommendations may affect each other when these are all implemented. as an example, a recommendation of a discrete distance between the keys of a keyboard may lead to the situation that the arrangement and form factor of nearby interaction components are affected accordingly. in order to minimise this effect, it is necessary to investigate the contextual relations between design recommendations more thoroughly. a major step could be to define dependencies between design patterns, which may result in developing a pattern language. conclusively, we believe that the conception of a design pattern language for mobile interaction devices in intelligent production environments should lead to a significant contribution to the advancement of design pattern languages. however, at this point, the necessity for the development of a design pattern language is less an issue for qualitative design patterns because of the broader design space and a higher freedom of design. as qualitative recommendations are more suitable for the realisation of new user interface concepts, quantitative recommendations are more efficient for the refinement and adaptation of existing user interface concepts. for this reason, we advise considering a balance between qualitative and quantitative design recommendations."
"toolkits. an approach to support the design process of physical interaction devices (e.g., keyboards, displays) was developed by the engineering design centre of the university of cambridge [cit] . the approach is based on a web-based toolkit called \"inclusive design toolkit,\" which can be considered as an online repository of design knowledge and interactive resources, leading to a proposal of inclusive design procedures and further more specific inclusive design tools, which designers may consult to accompany them through their product development process. [cit], inclusive design as such can be considered as a general design approach for products, services, and environments that include the needs of the widest number of people in a wide variety of situations and to the greatest extent possible [cit] . generally, inclusive design approaches are currently present in the consumer product sector. although these approaches are more commonly applied to special user groups, from a technical point of view there exist no limits regarding its application to industrial products such as physical user interfaces. in this respect, inclusive design can be seen as progressive, goaloriented process, an aspect of business strategy and design practice [cit] . however, an application to industrial use cases is not feasible as the focus is upon fulfilling only the requirements of users but less upon the entire context of the application. apart from this shortcoming, offered tools for inclusive design rely on the designers' assumptions; thus assumptions have a risk of not being accurate, which can drive to incorrect assessments [cit] . for supporting the early design phase of user interfaces the university of bremen developed a set of design tools which support designers of physical user interfaces from the sketch to the evaluation phase with a virtual user model [cit] . the results of the related eu project vicon (http://www.vicon-project.eu) can be obtained from the open source platform sourceforge and from the project website. although the focus was primarily set upon users with special accessibility needs, the approach extends inclusive design principles by considering not only the user requirements but also complementarily other contextual aspects such as environmental context. additionally, the approach implies model-based options for an adaptation upon other context domains beyond the consumer product sector."
"the netdb contains two types of network metadata: leasesets and routerinfos. for instance, bob's leaseset tells alice the contact information of the tunnel gateway of bob's inbound tunnel. a routerinfo provides contact information about a particular i2p peer, including its key, capacity, address, and port. to publish his leasesets, bob sends a databasestoremessage (dsm) message to several floodfill routers, which encapsulates his leasesets. to query bob's leaseset information, alice sends a databaselookupmessage (dlm) to those floodfill routers."
"our analysis in figure 9 shows that l-flagged peers are the most dominant in the network, with an average of about 21k peers per day. this result complies with the fact that the l flag is the default shared bandwidth of the i2p router software. with more than 9k peers on a daily basis, n is the second most dominant peer type. unreachable peers occupy roughly half of the network each. note that unreachable peers include the unknown-ip peers discussed in section 5.1. we further analyze the bandwidth capacity distribution of each group: floodfill, reachable, and unreachable. as shown in table 1, while reachable and unreachable groups have a similar capacity distribution to the whole network in which l-flagged type is the most dominant and n-flagged type is the second, the floodfill group has the n-flagged type as the most dominant, and the l-flagged type comes second."
"while helping users to browse the internet anonymously, these networks also provide hidden services (comprising the \"dark web\") in which the anonymity of both senders and receivers is preserved, thus protecting their privacy. because of its popularity and the support of volunteer-based \"exit nodes\" to the normal internet, tor has been widely used and extensively researched. on the other hand, i2p has not been studied as comprehensively. we identify two potential reasons i2p has been less appealing than tor. first, i2p's purely distributed network architecture, which lacks any centralized directory service, makes it harder to measure. second, the intermittent availability of exit nodes causes i2p to operate as a self-contained network (which only serves hidden services) most of the time, making it less attractive to users who want to casually browse websites on the public internet."
"the above blocking rate is calculated based on the assumption that the censor only uses ip addresses collected on a single given day. however, the actual situation could be even worse. previous studies on tor have shown that once an ip address is found to be joining an anonymous communication network or participating in other types of network relays (e.g., vpn servers), it may get blacklisted for several days, and sometimes even for more than a month [cit] . we utilize the results obtained from the churn rate analysis in section 5.2 to examine how blocking can be more severe if the ip blacklist time window expands to a period of 5, 10, 20, or 30 days."
"setting. the probabilistic blocking model comprises (1) a group of monitoring routers operated by a censor (e.g., isp, government) and (2) a victim whom the censor wants to prevent from accessing i2p. by operating some routers in the network, the censor can acquire information about a large portion of potential peers that the victim may need to contact in order to access the network, thus being able to prevent the victim from accessing the network. the blocking rate is then calculated by the rate of peer ip addresses seen in the netdb of the victim, which can also be found in the netdb of routers that are controlled by the censor."
"having these aspects in mind, we discuss several design methods and tools in the following section. notably, we have chosen methods and tools, which are in alignment with the criteria; they provide support in the early design phases and are fully or at least partially capable of integrating and considering user context. thus, our intention was to uncover their strengths and limitations in accordance with the application to work situations and user interfaces in intelligent production environments."
"advances in human-computer interaction 7 benefits of integrating emotional context through affective sensing systems in consumer-based applications such as marketing and consumer applications, health and wellbeing, and entertainment. this tendency unveils that there exists a need for more comprehensive research efforts regarding the value and implications of emotional awareness in professional domains such as in service processes in intelligent production environments. finally, the limitations in table 1 served as an essential basis for us in order to define requirements for a design method appropriate for conceptualising mobile interaction devices in intelligent production environments. as the envisaged design method should not only be declarative but also provide mechanisms, which fulfil certain functions with qualitative criteria, it is reasonable to distinguish between functional requirements and nonfunctional requirements. likewise, the differentiation between functional and nonfunctional requirements is well-established practice in requirements engineering [cit] . in this manner, the identified limitations in table 1 provided us with a basis for deriving functional and nonfunctional requirements for a respective design method."
"in this work, we attempt to estimate the size of the i2p network by running up to 40 i2p nodes under different settings for network monitoring purposes. we find that there are currently 32k active i2p peers in the i2p network on a daily basis. the united states, russia, england, france, canada, and australia contribute more than 40% of these peers. different from prior works, we also observed about 6k peers that are from 30 countries with poor press freedom scores [cit] . this is an indication that i2p is possibly being used as an alternative to tor in regions with heavy internet censorship and surveillance."
"in this paper, we have set the focus on the early design phase (sketch phase) of mobile interaction devices, particularly suitable for work situations concerning intelligent production environments. beyond the sketch phase, the proposed context model definitely possesses the potential to be integrated and utilised in the cax design phase. hence, the advantage of this possibility lies in the realisation of (data) annotations to priority existing 3d conceptional designs of product data models. accordingly, in the eu research project vicon (http://www.vicon-project.eu) first approaches were developed for realising annotations between product data models and design recommendations from the sketch phase. this approach and the underlying techniques were demonstrated with quantitative design recommendations for user interfaces of mobile phones and washing machines. it is conceivable that these results may be used effectively for leveraging the introduced design method to be integrated into the cax design phase. next to the integration in the cax design phase, we definitely see a great potential for integrating our modelling approach with prototyping tools for physical interaction such as for tangible user interfaces and phidget components. in this respect, the formal presentation of design recommendations of our modelling tool can be construed as xml-defined design patterns in order to achieve an interoperability with physical ui prototyping tools such as the ecce toolkit and vice versa [cit] . in this way, process and tasks configurations for work situations and their interrelation to design recommendations can be seamlessly integrated on an abstract level, complementing the contemporary features of physical ui prototyping tools."
"since there is no official guideline on how to operate a high-profile i2p router, we employ a best-effort approach to determine what specifications are sufficient to observe a significant amount of other i2p routers. specifications of interest include the hardware configuration of the hosting machine (e.g., cpu, ram) and configuration parameters of the i2p router software (e.g., shared network bandwidth, maximum number of participating tunnels, size of heap memory for the java virtual machine). note that the official i2p router software is written in java. this is a necessary step in order to understand the i2p software behavior. for example, increasing the number of connections allowed to a router, without tuning the available java heap space, can result in errors that will force a router to restart. similarly, if cpu is not adequate, a router might drop connections, block, or increase latency. these are all situations under which a router would be penalized by the i2p ranking algorithm and therefore have less chances of being chosen to participate in peers' tunnels. consequently, a router that is not fine-tuned will have less visibility into the i2p network than one that can maintain a high service quality. we empirically investigate the upper bounds of a system's specifications to decide the resources we will need to dedicate to our hosts. intuitively, we know that a higher-profile router will observe a larger number of routerinfos. we first run an i2p router using a high-end machine with a 10-core 2.40 ghz cpu and 16 gb of ram. the shared bandwidth of this router is then set to 8 mb/s because the built-in bloom filter of the i2p router software is limited to 8 mb/s. the maximum number of participating tunnels is set to 15k, and 10 gb is allocated to the heap memory for the java virtual machine. after running this router for 10 days, five days in each mode (i.e., floodfill and non-floodfill), we make the following observations:"
"except for implementing an infrastructure to collect and distribute bridges, no overhead is introduced to any parties in the aforementioned solution. since most active peers in the network are selected to help other peers to route traffic by default, the above approaches only changes how censored peers pick non-blocked peers in order to access the rest of the network. consequently, utilizing newly joined peers in combination with firewalled peers can be a potentially sustainable solution for restricted users who need longer access to the network."
"we show that a censor can easily prevent local users from accessing the i2p network at a relatively low cost, despite its decentralized nature. although the victim in our censorship resistance evaluation is assumed to be a long-term and strong peer that has been uninterruptedly participating in the network, we show that a censor can still block more than 95% of peer ip addresses found in the victim's netdb. this blocking rate can be achieved by operating only 10 routers in the network, while applying different blacklist time windows and running more routers (e.g., 20 routers) can help the censor to achieve a blocking rate of almost 100%."
"the aim at this point is described as identifying the scope and type of partial models that are sufficient for describing a context in intelligent production environments. the identification of relevant context elements is based upon a qualitative analysis regarding the categorisation of context for intelligent production environments. in this analysis, we identified necessary context elements and extended these into a context model for intelligent production environments. the foundation of the model is loosely related to schmidt's model for context-aware computing [cit] . the model of schmidt proposes a context feature space for context-aware mobile computing that incorporates human and physical environment factors. on a high level, these factors can be interrelated to the functional requirement fr5-integration and consideration of context information in respect to the work context in intelligent production environments. having this in mind, schmidt's model was considered as a rudimentary basis for specifying the context in intelligent production environments. figure 4 illustrates an extended context model for intelligent production environments, where schmidt's model was used as a basis [cit] . the extended context model focusses towards supporting the design of wearable computing systems in intelligent production environments. likewise, wearable computing systems consist of configurable input and output devices and offer advanced interaction opportunities to the user. accordingly, they represent mobile interaction devices in the broader sense. pertaining to the model of schmidt, we have maintained the differentiation between human and environmental context. these context elements are concurrently connected to the functionalities and properties of a potential mobile interaction device. in this scheme, human context is directly related to the role of the users, tasks, and interactions with the environment. all situations that can be captured with human senses are relevant here. the environmental context is defined through a type of context that may result from human context and that can be captured with the help of an intermediary system [cit] . thus, an intermediary system can be represented by a technical device that is capable of capturing environment data for instance through sensors. as such, the environmental context is a dynamic context like physical conditions of a working environment (e.g., light conditions, infrastructure, and temperature), as well as the technical properties and artefacts of the environment. the underlying idea of the extended model is based on the assumption that human and environmental context directly affects the type of interaction resource and interaction modality of the envisaged interaction device. therefore a partial model describing the characteristics of potential interaction devices is necessary which is referred to as \"platform context\" in the extended context model in figure 4 . notably, the area where human context and environmental context is connected over different instances can be regarded as the context domain of a mobile interaction device. the reason is that it represents the area where both human and environmental contexts are directly connected to the platform context. as an example, considering solely very bright light conditions in the working environment may lead to the design recommendation of a light adaptive display. however, when also considering the human context, e.g., that the primary task of the user requires full visual attention, the resulting design recommendation would be further constrained and consequently lead to an alternative design recommendation for the related platform. this example shows that considering only human context is not sufficient to acquire a valid recommendation for the most appropriate platform. moreover, we think it is necessary to consider all relevant contextual aspects and their mutual impacts in order to attain a valid design recommendation for the most appropriate mobile platform. accordingly, we have consulted the extended context model in figure 4 for identifying the most significant context elements for setting up an appropriate context model. for this purpose, it is constructive to cluster the context elements of the extended context model according to their correlation. elements that have a high correlation can be united to a single context element. as a result, six context elements were abstracted which concurrently implies all context main and subelements of the extended context model, as highlighted in figure 5 . with the identified context elements, it is possible to specify the context model more comprehensively. six context elements can be transferred into six individual partial models. from this perspective, the different context elements provide insight into the scope and type of possible partial models for the design method. however, when considering that an implementation of the context model is foreseen, the complexity of the model should be held as low as possible while maintaining functional requirements. this means that the fulfilment of the requirement universality of the context model yields in describing context elements in a higher level of abstraction. this prevents the consideration of interactions as a means of refinement/detailing of work tasks. moreover, we propose to rather consider interaction constraints, interaction preferences, and exemplary interactions since these are likely to have a direct impact on the type of interaction resources. when the user interactions are reduced to interaction constraints of the user, the interaction model can be merged into the user model that results to one partial model. in detail, this means that six context elements are the basis for five partial models, namely, task model, user model, environment model, object model, and platform model."
"as shown in figure 13, five days would be sufficient to achieve a high blocking rate. this is within the capabilities offered by highend firewalls used for nationwide censorship, which can easily keep such a large number of rules."
"under the premise that these approaches are integrated into the procedural model in figure 8, the model is refined and extended with respect to the procedural steps. figure 14 illustrates the procedural model of the design method, which is extended by the feature of visualising and generating design patterns. the lower part of the model shows the extended procedural steps of the design method. from a practical point of view, the functionality of creating design recommendations is employed in the modelling tool by a designated functionality that automatically retrieves the design pattern, which corresponds to the particular design recommendation. as mentioned, the design pattern follows the composition of hci design patterns, which incorporates a description of the context, problem, solution, and an image of the platform component. however, the design recommendation and the platform component information do not per default include a description of the problem and an image of the mobile interaction device."
"since the address-based blocking implemented in the gfw of china uses the null routing technique to route unwanted packets to a black-hole router, we configure our upstream router to silently drop all packets that contain peer ip addresses that we observed from the i2p network. we then set up three testing eepsites to test the impact of the address-based blocking to the page load time. these eepsites are designed with a simple and small html file to avoid wasting bandwidth of the overall network. in addition, we conduct the test on our own eepsites instead of publicly known eepsites to make sure that our experiment does not disrupt legitimate users of those eepsites. we first crawl our eepsites to test their average normal load time. the result in figure 13 shows that a censor can block about 65% to 98% of peer addresses found in a victim's netdb. we then crawl these eepsites 10 times for each blocking rate applied, measure the page load time, and count the number of timed out requests (i.e., an http 504 is returned). figure 14 shows that the average load time of our eepsites is 3.4 seconds without blockage. by blocking other peers with a rate of 65%, a censor could already introduce a latency of more than 20 seconds to the page load time and make 40% of requests timed out. any blocking rates in the range of 70-90% could cause a significantly higher latency in page load time (i.e., more than 40 seconds), with the number of timed out requests occupying more than 60% of total requests. blocking rates higher than 90% heavily depreciate the usability of the network, with 95-100% of requests timed out."
"therefore, we choose to run a total of 40 routers equally divided between both modes (floodfill and non-floodfill). each router is hosted on a machine with the specifications defined in section 4.1. as routerinfos are written to disk by design so that they are available after a restart [cit], we keep track of the netdb directory where these records are stored. note that although there is an expiration field in the data structure of routerinfo, it is not currently used [cit] . that means the actual active time of a peer is unknown. in other words, the existence of a given routerinfo only indicates the presence of the corresponding peer in the network, but it does not provide an indication about until when a peer was active."
"in accordance with the limitations defined in table 1, it was possible for us to relate a number of functional requirements for an appropriate design method. concurrently, the identified limitations represent functional limitations of existing design approaches with respect to designing physical user interfaces for intelligent production environments. the functional requirements are therefore directly derived from the respective functional limitation and can be considered as requirements for specific functionalities that an appropriate design method has to fulfil in order to provide a sufficient support for conceptualising mobile interaction devices."
"under these circumstances, the nonfunctional requirements in table 2 illustrate that there exists a connection between the qualitative characteristics of the envisaged design method and the general properties of reference models (middle column). this aspect underpins that the design method should incorporate a model-based character. in other words, the basis for designing mobile interaction devices for intelligent production environments should be represented by a model, which describes and interrelates the context of intelligent production environments. concurrently, the model can be used in order to infer design recommendations and present these in a comprehensive way. in the next 8 advances in human-computer interaction fr2-considerations of interactive products with multimodal interaction possibilities; consideration of physical user interfaces, supporting the selection and configuration of appropriate mobile interaction devices section of the paper, the identified requirements will be consulted in order to develop concrete procedures and define a conceptional framework in alignment to the design method."
"the task model should follow the predefined task definitions, which are configured to match mobile working situations in production environments. for this purpose, we have chosen a generic and standardised task description based upon the task catalogue for maintenance according to the din 31051 (german industrial standard for the fundamentals of maintenance). in this respect, profiling is a necessary step in order to pursue a task and activity selection. to achieve this, the instances (activities) are grouped into different classes (tasks) according to their data properties. at the same time, data relations between the instances and ontology classes are defined as a foundation for the generation of rules. through completing these two steps, the requirement for the configuration of the working situation is prepared and the manual configuration of a working situation is conducted, which leads to the third step in the procedural model. for this purpose, the task of the user is specified with the task model by selecting predefined tasks and activities in accordance with the work situation. as such, the basis for the modelling of the work situation is the database of the initial model. in order to enable and visualise the configuration process, we propose to apply a modelling tool, which shall be described in a later section. after the tasks of the working situation are selected, a number of textbased design recommendations are presented. subsequently, further contextual constraints such as environmental aspects, interaction and user constraints, user preferences, and object aspects are defined with the modelling tool."
"note that the sum of all flags is not equal to 100% for two reasons: (1) the fluctuation in the bandwidth of a peer can frequently change its capacity flag, and (2) for backwards compatibility with older software versions, a peer may publish more than one bandwidth letter at the same time [cit] . more specifically, p and x flags are added since version 0.9.20, and they override the previous highest bandwidth flag (o flag). in order for older versions of the i2p router software to function normally, a peer with a p or an x flag also has an o flag in its capacity field."
"one could consider the (temporary) collection of ip addresses as a potential violation of user privacy. the topic of whether ip addresses are personally identifiable information (pii) is controversial across many jurisdictions [cit] . as stated in section 3.3.3 of the guide to protecting the confidentiality of personally identifiable information published by nist [cit], ip address not readily linkable to databases or other sources that would identify specific individuals, are not considered as pii. therefore, the ip addresses observed in our measurements cannot be considered pii, since they are not linkable to any other data collected throughout our experiments that could be used to identifying any individuals. note that the current design of i2p does not hide the use of i2p from a user's internet service provider (isp)-the i2p router software only helps to maintain the secrecy of messages and the anonymity between peers. nevertheless, we still need to analyze ip-related data in a responsible manner that will minimize the risk of exposure to third parties (before it is deleted). for instance, when mapping ip addresses to their geographic location, we do not query any public apis. instead, we use a locally installed version of the maxmind database to map them in an offline fashion."
"(1) definition of partial models in a semantic language and definition of data properties (i.e., name, real value, and category) for all six ontology classes and instances. this implies manually defining the properties of the ontology classes and partial models. (2) profiling of the task model and creation of data relations."
"we find that if the censor expands the blacklist time window from one to five days, the blocking rate increases to more than 97% with 20 routers, or 95% with only 10 routers. moreover, if the blacklist time windows expands to a period of 10, 20, and 30 days, the blocking rates increase to above 98% with 20 routers, and about 96% with only 10 routers."
"for port-based censorship, blocking onion relay ports (orports) or directory information exchange ports (dirports) is effective enough to block tor relays, and blocking udp port 123 would prevent i2p from functioning properly because the i2p router software needs the network time protocol (ntp) service to operate properly. nevertheless, many tor relays have orports and dirports running over port 80 (http) or 443 (https), while many legitimate applications also use port 123 for the ntp service. furthermore, i2p is a p2p network application that can run on a wide range of ports using both udp and tcp. more specifically, i2p can run on any arbitrary port in the range of 9000-31000 [cit] . as a result, port blocking is not ideal for large-scale censorship because it can unintentionally block the traffic of other legitimate applications."
"since more and more oppressive regimes attempt to prevent local users from accessing the tor network, tor provides users in such restricted regions with a set of special relays called bridges [cit] . similarly, i2p can adopt the use of bridges to help those restricted users to access the network, along with a non-fingerprintable traffic pattern currently in development [cit] . while the tor community may have a difficult time recruiting bridges because new bridges are often found and blocked quickly [cit], i2p has a higher potential to adopt the use of bridges because of the high churn rate of its dynamic and decentralized network."
"similar to tor's onion routing, when an i2p message is sent over a tunnel (i.e., from the gateway to the endpoint of that tunnel), it is encrypted several times by the originator using the selected hops' public keys. each hop peels off one encryption layer to learn the address of the next hop where the message needs to be forwarded figure 1 : basic communication between two i2p peers using unidirectional tunnels [cit] ."
"in network management, firewall rules are often employed to allow or filter out traffic. popular blocking techniques often base on port number, protocol signature, and ip address. however, anonymity networks, including tor and i2p, are designed to withstand censorship [cit] . as a result, any attempts to block these networks could cause considerable collateral damage."
"(4) analysis of the work situation with a reasoning engine as an integral part of the modelling tool. this is carried out automatically through the principle of inference. through this approach new knowledge is inferred from existing knowledge (within the initial model), which finally is incorporated in the final model. practically this implies that the amount of context information is reduced to the specific context-dependent information. the result is a list of text-based design recommendations in accordance with platform components, functionalities, and variations, which refer to a specific production context of an intelligent production environment."
"we consider a long-term i2p node who has been participating in the network and has many routerinfos in its netdb, which is about to be blocked. to simulate the censor, we use ip addresses of daily active peers observed by 20 routers under our control. for the victim, we run an independent router outside the network that we use to host our 20 routers."
"peers in the i2p network are classified with different capacity flags based on their (1) operating mode (floodfill vs. non-floodfill), (2) reachability (whether or not they are reachable by other peers), and (3) shared bandwidth [cit] . these capacity flags, denoted by a single capital letter, are stored in the routerinfo file of each peer. we are interested in understanding the percentage of each peer type in the i2p network. prior to this study, [cit] analyzed the distribution of i2p peers across countries. however, the multiple ip addresses phenomenon necessitates a more thorough approach for analyzing peers that change address frequently. as mentioned in section 5.2.2, more than half of the known-ip peers are associated with two or more ip addresses. in this section, we analyze two aspects of i2p peers: capacity and geographic distribution. (unreachable). for example, the ofr flags found in the capacity field of a peer, mean that the peer is a reachable floodfill router with a shared bandwidth of 128-256 kb/s. analyzing these capacity flags provides us a better understanding of peer capacity distribution in the network, and allows us to accurately estimate the total amount of peers in the network."
"since floodfill routers apply a one-hour expiration time for all routerinfos stored locally, we choose to monitor the netdb directory on an hourly basis to capture any new routerinfo. every 24 hours we clean up the netdb directory to make sure that we do not count inactive peers on the next day. after running these routers for five days, we calculate the cumulative number of peers observed daily across 40 routers. figure 4 shows that operating 40 routers can help us observe about 32k peers in the network. the number of observed peers has a logarithmic relation to the number of routers under our control. the figure also shows that the number of observed peers increases rapidly when increasing the number of routers from one to 20, and then increases slowly and converges to about 32k. in fact, the aggregated number of observed peers from operating 20 routers already gives us 95.5% (i.e., more than 30.5k peers) of the total number of observed peers. beyond 35 routers, each added router only contributes the observation of an extra 10-30 peers. therefore, we conclude that 20 routers are sufficient for obtaining a good view of the i2p network."
"these can extend from multimodal interaction possibilities to powerful processing capabilities. from a technical point of view, this is due to the integration of sensors, microcontrollers, and telemetric units as the enabling technologies. in this way physical artefacts such as control devices, tooling equipment, vehicles, and robotic systems are upgraded with enhanced technical properties, i.e., transforming from passive objects to interactive products with enhanced capabilities [cit] . humans interacting in these environments are therefore exposed to a variety of complex systems."
"while previous works intensively crawled reseed servers and floodfill routers to harvest the netdb [cit], we only monitor the network in a passive manner to avoid causing any interference or unnecessarily overloading any i2p peers. i2p can be launched in a virtual network mode for studies related to testing attacks on the network [cit] . however, experimenting on a virtual network does not fit our research goal, which is to estimate the population of i2p peers and assess the network's resistance to blockage."
"according to the procedural guidelines identified in the preceding section, we propose the corresponding procedure model in figure 8 . the procedure model is initiated by the procedure of constructing six partial models, yielding to a basic context model that combines all six partial models into an initial model. the requirement for setting up the initial model is described stepwise as follows:"
"there exists a need for design methods that support the conceptualisation phase of physical user interfaces. this particular early design phase requires techniques that are capable of ensuring that user context and requirements are adequately integrated into the entire design process from the very beginning [cit] . the challenge is particularly evident when considering interaction within complex environments such as intelligent production environments, as this would require physical user interfaces with more sophisticated technical properties. these can include advanced input and output functionalities, wireless communication, and sensing techniques. the challenge of conceptualising physical user interfaces with suitable design methods becomes clearer when discussing intelligent production environments focussing on service processes as a case study in more detail, as outlined in figure 1 . in intelligent production environments, physical artefacts such as machinery, control devices, products, and vehicles increasingly possess enhanced technical capabilities."
"since i2p is a distributed network without any centralized authorities, we need to take a black-box approach to answer our research questions regarding the size of the i2p network and its resistance to censorship. in practice, there are several ways for an adversary to harvest i2p's network database (netdb). for instance, one can keep crawling the hard-coded reseed servers to fetch as many router-infos as possible. however, to cope with such malicious activities, reseed servers are designed so that they only provide the same set of routerinfos if the requesting source is the same. nevertheless, an adversary who has control over a large number of ip addresses can still continuously harvest the netdb by crawling the reseed servers from different ip addresses. another way of harvesting netdb information is to manipulate the netdb mechanism in an aggressive manner through the databaselookupmessage (dlm) interface. normally, peers that do not have a sufficient amount of routerinfos in their netdb and peers that need to look up leasesets will send a dlm to floodfill routers to request more routerinfos and leasesets. making use of this mechanism, adversaries could modify the source code of the i2p router software to make their i2p clients repeatedly query floodfill routers to aggressively gather more routerinfos."
"the rest of the paper is organized as follows. section 2 gives an overall background of i2p and presents related works. as an indispensable part of an anonymity network study, ethical considerations are discussed in section 3, where we justify the principles to which we adhere while collecting and analyzing data for this study. in section 4, we explain our measurement methodology, including machine specifications, network bandwidths, and the i2p router types that we used to conduct our measurements. the measurement results (e.g., the population of i2p peers, churn rate, and peer distribution) of the i2p network properties are analyzed in section 5. based on these network properties, we then examine the blocking resistance of the network in section 6, where we discover that i2p is highly vulnerable to address-based blocking in spite of its decentralized nature. finally, in sections 7 and 8, we conclude by discussing consequences of the network being censored and introducing potential approaches to hinder i2p censorship attempts using address-based blocking, based on the insights that we gained from our network measurements."
"interaction. prototyping of physical user interfaces is highly relevant to understanding physical interaction between the prototyping subject, i.e., the hardware component that can be any type of input/output device, embedded system or sensor/actuator, and the human. the design task itself is likely to be successful if the designer has sound technical understanding of the physical user interface and at the same time the required interaction procedures. for this purpose, a number of toolkits have emerged supporting different facets of physical prototyping of user interfaces [cit] . most well-known physical prototyping toolkits in the hci domain include \"shared phidgets\" a toolkit for prototyping distributed physical user interfaces and \"istuff mobile\" a rapid prototyping framework for exploring sensor-based interfaces with existing mobile phones [cit] . complimentarily there exist conceptual frameworks such as for tangible user interfaces, a paradigm of providing physical form to digital information, thus making bits directly manipulable [cit] . independent of the embedded characteristics of these toolkits and frameworks, prototyping physical interaction is specifically dominated by challenges such as programming interactions among physical and digital user interfaces, implementing functionality for different platforms and programming languages, and building customised electronic objects [cit] . while the abovementioned toolkits are more or less subject to these challenges, more recent efforts such as the \"ecce toolkit\" successfully address these issues by substantially lowering the complexity of prototyping small-scale sensor-based physical interfaces [cit] . in spite of these approaches, it must be noted that these tools usually support prototyping of a specific type or category of physical user interfaces (e.g., sensors). more important is the fact that these tools do not sufficiently consider processes and task descriptions and their concurrent interrelations to appropriate user interface functionalities."
"in the preceding chapter, we pointed out that a key property of the design method is model-based, which results in the need for a modelbased design method. in respect to the identified functional requirements (fr1-fr6), it is constructive that we consider models or modelling principles as one of the major means for attaining the functional requirements. modelling principles likewise imply techniques and tools for establishing and combining information. in this case, the information is represented in context elements, which is particularly relevant for working situations in intelligent production environments. through the qualitative analysis of the functional and nonfunctional requirements, it is possible for us to elaborate several tangible procedures, which can be directly incorporated into procedural guidelines of the design method. these are described in table 3 ."
"we have then enhanced the prototypical modelling tool with the additional functional requirement of considering an improvement of the visualisation of design recommendations and an extension of the context model. in this respect, we have introduced a mechanism that enables the automatic generation of design patterns from the context-dependent design recommendations and platform components. in close correlation to this feature, we have recognised that there is a need for a feature that ensures that the underlying context model is continuously held up to date. for this purpose, we have employed a feature that enables an on-demand creation of additional design patterns while integrated into the initial model. in summary, this led to a revised procedural model for the design method."
"knowing the bootstrapping mechanism of i2p, a censor can easily block access to the reseed servers to disable the i2p bootstrapping process. as a consequence, reseed servers present a single point of blockage, similarly to tor's directory servers (e.g., [cit] ). given the current design of i2p, a new peer cannot connect to the rest of the network if it cannot bootstrap via some reseed servers."
"when user requirements are to be integrated into the early design phases, often a mixture of qualitative and quantitative methods is common such as user studies, field and user trials, market studies, or interviews [cit] . since the early product development steps as promoted during the sketch phase where user context is analysed and specified are often characterised by loose creativity and innovation, software tools that provide a systematic design support in the sketch phase are rarely in use [cit] . the reason is that loose creativity and innovation are more commonly supported by conceptual methods such as 635 method, brainstorming, or storyboards than with specific software tools [cit] . moreover, information technology is limited to a more indirect role, such as providing structures for a quick idea exchange as in mind-maps or the preparation of sketches and initial drawings like in graphic design software such as adobe illustrator. on the other hand, for idea generation creative techniques like brainstorming or 635 method are well known and used in product development; thus they still require a great deal of subjective interpretation to allow the practitioner to translate the results into tangible design features [cit] . regarding product development methods with a special focus on integrating customer requirements, it has been confirmed that methods which enable an active customer integration, in comparison to methods where customers, are integrated only passively and are more suitable for attaining customer knowledge within innovation development [cit] ."
"a more effective approach is destination filtering. to implement this approach, a censor has to compile a list of active i2p peer addresses and block access to all of them. this address-based blocking approach will have a severe impact on the process of forming new i2p tunnels, thus preventing users from accessing the i2p network. furthermore, a simpler but still effective way to prevent new users from accessing i2p is to block access to i2p reseed servers, which are required for the bootstrapping process. consequently, first-time users will not be able to access the i2p network if they are not able to fetch routerinfos of other peers. 1 one of the goals of our work is to evaluate the cost and the effectiveness of the address-based blocking approach against i2p."
"what is the population of i2p peers in the network? while tor relies on a centralized architecture for tracking its public relays, which are indexed by a set of hard-coded authority servers, i2p is a distributed p2p network in which no single centralized authority can keep track of all active peers [cit] . tor developers can easily collect information about the network and even visualize it, as part of the tor metrics project [cit] . in contrast, there have been very few studies attempting to measure the i2p network [cit] ."
"as mentioned in section 5.2.2, 58.9% of peers change their address at least once. we are also interested in analyzing this change in terms of the geographic distribution of these peers. by mapping their ip addresses to asn and country, we find that most peers stay in the same autonomous system or the same geographic region in spite of their association with multiple ip addresses. this observation is reasonable given that although isps frequently rotate different ip addresses dynamically for residential internet connections, these addresses often belong to the same subnet. however, we notice a small portion of peers changing their ip addresses repeatedly between different autonomous systems and countries. the highest number of autonomous systems that a peer associates with is 39, while the highest number of countries in which a peer resides in is 25. figure 12 shows the number of autonomous systems in which i2p peers reside in. more than 80% of peers only associate with one asn, while 8.4% of peers are associated with more than ten different ases. based on a discussion with one of the i2p developers, one of the possible reasons for this phenomenon is that some i2p routers could be operated behind vpn or tor servers, thus being associated with multiple autonomous systems. note that users of tails [cit] (until version 2.11) could use i2p over tor as one of the supported features."
"since china actively blocks access to tor [cit] and vpn [cit], a portion of chinese users seem to use the i2p network instead. the number of chinese users may be expected to increase if more outproxies become steadily available in the network. although china is one of the countries where i2p peers are configured to be in hidden mode by default [cit], a router operator can always turns off this setting to make his router more reachable, thus improving performance. figure 11 shows 20 autonomous systems from which most addresses originate. as7922 (comcast cable communications, llc) leads the list with more than 8k peers. together these 20 ases make up more than 30% of the total number of peers observed."
"complimentary to the abovementioned aspects and limitations, integrating the user context in the design process as early as possible is a primary aim in any respect in order to obtain mobile interaction devices which are fully in line with the aspects of the situation where the envisaged device is to be used."
"in this work, we aim to fill this research gap by conducting an empirical measurement of the i2p network, which may help popularize i2p to both academic researchers and internet users, and contribute to understanding its structure and properties. with those two goals in mind, our investigation aims to answer the following main questions."
"there have been only a few studies on monitoring i2p prior to this work. [cit] built their monitoring architecture on the planet lab testbed to characterize the usage of the i2p network. planet lab is a network consisting of voluntary nodes run by research institutes and universities around the globe. therefore, bandwidth and traffic policies of nodes running on this network are often restricted. as acknowledged by the group, only 15 floodfill routers could be set up successfully due to the bandwidth rate restrictions of planet lab, thus limiting the amount of collected data. the authors later expanded their work to characterize the usage of i2p, particularly the use of file-sharing applications in the network [cit] ."
"in summary, the primary contribution of this work is an empirical measurement of the i2p network, that aims to not only improve our understanding of i2p's network properties, but also to assess the vulnerability of the i2p network to address-based blocking."
"utilizing newly joined peers as bridges, however, may only be suitable for censored users who need to access i2p for a short period of time. if the peers stay in the network long enough, they will be discovered by the monitoring routers of the censor and eventually will be blocked. a potential approach to remedy this problem is to use newly joined peers in combination with the firewalled peers discovered in sections 5.1 for a more sustainable censorship circumvention."
"the blue line (lowest) in figure 13 shows the cumulative successful blocking rate of an adversary obtained by running 1-20 routers for one day. by operating 20 routers in the network, a censor can block more than 95% of peer ip addresses known by the victim, while 90% can be blocked with just six routers."
tool. the refinement of the modelling tool focussed upon two major aspects identified during the evaluation: the extension of the context model and the improvement of the visual representation of the design recommendations.
"in recent years, internet censorship and surveillance have become prevalent [cit] . for this reason, anonymous communication has drawn attention from both researchers and internet users [cit] . as anonymous communication networks grow to support more users, more anonymity and censorship circumvention tools are becoming freely available [cit] . some of these tools include proxy servers, virtual private network (vpn) services, the onion router (tor) [cit], and the invisible internet project (i2p) [cit] . tor and i2p are the most popular low-latency anonymous communication networks, which use the onion routing technique [cit] to protect user anonymity."
"in spite the fact that the six context elements in figure 5 are sufficient for describing the context in an intelligent production environment, the model does not yet incorporate design recommendations that are necessary in order to gain qualitative design recommendations as a main function of the design method. in alignment with the model-based character, we find it reasonable that the context model should, therefore, include design recommendations as an additional context element resulting to an additional partial model. this additional partial model provides the necessary data basis for inferring design recommendations, which implies that there must be data relations between the design recommendations and all other remaining context elements. strictly speaking, the recommendation model possesses data relations to all other partial models such as task model, user model, environment model, object model, and platform model. in this way, the extended context model can be leveraged to include seven context elements that are the basis for six partial models. the partial models as highlighted in the upper right side in figure 6 include task model, user model, environment model, object model, platform model, and recommendation model. the first four partial models, namely, task model, user model, environment model, object model, and platform model, provide context information for potential aspects of a working situation in an intelligent production environment. when these are interlinked with the recommendation model, the initial basis is finally prepared for an overall, functional context model."
"churn. since most isps do not allocate a static ip address for residential internet connections, it is common for peers to be associated with more than one ip address. as shown in figure 8, there are 63k peers that are associated with a single ip address (45% of known-ip peers), while more than 76k known-ip peers (55%) are associated with at least two ip addresses. moreover, we notice a small group of 460 peers that are associated with more than a hundred ip addresses during a period of three months, occupying 0.65% of the total number of known-ip peers. we characterize this phenomenon in section 5.3.2 when we study the geographic distribution of i2p peers."
"i2p provides a way for peers behind nat or firewalls to communicate with the rest of the network, using third-party introduction points (aka introducers) [cit] . an i2p peer (e.g., bob) who resides behind a firewall that blocks unsolicited inbound packets, can choose some peers in the network to become his introducers. each of these introducers creates an introduction tag for bob. these tags are then made available to the public as a way to communicate with bob. having bob's public tags, another peer (e.g., alice) sends a request packet to one of the introducers, asking it to introduce her to bob. the introducer then forwards the request to bob by including alice's public ip and port number, and sends a response back to alice, containing bob's public ip and port number. once bob receives alice's information, he sends out a small random packet to alice's ip and port, thus punching a hole in his firewall for alice to communicate with him."
"as nationwide internet censorship is growing worldwide, deep packet inspection (dpi) is widely used by various entities to detect the traffic pattern of connections to anonymity networks [cit] . regardless of the use of well-known ports (i.e., 80, 443), the traffic of connections to tor entry relays is fingerprintable and easily blocked by dpi-enabled firewall. consequently, tor's pluggable transports have been introduced to cope with this problem [cit] . these pluggable transports make traffic from a client to tor bridges look similar to other innocuous and widely-used traffic. similarly, the design of i2p also obfuscates its traffic to prevent payloadanalysis-based protocol identification. however, flow analysis can still be used to fingerprint i2p traffic in the current design because the first four handshake messages between i2p routers can be detected due to their fixed lengths of 288, 304, 448, and 48 bytes [cit] . to solve this problem, the i2p team is working on the development of an authenticated key agreement protocol that resists various forms of automated identification and other attacks [cit] ."
"the necessary steps that lead to a design recommendation are dependent upon 4 modelling steps by configuring tasks, environment, user, and objects. figure 11 shows the original version of the graphical user interface (front end) of the prototypical modelling tool. we have programmed the modelling tool in java with the support of the java development kit (jdk) and the java runtime environment (jre). the layout of the user interface and the arrangement of the functional icons follow a classical programming structure. figure 11 was at the same time the basis for a technical validation and an evaluation with end users."
"to. when the message passes through an inter-tunnel (i.e., from an outbound tunnel to an inbound tunnel), garlic encryption (i.e. elgamal/aes) is employed by the originator [cit], adding an additional layer of end-to-end encryption to conceal the message from the outbound tunnel endpoint and the inbound tunnel gateway [cit] ."
"during the configuration of the use case, the majority of the participants had difficulties to distinguish clearly between the context elements user tasks and user interactions as according to the proposed context model, tasks, and interactions were integrated into one single submodel. while configuring the process some participants expressed the requirement to be able to add individually and more detailed defined context elements, which go beyond the data basis of the initial model. regarding the quality of the design recommendations, the majority of the participants considered these as comprehensive. besides, the participants perceived the design recommendations as beneficial since they still left enough space for implementing creative efforts of the designers. however, the participants had a consensus that the visual presentation of the design recommendations should be improved since the modelling tool claims to provide a significant benefit in the early design process. in this respect, the use of examples with images such as illustrations of the platform components was proposed. table 4 provides an overview up to what qualitative extent the modelling tool fulfils the nonfunctional qualities of a software according to iso/iec 9126-1. classifications between low, medium, and high fulfilment are based on the feedback of the evaluation results."
"by examining the ip address field of the introduction points in routerinfos, we can differentiate between firewalled and hidden peers. a firewalled peer has information about its introducers embedded in the routerinfo, while a hidden peer does not. figure 6 shows the number of peers in each group. in total, there are more than 15k unknown-ip peers per day, which consist of roughly 14k firewalled peers and 4k hidden peers. between these two groups, there are about 2.6k overlapping peers. in other words, there are 2.6k i2p peers per day that have their status changing between firewalled and hidden."
"we begin by considering a censor who tries to monitor the network and gather information about active peers (i.e., ip address and port), thus being able to prevent local users from accessing the network. we then evaluate the blocking resistance of an i2p peer and the usability of the i2p network under aggressive blocking pressure."
"in alignment with the introduction of design patterns, we believe it is a reasonable path to facilitate the creation of additional design patterns based on the expertise of the designers. the creation of additional design patterns would fulfil the requirement of ensuring a continuous update of the design recommendations and platform components. vice versa, extending design patterns by the users of the modelling tool indirectly leads to an extension of the context model as suggested during the evaluation process. this becomes obvious when the considering that the data from the additional design patterns can be seamlessly integrated into the platform and recommendation models. at the same time, this approach would grant dynamic properties to the context model."
"usability and maintainability were considered as being fulfilled to a low extent since; e.g., the presentation and visualisation of design recommendations as well as the limitation regarding extension were unsatisfactory to the majority of the participants. in addition, the user interface for configuring the work situations was not implemented in a self-explaining manner and thus still has room for further improvement. on the other hand, functionality in the sense of general applicability and inferability, namely, obtaining design recommendations from context, was perceived very well be the participants."
"the solution space of the platform component consists of a text-based recommendation, a description of the functionality, of the platform component, the variations, and the sources as illustrated exemplarily in figure 13 ."
"a limitation of using maxmind is that when mapping ip addresses to asns and countries, there are around 2k addresses that we could not resolve using this dataset. nonetheless, this does not mean that we could not identify 2k peers. our results in section 5.2.2 show that more than 55% of known-ip peers are associated with more than one ip address. therefore, the actual number of peers whose asn and country we could not identify are just those peers that are associated with only one ip address we could not resolve. as mentioned in our discussion of ethical considerations, we do not use any of the more accurate public apis on the internet to resolve these ip addresses for privacy reasons."
"we should note that throughout our study, we not only contribute additional routing capacity to the i2p network, but also help in maintaining the distributed network database. considering only the main experiment over a period of three months, each router under our control is configured to contribute a shared bandwidth of 8 mb/s in each direction, with an observed maximum usage of 5mb/s."
"in this paper, we dealt with the realisation of a design method for conceptualising physical user interfaces, namely, mobile interaction devices for intelligent production environments. the analysis of the state of the art had the aim of identifying the shortcomings of existing design methods, tools, and techniques. this enabled us to derive a series of functional and nonfunctional requirements. these were interrelated to one another while construing (at this point loosely defined) procedural guidelines for a suitable design method, which yielded in a comprehensive procedural model. in alignment with the procedural model, we proposed a model-based conceptual framework, which considers the scope, structure, and modelling techniques for describing the features of mobile interaction devices. thus, in line with the notion of a model-based framework, we further proposed an ontologybased context model, which consists of six interrelated partial models that provide the means for describing intelligent production contexts. correspondingly, we suggested the need for a modelling tool, which enables not only the configuration of work situations but also the inference of design recommendations through the utilisation of a reasoning technique. this allows the elaboration of text-based design recommendations in relation to platform components and their priority level for mobile interaction devices. the implementation of a modelling tool as a part of the design method was achieved through the implementation of a system architecture, which includes all modules required for the applying of the design method. within the prototypical version of the modelling tool, we managed to demonstrate that a series of design recommendations are inferred, including also those recommendations that are not explicitly excluded. due to this fact, the design recommendations were categorised context-dependently according to \"high,\" \"medium,\" and \"low\" priority."
"the i2p developers have foreseen a situation in which all reseed servers are blocked. thus, a built-in function of the i2p router software is provided to allow for manual reseeding. with this feature, every active i2p peer can become a manual reseeder. specifically, the function can be used to create a reseed file called i2pseeds.su3. the file can then be shared with other peers that do not have access to any reseed servers for the bootstrapping process. the sharing can be done via a secondary channel, similar to how tor distributes bridge nodes (e.g., emails, file-sharing services). under this circumstance, a censor who wants to prevent local users from accessing i2p has to find and block all addresses of active i2p peers. however, since i2p is a distributed p2p network, it is challenging to obtain a complete view of the whole network. we investigate the effectiveness and the efficiency of this blocking approach next."
"tenable, a network security company, provides a firewall service that contains some modules to detect i2p traffic. based on our review of their guidelines, none of them seem to be efficient in blocking i2p. for instance, one of the guidelines for detecting i2p outbound traffic is to manually inspect the system for any rogue process [cit], which may not be feasible for large-scale blocking such as nationwide censorship."
"when alice wants to communicate with bob, she sends out messages on her outbound tunnel. these messages head toward the gateway router of bob's inbound tunnel. alice learns the address of bob's gateway router by querying a distributed network database [cit] (discussed in more detail in section 2.1.2). to reply to alice, bob follows the same process by sending out reply messages on his outbound tunnel towards the gateway of alice's inbound tunnel. the anonymity of both alice and bob is preserved since they only know the addresses of the gateways, but not each other's real addresses. note that gateways of inbound tunnels are published, while gateways of outbound tunnels are known only by the party who is using them."
"the example in figure 1 illustrates a case in which i2p is used as a self-contained network, with participating peers communicating solely with each other. however, if bob also provides an outproxy service, alice can relay her traffic through bob to connect to the public internet. the returned internet traffic is then securely relayed back to alice by bob via his outbound tunnels, while alice's identity remains unknown to both bob and the visited destination on the internet."
"for the purposes of our research, the above approaches are impractical and even unethical. although one of the goals of this paper is to estimate the population of i2p peers, which requires us to also collect as many routerinfos from the netdb as possible, we need to conduct our study in a responsible manner. our principle is that experiments should not cause any unnecessary overheads or saturate any resources of other i2p peers in the network. [cit] showed that crawling reseed severs only contributes 7.04% to the total number of peers they collected, while manipulating the netdb mechanism only contributes 30.18%."
"apart from the challenges related to the subsequent design phases, we expect that software user interfaces will converge with physical user interfaces. an appropriate example which highlights this paradigm is to consider touch screens, as hardware and software elements are directly entangled with one another. thus, design methods in this area should be capable of considering both development strands. additionally, due to the continuous advancement of mobile interaction devices including wearable technology, we anticipate that adaptive hardware concepts will play an increasingly vital role in cyberphysical production environments. further, we believe that it is likely that adaptive physical user interfaces are capable of coping with a wide range of situations, as it is currently possible with discrete mobile interaction devices. thereby, new requirements and possibilities will emerge within user interface design. it will be less the case to uncover relations between interaction concepts and work situations. moreover, the point will be to describe the spectrum of configuration possibilities of adaptive interaction devices in relation to different work situations. when we consider the continuous emergence of the fourth industrial revolution, it is reasonable to predict that adaptive hardware concepts will be a well-established interaction concept in the future. this tendency will concurrently foster new paradigms in product development, which possess the potential of dissolving the limitations between the design process and the designed artefact."
"according to figure 6, there are around 14k firewalled peers in the network on a daily basis. without a public ip address, the censor cannot apply the address-based blocking technique introduced in section 6.2. in the current i2p design, the chance that a censor can discover the ip address of these firewalled peers depends on the probabilities that the routers under the censor's control (1) are selected to be introducers for these peers, and (2) they directly interact with these firewalled peers."
"a more comprehensive overview of the database of the partial models is made visible by applying the prototypical modelling tool. the focus of the modelling tool is restricted to the data basis of the partial models, and as such to the sequential configuration of a mobile work situation within a production scenario in order to acquire appropriate physical platform components and design recommendations for conceptualising mobile interaction devices. these are to be in line with the predefined context of a mobile work situation."
"we have conducted the validation of the modelling tool through the configuration of a representative working situation and comparison to the internal data relations of the model [cit] . as such, the validation provides the evidence that the modelling tool and finally the design method is capable of continuously generating the appropriate design recommendations for mobile interaction devices, which are in line with a specified work situation. as a typical work situation, we have chosen a maintenance process in a production environment where troubleshooting and calibration represent the tasks that are performed. in the work situation, the user is interacting with tools, wearing protective gloves and prefers visual data acquisition. we have further conducted the verification of the design recommendations on basis of the ontology data of the initial model. for every stepwise configuration of the work situation (task, environment, user, and object), the data relation of the design recommendation has been manually read out of the data model and documented. afterwards, we have compared these results to the results provided by the modelling tool when successively configuring the described work situation. the results confirm that the manually determined recommendations are fully in line with the results determined by the modelling tool. this means the manually determined design recommendations from the initial model are identical to the automatically determined design recommendation by the reasoning engine. the identification of inconsistencies would have been an evidence that there is a bug in the data relations, which might lead to different or inappropriate recommendations."
"directory. the network database of i2p, called netdb, plays a vital role in the i2p network by allowing peers to query for information about other peers and hidden services. the network database is implemented as a distributed hash table using a variation of the kademlia algorithm [cit] . a newly joining peer initially learns a small portion of the netdb through a bootstrapping process, by fetching information about other peers in the network from a set of hardcoded reseed servers. unlike tor directory authorities, these reseed servers do not have a complete view of the whole i2p network. they are equivalent to any other peer in the network, with the extra ability to announce a small portion of known routers to newly joining peers."
"in this manner, we decided to extend the modelling tool with a functionality that enables the automatic generation of physical design patterns as highlighted in figure 12 . in order to demonstrate the feasibility of this approach, we found it reasonable to realise twenty exemplary design patterns, consisting of input, output, and communication devices. subsequently, we have implemented an automatic mechanism in the modelling tool that enables an automatic generation of design patterns based on the design recommendations of the tool. the design patterns can then further be grouped according to their affiliation to ontology subclasses and instances with the aim of validating the consistency of design patterns for a certain working situation. this means design patterns belonging to the same ontology subclasses and instances can be grouped by a certain category, while design patterns with overlapping ontology subclasses and instances can be considered interrelated. for example, two design patterns describing a keyboard and a flexible display, although mainly possessing different ontology subclasses and instances, could both belong to the task instance \"maintenance\" and therefore be regarded as interrelated to one another. subsequently, for a configured work situation, although design patterns belonging to different subclasses and instances are suggested, it can nevertheless be assumed that there exists a relation between these individual design patterns. finally, related design patterns are more likely to be appropriate for a certain work situation than design patterns, which are not related in any sense. thus, the property of relations between design patterns represents one of the main requirements of a design pattern language, which is a good basis for an extension of this work."
"in the first version of the modelling tool as described in section 2.5, the design recommendations are prioritised regarding their relevance, based upon the approval of the applied user interface technologies in practice. however, the prioritisation is limited by a fixed predefinition in the initial model (high, medium, and low), which does not provide a mechanism to further distinguish between same priority levels of design recommendations. practically, this means that two or more design recommendations having the same priority levels and belonging to the same type of interaction component are not further distinguished. this leads to the fact that the designer has to decide which design recommendation is most suitable for the work context. through dynamically assigning priority levels to the design recommendations, it would be possible to differ, e.g., between relevant and optional design recommendations. this could be achieved by employing an intelligent algorithm, which may keep track of how often a design recommendation has been proposed and employed for a certain work context."
"in this work, we conducted a measurement study to better understand the i2p anonymity network, which then allowed us to examine its censorship resistance. although i2p is not as popular as tor, mainly because it is used as a self-contained anonymity network, the results of our measurements show that the network size is consistent over the three-month study period, with roughly 32k daily active peers in the network. among these peers, about 14k of them are connecting to the i2p network from behind nat/firewall. during our three-month study, we also discover a group of about 6k peers from countries with poor press freedom scores."
"through the analysis of the abovementioned design methods, approaches, and tools, we have unveiled a number of limitations and insufficiencies regarding their application to designing physical user interfaces for intelligent production environments. these are summarised in table 1 ."
"the graphic autocorrelation of the 2d image compares all possible pixel pairs and expresses the probability that both pixels will have similar values as a function of the distance and direction of separation. mathematically, the autocorrelation of an image i is the convolution of a function with itself, which can be calculated by:"
"in the encryption design, all the key space must produce chaotic behavior (i.e., strong keys) and avoid using weak keys (i.e., some secret key that produces non-chaotic dynamics). in some chaotic maps, the bifurcation diagram presents periodic windows combined with chaotic dynamics. in this sense, designers must avoid such values that generate periodic behavior. one method to determine if all the key spaces are strong is that the bifurcation diagram for each control parameter could be presented or calculating the lyapunov exponent to validate chaos."
"the guideline can be adapted for other kinds of plain text in chaos-based cryptosystems, e.g., alphanumeric text, biometrics, biosignals in telemedicine, or pseudo-random number generators. future detailed frameworks related to chaos-based image cryptanalysis could be presented to design algorithms robust against powerful attacks (for example, chosen-known plain image attack)."
"the pearson correlation coefficient (pcc) is the metric implemented to support the visual inspection of the graphic correlation. in statistics, pcc measures the grade of the linear relationship between two quantitative variables by the numeric index, which varies between [−1,1]. when it is −1, there is a perfect inverse relationship, whereas when it is one, there is a perfect direct relationship. just when pcc is zero, neither tested variables has a linear relationship (null correlation)."
"the encrypted image can be alterable either by noise attack or noise jamming in the transmission channel, which can make it difficult to recover the plain image. thus, the robustness against noise must be evaluated over chaos-based image cryptosystems. the encryption process of recent algorithms gives some advantages against noise since most of them are based on the substitution-permutation network, uniform chaos, and one-per-one pixel encryption. considering such principles and the bulk data of images, the original plain image can still be reconstructed with high visual quality."
"where p is the original plain image, d is the recovered or decrypted image, and e is the error in percentage. an ideal chaos-based image cryptosystem must present an error e of zero, i.e., the recovered image must be identical to the original plain image."
"the substitution-permutation network has been adopted in most chaos-based encryption algorithms proposed in the literature, with one or several rounds of each process to increase security. the main idea is to change the value of each plain element and change its position according to chaotic sequences to produce cryptograms that are statistically efficient."
"the encryption algorithm must be highly sensitive against the lowest change of the secret key to make all the key space efficient. the encryption and decryption process must show this important characteristic. in the encryption process, the same plain image p is encrypted two times with two similar secret keys (k 1 and k 2 ), and the corresponding cryptograms (e 1 and e 2 ) must be very different between them and visually unrecognizable."
"in some cases, the control parameter can be adjusted to guarantee chaos. for example, if the control parameter of the logistic map starts with 3.9 and a double precision (64 bits) floating-point format is adopted, then we can manipulate 14 digits (but reducing the complexity from 2 52 to 2 49, approximately), but it produces always chaotic sequences (strong keys). we suggest to complement this with some numerical validation by using several tests with several secret keys selected randomly to provide robustness."
"most of the chaos-based image cryptography proposed in the literature is based on the symmetric-key, since it is faster than asymmetric-key cryptography. in addition, most of them are based on matlab (software) implementations with c code due the fast prototype design and practical environment for testing. others are implemented with java code for professional applications to achieve better speed in encryption. on the other hand, the cost and performance of chaos-based image cryptosystems based on hardware implementations such as in field programmable gate array (fpgas) and application-specific integrated circuits (asics) are currently an interesting research area due to the wide applications, where large volumes of secure data need to be processed (such as in biometrics, telemedicine, military, pay tv, ssl accelerators, etc.), and due to the benefits in space and cost with high performance security in an embedded system. therefore, other requirements must be determined such as power consumption or occupied logical elements (slices, luts, dsp blocks, etc.) for hardware applications."
"the speed analysis can be presented in both encryption and decryption process in seconds, encryption throughput (et) in bytes per seconds, and number of needed cycles to encrypt one byte (n pcb). basic details of implementation must be provided such as the operating system, cpu with the main frequency, ram memory size, compiler, and programming language, since speed is dependent on these characteristics."
"information security in modern digital systems and telecommunication networks has been one of the major concerns during the last few decades with an increasing research area in cryptography to provide confidentiality and protect secrets from eavesdroppers, intruders, adversaries, or enemies by using cryptosystems. transmitting or storing confidential images over insecure channels can have a high risk. thus, such information must be protected prior to sending or storing, e.g., clinical images or radiological photos (for diagnosis or consultation) in telemedicine, personal identifiers such face, iris, or fingerprint images (for control access systems) in biometric systems, or satellite maps in military are some potential applications for image encryption. basically, an encryption algorithm will map the plain text (recognizable message) to encrypted text (unrecognizable message) with some specific key, and a decryption algorithm will map the encrypted text to plain text with the corresponding specific key. cryptosystems can be performed by hand methods, machine methods, or software based on shannon's model. in cryptanalysis, several kinds of attacks can be implemented to break a cryptosystem such as side-channel attack, physical attack, invasive attack, radio frequency attack, impersonation attack, chosen/known plain text attack, differential attack, exhaustive search attack, among others. for example, a physical attack in cryptographic implementations includes all chaos-based image cryptosystems. nist considers security as the most important criteria in a cryptographic evaluation, following the cost, algorithm, and finally, implementation [cit] . recent chaos-based image cryptosystems or hyperchaos-based cryptosystems do not present a comprehensive security analysis according to table 1, which could make them unreliable and insecure for some applications (e.g., telemedicine, biometrics systems, or in military affairs). even analysis about cost end performance and the algorithm and implementation are not considered in detail. such differences in security terms between recent chaos-based algorithms are due to the lack of frameworks or guidelines for security analysis, which is the main motivation of this paper."
"most of the chaos-based digital cryptosystems implemented in matlab use the ieee standard for floating-point arithmetic (ieee 754) for floating-point computation in 64 bits (double precision) to represent real numbers of chaotic states with the aim of avoiding digital degradation and providing a huge key space. note that not always all 64 bits (1 bit for the sign, 11 bits for the exponent, and 52 bits for the significant precision) are used for each variable of chaotic maps (i.e., chaotic states or control parameters). therefore, the cryptographic designer must take care about the range of these values to estimate the key space. for example, the well-known 2d hénon map is defined by:"
"on the other hand, cryptanalysis has been applied in chaos-based image cryptosystems by using the powerful chose/known plain image attacks [cit] . the chaotic keystream for encryption could be derived from both the secret key and the plain image to provide robustness against this kind of attack. nevertheless, if an error occurs in the transmitted cryptogram, than it could be impossible to decrypt such a cryptogram. some recent chaos-based digital cryptosystems do not considered this important fact. in addition, it provides higher sensitivity against plain images. recently, some advances over the algorithm design have been addressed to resist the chosen/known plain image attack, where the plain image is included for the chaotic keystream used in the encryption process [cit], adding random values to the plain image before encryption [cit], or by using the sha256 algorithm (hash function) for the plain image [cit], but the receiver must have the same hash value for each different image encryption; it could be included in the cryptogram, but if it is damaged during transmission, the receiver cannot retrieve the original image; or the sender could securely exchange it with the receiver each time a different plain image is encrypted."
"chaos-based image cryptosystems must meet another important aspect related to reliability, integrity, flexibility, simplicity, and ease of implementation since there are several types and formats of images and several kinds of digital platforms. the most-used bitmap image formats are bmp, gif, jpg, tif, and png. however, it is convenient to use a format that does not imply loss of information (e.g., jpg) for the encrypted image since the decryption process could not be successfully. high processing platforms such as fpga or asic can be used for cryptosystem implementation at the hardware level for low-cost embedded system applications of high-definition image encryption (hd), audio encryption, and video encryption. on the other hand, high-performance microcontrollers are interesting for the software level to medium quality image encryption applications at low cost. finally, the pc is the platform most widely implemented for image encryption algorithms based on matlab programming."
"chaos-based image cryptosystems require considerable memory space in both the hard drive or flash memory and ram memory due to the bulk data of images and real variables for chaos implementation. when countermeasures are added to increase security and chaos uniformity, the memory requirements are more demanding."
"the encryption algorithm is a step-by-step procedure written by some language, and its efficiency is highly important in terms of complexity and execution speed. the complexity is asymptotically estimated with big-o notation as a function that depends on the input size n, e.g., o( f (n)). the complexity of a series of sentences of an algorithm is of the order of the sum of the individual complexities, and some practical rules are considered, e.g.,: input-output simple sentences and if sentences are of order o(1), the for cycle is order o(1) for k iterations independent of the input n or o(n) for n iterations; the for double nested cycle is of order o(n 2 ) for n iterations for each cycle; the iterative cycles with divisive-multiplicative sentences are of order o(log n) for n iterations; and o(log n) in the for cycle with n iterations is of order o(n log n)."
"chaos-based image cryptography is currently a world-wide research topic with hundreds of papers that have been published in the last few years. the lack of security frameworks for chaos-based image cryptosystems make each author present different tests about security over the proposed algorithms. even some algorithms have been broken in the last few years. in this sense, security guidelines are required to make chaos-based image cryptography more consistent and promising."
"the graphic correlation is a visual inspection of the pixel's correlation of the image, where the horizontal axis represents the intensity value of the pixel and the vertical axis represents the neighbor pixel value, either horizontal, vertical, diagonal, or random. the expected graphic correlation of the plain image must present a strong pattern over a line at 45 degrees; the more close to this line, the more correlated is the tested image. in the encrypted image, the expected graphic correlation must present points over all the plane since most of the neighbors of any pixel have different intensity values with respect to that pixel."
"security analysis is a testing technique to determine if a cryptosystem protects the information adequately, and it must include analysis against all known attacks efficiently, since excellent performance and implementation advantages of a broken cryptosystem are irrelevant. therefore, every chaos-based cryptosystem must provide enough information regarding their security and efficiency."
"in image chaos-based cryptosystems, the recovered image is supposed to be identical to the plain image, which is required in sensitive applications such as telemedicine, biometrics, military, and others. in some recent schemes in the literature, designers implement techniques to enhance the security such as random pixel insertion in the plain image or save some specific data in the encrypted image before sending it over the insecure channel. therefore, the recovered image is not the same as the original plain image, and this error must be determined quantitatively. the decryption error is defined as follows:"
"the simplicity is related to the relative algorithm design simplicity to analyze and implement efficiently such as using less arithmetic operations, using simple substitution-permutation processes, including gui (graphical user interface) with intuitive symbols and easy operation, or including guide (graphical user interface development environment) by using matlab, among others. it is one of the most important factors in selecting cryptographic algorithms, e.g., the rijndael algorithm was less secure than its rivals in the aes competition, but it was selected as the aes winner due to its simplicity [cit] . in this sense, chaos-based image encryption algorithms must provide detail about their flexibility and simplicity."
"the cost and performance of image chaos-based cryptosystems must be provided in detail, since they are the next important point to evaluate after security. cryptographic designers and cryptanalysts could decrease the performance of some highly secure cryptosystems if it is at a high cost. therefore, the cryptosystem must provide enough security with a reasonable cost and performance. memory requirements, computational efficiency, and speed analysis are some constraints that must be evaluated."
"the mse of efficient cryptograms is high. therefore, the psnr of encrypted images with high quality is expected to be low, less than 10 db. on the other hand, the ssim determines the similarity between two images with a more consistent technique than mse, i.e., a burred image is perceived as a bad quality image, which is consistent with ssim, but not with mse. ssim considers the mean, standard deviation, and cross-correlation in both plain image p and encrypted image e. it is calculated with the following expression:"
"the pixel values of any plain image at any position have neighbors (horizontally, vertically, or diagonally) with pixel values that are highly similar, i.e., all plain images are strongly correlated. this natural property implies designing efficient cryptosystems to produce non-correlated encrypted images and reduce the risk of statistical attack."
"in noise analysis, the cryptogram is spread with salt and pepper noise with different densities by using the \"imnoise\" function of matlab. the percentage of affected pixels can be determined as follows:"
"in this paper, we present a suggested integral analysis framework based on a comprehensive security analysis, cost and performance analysis, and algorithm and implementation details, with the aim to establish a basis in security analysis for chaos-based image cryptosystems. the proposed guideline based on 20 analysis points can assist cryptographic designers to present a common study about an integral security analysis of their new cryptosystems with the aim to show exhaustive information about security, reliability, integrity, protection of secrets, easy of use, speed, and acceptable costs. if the authors consider this guideline, future comparisons of new proposals can be more consistent in terms of security and efficiency, which can help to develop more secure and efficient encryption algorithms. the suggested framework is based on a literature review, new analysis (such as histogram uniformity, graphic autocorrelation, and floating frequency), and the evaluation criteria of nist for cryptosystems (security, cost and performance, and the algorithm and implementation). we use the chaos-based rgb image encryption algorithm [cit] to determine the results of the suggested framework. furthermore, we present aspects regarding digital chaos implementation, chaos validation, and key definition with the aim to improve the overall security of new cryptosystems. designers are invited to follow the recommendations of this work, but without limiting their liberty, new analysis, and their own creativity to establish the security of their new cryptosystems."
"flexibility is related to the capacity of the algorithm to endure minor modifications according to the requirements. encryption algorithms with greater flexibility are preferable since they will meet the needs of more users (assuming good overall security and performance). for example, the 3des structure does not support any modifications (i.e., its flexibility is null), and rc6 [cit] bits with a multiple of 32 bits (secret key flexibility). examples of flexibility may include (but are not limited to) the following:"
"digital chaos implementation implies finite numbers either with floating-point arithmetic, fixed-point arithmetic, or other arithmetic at a reasonable digital word size. chaos degradation occurs principally for continuous chaotic systems, when a numerical solution such as euler or runge-kutta (rk4) is applied. on the other hand, chaotic maps are discrete by nature. nevertheless, a chaotic sequence can fall into a short chaotic cycle when it is realized in a low digital word size. it is known that fixed-point arithmetic requires low-power, low-cost, simpler, and smaller hardware than floating-point arithmetic for a reasonable word size in signal processing. however, the floating-point has a larger dynamic range and can represent numbers of the real world. therefore, the floating-point reduces the risk of overflow, quantization errors, and the need for scaling. for example, when a chaos-based cryptosystem is implemented in matlab, the format long is used for a 64-bit word size in fixed-point arithmetic, whereas format long e is for a 64-bit word size in floating-point."
"the rest of the paper is organized as follows. in section 2, digital chaos aspects are presented to improve the overall cryptosystem security. the suggested framework related to a comprehensive security analysis is given in section 3. the cost and performance analysis are shown in section 4. in section 5, the algorithm and implementation details of chaos-based digital cryptosystems are presented. finally, the conclusions are mentioned in section 6."
"the variance and standard deviation are metrics of dispersion implemented to support the results of visual inspection in graphic histograms. they measure how much the elements of a set of data vary with respect to each other around the mean. two datasets may have the same average value (mean), but the variations can be drastically different. the variance calculates the average difference between each of the values with respect to their central point (meanx). this average is calculated by squaring each of the differences and calculating its mean again. the squaring process is used to eliminate the negative signs and to increase the variance of dispersed (non-uniform) datasets. on the other hand, the more uniform is the graphic histogram, the lower is the histogram variance, which is determined with the following expression:"
"the suggested guideline in this paper does not intend to limit the liberty of cryptographic designers to implement new analysis, and it does not guarantee security. however, if cryptographic designers consider this framework based on 20 points, an integral analysis about the comprehensive security analysis, cost and performance, and algorithm and implementation of new chaos-based image cryptosystems can be presented more consistently in terms of security and efficiency. consequently, future chaos-based image encryption algorithms can be compared rigorously in terms of security and efficiency capabilities. cryptographic designers could present just the results about security in their works (e.g., in a table) pointing out to each of the 20 points suggested in this paper without describing all details about each test."
"in the last two decades, designers have been implementing cryptosystems based on one-dimensional chaotic maps such as the logistic map or piecewise linear chaotic maps and with two-dimensional chaotic maps such as the 2d logistic map, hénon map, arnold's cat map, the duffing map, the ikeda map, the standard map, the tinkerbell map, and the baker map."
"in this section, measuring techniques are used to verify the image encryption quantitatively such as mean-squared error (mse), peak signal-to-noise ratio (psnr), and structural similarity index (ssim). the mse is a parameter to measure the difference between two images, which is described as follows:"
"the sequences used for the encryption process must be validated chaotically to produce cryptograms with good statistical properties. the initial conditions and control parameter of chaotic maps are considered as the secret key. therefore, control parameters must be defined adequately to avoid non-chaotic regimes. for example, the logistic map produces chaos when the control parameter is between 3.57 and four. nevertheless, there are some values where the dynamics become periodic. a robust numerical validation of the chaotic sequence must be presented such as the lyapunov exponent [cit] or the recent 0-1 gottwald-melbourne test [cit] . a positive lyapunov exponent indicates chaos, and a value close to one in the gottwald-melbourne test indicates chaos."
"therefore, plain images do not need to present pcc close to one since they are highly correlated. on the other hand, encrypted images must present pcc close to zero to resist statistical attacks. the pcc of an image can be measured as follows:"
"cryptosystems based on digital chaos have attracted more attention than its analog version, since discrete chaotic systems or chaotic maps present advantages related to security, performance, flexibility, and cost. [cit], li proposed some basic suggestions to design fast and low-cost digital chaotic ciphers: choosing the simplest chaotic map to achieve better performance, using the minimum iterations in chaotic maps, avoiding use of multiple chaotic maps in software implementations, using fixed-point arithmetic as much as possible, avoiding chaotic maps with floating-point arithmetic requirements, and adopting a parallel mechanism in hardware implementations [cit] ."
"in this section, we present several statistical security analyses that can be applied over chaos-based image cryptosystems. the presented security analysis is based on a literature review, and we propose three new statistical analyses such as histogram uniformity, graphic autocorrelation, and floating frequency. table 1 shows us that most authors included basic tests such as key size, sensitivity of the secret key and plain image, information entropy, histograms, and correlation. in some cases, other analyses are considered such as encryption quality, noise, and occlusion. such differences make it difficult for a robust comparison among cryptosystems in terms of security or efficiency."
"an ideal uniform histogram of an encrypted image must have the same pixel frequency for all 256 intensity values. in this case, the histogram variance will be zero with a standard deviation of zero. the histogram uniformity percentage (hup) is introduced to determine the intensity values falling between the acceptable range, which is defined as follows:"
"where x and y are two variables defined by some pixel values and their neighbor pixel values, n is the number of pixel pairs, and r xy is the pcc."
"where s(i) is the power spectrum of the image i and f is the fourier transform. this theorem states that the fourier transform of the autocorrelation of image i is equal to the inverse fourier transform of s(i). s(i) can be calculated by squaring the magnitude of the fourier transform of image i, which is equivalent to multiplying the fourier transform by its conjugate. therefore, the autocorrelation of image i is determined by:"
"where δ(a, b) is the autocorrelation function, i(x, y) is the image intensity at position (x, y), \" * \" represents the convolution, and a and b represent the distance from the corresponding x and y position. equation (13) is almost never used due the required high computational complexity. autocorrelation can be calculated efficiently via fast fourier transform by using the wiener-khinchin theorem described as follows:"
"utp campus lv network is not separated, dedicated to street lighting supplying only; it also supplies offices, classrooms, and labs. both gas-discharge and led light sources were used during the tests. this network consisted of 36 lamps."
"the differences between smart metering and smart lighting in topology and services are the main reasons, which cause a need to create new communication protocols dedicated just to last mile smart lighting communication networks. the development of communication protocols has always been dictated by the necessity to implement new services or by new possibilities given by better technology. a good example can be frame relay technology, which replaced x.25, when new, better (more reliable) links have been applied in telecommunications. these two main reasons resulted in the existence of so many \"made to measure\" communication protocols. with smart lighting the situation is similar; it needs to have its own dedicated communication protocol or set of protocols, which meet all the specific requirements. the specific requirements for smart lighting are inter alia (1) fast autoconfiguration of the network,"
"the presented classifying system based on cluster analysis using local properties and a self-organizing map enables the fast classification of a perimetric examination into a scotoma class based on the tübingen scotoma classification. we do not aim for an autonomous software package that is coupled with the perimetric hardware and automatically outputs the scotoma type after an examination, but consider our approach auxiliary in the daily examination routine. the algorithms are embedded in a java-based software framework that allows not only for classification of a given examination, but also for parameter adaptation and algorithm training, e.g. when using other examination hardware or grids with other properties. we evaluated the classifying system on over 8800 examinations and found a mean classification success of 78%, varying from 92% for examination data of quality 4 to 79% and 69% for data with quality 3 and 2 respectively. results achieved in other approaches report similar classification accuracy [cit], however none of these approaches was validated on such a considerable amount of examination data as in this paper."
"architecture analysis & design language (aadl), an sae international standard, is a unified framework providing extensive formal foundations for model-based engineering (mbe) practices. these practices extend throughout the system design, integration, and assurance with safety standards. aadl distinctly represents a system hardware and software components and their interactions via interfaces. critical real-time computational factors such as performance, dependability, safety, security, and data integrity can be rigorously analysed with aadl. aadl also integrates custom analyses and specification techniques during the engineering process. this allows in the development and analysis of a single, unified system architectural model. aadl can be extended using the specialized language constructs that can be attached to the components of the architectural model defined by aadl. these components are reinforced with additional characteristics and requirements, referred to as annex languages. the architectural model components are annotated with these properties and annex language clauses for functional and nonfunctional analyses. error model annex (emv), which is an extension of aadl, aids in describing the failure conditions and fault propagations as error events, propagations, occurrence, and their distribution properties. with the integration of these constructs in the aadl model/s, as shown in figure 1 [cit], the existing components are extended as current models liable for safety evaluation and analyses. this can be done with the help of the algorithms in osate or by using other third party tools. the analyses of the runtime architectures. the eannex [cit] annotates the hardware and the software component architectures with error states, error events, error transitions, and error propagations that may affect the component interacting with each other. in error model annex subclause conditions can be specified under which the errors are propagated through designated component ports. error model annex basically helps in defining the fault models, hazards, fault propagation, failure modes, and effects, as well as specifying compositional fault behaviour. aadl error model annex supports architectural fault modeling at three levels of abstraction [cit] ."
"this paper presents our contributions as a case study implementation (speed control unit of power-boat autopilot) to the standard approach for the illustration of its application. the paper is organized as follows. firstly, we summarize the concept of architecture analysis & design language (sae aadl) and error model annex (eannex/emv2). next we provide an illustration of the architecture fault model specification for speed control unit of a power-boat autopilot (pba). we also discuss the various safety analyses methods involved in mil-std882 safety practice. finally, we conclude the paper with the assessment of these safety analyses based on the architecture fault models."
"analyzed data were obtained as a result of many field-trials carried out in three locations: kamienna street in bydgoszcz city, nieszawska street in torun city, and utp campus. in addition to these locations, observations were also done on separate smart lighting lv network test-beds prepared for studies related to the gekon project."
"given a perimetric examination, our classification workflow first automatically checks whether it represents a normal visual field. if this is the case, then the classification process ends here. otherwise a number of subroutines are successively and automatically executed until all scotoma in the given examination are found. details used in these classification subroutines for each defect type are presented in the following."
"sector-oder wedge-shaped defects to detect this defect type the algorithm searches for all clusters that are not in one of the above defect classes and approximates straight lines to the side boundaries using linear regression. if the resulting shape matches to the characteristics of a sector, the algorithm classifies the defect cluster as a sector defect."
"(1) modeling of faults in systems and their implications on other dependent components of the physical environment of its operation through propagation of these faults (including hazard identification, fault impact analysis)"
"analysis of the data contained in the graph of figure 8 proves well-known rule, namely; the biggest probability of the next error occurring is on the next bit after the erroneous bit [cit] . the chart of the bit errors distribution has a decreasing trend."
"to describe the algorithm above in a brief way, we provide an intuitive process to locally optimize the moving path according to the empower hamilton loop optimization algorithm in fig. 3 ."
"(country) once and for all. subsequently, this problem further evolved into a traveling salesman problem (tsp), namely, the problem of empowering hamilton loop minimization. it not only guarantees that the constructed path is a hamilton loop, but also requires that the selection target of the path is the minimum total distance of all paths. the classic hamilton algorithm is mainly to find a loop that can traverse all the chs, so that ma can start from the bs and pass through all the chs that are required only once and finally return to the starting point. next, in order to reduce the total cost of ma traversing these chs, we need to optimize the loop."
"normal visual field: less than 7 relative defects anywhere in the visual field, not in the edges of the grid, no cluster or well-defined shape visible, physiological blind spot."
"the small impact of the frame length on the fer(snr) characteristic in gas-discharge smart lighting systems causes a lack of need of data portioning during transmission. in other words, shortening the length of frames does not improve communication performance."
"smart lighting low voltage (lv) network is a power network between a mv/lv transformer and loads in which all the loads are dimmable lamps. the only exceptions to this rule are traffic concentrators which, unlike the lamps, do not practically affect the impedance of the lv network. smart lighting systems may be supplied from general purpose lv networks or from smart lighting lv networks. supplying the lighting systems from general purpose lv network is getting rarer and concerns mainly rural areas and suburbs. smart lighting systems allow controlling every lamp individually, where basic control functions are brightening and dimming as well as turning-on and turning-off. to control every lamp individually (not the entire street as it was with older systems) a last mile communication network must be employed. this part of the smart grid communication networks has a decisive impact on the cost of implementation and operation of the whole communications system. thus very cheap shortrange devices (srds) have been used to deploy last mile wireless or plc (power line communications) networks. both the smart metering and the smart lighting, last mile communication systems based on plc or rf technology, create a specific kind of the distributed sensor networks."
"using the three links with shared modem variant the traffic concentrator works in the hub mode, whilst using three independent links' variant it works in the router mode. thus, sending information to any lamp supplied from 1, this information will also be received by some lamps supplied from 2 and 3, when variant (a) is used. there are of course other differences between smart metering and smart lighting last mile communications; a good example can be services. using figure 1 (a), it is easy to imagine the difference between the last mile smart metering realized in rf and plc technology. in the case of rf, connections between lamps are independent from power lines but they depend mainly on physical distances between nodes (lamps)."
"(2) modeling of faults occurring in a component of the system and analysing the behaviour of the same across various modes termed as failure modes and their effects on other components and their related propagations and being also inclusive of the recovery strategies involved (3) compositional abstraction of system error behaviour in terms of its subsystems error model annex (emv2) overlays major focus on the standards set of error types and error propagation, defined by aadl as a standard syntactic construct through the introduction of annex libraries. these annex libraries provide an overlook of the formally specified error propagation behaviours [cit] . some of the common error types are as follows [cit] . (2) timing errors. they represent arrival rate, service too early or late, and unsynchronized rate. along with these the error model types can be referenced in the error model annex subclause. the constructs for the emv2 are similar to the syntax and style as defined for aadl. an exception is that any set of textual language constructs can be included within an annex that includes object constraint language (ocl) [cit] or a temporal logic notation [cit] ."
"after the election process of chs, ordinary nodes join the appropriate cluster once receiving the broadcast information from ch. in the algorithm, we proposed three (1)"
"in this section we exhibit the architecture fault modeling in aadl, along with the extension of emv2, at three levels of abstraction with a suitable case study, speed control unit of power-boat autopilot (pba). this unit is a simplified speed control model, including a pilot interface unit for input of relevant power-boat autopilot information, a speed sensor that sends speed data to the pba, the pba controller, a throttle actuator that responds to the commands specified by the pba controller, and a display unit. the type definitions defining the component, component names, their runtime category, and interfaces are identified and defined. the speed sensor, pilot interface, throttle actuator, and the display unit are modeled as devices, while the pba control functions are represented as process, as shown in figure 2 . with all these we perform the safety analyses with the specification of the source of error and its propagation across the system and its components. this is carried out by defining the error states and their corresponding compositional fault behaviour. this is followed by the expansion of the fault logic with respect to its error behaviour related to each component of the system and its response to the failures."
"using data presented in figure 5 the ratio of led to gas is about 2.32, which means that the length of the frame has bigger influence on transmission reliability over the lv network which supplies led lamps. by analyzing the data in figure 5, it can also be concluded that the transmission in an led light source environment is more reliable; it is a wrong conclusion as evidenced by results of the experiment, which are presented in figure 6 ."
the structure of this paper is organized as follows: in section 2 we describe the algorithm of the classifying system and the data used for validation. the validation results are presented and discussed in section 3. section 4 concludes this paper.
the above relationship was found for all the tests carried out. as an example in figure 5 the set of four fer(snr) curves is presented.
"international journal of distributed sensor networks figure 7 : frequency spectrum of the noise signal, measured in the range of 80 khz to 100 khz: (a) before applying the 1 mh filter, (b) after applying the 1 mh filter. knowing the fact that in the frequency of ten khz the impedance of the gas-discharge lv network is higher than the impedance of the led lv network and also that small differences in fer(snr) versus frames length have no relation with the level of noises in the transmission band, the distribution of the bit errors was studied. the results of these studies are presented in the next section."
"(2) the ability to work across all cenelec bands, (3) in space and time multichannel working, (4) real-time services supporting, (5) data transfer asymmetry (more data is downloaded)."
"with decreasing certainty in the subjective, examinerrelated classification quality (from examinations with quality 4 to examinations with quality 2) we find a decreasing performance of our algorithm (see figure 5 ). while the overall classification accuracy for examinations with manual classification certainty 4 is 92%, it decreases for the quality classes 3 and 2 to 79% and 69% respectively. the main reason for this is the high variability in the manifestation of scotoma types that leads to unclear assignments of an examination to a scotoma class by the physician or our algorithm. examples for such difficult classifications for the rnfl defects were previously shown in figure 4 (a) and 4(b) . other examples are presented in figure 6 (a) and figure 6 (b). furthermore during manual classification, physicians had to commit themselves to only one scotoma class (i.e. to the most relevant for the patient), although in many examinations we found more than one scotoma. this effect influences the classification results negatively."
"step 1: determine the starting point of the cycle with bs. a hamilton loop is obtained for each ch near bs using the classical algorithm; step 2: cut across the line (in the formed hamilton loop, the current ch is regarded as the starting point of cutting, and the intermediate node is isolated, and the other nodes are connected among the three chs connected to each other), generate an isolated ch; step 3: the isolated ch node is reconnected to the loop to form a new hamilton loop according to the principle of path energy consumption minimization, the minimum of energy consumption is measured by the weight between the cluster head nodes in formula (9); step 4: if the total weight of the path changes, replace the old loop with a new loop, and take the current ch as the new starting point of the cycle, put back to step 2. otherwise, move the starting point to the next ch, and enter step 5; step 5: determine whether the loop is completed, that is, whether the current cluster head node is a base station; if so, the algorithm terminates. otherwise, go to step 2."
"the waveform in figure 3 contains five frames; two of them were transmitted by concentrator and three were received by it. the first frame (from the left) is the 33-byte command frame sent by traffic concentrator using bpsk (binary phase-shift keying) coded modulation. the second frame is the 55-byte response frame sent by the queried lamp also using bpsk coded modulation. it is easy to notice that the level of the signal of the response frame is lower than the level of the signal of the command frame. the third frame is the 23-byte ack/cancel sent by traffic concentrator with the same signal level and the same kind of modulation that command frame was sent. the fourth and fifth frames are also the 23-byte ack/cancel frames copied and sent by two far lamps (not queried at the moment); their signals are weaker because they are far from the concentrator; also their duration is shorter because they were sent using qpsk (quadrature psk) modulation, which allows transmitting data twice faster than bpsk modulation. above example shows us differences in the shapes of the frames as a result of the transmission in the noisy environment, attenuation, and the length of the frame."
"(i) consistency checks. the consistency checks at the system integration level scan for the consistency in their functionality and the interfaces between various models/components, as shown in \"consistency report\" section. this thereby strengthens the virtual integration and analysis of the architecture model of the system. the consistency of various models deals with their integration feasibility while the consistency of the internal components in a model concentrates on the propagation capabilities, redundancies, and so on. with error model annex the concept of consistency across the error models as specified checks for the consistency with respect to the component error behaviour along with the composite error behaviour of the system. it helps in defining the correctness of the error state as per the components specified in the architectural model. this may be proven with the substantial inclusion of behaviour annexes (bannex) [cit] along with the error model annex. the consistency report generated by the osate plugin for the case study is as follows. (ii) fault tree analysis (fta). it is a widely used safety and reliability analysis [cit] feature in aerospace, medical electronics, and industrial automation industries [cit] . in this analysis the major focus is on the top-level event (minimal cut-set), from a set of combinations of basic events (faults). it provides a hierarchical representation of the errors of the system (top-level event) from the basic events, related to components as specified in component error behaviour, in the form of a tree. osate depicts this composite error behaviour of the system from the underlying component error behaviours as a fault tree that represents specific error state of the system. this is achieved in the form of two files from osate for the representation of the fault tree, one being the database of primary events (.ped), as shown in figure 4, causing the top-level error event, and the fault tree analysis file (.fta). these files are viewed using openfta, as shown in figure 3 . the fta analysis is in conformance with mil-std882 standard and the generated fault tree is validated, as shown in figure 5 ."
"(ii) specification of error source and propagation. the source of errors and their propagation with respect to each component of a system in pba speed control unit is defined, as shown in box 1. in the case study on flow src related to the device, pilot interface unit is sourcing the fault. the component error propagations are also defined with the error novalue & noservice."
sending copies of the same packet by a few intermediate nodes significantly improves the reliability of transmission and also allows using faster though less reliable kind of modulation.
"in this paper, we use non-uniform clustering method to divide the whole sensing area. first, each node will generate a random number rand i . we optimized the previous threshold setting, taking the residual energy of the node into account, and redefined the threshold t (n) as shown in formula (7)."
our approach aims at the reliable recognition of all eight defect types in terms of the tübingen scotoma classification. furthermore we design an algorithm with a parametric interface such that the algorithm parameters can easily be adapted to various test point arrangements (grids). the algorithm is based on inferring shape from the perimetric test locations by clustering and cluster shape analysis. clustering in case of retinal nerve fibre layer defects is based on a self-organizing map. the advantage of our approach is the consideration of different defect depths and integration of expert knowledge into the classifying methods. the developed classification was validated on 8868 anonymized visual field examinations from eight scotoma classes that were carried out in the centre for ophthalmology at the university hospital tübingen during the last years. with these examinations we achieved a mean classification success of 78%.
"the parameter fitting for each subroutine was done iteratively during training by starting with initial values and stepwise parameter adaptation. thereby we considered the values of true-positives, true-negatives, false-positive and false-negatives for each scotoma class."
"in 1857, hamilton, a british mathematician, proposed the famous hamilton circuit problem, which arose when the mathematician wanted to design a plan to travel around the world. hamilton wanted to travel to all the countries, and he wanted to find a suitable path, starting from the beginning, traveling to all the countries in a certain order, as long as he could not go to the same country repeatedly and finally back to the starting point. from a mathematical point of view, a loop is designed to pass through each vertex (6)"
"in the above example, 6 frames have been received by the traffic concentrator. this example was very simple, just to explain multihop and multipath techniques and also to explain how data are collected by the traffic concentrator. field-trials showed that even more than 10 frames may be received after sending one command by the traffic concentrator. during the one information exchanging action, frames received by traffic concentrator may usually have three lengths. sometimes they may have two lengths or the same lengths, but this is rather an untypical situation. these situations can happen when command and response frames have the same length or when command and response frames do not carry any packet payload; in this case all the received frames have the shortest possible lengths. the shortest frames in presented solution have the length of 23 bytes and consist of (i) four-byte preamble, for the frames longer than 23 bytes the structure is the same except the fact that frame payload is longer than 14 bytes because it contains also the packet payload. the ack/cancel frames are always the shortest ones. the typical length of the response frames is 24 bytes but the length of these kinds of frames can also be a quite long, for example, when registers of the lamp are readout in burst mode. there is no typical length of the command frames; they may have length from 23 bytes up to 250 bytes."
"further steps of our work include (1) the refinement of the classification routines for the distinction of the five stages of the retinal nerve fibre layer defects (figure 2 ), (2) the further improvement of the clustering to achieve more reliable classification, (3) further evaluations of our algorithms on different perimetric devices and examination grid types and (4) the development of a user-friendly interface for parameter adaptation."
"in the recent, many researchers take more efforts into using network energy properly to improve network performance [cit] . the routing protocols are classified into two categories to address the energy consumption problem, as the aforementioned techniques, flat routing protocols apply only to smaller networks [cit], therefore, we adopt hierarchical routing protocol with respect to the clustering process."
"author of the paper as the person responsible for ensuring the last mile communication in the smart metering system carried out series of tests. the smart metering system is currently developed by university of science and technology in bydgoszcz (utp), poland, and orion electric poland under the gekon project supported by the national centre for research and development and also by the national fund for environmental protection and water management. in this paper fer(snr) obtained for different frame length are presented. the study included lamps equipped with led or gas-discharge light sources. the differences in the characteristics of fer(snr) for different light sources have led to a detailed examination of the error nature of narrowband plc transmission in smart lighting lv network. the outcomes of this research are presented in this work."
"correction lens rim or holder may lead to defects mostly at high eccentricity. figure 3 . potential examination artifacts fication of automated perimetry examinations is based on machine learning, e.g. quadratic discriminant analysis and support vector machines [cit], hopfield-attractor network [cit], feed-forward, back-propagation networks or kohonen self-organizing feature maps (som) for pattern recognition or visual field progression in patients with glaucoma [cit] . however these approaches are either designed for a very specific perimetric device and do not foresee adaptation for other grid arrangements or consider only a small number of scotoma classes."
"systematic analyses of the architectural models modeled using the model-based engineering (mbe) [cit] practices, early and at every abstraction level, imbibe a greater confidence in the integration of the system. the creation and analysis of architectural models of a system support prediction and understanding of the system's capabilities and its operational quality attributes. these attributes include performance, reliability, reusability, safety, and security. all along the developmental lifecycle, the faults such as their failure modes and their propagation effects, at system-level, can be predicted. such issues remain unnoticed until system integration and testing. this proves to be a costly rework resulting in an unaccounted project time, cost, and maintenance."
"one-section nieszawska street lv network has a length of 3 km with two traffic concentrators located in the middle. the lv network on nieszawska street contains 108 lamps; there is one lamp on every electric pole. the old gas-discharge light sources have been used during the three-month period of testing. they have been subsequently changed to led. this network was used for functional testing, including traffic concentrators protection on/off switching."
"to develop a new communication protocol, knowledge of the behavior of lower layers is very useful, because it allows optimizing the parameters of the protocol. the physical layer is the lowest layer. the behavior of the plc physical links is generally described by the primary and secondary parameters of the power line [cit] as well as by noise models [cit] . the possibilities of the physical data transmission interfaces are described by fer (frame error rate) versus snr (signal to noise ratio) or per (packet error rate) versus snr."
"presented in this work, results can be helpful in the realization of any of the above processes as well as after replacing old overhead power lines with new cable ones."
"in simple terms, this problem can be handled by the enumeration method, but the calculation amount of the method is too large, reaching the number of n-1 factorial, so the enumeration method is not feasible for the complex network environment. later, heuristic algorithm is proposed to calculate the approximate solution of the problem through the features of some optimal solutions or the features that should not be present, but the solution obtained by this method is not the optimal solution of the problem. we put forward an idea of local optimization to optimize the known hamilton loop."
"one of the differences between smart metering and smart lighting last mile networks is their topologies. smart metering networks in most cases create a mesh topology whilst smart lighting networks a link topology. using the plc technique in three-phase network, as in most cases, there are two variants of the topology of the smart metering communication network, namely, (a) three links with shared modem topology, (b) topology with three independent links."
others: visual field defects that cannot be attributed to any of the above mentioned classes. the reliable classification of visual field defect types is very important for the adequate diagnosis of the underlying disease. the classification process implies expert know-how and long-term experience due to the high variability in the manifestation of a disease in the perimetric examination result. the computer-based automatic classification of visual field defect types from perimetric examinations is therefore a challenge and has been examined in several approaches. in most of the related work the classieyelid or eyebrow (e.g. when tilting head forward) may cause difficulties within the upper visual field.
clustering as presented in the previous defect categories would not lead to satisfactory results. to cluster the defects we used a self-organizing map [cit] . the selforganizing map (som) is an established concept in unsupervised learning for visualization and interpretation of high-dimension data by projecting it onto a m-dimensional map such that data points close to each other in the map are close to each other in the original data spaces [cit] . in our application we use a 2-dimensional mapping. every neuron i of a som is associated with an n-dimensional reference vector. there are several methods to visualize the data clusters obtained using a som. the most commonly used method is the unified distance matrix (u-matrix) [cit] that visualizes the distances between the neurons.
"in fig. 5, the network lifetime under different algorithms is analyzed. compared with other algorithms, the proposed algorithm greatly extends the life cycle of the network, which is mainly reflected in the longer stable stage of the whole network and the steeper slope of node death. it is apparent that our proposed algorithm did not appear the node table 1 death phenomenon until the number of rounds is close to 1000, while all the nodes of other algorithms have almost died during this time period, and leach-ga algorithm has crashed when it run to 724th rounds. based on the idea of empowering hamilton loop to collect data packets, an optimal agent traverse route is planned. it not only extends the longevity of the node, but also achieves the effect of balanced energy consumption and alleviates the problem of energy hole. as is clearly shown in fig. 6, in comparison with the sum of energy consumption among the other three algorithms, it is obvious that our proposed scheme is superior to other algorithms in terms of the stability period and the network lifetime. the gentler the slope is, the less energy will be consumed per round, indicating that our algorithm is more energy efficient compared with other algorithms. our proposed algorithm will consume 10.5 joules of energy per 100 rounds approximately, while the energy consumption of the other three algorithms are 11.3, 12.2 and 14.2 respectively. as can be seen from the chart, when the number of rounds reached 1200, the total energy of the network was exhausted, while other algorithms have a network life of less than 1000 rounds. in short, the less energy consumed, the longer the lifetime of the entire network."
"safety analyses involve various analytical processes such as consistency checks, fault tree analysis (fta), failure modes and effect analysis (fmea), functional hazard assessment (fha), and common mode assessment (cma) of the architectural model. the architecture model and its associated fault model are designed and developed in open source aadl tool environment (osate) [cit] . it is an eclipse based aadl modeling framework. there is also need to the safety analysis tool such as openfta [cit] . an open source tool for fta is integrated into eclipse environment, to assist in generation of fta and its relevant documents, while cma, fmea, and fha reports are generated as a built-in feature from osate."
"the presented algorithm was evaluated on all examination data of quality 4, 3 and 2 as shown in table 1 . figure 5 presents the mean accuracy (acc) and specificity (sp) for each quality class. accuracy and specificity values for each scotoma class and data quality are presented in table 2 . table 2 . classification results for all examination data of quality 2, 3 and 4"
"hemianopic to detect this defect the algorithm compares the defect depths between the left and right hemifield. is there a significant difference, this is an indicator for a scotoma respecting the vertical meridian."
"we assume that the system fails if either of the devices, that is, throttle actuator or the display unit, behaves in the failed state, while it tends to recover from the failed state and remains to be operational even if the display unit fails, as the speed control unit mainly depends on the throttle command in maintaining and controlling the speed of the pba."
"network average transmission delay is another important standard to evaluate network performance. the average delay time between the beginning of propagating a data and its arrival at the other node required to receive the data [cit] . where, the calculation formula of the average delay is obtained from formulas (10)- (12) . from the simulation performance of the experiment in fig. 7, the leach-ga algorithm and the proposed algorithm proposed are relatively low in delay, and the delay difference value between our algorithm and cbrp algorithm is 80 ms and 15 ms with respect of time delay, apparently, our algorithm is an ideal compromise algorithm, it combines the advantages of leach-ga and cbrp algorithm for alleviating the energy consumption and reducing the transmission delay respectively."
"in this work no problem was solved. this paper describes the characteristic phenomenon for the narrowband plc transmission in smart lighting lv networks, which supply gas-discharge lamps. to highlight this phenomenon all the presented transmission parameters and characteristics were compared with the same obtained from led smart lighting systems. most of the experiments were carried out in the same physical lv network, which supplied different types of lamps, making the results of experiments more credible and easier to analyze. the test results presented in this paper may be particularly useful in the plc communication protocols designing as well as in their parameterization. knowing bit errors distribution characteristics of not coded types of modulations can be very beneficial when selecting the method of forward error correction [cit] or if the implementation of the peak noise avoidance algorithms makes sense."
the artifacts related to fta as specified by mil-std882 deal with error composites and error events. fta is a topdown approach of analysis. the minimal cut set is evaluated in the openfta tool and is as shown in figure 6 .
"specific weight calculation formula is satisfied (9), where α is coordination coefficient, d(i, j) represents the distance between ch i and j, number(i) is denoted as the packet size of ch i."
"in this paper, empower hamilton loop based data collection algorithm by mobile agent is proposed in uneven clustering for wsn. with the aim of alleviating the problem of energy hole, we adopt a non-uniform clustering method, the method mitigate transmission load of the nodes in vicinity of the base station. furthermore, to further balance and decrease the resource expenditure of the entire network, we combined the pegasis algorithm and the hamilton loop algorithm, adopts a mixture of single-hop and multiple hops mechanisms, and includes ma on the optimal hamilton moving loop, ma is responsible for aggregating and fusing data packets from the chs on the loop. network performance analysis results show that the proposed routing algorithm can effectively prolong network life cycle, equalize energy consumption and reduce network delay. although leach-ga algorithm delay was slightly lower than our algorithm, but it will consume the most energy consumption of each round, so its life cycle is short. the energy saving effect of our algorithm is better than cbrp and eeuc algorithm, meanwhile, the efficiency of two algorithms are not ideal in terms of transmission latency. therefore, the proposed algorithm is more reliable and effective compared to other protocols. in the future, we intend to adjust the number of ma and change the value of the benchmark competition radius to specifically analyze the influence of these variables on network performance."
"analysis of the failure modes associated with the system and the determination of its effects over the hierarchical evolution, performed systematically with a bottom-up approach, is fmea. with respect to the errors of the system, fmea provides the information about the deficient component/models and their related effects. it also provides sufficient overview of the failing component such as its phase of failure, severity/impact, and so on. fmea is based on the artifacts that include error propagation paths (error source, error path, and the error sink). fha provides the possible list of error upon the synthesis of the architectural model of the system. the major artifacts from fha comprise the source of the error and the error events, as shown in table 1 . the details of fha are processed from the osate tool after the model is instantiated and the relevant error information is suitably extracted from these architecture models. the report will be in the form of an excel spreadsheet with the specification of the error event details."
the results of the august and september tests also confirmed the performance observations noted on the lab test-beds. using them it can be concluded that (1) the type of the lamps (gas-discharge or led) in smart lighting lv network does not determine the performance of the transmission in narrowband plc technology;
"these safety practices include various availability and reliability prognosis with the help of system architectural models. model-based engineering approaches for safety analyses address these issues and prove to provide consolidated information about the informal requirements and the architecture model of the system. the safety analyses performed on a system also take into consideration the physical environment of its deployment and functioning. due to insufficient support of the formal languages trend is to make use of architecture description languages such as architecture analysis & design language (aadl) and society of automotive engineers (sae) standard. aadl, a high-level architectural descriptive language, basically provides a platform for overall 2 international journal of aerospace engineering integration of various system recommended components via formal semantics and syntax. this component-based modeling language is extended with the introduction of sublanguages as annexes. aadl is packaged with multiple annex sublanguages such as error model annex (eannex) and behaviour annex (bannex) as standards. the eannex standard is suitably augmented with safety semantics and ontology of fault propagation, supporting error annotations on the architectural models [cit] . this thus enables the component error models and their interactions to be considered in context to the system architecture modeled using aadl."
"all data for the analysis were collected in traffic concentrators. analyzed data were erroneous or error-free frames received by the traffic concentrator with the different values of snr. when 3-phase lv network was under test, the traffic concentrator presented in figure 1 (a) was employed, whilst testing 1-phase lv network we employed traffic concentrator presented in figure 1 (b), using only one port. the snr range of the frame reception was from 0 db to 34 db. both traffic concentrators and lamps were equipped with our own construction modems based on stmicroelectronics st7580 chip. to ensure the long range transmission the multihop [cit] communication protocol has been designed and to ensure adequate data transfer reliability the multipath technique [cit] was also implemented. an exemplary data flowchart for one traffic concentrator-lamp action is presented in figure 2 . in this diagram, the differences in time of the same frame reception by lamps are not shown, because the differences of the propagation times are negligible compared to the time of the frame transmission."
"the visual field represents the area that can be perceived when the eye is directed forward. as diseases affecting the visual system result in visual field defects, the systematic measurement and documentation of the visual field, or perimetry, is an important diagnostic test. perimetry consists of measuring the sensitivity mostly in terms of differential luminance sensitivity (dls), of visual perception as a function of location within the visual field [cit] . the most common perimetry types are the kinetic perimetry and static perimetry. in both types of perimetry test objects, most commonly light stimuli, are projected onto a uniform background [cit] . in manual kinetic perimetry, a perimetrist moves a stimulus of constant luminance almost perpendicularly toward the assumed visual field border, coming either from outside of the supposed visual field or from inside of a supposed visual field defect. the position at which the presented stimulus is detected at first marks the boundary of the visual field. static perimetry is mainly performed by computer assistance. during a static perimetry examination the size and location of a collection grid of stimuli is kept constant while their luminances varies until the dimmest stimulus that can be perceived by the subject at each stimulus location is identified. usually subjects respond by pressing a response button to indicate that they detected the stimulus. the location and pattern of missed stimuli defines the type of visual field defect (vfd). areas of impaired stimulus perception or scotoma can be relative or absolute. absolute scotoma are characterized by severely reduced luminance sensitivity down to no light perception (e.g. the blind spot), within a relative scotoma however luminance sensitivity is reduced but not completely absent."
retinal nerve fibre layer (rnfl) defects (or arcuate scotoma) occur in five stages [cit] from relative visual field defects (left) to massive absolute defects (right).
"regardless of which type of charts in figure 4 is analyzed, it is clearly seen, from the curves, a well-known principle international journal of distributed sensor networks that bpsk modulation is more reliable but the cost of its robustness is the low bit rate; it is twice lower than the bit rate offered by qpsk. the reason for using two types of modulation is to increase reliability in this communication system, which is based srds. the most sensitive moment of exchanging data, using multihop technique, is the moment of the original packet sending, rather than sending a copy of it. the original packet can only be sent by one node while its copy can be sent usually by many intermediate nodes."
"for safety-critical advanced complex embedded systems, the system design and development are in compliance with the safety standards and engineered with practices as specified by mil-std882 [cit], sae arp-4761 [cit], and do-178b/c [cit] . the process of development, management, and controlling these systems in conformance with the safety practices proves to have an impact on the system requirements, postsystem integration, and test. with the evolution of the system, availability and reliability of these models are to be consistent and this poses a great challenge."
"many tests were done to answer two questions, namely: which kind of modulation is optimal and what is the maximum length of the frame for the narrowband plc transmission over the smart lighting lv network? for the short frames, practically, there was no difference whether analyzed data were collected from the smart lighting lv network supplied led or gas-discharge lamps. of course there was also no difference in fer(snr) curves when different types of lamps were used to illuminate the road."
"for comparison, in figure 9, the bit errors distribution versus bit distance obtained from observations in the lv network which supplied gas-discharge lamps is presented."
"for the bit errors distribution assessment, all the frames that had more than one error were examined. if there are bit errors in the frame, it is possible to determine − 1 distances."
central scotoma: absolute or relative defects primary within 5 eccentricity without respecting the vertical or the horizontal meridian. other types or defects within the central visual field are the paracentral scotoma (normal blind spot position) and the centrocecal scotoma (extending from the blind spot towards or into the fixation area symmetrically above an below the midline.)
"the algorithm first searches for defect clusters in the area of the blind spot. the cluster found is compared to the normal physiological blind spot cluster. when both clusters do not match in their centroids this indicates a shifted blind spot. comparison of areas reveals where there is an enlargement or shrinkage. the parameters used here, e.g. the tolerance areas for non matching blind spot areas were trained using the examination data."
"(1) installing the lamps already equipped with communication modules, (2) adding communication modules to already installed \"smart ready\" lamps, which were originally equipped with dali or 0-10 v lighting control interface [cit], (3) adjusting existing quite new gas-discharge lamps to be individually controlled."
"automated static perimetry is frequently performed using cupola perimeters (e.g. octopus 900, figure 1(a) ). the most important examination thereby is that of the central 30"
"where is the length of the frame, ef snr, is the value of the erroneous frame counter of the two-dimensional array element indexed by snr and, and eff snr, is the value of the error-free frame counter of the same element in which the eff snr, value is stored. the fer value does not only depend on snr and the length of the frame. it also depends on frame modulation and also if plc modem was configured to work in single or dual channel mode. using the fact that presented smart lighting communication system was designed to work in single channel mode only results in this condition are presented. all the types of psk frame modulation, supported by st 7580, were tested but to understand the errors nature of the narrowband plc transmission in smart lighting lv network only not coded kinds of modulation are considered in this paper; this modulation is bpsk, qpsk, and 8psk (8-phase psk) with their bit rates 9600 bits/ [cit] 0 bits/s, and 28800 bits/s, respectively. in figure 4, as an example, two charts are showed; they presented the same data but in different form: chart (a) has logarithmic axis of ordinates, which is more commonly used, whilst chart (b) has linear, which will be used in this paper only when it is necessary."
"the test data consisted of 8868 perimetric examinations with a non-uniform distribution over eight scotoma classes. the number of test data in each scotoma class and classification quality (i.e. physician's certainty) is presented in table 1 . the class central scotoma includes also the defect types paracentral and centrocecal. as the manifestation of the rnfl defect type is of high variability, in this first step of our work we do not distinguish between the several stages. the five different stages of rnfl are thus summarized in one defect class."
"the decision whether a perimetric examination presents a normal visual field is done by considering the number of sporadic relative or absolute defects (failed test locations) that can not be grouped to a cluster as they do not fulfill the distance measures. training resulted in an optimal number of 10 relative or absolute defects. this number includes defects within the blind spot area and sporadic defects, e.g. due to patients inattention."
"in this paper, we have proposed a novel approach of safety analyses of safety-critical systems using aadl and the related error model annexes. in spite of the comprehensive activities involved in safety analyses, the needs for such approaches are proved to be very much necessary. this is achieved and projected with the implementation of a suitable case study, speed control unit of power-boat autopilot. the employment of analysis techniques such as fault tree analysis (fta), functional hazard analysis (fha), and consistency of the model along with the conduction of qualitative and quantitative reliability analyses as part of these techniques can assess the system hazards and faults. the assessment covers the generation of suitable reports justifying the analyses. these methodologies or techniques provide grant for early identification and probability of the occurrence of potential problems. this also provides a perspective to explore additional architectural properties. reuse and analysis of the evolved models, provided with suitable extensions with limited effort, can be achieved with this approach. the overall effect induces a greater confidence over abstracted stages of development and safety analyses of these architectural models of the system. also analysing the system based on the safety-critical requirements, with the expectation of exceptional conditions, hazards are expedited in the development of safety system architecture models which will have an impact in certifying the same. this also avoids the unnecessary certification costs by understanding the change impact or the exceptional causes impacts during system engineering."
"when all the lamps are in the direct communication range of the traffic concentrator, no command frame can be received by it. this situation is presented in figure 3, which shows the oscillogram with 5 different frame signals recorded from 1-phase lv power line during the one querying process. the oscillogram was taken off close to the traffic concentrator."
"every received line frame is extended by plc modem with the information about its level and snr and sent to the microcontroller. after a frame reception, the traffic concentrator updates its two-dimensional array. every element of this array has two records: erroneous frames counter and error-free frames counter. one frame reception causes updating only the one counter of only one element of the array. this element is indexed by the snr value and the length value. the snr values are natural numbers or zero. for the given snr and length, the fer value is calculated by dividing the value of the erroneous frames counter and the sum of the value of the erroneous frames counter and the value of the error-free frame counter. this is formalized by"
"factors for selecting cluster, namely, the remaining energy of chs, the distance from current ordinary node to bs and the distance from the node to other chs. finally, the fitness function f (i, j) of ordinary node i added to ch j is proposed, as shown in formula (6) ."
"we present the simple radio energy consumption model in fig. 2 [cit] . firstly, the transmitter sends k bit packet, which can be divided into signal generation stage and signal amplifier stage. the energy consumption in the signal amplifier phase is determined by the size of the packet, while the energy consumption in the signal amplifier phase is determined by the size of the packet and the distance between the receiver and the transmitter. secondly, the receiver is responsible for gathering the packet and it also produces energy expenditure. the specific energy consumption formula can be seen in the following formula (1). this part mainly discusses the energy model of the network. the battery of sensor devices is a fundamental constraint with respect to the energy dissipation of the network. we regard the initial energy of sensor nodes as e 0, and these nodes are not rechargeable. network energy consumption mainly refers to the forwarding and receiving process of data packets, and the specific calculation formulas are respectively stated (1) and (2) [cit] . transmission energy is expressed as e tx (k, d), where coefficient k is the number of bits that forward signal packet, and d is the communication distance between the forwarding node and the receiving node."
"frame has bigger influence on transmission reliability over the lv network which supplies led lamps. test number 3 [cit] in the same conditions (lv network and led lamps); the only difference was that the 1 mh coil was added at the input of the led lamp power supplies. it was done to increase the input impedance of the lamp (at plc transmission band) and to separate the high frequency noise generated by the lamp from the transmission media (lv network). the results of this procedure are shown in figures 7(a) and 7(b). about 15 db noise reduction together with the increase of input impedance had significant, positive influence on fer(snr) characteristics (comparing these in figures 5 and 6) . it also proved that there is a quite big difference in fer(snr) characteristics depending on length of frames when they are transmitted over the lv network, which supplied led lamps."
"firstly, each node in the network will randomly generate a number between 0 and 1. if the random number of sensor node is less than the preset threshold value t, the node is elected as the candidate ch. secondly, the competitive radius of the candidate ch is set. different from the previous algorithm, the competitive radius of the eeuc algorithm is improved. by combining the euclidean distance and residual energy between the nodes, the competitive radius of the node is more reasonably planned, as shown in formulas (3)- (6) ."
"this provides a scope for redundancy management for fault management capability of the system as well as seek for extensive solutions for reliability and availability analyses through various hierarchical levels of the system architecture. this methodology is not advisable for markov chains as the systems tends to grow quickly with their dependencies among various components within a system, as the number of components increases."
"our algorithm is based on cluster analysis. clustering or unsupervised learning divides data points into groups or clusters [cit] and is considered in many application fields, e.g. computer vision, statistical data analysis, pattern recognition or image analysis. our algorithm consists of four steps: (1) aggregation of test locations in a perimetric examination result to clusters, (2) computation of the convex hull for each cluster, (3) cluster shape analysis and finally (4) detection of scotoma type. clustering is done using the agglomerative hierarchical clustering approach [cit] by starting with each point as a singleton cluster and then iteratively merging the two closest clusters until a single, all-encompassing cluster remains [cit] . as distance measure we use both the euclidean distance between two test locations as well as the def ect depth difference. due to the increasing euclidean distance between two test locations with eccentricity, we adapt the distance measure towards the visual field periphery."
"tests carried out on the lab test-beds were done with the different types and numbers of lamps. these tests were done for the results verification and also when we wanted to have condition impossible to obtain in the field, for example, transmission over the overhead power lines."
"due to the minor hardware and software resources requirements and the decoupling from the perimetric hardware the framework can be run on any common computer. it can not only be used in local diagnostic processes but due to its import/export interfaces also in tele-medicine, e.g. when further expertise is needed. furthermore, as our algorithm performs cluster shape analysis, shape information can be used beyond classification for scotoma progression diagnosis."
"the longest power line section of kamienna street lv network is 1.5 km long, with the traffic concentrator located at 500 m from one of the ends of the section. this section contains 100 lamps (terminal nodes) installed on 50 poles. the old gas-discharge light sources are used on this street. this network was used for testing the plc transmission, that is, modulation types, methods of data correction, and of course transmission coverage."
"in the part of experiment, we have simulated network environment in matlab on microsoft windows 7 professional platform to analyze the performance of our proposal. for convenience of comparison, leach-ga algorithm, cbrp algorithm and eeuc algorithm were used for comparison [cit] . using the energy consumption model mentioned in this article, the measurement results of each algorithm for the energy consumption from nodes, the packet forward rate of packets and delay are presented. the values of relevant experimental parameters are shown in table 1 ."
"in fig. 1, we set the sensing area as square, the cluster size is not uniform, the base station is mixed in the central point of the sensing region and remains stationary all the time, and ma moves in the sensing area according to the predetermined path. in the below figure, the dotted black line with arrows represents the moving trajectory of ma, and data packets transmission among the clusters can be described as red lines."
"(iv) component error behaviour. the modeler will have the flexibility of analysing the possible error behaviour that may correspond to individual components of a system. this also provides an insight into the component internal failures and the divergent factors that may result in failure mode, in turn having an impact on other components. the case study in this paper specifies that there might be multiple failure modes like failure and failed. in failed mode the entire component is assumed to be redundant while in the failure the component is working but having erroneous outputs/output states, as shown in box 3. the failure modes are represented using the error states with more likely coupled error behaviour of the subsystem/component. the consistency checker associated with the error model annex abstracts the propagation specification to introduce unique and distinctive error types. while the modeling tool associated with the error model annex validates the organization of the component error behaviour along with the propagation specification specific to each of the components in the system architecture, the actual system architecture must include the safety system component/s that regulates the fault management and aids in safety analyses."
"then, bs broadcasts clustering information to the entire sensing area. after receiving the broadcast, the candidate ch determines its competitive radius by using formula (3)-(5) according to the distance from the node to bs and the residual energy of the node. if the distance among the adjacent chs is less than their competitive radius, the node with more energy is selected as the cluster head node. once entering the clustering stage, the common node selects a suitable cluster according to the message broadcast by each ch. taking into account the distance between nodes to bs, the distance between nodes and the residual energy of nodes, the fitness function (6) of each ordinary node is obtained. a node is declared a member node of the cluster when the maximum fitness value between the node and a ch."
"retinal nerve fibre layer (rnfl) defects represent the most complex defect type for classification. the examinations show a high variability and, in contrast to the rnfl defect definition in figure 2, unclear shape characteristics. e.g. figure 4 (a) has been classified by a physician as a rnfl defect at stage ii although there are many intact test location within the marked defect clusters. characteristically the loss of retinal ganglion cells is here not identical for all patients. although typical clusters are not recognizable, the examination in figure 4 (b) has been classified by a physician as rnfl defect stage i (compare to the schematic depiction of this defect type by the tübingen scotoma classification in figure 2 )."
"furthermore, in fig. 4a -e are used to represent chs in different clusters respectively, meanwhile number represents the packets size of each ch calculated, d is denoted as the distance between two chs. the weight relationship between chs is presented by the distance between them and the size of their own packets, in the following work, we can find an optimal ms movement path through the weight relationship between them."
"migration is beneficial even if t1-t3 do not follow identical paths, as segment d illustrates. since t1 did not touch segment d, t2 will suffer due to the corresponding misses while executing on core-3. if t3 follows suit on core-3, it will not suffer any instruction misses for segment d."
"the lynx server and client library consist of ≈5000 lines of c++ code, plus 3500 lines for a custom rpc library. programmers specify user chains using lynx's javascript api; a lynx utility reads the application table schemas and generates javascript objects that programmers use to read and update each table. when executing a user chain, the coordinator transfers the javascript code of each hop to the appropriate server, which then caches and executes the code using the v8 javascript engine."
"the join table may also have other index tables derived from it. in this case, lynx spawns parallel sub-chains that start from the updated join table shard and update those index tables."
"alternatively, slicc-pp uses a hardware preprocessing phase to assign types to threads as they launch. slicc-pp exploits the observation that in oltp the first few instructions executed are the same for same-type threads, while they differ across different-type threads. slicc-pp only needs to know when a new thread is launched. a middle-ware layer assigns threads in groups to a core devoted for this purpose (scout core). there, each thread executes a few tens of instructions, while the instruction addresses are hashed. the resulting values are used as thread type identifiers. experiments show that slicc-pp is 100% accurate when executing a small number of instructions. slicc-pp dedicates one core for pre-processing. 4.3.2. type-aware migration using thread type information, slicc groups similar threads into teams. creating teams is useful for two reasons: (1) it groups similar transactions to improve opportunities for co-scheduling and overlap, and (2) it helps scheduling reduce waiting times. for each thread slicc records a unique numerical id, a type id, and an arrival timestamp. the timestamp of a team is that of its oldest thread. the oldest team is scheduled, without pre-emption if possible."
"steps [cit] aims to minimize instruction misses from the software side. like slicc, it groups threads executing similar transactions into teams. it, either manually or by using a profiling tool, breaks each transaction's instruction footprint into smaller instruction chunks in a way that each chunk can fit in the l1-i cache. then, all the threads in the same team execute the first chunk, rather than executing the whole transaction without any interruption, on the same core by context switching to the other thread when one completes the execution of the chunk. steps repeats this process for all the chunks, allowing instruction re-use across many threads for each chunk. slicc exploits the same way of re-using the instructions already brought into the cache by previous threads. however, rather than context-switching on the same core, slicc migrates threads to another core so that they can continue their execution. moreover, slicc dynamically detects the synchronization points in a transaction rather than using a priori manual or profiling based software instrumentation. future work may look at combining the time-domain pipelining of steps with the space-domain pipelining of slicc."
"microblogging. l-twitter is a simple twitter clone with tables and schemas modeled after [cit] . there are three tables: users, tweets, graph. graph differs from lsocial's graph because it captures an asymmetric follower relation."
"in addition, the driving factors are another sub-topic to be researched in the ocb field, and employee characteristics, task characteristics, leadership behaviors, and organizational characteristics are the most important sort of driving factors for conducting ocb [cit] . however, in megaprojects, actually, actions that participants act on are on behalf of the enterprises they belong to; thus, their behavioral motivations are more likely to be significant in sociality, and in the pursuit of long-term interests [cit] . meanwhile, megaprojects are normally launched and sponsored by governments, that is, participants are mostly state-owned enterprises or other successful enterprises cooperating with the government, especially in mainland china [cit] . hence, by participating in the construction of megaprojects, enterprises are likely to achieve their political appeal. to be specific, executives of state-owned enterprises are more likely to be promoted due to their good performance in megaprojects [cit] . moreover, except for internal factors like the potential promotion mentioned above, the external environment also drives ocb. practically, the external environment has an important impact on participants' behavior, such as regulations, project culture, corporate reputation and so forth [cit] . therefore, as summarized above, the driving factors considered in this study include project culture, potential promotion, corporate reputation, and public satisfaction."
"this work presents three different slicc designs. the first design (slicc) is transaction-type-oblivious, while the other two exploit transaction type information. given that threads of the same transaction type tend to have similar footprints, knowing each thread's transaction type can lead to better migration decisions. the transaction type information is either provided by the software (slicc-sw), or detected at runtime by the hardware based on the initial instruction sequence each thread executes (slicc-pp). these implementations represent the two extremes and an in-between solution in terms of hardware/software co-operation. future work may look at other alternatives."
"a simulation analysis was conducted after completing the model validation. in this model, the simulation period was set to 10 years given that the duration of one megaproject is longer than that of normal construction projects. the results of the baseline scenario are shown in figure 5a,b and table 1, respectively. figure 5a shows the impacts on which the ocb construction project participants would have in the final performance of megaprojects. meanwhile, the selected outputs included ocba, the mpp (performance of megaprojects), the avpc-1 (accumulated value of the project culture), the avpp (accumulated value of the potential promotion), and the avps (accumulated value of the public's satisfaction). figure 5b illustrates the trends of the avpc (accumulated value of the project compliance), the avib (accumulated value of the innovation behavior), the avhrm (accumulated value of the maintenance of harmonious relationships), the avcb (accumulated value of the collaboration behavior), the avc (accumulated value of conscientiousness), and table 1 shows the simulation values of all the selected variables."
"system chains. many self-conflicts arise among the system sub-chains created by lynx to update derived tables. figure 4 shows a one-hop user chain that modifies a base table causing a system chain. because a chain and its resulting system chains should be serialized together as one transaction, we consider the combined chain in the sc-graph. this chain unfortunately causes an sc-cycle on its two instances, because of self-conflicting hops with updates that do not always commute ( figure 4) ."
"chains for join views. to update materialized join views, we apply ideas from incremental join view update algorithms [cit], using chains to correctly update the views. figure 11 shows the sub-chains for updating the derived table lt-rt, which joins two base tables lt and rt on join key k join . we assume that the join key k join is not the primary key of lt or rt (the case when the join key is a primary key is simpler). therefore, in order to create the join view, programmers are required to add index tables (lt k join, rt k join ) indexing the join key. for updating a join view, there are two cases depending on whether the base table modification changes the existing value of the join key column. the top chain of figure 11 illustrates the case when no existing value of the join key column is changed with an insert operation to the base table lt. in this case, the sub-chain updates both lt's secondary index table for the join key (lt k join ) and the join table lt-rt using a local read-write transaction. the use of a local transaction is possible because the affected rows of the index and join tables lt k join, rt k join and lt-rt are co-located in the same shard. the bottom chain of figure 11 is generated when the existing value of the join key column is changed. in this case, two additional hops are required to maintain lt-rt, one to delete the existing value, another to add the new value."
"before application deployment, lynx performs a static analysis of all application chains to determine if lynx can execute each chain piecewise-one hop at a timewhile ensuring the entire chain and its sub-chains are serializable as a single transaction."
"a final static analysis indicates an sc-cycle: the 1-hop read chain to show a user's friends has an sc-cycle with the 2-hop befriend (or unfriend) chain: the read hop has two c-edges, to each hop of the befriend (or unfriend) chain. we use application knowledge to determine that this sc-cycle is spurious: since a user never befriends himself, the read hop conflicts with at most one of the hops of the befriend chain, so only one of the two c-edges is a real conflict."
an et1100 has an address space of 64k bytes. the first block of 4k bytes(0x0000-0x0fff) is dedicated for registers. the process data ram starts at address 0x1000. the memory space is read and written by ethercat frame or pdi. table i is the overview of address space of et1100.
"the late 80s saw pioneering work in distributed database systems, such as gamma [cit], bubba [cit], r* [cit], teradata, and tandem [cit], which aim to provide the same transactional updates and query interfaces present in centralized database systems. these systems pioneered distributed transactions."
"-it characterizes the memory behavior of tpc-c [cit] and tpc-e [cit] showing that transactions suffer from instruction misses, and that their instruction streams exhibit intra-and intertransaction recurring patterns leading to eviction of useful blocks that are re-accessed (96% of capacity misses are for instructions). -it demonstrates that recently proposed cache replacement policies [cit] reduce instruction misses by 8% on average for the best policy, but leave ample room for improvement. -it presents slicc, a hardware thread migration algorithm, and"
"before quantitative simulation and analysis, it is essential to ensure that the validity of the sd model through a series of tests. tests ensure that the accuracy of the model reflects the real world in a meaningful way [cit] . in this part, three typical tests for the sd were selected, namely, a structure verification test (test 1), a dimensional consistency test (test 2), and a sensitivity test (test 3)."
"view maintenance in database systems. there is much work on maintaining materialized views. incremental maintenance schemes typically update base tables and views in the same acid transaction [cit] . deferred maintenance schemes batch changes to tables, and update views periodically or when there is a query [cit], for efficiency. deferred maintenance is often used in data warehouses where only one update batch executes at any time [cit] . in the same spirit, lazybase [cit] optimizes data analytics by batching writes and updating materialized secondary indexes in epochs."
"slicc improves i-mpki less than the thread-type-aware alternatives, as it has to predict which is the next segment a thread will execute and where that segment currently is, using only a small preamble of the segment. the difference in reduction is more pronounced for tpc-c than tpc-e. tpc-c's overall instruction footprint is larger, resulting in higher variability in the instruction stream. nevertheless, slicc reduces instruction misses by 40.5%, on average."
"this work presented a solution based on thread migration, slicc. similar to csp [cit] and steps [cit], we exploit the code commonality observed across multiple concurrent threads. unlike csp, we do not limit code reuse to os code segments. unlike steps, instead of context switching on the same core, we distribute the instruction footprint across multiple cores and migrate execution. slicc is a low-level hardware algorithm that requires no code instrumentation and efficiently utilizes available cache capacity, by improving intraand inter-thread locality."
"prior to performing the simulation, it is necessary to ensure that all the variables in the model can be quantified. the variables used in this model were divided into three categories, namely, constant, dependent, and qualitative variables. each type of variable has corresponding data sources. the value of the constant variables-which are expected to remain unchanged and will not be affected by other variables during the whole simulation period-were generally quantified by referring to the materials available, such as literature. the values of the dependent variables depend"
"thrashing applications favor bimodal rrip (brrip), which predicts a distant re-reference interval for most blocks [cit] . dynamic rrip (drrip) selects the best policy from static rrip (srrip) and brrip at runtime. figure 2 shows that drrip chose brrip most of the time. contrary to the least insertion policy (lip), which promotes a referenced block to the mru position, brrip uses access frequency to promote cache blocks gradually towards the head of the chain."
"restrictions. a chain has two restrictions. first, application-initiated aborts can occur only at the first hop of a chain (this is needed to implement all-or-nothing atomicity). second, chains are static: each hop executes at a server that is known when the chain starts (needed to implement origin ordering). some transactions cannot be structured as chains. these can be executed as a distributed transaction in lynx."
"linked chains. applications can link together multiple chains so that they execute consecutively, like a chain of chains, where each chain individually satisfies the properties above. the set of linked chains may not be serialized as one transaction, but lynx ensures the following atomicity property: if chains are linked and the first chain starts then the other chains eventually start. like hops in a chain, linked chains can receive inputs from previous chains, and all linked chains must be submitted together."
"a lynx system consists of a number of geo-distributed datacenters, each of which contains many machines. a machine runs many logical lynx servers in the same process. this improves concurrency as having more (logical) servers imposes fewer constraints under origin ordering. the rows of a table are partitioned into shards based on row keys; that is, a shard is a set of rows of a table. the rows of a shard are replicated across the same set of servers, as we now explain."
"geo-replication performance. the right part of table 1 shows experiments where all base and derived tables are geo-replicated at two datacenters. geo-replication reduces throughput by half compared to the left results, because it produces twice the work; and it increases completion latency due to the extra communication."
"slicc handles these cases by considering the frequency of instruction misses; it restricts migration to the cases when a thread starts to miss more frequently. if the thread is moving to a new segment, it will incur more misses than hits. slicc counts the number of misses in a window of recent accesses. when this count is above the dilution threshold, dilution_t, migration is enabled. the miss shift-vector (msv) is a 100-bit fifo shift vector recording the hit/miss history for the last 100 cache accesses (enabled when cache is filled-up). a logic-0 and logic-1 represent a cache hit and miss, respectively. when the number of logic-1 bits reaches a threshold (dilution_t), slicc enables migration. slicc resets the msv with every migration. 4.2.3. remote cache segment search when slicc decides to migrate a thread it has to determine which cache, if any, contains the segment the thread is executing. to do so, slicc records recently missed tags in the missed tag queue (mtq), which is a matched_t entry fifo of n-bit entries, where n is the number of cores. a logic-1 on bit index c for mtq entry i indicates that the ith recently missed cache block was cached at core c. thus, by anding all bits at index c we know whether core c holds all the recently missed cache blocks. this information does not have to be exact or accurate, since it is used by a prediction mechanism. slicc gathers this information incrementally as misses occur and stores it in the mtq. the remote cache segment search is distributed and the decision is made locally by the core we migrate from. a directory coherence protocol could report the complete or partial sharing vector for misses that are tracked by the mtq."
"since an application usually processes a request at the datacenter that stores the requesting user's data, a chain's first hop can complete quickly. in both examples, the application returns control to the user after the first hop. the lack of external consistency is partly compensated by the optional read-my-writes guarantee of chains: in the first example, with read-my-writes user x is guaranteed to see her own message when she browses y's wall. however, unlike the external consistency guarantee, if x tells y about her message using external channels (e.g., the phone) and y checks his wall, y may not see x's message. this is an anomaly that applications must tolerate when taking advantage of transaction chain's low latency."
"as opposed to previous oltp instruction miss reduction techniques, slicc is a hardware solution, that avoids undesirable instrumentation, utilizes available core and cache capacity resources, covers user as well as system-level code, and requires no changes to the existing user code or software system. slicc incurs overheads due to thread migration; thus, context switching and increases in data misses must be amortized to improve performance. a hardware thread migration mechanism provides a programmer transparent solution that has low context switching overheads, and the positive impact of the reduction in instruction misses outpaces the extra data misses."
"data model and usage. application developers define a set of schematized relational tables [cit] sharded based on their primary key. lynx provides general transactions in the form of chains, and all operations are performed using chains. api details are given in section 5.1. we illustrate how applications can use lynx with an example from rubis [cit], a simple online auction service modeled after ebay. rubis stores data in many tables; two are shown in figure 1 . the items table stores each item on sale with its item id, current highest bid, and user who placed that bid. the bids table stores item ids that received a bid, the bid amounts, and the bidders."
"although ocbs have been observed in many megaprojects, studies in the context of megaprojects are still limited, especially on the dimensions and driving factors. one of the most presentative studies on the dimensions of ocb is the seven-dimensional model established by podsakoff [cit], which includes helping behavior, sportsmanship, organizational loyalty, organizational compliance, individual initiative, civic virtue, and self-development. however, the research carried out by podsakoff mentioned above was conducted in permanent organizations which are, in fact, different from temporary organizations like project-based organizations [cit] . thus, ocbs in megaprojects would have unique dimensions and motivations [cit] pointed out that no matter the project compliance or individual compliance, its essence is to obey the organization's regulations and related rules, and the difference between the two kinds of behavior is only the subject that they should be compliant to. the nature of organizational loyalty and sportsmanship behavior are initially individual staff dedicated to their work, namely adhering to the high self-demand, having the initiative to work overtime, and working in extreme conditions voluntarily without any supervision; hence, the two behaviors can be summarized as conscientiousness [cit] . civic virtue refers to good interpersonal relationships in organizations, and the core is the maintenance of harmonious relationships [cit] . self-development and individual initiative both mean work completed creatively or improving work skills spontaneously [cit] . thus, these two behaviors can be interpreted as innovation behavior in the megaproject context. helping behavior means providing direct help for others and taking the initiative to work with colleagues [cit] . while in megaprojects, [cit] believed this kind of behavior should be interpreted as collaboration behavior. therefore, project compliance behavior, innovation behavior, collaboration behavior, conscientiousness, and harmonious relationship maintenance behavior are summarized as the five dimensions of ocb in this study."
"there are two noteworthy user chains, one to process bidding requests (discussed in §2), the other to handle new user registration while ensuring unique usernames. in our first design, a register-user chain checks if a chosen username already exists in a secondary index of user based on usernames; if not, the second hop inserts the user into the user table. this chain has an sc-cycle between two of its instances. we subsequently changed l-rubis to use an additional table, usernames, which contains all the usernames that have ever been created. the register-user chain first checks that the chosen username is absent in usernames (and if so inserts it there) and in the second hop adds the user to users. if the chosen username is already taken, the second hop does nothing. the chain still has an sc-cycle with itself, but this cycle is spurious: if two register-user chains conflict on the first hop (due to both having the same username), then one of the chains sees that the username is already taken in its first hop and does nothing in its second hop, so there are no conflicts in the second hop."
"intuitively, depending on the workload, instruction misses can impact performance more than data misses. for example, modern architectures use instruction level parallelism (ilp) to hide data miss latencies. instruction misses restrict instruction supply, rendering such ilp techniques less effective. existing core architectures make no effort to balance the relative cost of the two types of misses. slicc provides a way of balancing the relative costs of data vs. instruction misses. in section 5 we show that slicc reduces the overall l1 miss rate."
"when user a posts a status message, l-social uses a chain to insert the message into status and add the announcement \"a has changed her status\" to activities. both hops commute with themselves. a similar chain is used to post messages on walls. the join table graphactivities allows a user to read the activities of his friends in one hop."
"test 1 was to make sure that the structure of the established model is logical and generally supported by the existing literature. this test was conducted by referring back to the causal loop diagram in figure 2 . obviously, all cause-and-effect chains in the diagram are based on existing studies and acknowledged knowledge. the purpose of test 2 was to ensure that each equation in the model is dimensionally consistent with the use of the parameters [cit] . the vensim software provides users with a test function automatically after all the units of the variables have been determined. dimensional consistency was correct verified by vensim in this model. the sensitivity test is the test for understanding how the proposed model will behave if the variables vary over a reasonable range. to be specific, the behavioral sensitivity analysis mainly focuses on identified variables and the rationality of the behavior of the system after adjusting the value of the identified variables [cit] . this test is regarded as vital to ensure the reliability of the model. due to the limited size of the paper, only one typical example in relation to the sensitivity of the model was conducted to demonstrate if the behavior of the model reflects the real-world situation. an example in figure 4 illustrates how the ocba (organizational citizenship behavior adoption) varies in line with the changes of the airpp (actual increasing rate of potential promotion). five scenarios were designed and simulated in this"
"therefore, the aim of this study is to assess the impacts of the ocbs of construction participants (including owners, contractors, and designers) on the performance of megaprojects with the aid of the system dynamic (sd) approach. the reasons for adopting an sd approach mainly lie in two aspects. firstly, since there are numbers of elements within the ocb system and their relationships tend to change and are complicated, the sd approach is demonstrated as a well-established method for studying and managing such complex feedback systems [cit], and facilitating better understanding on the mutual relationships between the behavior of a system. secondly, elements in the ocb system are largely interdependent [cit] . sd modeling is able to discover and illustrate interrelationships among elements, and to facilitate the measurement of dynamics among elements [cit] . in addition, the sd approach is also effective in evaluating the consequences of new policies and new structures [cit] . taking the aforementioned aspects, the sd approach is considered a suitable method to be used for fulfilling the research aims in this study. the proposed model is also expected to help decision-makers better understand how ocb affects the performance of megaprojects. the rest of this paper is organized as follows: section 2 entails the literature review including the explanation of the concept, dimensions, and driving factors of ocbs. section 3 introduces sd modeling with a detailed process of model development. section 4 is the model validation, including the tests for structure verification, dimension consistency, and sensitivity. section 5 is the policy analysis with a discussion on a base run and three policy scenarios (two single-policy scenarios and one multi-policy scenario). section 6 concludes the study."
"we examine one scale-out workload and two server workloads as described in table 1 [cit] . tpc-c [cit] and tpc-e [cit] of the scalable open-source storage manager shore-mt [cit] . the client-driver and the database are kept on the same machine, the buffer-pool is set big-enough to keep the whole database in memory, and due to the unavailability of a sufficiently fast i/o subsystem we flush the log to ram. we simulate 1k tasks or approximately 1.1b instructions. we use two different databases in tpc-c-1 and tpc-c-10 to demonstrate that slicc remains effective even with a larger database. tpc-c and tpc-e have larger instruction and data footprints compared to other scale-out workloads [cit] . to demonstrate slicc's robustness, we study the mapreduce cloudsuite workload [cit], which does not have a large instruction footprint [cit] . mapreduce divides the full input dataset across 300 threads, each performing a single map/reduce task. we focus most of our evaluation on tpc-c-1 (referred as tpc-c) and tpc-e. table 2 details the baseline architecture. we use misses per kilo instructions (mpki) as our metric for instruction (i-mpki) and data (d-mpki) misses. we measure performance by counting the number of cycles it takes to execute all transactions. with n-core, our baseline architecture can run up to n concurrent threads with the os making thread scheduling decisions. slicc manages a thread pool of up to 2n threads. unless otherwise indicated, all slicc results are for the slicc-sw configuration -i.e., the sw layer transfers knowledge about thread types to the hw layer."
"lynx supports derived tables-tables whose contents are automatically derived from base tables-for speeding up queries or safeguarding data. there are three types of derived tables: secondary indexes, materialized join views, and geo-replicas. for example, rubis has a secondary index on the item id of bids, to quickly find the bidding history of an item. derived tables are themselves sharded according to their key (secondary index key, join key, or replicated primary key) and spread across machines. when base tables change, lynx automatically issues sub-chains to update the derived tables. these sub-chains are called system chains, while user chains are written by application developers."
"only a few systems offer online distributed view maintenance and even fewer do so in a geo-distributed setting. bigtable now supports secondary indexes [cit] . pnuts added support for secondary indexes and join views that are asynchronously updated [cit] . lynx also updates derived tables asynchronously, in piecewise chains. unlike pnuts, lynx uses static analysis to provide serializability despite asynchronous updates."
"user chains may have unnecessary s-edges: a user chain may have hops that need not be serialized together, but were placed in the same chain because they require all-or-nothing atomicity. in that case, programmers can separate these hops into different chains and execute them as linked chains (section 3), which also provide all-ornothing atomicity but avoid s-edges."
"most of the increase in d-mpki is for stores, which form 45% of total memory accesses, while loads are nearly unaffected. due to slightly fewer migrations, slicc-pp increases d-mpki less than slicc-sw. as expected, slicc is worse with an average d-mpki increase of 9%."
"geo-replication. data shards can have geo-replicas across data centers. geo-replicas are configured by a configuration service that assigns each shard to a replica group, which consists of a set of lynx servers spread across datacenters. geo-replication across data centers is implemented by lynx using system chains as explained in section 6. to avoid having conflicting updates at different replicas, lynx uses home geo-replicas, similar to walter [cit] : each replica group has a designated server called the home geo-replica or home server, and the system forwards all updates on a shard to its home geo-replica. the home geo-replica can be chosen intelligently to be the server where updates are most likely to occur. for example, a web application may have a replica group for each user, where the home geo-replica is in a datacenter close to the user."
"if no matching remote cache is found, slicc will attempt to find an idle core. slicc either broadcasts a request for idle cores to report, or piggy-backs this information on the responses received during the miss tag search phase. thread migrations are relatively infrequent (every 3.2k instructions on average), reducing the relative overhead of remote cache segment and idle core searching."
"previous studies have demonstrated the important role of ocb in the construction sustainability management and considerable studies have been conducted to examine the relationship between ocb and project performance in the last few decades. however, there is still a gap to study the interdependent and dynamic relationships among the variables within the ocbs of construction megaprojects. therefore, this study proposed to use system dynamics to quantitatively study the impact of ocb on the performance of megaprojects."
"lynx has some limitations. first, it does not reduce the total execution time of a chain; rather, lynx can return control to the application after the chain's first hop. the first hop is often fast: it commonly executes in the local datacenter and writes some internal metadata to a nearby datacenter (for disaster tolerance), which adds only milliseconds of delay. this low first-hop latency does not benefit all applications, but we believe that it helps many web applications where users interact-for instance, by sending friendship requests, posting messages on walls, etc. these operations are well served by a chain whose first hop modifies the user's own data, while later hops modify other users' data in the background. the second limitation is that lynx cannot execute all chains piecewise to attain low first-hop latency: the static analysis may force some chains to execute as distributed transactions. the third limitation is that lynx does not guarantee external consistency or order-preserving serializability [cit], but to compensate lynx provides the guarantee of read-my-writes within a session [cit] ."
"compared to the first hop latency, the total completion latency is much longer, as each subsequent hop executes in a different datacenter. for example, the median completion latency for a simple chain of length 2 (no replication) is 86ms, and it grows to 253ms when the length is 3."
"based on the analysis of present industrial robots, improving communication speeds and increasing the feedback information are the essential tasks for industrial robots. hence, it is important to design a robot module joint controller with high speed field bus. in contrast to other field bus, ethercat has obvious advantages of a transfer of 125 bytes over 100mbit/s ethernet [cit] . based on precise clock synchronization, the ethercat circuit coupler accuracy is 23ns [cit] . hence, ethercat is the right field bus desired by robot joint controller."
"first, in a social networking application, suppose that user x posts a message on the wall of a friend y. to execute this request, a transaction chain first modifies x's data by inserting x's message in the message table, and then updates y's data by inserting the message id into y's wall in the wall table. as a second example, in figure 1 the chain for placing a bid first inserts the user's bid into the bid table and then changes global information by updating the high price in the items table."
"mapreduce, which has an instruction footprint that fits in the l1-i cache, remains practically unaffected with slicc. table 3 details the cost of all slicc's hardware components. section 4.2 described some of the components. in addition, slicc on each core, a slicc agent is responsible for managing the thread queue. the thread queue is a circular fifo buffer and the first entry is executed until it migrates, completes, or gets blocked for i/o. on the latter case, the thread is moved to the end of the queue. with an over-provisioned thread queue of 30 threads, and a copy of the team management table, per core, slicc requires a maximum of 966 bytes in addition to logic. all logic operations for slicc are not on the critical path."
"auction service. l-rubis is a port of the auction website in the rubis benchmark [cit] . the original rubis implementation is based on php using a local mysql database system. we ported the rubis schema to lynx and re-wrote its php functions in javascript. l-rubis has 10 sharded tables with 13 secondary indexes in total, where a table has at most 3 secondary indexes. we use a join table to unite the user table, which maps uids to usernames, and the comments table, which records users' comments. this table allows l-rubis to quickly find usernames of users who commented on a seller."
"l-twitter. we evaluate three common operations: readtimeline for showing a user's timeline, follow-user for starting to follow a user, and post-tweet for posting a tweet. we populate the database with 100, 000 users, each with 6 tweets and 6 followers on average. there are 3 datacenters, and we use different geo-replication levels for different tables. we geo-replicate the base tables (tweets, graph) at 2 datacenters, but do not geo-replicate secondary indexes or joins (e.g., graphtweets), which can be reconstructed if there is a disaster. figure 12 shows the operation throughput of l-twitter. for operations that write data, the throughput depends on how many hops the underlying chain has. the chain for post-tweet inserts a row into tweets, updates its replica across datacenters, inserts 6 rows into the join updates the secondary index of graphtweets, for a total 9 hops (6 of which run in parallel). this results in an aggregate post-tweet throughput of 173k tweets/s. the follow operation also inserts 6 rows into graphtweets (each user has 6 existing tweets), thus having the same number of hops as post-tweet and achieving similar throughput (184k ops/s). for follow, all 6 updates to the secondary index of graphtweets have the same secondary key and thus they could have been batched in one rpc. lynx does not currently have this optimization. the throughput for reading a user's timeline is high, at more than 1.35m ops/s. this is because the underlying chain only needs to read (many rows) in one server. table 2 shows chain latency for the l-twitter operations. all chains return after the first hop, so l-twitter achieves low user-perceived latency. the completion latency of post-tweet measures how long its chain takes to update the geo-replica of tweets, and update the join table graphtweets and its secondary index. the 99-percentile latency is 263ms, meaning that a tweet quickly appears in all followers' timelines."
"a log 2 (l1i cache blocks) wide saturating miss counter (mc) continuously counts the number of misses. when mc saturates at a value of fill-up_t slicc assumes that the cache has now captured a full segment and may trigger migrations accordingly. we experimentally found that using a value in the order of cache size 2 for the fill-up_t threshold works reasonably well, with little sensitivity to the exact value of this parameter. other fill-up detection mechanisms may be possible but are beyond the scope of this paper."
"l-rubis. the most interesting chain in l-rubis is the place-bid operation, with a user chain of 2 hops (figure 7 ) plus 4 hops of system sub-chains for geo-replication and secondary indexes. the aggregate place-bid throughput is 168k ops/s-3 times lower than the geo-replicated simple-3 chain, which also has 6 hops ( figure 1) . this difference is because place-bid runs javascript user code at the servers using the v8 engine, which imposes significant overhead, whereas the simple-3 chain does not."
"for a long time, in order to replace factory workers industrial robots have been widely adopted, which have improved automatic level of manufacturing industry and labor productivity. in the environment of intelligent manufacture, robots which are the key point in the integration of industrialization and informatization will play a significant role. however, several questions exist in the current industrial robots. firstly, the fieldbus communication speed is insufficient to meet the requirements of large data transmission [cit] . secondly, most of robots based on position control are lack of the force sensing information [cit] . these problems limit the application of robots in the intelligent manufacturing system."
"esc must be initialized at startup, but configuration information cannot be stored inside of it. hence, esc requires an external eeprom memory chip to store the configuration data. the eeprom will first be read to acquire internal information and esc would not start working until it completed configuration."
"we now describe how chains work at runtime. we give an overview of the implementation ( §6.1), and then explain the details on how lynx ensures the various chain properties ( §6.2) and how it uses system chains ( §6.3)."
"chain throughput. table 1 shows lynx's throughput in thousands of chains/s. we first examine the experiments without geo-replication (left of table). the simple-1 experiment provides a baseline aggregate throughput of 3, 570k chains/s using 12 servers in 3 datacenters. we expect the throughput of simple chains with m hops to be ≈1/m the throughput of a 1-hop chain. the experiments confirm this. throughput drops by over half going from simple-1 to simple-2 because in simple-1 only clients forward chains whereas in simple-2 servers also do that. the system chain for updating the secondary index table has two hops and its aggregate throughput is 1, 220k chains/s. this is lower than simple-2 because of the overhead of checking if a table modification needs a system sub-chain and if so, coordinating the system sub-chain. the throughput of the join experiment is 808k chains/s, much lower than in the secondary index experiment, even though both chains have two hops. this is because the second hop of the join chain requires more computation: it reads rows from the rt table and inserts them into the join table, all in a local transaction (figure 11 ). in the experiments, we pre-populated the rt base table so that there are 6 rows to be read and inserted into the join table every time a chain modifies a single row in lt."
"if the lynx server does not use the cluster storage system, or the cluster storage system is crashed, then recovery relies on geo-replication and reconstruction. before using a geo-replica, the system must ensure it is up-to-date, by restarting and waiting for the completion of any replication sub-chains that might be coordinated by the failed lynx server; how this is done is explained in (2) below. derived tables might not be geo-replicated; these tables are reconstructed using the base tables. then, the system reconfigures the replica groups to replace the failed server with a server holding the geo-replicas or reconstructed tables."
"setting. lynx is a geo-distributed storage system for large web applications, such as social networks, webbased email, or online auctions. lynx scales by partitioning data into many shards spread across machines. each shard can be geo-replicated at many datacenters, based on requirements of locality, durability, and availability. unlike other systems [cit], lynx does not require that all datacenters replicate all data, so lynx can have many datacenters with low replication cost."
"per-hop isolation. lynx stores each shard at one server. because each hop of a chain accesses one shard, we can ensure per-hop isolation by simply executing it using a local serializable database transaction. our current implementation requires shards to fit on a single machine, but it is possible to generalize this to split a shard among several machines and substitute local transactions with distributed transactions within a single datacenter."
"chains for geo-replication. when a hop of the chain wishes to modify a geo-replicated base or derived table, the hop is forwarded to the corresponding shard's home datacenter for execution. the responsible server at the home datacenter generates a sub-chain to propagate the modification to replicas at other datacenters. because of the origin ordering property of these sub-chains, all replicas are updated in the same order."
"mapreduce is a cloud workload featuring a relatively smaller instruction footprint [cit] . since 71% of the total l1 misses are compulsory for 32kb caches, larger l1 instructions or data caches are not as beneficial. figure 1 also shows overall performance improvement normalized to the 32kb baseline. performance improvements with larger l1-d caches are negligible at 1%, but can be as high as 16% with larger l1-i caches (mapreduce shows less than 3% improvement). if increasing cache size did not also increase latency, performance improvements would be higher; for example, a 512kb l1-i with the latency of a 32kb l1-i would result in a 61% performance improvement for tpc-c."
"before quantitative simulation and analysis, it is essential to ensure that the validity of the sd model through a series of tests. tests ensure that the accuracy of the model reflects the real world in a meaningful way [cit] . in this part, three typical tests for the sd were selected, namely, a structure verification test (test 1), a dimensional consistency test (test 2), and a sensitivity test (test 3)."
"user chains. user chains can have spurious c-edges because the notion of conflict is coarse-grained, being based on table accesses. this problem is exacerbated by self-conflicts between instances of the same chain. in figure 3, t bid modifies two tables, creating an sc-cycle on its own instances. closer inspection reveals that the hop \"insert to bids\" inserts a row with a unique id; this hop commutes with itself, so it does not self-conflict. developers can use annotations to indicate that the hop self-commutes, which removes the c-edge between its instances, breaking the cycle. other systems also exploit commutativity [cit], but in different ways."
"local replication and cluster storage system. data shards may also be replicated within a datacenter to provide fast fail-over. this replication is provided by a cluster storage system that provides synchronous updates and transparent failover; such a service is implemented using well-known techniques (e.g., [cit] )."
"having determined a good slicc configuration, this section shows that the three slicc variants are able to reduce instruction misses much more than they increase data misses. figure 10 shows the l1 i-mpki for the baseline, slicc, slicc-sw, and slicc-pp. for mapreduce, since the instruction footprint fits in a 32kb cache, slicc does not affect instruction or data misses."
"for local host controller, dsp, of the joint controller, esc is used as an external memory that can generate interrupt. and the process data interface(pdi) realizes the connection between local controller and esc. through reading or writing corresponding memory regions on the esc, the local controller can implement a data transmission. fig. 3 shows the general functionality of esc."
"many web applications rely on geo-distributed storage systems, such as cassandra [cit], megastore [cit] and spanner [cit] . these systems hold the promise of both high availability (by replicating data across datacenters) and low latency (by placing data close to clients). a useful feature of storage systems is serializable transactions, which group many read/write operations to ensure consistency despite failures and concurrency. unfortunately, existing mechanisms to provide transactions [cit] are expensive for a geo-distributed setting, incurring interdatacenter delays of up to hundreds of milliseconds."
"in order to meet these requirements, slicc needs to maintain runtime information about caches and individual threads to be able to make judicious migration decisions. first, slicc needs a mechanism to determine whether the cache is filled-up with useful cache blocks or not. in addition, with respect to a given core, slicc should be able to predict which remote core holds the cache blocks that will be touched next."
"having discussed the restrictions, we describe our experience in using transaction chains for web applications. we focus on web applications where users interact, which require scalability and low latency. in such applications, we recommend co-locating data owned by the same user in the same datacenter (possibly with geo-replication). to process a typical user request, one uses a transaction chain which first modifies a user's own data and then updates other users' data or global data. we give two examples."
"unless otherwise stated, in all experiments each region has 4 lynx servers and 4 client machines, where a machine is an extra-large instance with 15gb of ram and 4 virtual cores. the geo-replication factor is two datacenters. we perform three runs for each experiment and report the average. (standard deviations were low.)"
"data-oriented transaction execution (dora) indirectly affects the instruction footprint of a transaction [cit] . it divides a transaction into smaller actions based on the data being accessed at a particular transaction part. then, each of those actions are sent to their corresponding worker threads, reducing the overall number instructions executed by a single thread per transaction. such a design might not necessarily break a transaction into instruction parts that can fit in l1-i. however, if combined with slicc, it can give better hints on where to migrate or reduce the total number of migrations needed to be done per worker thread."
"the rubis developers denormalized the schema to duplicate the highest bid in the items table, to improve the performance of a common operation: display the current highest bid price of an item. when a user places a new bid, rubis must insert the bid into bids and update the corresponding high price in items in the same transaction to ensure consistency. with lynx, programmers write such a transaction as a chain (figure 1, bottom) ."
"our current prototype misses four pieces from the design. first, it lacks the configuration service, instead relying on a static configuration file to indicate what server has what shards. second, a lynx server and coordinator have their stable storage on a local disk, not a cluster storage system. third, our prototype does not yet implement the recovery protocol (section 6.2) for handling server or datacenter failures. fourth, there is no implementation for executing a chain as a distributed transaction."
"as an example, consider the auction application from section 2 (figure 1), with the three chains: t bid for placing a bid, t item for adding an item to be auctioned, and t read for browsing an item. t bid has two hops, while the others have one hop. for simplicity, let us ignore the system chains. figure 3 shows the resulting sc-graph. there is an sc-cycle involving two instances of t bid, so this chain cannot safely execute piecewise."
"with the development of the industry 4.0, the speed of integration of industrialization and informatization has been accelerated. intelligent manufacturing system has become more and more intelligent and flexible [cit] . in order to develop intelligent manufacturing systems, we need more information of sensors and automatic devices."
"the policy scenario b, similarly to scenario a, is also a single policy scenario that is designed to verify the effect of the increase in the airpc (actual increasing rate of project culture) on pp, the ocba, the avc, the avcb, the avhrm, the avib, and the avpc over the simulation period. to examine the impact of the improvement in the airpc on the selected variables, two devised scenarios were simulated under policy scenario b for the comparison against the baseline scenario, namely psb-1 and psb-2. the initial value of the airpc was 0.05 in the base run and increased to 0.2 and 0.4 in psb-1 and psb-2, respectively. as presented in table 3, the results indicated that the rise in the airpc could increase the value of the ocba and pp, from 50.78 and 14.09 in run psb-1, to 63.85 and 17.43 in run psb-2, at the end of the simulation period. compared with the baseline scenario, the ocba was improved by 16.23% (psb-1) and by 46.14% (psb-2). meanwhile, the mpp has increased by 12.27% (psb-1) and by 38.88% (psb-2) by the end of the period. moreover, the enhancements were also observed in the other five selected variables. the values of the avpc, the avib, the avcb, the avc, and the avhrm arrived at 35.11, 31.90, 34.22, 33.50, and 33.85 in psb-2, and 38.95, 35.39, 37.95, 37.16, and 37.44 in psb-2. it is obvious that the value of the avpc is the highest among the five variables, with improvements of 6.78% in the psb-1 and 18.46% in the psb-2. the improvements in all the selected variables could probably be a result of the fact that the participants of the construction influenced by a positive project culture within organizations and contributed to a high quality of their daily work, which is likely to lead to an improvement in the performance finally. however, although the selected variables have increased in both psa and psb, some differences still exist in the simulation results of these two policy scenarios. firstly, the simulation results of psb are more moderate than those of psa, indicating that providing opportunities of promotion is more effective than the project culture to improve ocb and project performance. this finding echoes with the existing results of other researchers [cit] that pointed out that the motivation of promotion is more attractive to persons in the top of the management team. secondly, psa and psb both posed different effects on the avpc, the avib, the avcb, the avc, and the avhrm. to be specific, in scenario a, the avpc, the avc, and the avcb are the top three variables which improved the most significantly/ in the meantime, the avpc, the avc, and the avhrm are the top three in scenario b. this result suggests that scenario a could impose more significant effects on the collaboration behavior rather than on the maintenance of harmonious relationship in scenario b. the explanation is that project culture effectively contributes to foster a harmonious atmosphere within organizations [cit], especially in countries like china where there is an emphasis on harmony and people show high concern for group harmony."
"prior to application execution, lynx performs a global static analysis of its transaction chains. the analysis determines if it is possible to execute each chain piecewisethat is, as a series of local transactions, one per hopwhile preserving serializability of the entire chain. the analysis uses the theory of transaction chopping [cit] to construct a graph based on the operations within the trans-actions. lynx has two ways to enhance the opportunity for piecewise execution. first, lynx lets programmers provide annotations about the commutativity of pairs of hops that would otherwise be considered to conflict. second, when chains are executed piecewise, lynx ensures origin ordering: if chains t 1 and t 2 start at the same server, and t 1 starts before t 2, then t 1 executes before t 2 at every server where they both execute. this property eliminates many conflicts in the internal chains that lynx uses for updating secondary indexes and join tables."
"the server then notifies the coordinator that the hop is done, attaching the hop's output. the server deletes the hop entry from its history table when it gets an acknowledgement from the coordinator. the coordinator updates the current progress of the chain in its history table; it deletes the chain's entry after the entire chain completes."
"naive construction of the sc-graph. to apply the theory of transaction chopping in our context, a chain corresponds to a chopped transaction and its hops are the pieces. thus, in the sc-graph, s-edges connect the hops of a chain, while c-edges mark potential conflicts between hops of different chains. static analysis cannot determine exactly what data items a hop accesses (which rows); therefore, we conservatively add a c-edge between two hops of different chains if the hops access the same table and an access is a write. since instances of the same chain may be in conflict (if they update data), the sc-graph includes two instances of every chain that updates data; 3 for read-only chains, one instance suffices. we must also consider system sub-chains caused by user chains (recall that system chains are automatically created to update derived tables when base tables change); we want these sub-chains to be serialized with the originating chain. a simple idea is to combine a user chain and its sub-chains in the sc-graph: when a user chain hop modifies a base table, the hop is expanded into the subchains that update derived tables. later, in section 4.2, we improve on this simple idea."
"literature showed that memory stalls for oltp workloads account for 80% of their execution time, and l1 instruction misses account for 70-85% of overall stall cycles. we corroborate these results and show that 94% of l1 capacity misses are for instructions. additionally, we show that recently proposed replacement policies, which reduce miss rates for some workloads, leave a lot of room for improvement compared to using larger l1-i caches. previous works tackle this problem in software or hardware, but they are either impractical (require code instrumentation) or relatively expensive (large on-chip data structures)."
"chain latency. table 1 shows the median and 99-percentile latency for completing both the first hop and the entire chain. the experiments were done under low load and we measured the latency of chains starting in the west coast. since the first hop of a chain executes in the local datacenter, first-hop latency is below 4 ms (99-percentile) across workloads. this latency number is optimistic for two reasons. first, it does not reflect disk latency: although our server implementation synchronously writes its log to disk, the disk latency is absorbed by on-disk caching which cannot be disabled in ec2. second, it does not reflect the delay in replicating the chain coordinator's log to a nearby buddy datacenter: our prototype currently logs to the local disk as opposed to a cluster file system."
"the first server of a chain coordinates its execution in a coordinator thread. the coordinator first stores information about the chain in its history table kept in the cluster storage system. the history table keeps the chain id, the chain parameters from the client, and the origin ordering sequencers ( §6.2). the coordinator may execute the chain piecewise or as a distributed transaction."
"similar to d-mpki, d-tlb misses increase on average by 11% and 8% with slicc and slicc-sw, respectively. i-tlb misses are within +/-0.5% of the baseline."
"the analysis uses knowledge of the table schemas and the application chains, specifically the table accessed by each hop of each chain and the type of access (read or write). the analysis determines what chains can be executed piecewise while preserving serializability."
"recall that a coordinator orchestrates the execution of a chain. we must address three failure types that break chain execution: (1) crashes of a lynx server, (2) crashes of the coordinator, and (3) failures of an entire datacenter."
"read-my-writes in sessions. this property ensures that a chain in a session sees the writes of chains in the same session that have already returned. to do so, the application associates a session with a server, and lynx forces all session chains to start at that server by adding a no-op first hop if necessary. a possible optimization in practice is to pick a server where most session chains start anyways, to avoid adding the no-op hop. if a session chain reads from a base is similar to that of a coordinator."
"the eiger experiments use the same setup with 3 availability regions. we observe an aggregate throughput of 12k tweets/s. by comparison, l-twitter running on lynx achieves 173k tweets/s. thus, lynx has better throughput with serializability while eiger offers only causal+ consistency. admittedly, the performance difference can be an artifact of the two systems' implementation choices; an apples-to-apples comparison is impossible."
"using these ideas we built lynx, a geo-distributed storage system that provides serializability with low latency. to scale, lynx partitions tables into many shards, each possibly replicated in a subset of datacenters. lynx provides a new primitive called transaction chain or simply chain. a chain is a sequence of hops, each accessing data on one server, such that all hops execute exactly once or none of them do, similar to the notion of a saga [cit] . applications submit transactions to lynx as chains; lynx also uses chains internally to update secondary indexes, materialized joins, and geo-distributed replicas."
"a simulation analysis was conducted after completing the model validation. in this model, the simulation period was set to 10 years given that the duration of one megaproject is longer than that of normal construction projects. the results of the baseline scenario are shown in figure 5a,b and table 1, respectively. figure 5a shows the impacts on which the ocb construction project participants would have in the final performance of megaprojects. meanwhile, the selected outputs included ocba, the mpp (performance of megaprojects), the avpc-1 (accumulated value of the project culture), the avpp (accumulated value of the potential promotion), and the avps (accumulated value of the public's satisfaction). figure 5b illustrates the trends of the avpc (accumulated value of the project compliance), the avib (accumulated value of the innovation behavior), the avhrm (accumulated value of the maintenance of harmonious relationships), the avcb (accumulated value of the collaboration behavior), the avc (accumulated value of conscientiousness), and table 1 the mpp is a key variable for measuring the performance of the construction project in the studied model. it was assigned to a value ranging from 0 to 100, where the value of 0 was expected to represent the lowest level and 100 was expected to represent the highest level. the ocba is a variable for examining the ocb of participants in the model from 0 to 100 as well. obviously, as shown in figure 5, the value of the five outputs all increased across the whole period. the main difference is that the value of the mpp increased at a low rate in the first five years, but experienced a relatively sharp increase in the last five years (a rate of 12.55; table 1) . by contrast, it shows that the ocba is on an increasing trend in the first seven years and then steadily ascends from 39.31 to 43.69 by the end of the period ( table 1 ). the possible explanations to the different trends could be that the delay in the ocb positively contributes to the increase of the mpp as the simulated project proceeds. thus, the pp is expected to present a noticeable rise at last. as for the avps, its value rises from 5.17 to 7.17; for the avpp, the value reaches 6.73 at the last year; the simulation figure of the avpc-1 witnesses the slowest growth among the three, reaching its peak of 6.08 at the end of the period. the simulation results above indicate that the mpp was gradually improved along with the progression of a simulated project which echoes with the findings from the previous papers, such as nielsen [cit], who stated that the ocb could positively contribute to the project performance but that there is indeed a delay in its reflection on the performance. in figure 5b, the value of the avpc is the highest, reaching 32.88 at the end of the simulation period, followed by the avcb (32.04), the avc (31.62), the avhrm (30.77), and the avib (29.88). notably, this simulation result indicates that the project the mpp is a key variable for measuring the performance of the construction project in the studied model. it was assigned to a value ranging from 0 to 100, where the value of 0 was expected to represent the lowest level and 100 was expected to represent the highest level. the ocba is a variable for examining the ocb of participants in the model from 0 to 100 as well. obviously, as shown in figure 5, the value of the five outputs all increased across the whole period. the main difference is that the value of the mpp increased at a low rate in the first five years, but experienced a relatively sharp increase in the last five years (a rate of 12.55; table 1) . by contrast, it shows that the ocba is on an increasing trend in the first seven years and then steadily ascends from 39.31 to 43.69 by the end of the period ( table 1 ). the possible explanations to the different trends could be that the delay in the ocb positively contributes to the increase of the mpp as the simulated project proceeds. thus, the pp is expected to present a noticeable rise at last. as for the avps, its value rises from 5.17 to 7.17; for the avpp, the value reaches 6.73 at the last year; the simulation figure of the avpc-1 witnesses the slowest growth among the three, reaching its peak of 6.08 at the end of the period. the simulation results above indicate that the mpp was gradually improved along with the progression of a simulated project which echoes with the findings from the previous papers, such as nielsen [cit], who stated that the ocb could positively contribute to the project performance but that there is indeed a delay in its reflection on the performance. in figure 5b, the value of the avpc is the highest, reaching 32.88 at the end of the simulation period, followed by the avcb (32.04), the avc (31.62), the avhrm (30.77), and the avib (29.88). notably, this simulation result indicates that the project compliance could be the most significant ocb in the megaprojects, in agreement with a survey that concluded that compliance is, in fact, the most common kind of ocb in the megaprojects [cit] ."
"all-or-nothing atomicity. if the first hop of chain commits, subsequent hops are executed exactly once despite failures. lynx ensures this property by replaying chains that stop due to failures, using history tables to prevent duplicate execution, as we now explain."
"we evaluate three types of chains. in the simple-n experiments, a client operation is a chain with n hops, each inserting a row into a different base table. in the secondary index experiment, a client operation inserts a row into a table with a secondary index, resulting in a system chain of 2 hops. in the join experiment, a client operation inserts a row into the lt base table which has both a secondary index table and a join table (with another base table) . in all chains, the first hop executes in the local datacenter and the subsequent hops execute in different remote datacenters. all chains run only c++ code at servers."
"lynx uses much less storage space than eiger. in ltwitter, lynx geo-replicates base tables only once and derived tables zero times, which suffices for disaster tolerance. by contrast, eiger forces all data to be replicated at all datacenters, causing a large space overhead."
"online transaction processing (oltp) is a multi-billion dollar industry that increases 10% annually [cit] . oltp needs have been driving innovations by both database management system and hardware vendors, and oltp performance has been a major metric of comparison across vendors [cit] . unfortunately, modern cloud and server infrastructures are not tailored well for the characteristics of oltp applications [cit] . literature shows that oltp workloads are memory bound; memory access stalls account for 80% of execution time, most of which are due to first-level instruction cache misses [cit] . software [cit] and hardware [cit] efforts are trying to alleviate stall time related to instruction misses."
"section 2.1.3 showed that the instruction footprint overlap is higher among threads of the same transaction type. the basic slicc does not directly exploit this phenomenon. it tries to detect, on-the-fly, whether a thread matches the segment on the core it is currently executing. thus, a thread of type x may partially kick-out cache blocks used by threads of type y. if the transaction type for each thread were known, slicc could schedule similar threads on the same set of cores to reduce conflicts. we propose two slicc variants that exploit such thread transaction type information. 4.3.1. assigning transaction types slicc-sw relies on the oltp software layer to annotate each thread upon launch with a transaction type. this guarantees correctness, but requires some modifications to the software/hardware interface."
"the basic slicc design is a type-oblivious algorithm, i.e., no information is provided about which threads resemble the same transaction type. information about thread types could enhance slicc's process. thus, there are several alternatives. on one extreme, the hardware can dynamically migrate threads to cores irrespective of their types. on the other extreme, the software layer can transfer knowledge about thread types. in between, threads can be pre-processed to detect threads with similar starting address ranges. we detail the three alternatives in section 4."
"(2) the coordinator crashes while executing a chain. in this case, the system restarts the coordinator at another host. the new coordinator determines the outstanding chains using the history table of the previous coordinator, which is kept in the cluster storage system. to handle datacenter failures (see below), the coordinator's cluster storage system is geo-replicated at buddy datacenters ( §5.2). (note that the cluster storage of the coordinator is separate from the cluster storage of a lynx serveronly the former uses buddies; the latter is contained in a single datacenter.) for each outstanding chain, the new coordinator replays the chain from its first hop, executing one hop at a time using the origin ordering sequencers stored in the history table. servers that already executed the chain avoid duplicate execution as explained above."
"test 1 was to make sure that the structure of the established model is logical and generally supported by the existing literature. this test was conducted by referring back to the causal loop diagram in figure 2 . obviously, all cause-and-effect chains in the diagram are based on existing studies and acknowledged knowledge. the purpose of test 2 was to ensure that each equation in the model is dimensionally consistent with the use of the parameters [cit] . the vensim software provides users with a test function automatically after all the units of the variables have been determined. dimensional consistency was correct verified by vensim in this model. the sensitivity test is the test for understanding how the proposed model will behave if the variables vary over a reasonable range. to be specific, the behavioral sensitivity analysis mainly focuses on identified variables and the rationality of the behavior of the system after adjusting the value of the identified variables [cit] . obviously, all curves under the five different scenarios show a similar shape, demonstrating the fact that the larger the airpp is, the better the phenomenon of the ocb is. this is in accordance with the findings that political motivations, especially promotion, is an effective instrument for promoting ocb within organizations [cit] and that the people who have a higher possibility of being promoted would also have a significantly higher probability of improving their ocb [cit] . therefore, it can be concluded that the established model can properly reflect the outcomes of the changes in variables. thus, further simulation and policy analysis can be conducted. obviously, all curves under the five different scenarios show a similar shape, demonstrating the fact that the larger the airpp is, the better the phenomenon of the ocb is. this is in accordance with the findings that political motivations, especially promotion, is an effective instrument for promoting ocb within organizations [cit] and that the people who have a higher possibility of being promoted would also have a significantly higher probability of improving their ocb [cit] . therefore, it can be concluded that the established model can properly reflect the outcomes of the changes in variables. thus, further simulation and policy analysis can be conducted."
"to avoid this bandwidth overhead, we use an approximate cache signature in the form of a partial-address bloom filter that supports evictions [cit] . when the index size of the bloom filter is larger than the cache set index, collisions occur only within sets. hence on evictions, only the set of the evicted block is checked for collisions. every core maintains such a filter, representing a superset of the currently cached blocks. in this design, once migration is triggered, remote-cache search requests are answered by the approximate signature, avoiding contention with the original cache references of the remote core. in section 5.3, we evaluate the tradeoff of the bloom filter's accuracy versus its size. we find that for a 32kb cache, a 256b bloom filter is sufficient."
"we examined data prefetching to mitigate the increase in data misses. for each thread, we recorded the tags of the last n-referenced data blocks and then prefetched those blocks to the core the thread migrated to. this prefetcher did not improve performance, and past a value of n, it hurts performance. there are several reasons why this prefetching proved ineffective. (1) the prefetched data increased the bandwidth on lower cache levels, which affects overall performance when n is high. (2) when n is low, there was not enough reuse. (3) not all prefetched blocks are referenced again. (4) finally, since 45% of the data accesses are stores, prefetching causes invalidations that would not have occurred otherwise. this section showed that all slicc variants improve i-mpki significantly with a minor increase in d-mpki, which is negligible with the larger database for tpc-c. these results suggest that slicc can improve performance if, as expected, instruction cache misses degrade performance more than data cache misses; data misses can be partially overlapped with out-of-order execution. if this is the case, slicc has the potential to offer a better balance of instruction vs. data cache misses over a conventional architecture."
"as per the description of section 4.2.3, a thread that wants to migrate has to find which cache, if any, holds the next code segment. our results, thus, far modeled this searching by including separate messages for the corresponding miss messages using separate broadcasts. we do so to obtain an upper limit of the overhead these messages may induce. we report the frequency of these messages as broadcasts per kilo instructions (bpki) and find that it is very low. for tpc-c, bpki is 2.204 for slicc and 0.28 for slicc-sw and slicc-pp. for tpc-e, bpki is 1.328 for slicc and 0.367 for slicc-sw and slicc-pp. as section 4.2.3 explained these requests are required anyhow for normal miss processing. the ownership information required by slicc is either already available or should be possible to piggyback on existing responses."
"the correctness of the join process is assured by two features of the chains. first, with origin ordering, modifications on the same row of lt interleave correctly. second, with per-hop isolation, the local read-write transaction updating lt k join, rt k join, and lt-rt ensures that lt-rt is always the join of the secondary indexes lt k join and rt k join . this reduces the correctness of updating the join table to the correctness of updating secondary indexes, which is evident."
"considering the contribution of ocb to the improvement of the performance of megaprojects, scholars in the academic and industrial areas have carried out various research on how to promote ocb. according to existing studies, political motivations, especially when reflecting a promotion, and project culture are widely regarded as typical factors [cit] . given the fact that illustrating all factors with respect to ocb is impractical due to the limited length of the paper, the two factors mentioned above were selected to simulate the policy analysis. by implementing the two policies individually and in combination with each other, various scenarios were simulated and analyzed. policy scenario a and b are both single-policy scenarios, which means that only one variable was changed while the others remained unchanged. policy scenario c is a multi-policy scenario and two variables were changed simultaneously."
"workflow management [cit] . transaction chains resemble application workflows in systems like travel planning or insurance claim processing. an application workflow naturally consists of many activities, each executing as a transaction. like lynx, workflow systems guarantee that all activities are eventually executed completely and exactly once. however, these systems are designed to manage sophisticated workflows often involving people actions, while lynx uses chains to efficiently execute logical transactions while guaranteeing that entire chains are serializable."
"we compare the application performance of lynx to eiger [cit], a geo-replicated key-value storage system with write-only transactions and causal+ consistency, built over cassandra [cit] . we implemented the l-twitter operations using eiger's column-family key-value data model. each user x has a row with four column families: followers has a list of sparse columns for users that follow x; followees has the users that x follows; tweets has the list of posts written by x; and timeline has posts from users that x follows. to post a tweet, user x reads the list of followers and uses a write-only transaction to insert the tweet and update the followers' timelines."
"we implemented three applications using lynx: a social network website (l-social), a microblogging service (l-twitter), and an auction service (l-rubis). the applications use secondary indexes and join views extensively, and all of their chains can execute piecewise. this required modifying some chains slightly (while retaining the same behavior). in particular, a user chain which reads a base table and its derived table creates an sc-cycle. we addressed this by duplicating the needed columns of the base table in the derived table, so a user chain needs to only read the derived table."
"the main contributions of this study lie in four aspects. first, the inherently dynamic nature of the ocb system which has been neglected by most previous studies has been well envisaged. second, the sd model not only facilitates the illustration of interrelationships among the variables from a quantitative perspective, but also deepens the stakeholders' understanding of the entire system of ocb. third, the established model can serve as a laboratory and as a platform to better simulate the potential effects of ocb on the performance of megaprojects, and to test the different scenarios of the possible futures which are have been relatively less studied before. finally, the policy scenarios could identify the benefits that ocb would bring about, delivering a clearer and perhaps more realistic view of the appropriate actions that improve the performance of megaprojects in the real world. despite significant contributions, the two main limitations cannot be ignored. on the one hand, the sd method only pays attention to the general dynamic trends of prediction and does not emphasize the accurate value in a specific year. therefore, it is suggested to be applicable to long-term predictions which do not require accurate results. on the other hand, the sd model is a simplification and an abstraction of the system in the real world, thus, only major variables and their interrelationships are considered in the process of model development in this study, which would lead to an adverse impact on the accuracy and reliability of the simulation results. therefore, future research is encouraged to encompass more variables involved in the ocb system to increase its credibility and predicting accuracy. in addition, given the limited length, only three policy scenarios have been simulated and analyzed through a comparison of the results with the base scenarios. in the future study, more similar simulations that are composed of different designed policies can be conducted and analyzed under different scenarios by using the method proposed in this study."
the implementation stores tables in a custom storage system rather than a local database system. the custom system keeps tables in memory with transactional logging to stable storage.
"in this section, the design of process data interface(pdi) between esc and dsp is mainly introduced. according to the architecture of ethercat system, hardware design of the joint controller can be divided into three parts: physical layer, data link layer and application layer. in physical layer, standard rj45 connectors, pulse transformers and standard phys are indispensable. et1100 is a chip for data link layer where the chip handles ethercat frame all by hardware. the application layer controller is tms320f28335 type of dsp produced by ti company. the core part of joint controller is this dsp, which controls brushless dc motor and reads the force feedback signal. the mii interface is between the physical layer chip and esc. in addition, the esc processes ethercat frames and provides data for local host controller via spi. fig. 4 shows the overall structure of joint controller hardware. fig.4 overall structure of joint controller hardware esc supports several types of pdi, which can be configured as spi, parallel interfaces or digital i/o interfaces by the registers of 0x0140~0x0141. for high quality motor control, the spi mode is utilized for process data interface. fig. 5 shows the connection diagram of spi master and slave. in this mode, spi master and slave are dsp and esc respectively. an spi access starts while the master asserts spi_sel and terminates by taking back spi_sel. each spi access consists of an address phase and a data phase. in the address phase, the master transmits the address to be accessed and the command. in the data phase, output data is presented by the slave and input data is transmitted by the master. during accessing to spi slave, the master has to cycle spi_clk eight times for each byte transfer."
"copyright is held by the owner/author(s). sosp '13, nov. 3-6, 2013, farmington, pennsylvania, usa. acm 978-1-4503-2388-8/13/11. http://dx.doi.org /10.1145/2517349.2522729 studies done at google and amazon show that web users are sensitive to latency [cit] : even a 100ms increase in latency causes measurable revenue losses. it is therefore important to reduce the latency of transactions as much as possible. a common way to achieve low-latency is to drop serializability [cit] and offer relaxed consistency (e.g. causal+ [cit], psi [cit], red/blue [cit], hat [cit] ). many systems with weakened consistency also have other limitations: some systems require all data to be replicated at all datacenters [cit], while others [cit] lack a scalable design within a datacenter."
"chains for secondary index tables. when a row is inserted, deleted, or updated in a base table, the server where the modification occurred spawns a subchain to modify the index tables. (if an index table is geo-replicated, the corresponding server at the home datacenter generates additional sub-chains for georeplication.) the sub-chain has one or two hops for each index table: if the indexed value does not change, one hop suffices to update the index table; if the indexed value changes, the old and new rows of the index table may belong to different shards, in which case two hops are needed, one to delete the old row, the other to insert the new row. figure 10 's top chain shows the case where only one hop is needed."
"finally, it is necessary to emphasize eeprom_loaded signal. this signal can be set only if the data of eeprom have been loaded into esc correctly. the signal provides conveniences to detect the state of esc. 6 shows the structure of ethercat slave stack which is consisted of hardware access, generic ethercat stack and user application. the hardware access layer provides the pdi and hardware abstraction functions and macros for esc access. the generic ethercat stack provides the functions that handle the process data, maxilbox data and ethercat state machine. and the coe protocol is applied in the joint controller. the canopen over ethercat(coe) protocol is utilized to configure motion controller and exchange data objects on application level. cia402 drive profile is specified by iec for motion control industry. and it makes a clear specification for the structure function of servo drive system. cia402 drive profile is mapped to ethercat. this implementation provides interface between the motion controller application and communication layer. in this paper, ethercat application layer is designed according to cia402 drive profile. fig.8 shows the cia402 servo controller based on ethercat."
"web applications typically have an a priori known set of transactions, permitting a global static analysis of the application to determine what chains can be executed piecewise while preserving serializability. if the analysis determines that executing a chain piecewise would violate serializability, lynx executes the chain as a distributed acid transaction [cit], incurring higher latency. alternatively, the developer can remove conflicts using annotations or linked chains, as we describe below."
"to allow for queuing threads, the thread migration performed in slicc transfers architectural register files as in thread motion [cit] . the thread's context is saved in the l2 cache closest to the target core and is then retrieved at the target core. this minimizes the set-up time for the thread. since modern commercial processor technologies (e.g., intel virtualization (vt) [cit] and amd secure virtual machine (svm) [cit] ) provide hardware support for thread migration, minimal modifications are required to make the migration process transparent to higher software layers. canonical os kernels are responsible for assigning threads to cores. hardware support for thread migration that is transparent to higher layers avoids any software overhead. otherwise, the os scheduler must be informed about these migrations. an alternative is a hybrid system in which hardware mechanisms provide counters and migration acceleration, while leaving the policy choice to software. this enables easier integration between existing schedulers and platforms with virtualization support."
"object dictionary is the critical point of coe protocol, and it is the description of whole servo controller. each of the objects consists of the 16 bits index and the data. objects with the index of 0x6000~0x67fe are specified for cia402 drive profile. drive state machine plays an important role in cia402 drive profile. the state machine describes the device status and the possible control sequence of the drive. the motor cannot run, until the state machine has been switched correctly. fig. 9 shows the device state machines and their relationship. state transactions are requested by setting the object with index 0x6040 called controlword. not_ready_to_switch_on is an internal state in which communication is enabled only at the end, and the user cannot monitor this state. switch_on_disabled is the minimum state to which a user can switch. in this state, the drive initialization is completed. the drive can perform transitions 0 and 1 after initiation automatically. the high voltage may be applied to the drive in the state of ready_to_switch_on, however the drive operation is still disabled. switched_on is the state that the power amplifier is ready but the drive operation is disabled all the same. the drive function can be enabled as the state is switched to operation_enable. no matter what state of the drive is, the state can be transmitted to fault in case of a fault occurring. through the device description file of slave, the ethercat master can acquire the information of slave and configure it. the device description file of ethercat system is generated by xml schema language [cit] . fig. 10 shows the general structure of a device description file. the ethercatinfo element is the root element of ethercat slave device description file [cit] . the element of vendor describes the identity of the device vendor including its name and ethercat vendor id assigned by ethercat technology group(etg). the description element describes ethercat device via the elements groups, devices and modules. the groups and devices are main elements of slave device description. through the element groups, similar devices can be assigned to one group, and the element devices describes the device with its ethercat features such as syncmanagers, fmmus, and object dictionary. however the element modules is optional, according to the modules device profile, it is used when the slave device is structured."
"l-social. we evaluated a common multi-hop user chain in l-social, post-status. its first user hop inserts a new status to the status table and the second user hop adds a message \"user x has changed her status\" to activities. the system chains generated by the second user hop are similar to that of post-tweet in l-twitter. the overall throughput for post-status is 64k ops/s."
"the design of this paper according to the object dictionary of cia402, and the object dictionary is described in the element of profile. as shown in fig. 11, the channelinfo describes information of protocol, and the dictionary which contains two elements of datatypes and objects describes content of object dictionary. the datatypes are the data type of corresponding objects. moreover, the description file also includes the information of eeprom, which is in the element of eeprom. in the first experiment, cia402 state machine transform has been executed. firstly, twincat system manager in master pc was started. secondly, right mouse click on \"i/o devices\" and select \"scan devices\", after successful device scan fig. 13 was shown on the screen. click on the \"freerun\" button, then the system enters operation mode. assign a series of values to controlword(0x6040), which are 0x06, 0x07 and 0x0f. then the feedback of statusword(0x6041) is 0x4663, which signifies the state of operation_enable. fig. 14 shows that the drive device has been switched to the state of operation_enable. in the second experiment, a timing analyzer nanl-b500e-re, the product of hilscher, has been used to analyze the performance of ethercat system. the experiment has been carried out with a real-time task period of 1000μs. fig. 15 shows the timing analysis of the experiment. almost every sample concentrates on the point of 1000μs. obviously, the round trip time delay of ethercat system is very stable. these results show that the performance of ethercat system is remarkable."
"chain analysis. prior to application execution, lynx statically analyzes chains based on application code and table schemas ( §4.1). the analysis outputs sc-cycles, if any. programmers can use this information to add annotations or use linked chains to break the cycles ( §4.2)."
"when a thread migrates, it leaves data that might be reused behind. this may increase the data miss rate. section 5 shows that while slicc does increase the data miss rate, the benefit from reducing instruction misses outpaces the performance loss due to data miss increase."
"in addition, the significant increase are also illustrated in the other five selected variables, namely, avpc, avib, avcb, avc, and avhrm, with values of 35.68, 32.42, 34.77, 35.04, and 33.38 in psa-1 and 39.54, 35.92, 38.52, 38.72, and 36.99 in psa-2, respectively, at the end of the simulation period. in the meantime, the avpc is the most significant one among the five variables with an increase of 8.52% in psa-1 and 20.26% in psa-2, increasing from 32.88 (the baseline run) to 35.68 and 39.54, respectively; then followed by avc, avcb, avhrm, and avib. a possible reason for these simulation results is that the increased promotion opportunities drive the construction participants to work more actively in such ways as to improve their special skills and to have more of an initiative to participate in project meetings and activities, which are likely to improve ocb and contribute to a better performance of megaprojects."
"at the other end of the tradeoff, cassandra [cit] and dynamo [cit] are key-value storage systems offering eventual consistency, while pnuts [cit] offers the slightly stronger per-record timeline consistency. other systems provide stronger but still relaxed semantics to achieve low-latency. cops/eiger [cit] offer causal+ consistency where write conflicts are resolved deterministically. these systems do not support general transactions and moreover cops/eiger require replication of all data across all datacenters. walter provides parallel snapshot isolation [cit] and gemini provides red/blue consistency [cit] . apart from weakened semantics, the latter two systems do not have a scalable design within a datacenter."
"complete construction of the sc-graph. with the above ideas, we modify the naive construction of the sc-graph (section 4.1) as follows. first, we omit system chains and only consider user chains when adding c-edges. a user chain may read from derived tables but can never directly modify them. thus, two hops from different (instances of) user chains have a c-edge between them iff (1) both hops access the same base table and an access is a write, or (2) one hop reads from a derived table t and the other hop modifies a base table from which t derives. additionally, if two hops are annotated as commutative, we do not add a c-edge between them. finally, chains that are linked are included as separate chains in the sc-graph; the fact they are linked does not affect the sc-graph."
"before the development of a qualitative model, key variables were majorly identified on the basis of the literature (discussed in section 2). in this study, key variables in the proposed model are mainly two aspects, namely, the identified dimensions of ocb and its driving factors. ocb refers to project compliance behavior, innovation behavior, collaboration behavior, conscientiousness behavior, and harmonious relationship maintenance behavior. its motivations mainly involve project culture, potential promotion, corporate reputation, and public satisfaction. these variables not only serve as essential units of the model but they define the boundary of the model. additionally, in the established model, ocb is the only kind of factor that influences the performance of megaprojects, which means the time, budget, and quality in this study. the other factors are out of the scope of this study. after identifying the variables with the potential of influencing the behavior of the proposed system, a qualitative analysis was carried out to identify the interactions among the variables. figure 2, which consists of four feedback loops that determine the behavior of the system by establishing connections among variables, illustrates the conceptual model of the qualitative analysis."
"slicc is a dynamic hardware thread scheduling and migration algorithm that is programmer transparent. slicc attempts to partition on-the-fly the instruction footprint of transactions into several segments where each segment fits in the l1-i cache, but two segments do not fit together. ideally: (1) a thread will migrate to another core when it starts touching a different segment, and (2) the destination core will already have the segment cached. figure 5 shows the sequence of events that lead to thread migration. in the steady state, each core has a running thread and a hardware queue of waiting threads. using a naïve load-balancing strategy, newly arrived threads are scheduled to the least congested core (i.e., the core with the least number of waiting threads). a slicc agent at each core continuously monitors execution locally in order to determine whether (q.1) the local cache is filled-up with useful instruction blocks, if so, (q.2) whether these blocks are useful to the current thread and for how long, and (q.3) where to migrate to if needed."
"we eliminate these cycles using the origin ordering guarantee of chains. specifically, sub-chains updating identical rows in derived tables either commute or start by updating the same base table row at the same server. in the latter case, origin ordering ensures that these subchains are consistently ordered and thus need not be connected in the sc-graph. note that origin ordering cannot eliminate c-edges in user chains, because the static analysis cannot determine if two user chains start at the same server: that depends on what table shard they access, which may be determined only at run-time."
"lynx also uses the cluster storage system to synchronously replicate internal metadata across buddy datacenters. two datacenters are buddies if they are near enough to communicate with low latency, yet far enough so that one datacenter is safe from a disaster that affects the other. for example, this criterion may be met by datacenters that are a few hundred miles apart with roundtrip latencies of several ms, which is comparable to disk latencies. lynx relies on buddies only to geo-replicate some internal metadata; application data can be geo-replicated using chains across any datacenters chosen by the developer, not just buddies."
"programmers can read or write base tables (e.g., line 5 and 14); derived tables are updated only by the system. programmers can specify commutative relationships (lines 20-21 specify hops that self commute). when executing a chain, programmers can optionally indicate a session for the chain (line 31). lynx ensures that chains in a session see the writes of chains in the same session that have already returned (read-my-writes). we explain how lynx provides this guarantee in section 6.2."
"in what follows, we explain how the analysis works ( §4.1), how to improve the chances for piecewise execution ( §4.2), how to cope with the lack of external consistency ( §4. 3), and what limitations chains have ( §4.4)."
"when we naively apply the theory of transaction chopping, we find little opportunity for piecewise execution, because sc-cycles are everywhere! below, we consider the problems and propose ways to avoid these cycles."
"section 4.2.3 explained that using a partial-address bloom filter reduces the overhead of remote cache segment searching. figure 9 shows the accuracy of bloom filters of different sizes. the smallest bloom filter requires 512 bits to support evictions for a 32kb cache, with 64b blocks, and 512-sets. accuracy is measured for all cache accesses and an access is accurate if the bloom filter and the cache agree on whether this is a hit or a miss. the trend is similar for tpc-c and tpc-e. in the rest of this paper, we experiment with 2k-bits filters as their effect on performance is less than 0.5% (99.3% accuracy)."
"(q.1) is the cache full with useful blocks? as a thread starts executing on a core it may experience many misses. if the cache contains a segment that may be useful for other threads, it is best to migrate the current thread to another core. otherwise, it is best to allow the current thread to load a new segment in the cache. slicc uses a \"cache full\" detection heuristic to make this decision. initially, all caches are \"empty\". to detect whether a cache has been filled up with a segment, slicc counts the number of misses using a resettable, saturating miss counter (mc) local to each core. when the number of misses exceeds the threshold, fill-up_t, the cache is considered full. in the long run, all mcs will saturate, preventing new segments from being cached effectively due to premature thread migration. to create opportunities for loading new segments, slicc resets the mc when the core's thread queue becomes empty. the currently cached blocks are not flushed, so if a subsequent thread requires the same segment it will still find it there. however, a thread touching a new segment will be given the opportunity to cache it."
"alternatively, or if the coherence protocol is snoop-based, slicc could broadcast the missed tags as they occur and explicitly request that remote cores identify themselves. on snoop coherence systems, these requests can piggyback on the existing snoop requests. searching remote l1-i caches requires extra bandwidth on the remote caches that is proportional to the number of missed tags and cores."
"with the interrelationships underlying the identified variables defined within the causal-loop diagram, a stock-flow diagram was developed to quantify their impacts with the assistance of the vensim software. a stock-flow diagram is a more detailed illustration of a causal-loop diagram. furthermore, a stock-flow diagram consists of different kinds of icons so that the computer-based simulation can be run. to facilitate a better understanding, the proposed model, along with brief definitions of the variables within the model, is shown in figure 3 and appendix a, respectively."
"before the development of a qualitative model, key variables were majorly identified on the basis of the literature (discussed in section 2). in this study, key variables in the proposed model are mainly two aspects, namely, the identified dimensions of ocb and its driving factors. ocb refers to project compliance behavior, innovation behavior, collaboration behavior, conscientiousness behavior, and harmonious relationship maintenance behavior. its motivations mainly involve project culture, potential promotion, corporate reputation, and public satisfaction. these variables not only serve as essential units of the model but they define the boundary of the model. additionally, in the established model, ocb is the only kind of factor that influences the performance of megaprojects, which means the time, budget, and quality in this study. the other factors are out of the scope of this study. after identifying the variables with the potential of influencing the behavior of the proposed system, a qualitative analysis was carried out to identify the interactions among the variables. figure figure 1 . the research path for model development."
"single datacenter storage systems. since the network latency within a single datacenter is low (submillisecond), it is generally agreed that the storage system should provide strong consistency."
origin ordering. a naive way to provide this property would be for coordinators to execute one chain entirely before starting the next chain. this scheme has low concurrency and poor performance.
"slicc utilizes three thresholds to make thread migration decisions: fill-up_t, matched_t and dilution_t. this section explores their effect on l1 cache misses and overall performance. as defined in section 4, fill-up_t sets the threshold for the initial fill-up period for an l1-i cache, during which instructions are brought in until the cache is almost full. when the miss counter (mc) is lower than fill-up_t, a thread is not allowed to migrate. matched_t sets the minimum number of tags that should be found on a remote cache before a thread migrates to it. larger matched_t limits migration, while smaller values trigger too frequent migrations. dilution_t is the minimum number of misses in the last 100 accesses to allow migration. it tends to restrict migration to the cases when more frequent misses are observed by a thread. the parameter choices could be thought of as a 3d space. to simplify, we first keep dilution_t value at zero, and explore the parameter space of fill-up_t and matched_t. in addition, we assume zero-overhead to search for remote tags. we later model an actual search mechanism. figure 7 reports i-mpki, d-mpki and performance relative to the baseline as a function of fill-up_t and matched_t. the fill-up_t values shown correspond to fractions of the l1-i cache capacity (512 cache blocks): 1 ⁄4, 1 ⁄2, 3 ⁄4, and one. the matched_t range shown is 2 − 10; larger matched_t values further degrade performance. slicc reduces instruction misses and increases data misses. since instruction stalls, for oltp workloads, account for 70% of overall cycle stalls [cit], reducing instruction misses has a major effect on performance."
"the results show that slicc is not sensitive to different values of fill-up_t. fill-up_t is actually a proxy for warming-up the caches; it affects only the first migration from a core. thus with more migrations, the effect of fill-up_t diminishes. tpc-c and tpc-e transactions have large instruction counts and migrations. figure 7 demonstrates that for matched_t values larger than four, performance benefits drop. on the other hand, although the overall mpki at two is lower, performance at four is higher due to fewer migrations, and thus, lower overhead."
"transactions of canonical oltp systems are randomly assigned to worker threads, each of which usually runs on one core of a modern multi-core system. the instruction footprint of a typical transaction does not fit into a single l1-i cache, thus thrashing the cache and incurring a high instruction miss rate. although l2 and l3 caches are growing in size, today's technology and cpu clock cycle constraints prevent deploying l1-i caches larger than 32kb. as this work demonstrates, the instruction footprint of a typical oltp transaction fits comfortably in the aggregate l1-i cache capacity of modern many-core chips. provided that there is sufficient code reuse, spreading the footprint of transactions over multiple l1-i caches would reduce instruction cache misses. fortunately, as corroborated by our experimental results, oltp workloads exhibit a high-degree of instruction reuse both within a transaction and across concurrently running transactions [cit] . this paper proposes slicc (self-assembly of instruction cache collectives), a hardware technique that utilizes thread migration to minimize instruction misses for oltp workloads. slicc divides the instruction footprint of a transaction into smaller code segments and spreads them over multiple cores, so that each l1-i cache holds part of the instruction footprint. as part of this process the l1-i caches self-assemble to form a collective that reduces the instruction misses for this transaction and other similar ones. slicc exploits intra-and inter-thread instruction locality in two orthogonal ways: (1) a thread looping over multiple code segments spread over multiple caches observes a lower miss rate (as opposed to a conventional system in which each segment would evict the others from the cache), thereby avoiding thrashing. (2) a preamble thread effectively prefetches and distributes common code segments for subsequent threads, thereby reducing the total miss rate. as execution progresses, old cache collectives are naturally disassembled and new ones are formed to hold the footprints of new transactions."
"to befriend users a and b, the application must create two friendship edges and two new-friend activity announcements, one for each user. a naive design uses a chain with four user hops, two for inserting into graph, two for inserting into activities. this chain creates an sccycle with c-edges from each graph insertion hop to the one-hop read-activity chain that reads the secondary index of graphactivities. to avoid this cycle, we break the befriend chain into three linked chains: one chain inserts the friendship edges, two chains each insert once into activities. the first chain still has an sc-cycle with the unfriend chain and itself. we break this cycle by making the insertion/deletion of friendship edges a commutative operation: we use a counter column in the graph table, and we increment/decrement the counter to insert/delete edges. this is similar to the counting sets in walter [cit] ."
"lynx provides serializability with low-latency in geodistributed storage systems. the key insight is to express transactions as chains with multiple hops, and then perform a global static analysis of the chains, to find conflicts and determine when chains can execute piecewise without violating serializability. chains are also useful for implementing several features: secondary indexes, materialized join views, and geo-replication. we demonstrated the use of lynx in an auction service, a microblogging service, and a social networking site."
"during thread migration from core-a to core-b, three possible scenarios lead to extra data misses that would not have occurred otherwise: (1) a thread may read data on core-b that it fetched on core-a (extra misses on core-b for the same data blocks), (2) data writes on core-b to blocks fetched on core-a lead to invalidations that would not have occurred without migration (extra misses on core-b and invalidations on core-a), and (3) when a thread returns to core-a, it may find that data it originally fetched has since been evicted by another thread, or invalidated by itself (extra miss on core-a). section 5.6 shows that instruction misses are more expensive than data misses performancewise. most data misses that result from migrations are served on-chip, allowing out-of-order execution to mostly absorb their latency. figure 10 reports the d-mpki for all three slicc variants and shows that slicc-sw incurs an increase in d-mpki of 11%, 1% and 4% over the baseline for tpc-c-1, tpc-c-10 and tpc-e, respectively. the other two variants exhibit a similar trend in d-mpki increase. there is less locality and sharing in the larger data set of tpc-c-10, reducing the d-mpki overhead when migrating."
"to execute the chain piecewise, the coordinator serially executes each hop of the chain, by invoking the appropriate server (the first server is local) and waiting for a completion acknowledgement. after the first server executes its hop, the coordinator returns an indication of first-hop completion to the client library. then, if the server executed a hop that modified data, it spawns in parallel sub-chains to update derived tables, if any. these sub-chains are coordinated by the server and execute like any other chain-in particular, lynx ensures origin ordering based on where the sub-chains start. the server waits for the sub-chains to complete before sending an ack to the coordinator of the higher-level chain. if a chain cannot execute piecewise, the coordinator executes it as a distributed transaction using standard two-phase locking and two-phase commit [cit] ."
"without thread migration multiple requests would be repeatedly sent out for the same cache block from multiple cores. with thread migration, an instruction cache block is, under the ideal scenario, requested only once, and reused multiple times."
"(q.2) are the current cache contents useful to this thread and for how long? when running a thread on a full cache, slicc tries to determine whether the thread is going over the cached segment, or whether it is about to move to a new segment. for this purpose slicc measures miss dilution, that is, the recent frequency of misses (detailed in section 4.2.2). if miss dilution is low, then slicc predicts that thread is only temporarily diverting away from the cached segment. since the thread will converge again soon, it is best to not migrate to benefit from the forthcoming instruction reuse. if miss dilution is high, then slicc predicts that the thread is moving to a different segment. if it continues execution on this core it will evict useful cache blocks, which could be reused by other threads. slicc predicts that it might be better to migrate the thread elsewhere. the question at this point becomes where to go? (q.3) where to migrate to? ideally, slicc would migrate a thread to a cache that has the thread's next segment. slicc attempts the following in order: (1) if the thread is going to touch a code segment that is available on another core, the thread migrates there. (2) otherwise, the thread migrates to an idle core, if any. (3) the thread stays put. in the last case, migrating the thread would incur overheads and would evict remotely cached segments that may be useful for other threads. slicc opts for incurring the instruction misses locally avoiding the migration overhead."
"the techniques we described do not ensure external consistency or order-preserving serializability [cit] . order-preserving serializability requires that if a transaction commits before another one starts, the first appears before the latter in the equivalent serial order. the analogous property for chains does not hold: a client may submit chain t 2 after chain t 1 returns (after committing t 1 's first hop), but t 2 may be serialized before t 1 ."
"it turns out that giving up serializability for low-latency is unnecessary. this claim is predicated on two observations. first, typical web applications run a predefined set of transactions, so it is possible to perform a global static analysis of its transactions before execution, to find opportunities to execute them quickly without violating serializability. second, one can decompose a general (geo-distributed) transaction into a sequence of hops, each modifying data in only one server. with the aid of static analysis, one can safely run these hops as separate transactions while preserving serializability, and return quickly to clients after the first hop (often in the local datacenter)."
"(3) an entire datacenter is destroyed or becomes unavailable beyond a time threshold. in this case, the system first recovers the lynx servers using geo-replicas and reconstruction, as described in (1). then, the system recovers from crashed coordinators, as described in (2)."
"recall that system chains are generated internally by lynx to update derived tables. there are three types of system chains, one for each type of derived table."
"slicc reduces the instruction misses for oltp by 56% on average at the expense of an 5% average increase in data misses. slicc improves the overall performance by 68% on average over the baseline and performs better when the input database is larger. compared to a state-of-the-art instruction prefetcher (pif), slicc improves performance by 21% for tpc-e and comes within 2% for tpc-c, with only 2.4% of relative area overhead. when tested on mapreduce, a cloud workload that has a relatively small instruction footprint, slicc was robust and did not affect the l1 miss rates or performance."
"social networking. the l-social application implements the basic operations of a website like facebook (e.g., befriending users, posting to walls). l-social has 5 base tables: graph, status, users, wall, activities. there is one join table graphactivities with a secondary index to allow a user to read her friends' activities quickly, with one lookup to the secondary index."
"there are two limitations in the current design of ltwitter. first, when user x starts to follow y, the underlying join chain inserts all of y's existing tweets into x's timeline (the secondary index of graphtweets). it would be better to insert only y's recent tweets. this can be done adding a selection operation to the join view, to filter out old tweets with a smaller timestamp than the follow edge timestamp. supporting such selection operations in lynx is future work. second, when a user with many followers tweets, there are large overheads to update their followers' timelines. thus, l-twitter's current push-based approach should be combined with pull-based queries for users marked as popular [cit] ."
"the remaining of this document is organized as follows. section 2 analyzes the nature of the problem and section 3 sets the requirements for an ideal solution. section 4 describes the automated thread ϯϭ ϯϭ ϰϭ ϱϭ ϲϭ ϭϲ ϯϯ ϲϰ ϭϯθ ϯϱϲ ϱϭϯ ϭϲ ϯϯ ϲϰ ϭϯθ ϯϱϲ ϱϭϯ ϭϲ ϯϯ ϲϰ ϭϯθ ϯϱϲ ϱϭϯ ϭϲ ϯϯ ϲϰ ϭϯθ ϯϱϲ ϱϭϯ ϭϲ ϯϯ ϲϰ ϭϯθ ϯϱϲ ϱϭϯ ϭϲ ϯϯ ϲϰ migration algorithm slicc. section 5 demonstrates experimentally the performance benefits of slicc. sections 6 reviews related work, while section 7 presents our conclusions."
"with the interrelationships underlying the identified variables defined within the causal-loop diagram, a stock-flow diagram was developed to quantify their impacts with the assistance of the vensim software. a stock-flow diagram is a more detailed illustration of a causal-loop diagram. furthermore, a stock-flow diagram consists of different kinds of icons so that the computer-based simulation can be run. to facilitate a better understanding, the proposed model, along with brief definitions of the variables within the model, is shown in figure 3 and appendix a, respectively."
we now explain how lynx provides the properties of chains ( §3) when chains execute piecewise. figure 9 gives a summary. these techniques are efficient as they require little or no coordination across servers.
"the analysis is based on the theory of transaction chopping, originally developed for breaking up large trans-actions into smaller pieces in centralized database systems [cit] . the chopping algorithm takes a set of chopped transactions and constructs a graph, which we call scgraph, where vertices represent transaction pieces and edges represent relationships between pieces. there are two types of edges: s-edges connect vertices of the same unchopped transaction, c-edges connect vertices of different transactions if they access the same item and an access is a write. an sc-cycle is a simple cycle containing a c-edge and an s-edge ( figure 2 ). it is shown that serializability is assured if the sc-graph has no sc-cycles [cit] . intuitively, an sc-cycle indicates a non-serializable interleaving. for example, figure 2 (a) allows the problematic interleaving"
"creating and using chains. all operations are performed using chains. figure 7 shows the chain for placing a bid using lynx's javascript api. the chain has two hops, one to insert the bid (line 3) and another to update the current highest bid price of the item (line 10). each hop has access to the chain's context (ctx) which contains input arguments of the chain. lynx exposes relational tables as auto-generated table objects whose names start with '@'. this syntax simplifies the static analysis tool that generates the sc-graph. since '@' is not allowed in javascript identifiers, it is removed before execution."
"megaprojects are defined as large-scale and complex ventures that typically cost $1 billion or more and take many years to develop and build; these projects generally involve multiple public (national interested) and private stakeholders with transformational impacts on millions of people [cit] . megaprojects are not just magnified versions of smaller projects, but are comprised of interdependent subsystems aiming to achieve a comprehensive socio-economic development [cit] s, megaprojects have become an emerging area in the field of construction project management due to global urbanization, urban revitalization, and redevelopment [cit] . however, most megaprojects suffer from obsolete performance such as high risks, cost overruns, and schedule delays."
"focusing on the other workloads, slicc-sw reduces i-mpki more than slicc or slicc-pp. compared to the baseline, slicc-sw reduces i-mpki by 56% and 61% for tpc-c and tpc-e, respectively. i-mpki reductions are slightly lower with slicc-pp, more so for tpc-e than tpc-c, for the following reason: slicc-pp devotes one core to preprocessing. given the transaction mix and transaction footprint sizes, having that extra core can be more important. all three variants of slicc are sometimes forced to overcommit the caches by concurrently running transactions whose aggregate footprint does not fit on the total available l1 cache capacity. when overcommitting the caches, it is best to use stray threads since little opportunity for instruction reuse is lost when overcommitting with stray threads (a stray thread by definition is one that has few, if any, other ready threads sharing the same footprint). overcommitting with non-stray threads happens more often for tpc-e than tpc-c partly because only 3% of tpc-e threads are stray compared to 12% of tpc-c threads. furthermore, the need for stray threads is higher for tpc-e than tpc-c; slicc spreads the transactions of tpc-e across 8 − 10 cores, while tpc-c's transactions are spread across up to 14 cores."
"to evaluate slicc, we execute two popular transactional benchmarks, tpc-c [cit] and tpc-e [cit], as well as a mapreduce [cit] cloud workload. our experiments show that, on average, thread migration eliminates 56% of the l1 instruction misses resulting in a 68% overall performance improvement over the baseline (described in section 5.1). compared to pif [cit], a state-of-the-art instruction prefetcher, slicc improves performance by 21% for tpc-e and comes within 2% for tpc-c, with only 2.4% of relative storage area overhead. slicc is also robust as it does not affect the performance of mapreduce [cit], a cloud workload, which has a relatively small instruction footprint. in summary, this paper makes the following contributions:"
"t4-t5 that access different code segments benefit as well if they get assigned to a different set of cores, avoiding conflicts with t1-t3. this process applies to all subsequent threads: if they touch a code segment that exists in some l1-i cache, they migrate to the corresponding core and avoid missing for these segments."
"prior to performing the simulation, it is necessary to ensure that all the variables in the model can be quantified. the variables used in this model were divided into three categories, namely, constant, dependent, and qualitative variables. each type of variable has corresponding data sources. the value of the constant variables-which are expected to remain unchanged and will not be affected by other variables during the whole simulation period-were generally quantified by referring to the materials available, such as literature. the values of the dependent variables depend on one or more variables within the model in terms of mathematical functions. the values of these kind of variables can be quantified by various functions in the vensim software [cit] . the values of the qualitative variables were quantified, in this study, by the judgement of experts. [cit] in shanghai and lasted around 15 min. the expert group consisted of five managers, each with 5 to 10 years of working experience in construction project management. to facilitate understandings in the process of quantification, detailed data and equations related to the established model are shown in appendix b. on one or more variables within the model in terms of mathematical functions. the values of these kind of variables can be quantified by various functions in the vensim software [cit] . the values of the qualitative variables were quantified, in this study, by the judgement of experts. [cit] in shanghai and lasted around 15 min. the expert group consisted of five managers, each with 5 to 10 years of working experience in construction project management. to facilitate understandings in the process of quantification, detailed data and equations related to the established model are shown in appendix b."
"configuration service. lynx relies on a separate configuration service to maintain the mapping from each shard to its replica group. our design of this service follows other systems [cit] . nodes consult the service to determine the server responsible for a given shard. this information is subsequently cached. each server obtains a lease for its responsible shards and rejects requests des- figure 8 : lynx client library and server processes. the client dispatches chains using rpcs. the server process receives chains, queues them, and executes them against a local database. the server process also implements georeplication, secondary indexes, and materialized join views using system chains."
"based on sd modelling, both a causal loop diagram and a stock-flow diagram were proposed to identify the major variables and to describe their interrelationship. once the model was validated, three policy scenarios including two single policy scenarios and a multi-policy scenario, were adopted to simulate the performance of megaprojects under various levels of the ocb adoption in megaprojects. the simulation results indicated that an increase in the airpp carries out more obvious effects on the improvement in ocb and in the performance of megaprojects than those of the airpc. moreover, the simulation results of the multi-policy scenarios show that the higher the value of the airpp in combinations (the total value has been restricted), the higher the value of ocba and the project performance would be. thus, a finding that highlights the importance of considering improving airpc first, especially when resources are limited."
"permission to make copies of part or all of this work for personal or classroom use is granted provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for third-party components of this work must be honored. for other uses, contact the owner/author."
"to recover a lynx server, the system can optionally store the server's data in a cluster storage system within the datacenter. in that case, recovery is simple: the system starts a new server and reconfigures the replica group to replace the old server with the new one. the new lynx server operates on the same data as the old server using the cluster storage system."
"the pdi operation is related to the synchronization mode of ethercat system. in the periodic communication mode of ethercat, there are three modes of synchronization, which are freerun, sm-synchron and dc-synchron respectively. in freerun mode, the slave does not generate interrupt signal on spi_irq. and the local host controller generates interrupt in itself to access esc periodically. in sm-synchron mode, the event of local controller is synchronized as data exporting or entering. the spi slave will generate interrupt signal on spi_irq to trigger the dsp to operate process data when esc receives the frame from the ethercat master. in dcsynchron mode, the event of local controller is synchronized by sync event that is controlled by event of distributed clocks system of ethercat. based on distributed clocks, all of the escs will generate sync event simultaneously. however, the process data must be exchanged before the operation of local controller in this mode."
"we perform two sets of experiments, one without georeplication, one with geo-replication factor of two. even in experiments without geo-replication, data is spread over the three ec2 regions."
"this section examines the oltp memory behavior that motivates thread migration for instruction cache miss reduction. the analysis targets tpc-c [cit] and tpc-e [cit], which resemble state-of-theart commercial oltp applications. we contrast their behavior with mapreduce [cit], a data center workload which has a smaller instruction footprint. we find that for oltp transactions: 1. most instruction misses are due to limited cache capacity, whereas most data misses are compulsory. 2. the instruction footprint of most transactions would fit in the aggregate l1 instruction cache capacity of even small scale chip multiprocessors (eight cores). the same is not true for data footprints. 3. existing non-lru cache replacement policies reduce the instruction miss rate, but only by a fraction of what would be possible with larger caches. 4. there is intra-thread locality, but over code regions that are larger than a typical l1 cache size. 5. there is significant inter-thread locality, particularly across threads of the same transaction type."
"there are two ways to address this issue, if necessary. first, there is a barrier operation that blocks a client until its outstanding chains complete. this is analogous to memory barriers in multiprocessor systems, which allow programmers to enforce ordering when necessary. for example, the operation to change a user's privacy settings should be followed by a barrier. doing so is akin to enforcing application-defined explicit causality rather than every possible causality [cit] . second, we can provide the simple guarantee of read-my-writes [cit], which in our setting ensures that a client sees the entire effects of her previous chains (even if they return early), a useful property in practice. we explain how lynx ensures this property in section 6.2."
"to detect which, if any, remote cache has the next segment, slicc uses a short sequence of matched_t number of tags of recent misses, predicting that they form the preamble of the next segment. conceptually, once slicc decides to try to migrate a thread, it searches all remote l1-i caches for these recently missed tags. section 4.2.3 explains how this search can be implemented including an incremental method that uses the existing coherence protocol responses. figure 5 summarizes the execution stages of a thread on a core until it migrates, or completes execution. figure 6 shows that slicc's implementation comprises: (a) a cache full detector, (b) a miss dilution tracker, and (c) a remote cache segment search unit. slicc uses hardware thread migration, and thus, interacts with the os as section 4.4 explains in more detail. the three aforementioned units, described subsequently, track all cache accesses, including speculative ones."
"geo-distributed storage. prior geo-distributed systems face the unpleasant tradeoff between strong semantics and low latency. spanner provides strong semantics with order-preserving serializable transactions [cit], but these are expensive: like its predecessor megastore [cit], spanner's update transactions take many cross-datacenter roundtrips to execute and commit. replicated commit [cit] and mdcc [cit] are faster but still incur crossdatacenter latency to execute and commit transactions."
"chains are implemented by the lynx client library and server process (figure 8) . the client dispatches a chain to its first hop, at a server storing the data accessed by the hop. if the first hop writes data, the client chooses the server in the shard's home datacenter; otherwise, it chooses a server in a nearby datacenter that has a replica."
"the message overhead for enforcing origin ordering is low: the number of sequence numbers attached to a chain is proportional to its length. origin order may sometimes introduce latency overheads, but this is the behavior we desire for consistency. specifically, if two chains start at the same server and follow different paths before overlapping again at another server, the first chain may delay the second chain."
"based on our preceding discussion we identify three requirements for slicc. slicc should dynamically detect: (a) when a thread should migrate, i.e., when is the current cache full; and (b) where the thread should migrate to, i.e., which remote cache, if any, holds the code segment the thread will touch next. since transactions vary in their control flow, slicc should (c) not impose any specific pipelining, i.e., it should not restrict similar threads to follow the exact same path."
"there are a lot factors that contribute to the variation of the performance of megaprojects. academic research and industrial practices of megaprojects have shown that, apart from construction technologies and managerial strategies, a high degree of subjective initiatives and creativities is equally important to affect project performance [cit] . for example, participants' (for example, construction workers) willingness to work in extreme conditions is key to ensure the accomplishment of megaprojects [cit] . this kind of phenomenon is a typical behavior known as organizational citizenship behavior (ocb) in the context of megaprojects, referring to the positive behavior of participants in megaprojects, which contributes to the achievement of the construction projects' goals, but has not been directly or explicitly stipulated in formal contracts [cit] . having originated in the field of organizational behavior, ocbs have been drawing increased attention in the context of construction management and sustainability management [cit], particularly in areas such as governance and so forth. although existing studies have demonstrated that ocb positively contributes to project performance [cit], two problems are still found in previous studies. firstly, most of existing papers have studied the relationship between ocb and project performance from a static and isolated perspective without a systematic consideration to the interrelationships among different influencing factors. secondly, the existing studies have mainly concentrated on qualitative analyses based on surveys, failing to quantify the impact of ocb on project performance."
"slicc exploits intra-and inter-thread locality. (1) it virtually increases the l1-i cache capacity observed by a thread; thus, it improves locality within a thread. (2) it pipelines similar threads, such that one thread fetches instruction cache blocks that are reused by many threads."
"to improve the accuracy of gait recognition, we conducted continuous research on gait identification based on smartphone accelerometers, and the results are mainly reflected in the following three aspects."
"experimental results show that the new feature has a good recognition effect for the five base classifiers when using the test set; the highest accuracy rate achieved was 88.35%, and the relieff value of the new feature is also higher than the traditional feature value. these results suggest that the new feature has good separability. new features and the traditional feature fusion also achieved a high recognition rate on the test set, and, compared to the single traditional feature recognition rate, the highest recognition rate reached was 95.47%. after the msv algorithm fusion, the recognition rate increased by 1.34% on average compared to the maximum recognition rate for the five base classifiers, and the highest recognition rate was 98.42% for the fusion features. for the feature set that combined new and traditional features, compared to other multiclassifications' fusion algorithms, the msv fusion algorithm increased by an average of 2.33%."
"feature extraction is used to extract feature vectors that can represent individual identities from the obtained single gait period, which is the key in classification recognition technology. the effect of feature extraction will directly affect the accuracy of classifier training and final classification recognition."
"we now describe how we combine these various cues to produce a combined confusion map. intuitively, the optimal combination scheme would be one that generates a recommendation map that assigns high values to regions that a user would scribble on, if they were to exhaustively examine all segmentations. users typically scribble on regions that are incorrectly segmented. we cast the problem of learning the optimal set of weights for our cues, as that of learning a linear classifier (logistic regression) that maps every superpixel (represented by a 7-dimensional feature vector corresponding to each of the 7 cues described above) to the (binary) segmentation error-map. our cue combination scheme is illustrated in figure 3 ."
"first, the linear correspondence between points on two sets of sequences is calculated using ltn, from which the point matching relationship of figure 5 (a) is obtained. then, the shape context histogram of each matching data point is calculated using the shape context, and the point matching relationship of ltn is verified again to accurately match each point on two sets of sequences. specific steps for using the scltn algorithm to extract typical periods are as follows."
"the vmware esx server, the xen hypervisor, and the microsoft hyper-v hypervisor are examples of popular vm systems. the vmware esx server hypervisor was principally designed to support full virtualization. the xen [cit] hypervisor started with the concept of para-virtualization, but currently also supports full virtualization. the microsoft hyper-v hypervisor also supports full virtualization. the concepts developed in this article apply equally well to all these vm systems because all share the fundamental concept of multiplexing many virtual resources on fewer physical resources; it is the multiplexing of virtual processor cores over real processor cores that creates a fundamental runtime problem of correctness and runtime efficiency of pdes over vms."
"to realize the pdes optimized hypervisor scheduler, we require (a) each μsik kernel instance to independently communicate its lvt value to the xen scheduler, and (b) a new xen hypervisor scheduler that utilizes the lvts to optimize the compute resource lvt value for every vcpu dynamically supplied by application; initialized to 0 sharing specifically for pdes applications. also, note that the pdes specific hypervisor scheduler should be active only during the execution of the pdes application. to realize this, we need at least two different modes of hypervisor scheduler operations. we refer to the mode of operation during pdes execution as simulation mode, which otherwise is referred to be in a normal mode of operation."
"as can be observed in fig. 5 and fig. 7, the reconstruction can reduce the deformation caused matching distance so that the intra-class matching distance can be reduced. at the same time, however, the inter-class matching distance may also be reduced. fig. 8 shows some examples. one can see that the imposter inter-class matching distance is also reduced after the query sample is reconstructed (global reconstruction with l 2 -regularization is used). what we expected is that the intraclass genuine matching distance can be reduced more than the inter-class imposter matching distance, but there is no guarantee for such an ideal situation. if we directly apply the compcode scheme [cit] to the reconstructed image for verification, incorrect decision can be made. to make this clear, we apply compcode to the original images and the reconstructed images, respectively, for fkp verification. the equal error rates (eer) are shown in table i, from which we can see that using only the reconstructed images for verification leads to worse performance because some useful texture features are smoothed out in the reconstructed images. for a more robust and accurate verification, the matching scores (or distances) of both the original image y and the reconstructed imageŷ should be considered for decision making."
"in order for real-time recognition, compcode uses three bits to represent each orientation [cit] . for matching two compcode maps p and q, the normalized hamming distance based angular distance is commonly adopted [cit] :"
"where represents the weighting coefficient threshold of the th base classifier for the class sample. after the number classifier classifies class samples, the classifier threshold with the highest credibility is set to the average credibility, and the weighting coefficient thresholds of the other − 1 base classifiers are set to 1-average credibility."
"during simulation initialization, the μsik library sets switch scheduler in shared inf o of its host dom to true, using our system call. the scheduler reads this variable to change its mode of operation from normal mode to simulation mode. similarly, during the termination of simulation, the switch scheduler is set to false, suggesting to the scheduler to revert back to its normal mode of operation. the same system call is used by the μsik library to write its sl's lvt value to simtime of the shared inf o of host dom."
"to be fair, we should note that what allows us to set-up an efficiently solvable energy function is our incorporation of a user in the co-segmentation process, giving us partially labelled data (scribbles). while this user involvement is necessary because we work with globally related images, this involvement also means that the co-segmentation algorithm must be able to query/guide user scribbles, because users cannot be expected to examine all cutouts at each iteration. this is described next."
"mode. the hypervisor switches to the simulation mode after the pdes execution is started on all vms. each pcpu maintains an rq (priority queue) as shown in figure 4 and, in the simulation mode, psx en-queues the vcpus to be scheduled in llf priority. we use the lvt as the vcpu priority; the lower the sim time (vcpu data structure; figure 4 ), the higher is its priority in the rq, hence the earlier it is picked by psx to allocate compute resource. the scheduler allots a tick amount of pcpu cycles for the selected vcpu. also, note that the sim time corresponding to the vcpus of the dom0 is always maintained to be lower than that of other vcpus regardless of the psx's mode of operation. this guarantees that dom0 vcpus are always preferred over the other vcpus, which in turn ensures a good and responsive user interactivity with dom0 before, during, and after pdes executions."
"it can be concluded from tables 4 and 5 that the selected medium-based classifiers in table 5 have better recognition effects in both test set and verification set; the recognition rate is almost the same, showing that the selected base classifier has a good stability."
"in pdes, the lp with the least value of local virtual time (lvt) affects the progress of the entire simulation. hence, by prioritizing the vcpu that hosts the sl with the least lvt value, the runtime performance can be optimized. to achieve this, the sls on different vms need to communicate the lvt values to their corresponding vcpus, so that the hypervisor scheduler can use that information to schedule the vcpus onto the pcpus. figure 2 shows the system architecture of a hypervisor-based execution platform customized for pdes applications. in the figure, for clarity, only a single lp is shown per sl and a single sl is shown per vcpu. however, any number of lps per sl and any number of sls per vcpu can be supported by our approach with simple modifications. as illustrated in figure 2, the lvt of an lp is passed to the vcpu in its vm. the vcpu records the lvt inside the hypervisor's scheduler data structures as the vcpu lvt. the hypervisor scheduler uses the lvt values of the vcpus in a least-lvt-first (llf) strategy during scheduling. with the llf scheduling policy, the scheduler gives the highest priority to the vcpu with least the lvt value."
"if we do want to update the dictionary d when new enrollments come, there are two strategies to save cost. first, we can update d once a batch of new enrollments is available. second, we can let the new dictionary be"
"according to the relieff algorithm, the larger the relieff value corresponding to a feature, the greater the distinguishing ability of the feature. table 4, compared to the average 6.44% increase in recognition rate for traditional features, the combination of new features and traditional features has a significant increase in the recognition rate of all candidate classifiers. the highest recognition rate can achieve 95.47%, indicating that feature fusion is very successful."
"where s is the area of the code map, p i (q i ) is the i th bit plane of p (q), and ⊗ represent the bitwise \"exclusive or\" operation. in practice, multiple matches are performed by translating one of the two feature maps vertically and horizontally, and the minimum matching distance is regarded as the final angular distance. nonetheless, the compcode scheme is sensitive to the variation of fkp image and then resulting in false rejection. even a small rotation and misalignment can lead to an incorrect matching."
"0.3761 and 0.3631, respectively. one can see that the global reconstruction can reduce the genuine matching distance, while the patched based reconstruction can further reduce the matching distance. though imposter matching distance will also be reduced after reconstruction, we will see that with an adaptive binary fusion strategy proposed in next section, more accurate verification results can be obtained by adaptively fusing the matching distances before and after reconstruction. in the experimental results in section 6, we will also see that patch based reconstruction can lead to better verification performance."
"(2) implement multiclassifier screening. the purpose of multiclassifier screening is to select the most suitable classifier among eight alternative classifiers for fusion. to complete the screening of multiclassifiers, this paper mainly demonstrates the method through following three, small experiments."
"combined with figures 6 and 7, it can be concluded that the collected gait data are repeated during the standing period and the swing period, so the data collected under ideal conditions should have strict periodicity. however, due to many problems such as sampling equipment and environment, the collected gait data presents the features of quasiperiodicity. although it loses perfect periodicity, it also retains more uniqueness for the individual, and individual features can be excavated from more data."
"a hypervisor scheduler time-multiplexes many vcpus and on to few pcpus. to accomplish this, each pcpu maintains a run-queue, which is a list of vcpus that share the pcpu resources, as shown in figure 2 . the hypervisor scheduler follows a systemdefined scheduling policy for dynamically mapping the vcpus onto pcpus. a fairshare policy is usually adopted by the default hypervisor schedulers to suit a wide range of workloads."
"we first analyze the informativeness of each of our 7 cues. we start by generating a foreground and background scribble on a random image in a group. we then compute each of our cues, and treat each individual cue as a recommendation map. we generate the next synthetic scribble as guided by this recommendation map. we repeat this till we have scribbled about 1000 pixels across the group, and compute the average segmentation accuracy across the images of a group. 4 we rank the 7 cues by this accuracy. figure 5 shows the mean ranks (across groups, average of 10 random runs) achieved by these cues. out of our cues, the graph-cut cue (gc) performs the best, while both distance transform (dt) and intervening contour (ic) are the weakest."
"we compare the proposed reconstruction based fkp verification method with abf, denoted by \"r-abf\", with stateof-the-art fkp verification methods, including the improved compcode (imcompcode) [cit], blpoc [cit], lgic [cit] and the local feature integration (lfi) method [cit] . considering that the lgic scheme is a combination of the compcode (which employs the image local orientation features) and the blpoc (which employs the image global fourier transform features) methods, for fair comparison we will first compare r-abf with imcompcode, blpoc and lfi, and then couple r-abf with blpoc and compare it with the lgic method."
"it can be concluded from table 4 that eight alternative classifiers obtained better recognition rates whether used in traditional features or new features, or traditional features and new features, which shows that all 8 classifiers have excellent stability. it can be concluded from figure 10 that classifiers c7 and c8 require much training time, and classifiers' time consumption will be greatly integrated if it is used for classifier fusion. therefore, considering both time consumption and accuracy, c2, c3, c4, c5, and c6 are selected as the base classifiers of the fusion algorithm in this paper."
"although the experimental results show that these proposed features can be effectively classified and identified, these types of features are still relatively traditional timefrequency domain features, and motion features of gait have not been studied."
"a query sample y and the reconstructed images of it with l 2 -regularization.ŷ g is the output of global reconstruction whileŷ p is the output of patch based reconstruction. the matching distances between a gallery image (which is from the same class as y) and y,ŷ g andŷ p are 0.3783, 0.3761 and 0.3631, respectively. one can see that the global reconstruction can reduce the genuine matching distance, while the patched based reconstruction can further reduce the matching distance."
"this paper presented a novel reconstruction based fingerknuckle-print (fkp) verification method to reduce the false rejections caused by finger pose variations in data collection process. for an input query image whose matching distance falls into the uncertain interval, we reconstructed a new version of it by using a dictionary learned from the gallery set. then a new matching distance can be obtained. an adaptive binary fusion (abf) rule was then proposed to fuse the two matching distances for the final decision making. the proposed reconstruction based fkp verification with abf, denoted by r-abf, can effectively reduce the false rejections without increasing much the false acceptances. our extensive experimental results demonstrated that the r-abf can result in much lower equal error rate than existing state-of-the-art methods."
"every μsik sl maintains a variety of simulation times based on its event-processing state at any given moment. they are distinctly classified into four classes: committed, committable, processable, and emittable [cit] . we can transmit any of these lvt values to the hypervisor. in practice, we observed that the use of the earliest committable time stamp resulted in better performance than the others, and hence, this simulation time value was used in all our experiments."
the pdes scheduler for xen (psx) scheduler replaces the default csx in scheduling the vcpu onto the pcpu. the strategy that we use to replace the scheduler is similar to the one presented by yoginath and perumalla [cit] . ps private (figure 4) is the structure of the global data-structure object that the psx scheduler maintains. the switch sched variable of the ps private object is updated during vcpu scheduling after the corresponding value is read from shared inf o by the vcpu from its relevant dom. setting the switch sched in the psx's ps private global variable enables psx to switch from normal mode to simulation mode and vice versa.
"to efficiently communicate lvt to the hypervisor, support from the guest-os kernel is necessary. the guest-os kernel should also impart the obtained application-level information to the hypervisor. further, the pdes μsik library should also be modified to communicate the change in hypervisor scheduler mode of operation, and the lvt value, to the guest-os kernel."
"the xen hypervisor is a popular open-source industry standard for virtualization, supporting several architectures including x86, x86-64, ia64, and arm, and guest os types including windows, linux, solaris, and various versions of bsd os. figure 1 shows a schematic of guests running on the xen hypervisor. xen refers to vms as guest domains or doms. each dom is identified by its dom-id. the first dom, dom-0, possesses special hardware privileges. such privileges are not provided to other user doms, which are generically referred to as domus. each dom has its own set of virtual devices, including virtual multiprocessors called virtual cpus (vcpus). system administration tasks such as suspension, resumption, and migration of doms are managed via dom0."
"the score level fusion is a kind of combination-based approach, where the matching scores of individual matchers are integrated to generate a single scalar score for final decision making. denote by s the fusion result of s 1 and s 2 . three commonly used score level fusion methods [cit] are the simple-sum (ss):"
"in this section, we verify that the proposed abf rule is more effective than the commonly used ss, min, max and mw rules. in the experiment, the gallery set is composed of the first 165 fingers in the polyu fkp gallery set, and the probe set is composed of the first 330 fingers in the polyu fkp probe set. (other settings of the gallery and probe sets lead to similar conclusions.) that is, there are 165 classes out of the gallery set. the gallery set is used to train the dictionary d to code the samples in the probe set."
"for shortcomings of the traditional voting method, this paper proposes a multiscale voting method (msv) by combining the principles of the voting method with its arbitration principle. the basic idea of msv algorithm is as follows. when the multiclassifier cannot judge a sample category accurately using the voting method, an unknown sample should classify samples by combining different measures (such as recognition rate of each classifier for its output category and f1 score). the msv algorithm not only retains the features of traditional voting methods but also combines them with a single classifier for the recognition performance of different types of samples, which makes the advantages of each classifier complementary. the msv algorithm is mainly divided into two steps: extracting discriminants and constructing voting functions."
"we started by listing the advantages of using cloud platforms for pdes applications. we found that the runtime performance (one of the core reasons for pdes execution) of a pdes application suffered acutely on the cloud platforms. to understand and unravel the performance issues of pdes application payloads on cloud platforms, we undertook an extensive pdes performance study on a custom-built hardware with a hypervisor capable of hosting hundreds of vms. several performance behaviors, such as (a) almost similar runtime performance of pdes application over vm and native platform, when virtual and physical resources match; (b) a counterintuitive performance trend while using various compositions of vm compute resources for pdes execution; (c) monetary implications of observed performance; (d) severe performance degradation with a slight increase (10%) in the virtual to physical multiplexing ratio; and (e) better performance of pdes using smaller time slices in vcpu scheduling, were discovered."
"where i (·) is an indicator function that is 1(0) if the input argument is true(false), d ij is the distance between features at superpixels i and j and β is a scale parameter. intuitively, this smoothness term tries to penalize label discontinuities among neighbouring sites but modulates the penalty via a contrast-sensitive term. thus, if two adjacent superpixels are far apart in the feature space, there would be a smaller cost for assigning them different labels than if they were close. however, as various authors have noted, this contrast sensitive modulation forces the segmentation to follow strong edges in the image, which might not necessarily correspond to object boundaries. for example, [cit] modulate the distance d ij based on statistics of edge profile features learnt from a fully segmented training image."
"to solve this mismatch, a new algorithm has been presented for pdes-specific scheduling of vms by a hypervisor. the algorithm schedules vms primarily by their lvt order, and incorporates mechanisms that prevent deadlocks that are otherwise possible in a purely lvt-based scheduling. the new scheduler has been implemented and exercised in an actual hypervisor system (xen) that is popularly used in major cloud platforms worldwide."
"gabor filtering has been widely used as an effective feature extraction technique in face, iris, fingerprint, palmprint, as well as fkp recognition systems. a 2d gabor filter can be mathematically expressed as"
"to test the performance of traditional features of data collected in this paper and to verify new feature separability, the fusion of new features and traditional features, and the performance of msv multiclassifier fusion algorithm, the following four experiments are designed."
"(c) compare the ability to distinguish samples between new features and traditional features, fusing and classifying new features and traditional features, selecting the final suitable classifier from candidate classifiers, and determining the fusion classifier after comparing with classifiers selected by the former two ones. (3) verify the reliability of the msv fusion algorithm through experimental comparison."
"a very attractive product for both business operators and users alike arose from tapping virtualization technology through internet services, which famously came to be known as cloud computing. infrastructure as a service (iaas), platform as a service (paas), software as a service (saas) and network as a service (naas) are prominent among the types of services offered currently by the cloud computing service vendors [cit] . apart from iaas, the other services are completely unaware of the physical hardware on which they are executing. exploiting the unlikeliness of 100% resource utilization from all clients at all times, cloud operators multiplex vms on limited resources and hence are able to provide easy accessibility to large computing resources at an impressively competitive price."
"( ) period detection. gait data are periodically detected using the local minimum value [cit], which means that data contained between two adjacent minimum values are considered to be a single gait period. ( ) eliminate the abnormal period. gait data can be divided into a single gait period through the two steps mentioned above, but these periods are not all suitable to feature extraction. the abnormal period should be eliminated after filtering these periods. this paper uses dtw to calculate similar distances in the normal period, and periods with large differences in distance are eliminated."
"in the first experiment, we take d 1 as the dictionary and take the 60 new classes (6 samples per class) as the query set, and the eer is 0.8% by the proposed r-abf-g-l 2 method. if we take d 2 as the dictionary and take the 60 same classes as the query set, the eer is 0.76% by r-abf-g-l 2 in the second experiment, with d 1 and using all the 660 classes as the query set (6 samples per class in the second session), the eer is 1.21% by r-abf-g-l 2, while with d 2 and using all the 660 classes as the query set, the eer is 1.19% by r-abfg-l 2 . clearly, in both the two experiments, the eer values by d 1 and d 2 are very close, implying that there is no necessary to further update the dictionary since d 1 is already good in reconstruction."
(b) calculate the absolute error between and . the result is denoted as 2 . (c) calculate the absolute error between and . the result is denoted as 3 . sum the weighted number of votes for each sample in the database.
"for the scatter benchmark, psx performs extremely well. while csx is only a few times slower than native linux runtime, psx is very close to the native runtime performance. this is because the scatter scenario is computationally intensive, very well load balanced, and optimistic synchronization with zero-rollbacks yields very good performance. psx with its lvt-based scheduling further improves the performance. 5.4.5. variance in performance. the data points of csx showed high variance when the number of vcpus multiplexed were greater than the number of pcpus. figure 16 plots the variance of data points of psx and csx runtimes in phold benchmark, with lookahead 1. in our previous plots, we had used the best runtimes obtained using csx plots from multiple runs for comparison with the runtimes of psx. the observed behavior can be expected from csx because of the vcpu scheduling strategy it uses. in contrast to csx, the psx readings show a very low variance regardless of the number of multiplexed vms."
"therefore, the pressure on the foot is maximum at this time. and then the individual will have the force of the acceleration required for the upward movement, so that the other foot can be lifted up and swing successfully. when the individual gait is about to enter the swing stage, the force of the ankle to backward push the ground will gradually become larger, but, during the swinging stage, the ankle will only be subjected to the inertial force generated from backward pushing the ground. hence, it can be considered as the inertial force when the ankle backward pushing the ground makes the gait in the swing stage, so the swing of the individual legs after the foot leaving the ground is caused by the inertial force. however, since individually oscillating legs are less stressed at the joints, the individual's acceleration while walking is mainly relying on the opposite reaction of the ground to the foot and also gives the individual the force to move forward. but there is no fixed law in the horizontal direction of the individual, and it sometimes becomes positive and sometimes becomes negative. the individual's gait is the continuous movement of one leg to support the body and another leg to repeat swing, which is a process of constantly losing balance and finding balance. loop back and forth, and therefore most of the individual's muscles appear relaxed during the walk. combined with the above discussion and biomechanical principle of walking from qian [cit], it is concluded that the contact force of the foot has an extreme value at the turning point of each gait cycle, and there is a maximum value when the heel touches the ground. when the foot gradually flattens and the force area gradually increases, the force will reduce and reach a minimized value when the foot is fully flattened. another maximum value will appear when the heel leaves the ground and toes are squatting, in which the force curve has typical symmetrical bimodal properties throughout the gait cycle. and the study suggests that there is no significant difference in the contact force of the human body at different ages, which has a certain stability."
(a) verify that the acquired data have excellent separability according to traditional features and select the appropriate classifier from eight potential base classifiers for fusion.
"the next, the vcpu to be scheduled next (v n ) is determined as follows. the run queue rq [ p] is first searched to find if any vcpu is starving for cycles. this is indicated by a gvt counter value exceeding the gvt threshold g. if any vcpu satisfying this preemption condition is found, that vcpu is selected as v n (lines 9 to 14), and scheduled next after resetting its gvt counter g [ p] to zero (line 37). if no such vcpu is found, then an effort is made to find a vcpu with a lower lvt across all the pcpu run queues (lines 15 to 36). in this code segment, first the least lvt vcpu of rq [ p] is picked as the default candidate v n and its lvt is compared with that of the vcpus at the head of run queues of other pcpus. if this search is unsuccessful, the local candidate is returned as v n . otherwise, the vcpu v r stolen from the peer pcpu run queue (rq[ p ] ) is returned as v n . note that code segment in between (lines 32 to 35) increments the gvt counter of all the vcpus in rq[ p] on successfully stealing the vcpu v r . not incrementing the gvt counter after stealing of lower lvt vcpu v r results in scenarios wherein the lower lvt vcpus may be continuously exchanged between the pcpu run queues without altering the gvt counters of the vcpus. hence, not incrementing a gvt counter of all vcpus in rq [ p] after successfully stealing makes the deadlock persist. finally, the gvt counter of the v n is reset before it is scheduled on the pcpu."
"we verify the advantages of the scltn method over a range of variant methods of euclidean distance, dtw, and dtw. by calculating the \"typical period\" proposed in each method in different data sets and the average absolute deviation between its test periods, if the average absolute deviation is small, this indicates that the extracted period is representative and can characterize features of the entire set of gait data and vice versa. figure 9 : scope of shape change at different radius."
"the deadlock does not occur when at least one of the aforementioned conditions is violated. hence, a simple way to break the deadlock is through preemption. in the hypervisor scheduling process, this can be done by occasionally ignoring the application-supplied lvt and increasing the lvt value of the vcpu to a large value. one simple algorithm to achieve this is to make the vcpu lvts toggle between the actual lvt and a very large lvt value in regular scheduling intervals. by such toggling, the vcpus self-preempt themselves momentarily. this simple algorithm breaks the deadlock and ensures that all the sls get the necessary pcpu cycles to participate in gvt or lbts algorithms, as demonstrated in yoginath and perumalla [2013b] . however, this method is inefficient because of the significantly large number of selfpreemptions. this simple algorithm can be optimized by relaxing it into a counter-based approach, wherein every vcpu maintains a counter in addition to its vcpu lvt. this counter in the vcpu is incremented whenever it is bypassed during scheduling by another vcpu with a lower lvt. when the counter of any vcpu reaches a prespecified threshold (empirically set), this vcpu preempts others in the run queue to break the deadlock."
". ltp is our custom-built machine with a supermicro h8dg6-f motherboard supporting two 16-core (32 cores in total) amd opteron 6276 processors at 2.3ghz, sharing 256gb of memory, intel solid state drive 240gb, and a 6tb seagate constellation comprising 2 sas drives configured as raid-0. ubuntu 12.10 runs with linux 3.7.1 kernel runs as dom0 and domus, over xen 4.2.0 hypervisor. all domus are para-virtual and networked using a software bridge in dom-0. dom-0 is configured to use 10gb of memory, and the guest doms were configured to use at least 1gb memories each, which were increased as necessitated by the application benchmarks. each guest dom uses 2gb of lvm-based hard disk created over sas drives, while the dom-0 uses an entire solid state drive (ssd). openmpi 1.6.3 (built using gcc-4.7.2) was used to build the simulation engine and its applications. a machine file listing the ip addresses of the vms was used along with mpirun utility of openmpi to launch the mpi-based pdes applications onto vms. the 1ms tick size was used with both csx and psx schedulers."
"algorithm 1 gives the pseudo-code for our new deadlock-free pdes-specific hypervisor scheduler algorithm. as mentioned previously, the hypervisor scheduler inserts the currently executed vcpu v c into the run queue and picks a new vcpu v n for scheduling onto the pcpu. the hypervisor scheduler servicing the interrupt of the pcpu performs this scheduling action continuously. algorithm 1 starts executing on pcpu p with v c as input and determines v n as the output. the process of selecting the v n follows the llf principle for psx, modified for deadlock avoidance and for minimizing interprocessor synchronization (locking)."
"base classifier selection is the basis of multiclassifier fusion to select excellent classifiers for as much fusion as possible, and this paper selects the ten commonly used classifiers as alternatives, shown in table 2 ."
"the rest of the article is organized as follows. in section 2, a brief background is provided on the major concepts used in the work. in section 3, the design of the pdes-specific vm scheduler algorithm is presented, followed by the details of its implementation in section 4. a detailed performance evaluation is presented in section 5."
"among the shared resources multiplexed by the hypervisor, the physical processor cycles are especially important for pdes. to seamlessly share the physical cpu (pcpu) resources among the virtual cpus (vcpus), the hypervisor contains a scheduler that allots pcpu cycles to the vcpus using a scheduling policy. the hypervisor's scheduling is distinct from multithreading in operating systems hosted in the vms over the hypervisor [cit] ]. essentially, there exist three scheduling tiers in xen: (a) a userspace threading library schedules user-space threads over os-level (kernel) threads within a vm, (b) every guest os schedules its kernel threads to vcpus, and (c) the hypervisor schedules the vcpus over the pcpus. the focus of this article is on the lowest layer, namely, the hypervisor-level mapping of vcpus to pcpus."
"5.4.4. psx, csx, and native performance comparisons. in this section, we compare the performance benchmarks on the vm platform with the native linux platform, on the same hardware device. for a fair comparison, the number of μsik sl processes equivalent to number of vms used (in vm setups) were spawned on linux. the executions involving 128 vms using phold, dsb, and scatter were used for comparison. the best runtime results, regardless of the pdes synchronization scheme exercised, was used for comparison."
"from the experimental results in both sections 6.3 and 6.4, we can see that the proposed r-abf scheme leads to state-of-the-art verification accuracy, no matter using only the local orientation feature or using both local and global features. specifically, the r-abf-p-l 1 and r-abf-p-l 1 +blpoc methods achieve the best accuracy, respectively. by relaxing the l 1 -regularized sparsity constraint in the reconstruction, the l 2 -regularized reconstruction can also lead to very competitive verification results but with much less complexity, which is a good solution in practical fkp recognition systems."
"most existing works on co-segmentation [cit] work with a pair of images with similar (sometimes nearly identical) foreground, and unrelated backgrounds (e.g. the \"stone-pair\" in figure 2 ). this property is necessary because the goal of these works is to extract the common foreground object automatically, without any user-input. due to the nature of our application (i.e. multiple images of the same event or subject), our images typically do not follow this property (see figure 2) . hence, without user-input, the task of extracting the foreground object \"of interest\" is illdefined."
"as shown in figure 7, the standing stage accounts for 62% of the entire gait cycle, and it mainly consists of the following five parts. the first is the initial-stance stage, which refers to the period after the heel touches the ground and before the forefoot touches the ground. the second is the support reaction stage, which refers to the period after the foot begins to touch the ground and before shifting the weight to another supported foot. the third is the mid-stance stage, which refers to the period after the weight on the supporting foot and before the tiptoes leave the ground. the fourth is the terminal-stance, which refers to the period after the heels leave the ground and before the tiptoes leave the ground. the fifth is the preswing stage, which refers to period when feet successfully leave the ground, and this stage often is referred to as the push-off stage."
"(b) classify new features using eight alternative base classifiers and look for alternative classifiers that are suitable for fusion, comparing them to traditional classifiers that are selected by features."
"this application is a discrete-event formulation and a parallel execution framework for vehicular traffic simulation. a simulation scenario is set up by reading an input file that specifies the road-network structure, number of lanes, speed limit, source nodes, sink nodes, vehicle generation rate, traffic light timings, and other relevant information. dijkstra's shortest-path algorithm is used to direct a vehicle to its destination."
"eq. (5) is a joint optimization of dictionary d and the coefficient matrix w, and it can be solved by optimizing d and w alternatively [cit] . once the dictionary d, which has less number of atoms than x, is computed, we use it to code the input fkp image y as follows:"
"let the traditional feature set selected in this paper be t1, the new feature set be t2, and fusion feature set of new feature and traditional feature be t3. table 4 and figure 10, respectively, show the test set recognition rate of eight candidate classifiers on three feature sets t1, t2, and t3 and the time required to train corresponding classifiers and test set. figure 11 shows the relieff values corresponding to the 93 out-of-synchronization features."
"the algorithm described in table ii is used to perform the fkp verification experiments with different fusion rules in step 5. for the mw rule, the whole dataset is used to train the weights. the eer results by using the different fusion rules are listed in table iii . we can clearly see that the lowest eer is obtained by the proposed abf rule, which works much better than other rules. even that the mw rule uses more information with a training dataset, it is only slightly better than the min, ss and max rules. in the following experiments, we only report the results by using the abf rule."
"in the proposed method, we learn a dictionary d from the gallery set to reconstruct a query sample. when a new subject is enrolled, we can update d by solving eq. (5). however, if the dataset has a large scale, this can be very costly. fortunately, it is not necessary to update d for a new enrollment in large-scale dataset."
"organization. the rest of this paper is organized as follows: section 2 discusses related work; section 3 presents our energy minimization approach to interactive co-segmentation of a group of related images; section 4 presents our recommendation scheme for guiding user scribbles; section 5 introduces our benchmark dataset; section 6 discusses the results of simulated machine experiments and a real user-study; finally, section 7 concludes the paper."
"first, a dictionary is learned from the template fkp images, and this dictionary defines the subspace of the gallery fkp dataset. for a given query sample which may have pose variation, we represent it as the linear combination of the atoms in the learned dictionary. this process actually projects the query sample onto the subspace spanned by the gallery fkp images. the compcode scheme can then be applied to the reconstructed image for feature extraction and matching. nonetheless, the reconstruction of the query sample will not only reduce the intra-class distance, but also reduce the interclass distance. in other words, it can reduce false rejections but may also increase the false acceptances. to effectively exploit the discriminative information of the query sample before and after reconstruction, a simple yet powerful score level adaptive binary fusion (abf) rule is proposed to make the final decision by fusing the matching scores before and after reconstruction. the abf ensures a good reduction of false rejections without increasing much the false acceptances, leading to much lower equal error rates than state-of-the-art methods."
"from the above discussions, we can see that the main difficulty in fkp recognition is the false rejections caused by finger-pose-variation in the query samples. one strategy to solve this problem is to correct the pose deformations by affine transformations. however, estimating the affine transformation parameters is itself a very difficult problem, particularly for fkp images where very few distinctive key points can be extracted. since our ultimate goal is fkp verification but not pose deformation correction, another strategy is to enhance the fkp matching process without pose deformation correction. considering that the finger pose variation caused fkp image deformation enlarges the matching distance between two fkp images from the same person and hence results in false rejections, we propose a reconstruction based matching scheme to reduce the enlarged matching distance."
"(2) then, calculate the c value between each data point, finding the optimal alignment of each point in the sequence to ensure that the total c value is the smallest."
"three μsik library-based application benchmarks-phold [cit] ] (a synthetic pdes application generally used for performance evaluation), pdes-based disease spread simulation [cit], 2009 ] (a pdes-based vehicular traffic simulation application)-were used for our performance studies. various abbreviations used by the pdes applications and performance graphs have been consolidated in table ii . 5.2.1. phold benchmark. this is a widely used synthetic benchmark for performance evaluation in the pdes community. this pdes application randomly exchanges a specified set of messages between the lps. the μsik implementation of phold allows the exercising of a wide variety of options in its execution. the nlp, nmsg, la, loc, and synchronization techniques comprising cons and opt were varied to realize a range of simulation scenarios."
"in this section, we reason the presence of deadlocks that arise in a purely llfbased scheduler. for a system to deadlock, four conditions are necessary and sufficient [cit] ]:"
"similarly, we can calculate the decidability index ofŷ and denote it as d 2 . a bigger decidability index means that the within-class and between-class matching scores can be better separated, and hence the matcher is more accurate, vice versa. therefore, d 1 and d 2 can be used to adaptively determine the weights ω 1 and ω 2 that are assigned to s 1 and s 2 . the higher the decidability index, the higher the weight. however, designing an optimal function to map d 1 and d 2 to ω 1 and ω 2 is not a trivial work. in this paper, we propose to use the simple binary logic operation for the weight determination. the above proposed abf rule is a kind of \"winner-takeall\" strategy and is similar to the notion of using cohort scores for multi-biometric fusion [cit] . however, there are clear differences between them. in cohort score based multibiometric fusion, the fusion weight is fixed for each biometric identifier once the weights are learned offline. all the users of one biometric identifier share the same pre-learned fusion weight. instead, in the proposed abf the fusion weight is adaptively determined for each user online. the abf rule is simple but fits our application very well. the reconstruction of y can only lead to two situations: the reconstructed imagê y is either better or worse than y for verification. hence, it is reasonable and effective to adaptively choose one of them for the final decision making. our experimental results in section 6 validate the effectiveness of the proposed abf rule."
"with a hypervisor, the physical platform can host vms of different capacities concurrently. this flexibility provides various options to the user for vm selection. for example: a physical platform with 32 cpu cores can host a single vm with 32 vcpus or 2 vms each with 16 cpu cores or 4 vms each with 8 cores, and so on. the cloud platforms utilize this feature, offering choices to the user with vms of different capacities, and the vm usage cost is usually directly proportional to its compute capacity. hence, it becomes necessary to empirically evaluate the performance of pdes applications across a range vm configurations. figure 6 and figure 7 presents the runtime performance trends of dsb with the increase in number of vms on ltp and amazon's ec2 cloud platform, respectively. note that even though the number of hosted vms are different, the aggregate compute resource in each of our test vm configurations exactly matches the physical compute resource of the ltp. an interesting and common trend in both graphs is that the performance degrades with fewer vms beyond 1, and the performance gets better with an increase in the number of vms. this trend was also observed with phold benchmarks. more information on this counterintuitive behavior can be found in yoginath and perumalla [2013a] . this trend affects the monetary cost in vm utilization, as the cost of a vm in the cloud is predominantly determined by its compute capacity (number of vcpus)."
"currently, applications based on virtualization technology span from single-user desktops to huge data centers. on the lower end, virtualization allows desktop users to concurrently host multiple os instances on the same hardware. on a larger scale, vms can be moved from one hypervisor (or device) to another even while the vms are actively running. this capability for mobility is used to support many advantageous features of cloud computing, including load balancing, fault tolerance, and economical hosting of computing/storage services."
"it can be seen that the eer decreases from experiment 1 to experiment 2, and increases from experiment 2 to experiment 3. the reason can be as follows. from experiment 1 to experiment 3, the number of gallery classes is increasing. the increased number of gallery samples makes the dictionary d more capable to reconstruct the query sample, but it also makes the verification tasks more challenging. there are only 165 gallery classes in experiment 1, so the learned dictionary d may not be representative enough to reconstruct the query sample. the number of gallery classes is increased to 330 in experiment 2, and the representativeness of d is much improved so that the far and frr are decreased simultaneously. as a consequence, the overall eer in experiment 2 is reduced. with 330 gallery classes, the representativeness of learned dictionary d is already good. thus, the benefit of using 660 gallery classes in experiment 3 is not big in term of learning dictionary d; however, the far and frr are increased simultaneously due to the increased number of gallery classes, resulting in a bigger eer than experiment 2."
"5.3.1. native vs. vm. in figure 5, we compare the pdes performance of phold and dsb on a native linux and vms over the same hardware platform. performance on vm, which can be either privileged (dom0) or nonprivileged (domu), is also presented. the phold scenarios were configured to use 100 lps/sl, 100 and 1000 messages/lp, with 32 sls for 50% and 90% loc values. the dsb scenario involved simulation of disease spread across 320 locations among a population of 320,000 for 7 days of simulation time using 32 regions (sls), each with 10 locations (lps) and each location with a population of 1000 (event messages). the loc of 50% and 90% in dsb suggest that 50% and 90% of the trips the population performs are within the same region, respectively, which also suggests that the rest of the trips performed are across the regions (sl). figure 5 shows the benchmark runtimes' three setups: (a) native without hypervisor, (b) dom0 with 32 vcpus and, (c) domu with 32 vcpus along with dom0 without computational load. the runtime results across all three setups were found to be almost identical for all benchmarks. this result is significant because it demonstrates that when the virtual resources exactly match the physical resources, the performance of native and vm platforms are almost identical, suggesting a very low overhead due to the presence of the hypervisor."
"the fkp verification accuracy can be improved if we could correct the finger-pose-variation caused deformations of query samples via affine transformation. unfortunately, it is a particularly difficult problem to estimate the affine transformation parameters for fkp images because very few distinctive key points can be extracted from them. therefore, this solution is impractical. in this paper, we propose to reduce the pose variation caused false rejections by enhancing the matching process without pose deformation correction."
"parallel computing platforms based on virtualization technologies, such as commercial cloud offerings, have emerged lately, and are seen as a good alternative to native authors' address: s. b. yoginath and k. s. perumalla, 1 bethel valley road, oak ridge, tn 37831. permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. permissions may be requested fromexecution directly on specific parallel computing hardware. there are several benefits to using the virtualization layer, making such platforms very appealing as an alternative approach to execute parallel computing tasks. in the context of parallel discrete event simulation (pdes), the benefits include the following: -the ability of the virtualization system to simultaneously host and execute multiple distinct operating systems (os) enables pdes applications to utilize a mixture of simulation components written for disparate os platforms. -the ability to oversubscribe physical resources (i.e., multiplex larger number of virtual machines [vms] than available physical computing resources) allows the pdes applications to dynamically grow and shrink the number of physical resources as the resources become available or unavailable, respectively. -the dynamic imbalances in event loads inherent in most pdes applications can be efficiently addressed using the process migration feature of the virtual systems. -the fault tolerance features supported at the level of vms in concert with the vm migration feature also automatically helps in achieving fault tolerance for pdes applications."
"virtual time-based scheduling. in pdes, the simulation time advances with the processing of timestamped simulation events. in general, the number of events processed in a pdes application varies dynamically during the simulation execution (i.e., across simulation time), and also varies across processors. this implies that the amount of computation cycles consumed by a processor for event computation does not have any specific, direct correlation with its simulation time. a processor that has few events to process within a simulation time window ends up consuming few computational cycles. it is not ready to process events belonging to the simulation-time future until other processors have executed their events and advanced their local simulation time. however, a fair-share scheduler would bias the scheduling towards this lightly loaded processor (since it has consumed fewer cycles) and penalize the processors that do, in fact, need more cycles to process their remaining events within that time window. this type of operation works against the actual time-based dependencies across processors, and can dramatically deteriorate the overall performance of the pdes application. this type of deterioration occurs when conservative synchronization is used. similar arguments hold for optimistic synchronization, but, in this case, the deterioration can also arise in the form of an increase in the number of rollbacks. the only way to solve this problem is to design a new scheduler that is aware of, and accounts for, the simulation time of each vm, and schedule each in a least-simulation-time-first order."
"the compcode scheme is simple and fast, and it leads to acceptable accuracy in fkp verification [cit] . if the query fkp image is well aligned after roi extraction, the compcode scheme can work very well. as we discussed in the introduction section, however, there can be certain degree of variations of the finger pose in the data collection process (refer to fig. 2 please), which lead to deformations in the fkp images and consequently result in false rejections because compcode is sensitive to image deformations. fig. 3 shows the genuine and imposter matching distance distributions of a typical fkp verification system. based on the curves, it is obvious that we can divide the matching distance into two parts: an uncertain interval, which is between thresholds t 1 and t 2, and a confidence interval, which covers the remaining part. generally speaking, if the matching distance of two fkp feature maps falls into the confidence interval, it can be easily decided if the query sample is a genuine or an imposter, while most of the false acceptances and false rejections occur when the matching distance falls into the uncertain interval [t 1, t 2 ]. on the other hand, the finger pose variation is the main cause that increases the intra-class distance (see the long tail of the blue genuine curve in fig. 3 ), making the false rejections happen."
"nonlinear cost structure. the nonlinear cost structure is fundamentally rooted in the principles of economies of scale -the cloud hosting company gains flexibility of movement and multiplexed mapping of smaller logical units over larger hosting units, ultimately translating to monetary margins. moreover, a high-end multicore configuration on native hardware is not the same as a high-end multicore configuration on virtual hardware because the interprocessor (inter-vm) network appears in software for vms, but in \"silicon and copper\" for native hardware. the aggregate interprocessor bandwidth is significantly different between the virtualized (software) network and insilico (hardware) network (performance analysis supporting this insight is presented later)."
"the cues described so far, are local cues, that describe which region in an image should be scribbled on next. in addition to these, we also use some image-level cues (i.e., uniform over an image), that help predict which image to scribble next, not where. segment size (ss). we observe that when very few scribbles are marked, energy minimization methods typically over-smooth and results in \"whitewash\" segmentations (entire image labelled as foreground or background). this cue incorporates a prior for balanced segmentations by assigning higher confusion scores to images with more skewed segmentations. we normalize the size of foreground and background regions to get class distributions for this image, and use the inverse of the entropy of this distribution as our cue. codeword distribution over images (cd). this imagelevel cue captures how diverse an image is, with the motivation being that scribbling on images containing more diversity among features would lead to better foreground/background models. to compute this cue, we cluster the features computed from all superpixels in the group to form a codebook, and the confusion score for each image is the entropy of the distribution over the codewords observed in the image. the intuition is that the more uniform the codeword distribution for an image the more diverse the appearances of different regions in the image."
"as shown in table 3, the averaging method normalizes the lengths of all gait sequences and fails to adequately correct misalignment among periods, such that the average absolute deviation is the largest. dtw and its series of deformation algorithms make up for deficiencies in the averaging method, such that it is unnecessary to normalize the length of sequence that is waiting to be compared, and it also effectively solves the time warping problem that is caused by the speed change."
"the hypervisor is a critical component of the virtualized system, enabling the execution of multiple vms on the same physical machine. to support the largest class of applications on the cloud, a fair-sharing scheme is employed by the hypervisor for sharing the physical processors among the vms. the concept of fair sharing works best either when the vms execute relatively independently of each other or when the concurrency across vms is fully realized via uniform sharing of computational cycles. this property holds in the vast majority of applications in general. however, in pdes, fair-share scheduling does not match the required scheduling order and, in fact, may run counter to the required order of scheduling. this mismatch arises from the fundamental aspect of interprocessor dependency in pdes, namely, the basis on the global simulation time line."
"in practice, it is not necessary to reconstruct every input query image for verification. as shown in fig. 3, if the matching distance of a query image y falls into the confident interval, we can directly make the decision; only when the matching distance falls into the uncertain interval [t 1, t 2 ], the reconstruction is needed, and the abf rule is applied for the verification. the proposed algorithm of reconstruction based fkp verification with abf is summarized in table ii . the output is the final matching distance s. the final decision"
"hold and wait. as long as the scheduled vcpu has the least lvt, it is always assured of coming back to the pcpu even after its timeslice is exhausted. if the lvt of such a vcpu (with least lvt) does not progress, that vcpu holds up the pcpu resource (hold). the pcpu resource is indefinitely held up by this least-lvt vcpu (wait) because it is waiting for another processor to participate in gvt/lbts computation. this will not be resolved because at least one of the sls (with a larger lvt) does not participate in the gvt computation due to nonavailability of pcpu compute cycles."
". while the cost model for the utilization of cloud compute resources makes economic sense, the options and related pricing of the offered vms generally confound the user. this is more true if the user intends to use the cloud resource for a parallel computing application, like pdes application. in this case, the monetary cost of executing the same pdes application among various configurable options of vm resources provides necessary insight. in figure 8, we plot the cost of executing various phold simulation scenarios on a cluster formed using various ec2 cloud resources. in this graph, the left-most vm (on the x-axis) is the costliest, while the right-most is the cheapest in terms of monetary cost. from this graph, we observe that the costliest resource might not yield the best performance, while the cluster of vms formed by single-vcpu (m1.medium) seems to provide a better value for the cost. in figure 9, we plot the runtime and the cost of one of the dense simulation phold scenarios using conservative synchronization (with better runtime than its optimistic counterpart) for comparison. a similar trend was observed with the dsb benchmarks. we refer the interested reader to yoginath and perumalla [2013a] for more information."
"the corresponding performance plots are presented in figure 14 . the runtime of the optimistic plot for the psx remains almost the same with the increase in number of vms. the psx conservative runtime also shows a similar trend, except when the number of vms hosted is 128, when its runtime slightly increases. in comparison, the csx runtime suffers as the number of vms hosted increases. however, the csx using optimistic synchronization is able to curtail the performance degradation significantly in comparison with its conservative synchronization."
"in a single-image setup, a user visually inspects the produced cutout and gives more scribbles to correct mistakes made by the algorithm. however, this approach would not work for interactive co-segmentation because 1) as the number of images in the group increases, it becomes increasingly cumbersome for a user to iterate through all the images in the group to find the worst segmentation; and 2) even if the user were willing to identify an incorrect cutout, there might be multiple incorrect cutouts in the group, some more confusing to the segmentation algorithm than others. observing labels on the most confusing ones first would help reduce the number of user annotations required. it is thus necessary for the algorithm to be able to suggest regions in images where scribbles would be the most informative."
"distance transform over scribbles (dt). for this cue, we compute the distance of every pixel to the nearest scribble location. the intuition behind this (weak) cue is that we would like to explore regions in the image away from the current scribble because they hold potentially different features than sites closer to the current scribbles. intervening contours over scribbles (ic). this cue uses the idea of intervening contours [cit] . the value of this cue at each pixel is the maximum edge magnitude in the straight line to the closest scribble. this results in low confusions as we move away from a scribble until a strong edge is observed, and then higher confusions on the other side of the edge. the motivation behind this cue is that edges in images typically denote contrast change, and by observing scribble labels on both sides of an edge, we can learn whether or not to respect such edges for future segmentations."
"to conduct a thorough set of experiments and evaluate various design choices, it is important to be able to perform multiple iterations without explicitly polling a human for scribbles. thus, we develop a mechanism to generate automatic scribbles, that mimic human scribbles (we also present results of a user-study in section 6.2). we model these synthetic scribbles as (smooth) random walks that do not cross foreground-background boundaries. our scribble generation technique consists of sampling a starting point in the image uniformly at random. a direction angle is then randomly sampled such that it is highly correlated with the previous direction sample (for smoothness) for the scribble, allows us to control the length of scribbles and observe the behavior of the algorithm with increasing information. example synthetic scribbles are shown in figure 4 ."
the above three fusion rules do not use any additional information apart from the matching scores s 1 and s 2 . another popular method is the matcher weighting (mw) scheme [cit] . the fused score is the weighted average of the two scores:
"as seen in figure 15, the phold benchmark runtime on linux using 128 processes is several orders of magnitude faster than results from csx and psx. though psx is able to alleviate the performance degradation to a certain extent, it still is inefficient because the phold benchmark is very fine grained and the low lookahead (0.1) requires frequent synchronization. this is in spite of optimistic synchronization trying its best to keep the runtime lower. the native runs are almost over an order of magnitude faster than a vm environment using psx or csx."
"with the proliferation of cloud and vm-based platforms for parallel computing, it is now possible to execute pdess over multiple vms, in contrast to executing in native mode directly over hardware, as has been traditionally done over the past decades. however, while most vm-based platforms are optimized for general workloads, pdes execution exhibits unique dynamics significantly different from other workloads. here, we present results that identify the gross deterioration of the runtime performance of vm-based pdes simulations when executed using traditional vm schedulers, quantitatively showing the bad scaling properties of the scheduler as the number of vms is increased. the mismatch is fundamental in nature in the sense that any fairness-based vm scheduler implementation would exhibit this mismatch with pdes runs."
technique. our approach is composed of two main parts: 1) an energy minimization framework for interactive cosegmentation; and 2) a scribble guidance system that uses active learning and some intuitive cues to form a recommendation map for each image in the group. the system recommends a region with the highest recommendation score. see figure 1 for an overview.
"2) experiment 2: in the 2 nd experiment, 330 classes are involved in the gallery set. therefore, the numbers of genuine and imposter matchings are 11,880 and 7,828,920, respectively. fig. 9(b) shows the det curves by the different verification schemes, while the eer values and decidability indices are listed in table iv . the percentage of matchings in which the score from the reconstructionŷ is selected is listed in table v . again, the proposed r-abf methods get much better results than imcompcode, blpoc and lfi."
"equations (18) and (19) are, respectively, the fourth voting weight assignment and voting function for an unknown sample, it is first classified by a multiclassifier, and then the multiclassifier classification result is subjected to msv fusion algorithm to obtain the final classification, which can be calculated using experimental environment: this article uses python 3.6 as the software platform and an intel core i7 processor as the hardware platform, with a main frequency of 2.7 ghz and a memory of 8 gb. the 64-bit windows 10 operating system is adopted."
"it can be obtained from the second row of table 4 that gait data collected in this paper has a good recognition rate for the table 4 that the 16 new eigenvalues extracted in this paper are inferior to the traditional eigenvalues in the classification effect of each classifier, but the gap of recognition rate is not very large. the minimum phase gap is 0.4%; the maximum phase gap is 11.1%; the highest recognition rate of new features is 88.35%, which is 1.78% of the gap from the highest recognition rate of traditional features. this indicates that the new features have excellent separability and provide a theoretical foundation for subsequent feature fusion."
"refer to fig. 1, although a triangular block is used to control the finger freedom in fkp image acquisition, there is still much flexibility for the users to position their fingers. this is good to increase the user friendliness but also allows much variation of the finger pose in query sample collection process. fig. 2 shows some examples. we can see obvious deformations between the two fkp samples due to the finger pose variations. unfortunately, the compcode and lfi based fkp recognition methods are sensitive to such variations, resulting in false rejections and degrading the fkp verification performance."
3 where indicates the confidence of the th base classifier output the class sample; ( ) indicates the actual number of class samples; ( ) indicates the number of class samples that the test sample is classified into as class samples after passing the th base classifier.
research. the four related research literatures in recent years are compared with the methods used in this paper to further demonstrate the superiority of the algorithm.
"(4) compare with existing research. the method of this paper is compared with related research in recent years to further reflect the superiority of the identification method adopted in this paper. figure 5(b) that the size of radius selected in log-polar affects the context information of each point in the sequence. therefore, it is important to select an appropriate radius. to verify the validity of shape context information, it is assumed that the first 10 points of gait acceleration data are selected, and then context information changes of 10 points at different radii are, respectively, obtained, as shown in figure 9 ."
"it can be seen that the r-abf methods outperform much imcompcode, blpoc and lfi. this validates that the reconstruction of query sample y can reduce much the image deformation induced intra-class distance, and the abf rule can prevent the less discriminative reconstruction of y from being adopted for final decision making. among the four variants of r-abf, the l 1 -regularized ones have higher accuracy than the l 2 -regularized ones, while the patch based ones have higher accuracy than the global based ones. specifically, r-abfp-l 1 achieves the lowest eer. this is consistent with our discussions in section 3. table v lists the percentage of matchings in which the score from y or the reconstructionŷ is selected as the final matching score. we can see that for genuine matchings, about 40% ∼ 45% of the matching scores fromŷ are selected by the abf rule."
"4.1.1. linux kernel modifications. each guest os maintains a shared inf o page (figure 3), which is dynamically updated as the system runs. we added two fields, simtime and switch scheduler, to the shared inf o data structure. the simtime field is used to record the lvt from the sl and the switch scheduler flag indicates the switch between different modes of scheduler operation. to send the lvt information from the application level, we implemented a system call for our guest os (linux). this system call allows the lvt information to transit from user space to kernel space, and here the lvt value is written into the simtime of shared inf o data structure of the host dom. this shared inf o data structure is accessed by the hypervisor during scheduling."
"if there is one thing that the growing popularity of photosharing website like flickr and facebook (4 and 10 billion photos respectively, as of oct. 2009) has taught us -it is that people love taking photographs. consumers typically have several related pictures of the same object, event or destination, and this rich collection is just waiting to be exploited by vision researchers -for something as simple as building a collage of all the foregrounds to something more sophisticated like a complete 3d model of a particular object. in many such tasks, it would be useful to extract a foreground object from all images in a group of related images. this co-segmentation of foreground objects from multiple related images is the goal of this paper. what is foreground? the stone-pair (a) has significant variation in background with nearly identical foreground and thus unsupervised co-segmentation can easily extract the stone as foreground. the stonehenge-pair is fairly consistent as a whole and thus the stones cannot be cut out via unsupervised co-segmentation. bringing a user in the loop is necessary for the problem of foreground extraction to be well defined."
"multiplexing ratio. given that multiple vms must be used to avoid the high price of a single many-core vm, the performance of pdes execution now becomes dependent on the scheduling order of the vms (virtual cores) on the host (real hardware cores). this puts pdes performance at the mercy of the hypervisor scheduler's decisions. when the multiplexing ratio (ratio of sum of virtual cores across all vms to the sum of actual physical cores) even fractionally exceeds unity, the pdes execution becomes vastly suboptimal. in all cloud offerings, this multiplexing ratio can (and will very often) exceed unity dynamically at runtime. thus, we have a conflict: one-to-one mapping (multiplexing ratio of unity or smaller) incurs a higher monetary cost, but increasing the multiplexing ratio causes a scheduling problem, and increases the runtime, thereby stealing any monetary gains."
"3) experiment 3: at last, all the classes (i.e., all fingers) are involved in the gallery set, and the numbers of genuine and imposter matchings are 23,760 and 15,657,840, respectively. the det curves by different verification schemes are illustrated in fig. 9(c) . table iv lists the eer values and decidability indices, and table v lists the percentage of matchings in which the score from the reconstructionŷ is selected. similar conclusions to the previous two experiments can be made."
"therefore, for conditions of gait motion features that are not extracted in the above literature, this paper analyses the motion features of gait by extracting the acceleration rate change and velocity change relative to uniform acceleration. moreover, the paper proposes a multiscale fusion algorithm for the deficiency of the multiclassifier fusion algorithm."
"the vcpu scheduling based on a purely llf-based policy leads to deadlock of pdes applications when the number of hosted vcpus is greater than the available pcpus. this is because the vcpus with smaller vcpu lvt values (i.e., having a higher scheduling priority) would prevent the vcpus with a higher vcpu lvt value from being scheduled for execution. consequently, some of the sls (those executing in vms with higher lvt values) would never get a chance to participate in inter-sl time synchronization operations such as global virtual time (gvt) or lower bound on incoming time stamp (lbts) computation. since gvt or lbts constrain the progress of the simulation, the execution deadlocks. note that this deadlock can arise for both conservative or optimistic synchronization. in conservative operation, simulation progress is critically dependent on global time guarantees, and hence no processor will be able to go past the most recently computed lbts. optimistic operation, although resilient to an extent to slow gvt progress rates, can also deadlock if/when the processor with the least lvt runs out of memory because fossil collection gets stalled when gvt computation does not progress."
"it can be concluded from tables 5 and 6, compared to other multiclassifier fusion algorithms, that the recognition rate of three feature sets is improved after the msv algorithm is merged, and the recognition rate for each feature set is better than those of other fusion algorithms. the recognition rate of three feature sets is 3.30% higher than that of five base classifiers, and the highest recognition rate is 98.42% of the fusion feature. from the above analysis, the msv algorithm is shown to have excellent multiclassifier fusion performance."
"let us use two experiments to validate the above statement. we use the first 600 classes out of the 660 classes in the polyu fkp database as the gallery set (6 samples per class in the first session) to learn a dictionary, denoted by d 1 . we then use all the 660 classes to learn another dictionary, denoted by d 2 ."
"it is apparent from table 4 that the gait data collected in this paper have a good recognition rate for the traditional features on most candidate classifiers, and the highest recognition rate can reach 91.7%. this illustrates that a kind of biometrics, an individual's gait characteristics, can be used for human identification, and traditional gait features have certain separability to gait; however, the recognition rate is not very high."
-mutual exclusion: tasks claim exclusive control of the resources. -hold and wait condition: tasks hold resources already allocated to them while waiting for additional resources.
"to evaluate our proposed approach and to establish a benchmark for future work, we introduce the largest cosegmentation dataset yet, the cmu-cornell icoseg dataset. while previous works have experimented with a few pairs of images, our dataset contains 38 challenging groups with 643 total images (∼17 images per group), with associated pixel-level ground truth. we built this dataset from the flickr® online photo collection, and hand-labelled pixellevel segmentations in all images. we used the \"group\" feature in flickr, where users form groups around popular themes, to search for images from this theme. our dataset consists of animals in the wild (elephants, pandas, etc.), popular landmarks (taj mahal, stonehenge, etc.), sports teams (baseball, football, etc.) and other groups that contain a common theme or common foreground object. for some (though not all) of the groups, we restricted the images to come from the same photographer's photo-stream, making this a more realistic scenario. examples of these groups are shown in various figures in this paper and more examples may be found online [cit] . we note that this dataset is significantly larger than those used in previous works [cit] . we have made this dataset (and annotations) publicly available [cit] to facilitate further work, and allow for easy comparisons."
the top two graphs in figure 10 plot performance runs with lowest possible computational load for varying communication loads and for varying lookaheads 0.1 (left) and 1.0 (right). these show the effect of vcpu scheduling in the absence of a significant computational load. these plots show several orders of magnitude of degradation in performance with a negligible increase in load. these readings constitute some of the worst possible performances that can be expected on a cloud platform.
"the concept of virtualization has been realized at different levels of the computer systems architecture. at the hardware level, two methodologies are used for virtualization: full virtualization and para-virtualization. although they are similar in functionality, they differ in the means to realizing virtualization. both the methodologies run on the top of the hardware by pushing the os above them and make use of highly configurable vms comprising virtual peripheral i/o components. para-virtualization differs from full virtualization in that it requires the modification of the guest os kernel, while the full virtualization can host any os without modifications."
"before presenting the performance comparison of the psx over the default csx, we touch upon certain interesting aspects of csx performance. some information presented here is borrowed from our prior detailed study [cit] ] ."
"incidentally, the time-warp operating system [cit] ] [cit] s is one of the earliest works that addressed pdes performance issues by realizing the simulation scheduler (and related functionality) at the bottom-most hardware levels; however, this was limited to a single operating system as opposed to a hypervisor. there is also a superficial semblance to related work in vm-based network simulations [cit] ]. however, vm-based network simulations are fundamentally different from pdes execution over vm platforms. in vm-based network simulations, the simulation time of each vm is determined by the hypervisor itself (in terms of computation time consumed by each vm, tracked and accounted by the hypervisor), whereas for pdes execution over vms (discussed in this article), the virtual time used during vcpu scheduling corresponds to the user's simulation model."
"although the method has excellent real-time performance and can maintain pattern features of the gait, it does not best-reflect the phase of gait and solve time variation problems caused by the gait mode. unlike frame-based segmentation, the period-based segmentation method is based on the gait period and can fully reflect the hidden biometrics of gait data. considering that gait acceleration data usually appears as a set of data with periodic features, this paper uses a period-based segmentation method, and the specific algorithm is divided into the following three steps."
"as shown in figure 6, the swing stage accounts for about 38% of the entire gait cycle. first, the swing begins, then it reaches the mid-swing stage through accelerating, and finally it slows down to the terminal-swing stage."
"the rest of this paper is organized as follows. section 2 briefly reviews the compcode scheme and indicates its problems. section 3 presents the proposed reconstruction based methodology for fkp recognition. section 4 presents the abf rule. section 5 summaries the proposed algorithm. section 6 presents extensive experimental results and discussions, and section 7 concludes this paper."
"(3) for the alignment sequence obtained in step (2), the linear rule in the ltn algorithm is used again to verify the linear relationship between matching alignment points to ensure that the best matching alignment is achieved. should still be calculated using the ltn algorithm, and the period corresponding to the minimum distance is selected as the typical period. this paper does not use the shape context algorithm to determine the final distance between two sets of sequences but makes further improvements of matching points through the shape context. when extracting the typical period, the ltn algorithm is still used to calculate the sequence between the two groups, and the period is extracted according to the distance. the shorter the distance between sequences is, the more similar the two groups are; the greater the distance is, the greater their difference is."
"note that we use all the gallery samples, not only the samples from the class that y claims, to computeŷ. there are two reasons for such a configuration. first, the number of samples of each class is usually small (e.g., 6 samples per class in the gallery set of polyu fkp database), and thus reconstructing y using only the samples from one class is not accurate. second, the query sample y can be an imposter, i.e., it may come from a class out of the gallery set, or from a class that is different from the class it claims. reconstructing y using only the samples of one class will make the distance fromŷ to this class too small so that false acceptance will happen. therefore, all the classes in the gallery set should be involved in reconstructing y."
"energy minimization. given user scribbles indicating foreground / background, we cast our labelling problem as minimization of gibbs energies defined over graphs constructed over each image in a group. specifically, consider a group of m image-scribble pairs"
"after recognizing scheduling policy mismatch in cloud platforms as the reason for poor pdes performance, we designed, implemented, and extensively evaluated the performance of a new pdes specific scheduler. we used different scenario configurations of synthetic phold, disease spread simulation benchmarks, and vehicular traffic simulation benchmarks to evaluate the performance. we demonstrated a significant speedup using a psx scheduler over a csx scheduler across all the benchmark runs. note that all the performance comparisons were done with csx using small time slices; hence, the speedup against csx with default time-slice configurations can be expected to be much higher (order of magnitude). we also compared the runtime performance of our three simulation benchmarks involving 128 vms with 128 processes hosted on a linux machine with the same hardware, and demonstrated that a tremendous reduction in performance degradation can be achieved in vm-based execution platforms. we also demonstrated the high variance of pdes application runtime with csx, and very low variance with psx execution setups."
"in this section, we develop an intelligent recommendation algorithm to automatically seek user-scribbles and reduce the user effort. given a set of initial scribbles from the user, we compute a recommendation map for each image in the group. the image (and region) with the highest recommendation score is presented to the user to receive more scribbles. instead of committing to a single confusion measure as our recommendation score, which might be noisy, we use a number of \"cues\". these cues are then combined to form a final recommendation map, as seen in figure 3 . the three categories of cues we use, and our approach to learning the weights of the combination are described next."
"the resulting classifier thresholds and voting methods are combined at the end of the hrwv algorithm. experimental results show that the recognition rate of the hrwv algorithm is 3% higher than the single classifier, but the average confidence will be less than 0.5 if most classifiers identify poorly for a certain type of sample, which will result in an increased misclassification of the classifier weighting threshold and the desired fusion effect cannot be achieved."
"we also ran our benchmarks on amazon's ec2 cloud platform. we built a cluster of para-virtual vm instances of ubuntu 12.04 lts. table i lists the vms using the clusters that were built to run the performance benchmarks. in table i, the term ecu refers to an ec2 compute unit, which is an abstraction defined and supported by amazon as a normalization mechanism to provide a variety of virtual computation units independent of the actual physical hardware support that they use/maintain/upgrade without user intervention. openmpi 1.6.3 was built on the virtual instance, which was used to build the simulation engine and all the pdes applications. a machine file listing the dns names of the allotted instances was used to launch the mpi-based pdes applications using mpirun."
"in eq. (6), the l 1 -norm sparsity constraint is imposed on the coding coefficients to enforce that only a small portion of the atoms are dominantly used to reconstruct y. however, l 1 -minimization is time consuming. though many fast l 1 -minimization solvers such as fista [cit], alm [cit] and homotopy [cit] have been developed, they may not be fast and accurate enough for practical use in the application of fkp verification, where real time implementation is expected. one intuitive solution is to relax the strong l 1 -regularization to the weaker l 2 -regularization in eq. (6). the l 2 -regularization offers a closed form solution to w, which can be very efficiently computed. though the resolved coefficient w is not sparse anymore, the l 2 -regularization can still make w have a small energy. as we will see in the experimental results, the fkp verification accuracy by l 2 -regularized reconstruction is only a little lower than that by l 1 -regularized reconstruction, but the computational complexity is greatly reduced."
"to better train and judge each classifier and to obtain prior knowledge for the msv fusion algorithm such as the recognition rate of the classifier for a single individual and recall rate, this paper divides the experimental data into 10 equal parts in all classification experiments. six parts were used as training sets, and the classifier was trained using a ten-fold method during training; two parts were used as test sets to test the classifiers and obtain prior knowledge; and two parts were used as a verification set, mainly verifying the reliability of algorithm and the effect of the fusion algorithm."
"resources cannot be forcibly removed from the tasks holding them until the resources are used to completion. -circular wait: a circular chain of tasks exists, such that each task holds one or more resources that are being requested by the next task in the chain."
the decidability index [cit] can be used to measure the separability of the distributions of within-class and betweenclass matching distances. the decidability index for y is calculated as follows:
"in the proposed method, the query image reconstruction can be regularized by either l 1 -norm or l 2 -norm, and can be done either globally or patch-by-patch. therefore, there are four variants of the proposed r-abf scheme, denoted by r-abfg-l 1, r-abf-g-l 2, r-abf-p-l 1 and r-abf-p-l 2, respectively, where \"g\" is for \"global\" and \"p\" is for \"patch\". in order to evaluate the proposed r-abf method more comprehensively, we conduct 3 experiments with different sizes of the gallery set (165, 330 and 660 fingers, respectively). in all the experiments, the gallery set is extracted from the fkp images collected in the first session, while all the fkp samples collected in the second session are used as the probe set (all the 660 fingers)."
"in this paper, a single set of gait period data is divided into five motion regions using the maximum and minimum values; the velocity variation of relative uniform acceleration in the motion region is extracted, and the acceleration variation per unit time is taken as a new feature. new features are combined with traditional time domain and frequency domain features to form a new feature set, which can be used to train the five basic classifiers. prediction results of each classifier are then subjected to the msv multiclassifier fusion algorithm to obtain a final prediction result."
"( ) estimation of period length. according to statistics, normal people walk two steps on average from 0.8 s to 1.2 s. the two steps correspond to one period in the data since only one mobile phone is placed in the right side of the pants pocket when collecting the data. in addition, the sampling frequency is set as 100 hz, such that approximately 80 to 120 data points correspond to one period."
"the paper improves a method based on ltn, introduces the concept of shape context, proposes a method called scltn to extract the typical period from gait data, and adds pointto-point local structure information to enhance ltn in the matching process. this method not only solves the problem of point matching error but also makes all point sets of the gait time series strictly aligned according to point to point. the main advantages are as follows."
"as a generic guideline when executing a pdes application for the cloud, if the number of processor cores of the physical system on which vms are hosted is known, then use a single vm with the number of vcpus equal to the number of processor cores. in the absence of these details, the best performance for the monetary cost involved can be obtained by using a cluster of vms, each with a single vcpu. note that this guideline is valid when the virtual compute resources exactly match the physical compute resources. even a slight increase (less than 10%) in the virtual to physical multiplexing ratio yields poor performance. we discovered that the poor performance of the pdes application was related to the hypervisor scheduling policy."
"combining the above analysis, the paper, respectively, extracts six features from the single gait cycle: cycle start point ( ), first maximum point ( ), first minimum value ( ), last minimum value ( ), last maximum value ( ), and gait cycle end point ( ) using the 6 feature points to divide the individual gait data into 5 different motion intervals. the results of the interval division of different individuals according to this method are shown in figure 8 . the connection between two feature points is regarded as a uniform acceleration motion, such that the amount of acceleration change that changes and decelerates per unit time can be expressed as the slope of the five straight lines. the velocity variation of a relatively uniform acceleration motion in a single interval can be represented by the area enclosed by the original variable acceleration curve and the uniform acceleration line. therefore, this paper provides 16 eigenvalues, namely, 6 eigenvalue points, 5 slopes, and 5 areas."
"the study and results presented here are among the first to evaluate the characteristics of cloud and vm-based pdes in detail, and the first to propose a deadlock-free pdes-customized hypervisor scheduler."
"in this paper, we propose a new fusion rule, which is adaptive to the query image and does not need a preset training dataset. the weights ω 1 and ω 2 are adaptively determined online based on the input image pair y andŷ. the idea is as follows. for the query image y which is claimed to belong to class c, we can calculate its within-class and betweenclass matching distances. using compcode, those matching distances can be computed very fast by eq. (3). so does for the reconstructed query imageŷ. then the higher weight is assigned to the matcher whose within-class and between-class matching distances are better separated."
"however, dtw is generally used for nonlinear sequence alignment matching, and dtw fails to consider shape features of the sequence itself when matching the alignment. the ltn algorithm is linear and more suitable for the gait data presented in the paper. compared to the dtw algorithm, the linear relationship between gaits is fully considered and the results are also slightly improved. however, neither dtw nor ltn considers local shape features of gait data or the context information of each point. therefore, this paper uses the scltn algorithm to extract the typical period, from results presented in table 3, and it is obvious that the difference value of this part is smaller than those of other results. the scltn algorithm considers the linear relationship between gaits, but when the point matching is aligned the local shape at each point is also taken into consideration, and \"feature-feature\" alignment is realized in the true sense. hence, a typical period extracted using this method is closer to the original data in terms of numerical value and shape features and is the most representative of the original data features. table 1, the following 15 traditional eigenvalues of the single gait period x, y, and z axes for classification experiments are extracted and listed: mean, standard deviation, skewness, kurtosis, root mean square, over zero rate, interquartile range, period length, energy, and the first 6 fourier coefficients. since these are common time-frequency domains and statistical features, the calculation formula is relatively simple and will not be described here. introduced in this paper from the x, y, and z axes of a single gait period and using the alternative eight classifiers for classification experiments."
"experimental results have been documented from detailed experiments with multiple discrete event models over a range of scenarios (with different lookahead values, interprocessor event exchange frequencies, and conservative and optimistic synchronization), all of which show (a) the high variability and suboptimality of the default credit-based vm scheduler that is pdes-agnostic, and (b) the well-behaved scalability and significantly faster execution of our new algorithm."
"n k }, and scribbles for an image s (k) are represented as the partial (potentially empty) 1 set of labels for these sites. for computational efficiency, we use superpixels as these labelling sites (instead of pixels). 2 for each image (k), we build a graph,"
"while our implementation and experimentation have been performed in the context of the xen [cit] ] hypervisor and the μsik [cit] ] parallel/ distributed simulation kernel, the concepts developed in this article for vm-based pdes are sufficiently general, and can be applied to other hypervisors and parallel discrete event simulators."
"the plot on the left in figure 12 is magnification of the initial set of points. they demonstrate the behavior of psx with respect to csx when the multiplexing ratio of vcpus on to pcpus are low. as seen, csx performs best when no mismatch between pcpus and vcpus exist and suffers significantly even due to a slight mismatch. in contrast, the psx suffers in the absence of the mismatch due to unnecessary overhead of writing lvts to the hypervisor and performs better than csx as the mismatch grows, as expected."
"we now evaluate icoseg, our recommendation system, as a whole. the experimental set up is the same as that described above, except now we use the combined recommendation map to guide subsequent scribbles (and not individual cues). the cue combination weights are learnt from all groups except one that we test on (leave-one-out cross validation). we compare to two baselines. one is that of using a uniform recommendation map on all images in the group, which essentially means randomly scribbling on the images (respecting object boundaries of course). and the other (even weaker) baseline is that of selecting only one image (randomly) in a group to scribble on (with a uniform recommendation map on this image). figure 6 shows the performance of our combined recommendation map (icoseg) with increasing scribble length, as compared to the baselines. we see that our proposed recommendation scheme does in fact provide meaningful guidance for regions to be scribbled on next (as compared to the two baselines). a meaningful upper-bound would be the segmentation accuracy that could be achieved if an oracle told us where the segmentations were incorrect, and subsequent scribbles were provided only in these erroneous regions. as seen in figure 6, icoseg performs very close to this upper bound, which means that users following our recommendations can achieve cutout performances comparable to those achieved by analyzing mistakes in all cutouts with significantly less effort without ever having to examine all cutouts explicitly."
"pdes has traditionally assumed execution at the highest end of the computing platform available to the user. however, the choice is not so straightforward in cloud computing due to the nonlinear relation between actual parallel runtime and the total cost (charged to the user) for the host hardware. for example, suppose that a multicore computing node has 32 cores on which a pdes with 32 concurrent simulation loops is to be executed. generally speaking, traditional pdes maps one simulation loop to one native processor. however, with cloud computing, the monetary charge for such a direct mapping (i.e., a virtual machine with 32 virtual cores) is typically much larger than the total monetary charge for aggregates of smaller units (i.e., 32 virtual machines each with only 1 virtual core)."
". this is an epidemiological disease spread pdes application [cit] that uses a discrete event approach to model the propagation of a disease in a population of individuals across locations and across regions (aggregates of locations). each region is mapped to an sl and each location is housed in an lp. multiple individuals are instantiated at each location; they not only interact with individuals within the same location but also periodically (conforming to an individual-specific time distribution function) move from one location to another within and across regions. the scenario configuration parameters for this application are the same as for phold, for which nlp refers to number of locations in a region (sl), nmsg refers to population/location, and loc refers to percentage of population movements within the same region."
"this paper deals with interactive co-segmentation of a group (typically 2) of related images, and presents an algorithm that enables users to quickly guide the output of the co-segmentation algorithm towards the desired output via scribbles. our approach uses successful ideas from the single-image interactive segmentation [cit] literature. a user provides foreground/background scribbles on one (or more) images from a group and our algorithm uses these scribbles to produce cutouts from all images this group."
"4.2.1. scheduling in normal mode. the scheduler is said to be in normal mode if the switch sched (of ps private data structure; figure 4 ) is false. this corresponds to the mode in which the vms are booted and operational, but no pdes run is active (hence lvt-based scheduling is undefined). in this mode of operation, psx maintains the sim time (vcpu data structure; figure 4 ) of all dom0 vcpus lower than all the domus. in the normal mode, all guest-dom vcpus have their sim time initialized to a constant 1, while dom0 vcpus have their sim times initialized to a constant 0. only after the switch sched is set to true by pdes sl, the sim time value of the relevant vcpu is updated after reading the shared inf o. however, the sim time of vcpus of dom0 continues to be 0 even after switching to simulation mode."
"mutual exclusion. at runtime, only one vcpu can execute at any given time on a pcpu. when a vcpu is scheduled, no other vcpu can get computational cycles. the scheduled vcpu always has exclusive control of the pcpu resource. this satisfies the mutual exclusion criterion for deadlock."
"the lvt values to each of the vcpus are passed from the pdes application. to ensure the progress of the simulation, each of the sls need to regularly exchange their lvts with their peer sls to compute the gvt. for the gvt computation, sls need pcpu cycles. when the number of vcpus is greater than number of pcpus, some vcpus do get pcpu cycles. hence, the least-lvt vcpu waits for the gvt message from its peer, while its peer vcpus (with higher lvt value) wait on the least-lvt vcpu to release the pcpu resource (to participate in gvt computation). thus, a circular wait condition emerges."
"literature [cit] extracted the gait frequency, symmetry coefficient, numerical fluctuation range, and the similarity coefficient of the feature curve from gait data as features, and they proposed a weight voting mechanism for the classification and recognition of gait according to these features."
"denote by s 1 and s 2 the matching distances of y andŷ to a gallery image, respectively. we propose to fuse the two distance scores for final decision making. in the following, we first briefly review the existing popular score level fusion methods, and then propose a simple but very effective adaptive fusion method, namely the adaptive binary fusion rule."
"in pdes, the model is divided into distinct independent virtual time lines referred to as logical processes (lps). each lp typically encapsulates a set of state variables of modeled entity. the time lines of lps within and across processors are kept synchronized by the pdes simulation engine. pdes engines may support optimistic synchronization or conservative synchronization, or a combination. the runtime environment allows multiple lps to be hosted per simulation loop (sl), and each sl is mapped to a single processor core. when executed on vm, this implies that each sl is mapped to a single vcpu. thus, the scheduling problem for pdes on vms becomes one of (indirectly) scheduling sls over pcpus."
"of virtual resources hosted on a hypervisor were equivalent to number of physical resources utilized. this is an important study characteristic because the cloud computing scales economically based on this concept. the understanding and a fact that not every vm hosted on a physical resource utilizes its compute resources continuously allows the vendors to host vms whose aggregate compute resources surpass the actual number of physical compute resources. while, this oversubscription does not introduce any problems, when vms are independent of each other or when the parallel tasks are embarassingly parallel, it makes a highly negative impact on a fine-grained pdes application's performance. this performance issue can be directly attributed to the virtual compute resource scheduling (hypervisor scheduling) strategy. figure 10 captures this effect using phold benchmarks over ltp, and these plots assert the need for a pdes-based hypervisor scheduler. in this set of experiments, the dom0 was set up to use two vcpu cores, thus at any time instance 30 remnant pcpus are available for a hosted vm or vms. we launched single vcpu vms equal to remnant pcpus (30) and increased the number of vms hosted until the number of vcpus became 10% greater than the number of pcpus. figure 10 plots the runtimes for varying pdes loads for a mere 10% increase in the number of hosted vcpus."
"a final consideration is that a scheduling algorithm based solely on least-simulationtime-first-order is susceptible to deadlocks that need to be resolved and implemented in a scalable manner with respect to the number of vms multiplexed by the hypervisor. thus, a new, deadlock-free, scalable hypervisor scheduling algorithm is needed to deliver the most efficient execution of pdes on cloud/vm platforms."
(3) the characteristic curve of linear acceleration is compared with the sample in database and calculate the similarity coefficient .the result is denoted as 4 . (4) the weighted voting process is as follows. (a) calculate the absolute error between and . the result is denoted as 1 .
"(c) fifteen traditional eigenvalues and 16 new eigenvalues (93 features in total) were extracted and fused for x, y, and z axes of a single gait period. first, the ability of each feature to distinguish samples was calculated using the relieff algorithm [cit] to verify whether or not the new features have better separability; second, alternative classifiers were used for classification experiments."
"we present an algorithm for interactive co-segmentation of a group of realistic related images. we propose icoseg, an approach that co-segments all images in the group using an energy minimization framework, and an automatic recommendation system that intelligently recommends a region among all images in the group where the user should scribble next. we introduce and make publicly available the largest co-segmentation dataset, the cmu-cornell icoseg dataset containing 38 groups (643 images), along with pixel groundtruth hand annotations [cit] . in addition to machine experiments with synthetic scribbles, we perform a userstudy on our developed interactive co-segmentation interface (also available online), both of which demonstrate that using icoseg, users can achieve good quality segmentations with significantly lower time and effort than exhaustively examining all cutouts."
"the results are timely due to the great appeal of commercial cloud offerings that many find to be very user friendly and convenient to access and manage. future work of interest includes incorporating and benchmarking the support for dynamic growth and shrinkage of physical processors allocated to a pdes run dynamically during its execution. using cpu-pools support of xen to get our pdes-specific scheduler into the realms of current cloud-computing infrastructure can make it accessible by more pdes application developers and users. pdes runs may also benefit from the inclusion of a network metric in the specification of the abstract computational unit for vms, the absence of which leaves the computation highly sensitive to the vagaries of virtual network devices. cloud-specific synchronization algorithms may also be needed to be resilient to variations in virtual network latencies."
"the above studies develop a universal feature selection for the human acceleration signal and obtain excellent recognition rates, but they do not explore the motion features of the person while walking and only select the single classifier with the best recognition rate to explore. due to these deficiencies, this paper extracts the rates of acceleration changes and amounts of velocity change associated with relative to uniform acceleration from gait data as new features, combines common time-frequency domain features to model and identify multiple classifiers, and proposes multiscale voting (msv) for fusion processing to obtain final recognition results."
"for local shape information for each point context that was ignored in the ltn algorithm, the paper introduces the concept of shape context (scltn) [cit] in ltn, which allows for the determination of correspondence between each data point in the periodic sequence. the shape context is a description for the shape, whose main purpose is to capture the relative position of each data point in space to achieve a stricter and more accurate match between points, as shown in figure 5 ."
"the human gait has a very unique pattern that can be used for identification and verification. traditional gait identification that is based on wearable sensors acquires gait data through bundling dedicated sensors on the fixed part of the human body [cit] . with the rapid popularization of smartphones and the continuous development of their functions in recent years, it has become possible to use the built-in accelerometer of the smartphone for gait identification for users in conditions that do not affect users' normal work, study, and life [cit] . however, factors such as the variety of smartphones and very large differences in performance of the built-in accelerometers lead to significant differences in the collected data, which increases the difficulty of identifying people and directly affects the accuracy and credibility of the recognition."
"description for the vmi system. consider a periodicreview, single item vmi supply chain system composed of a retailer and a manufacturer. in a vmi program, the retailer should provide the manufacturer with demand and inventory information. on the basis of these information, the manufacturer makes replenishment decision to control the inventory for both the manufacturer and the retailer."
"in order to verify the model, an example is generated from the assembly process of an auto-laydown robot. the auto-lay down robot, consists of the mechanisms of adhesive dispensing and auto-laydown, a pneumatic system and a control system."
to validate the in vitro test bench conception a comparison between post-processed in vivo and in vitro measurements was performed. thus the velocity measured in vivo using the combo wire as well as flow rate measured in vitro by the flow meter were processed for further analysis. the comparison was based on the same methodology i.e. the womersley's solution either set as a desired signal or used to calculate desired flow-generated quantities such as wss or wsstd.
"mosift not only encodes local appearance of an interest point but also explicitly models the local motion of the point. in the aspect of action recognition, [cit] have also demonstrated the superiority of the mosift over four existing descriptors, i.e., 3d histogram of gradients (hog), histogram of optical flow (hof), hnf (a combination of hog and hof), and grid aggregated hog and hof."
"where, u the iterative steps were repeated until the local maximum velocity of in vivo velocity vs. cfd velocity reached relative error a value less than 1%."
"in this section, we begin by studying the stability and inventory oscillation of the retailer. in the vmi program, the retailer may assume the risk of incurring high inventory cost or poor customer service level. to avoid such a problem, the retailer can increase its benefits by signing effective contractual agreements, which are characterized by the penalty cost once the retailer's inventory beyond an upper bound or a lower bound. to reduce the penalty cost and maintaining satisfactory service level for the retailer, the manufacturer must clarify how the replenishment rule affects the range of inventory fluctuations of the retailer. to this end, as a first step, we shall now discuss the inventory fluctuation range of the retailer under uncertain demand with theorem 1."
"in a review article dedicated to the shear stress biology of the endothelium, [cit] suggested future directions in this field of research. two points caught our attention: (1) the design of in vitro experiments that critically test the dominant fluid dynamic conditions existing in vivo and address the validity of in vitro models for intact tissues; (2) the determination of the importance of wall shear stress gradients on endothelial biology and of the temporal changes occurring throughout the cardiac cycle."
"visualization of microscopy data. volume visualization of microscopy data is an ongoing research topic due to their inherent visual complexity. different techniques for displaying features in dense em image stacks have been proposed, including multi-dimensional transfer functions [cit], local variance-based transfer functions [cit], and view-dependent on-demand filtering and edge enhancement [cit] ."
"to support the dynamic creation of queries in a visual way, we have implemented a visual query builder (see fig. 5 and the video). it allows the user to specify an arbitrary number of input objects and sets on which any specified predicate or operator should be evaluated. in addition to using already defined sets as input, the user can also directly select or pick input objects and sets from any of the linked views (see fig. 5 ). when the user has finished the specification of inputs and predicates/operators, the create set button evaluates the query and creates the corresponding result set. the result set is automatically stored internally, and can be used right away as an input set in subsequent queries. all sets (i.e., default sets and user-created sets) are listed in a set list widget, and can be inspected in detail with the set inspector widget. the set inspector lists all elements of a single set and allows inspecting the actual value of each individual object's attributes."
"in addition to the 3d volume view, we support 2d slice views. our slice views employ the same virtual memory architecture as the volume view and support arbitrary slicing planes. nevertheless, the scien-tists most often view the xy-plane, because of the high in-plane resolution of our em data. we also provide a manual segmentation tool that allows painting directly in the current slice view. the brush strokes are rendered into an off-screen buffer that is read back, and also transmitted from the client to the server for centralized storage. our current segmentation tool is completely manual, but in the future we would like to incorporate semi-automatic corrections for proof-reading [cit] ."
"the anycast communication model implemented in nephila does not impose a specific naming convention. for example, it can be used by mobile or fixed devices to announce that they provide a specific service (see figure 2 ). messages would then be indifferently forwarded by nephila toward these mobile devices, thus improving the delivery delay and ratio of these messages. this system is used in nephila to identify with an unique id the fixed devices that offer an \"internet access\". a message can thus be received, stored, carried and forwarded by several intermediate mobile devices, and be transferred to the destination through the same, or different, access point(s). nephila does not include any coordination mechanism allowing mobile devices connected to standard access points to decide which one must forward a given message, or allowing them to announce to the other connected devices that they sent a particular message. therefore, several copies of a message could be received by the destination. we assume that this feature is tolerated by the application."
"to improve both performance and computational efficiency, we employ a nonparametric density estimation method to select the most representative features from the extracted moiwld features."
"the goal of our first user was to analyze multiple-hit axons, i.e., axons making multiple synapses with the same dendrite, and to jointly look at their connectivity and morphology. the session started by querying all axons inside roi 'cyl 1'. next, their connections were grouped and counted, to see the dendrites each axon connects to and the number of shared synapses. in the next step, the user navigated to the synapses of these axons in 3d (with a single mouse click) and examined their local morphology. the main goal was to gain intuition for what the data look like and to discover new patterns that would trigger and guide further analysis. in this case, he wanted to analyze the location of the synapses of a multiple-hit axon that are connected to the same dendrite and find out if they are arranged sequentially or if the axon makes synapses to other dendrites in between. a sequential arrangement might indicate that synapses were made by chance, whereas a non-sequential order might indicate an underlying reason for why these two structures are connected. by using our tool the scientists were able to discover that the location of the synapses was non-sequential, which they now want to analyze in more detail."
"as standard access points do not run the nephila platform, the mobile devices connected to these access points will automatically add them in their list of neighbors, and identify these access points by their anycast common id. this id is obtained by applying a hash function on the name of the anycast group, and is disseminated like the id of devices using trail bloom filters."
"the first part of this section describes the scenario and the simulation environment we consider, and it enumerates the simulation parameters we used. the second part of this section presents the results we obtained, and discusses these results, mainly regarding message delivery delay and ratio. these simulation experiments were conducted using the one simulator 1 . one of the main objectives of this evaluation was to assess if data can efficiently be shared by mobile devices using nephila in an ichn."
"sparse representation has received a lot of attention in action recognition area, but most sparse models mainly consider minimizing the reconstruction error, and little attention is paid to better classification. recent research on supervised dl for sparse coding has been targeted on learning more discriminative sparse models [cit] . according to the predefined relationship between dictionary atoms and class labels, we can divide the existing supervised dl works into three categories: shared dl, class-specific dl, and hybrid dl."
"the optical flow approach detects the movement of a region by calculating the temporal differences of the region in two consecutive frames. compared with video cuboids or volumes, optical flow explicitly captures the magnitude and direction of a motion, instead of implicitly modeling motion through appearance changes over a time period. explicitly measuring motion is beneficial for recognizing actions. in our work, to add motion information into the iwld, we apply the same aggregation idea as that for generating the iwld to integrate the optical flow of every grid in a region into the iwld and form the moiwld."
"like all other queries, spatial queries are evaluated fully dynamically. however, intermediate data required by the implementation (such as blocks of object id histograms) are cached, which enables potentially re-using them without re-computation when evaluating future queries."
"lately, several investigators have tried to improve the different types of systems in order to overcome these limitations [cit] . two of these attempts involve [cit] similar systems. both aim to reproduce as accurately as possible the effective in vivo pressure signals. they also are closed, circular systems starting and ending in the same reservoir. the third one is based on the cone-plate model. they all have their advantages and disadvantages, the most important inconvenience being that they are mostly designed for one particular selected experiment. thus one of them [cit] was built to work with ecs in 3d geometry (and involves straight pipes). the constructors focused mainly on pressure signal control. the second one [cit] was built to allow researchers to work with different vessel geometries such as for example aneurysms and visualise different flows by using a piv laser. in this case, researchers focused on simple flows. the corn-plate system [cit] was designed to apply various shear stress signals on ecs attached to the plate. a 3d vessel model is also required to test medical devices such as flow diverted stents. however, a pipe system requires large medium volumes which may create sterility problems. this problem was circumvented by the cone-plate system as a little mound of medium is used. finally, none of these systems is fully automatic, they require manual adjustments, they have poor mobility and flexibility and they need an additional device to keep sterile conditions."
"statistics over sets of tuples. multivariate statistics can be computed automatically over all tuple dimensions, e.g., the covariance matrix or the product-moment correlation coefficients. these computations can be restricted to any subset of dimensions of a set of tuples."
"3d visualization. the need for 3d visualization of dense, noisy, and highly anisotropic em data was not obvious to our neuroscientists at the beginning. however, they now see the combination of 3d visualization with powerful query and analysis capabilities as having very large potential for data analysis, especially of the inherent spatial relationships, and for proof-reading segmentations. the design decisions for our volume processing pipeline have been discussed before [cit] ."
"in this paper, we present an anycast communication model and a network healing mechanism that offer, in intermittently-connected hybrid networks, enhanced data exchanges between mobile devices and remote hosts accessible through standard access points. this communication model and this mechanism have been implemented in a middleware platform, called nephila, with which we investigate communication and data offloading in wide ichns, using a peer-to-peer approach 11 ."
"3) results on the behave data set: we compare our approaches with the state-of-the-art approaches implemented by us on the behave data set, where 20 clips of this data set are randomly picked for training. table iv presents the results obtained with the above-mentioned methods on this data set. the dictionary size is fixed to 1800 in this set of experiments. as it can be seen from table iv, our proposed src-based method outperforms other approaches. this again demonstrates that the proposed approach is significantly superior in performance to all other approaches. the performance of the rvd method [cit] and the amdn [cit] on this data set is consistent with their performance on the hockey fight data set. furthermore, our moiwld combined with our proposed sparse classification method outperforms src methods. it validates that the representation-constrained term and coefficient adjustment term can improve the discriminative fig. 9 . examples of false alarms on the behave data set. ability of sparse representation model. the results on this data set demonstrate that our algorithm is also effective for detecting violence in a group fighting scene. false alarms happen only when a group of people get together to do some strenuous nonviolence activities (example frames are shown in fig. 9 )."
"finally, we simply use the linear predictive classifier to estimate the label i of vector l, and the label index corresponds to the largest element of the vector l."
"2. an inlet boundary condition with normal inflow velocity. starting with a mean velocity equals to half of the peak velocity from the in vivo data, an iterative adjustment is performed on that inlet mean velocity in order to fit the unsteady in vivo measured peak velocity as follows:"
"the presented post-processing method is compatible with any in vivo signals and demonstrated the heterogeneity among patients in the pulsatile pattern in coronary arteries. in addition, we provide a new test bench able to reproduce human coronary blood flow for in vitro experiments."
"perpendicularity error is the end point error affects the maximum error term. through the simulation can be found, the perpendicularity error and initial system indicates the vector has obvious relationship, the initial vector indicating the direction and single perpendicularity error corresponds to two axis formed by the plane included angle is bigger, the perpendicularity error of end point error smaller effect. figure 4 shows the shaft rotates, different inner frame axis and the vertical axis of the frame errors on end point error effect curve."
"nephila performs a proactive discovery of fixed and mobile neighbor devices. this discovery relies on both a beaconing mechanism and a cyclon-based service 12 . based on this service, nephila creates and maintains logical links between fixed devices so as to form a backbone. this backbone helps cover a wide area and support communications between remote mobile devices. the scalability of our system results from the existence of this backbone."
"when running in ad hoc mode, mobile devices can advertise the anycast groups they belong to in their beacon messages (as a hash value). like for the access points, the id of these anycast groups will be automatically added by the neighbor devices to their tbf. an example is shown in figure 2 . in this figure, the results of the hash function applied to anycast groups \"internet access\" and \"service#2\" are respectively 12 and 13. in this example, node 3 can add 12 in its neighbor list, and node 2 can add both 5 and 13. node 5 can thus be reached by node 2, using either its own identity (i.e., 5) or its anycast group identity (i.e., 13). application-level messages that must be sent to remote hosts on the internet will be encapsulated by the emitters in nephila's messages, with the anycast group id of access points as destination address (i.e., 12 in our example). these messages will be extracted from nephila's messages by the mobile devices connected to the access points, in order to invoke the appropriated remote host on the internet. these mobile devices will perform the reverse operation when they must forward a response back to the client mobile node. the best trail selection algorithm (btsa) is let unmodified as anycast groups are transparent for it."
"nephila implements a forwarding algorithm called btsa for \"best trail selection algorithm (btsa)\". this algorithm uses the transitive property of the tbfs and promotes, while forwarding a message, the devices that met the destination of the message the most recently. to do so, each device takes a local decision based on its own tbf and on those sent by its neighbors. when a device receives a message from a neighbor or from a local application, it forwards this message to the neighbor that has the greatest trail value for the destination, provided that this value is greater than its own value. when a device receives a tbf from one of its neighbors, it looks for all the messages it maintains in its local cache, and it sends to this neighbor copies of the messages for which it considers the neighbor as a better forwarder than itself. more details about the btsa algorithm can be found in 11, together with simulation results that confirm its efficiency."
"multi-volume rendering. the modularity of our volume handling and rendering system supports a rather straightforward extension for multi-volume rendering. our application employs multiple instances of the same virtual memory architecture, and allocates separate cache textures for the em and segmentation volumes, respectively. actual mixing of these volumes is performed in the ray-caster. this can be done by using the object id of the current sample mapped to either a procedurally assigned color, or via a user-defined color table that assigns a specific color to each object id. furthermore, we support different rendering modes and individual clipping of each volume. by default, we blend the color of the current sample's object id with the color of the original em data after applying a 1d transfer function."
"the aim of this project was also to provide bioengineers and physicians with a tool that will be able to mimic blood flows and later automatically calculate desired flow properties, such as wss and wsstd."
"in five patients, in order to gain knowledge of in vivo instantaneous maximal velocity of coronary arteries. in addition, the diameter of the artery at different points was measured by quantitative coronary angiography (qca). moreover, the hematocrit level from each patient was obtained. in order to extract additional derived physical variables, such as instantaneous flow rate, wss and wsstd from the in vivo measurements, these quantities were post-processed. this applied post-process consisted in analytical integration of the unsteady navier-stokes equation using the womersley's relation based on the assumption of a newtonian fluid with constant viscosity in a straight rigid pipe. since blood has a viscosity that varies with the shear rate [cit], we used 2d axisymmetric, non-newtonian cfd simulations (using a carreau model) to validate the approximation of a newtonian behavior. several projects are being carried out worldwide to provide a system capable mimicking, outside the human body, the circulatory system [cit] . however, these in vitro systems mostly focus on constant and simple harmonic flow oscillations. this can be an important limitation because in vivo studies have found [cit] that the flow topology and the consequent shear stress do have a significant role in vascular response. moreover, some of these experiments hardly incorporate physiological properties in their experimental conditions such as sterility, gas balance, proper ph and osmolarity as well as duration of experiment."
the patients were selected from those who needed a coronarography. the protocol number b3252006117 was approved by the ethics committee of om008 of the isppc (intercommunale de santé publique du pays de charleroi) hospital and the volunteers gave their written informed consent. coronary arteries were selected devoid of any pathology. the subjects included in the study are described in detail below:
"the last but not least assumption regards the non-newtonian behavior of the blood. in normal conditions blood is a heterogeneous media principally made of about 55% of liquid plasma and of about 45% of red cells in suspension in the plasma. the concentration of red blood cells modifies the apparent viscosity of blood. moreover, depending on the level of shear rate, that apparent viscosity changes and leads to the non-newtonian behavior of blood, with viscosity decreasing as shear rate increases. however for our analytical estimation, we can assume that blood viscosity of a vessel whose diameter is larger than 1 mm is relatively constant, as the values of shear rate on the wall passes into a deformation region (above 100 s ) where the viscosity is relatively constant. therefore for the first calculations we used the average viscosity from carreau model in the range of shear rate (0-1,000 s"
"using matlab 7.0, we can calculate the value of 2 based on the transfer function model (17) accurately. the following numerical analysis will disclose the influences of replenishment parameters, production lead time, and forecasting parameters on the worst-case bullwhip effect for unknown demand. we see that the parameter is not included in the state space model (13) and thus it has no influence on the smoothness of production fluctuations. meanwhile, the robust metric 2 is irrelevant to the retailer's lead time ."
"intermittently connected hybrid networks (ichns) represent a specific class of intermittently connected networks, in which fixed equipments are interconnected together in order to form a backbone. in this paper, we have shown how ichns and a peer-to-peer overlay system such as nephila could be used by mobile operators to offload data from their mobile network in order to improve the quality of service they provide to end-users. nephila com-putes message forwarding paths based on information about past contacts between nodes, and using both utility-based functions and specific bloom filters called trail bloom filters. it forwards messages towards access points using an anycast communication model in order to improve both the message delivery delay and ratio. finally, it implements a healing mechanism in order to reduce drastically the number of messages that are disseminated in the ad hoc parts of the ichn. simulation results confirm the efficiency of both the anycast communication model and the healing mechanism."
"measurements were made using a combomap ® analysis unit (volcano corp.), a pcbased system with an lcd color touch screen and a remote control operation. the combomap ® system provides both pressure and flow rate (fractional flow reserve (ffr) and coronary flow reserve (cfr)) and allows digital storage to the hard drive or archival storage to a cd. a doppler flow intravascular wire (combowire ® with a diameter 0.3556 mm) was used to measure the instantaneous velocity. the experimental data obtained by in vivo measurements in the coronary artery of all patients were recorded for further analysis. the resulting measured physiological data shows variations in the time period and velocity between each time cycle (fig. 2) . the variations indicate that the heartbeats are not constant and there are natural variations between cycles. moreover, there are perturbations/artefacts in the measured data (not shown). therefore, the measured data had to be processed in order to remove aberrant data and perturbations as well as to extract a representative signal for the measured velocity. the principle of the filtering strategy is presented in fig. 2 . firstly, the in vivo data were processed to remove perturbed data probably caused by movements of the patient and/or the measurement sensors. the whole periodical signal was then cut into period signals, in the values, where minimum of a periodic velocity is, marked as circles (fig. 2a ). the cut periods were then filtered based on the following algorithm: the maximum, mean and minimum velocity, as well as the time period of specific cut periods should not vary more than 20% from the average respective value. however, the resulting physiological data still showed noticeable variability in the time period and the velocity between time cycles. this variability is illustrated in table 1 and fig. 2b . as shown for the first patient in fig. 2b, the maximum velocity varied from 23.9 to 41.9 cm/s and the minimum velocity from 0.2 to 7.2 cm/s; the time period varied from 0.93 to 1.19 s. therefore, to obtain a representative period, an average of all the filtered cycles was constructed, shown as the solid black line in fig. 2b . the resulting average artery velocity signal was decomposed into a complex fourier series for further analysis. we used 50 harmonics to obtain good accuracy in the fourier representation of velocity for each patient. the mean relative error was less than 0.5% for all cases and bellows the standard deviation resulting from the time variability of the measured signals. the minimum, maximum and average results for all patients are given in table 1 . results of the fourier decomposition are shown in fig. 3 . the right panel (from top to bottom) illustrates the maximum filtered velocities. the velocity data varied from patient to patient both in intensity and curve shape. the fourier series spectral analysis was also drawn in order to illustrate the filtering effect on the processed signals ( fig. 3, left-hand panel, from top to bottom). results show that the filtering effects were different from patient to patient. in the differences between cut points present a period δ of the heart beat. b filtered coronary velocity cycles for patient 1. gray lines represent superimposed filtered averaged velocity curves measured for all cycles in patient 1. the block line represents the mean filtered velocity signal. variation in maximum velocity is indicated by the double-headed arrow. u min ± δ, mean velocity; u mean ± δ, maximum velocity; u max ± δ as well as period of the cycle δ ± δ (arrows). patients 1 and 5, the spectral peaks were not altered by filtering. in contrast, the noise that appeared in patients 2, 3 and 4 required a strong filtering to obtain a representative signal. as a result, the number of representative cycles had to be reduced; therefore, for these patients, there is a shift between the spectral peaks."
"1. what is the average number of vesicles of a specific axon that was picked by the user in the 3d view? table 3 . timings and results for analyzing our use cases. we state the typical time the user took to answer the questions defined in sec. 7.1. the numbers in the first column refer to the query numbers given there. we also state the time it takes our system to evaluate the query (query evaluation), and the time the scientists took to evaluate and look at the results (post-analysis). results of individual queries are shown on the right."
"angiographies showing the position of the wire for in vivo measurements. an angioplasty was performed (pictures took for patient 1) in the left anterior descending artery (not shown). access was obtained via the radial artery using a 6 fr xb 3.5 guide catheter. it was then used to insert a combowire ® allowing coronary blood velocity to be measured. angiographies were taken from two different angles to visualize the position of the measurement point at a central position (right, zoom)."
. the position deviation at any point of the part due to the rigid body motion is completely determined by the six components [cit] .
where the arctangent function is used to prevent the output from being too large and thus partially suppressing the side effect of noise and α is a parameter used to adjust the intensity difference between neighboring pixels.
"furthermore, to assess the impact of dictionary size on classification accuracy, we have run the experiments with dictionaries of different sizes. as shown in table ii, with the increase in the dictionary size, the performance begins to rise and then stays stable. this phenomenon indicates that selecting an appropriate dictionary size is significant to both accuracy and computational efficiency. table iii shows the results obtained by the proposed approach after adopting the bow approach and the proposed sparse classification model into the moiwld approach. the dictionary size is fixed to 1800 in this experiment. we compare the results with not only hog, hof, hnf, mosift, mowld, and moiwld paired with bow, but also vif, rvd, and amdn. the rvd method [cit] adopts a gaussian model of optical flow to extract candidate violence regions, which has reduced many noise disturbances, so its performance is better than the bow-based approaches. the amdn approach [cit] utilizes optical flow as the input image feature, and there exist many redundant and interference features, so its performance is not very good. it can be seen from table iii that using our proposed moiwld together with the proposed sparse classification model has performed the best. furthermore, our approach outperforms the competing src algorithm and mowld + sparse coding with the same size of dictionary. this is due to the fact that our proposed supervised class-specific dl framework incorporates representation-constrained and coefficient adjustment terms resulting in the highest recognition rate."
"in our proposed model, two terms, i.e., the representationconstrained term and the coefficient adjustment term, are introduced and described in the following to ensure that the learned dictionary is sufficiently discriminative. the representationconstrained term is utilized to enforce the class-specific subdictionary to have a good capability when reconstructing a query image using training samples having the same class label. on the other hand, the coefficient adjustment term is utilized to enforce the class-specific subdictionary to have a poor capability when reconstructing a query image using training samples with different class labels. based on these two terms, a classification scheme is then developed to exploit the discriminative information."
"the study can be divided into three parts: (1) measurements, including in vivo real time velocity; (2) post-processing measured data including filtering of the measured velocity and calculating derived quantities, such as flow rate, wss and wsstd, 3) reproduction of in vivo patient's signal by presented developed in vitro test bench."
"we currently support two basic-but very similar-types of topological queries. both are specified with an identical syntax via the connected predicate. this predicate determines if objects are either connected or \"a part of\". for example, a connected axon and dendrite, or a spine of a dendrite, respectively. our system encodes and implicitly uses all topological knowledge according to object type. 5 . dynamic queries and linked views. all views allow direct user interactions (e.g., picking, selection) that can be used as input for new queries or for exploration of the data set. new queries are specified in the visual query builder and automatically update all views after query evaluation."
"as an improvement, in our proposed model (detailed as follows), each sample is locally approximated by a linear combination of its nearby samples and a classification error is introduced. the proposed sparse representation model selects the samples that can best represent their own action classes as the bases of their class-specific dictionaries. specifically, we combine the reconstruction error with a classification error to form a unified objective function. the learned dictionary encourages the signals from the same class to have similar sparse codes and those from different classes to have dissimilar sparse codes to achieve accurate classification results."
"these calculations of the real coronary wall shear stress signals are critical for future development. the challenge will now be to reproduce these signals perfectly in this in vitro system. meeting this challenge (see above) will also allow two critical observations made in this paper to be exploited: patients can be divided into different groups with different behaviors, which are much more complex than described in previous publications. these observations in turn raise two critical questions: which of these behaviors is important in triggering the ec pro-atherogenic genes? do these different behaviors explain why many patients with no risk factors develop a clinical condition and some patients with risk factors do not?"
"the main goal was to perform experiments with the simplest settings. first in vitro tests involved straight rigid pipes containing a medium with newtonian properties. however, the necessary velocity measurements had to be made with blood, which has non-newtonian behavior. in this study, we extracted detailed information on the physical flow quantities, such as pulsatile flow, wss and wsstd from in vivo measurements using the wormesley's solution (usually applied to a newtonian behavior). our subsequently comparison with numerical non-newtonian, based on the carreau model, cfd calculations indeed validated the womersley's solution for five patients tested. the mean relative errors for the flow rate vary between 3.2 and 6.3%. as presented, both techniques give rise to similar results with a difference growing for smaller flow rates (figs. 6, 7, 8 ). thus, the womersley's solution is a fast method that can be used to provide satisfying approximations especially for the higher flow rates where shear rate is higher than 100 s −1 and where the non-newtonian behavior becomes negligible. for some cases, where more accurate and detailed analysis is required, is should nevertheless be recommended to use cfd techniques. based on the previous results and the womersley's solution, the comparison with experimental in vitro data was performed. the results in table 4 suggest that the concept behind the whole control system is valid. the mean relative error is below 5% for all investigated signals. in addition, we have to bear in mind that we are dealing with very fast control loops. labview ™ executes one cycle for all the input/output signals in a time period of 5 ms (200 hz). it is much quicker than in most industrial controls of mechanical systems. also, the system is highly non-linear and depends on external conditions such as medium composition that can vary a lot depending on supplements or serum. other sources of variability include properties of the equipment such as, for example the diaphragm age, external temperature and so on."
"dimensional variation reduction of the product is a crucial engineering objective in both design and manufacturing stages dimensional variation can stem from both the design and manufacture of a product. since some manufacturing induced variation is inevitable, it is important to minimize the level of inherent dimensional variation caused by product and process design. many of the problems associated with dimensional accuracy occur because the capability of the manufacturing process is not considered when designing the product and process. these problems may affect the final product functionality and process performance. for example, large product dimensional variation in an automotive body assembly process may cause product problems such as water leakage and wind noise, as well as process difficulties such as fitting problems in subsequent operations. clearly, reducing dimensional variation in an assembly process is of critical importance to improve the final product quality. in addition, a better understanding of a process behavior can also bring a reduction in the time needed to launch a new manufacturing system [cit] . early and accurate evaluations of inherent process variation are crucial factors in determining the final dimensional variation of an assembled product. in recent years, the importance of dimensional variation has been observed by an increasing amount of research conducted in the area of sheet metal assembly processes. since take observation that for compliant sheet metal assemblies the traditional additive theorem of variance is no longer valid, several models have been proposed to represent the variation propagation on assembly processes."
"whenever a predicate is specified, the result set comprises all elements for which the predicate evaluates to true. for operators, the result set depends on the type of operator. predicates can be restricted to any subset of tuple dimensions for their evaluation."
"to limit the scope of this model, in this paper, we focus on the analytical modeling of the relationship between setup errors and dimensional variation propagation in a multistage process. since the setup error is the key cause of variation propagation, the influence of setup error on product quality will be thoroughly studied through kinematics analysis. although other types of errors are not studied in this paper, interfaces to other errors are included in the developed model, making the model flexible: the model can be extended to include other errors components influencing the product quality. the rationality for considering only dimensional instead of both dimensional and geometric variations is that this model focuses on describing the machining variation propagation among different stages. since most of the geometric variation is determined on a single stage and remains unchanged throughout the whole process, it is unnecessary to build a model describing its propagation. therefore, geometric variation is not included in this model."
"this dimensional deviation process can be decomposed into six independent steps in accordance with the six rigid body motion components. since translational deviations are the same for the whole part, we only need to focus on the rotational movement."
"in addition to the previous literature, optimal replenishment policies of vmi supply chain systems with multiple retailers have been studied frequently over the recent years [17, [cit] . nevertheless, we see that the existing literature has ignored the dynamic fluctuations of order and inventory, which can be hardly incorporated into the previous optimization or game-theory based models. although our vmi model is similar to an early work by disney and towill [cit], we generalized their model by allowing for free adjustment of the amount and timing of replenishment frequency. another important difference is that we pay attention to the trade-off between the production smoothing and inventory fluctuations by providing some analytical results, while their work is focused on simulation experiments."
"this paper has targeted on effective stip detection and feature representation for detecting violent behaviors in a real video scene. following our recently proposed mowld, two major improvements have been made. first, considering the shortcomings of the well-known wld, we have proposed the iwld, and then proposed to extend the iwld by adding a temporal component to the appearance descriptor to obtain moiwld. moiwld implicitly captures local motion information together with low-level image appearance information. second, a modified sparse model has been proposed to learn a dictionary for classification. in the proposed sparse model, the representation-constrained term and the coefficient incoherence term have been introduced to ensure the learned dictionary to obtain a powerful discriminative ability. with this learned dictionary, both the representation residual and the representation coefficients are discriminative. based on the proposed src, we have presented a corresponding classification scheme. the experimental results on three benchmark data sets have demonstrated the superiority of the proposed approach over the state-of-the-art approaches."
"in contrast of the anycast message delivery system proposed in 14, the system we have designed is meant to be as simple and as generic as possible. it does not make assumptions about the mobility of the people. in 14, each node computes its message delivery probability to the destination anycast group. three different forwarding algorithms exploiting historical node encounter information are considered to guide the transmission of messages. like in nephila, the group forwarding metric treats the entire group as a whole. it is defined as the probability of meeting any of group members to deliver message. the probability forwarding metric is defined as the probability of encountering at least one anycast group member and delivering the message to the member. finally, the distance forwarding metric estimates the delivery probability to a group member as the distance to that member. in this anycast routing algorithm, a single-copy of a message is forwarded to the destination anycast group. so, it is suited for relatively dense opportunistic networks, where nodes have regular mobility patterns. like in nephila, the anycast routing algorithm presented in 15 does not make assumptions regarding the mobility of the nodes. nevertheless, it supposes that messages can initially be sent only by stationary nodes, the mobile nodes only acting as message carriers. obviously such a supposition is not realistic, because in practice contents are produced by users of mobile devices."
"our system offers a variety of views to support users in data analysis and exploration (see figs. 5 and 6 ). all views are linked and update to display query results, but also allow for independent data exploration."
"most previous research has focused on the actual image acquisition and subsequent semi-automatic or fully automatic segmentation of em slice stacks. however, less research has focused on the next step: on how to enable efficient neuroscientific analysis of volume and segmentation data of this size and complexity. neuroscientists now have huge collections of high-resolution em volumes and their segmentations, but no efficient means for analyzing them or directly answering high-level domain-specific questions. most current tools only provide 2d visualizations of subsets of the em data or the corresponding segmentations. the latter are either provided at the voxel level or-more commonly-consist of extracted geometry. to the best of our knowledge, no existing tool offers interactive 3d visualization of multiple teravoxel volumes, e.g., em and segmentation volumes, while at the same time allowing scientists to dynamically explore and analyze the entire data set by posing domain-specific questions in an intuitive way."
"as mentioned earlier, several studies have correlated shear stress perturbations with ec behavior. recently, other studies have correlated wsstd with the development of the atherogenic phenotype [cit], with for example, wsstd-induced ec proliferation, upregulation of platelet-derived growth factor (pdgf-a) and monocyte chemoattractant protein-1(mcp-1) and enhanced monocyte binding [cit] . investigations have also demonstrated effects of wsstd and temporal gradient in shear on ec remodeling [cit] . however, the wsstd signal in these studies was deduced as the result of ramp, step, sinusoidal or impulse laminar flow. in this paper, we calculated wsstd (figs. 8, 10, right-hand panel) based on real in vivo and in vitro measurements. clearly wsstd signals are not a simple impulse or ramp. thus, during one heartbeat, there was a wide variety in the signal, with rapid increases and decreases, as well as different magnitudes observed in our patients. how this complex signal variety can influence ec remodeling and gene expression remains an open question, which needs to be addressed in future studies. we can classify signals recorded from the 5 patients into two groups: (1) patients 1 and 5 with two wsstd peaks at the beginning and end of the cycle and relatively constant values in-between; and (2) patients 2, 3, 4 with many random peaks during the whole cycle. this observation raises several questions: is there a difference between these two groups regarding their influence on ec remodeling? could one of these two groups reflect patients who develop atherosclerosis without any classical risk factors, such as smoking, fat diet or hypertension?"
"vendor-managed inventory (vmi) is a well-known collaborative program [cit], in which a retailer propagates inventory information and point of sales data to its supplier, while the upstream supplier makes replenishment decision for both the retailer and itself. [cit] s [cit], vmi has been successfully implemented by many leading firms from different industries, such as glaxosmithkline [cit], nestle and tesco [cit], boeing and alcoa [cit], and shell chemical [cit] ."
"an analytical approach has been developed to perform variation analysis and robust design for part locating datum systems. the kinematics equations for rigid body motions are simplified through linearization. the simplified formulations explicitly relate the dimensional deviations of a rigid part with its datum scheme configuration and dimensional variations at datum target points. this simplified approach can be used with either the first order taylor series approximation or monte carlo simulation to study the statistical characteristics of datum scheme variations. in order to verify the model, an example is generated from the assembly process of an auto-lay down robot."
"after several sessions with connectomeexplorer, the user found our system extremely helpful for exploring their data. especially the ability to form and try out new hypotheses interactively was considered to be very useful, because the scientists still know relatively little about their data, simply because they did not previously have adequate tools for analysis. they greatly appreciated and used the graph view for analyzing the connectivity. however, an additional suggestion was that they would like to have a graph view that also represents a simplified spatial morphology of the data instead of only abstract connectivity."
"iolent behavior seriously endangers social and personal security [cit] . a violence detector has immediate applicability both in the surveillance domain and for rating online video contents. currently, millions of video surveillance devices have been used in public places such as streets, prisons, and supermarkets (some sample frames from benchmark data sets are shown in fig. 1 ). visual surveillance systems collect huge amounts of videos, but humans must still review most of the data to extract informative knowledge. our goal is to automatically recognize violent behaviors without carefully labeling data over large archives."
"finally, to display a roi in the renderer, we restrict ray-casting to voxels inside the region by simply adjusting the start and stop positions of rays according to the geometry of the specified roi (see fig. 5 )."
"to support the on-the-fly evaluation of spatial queries, such as evaluating a region of interest (roi) or the proximity of one object to another, we employ an octree-based culling structure that allows quickly finding all volume sub-blocks corresponding to a specified roi or object."
"nephila 11 is a middleware platform we have designed to support communications and data offloading in intermittentlyconnected hybrid networks (ichns) such as that described in section 1. nephila allows to build dynamically a decentralized and unstructured peer-to-peer overlay network to support the communication between mobile devices in a wide ichns. it also provides users of mobile devices with an enhanced internet access in ichns thanks to an anycast communication model. this model permits to identify the standard access points with the same id. it aims at reducing the message delivery delay while at increasing the delivery ratio, and therefore the quality of service provided to the end-users. this communication model is combined with a \"healing\" mechanism devised to stop the opportunistic dissemination of message copies in the network. this anycast communication model and this healing mechanism are detailed in the next sections."
"1) representation-constrained term: z i denotes the sparse coefficients of a i over dictionary d, so a i ≈ dz i . since z i is associated with class i, it is naturally expected that z i can be well represented by only m i because m i denotes the mean vector of z i in class i . therefore, there should exist a z i such that z i − m i 2 f is small. this term can control the reconstruction error of coefficients z i . on the other hand, w denotes the matrix of classifier parameters and b records the class labels, so"
"for a manufacturer, reducing production fluctuations is significant because it is closely related to production cost. smoothing production process helps arranging labor force, deciding the manufacturing capacity, and also enlarging the life of production machines. it is well recognized that production fluctuations can be smoothed by mitigating the bullwhip effect [cit] . actually, stability is a fundamental step for smoothing production fluctuations because it makes no sense to study the bullwhip for an unstable supply chain system. in the following, based on the stability conditions in theorems 2 and 3, we will further study the production smoothing behavior in terms of the worst-case bullwhip effect based on the transfer function model (17) . the bullwhip effect problem has received considerable attention since it causes high cost for upstream firms in a supply chain firm due to the distortion of demand information. the majority of the literature focuses on the bullwhip effect of supply chain system for specific demand with statistical method and control theory method [cit] . however, the customer demand in real life is highly uncertain over time and the probability distribution used to approximate real life demand can be inaccurate or unreliable [cit] . to this end, we aim to understand the bullwhip effect of the vmi supply chain by combining the state space model and transfer approach for unknown demand. as mentioned, the state space model can be readily constructed from the difference model, which can be conveniently used to derive transfer functions. as mentioned, it is complicated to derive transfer functions by combining block graphs. here we study the bullwhip effect for unknown demand using transfer function because it is easier to calculate the robust bullwhip metric."
"based on the experimental in vitro data as well as in vivo data, two important physiological quantities i.e. wall shear stress (wss) and wall shear stress time derivative (wsstd) were also derived and compared. they are shown in fig. 10 . the differences observed between the in vivo and in vitro averaged data were correlated to the flow values measured and presented in the previously paragraph. like for the flow, the main shape of the wss as well as the wsstd (grey line) generated in vitro matched well the reference signal generated from in vivo data (black line). however, as mentioned for the flow results, the calculated physical quantities for in vitro data are mostly underestimated compared to calculated in vivo data. again, periods of rapid changes (peaks) of the wss/wsstd were associated to the largest calculated discrepancies. moreover, due to the difficulties encountered in mimicking the fast change of the wss/wsstd it was not possible to reconstruct accurately enough negative values of the wss/wsstd."
"the benefits of vmi for both suppliers and retailers have been extensively studied in the literature [cit] . for vendors, the vmi program improves decision-making flexibility since they gain the right to determine the amount and timing of the order for retailers, which further leads to reduced bullwhip effect [cit] and better utilization of manufacturing capacity, as well as better synchronization of replenishment planning [cit] . for retailers, vmi saves ordering cost and inventory cost and ensures the customer service level through signing vmi contracts. for example, a supplier will be penalized once the inventory of retailer exceeds certain ranges [cit] . to successfully implement the vmi program, how to make use of the flexibility of decision-making in inventory control to benefit both upstream and downstream members is a highly challenging problem."
"2) coefficient adjustment term: given a test sample, [cit] suggested that its sparse representation could be found by an src scheme, and in the sparse coefficients recovered by src, the largest coefficients were associated with the training samples that had the same class label as the test sample. it implies that the test sample can be approximated by a weighted linear combination of its own training samples with these largest coefficients. likewise, in our proposed class-specific dl, because w denotes the set of classifier parameters, minimizing w 2 f is for the class-specific dl to reach the minimum classification error."
"systemic risk factors (e.g., dyslipidemia, smoking, hypertension, diabetes and genetic predisposition) [cit], atherosclerotic lesions form at specific regions of the arterial tree. this behavior is associated with local variations in arterial hemodynamic properties and more particularly with the pulsatile time dependent evolution of the wall shear stress. the effects of shear stress are linked to the expression levels of atheroprotective and atherosensitizing endothelial genes [cit], cells need to be able to adapt to these constantly changing stimuli."
"for the in vivo measurement part of this work, the critical question relates to the diversity of in vivo velocity signals. despite this variation, a method that allows us to deduce desired additional flow properties such as flow rate, wall shear rate, wall shear stress and wall shear stress time derivative has to be found. moreover, as mentioned, the main purpose is to find the best method to quantify the flow properties from in vivo and in vitro measurements for further in vitro tests. the chosen method has to be accurate enough to reasonably describe properties of blood flows. furthermore, it has to be fast enough in order to take as few computer resources as possible and easy to implement in any existing programming language such as c, matlab ® or labview ® . for that purpose, the following elements had to be taken into account. first, as mentioned above, at the first stage, in vitro experiments will be carried out on a straight, rigid and circular tube, seeded with endothelial cells. moreover, the medium that will be used to fill the in vitro circuit is a newtonian medium. the flow in most coronary arteries excluding the heart, perturbations in the arteries such as stenosis and big arteries has a laminar character. these physical simplifications of the future in vitro model lead to many mathematical simplifications which allow to select the most widely used method in the blood calculations i.e. womersley's solution [cit] . this is the analytical solution obtained by integration of the unsteady navier-stokes equation in a circular vessel. womersley's solution is a valid solution for a newtonian liquid with a constant viscosity. however, blood viscosity has non-newtonian character. therefore, a 2d axisymmetric, non-newtonian cfd simulation was used to validate the assumption of an approximate newtonian behavior of blood. below, briefly, are presented equations that were used to calculate physical quantities and assess the validity of assumptions. for the analytical calculations a script in matlab ® program was written."
"although the opportunities of vmi are evident in many industries, the benefit allocation and coordination problem has been argued continuously [cit] . a number of authors argued that the majority of the benefits from vmi tend to flow to the supplier, rather than to the retailer [cit] . lee and cho [cit] examined the problem of designing a vmi contract with consignment stock and stockout-cost sharing in a (, ) inventory system between a supplier discrete dynamics in nature and society 3 and a retailer. the result showed that the retailer may not always benefit from vmi. however, mishra and raghunathan [cit] found that vmi intensifies the competition among manufacturers of competing brands, thus providing benefits to retailers. pan and so [cit] analyzed the interaction among the assembler and two component suppliers under a vmi contract. one supplier has uncertainty in the supply process, in which the actual number of components available for assembly is equal to a random fraction of the production quantity. under deterministic and stochastic demand, the optimal component prices offered by the assembler are derived."
"the min/max values and object id histograms required for culling are initially computed for each 2d image tile in the data-driven part of our pipeline (sec. 3.2). during the block construction stage, this information is combined according to the requested 3d block, and transmitted to the client. this speeds up empty space skipping and allows culling for spatial queries to be performed hierarchically (sec. 6)."
"the principle of the control loop of the flow pulsatile rate is presented in fig. 5 . the reference signal is divided into two signals: the mean flow and the pulsatile fluctuations of the flow. from that division, we have two main controls. in the first control (centrifugal pump in fig. 5 as a red square), the mean flow is controlled by the pi controller number ii and the function \"mean ii\". the function \"mean ii\" is responsible for calculating the mean flow at each cycle. it adds every point of the measured flow cycle and divides the latter by the number of the measured samples for this cycle. then the signal is sent as a measured signal to the adder in order to compare with the mean reference value. after that, the controller pi ii sets the rotation of the centrifugal pump in order to get the desired mean flow rate [the transfer function g cp (s)]. thanks to the function \"mean ii\" the control loop is less sensitive to disturbances on the mean flow, which helps to achieve the stability of the whole system. the second control loop is responsible for controlling the shape of the pulsatile flow. this loop is based on the internal model control (internal model control in fig. 5 as a blue square) and is additional to the pi controller. basically, the reference signal is compared with a measured signal and is sent to the internal controller g c (s). after that, the controlled signal goes through a saturation block which sets the movement limits of the linear motor. the signal is then sent to the controller of the linear motor and drives the movements. later the flow is read by the electromagnetic flowmeter that is presented in fig. 5 as a transfer function between the position of the linear motor and flowmeter g p (s) and in addition is read by model process g mp (s) in order to predict the position of the linear motor. the signal then passes by a filter f and is compared with the actual position of the body. in addition, in order to prevent the linear motor from drifting, we incorporated an additional loop (drift of the piston in fig. 5 as a green square). this loop calculates a mean position for every cycle separately and fig. 5 the concept of the control system. the main control strategy consists in three parts. the internal model control (marked as a blue square) is responsible to control the position of the piston pump. in order to reduce noises from measured flow signal as well to avoid drift of the piston a filter (marked f) and an additional loop which is based on a proportional-integral controller and a function mean i were incorporated (marked as a green square). to control the mean flow rate a proportional-integral controller and a function mean ii were added (marked as a red square)."
"the impact of the healing mechanism on the volume of messages disseminated in the mobile parts of the ichn is shown in figures 4g and 4h . the efficiency of this mechanism depends on the number of messages that have been delivered, on the number of copies of messages stored on devices, and on the number of contacts between devices (since each contact is an opportunity for devices to exchange their healing tables). the efficiency gets higher as the number of devices increases. it allows to save up to 175 gbytes for 4000 pedestrians and 800 access points. the additional average load generated by this mechanism is shown in figure 4h . in the worst case, the average cost per node is about 8.5kbit/s which is relatively low in comparison of the capacity of wi-fi links."
"we currently support the evaluation of cylindrical, spherical, and boxshaped regions of interest. multiple rois can be combined in a query to create more complex rois. once the user has specified a roi, we compute the set of contained objects in that region, which can either be displayed directly or used as input for subsequent queries."
"we will validate the theoretical results with uniformly distributed demand ∼ [, ] . the reason we choose uniformly distributed demand is that the upper bound and lower bound can be readily known. it is worth noting that the above results can be applied to other demand functions as long as we know the maximal value and the minimal value of the demand data. based on theorem 1, we define ℘ 1 ≜ ( + ) − 2 and ℘ 2 ≜ − ( + 2) as the upper bound and lower bound of the retailer's inventory, respectively. further, we measure the range of inventory fluctuation as"
"collecting the values of weber magnitudes and orientations in iwld on an image region, we can build an iwld histogram for the region. however, the resultant iwld histogram is not rotation invariant, and is sensitive to partial occlusion and deformation. aiming to address these two problems, we propose to rebuild the iwld histogram by aggregating the iwld histograms of neighboring regions and also aligning these histograms to their dominant orientation, as follows."
"the iwld feature descriptor describes only the properties of still images and carries no motion information of a video. therefore, the detected candidate points are distinctive in appearance only, but are independent of the motions or actions in a video. clearly, motion information is essential for extracting interest points to eliminate irrelevant information for action recognition. in our previously proposed mowld algorithm [cit], we adopted the widely used optical flow approach to detect the movement within an image region. a local extreme among all iwld feature points can become only an interest point if it has sufficient motion in optical flow field."
"picking in 3d. we support picking segmented objects and synapses directly in the 3d volume view. to enable picking objects, the raycaster makes use of an additional output buffer for object ids. the ray-casting pass writes out the object id of the first object that was hit encoded in a 24-bit rgb value. to determine the object at the current mouse position, we then simply do a look-up at the corresponding location in the object id buffer. selected objects are automatically highlighted in all linked views, and can be subsequently inspected in more detail, or used as input for a dynamic query. for picking of synapses, we render the synapse ids into a selection buffer, which we then use to determine the closest, non-occluded synapse at the current mouse position. in order to limit picking to synapses that are currently at least partially visible, we have to restrict which synapses we render into the selection buffer. during ray-casting we therefore write the final depth of each pixel (where a ray leaves the volume or is terminated by early ray termination) into a depth texture. next, when rendering synapses into the selection buffer, we perform depth testing against this depth texture. this results in restricting the synapses that can be picked to those that are not completely occluded in the volume rendering."
"it means that once the inventory position is higher than the reorder point, it starts to decrease. the inventory level at the end of each period for the retailer is represented as"
"as for the forwarding of responses, it can be observed that about 90% of them are delivered in a population of 100 mobile devices, and that this percentage decreases drastically for 500 mobile devices, before increasing slightly for larger populations of devices (see figure 4d ). the explanation resides in the fact that in an ichn composed of only 100 mobile devices, the delivery of requests and responses is achieved almost exclusively by direct transmissions (i.e., 1 hop). this is why the delivery delay of responses is relatively short for 100 mobile devices, and why this delay increases when more intermediate devices are involved in the transport of a message (see figures 4a and 4f ). this delay increases more significantly when the number of access points is low, because forwarding paths between mobile devices and access points are more discontinuous than in an ichn including many access points."
patient 5 was a 63-year-old man with stable angina pectoris and a positive myocardial perfusion scan. his cardiac risk factors included dyslipidemia and type 2 diabetes. access was obtained via the right radial artery using a 5 fr xb 3.5 guide catheter. coronary angiogram showed no significant coronary lesion. the combowire ® was placed in the lad for coronary blood velocity measurements. all patients had normal left ventricular systolic function except for patient 1 who had moderate left ventricular systolic dysfunction (ejection fraction of 45%).
"we describe two specific use cases where our system has helped domain scientists and data analysts to gain new insights into their data. the first user was a neuroscientist interested in analyzing \"multiplehit\" axons within a fully segmented region of interest in the volume (section 7.2.1). the task of the second user was to numerically analyze and clean up the data from segmentation errors (section 7.2.2). general feedback from our users was that they are very happy with the key features of our system, such as simultaneous 3d visualization of em and segmentation data, and a linked graph view for neuronal connectivity. after an initial training period, they were able to use the visual query builder to quickly narrow down their current \"working set\" to the subset of neuronal structures they were interested in for a certain task. especially the ability to switch effortlessly and quickly between the different views, defining and extending powerful queries, and interactively selecting structures in the linked views, was seen as very helpful by the scientists. however, they are still at the beginning of their analysis. many synapses have not been identified and fully labeled yet, and the segmentation is still very sparse in some areas. in the future, more complete data will help them formulate and query more detailed hypotheses that they can analyze with connectomeexplorer."
"connectomeexplorer was developed in collaboration with neuroscientists working in the field of connectomics to support them in analyzing the detailed interconnections of neuronal structures in the mammalian brain at the individual nanometer scale. each neuron processes and transmits information by forming connections (i.e., synapses) to other neurons. a typical neuron consists of a cell body (soma), an axon, and multiple dendrites [cit] . roughly speaking, an axon corresponds to the output of the neuron, and its dendrites correspond to all its inputs. axons are long and narrow tubular structures that conduct electrical impulses away from the neuron's cell body when the neuron spikes, while a dendrite is a treelike extension of a neuron that receives electrical impulses from many other neurons. axons make connections with dendrites at synapses, which comprise the synaptic cleft between an axon terminal (or bouton) and a post-synaptic density of the main part of a dendrite or a dendritic spine. neuroscientists are interested in trends and correlations as well as individual neuronal structures and synapses and their detailed attributes. for example, the location of a synapse on a dendrite (e.g., whether or not it is on a spine) may be an indicator for whether the entire cell is excitatory or inhibitory, i.e., if it increases or decreases the likelihood of connected neurons to spike."
"on the other hand, minimizing the coefficient adjustment term and representation-constrained term is efficient for classification, which allows feature sharing among the classes. we will show that good classification results can be obtained using only a single unified dictionary by a simple extension to the objective function for joint dictionary and classifier construction. it encourages that the largest classification parameters of training samples from a different class over d are associated with the corresponding different subdictionary. therefore, the coefficient adjustment term enforces a label consistency constraint on the sparse codes."
"the goal of our second user was to proof-read and numerically analyze the segmentation data. connectomeexplorer can also help in data clean-up tasks performed by data analysts and technical staff, via basic capabilities for adding, deleting, and modifying objects and their properties. the data analyst in our evaluation session used our tool primarily to look at data statistics, such as average voxel size, number of connections, etc. this enabled finding several errors and inconsistencies in the data that he was not aware of before. he found segmentation errors (wrong splits/merges of objects that were visible in the 3d view), incorrectly labeled segmentations (objects labeled as spines but with a huge volume), duplicate synapses (at almost the same location but with different names), and other wrongly assigned attributes. additionally, our tool enabled identifying \"orphans\" in the densely segmented roi cyl 1, i.e., objects that only exist inside the cylinder."
"most of the existing literature has focused on game theory or optimization models [cit], while this paper explores the impact of replenishment parameters on the inventory and production dynamics [cit], which are closely related to the total costs. significant costs can be incurred by production fluctuations due to frequently switching production quantities up and down, which further complicate the activities in labor force arrangement, capacity adjustment, and equipment maintenance and management. however, in a dynamic system with uncertain demand, deriving optimal solutions from an optimization model incorporating all these cost components is impractical. furthermore, in practice, through information sharing even with order batching [cit], vmi has become one of the frequently employed programs for eliminating bullwhip effect [cit] . this implies that production can be smoothed and transportation cost can be 2 discrete dynamics in nature and society saved via the implementation of vmi program. however, we notice that the majority of existing bullwhip literature has ignored the interaction between inventory and production fluctuations."
"the messages emitted by the local applications, and those forwarded by neighbor devices, are stored by nephila in a local cache until they expire, or until they are delivered directly to their destination. when two devices discover that they are neighbors, they exchange the messages they have in their local cache, thus implementing the \"store, carry and forward\" principle. in order to perform an efficient message forwarding, each device equipped with nephila computes a list of so-called \"trail values\" (tv) and shares these values with its neighbors. a trail value computed by a device for a given destination reflects its capacity to forward a message to this destination, either directly or through intermediate devices. disseminating such pieces of information can be costly because each device can maintain trail values for a large number of devices, even in the worst case for all the devices present in the network. in order to address this issue, we have implemented in nephila a modified version of the exponential decay bloom filter (edbf 13 ), which is itself an extension of the traditional bloom filter that encodes probabilistic forwarding tables in a highly compressed manner. this modified version of edbf is called tbf (for trail bloom filter). it allows to store and disseminate efficiently the trail values of each device in the overlay system."
"the system described here tries to preserve all advantages and overcome all disadvantages of the systems mentioned above. an additional key feature was to construct an universal machine capable reproducing, as accurately as possible, any in vivo flows measurements. therefore, the research and development was focused on the control of pulsating flow rate and corresponding shear stress instead of pressure. nevertheless, the system is designed so that in the future both pressure and flow rate would be controlled simultaneously. on top of mimicking the pulsatile flow behaviour and values recorded in vivo in the cardiovascular system, a supplementary constrain was to build a test bench easy to work with, mobile, as automatic and compact as possible and easily adaptable to work with additional supporting equipment such as piv laser or ct scanner [cit] . downstream it should also automatically record values on demand such as flow rate, pressure, temperature and/or allow to capture images of the ec culture chamber while the system is running. all these requirements are met by the presented test bench."
"in a vmi system, the upstream supplier has the flexibility to determine the amount and timing of the replenishment order for the retailer. however, as mentioned, the supplier might be penalized once the retailer's inventory exceeds the predetermined range. therefore, in order to reduce penalty cost, the supplier should know how the replenishment parameters for the retailer affect the range of inventory fluctuations of the retailer. in addition, the supplier should also consider the production or ordering cost and inventory cost for the whole system when designing its own replenishment policies. in this paper, we aim to answer three questions for the implementation of vmi: (1) how the replenishment rules designed by the supplier for itself and retailer affect the stability, inventory fluctuations, and production smoothing for the entire system under uncertain demand? (2) what is the trade-off between inventory fluctuations and production fluctuations? specifically, what actions can be taken to limit inventory and order fluctuations. this question will be addressed in terms of stability. as investigated in the literature, stability is a fundamental problem for any dynamical system including supply chain systems [cit] . after being disrupted, the state of a stable system will return to its steady state gradually. actually, a stable supply chain system tends to be resilient in the presence of demand disruptions [cit] . furthermore, stability analysis simplifies the parameters selection and performance optimization. for a stable supply chain system, stability limits the fluctuation of inventory and order in a certain range after the sudden change of demand. by contrast, unstable designs will cause undesirable fluctuations for order and inventory, which leads to high cost due to inventory accumulation or stockouts. (3) how the replenishment parameters affect the magnitude of production smoothing for the manufacturer."
"neuroscience and connectomics. a very good introduction to connectomics and recent developments is given by seung [cit], also highlighting advances in high-throughput, high-resolution electronic imaging. lichtman and denk [cit] describe the challenges in reaching the ultimate goal of connectomics-understanding the relation between function and structure in the brain. [cit] present a powerful example of how em circuit reconstruction allows determining the relationship between structure and function of the visual cortex. segmentation and annotation tools for neuroscience. segmentation and tracing of neuronal structures ranges from manual [cit] or semi-automatic [cit] to fully automatic segmentation algorithms [cit] . however, only a handful of tools are actually publicly available to the neuroscience community. in eyewire [cit], users trace neurons in the retina in an online game setting. mojo [cit] offers fast proof-reading of segmented em slices. neurotrace [cit] supports semi-automatic segmentation of neurites with concurrent 3d visualization. catmaid [cit] and the viking viewer [cit] are collaborative annotation environments for skeleton extraction in terabyte data sets."
"a crucial design criterion of our query algebra was achieving simplicity from a user perspective by allowing the connectivity of objects to be used implicitly, without requiring the user to know what and how the connectivity of different kinds of objects is actually represented and stored internally. in this way, our query algebra enables expressing specific neuroscience questions in a much simpler way than is possible at the abstraction level of standard relational algebra or sql statements. in contrast to standard database queries, our query algebra allows the connectivity between all kinds of neuronal objects listed in table 1 to be exploited without having to perform an explicit natural join or a cartesian product in query languages such as sql [cit] . explicitly exploiting connectivity information usually requires joining two or more tables on their common attributes, which implies that the user must know what information is stored in which table, if the common attributes specify the desired kind of connectivity, and thus what the result of a natural join will be and what it means. instead, we allow the direct creation of sets of tuples of objects such that each tuple is only created when the corresponding objects are actually connected."
"in this paper, we introduce connectomeexplorer, which is an integrated application for the exploration and query-guided visual analysis of large em volumes in connectomics research. our system is based on a scalable volume visualization framework that scales to petascale data from high-throughput microscopy data streams [cit], which, however, did not support data analysis or querying. we introduce a knowledge-based query algebra that enables analysis via interac- fig. 2 . system overview. our system consists of two main parts: data-driven modules (left, light gray) are triggered by image acquisition, while visualization/user-driven modules (right, yellow) are active at run time. all generated data are stored in the visualization archive. at run time, the user initializes data requests either by specifying a dynamic query or by directly interacting with the visualization. a visual query builder allows users to intuitively specify queries which are translated into a powerful query algebra and evaluated. results are shown in different linked visualizations."
"objects. each object o i has a unique id, an object type (e.g., axon), and named attributes such as synaptic strength. all types and attributes are pre-defined (see table 1 ). all objects of the same type have the same attributes. the object type itself is also an attribute, which makes querying according to type identical to querying any other attribute."
"as shown in figure 3 is the shaft rotates the three intersection errors of rotary inner shaft center position effect. ignoring other errors, in the intersection between the frame and inner frame of system error impact is the biggest, the intersection of outer and inner frame and outer frame and the middle frame intersection degree influence the relative minimum; at the same time, the system error and error of angle error change this error effect small."
the paper is focused on modeling and simulation. all the data is generated by simulation experiments with different parameter settings or available from the corresponding author upon request.
"the remainder of the paper is organized as follows. section 2 gives a brief description of the nephila platform. sections 3 and 4 present the new mechanisms we have introduced in nephila in order to support communications between mobile devices and remote hosts connected to the internet, and compare our solution with existing related works. section 5 presents evaluation results. section 6 summarizes our contribution, and gives perspectives for future work."
"our system aims to facilitate the intuitive visual analysis of large-scale connectomics data based on a domain-specific query-based interface. a comparison of our system to other systems is shown in table 4 . we summarize key motivations for our main design decisions, mention alternatives that we have abandoned, and discuss limitations."
"the current implementation of nephila does not support 3g and 4g communications. cellular technologies should be integrated in the platform in the near future. we also consider implementing qos metrics regarding content delivery. nephila will thus be able to exploit simultaneously multiple communication channels, and to select automatically the interface that allows to provide end-users with the best quality of service in terms of content delivery ratio and delay. it will also permit to offload traffic from mobile operator networks transparently whenever it is possible. indeed, when neighbor users want to share content, or when the network of the mobile operator is overloaded, the system can automatically choose opportunistic communications if this type of communication is deemed to provide better quality of service."
"if the pdf of a feature is bimodal or multimodal, this feature is considered to be more discriminative than those of only a single mode. we estimate the pdf of each feature on the original 1536 moiwld features. according to the number of modes, we sort the 1536 moiwld features in descending order. finally, the first 550 features are selected to form the reduced moiwld, which is more effective than the original ones (as shown in fig. 7 )."
"segmentation data are stored as separate image data, where each voxel stores an integer object id. in order to accommodate a large number of distinct objects, which is crucial for visualizing the result of automatic em segmentation algorithms, we currently support up to 24 bits per object id. this allows representing more than 16 million distinct objects, but could be extended further. segmentation data are rendered concurrently with the em data via a multi-volume rendering approach. in addition to the volume data, we can concurrently render synapses at their exact location in volume space, as explained below."
"we empirically demonstrate the superior performance of our ideas of mowld and moiwld over existing popular feature descriptors, including hog, hof, and mosift [cit] . table ii table ii ) outperforms all the above-mentioned approaches. moiwld has further improved the results, so moiwld is more discriminative and effective."
"the visualization capabilities of connectomeexplorer comprise several linked views, from a 3d volume view to a topological 2d graph view for neuronal connectivity, a slice view that allows manual segmentation, data and table views for attribute and meta data, and analysis views for visualizing the results of statistics queries. all views are linked and depict the result of queries via the corresponding result set."
"for all the measurements, special care was taken to keep the tip of the wire in the center of the artery lumen and to obtain optimal measurements ( fig. 1 ) so that the measured velocity was maximal. the diameter of the artery at different points was measured by quantitative coronary angiography (qca) and the values are shown in table 1 ."
"at first, we compared the flow rate that constitutes the physical quantity set as parameter for controlling the system. the results obtained for five patients are presented in fig. 9, right panel and table 4 . to assay the capacity of the system, the signals were recorded during a time period of 60 s and then post-processed by decomposing the whole signal in cycles (fig. 9, right panel grey lines) . the average signal was then calculated by adding individual cycles and set as a reference signal (fig. 9, right panel black line). all further calculations were made based on this average signal. it is worth emphasis that the signal can be precisely followed with a reproducibility having variability below 7% for all investigated patients. the measured flows fitted well the limits described hereinabove. moreover, the achieved flows accurately reproduce the reference in vivo signals with a mean relative error below 5% for all investigated signals. however, the control strategy still has difficulties to adapt to rapid changes (peaks) of the flow and biggest discrepancies were noticed at these points. to visualise differences, the fourier spectral analysis were calculated (fig. 9, left-hand panel) ."
"we have presented connectomeexplorer, a novel application for the query-guided visual analysis and exploration of large volumetric neuroscience data sets. our system is based on a powerful query algebra that has already helped our collaborators in connectomics research to answer some of their concrete questions. the variety of queries in conjunction with linked visualization views has allowed them to discover facts about their data set that they did not know before. the queries we enable are made simpler-and at the same time more powerful-by encoding and building on domain-specific knowledge about the intrinsic semantics of the connectivity of neuronal structures."
"section iii introduces our new moiwld feature descriptor and the corresponding feature selection method. section iv details the proposed modified sparse framework and the supervised class-specific dl method for classification. in section v, the experimental results and analysis are presented. finally, the conclusions are given in section vi."
"from above, we see that the production process of the manufacturer can be smoothed even with low delivery frequency determined by the manufacturer to control retailer's inventory. in this sense, the manufacturer can reduce transportation cost and production cost simultaneously. however, the manufacturer must be cautious to determine the replenishment frequency or the size of order batching, because there still exists a trade-off between inventory cost and production cost caused by production fluctuations. this trade-off will be further explored via simulation experiments in the next section."
"the \"best trail selection algorithm\" (btsa) implemented in nephila creates multiple copies of messages, and replicates these copies on intermediate nodes that are considered as good relays to deliver these messages to their destination. some of these copies of messages can be disseminated in the network, while one of them has already been delivered to the recipient, because the deadline assigned to the message has not expired yet. to reduce the number of copies forwarded in the ichn, we have implemented in nephila a simple healing mechanism. this mechanism indicates to intermediate nodes that a message has been delivered, and therefore that they can remove any copy of this message from their local cache and thus stop forwarding it. this healing mechanism relies on the periodic broadcast of healing tables containing the ids of the messages and their deadline. a mobile node also sends its healing"
"in the future, we would like to integrate more advanced segmentation and proof-reading methods [cit] into our system, and research intuitive 3d navigation metaphors to navigate and explore petascale volume data sets efficiently. furthermore, we would like to extend our system toward the comparative visualization of query results."
womersley's solution obtained by the analytical integration of eq. 2 is based on fourier series in time and bessel's function in space. the final equations used to calculate blood properties in a matlab script are therefore as presented below.
"we perform empty space skipping depending on both the em volume and the segmentation volume to speed up rendering and to reduce the amount of gpu memory the ray-caster needs. for em data, we cull all visible blocks against the current transfer function to determine if a block is fully transparent or not. for segmentation data, we cull against the object ids that are currently enabled or contained in the current result set. if all objects in a block are disabled or not in the result set, the block is classified as invisible. if a block is classified as invisible, its page table entry is set to empty, no data is downloaded to the gpu, and the block is skipped during ray-casting."
"the main goal of connectomeexplorer is to facilitate the interactive visual exploration and analysis of large volumetric connectomics data sets. a high-level overview of the interaction for analysis is depicted in fig. 3 . our visualization system is based on a flexible rendering framework that is scalable to petascale data and that supports multiple volumes and linked views. however, the main feature of connectomeexplorer is that it enables the user to formulate and answer domain-specific questions, either by interactively exploring the data or by posing dynamic queries in an intuitive user interface that translates queries into a query algebra. we support three main types of queries, and their combination: spatial queries based on regions of interest or distances, topological queries based on neuronal connectivity, and attribute queries based on either automatically computed attributes (e.g., the volume of an object), or manually labeled attributes (e.g., the number of vesicles in a synapse). the results of queries are always represented as sets (see sec. 4), which can be examined in all views, used as input for more advanced queries, or stored to and loaded from disk. statistical parameters can be computed directly on the sets of query results, and can be inspected both visually in histogram and scatterplot views, as well as textually/numerically in table views."
"the models developed can be grouped into four different categories, depending on whether the model is for a single station or a multi-station process, or if the model considers rigid or compliant parts. station level models treat the assembly process as if it is conducted in one step. in contrast, station models analyze the process recursively as the assembly is moved from one station to the next. rigid part models do not consider part deformation during assembly so that the part and tooling variation can be solely represented by kinematics relationships. compliant part models consider the possible deformation of the parts during the assembly process. the models include a force analysis that take into consideration the stiffness of each part and the forces applied by each tool. dimensional variation modeling and analysis for multi-station manufacturing processes has been developed mainly for rigid parts. multi-station assembly processes with rigid parts cover a large number of currently used processes such as power-train assembly and general assembly in automotive industry. however, a large group of multi-station assembly processes consider non-rigid parts. for example, most of all assembly stations in automotive body structure manufacturing assemble no rigid parts. variation propagation analysis for a multi station assembly process introduces new modeling challenges. in comparison to the station level approach, it is necessary to define an appropriate variation representation in order to track the variation propagation from station to station. the variation simulation process is sequential, i.e., to estimate the variation at station i, it is necessary to know the variation at station i21. moreover, there is a station-to-station interaction introduced by the release of holding fixtures and the use of new fixtures in subsequent stations. finally, compliant assembly variation analysis requires applying finite element methods to calculate the deformation after assembly. therefore, the number of calculations increases with the number of stations. recent publications in each of these areas are summarized in table 1 . as can be seen, most of the dimensional variation analysis has been conducted for single station or multistation rigid part assembly and some work exists at station level for compliant assembly."
in order to check the validity of the first assumption-the laminar behavior of the fluid-the patient's maximum mean velocity was taken into account in order to find the corresponding reynolds number using as the reference value the pipe diameter and the mean velocity. a maximum re number of 442 for patient 4 was found and it validated the assumption that the flow has a laminar behavior for the analyzed vessels.
"rendering synapses in 3d. synapses are rendered as small shaded spheres at their 3d volume position, and combined with the volume rendering using correct visibility compositing. for the latter, we adjust the ray-casting setup step such that rays terminate accordingly when they intersect a synapse. to render a shaded sphere without explicit geometry, we render a view-aligned quad for each synapse sphere, and use a fragment shader to perform shading for those positions that intersect a procedurally computed sphere, and discard the pixels outside."
"the results obtained for all examined patients with the real-time system are satisfactory. however, the system experienced difficulties to follow rapid flow changes. therefore, when fast wss and wsstd acceleration/deceleration appeared, a large error was generated. this results from the foundations behind the wss calculation method. it indeed involves the use of derivatives which amplifies the noise particularly at higher frequencies. as the system struggles to reproduce fast changes, the calculated wss values between the reference flow and measured flow may therefore display significant discrepancies, reflected in the calculated relative error. the same principle may be applied to wsstd, whose values are function of a second derivative of the flow. therefore the relative error becomes even higher."
"1) in order to detect a sufficient number of interest points containing the necessary information to recognize a violent activity, we follow our recent proposal of mowld [cit] and present an improved version of mowld (i.e., moiwld) to extract the low-level image and motion features of a query video more effectively. the proposed moiwld detects spatially distinctive interest points having substantial motions. in a sense, this descriptor takes the advantages of both sift in terms of computing the histogram using the magnitude and orientation of a gradient and local binary pattern (lbp) [cit] in terms of computational efficiency. 2) we propose a modified sparse model that minimizes both the reconstruction error of coding coefficients and the classification error. based on this model, a class-specific dictionary, i.e., the dictionary atoms that correspond to the class labels, is learned using the class labels of training samples. with this learned dictionary, not only the representation residual but also the representation coefficients become discriminative. a classification scheme integrating the modified sparse model is then developed to exploit such discriminative information. the experimental results on three challenging benchmark data sets demonstrate the superior performance of our proposed approach over the state of the arts."
"specifically, we attempt to explore the inventory and production fluctuations in the context of a vmi supply chain system composed of a manufacturer and a retailer. the manufacturer uses a reorder point policy to manage the retailer's inventory and the apiobpcs (automatic pipeline, inventory, and order based production control system) for production control [cit] . the contributions of this paper lie in threefold. firstly, we analytically studied the impact of reorder point, delivery frequency, lead time, and demand characteristics on the range of the retailer's inventory under uncertain demand by extending a more generalized replenishment policy. secondly, we derived the necessary and sufficient stability conditions of the vmi system for arbitrary lead times by considering the interaction between the retailer subsystem and the manufacturer subsystem, which are significant due to the development of global supply chains and complicated manufacturing process [cit] . we prove that production fluctuations are independent of the replenishment parameters in the reorder point policy. besides, we demonstrate that although the production of the upstream manufacturer can be perfectly smoothed without paying extra attention to the quantity and frequency of replenishment for the retailer, the parameters in the reorder point policy have significant influences on the inventory fluctuations for both retailer and manufacturer. finally, to further quantify the production smoothing magnitude, the impact of all the decision parameters on the robust bullwhip metric is further studied based on a transfer function model. the results of this research provide insightful guidelines on the implementation of vmi program."
"the replenishment rule used by the manufacturer for production decision is called apiobpcs [cit] . in traditional supply chains, the replenishment decisions for upstream members are made by themselves, whereas the replenishment decision in a vmi model should exploit systematic information. for this purpose, we define system inventory as"
"). the values for all patients are presented in table 2 . based on the assumptions described above, the flow momentum equation for direction z (flow direction) in a cylindrical system becomes:"
"in the simulation experiments, we will firstly validate the theoretical results on the range of inventory fluctuations of the retailer subsystem. then, we will study the dynamics interaction between the retailer and the manufacturer. finally, the impact of different parameters on the production smoothing will be numerically studied with the worst-case bullwhip measure."
"based on these suggestions this manuscript focus on the signals related to real flow conditions as well as in the design of an in vitro device capable of reproducing hemodynamic patterns of atherogenic conditions in vitro. to build this synthetic device, one needs to be able to mimic, in vitro, the exact signals encountered in the blood vessels. this ability would enable signals to be generated and applied to ecs or multisheet culture systems containing ecs and smcs (smooth muscle cells) seeded in a transparent tube that would also allow observation under the microscope, recording of flow parameters and cell harvest after treatments. in order to development such device, the in vivo signal, such as blood velocity, have to be recorded and afterwards, post-processed to enable extraction of derived physical quantities, such as flow rate, wss and wsstd. with this goal (building an in vitro system that mimics real pulsatile blood patterns) in mind, the first objective was to accurately record real time dependent hemodynamic data. we performed in vivo measurements using a doppler flow combowire ® (volcano corp.)"
"in fact, the precise stable regions gradually converge to the stable region independent of production lead times as the production lead time increases. figure 2 shows the stable regions in the − plane for different production lead times, in which − represents precise stable regions tailored for specific lead times with theorem 2, and − represents the stable region independent of lead times. it is obvious that the area of − will cover most of the areas of the precise stable region − as the production lead time increases. it implies that the − simplifies the selection of replenishment parameters."
"in a vmi system, a vendor makes replenishment decisions based on information sharing with respect to downstream member's demand and inventory. in our periodic inventory model, the balanced equations for the retailer's inventory level,"
"various methods can be used to evaluate the statistical characteristics of the variation phenomena. monte carlo simulation and taylor series approximation, among others, are probably the most commonly used ones. for datum scheme induced variations where normal distribution well applies and the system is very linear, both methods should generate comparable outputs. however, taylor series approximation has obvious advantages here because it is in explicit close-form format, easy to use and generate instant results with handy computational tools. therefore, taylor series approximation is recommended."
"to enable the demand-driven determination of which segmented objects are inside a roi, we use a dynamically created octree data structure that stores the list of contained object ids for each octree node. the octree is only grown and updated on demand. we then traverse the octree to find the biggest nodes that are still completely inside the region of interest, and return the corresponding object ids. to optionally speed up the computation, two different accuracy levels can be specified. the optimized computation only evaluates nodes that are completely inside a roi, whereas the accurate computation also evaluates all nodes that intersect the roi. only the worst case of intersecting leaf nodes requires iterating over voxels to determine the object ids inside the roi. whenever a volume block is constructed on the server and sent to the client, the histogram of that block is also sent to the client and added to the octree. if during roi evaluation the histogram of a block is not available yet, it can be requested from the server without having to request the volume data of the entire block."
"compares it with the mean position previously set. if there is any drift, the pi controller number ii modifies the reference signal of the flow. the purpose of the system is to be as close as possible to reality. in the real human body, the condition that keeps the cells alive is temperature, which must be kept between 36 and 38°c. from a fluid-mechanics point of view the constant temperature helps to maintain certain conditions of the liquid such as viscosity. viscosity is an important variable in the investigated system as it plays a role on the response of the system and is used to calculate the shear stress that acts on the vascular cells. therefore, the additional heating system had to be incorporated in order to control the temperature of the medium. the heat is controlled by a ptc heater with a fan (fig. 4, marked as 15 and 16 ). the control strategy is based on the temperature measured by the liquid thermocouple and the air heater with the fan. basically, the thermocouple measures the apparent temperature of the liquid and compares it with the desired one. if there is a difference, the control loop sends a signal to the relay (switch) that opens or closes the electrical circuit and directly powers the heater and air inside the box is heated."
"violence detection involves similar techniques to those used in many related computer vision applications, e.g., action recognition, object detection and surveillance [cit] . compared with action recognition, relatively little research has been found for detecting action or violent contents. timely detection of violent outbreaks in crowds may mean the difference between life and death. for this practical consideration, in our work, we focus on the challenging work of detecting violence in surveillance videos and aim to develop a system to effectively detect violent behaviors using computer vision techniques."
"all queries are evaluated dynamically at run time. because of the involved data sizes, we do not employ pre-computations. an exception is the computation of min/max values and object id histograms for each processed 2d mipmap tile, which is done during mipmap generation. typical query evaluation times are given in table 3 ."
"the result of every query is an (unordered) set s. each set can contain an arbitrary mixture of (1) individual objects o i, (2) ordered tuples t i of objects, and also (3) individual sets s i, i.e., we allow sets of sets."
"in vivo pulsatile blood velocity data were successfully obtained for five patients using a doppler flow combowire ® (volcano corp.), although there were some difficulties to obtain the doppler flow signal. for example, placing a wire in a suitable position, i.e., as centrally as possible in order to get sufficient signal intensity took several minutes, an inconvenience probably related to the difficulty in choosing the best coronary artery/ place to perform an accurate measurement i.e. to measure maximum velocity avoiding any artifact and noises. patients were selected from those needing coronagraphy according to the ethical board's policy, and we were limited to the use of sites where coronagraphy had to be performed. within these restrictions, our goal was to find healthy vessels without stenosis and as straight as possible in order to obtain proper measurements. nevertheless, some measurements had to be performed close to branches or curves. these practical problems add to the inherent variability between successive human heart beats, perturbed signals due to movements of the patient, etc. hence, the data obtained were often noisy and additional filtering was needed. after filtering, the five different in vivo signal measurements were analyzed and postprocessed. results showed that the flow pattern varied widely among patients and confirmed the reported variability of pulsatile flow waveforms observed in the arteries (see, e.g., [cit], mcdonald [cit] ). the differences among patients in the magnitude of the velocities as well as in the shape of the curve could be due to intrinsic differences between patients and/or measurement positions."
"visualization of connectivity. our initial approach for visualizing neuronal connectivity was integrating 2d graph interaction directly in the 3d volume view. after initial evaluation with the domain scientists, we decided for a separate graph view. only after understanding the abstract connectivity of the data would they now like to include more details by again adding spatial information to the connectivity."
"since the multiscale moiwld is based on iwld and optical flow, it has the advantages described as follows. instead of combining the histograms obtained from iwld and optical flows, we build a single feature descriptor through the early fusion that concatenates both histograms into one vector. this single future descriptor captures, at the same time, the appearance and motion information that are essential for classifying actions. moreover, the moiwld captures local appearance using an aggregated hog in neighboring regions, so it is tolerant to partial occlusion and deformation. furthermore, when an interest point is detected, a dominant orientation is calculated and all gradients in the neighborhood are rotated according to the dominant orientation. therefore, the multiscale moiwld is rotation invariant."
"healing max number of elements 1000 the simulation environment is intended to be as realistic as possible. it involves a variable number of users of mobile devices that move freely in the streets of the french city of vannes, which is a mediumsize city of about 50 000 inhabitants and whose area is about 25 km 2 . a variable number of access points are assumed to be distributed in the city. these access points are connected to the internet. for the sake of realism, we have divided this set of access points in four distinct groups. these groups are managed by different mobile operators. mobile devices carried by people and running nephila can communicate directly together (in ad hoc mode) when they are in the communication range of each other, using either their bluetooth or their wi-fi interface. they also can forward the messages they received opportunistically towards their destination, thanks to the nephila middleware. we assume that mobile devices are configured to discover wi-fi access points, and to automatically connect to those managed by the operator of their respective owner. the mobile devices that are connected to access points can act as gateways between the mobile ad hoc parts and the infrastructure part of the network, thanks to their different wireless interfaces. in our simulation, only 100 pedestrians emit messages (requests) towards fixed host located on the internet. the responses generated by these hosts are forwarded back to the emitters of the requests by nephila."
"our data managment approach comprises two main parts: a datadriven part, which is triggered by actual data acquisition and generation, and a visualization-driven part, which is active at run time. we generate tiled 2d mipmaps of the acquired raw image data, which subsequently serve as the source for all data requests from the visualization and analysis modules. for each mipmap tile, we additionally compute min/max values to facilitate hierarchical culling. segmentation data are processed in a similar way, but in order to avoid interpolating object ids during down-sampling, we use nearest-neighbor filtering for the computation of segmentation image mipmaps. more elaborate downsampling algorithms (e.g., a rank filter) could also be used. for each tile of the segmentation data, we additionally compute a histogram of all contained object ids, which facilitates hierarchical culling during the dynamic evaluation of spatial queries."
"this data set is more challenging than the other two data sets because it contains many crowded scenes. the set contains 246 clips divided into five splits, each containing 123 violent and 123 nonviolent scenes. table v presents the results obtained using different methods mentioned above on this data set. the dictionary size is again fixed to 1800 in this set of experiments. in this data set, due to more crowded scenes, the detection rate of rvd method decreases. on the contrary, the performance of amdn method is still very stable. however, because of the introduction of optical flow noise, amdn's performance is not very good. our proposed sparse classification-based approach still outperforms other approaches. moiwld is still significantly superior in performance to hog, hof, hnf, rvd, amdn, and mowld. it confirms that our proposed moiwld is a more effective descriptor for describing action feature. consistent with the results on the previous two data sets, our moiwld combined with the proposed sparse classification method outperforms the src methods. it indicates that the proposed classification model has a smaller classification error rate compared with the original src. the results on this data set demonstrate that our algorithm is also effective for detecting violence in a crowded scene. some false alarms (some examples are shown in fig. 10 ) are caused by people waving flags, vigorously clapping hands, or sharply and disorderly waving hands. by verifying the results, we can conclude that our proposed system is effective and robust for detecting violence with complex scenarios, such as various distances from cameras and severe occlusions between people and crowed scenes."
"weber's law indicates a fact that, for a stimulus, the ratio between the smallest perceptual change and the background is a constant, and it implies that stimuli are not perceived in absolute terms but in relative terms. in another word, only if the ratio of the change of a stimulus to the original stimulus is big enough, this change can be recognized. most of us can easily catch a whispered voice in a quiet room, but in a noisy condition, we may not notice someone shouting in our ear. this is the essence of weber's law."
"the position of a rigid part or component, when installed in a system, will generally appear off its design position, or the nominal position. the primary cause for this type of dimensional variation is due to the variations in the locating datum scheme. since the part or component under discussion is considered rigid, its dimensional deviation from its nominal position is treated as a rigid body motion process. consider a rigid part that originally sits in its nominal position. its position and orientation must deviate to accommodate the variations at the datum points that are used to locate the part. as the results of the dimensional deviation, the whole part slides a linear distance and rotates an angle. in cartesian coordinate system, the linear distance is represented by three translational components,"
"comparing the iwld with the original wld, we can see that the iwld represents local patterns more effectively and accurately in the patch. as shown in fig. 3, when a point falls in a nonflat area, (6) gives a nonzero response."
"set operators. we support the most common operators that operate directly on sets. for convenience, we allow their use in a unary, binary, or n-ary fashion where possible. we support the set operators union (all elements in at least one input set), intersect (all elements in all input sets), the relative complement relcomp of two sets (all elements in one set, but not the other; i.e., the set difference of two sets), the absolute complement abscomp (all elements in none of the input sets), and the symmetric difference diff (all elements in exactly one set but not in several). the absolute complement depends on implicit determination of what the enclosing set (universe) is, e.g., the complement of a set of axons with respect to the set of all axons."
"mobile phones and tablets have undoubtedly become the preferred -and sometimes the exclusive-communication means for billions of people. with the generalization of 4g networks, people are prone to share an ever growing volume of multimedia contents (e.g., photos, videos), usually using centralized social-based online platforms. in some circumstances, such as during peak times in crowded metropolitan environments, people may experience long latencies, low throughput, and even network outages due to congestion induced notably by the simultaneous production and consumption of large amount of multimedia content. to cope with these problems, mobile operators usually resort to mesh networking techniques and deploy numerous base stations providing small communication cells (i.e., wi-fi access points, femtocell devices). mesh networking projects, such as roofnet 1, serval 2 and opengarden, have shown that it is possible to provide nomadic users in cities with broadband multi-hop connectivity to the internet, thanks to a set of wi-fi access points acting as mesh routers and forming a backbone infrastructure with a limited number of wired links to the internet. however, several issues must be addressed to relieve the mobile operator networks using such mesh networks. indeed, the mobility of people, the short communication range of wireless interfaces, radio interferences and the volatility of the devices -which are frequently switched off in order to reduce their power consumption-can yield frequent and unpredictable disruptions in communication links, thus entailing the creation of connectivity islands in the ad hoc segments of the network. maintaining end-to-end connectivity between all network nodes then becomes quite a challenge, as solutions relying on the ieee 802.11s standard or on dynamic routing protocols designed for manets (mobile ad hoc networks), such as roofnet and opengarden, prove unable to ensure message forwarding in such conditions. they indeed assume that the network is dense enough, so it can be viewed as a fully connected graph over which end-to-end paths can always be established. such an assumption proves unrealistic in many real situations."
"as mentioned in lalwani [cit], state space model (13) lays a solid foundation for studying the controllability and observability problem, and it might open a door for studying nonlinear dynamics and incorporating time-varying lead time into the apiobpcs model, which makes the model more realistic and the results appealing. in contrast to the existing literature, the transfer function (17) in this study is formulated in a matrix form, which facilitates the implementation in computers. more importantly, the derivation process is simpler than obtaining the transfer function by combining block graphs with mason's gain formula [cit], which enables us study the production smoothing behavior in terms of worst-case bullwhip for unknown demand."
"tao zhang, wenjing jia, xiangjian he, senior member, ieee, and jie yang, member, ieee abstract-automatic violence detection from video is a hot topic for many video surveillance applications. however, there has been little success in developing an algorithm that can detect violence in surveillance videos with high performance. in this paper, following our recently proposed idea of motion weber local descriptor (wld), we make two major improvements and propose a more effective and efficient algorithm for detecting violence from motion images. first, we propose an improved wld (iwld) to better depict low-level image appearance information, and then extend the spatial descriptor iwld by adding a temporal component to capture local motion information and hence form the motion iwld. second, we propose a modified sparse-representation-based classification model to both control the reconstruction error of coding coefficients and minimize the classification error. based on the proposed sparse model, a class-specific dictionary containing dictionary atoms corresponding to the class labels is learned using class labels of training samples. with this learned dictionary, not only the representation residual but also the representation coefficients become discriminative. a classification scheme integrating the modified sparse model is developed to exploit such discriminative information. the experimental results on three benchmark data sets have demonstrated the superior performance of the proposed approach over the state of the arts."
"note that the conventional src scheme does not consider the relationship between the subdictionaries of two classes. it is used to minimize reconstruction error instead of classification error. in addition, the discriminative information in training samples cannot be sufficiently exploited by such a naively supervised dl method."
"model control and supported pid controllers in order to allow a feedback to all measured changes of the environment such as temperature, viscosity, flow or pressure."
"in order to demonstrate the performance of the proposed sparse classification model proposed in section iv, we first compare our method with the original src classification algorithm on the three databases. the average results shown in fig. 8 demonstrate that our proposed model achieves a higher classification rate than the original src algorithm. also, the comparison result on their average computation times for classifying images is shown in table i . as our algorithm learns a single overcomplete dictionary and an optimal linear classifier jointly, compared with the src model, on average, it is around 25 times more efficient."
this short analysis shows that it is difficult to agree on a constant value and that the patterns of variability in different patients are considerable. nevertheless our study in 5 patients provides a limited set of preliminary data to plan in vitro experiments and create a database of patient flow patterns.
"segmentation tree view. this view allows flexible hierarchical structuring of the list of segmented objects according to arbitrary userdetermined semantics, including which user performed the segmentation. our collaborating scientists specifically want to be able to subdivide their data into custom categories for visualization in the gui (see fig. 6a ), which can be independent of the anatomical parent/child relationship of segmented structures. furthermore, this view also allows quickly enabling/disabling entire sub-trees of objects, assigning different colors to individual elements as well as to entire sub-trees. the latter colors are used when a sub-tree is collapsed to a single node. table views . our system supports displaying meta data of all existing objects. fig. 6b shows an example of a list of synapses and their properties. each table supports filtering and sorting based on attributes, is fully linked to all other views, and can either show the result of the current query, or can be used for free manual exploration."
". even more, the shear rate can also drop under zero value and achieve negative values. these patients display the most variable viscosities and the largest differences between the investigated methods (again in patients 1 and 2). the flow rate is the physical property that set the conditions for system control. therefore we calculated flow rates for all patients as shown in fig. 7 and 1st column. although the shape of the pulsatile flow rate is similar, the flow rate predicted with cfd is higher than the one obtained with the analytical womersley's solution. nevertheless, a similar shape of flow pattern was deduced from calculations by both methods: for all patients, a similar flow rate peak for the systolic part and a slowly decreasing flow rate during the diastolic part were observed. we can notice that the flow pattern as well as the regularities or irregularities in the flow varies strongly form one patient to the other. the higher flow was observed for patient 1 and the lowest for patient 3 (fig. 7, 1st column, graphs 1 and 3) . besides, the second patient presents a very irregular flow rate (fig. 7. 1st column, graph 2) . that strong physiological variability is a key and challenging issue for the control system of an in vitro system that should mimic accurately the in vivo behaviour. finally we also calculated physiological properties that we would like investigated the most i.e. wall shear stress (wss) and wall shear stress time derivative (wsstd), as shown in fig. 8, 2nd and 3rd columns. the differences between the two calculation methods are not significant. the curve obtained using analytical solution follows the shape of cfd calculations for all patients and achieves almost the same maximum and minimum values for wss as well as for wsstd. as can be noticed, for patient 1 and 2, the velocity profile presents some reverse flow near the wall during a part of the pulsatile cycle (fig. 6, 1st column) . it is well presented for patients 1 and 2 for time step of 0 s. this is also marked by the fact that the wss becomes negative (changes direction) during these phases. the wss is the highest for patient 4 and the lowest for patient 1. all wsstd also presents a peak at the beginning and end of the cycle. between these two peaks oscillations can vary from patient to patient there are more oscillations for patients 2-4 comparing to patients 1 and 5. here again, womersley's solution slightly underestimated values as compared to cfd values. the lowest and highest values can be found for patient 5. table 2 summarizes minimum, mean and maximum values for all calculated physical quantities."
"analysis views. for statistical analysis and comparison tasks, we provide standard views such as histograms and scatterplots, which support dynamic query-based visual analysis (see fig. 6c,d )."
"we report on several domain-specific questions and analysis tasks for which our domain experts have used connectomeexplorer. table 3 summarizes the following scenarios and results, and lists the amount of time our collaborators spent on each individual sub-task."
"patient 3 was a 63-year-old man with stable angina pectoris and a positive myocardial perfusion scan. his past medical history included insertion of a des in the midleft anterior descending artery, hypertension and type 2 diabetes. access was obtained via the right radial artery using a 5 fr xb 3.5 guide catheter. the coronary angiogram showed no significant lesion on the coronary tree. the combowire ® was placed in the intermediate artery for coronary blood velocity measurements. patient 4 was a 46-year-old man with dyslipidemia and previous implantation of bmss in the proximal lad and mid-left circumflex artery (lcx), who was undergoing control angiogram for recurrent angina. access was obtained via the right radial artery using a 6 fr xb 3.5 guide catheter. after successful pci of a lcx intra-stent occlusion, the combo wire was advanced in the intermediate artery for coronary blood velocity measurements."
"the general outline of the test bench is presented in fig. 4 . the circuit starts in an incubator (2, revco ultima ii rmi3000 thermo scientific) where a reservoir (1) filled with a medium dmem (dulbecco's modified eagle's medium) with 1 g glucose, 25 mm hepes and supplemented with: 10% fbs (foetal bovine serum), 1% l-glutamine, 1% penicillin + streptomycin, 2% hat (hypoxanthine aminoptein thymidine) and 1% neaa (non-essential amino acid). in order to set the desired viscosity level, a dextran complex is used. the reservoir is placed in the incubator in order to maintain physiological"
"we have implemented a graph view to visualize the connectivity between different structures (see fig. 6b,c) . we extract the connectivity information from meta data provided by our collaborators, which is based on tables of annotated structures. in the graph view, nodes correspond to axons or dendrites, and the synapses between them are displayed as links between nodes. the graph layout is computed via a force-directed graph drawing algorithm [cit] based on the connectivity of the data. this automatically places nodes with a large number of connections toward the center of the graph, and less-connected nodes toward the edges. the connectivity graph view is fully linked to all other views. it also supports picking, selection, and mouse hovering, to provide user input for queries or to examine individual elements."
"knowledge-based queries. a knowledge-based query algebra geared toward a specific domain has the inherent limitation that it is not as general as a general-purpose query language such as sql or mathematical analysis tools like matlab. we use this limitation as an advantage that allows us to greatly simplify the user interaction. however, specifying complex dynamic queries still is not trivial and needs an initial training period to get acquainted with the basic concept."
"in recent studies, some approaches based on spatiotemporal interest points (stips) [cit] have been proposed for violence detection. in general, after extracting interest points over the frames, the bag-of-words (bow) approach is used for fig. 1 . sample frames from the hockey fight data set (first row), the behave data set (second row), and the crowd violence data set (third row). in each row, the left three columns are violent scenes, while the right three columns are nonviolent scenes."
"in this section, we give an overview of the basic components of our system, from data acquisition and management to the integration of visual analysis and exploration tools. a high-level overview is depicted in fig. 2 . table 1 gives a comprehensive list of object types and pre-defined sets of objects that we support as basic inputs to queries."
"the process error sources consist of machine errors caused by machine geometric accuracy, thermal error, etc.; setup error due to the fixture and datum errors; and finally, force-induced error caused in part by deformation during cutting. the variation propagation is caused by the datum error. standards for geometric dimensioning and tolerance were developed in order to regulate the deviation of the work piece features. these standards provide a description of the dimensional and geometric accuracy of a work piece. another way of classification is to classify the errors into two categories: systematic and random errors. systematic errors are constant and repeatable and can be viewed as a constant offset, whereas random errors arise from random fluctuations in the system. this classification overlaps the previous one. for example, the fixture error can be both systematic error and random error. in the proposed model, we can consider both systematic and random fixture error. given the complexity of machining processes, it is very difficult, if not impossible, to develop comprehensive analytical models of the relationship between all error sources and all quality measurements."
"the duration of the simulations is 1 hour and 43 minutes. during the first 3 minutes, both fixed and mobile devices perform a discovery of their neighbors, and compute and disseminate trail bloom filters (tbfs). after this warm-up period, mobile devices start a client/server application that sends messages during 1 hour. the application, and thus the emission of messages, is stopped 40 minutes before the end of the simulation in order to be able to evaluate how many requests and responses are really received by their final recipients, and how many messages are dropped. in our simulations, we assume that pedestrians move at a speed that varies between 1 m/s and 1.6 m/s. we performed simulations with 1000, 2000, 3000 and 4000 pedestrians. the environment is additionally populated by either 100, 200, 400 and 800 fixed access points. the lifetime of messages is set to 20 minutes. the communication range of the wireless interfaces is limited to 50 meters, and their bitrate to 10 mbit/s. the bitrate of wired links is limited to 20 mbit/s. the nephila's parameters used for the simulations are defined in figure 3 and table 1 . they were determined empirically through extensive experiments. figure 4 presents the results we obtained for the delivery of messages (requests and responses) in terms of delivery ratio and delay. requests are forwarded using the anycast forwarding mechanism we have implemented in nephila, while the responses are sent back to the emitters of requests using a unicast forwarding mechanism. when the number of access points and pedestrians increase, both the delay and the ratio of delivered requests increase. a peak of 98% of delivered requests is reached for 800 access points and 4000 pedestrians. these results are consistent with what can be expected in such circumstances. indeed, the probability of both encountering an access point and finding a forwarding path is higher in a dense network than in a sparse one. this is confirmed by results shown in figures 4a and 4b . when the number of mobile devices is low (100), most of the requests are delivered directly, and this proportion gets higher as the number of access points increases. when the number of mobile devices and access points increases, we observe that most requests are delivered with a number of hops lower or equal to 4. for example, for 4000 pedestrians and 800 access points, 80% of the requests are delivered in at most 4 hops. moreover, for 800 access points, the delivery time of most requests ranges between 0 and 200 seconds (see figure 4b ). the number of requests delivered in a short time increases significantly with the number of mobile devices. figure 4c provides a comparison between the anycast forwarding mechanism implemented in nephila and a traditional unicast forwarding mechanism considering the delivery ratio of requests for 800 access points. the results confirm the efficiency of our anycast forwarding mechanism, since 98% of requests are delivered in similar conditions with this mechanism, while only 20% are delivered with the unicast forwarding mechanism. therefore, the forwarding mechanisms implemented in nephila can be considered as efficients as they allow to forward messages towards both access points and mobile devices with a small number of hops, a high delivery ratio and a short delay."
"based on the eq. (18), the length of the pipe for all examined here diameters was set at 0.05 m and the measurement point for the comparison of all data was located at 0.025 m. the 2d axial symmetry model requires four boundary conditions:"
"although none of the assumptions quoted above are 100% true for blood flows, most of them are a fair approximation of reality. moreover, to assess the effect of the nonnewtonian character of blood, a cfd computation was performed on comsol ® multiphysics software. these cfd calculations are based on the unsteady navier-stokes equations that for incompressible and non-newtonian fluid lead to:"
"our volume renderer is an extension of previous work [cit] that targets scalability to very large electron microscopy volumes. in addition to concurrently rendering segmentation volumes [cit], we have extended the 3d volume view to support picking and highlighting objects in 3d, linking to other views, and the automatic visualization of all objects that are in the result set of a query. we employ a data construction back-end that constructs small 3d sub-blocks for a requested position and resolution from 2d image data on demand (see sec. 3.2). the construction of 3d sub-blocks of data is driven by the visibility of small 3d blocks requested by the ray-caster. only the blocks that are currently visible are requested and downloaded to gpu memory. 6 . example analysis session. a) the user starts with a manual inspection of the segmentation, before looking at a single dendrite (in red) and its connected axons. b) looking at all connections that the red dendrite makes, and querying the axon it makes most synapses with (blue axon). c) looking at all connections of the blue axon and their strengths, grouped by dendrite, viewed in a histogram. d) comparing all synapses of the red dendrite based on specific properties: the size of dendritic spines vs. the vesicle count of axons/boutons were analyzed using a scatter plot."
"be an arbitrary point of the part. it moves to ) ', ', ' ( ' z y x p due to the overall rigid part deviation. firstly, let's look at the movement of the point on the xoy plan purely due to a rotation about the z-axis, i.e.,"
"the first questions asked by our domain experts were based directly on attributes of axons, dendrites, and synapses. pre-synaptic axon terminals (boutons) contain neurotransmitters that are essential for transmitting signals over synapses. these neurotransmitters are stored in small membranes called vesicles. the number of vesicles has been labeled in our data, because our collaborators are interested in examining the distribution and number of vesicles in different axons and whether there is a correlation between the number of vesicles in a synapse and other attributes of the axon, dendrite, or synapse in question. the specific questions they asked, and the corresponding queries, were:"
"patient 1 was a 58-year-old male with stable angina admitted for elective percutaneous intervention (pci) of a long and severe stenosis involving the third marginal branch. his past medical history included previous anterior myocardial infarction treated with a drug eluting stent (des) on the left anterior descending artery (lad), diabetes, hypertension, active smoking and dyslipidemia. access was obtained via the right radial artery using a 6 fr xb 3.5 guide catheter. angiographic images from the patient are shown in fig. 1 . after completion of the third marginal angioplasty, a combowire ® was advanced into the proximal part of the lad for coronary artery blood velocity measurements. patient 2 was a 44-year-old female undergoing coronary angiography for unstable angina. her cardiac risk factors included dyslipidemia and active smoking. access was obtained via the right femoral artery using a 6 fr xb 3.5 guide catheter. after implantation of a bare metal stent (bms) in the proximal lad, a combowire ® was advanced into the proximal part of the lad."
"the grouping operator (group) converts an input set into a set of sets (one for each group) according to either a common tuple dimension, or a common value-or value range-on a specified attribute."
"reconstructing the anatomical and functional connectivity within the brain has become one of the most active research areas in neuroscience. by ultimately mapping and deciphering a human's entire connectome [cit], i.e., the full \"wiring diagram\" of the brain comprising billions of neurons and their interconnections, scientists hope to gain an understanding of how the brain develops and functions, and how pathologies develop or can be treated. to support this goal, highthroughput methods for neural imaging have been developed that en-"
"in the literature, the production smoothing behavior is usually characterized by bullwhip effect metrics [cit] . to facilitate the study of production smoothing in terms of bullwhip effect, we develop a transfer function model by treating the demand of the retailer as input and the production of the manufacturer as output. unlike existing methods in the literature, in which transfer functions are derived by combining the components of block graphs [cit], we directly obtain the transfer function by formulating a state space model and manipulating the difference equations using z-transform [cit] . this change avoids the complicated computation process of the well-known mason's gain formula [cit] ."
"the remaining of this paper is organized as follows. related works on action recognition, anomaly detection, and dictionary learning (dl) are discussed in section ii."
"in addition to standard set operators (such as set union), projection, and grouping (sec. 4.2.1), our set algebra unifies three different kinds of queries: (1) topological relationships of objects (sec. 4.2.2), i.e., their connectivity, which is represented internally; (2) spatial relationships of objects (sec. 4.2.3), such as distance or proximity; and (3) queries on attributes of objects (sec. 4.2.4), which can be physical properties, e.g., size/volume, or abstract properties, e.g., function."
the remainder of this paper is organized as follows. section 2 introduces related literature. section 3 describes the vmi model in terms of the retailer subsystem and manufacturer subsystem. section 4 studies the stability and inventory oscillations for the retailer subsystem. the production smoothing effect and inventory fluctuation for the manufacturer are studied in section 5. section 6 focuses on the robustness of the bullwhip effect problem under uncertain demand. this paper is concluded in section 7.
"furthermore, multiscale optical flows are also computed on each gaussian filtered image. multiple-scale optical flows are calculated according to the iwld scales. a local extreme of the multiscale iwld feature points can only become an interest point if it also has sufficient motion in terms of multiscale optical flows in the gaussian pyramid. we assume that a complicated action can be represented by the combination of a reasonable number of interest points. therefore, we do not assign strong constraints to stips. as long as the candidate interest points have made movements with distances larger than a minimum value, they can be deemed as moiwld interest points. the extracted moiwld interest points are invariant to scale and rotation in the spatial domain, but they are not invariant to scale in the temporal domain. the multiscale moiwld selects distinctive interest points with sufficient motions from which humans can see the actions happened at the corresponding points and machines can learn an action model."
the third assumption consists in the approximation of a blood vessel as a rigid pipe. this will use in a first setting of our calculations as the in vitro model will be a rigid tube. the systemic human arteries can be subdivided into two types-muscular and elastic-according to the relative compositions of elastic and muscle tissue in their
"each set that is the output of a specific query can be used right away as the input to follow-up queries. in this way, very complex queries can be built up incrementally from simple queries in a hierarchical manner. fig. 4 illustrates a simple example. queries can be visualized as trees, where the input sets are the leaves of the tree, and the predicates or operators are the internal nodes of the tree. sec. 4.3 describes how users can specify queries in a visual way, and how they can visually inspect each result set of the evaluation of a predicate or an operator."
"the computation of pair-wise euclidean distances between objects is based on the octree data structure described above. we base our algorithm on the work of dyllong and grimm [cit], who compute the distance between two octree-encoded objects at interactive rates, even for octrees with a large number of hierarchy levels. the algorithm is based on the idea to track the nodes that potentially minimize the distance between both objects. to compute the distance between two table 2 . data set statistics for the data used in sec. 7. we received two different versions of (partly conflicting) manually labeled synapses, which we then merged into a single consistent data set. statistics of the segmentation data are given in the middle part of the table. at the bottom we list statistics of three regions of interest that were specified by our collaborators and exhibit a higher density of segmented structures. objects, we update this list iteratively until the set of nodes closest to each other is found. for proximity queries, we can stop this process as soon as two objects are closer/farther than the specified predicate. for voxel-accurate distances, a final step iterates over individual voxels."
"at this point, we can create an android activity component 6, and start deploying our sudoku application: the class contains a handler instance as field, that is initialized when the activity is created as an androidhandler. required parameters include the android context (an android utility, needed to start an android service component) and the type of androidservice to use -in our case, a dlvandroidservice. in addition, in order to represent an initial sudoku schema, the class features a matrix of integers as another field where position (i, j) contains the value of cell (i, j) in the initial schema; cells initially empty are represented by positions containing zero."
"to set up the evaluation environment, we have considered the reference scenarios shown in section 2. we envision a fog architecture where an organization has resources in different geographical regions. these resources can be of any nature: devices, servers, or any software component that resides on the edge and fuels the applications from users on different locations. since the regions are distant from each other, there are bandwidth and latency restrictions between them. as previously shown in figure 1, a fog architecture assembles a tree as a hierarchical structure. a central region is located at the highest level and all the other regions communicate with it, either to pull data that applications at the edge may need, or to push data needed at the central location. from the root of this tree, the organization needs to monitor and makes decisions about its fog infrastructure. for example, thanks to the edge monitoring agents, we can detect overloaded edge devices in a given region or a node that went down. there are several issues that arise when monitoring this type of environment. the first one is whether sending all this monitoring information to a central location affects application performance (section 5.2). we hypothesize that it does, since at the edge the connectivity resources such as available bandwidth are limited. the flexible monitoring pipelines that fmone provides are expected to alleviate that effect. overhead and resource usage are also important characteristics, since we do not want the monitoring process to interfere with the host performance (section 5.3). we also need to know how fast we can deploy a monitoring agent in an ever-changing infrastructure where elements are constantly joining and leaving the network (section 5.4). finally, we need an autonomous and resilient monitoring system in such an unstable scenario (section 5.5)."
"with large high-resolution displays, users are expected to perform pointer movements over a wide range of distances and target sizes, ranging from fast long distances to precise small targets. such multiscale displays are capable of displaying small objects far apart from each other [cit] . fast and accurate acquisition of such objects imposes two main requirements on the pointing device: fast pointer movement for long distances [cit] and high precision for fine-grained refinement of pointer position."
"our study is composed of two parts. the first part (rq1-2) focuses on the accuracy of cilla and compares it to tools currently available to web developers for analyzing css selectors. the second part (rq3) empirically explores how omnipresent unused css code is in web applications. the experimental data produced by cilla is available for download. 2 to run the experiment, we provide the url of each experimental object to cilla and choose the default crawling settings for event generation and dom state comparison. then we run the tool and save the generated output."
"when the initial index page of a web application is loaded, cilla extracts all external and embedded css sources. then it retrieves and parses the css code to create a map 2 http://salt.ece.ubc.ca/content/cilla/ of all the css rules, selectors, and property declarations associated with each source. each rule's relation to the elements of the current dom tree is examined and the map is annotated with the findings (algorithms 1-2) . then the class attributes of all the elements on the dom tree are analyzed (algorithm 3)."
"viii. concluding remarks this paper presents an automated approach for analyzing the relation between css rules and dom elements of web applications. our technique, implemented in a tool called cilla, is capable of detecting unmatched and ineffective selectors and properties as well as undefined class values."
"to further reduce physical and cognitive costs of dynamically switching cd curves, refined design ideas for integrating these concepts into a single mode of interaction can be explored in future work, such as using pressure sensitive touchpads. the core concept of cleanly combining implicit and explicit granularity control into a single mode of interaction can be applied to other kinds of input devices, such as \"smarties\" [cit] that uses mobile touchscreen devices for wall display interactions. alternatively, the number of fingers touching the surface of a mouse could act as a trigger to change pointer acceleration multipliers, similar to lmh. furthermore, beyond varying the number of touching fingers, other input mechanisms could be employed, such as distance [cit] from the screen. finally, these integrated curve-switching techniques should be compared with dualor multimode techniques that link the various cd curves to different modes of interaction (e.g., chairmouse [cit] ) to more closely examine the effects of interaction modality."
"2⇥precision⇥recall precision+recall where t p (true positives), f p (false positives), and f n (false negatives) respectively represent the number of unused css selectors that are correctly detected, falsely reported, and missed. to document t p, f p, and f n in a timely fashion while preserving accuracy, we randomly select 20% of the css rules from the first ten systems in table i as follows:"
"(i) we set up a dc/os cluster with 1 bootstrap node, 3 master nodes, 1 public node, and 73 private nodes, which are going to be monitored. remember that dc/os is the container orchestrator that is going to deploy the fmone pipelines. further information about its parts can be found in their website (https://docsmesosphere.com/1.10/overview/ concepts/ [cit] ). (ii) we divide the 73 private nodes in 4 regions to emulate a geo-distributed fog environment:"
"to address the problem of continuous pointing on large high-resolution displays (lhrd), providing both fast and accurate pointing with minimal cognitive barriers, we investigate the use of multiple (specifically, three) pointer acceleration curves to combine the benefits of implicit and explicit pointer acceleration. we propose design goals for the approach, suggest a specific technique implementation, and empirically study the tradeoffs of this multicurve approach against the single-curve approach. the design goals are as follows."
"r2 : :-red(x, y ), red(x, z), red(y, z). r3 : :-blue(x, y ), blue(x, z), blue(y, z), blue(x, w ), blue(y, w ), blue(z, w )."
"in this section we explain the design principles of fmone, a monitoring tool created to satisfy the requirements of section 3. by leveraging existing technologies, fmone facilitates to the user the deployment of the monitoring agents across the fog infrastructure. the main advantage of its plugin-based design is to enable the creation of monitoring pipelines that are adapted to the above-mentioned fog particularities."
"writing css code is not trivial [cit] . it requires human computer interaction, graphic design, as well as web programming skills [cit] . in addition, the language has a number of characteristics [cit] such as inheritance, cascading, and selector specificity, which makes understanding how css properties are applied to document object model (dom) 1 elements at runtime a daunting endeavour for web developers. therefore, developers are continuously faced with challenging questions during web development and maintenance tasks: is my web application using all of the defined css rules? which ones are obsolete? what will happen if this css rule is removed? will it break the layout in some pages? is this selector really effective or is it overridden by another rule at runtime? which dom elements does this rule affect? are there any undefined classes in the html code? consequently, as a web application evolves, unused rules or ineffective ones with properties that are always overridden start to accumulate over time. this accumulation of unused code has a number of negative consequences:"
"inheritance. inheritance in css [cit] allows a styling property value to be propagated from the parent to the descendent elements. our motivating example shows a simple case of inheritance in working. the span element inherits the background color property (silver) from its parent element p, through the css rule #news. an inherited property can also be overridden in css. for instance, the same span element has a parent with a font color of black, but since there is a more specific color property defined for the element through #news span, the value of the inherited property is overridden by red."
fmone is designed as a framework that includes the monitoring agents together with the management of those agents and their back-ends. the general architecture of the system is depicted in figure 2 . we explain here each of the parts.
"as with any kind of distributed infrastructure, monitoring the fog and its different elements is a valuable tool to contribute to maintain quality of service (qos) and the performance of the applications running on top. this presents 2 wireless communications and mobile computing several challenges due to the unique characteristics of its nodes and architecture [cit] : (i) heterogeneity: machines comprising the fog can have different architectures and resources, like cpu or memory. some examples include mobile devices, settop boxes, or raspberry pis. (ii) mobility: nodes are no longer static but can reside at mobile stations, such as trains or cars, or fixed locations, like shops or buildings. (iii) connectivity: nodes in the edge are able to communicate effectively through different types of wireless connections. this connectivity can be intermittent because of the above mobility reason. (iv) resource-constrained nodes: every device near the edge providing computation and/or storage can be part of the fog infrastructure. instead of having a small number of powerful machines, many lowpower devices can answer the requests of nearby clients. (v) geo-distributed: the nodes that make up the fog are distributed across different locations. this imposes some latency between machines at distant locations that needs to be considered in the monitoring requirements."
"we know by definition that the declaration property of a selector that has a higher cascading order and specificity weight wins over all the competing selectors. thus, once the selectors for an element are ordered based on their specificity and location, the algorithm starts annotating the properties in that exact order. for each property in a selector, the algorithm first checks whether the property has already been overridden by a higher order property (line 10). if that is not the case, the property is marked as effective (line 11). when an effective property has been identified, all the properties of the competing selectors that have the same property name are marked as overridden (lines [cit] . at the end of this step, we can distinguish between effective and ineffective properties."
"first, there is a disparity of technologies that will form the backbone of this platform, such as databases, messages queues, or data processing frameworks. ensuring that each of these pieces are working properly means different types of metrics, depending on the technology that is being monitored. secondly, we expect high computing and networking demands in these kinds of applications. much of the data has to be moved from the server to the gaming clients in a multiplayer environment; thus the monitoring system should not clog the network connection or stall the computing power of fog nodes, especially when their resources are limited. finally, we may need to look more closely at the problems that could arise in a particular region. this means that we should be able to deploy monitoring agents on a subset of problematic devices, e.g., all the devices corresponding to a user segment."
"in this section we introduce embasp, an abstract framework for the integration of asp in external systems for generic applications; then, we propose a java implementation."
"the user interacts with the app via a standard graphical interface; the reasoning module is actually in charge of building a proper asp program, which is in turn fed to dlv via embasp. such program matches the classical \"guess/check/optimize\" paradigm introduced in section 2, thus resulting easy to understand, enrich and customize:"
"fog computing is powered by a complex and heterogeneous infrastructure that is constantly evolving. this presents several challenges when ensuring the performance and quality of service of the applications they host. in this paper, we have listed those challenges and proposed fmone, a monitoring framework that coordinates highly customizable monitoring agents and their back-ends in a flexible way. we have demonstrated that this tool can satisfy the different requirements through a series of experiments that demonstrate its resilience, elasticity, and nonintrusiveness. we have also showed that the centralized monitoring approach used by the cloud is not the best fit for fog computing. the devices that comprise the fog have features such as location, connectivity, or hardware resources that vary across the infrastructure and, therefore, the monitoring process should adapt accordingly. our experiments also show that container technologies can enable the fog computing paradigm. same as with fmone, other fog applications can be deployed on disparate devices, such as smart phones or any other system as long as they support the deployment of containers. this will completely detach the application from the hardware and operating system on which it runs. this also opens up the possibility of a lightweight container orchestrator for fog that coordinates thousands of devices with limited resources. we have used dcos to ease the execution of the experiments but a more lightweight container orchestrator like hypriot cluster lab [cit] might be needed when using resourceconstrained devices at the edge."
"after all the dom states are covered and having annotated all the matched selectors and selectable elements, the algorithm iterates over all the rules to find the set of unmatched selectors (lines [cit] . going back to our motivating example, this algorithm returns p.select, h3, and h4 as the detected unmatched selectors since these do not touch any of the elements in the two given dom states. in addition, through this algorithm, we know exactly which elements in the two dom states are selectable by which css selectors. we use this information in the following step of our analysis."
"the problem of embedding asp reasoning modules into external systems and/or externally controlling an asp system has been already investigated in the literature; to our knowledge, the more widespread solutions are the dlv java wrapper [cit], jdlv [cit], tweety [cit], and the scripting facilities featured by clingo4 [cit], which allow, to different extents, the interaction and the control of asp solvers from external applications."
"mobile online games could be one of the industries that most benefit from fog computing. it would reduce latency problems for users, by bringing the game endpoint closer to them, as well as adding an additional location context that is required by many augmented reality applications [cit] . within this field there are companies that offer a paas for game developers to abstract things like scaling, player management, user segmentation, and many complex multiplayer features (https://www.gamesparks.com/ [cit] ). it also assures game developers that their games are going to run within some qos and slas that paas has to comply with. if we imagine this paas running in a decentralized fog environment, there are some challenges to be considered in terms of monitoring."
"notably, such feature is meant to give developers the possibility to work separately on the asp-based modules and on the java side. the mapper acts like a middle-ware that enables the communication among the modules, and eases the burden of developers by means of an explicit, ready-made mapping between java objects and the logic modules."
"since a css rule can have grouped selectors, we iterate over the set of selectors of the current rule (lines 7-9). then, in case the xpath expression retrieves elements from the dom (line 12), we annotate the selector as matched (lines [cit] and add the retrieved elements to the overall set of selectable elements (line 15)."
"the back-ends do not have to be in the same region as the agents, although it is recommended because of the previously mentioned latency between locations. this user specification will be translated into a container deployment plan inside the fog infrastructure, in our case, powered by marathon. we have included the different plugins that we have developed so far and the different parameters that they require as an appendix in tables 3, 4, and 5. as a final summary of how these pipelines are translated into docker commands, we detail an example in which the user wants to monitor all of the container metrics in a single region, send them through a rabbitmq message queue to another fmone agent that averages the metrics for the last 5 seconds, and finally store them into a mongodb. the example is depicted in figure 5 . note that all these docker commands are automatically executed by marathon, the container orchestrator tool, while fmone takes responsibility for building the deployment plan needed by marathon for each monitoring pipeline."
"(a) inplugin: the plugin extracts the monitoring information from a component of the system every seconds. this set of metrics is kept in memory before publishing it to the back-end. examples of these metrics are the cpu load, the stats of a docker container, or the number of messages received per second in an mq queue. we have defined a series of plugins that the users can use, allowing them to create new ones on their own by implementing a simple interface. this versatility to monitor different technologies answers to requirement r7. to extract the metrics, the docker image for fmone is started in each host, with docker as the inplugin parameter, inout for the midplugin, and rabbitmq as the outplugin. the rabbitmq container will also be created as part of the pipeline and its hostname used as a parameter for the previous outplugin. the aggregation will be performed by a different fmone agent that will pull the metrics with its corresponding rabbitmq inplugin, average them with the average midplugin, and store them in a database with the mongodb outplugin. the pipeline will be completed with the creation of a containerized mongodb back-end. marathon and mesos are used to start these containers in the desired regions."
"our main goal in this study is to determine how users leverage the combination of implicit and explicit pointing granularity control to interact with multiscale tasks on a lhrd. we specifically observe how users utilize the ability to dynamically alter acceleration curve properties during task performance by simply varying the number of contacting fingers touching the surface of a touchpad. does the addition of multiple acceleration curves into a single mode of interaction enable users to improve performance over a single acceleration curve? does this access mechanism overcome physical and cognitive barriers introduced by explicit switching? what implicit versus explicit switching behaviors do users exhibit during pointing tasks of varying scale characteristics? to study and understand these issues, we compared multifinger lmh with the standard singlefinger technique at each of the three fixed curve multipliers (l, m, and h)."
"to answer rq2, we compare the results produced by cilla to the results generated by two industrial open source tools, namely dust-me selectors (dms) [cit] and cssess [cit] . similar to cilla, both these tools aim at spotting unused css rules. since dms is a firefox plugin, we use firefox for the state exploration phase in all our experiments to make comparisons possible (cilla supports ie and chrome as well). note that the same set of randomly selected samples are used to conduct the comparisons through precision, recall, and f-measure as defined above. since cilla, dms, and cssess are not able to analyze pseudo classes/elements, we ignore pseudo items in the evaluation of all the three tools (see table ii, # ignored selectors)."
"it is worth noting that, although the framework has been mainly conceived for fostering the usage of asp, its abstract core makes it also adaptable to other declarative knowledge representation formalisms."
"(a) one region called \"central region\" which represents the upper cloud layer. this region has a cassandra cluster installed, which will be used to measure performance with yahoo cloud serving benchmark (ycsb) [cit] . in addition, this region also hosts a kafka cluster, which we will later use to collect the monitored metrics, acting as a traditional centralized monitoring system. both technologies will be hosted in 4 different nodes each so as to avoid any interference or overhead. (b) in the remaining three regions, named \"edge regions\", we install the ycsb clients that are going to query the cassandra database in the central region. this represents the communication between components at the edge and the cloud in terms of operations/sec. we will use this metric later to assess how a centralized monitoring approach can degrade the performance of the other components. the clients are configured to perform as many operations as they can per second until they complete 1000 operations."
"in this sense, telcos should be able to properly manage their resources, to enable infrastructure as a service (iaas) at the edge of the network. all of this requires to monitor the different running devices to make decisions that are analogous to those needed in a data center: scheduling, resource allocation, qos, or slas, among others. but in this case, the resources are not traditional nodes running in a data center, but small to medium smart devices, with different hardware specifications. there can be thousands of these smart nodes in every geographical region, which must be monitored despite their differences."
"the logic program is able to find the combinations of activities that should be performed in order to burn the remaining calories. obviously, this goal can be achieved, in general, in many different ways, each of them modelled by a different answer set. part of the rules of the program that we used are reported hereafter; full program is available online. the guess part chooses how much time to spend on each exercise. the check part checks that each activity selected has one specific amount of time, it ensures that all the remaining calories are burnt and that not more calories than the remaining (with the surplus) are burnt and it ensures to not exceed the maximum time that the user wants to spend on the workouts. the optimize part makes use of weak constraints [cit] : in case the user specified preferences about activities, tries to select the favourite ones; in case she specified preferences about the time spent exercising, tries to minimize it; if she specified preferences about the number of different activities, tries to minimize it."
"long distance. for long distance targets (figures 12(d), 12(e), 12(f) and 12(g)), most lmh users exhibited consistent use of three fingers. on average, three fingers were used to cover 84% of the pointer's total trip to the target. one and two fingers contributed only 9% and 7%, respectively. however, only one participant deliberately used only one or two fingers ( figure 12(d) ), ignoring the three-finger mode. pointer overshooting was observed for some of the participants, especially during the practice trials as users accustomed themselves to lmh. we believe that overshooting is primarily attributed to the fact that users need some time to learn how to smoothly switch out of the three-finger mode as the pointer approaches the target. for the entire experiment, three general patterns emerged: (1) \"spot on\" pattern ( figure 12 (e)) with straight trajectory, minimal use of slow speeds, and minimal overshooting, (2) \"gradual descent\" pattern ( figure 12 (f)) with straight trajectory, prolonged use of slower speeds on approaching the target, and no overshooting, and (3) \"zig-zag\" pattern ( figure 12 (g)) with a bent trajectory, overshooting at high speed, and late use of slower speeds to hone in on the target. from these results it is clear that most users did make good use of the ability to explicitly choose among multiple acceleration curves. they did appropriately adjust their usage patterns according to the varying multiscale targeting conditions of each task, using primarily three fingers (h) for the long distance tasks, two fingers (m) for medium distance, and one finger (l) for short distances. they did dynamically switch curves during the course of individual targeting tasks. they primarily used a \"high-to-low\" pattern, switching to fewer fingers during the course of a targeting task to hone in on the target, similar to the theoretical ideal pattern. they almost always finished tasks with a single finger (l) for final refinements. there was variance in the margin within which they switched down to a lower multiplier, including some undershooting and some overshooting. the usage patterns indicate that they tended to clutch when switching curves, and vice versa, suggesting that more training time might enable them to use the technique more smoothly with less clutching."
"does the multifinger access mechanism overcome physical and cognitive barriers introduced by explicit switching? this evidence indicates that the lmh multifinger pointing technique did overcome much of the physical and cognitive barriers associated with explicit curve switching by preserving the same model of interaction across scales. participants said they found it easy to use and natural to dynamically switch settings, and their usage patterns indicate that they did fluidly switch the number of fingers during the course of each task in helpful ways. the fact that users performed as well as they did with minimal training is evidence of the natural fit of the multifinger technique."
"thanks to embasp and the use of asp, guessandcheckers features a fully-declarative approach that made easy to develop and improve several different strategies, also experimenting with many combinations thereof."
"we have shown how it is possible to deal with a problem by means of an asp program such that the instance at hand has some solution if and only if the asp program as some answer set; in the following, we show an asp program whose answer sets witness that a property does not hold, i.e., the property at hand holds if and only if the program has no answer sets."
"based on this definition, a selectable element is a dom element matched by a selector. algorithm 1 shows our algorithm for analyzing css selectors and their counterpart dom elements. for each detected dom state (line 5), the algorithm extracts all the new css rules and adds them to the overall set of css rules (line 6). the utility function extractnewcssrules scans each new dom tree looking for unexamined embedded rules and external css files. using the url of each external css resource, the content is retrieved through a http request. then the extracted css code is parsed and transformed into an object model, containing all the information about the rules and declarations, as well as their resource (e.g., url of the css file) and location (e.g., line number)."
"in our implementation, such translations are guided by java annotations 3, a form of metadata that mark java code and provide information that is not part of the program itself: they have no direct effect on the operation of the code they annotate. they have a number of uses, such as directions to the compiler, compile-time and deploymenttime processing, or runtime processing. for more details, we refer the reader to the java documentation."
"in this example the activities that can be performed (\"on_-bicycle\", \"walking\" and \"running\") are specified along with the calories they allow to burn per unit of time; then, the amount of time spent for each activity is reported. moreover, there are pieces of information about the calories that remain to burn in the current day (at least 200, and up to 300 due to the surplus) and the maximum time that the user wants to spend on the workouts (20) ."
"(ii) aggregation/filtering of metrics (r2). fog applications have an implicit aggregation workflow that starts from the edge and ends at the central cloud, as seen in figure 1 . we believe monitoring of such an infrastructure should be done the same way. finegrained data can be kept at the edge for regional data analysis, and aggregated information can be sent to the cloud for central reporting or visualization purposes. for example, a telco could have a dashboard that shows the cpu usage by region, overlapped with a map showing the location of its fog computing assets at the time. the telco can navigate up and down the data to have a better control of the infrastructure and its metrics. in addition, users should be able to define a criteria for nonimportant data that can be filtered out to not create any additional traffic or storage burden."
"this section evaluates fmone, demonstrating that the prototype we have built meets the above-mentioned requirements (see section 3). keep in mind that these experiments are part of a quantitative evaluation of fmone performance. a complete qualitative evaluation of the whole set of requirements is included in the related work section, together with a comparison between the different state-ofthe-art monitoring tools (see section 6). a prototype of the agent can be found on docker hub (https://hub.docker.com/ r/alvarobrandon/fmone-agent/ [cit] ). we plan to upload in the future the dcos framework able to coordinate the deployment of the fmone pipelines."
"this shortcoming introduces the potential need for dynamic variation of the acceleration curve itself. large highresolution screens create a multiscale targeting environment where efficient continuous pointer positioning requires a diverse range of cd gains and acceleration curves for all scales of interaction that are possible on the display. furthermore, since targeting conditions can rapidly vary in levels of scale, acceleration curves should be dynamically changeable in real time."
"css also has the notion of pseudo elements (e.g., p:first-line) and pseudo classes (e.g., a:visited) to allow style formatting based on information that lies outside the dom tree [cit] ."
"in general, these results validate the basic hypothesis that individually l, m, and h excel at different distance scales. in particular, l exceled at short distances but performed very poorly at long distances. m performed well at the longer distances. h appeared to be overkill, but given the trend it would likely do well at longer distances or larger targets than we tested."
"the precision and recall rates, measured for cilla, dms, and cssess, are presented in figure 4 . the f-measure is shown in figure 5 . as far as rq1 is concerned, our results show that cilla is highly accurate in detecting unused css selectors. the recall is 100%, meaning that our approach can successfully spot all unused selectors of type class, id, and element, present in a web application. the precision oscillates between 80-100%, which is caused by a low rate of false positives (discussed in section vii under limitations). the f-measure varies between 90-100%. the comparisons in figure 4 and 5 emphasize that the accuracy of cilla is higher than that of dms and cssess in recall, precision, and f-measure (rq2). dms performs better than cssess but it still suffers from a high rate of false positive and negative."
"otherwise where, ↵ is the number of randomly selected css rules and n is the total number of css rules. we manually examine the samples against the reported output. it is worth mentioning that manual checking of css rules is a labour intensive task. in this study, examining the ten samples took approximately 36 hours (in 4 days)."
"optimize ( \" running \", 1, 3). optimize ( \" on_bicycle \", 3, 3). optimize ( \" walking \", 2, 3). optimize ( time,0,2). optimize ( activities, 0, 1)."
"edge computing has been proposed as an alternative method that could solve these problems, by performing data processing closer to its source [cit] . in this line of thought, a new architecture complementary to the cloud, named fog computing [cit], has been proposed. in this approach, the central cloud is still present, but some intermediary nodes stand between the edge devices and the cloud in a hierarchical manner [cit] . this enables to filter and preprocess the data before sending it to the cloud, reducing the network traffic. it also allows pushing some complex applications closer to the edge, reducing the communication latency of the clients using them."
"limitations. similar to other tools, one of the limitations of our technique is that pseudo elements/classes are currently not supported. table ii presents the number of ignored pseudo selectors and properties. as mentioned earlier, we ignore pseudo items in the evaluation of all the three tools. therefore, any unused pseudo item that is ignored could be regarded as a false negative. as shown in figure 4, unlike dms and cssess, cilla achieves a 100% recall rate. this implies that in cilla, false negatives could potentially be caused only by unused pseudo classes/elements. since our approach is based on dynamic analysis, the explored state space is only a subset of the entire state space. this limitation is in part inherent to all dynamic analysis techniques. thus, if a css selector is used in a certain dom state not present in the explored set, then that selector is mistakenly marked as 'unused'. a low rate of false positives produced by cilla is caused by these css rules. the conclusive outcome from our evaluation, apart from the fact that unused css code is omnipresent on the web, is that within the automatically explored state space, our technique is capable of detecting unused selectors with a high level of accuracy."
"in the following, we propose a java 1 implementation of the architecture described above, along with proper specializations for two of the state-of-the-art asp systems. in particular, we implemented the main modules by means of classes or interfaces, and we created specialized libraries that permit the use of dlv on android 2 and clingo on desktop (i.e., any java-enabled desktop for which clingo is available). figure 2 provides some details about classes and interfaces of the implementation. for the sake of presentation, we do not report the complete uml [cit] class diagram, which is quite involved; rather, we illustrate a simplified version. although methods inside classes have been omitted to further improve readability, adopted connectors follow uml syntax. in order to better outline correspondences with the abstract architecture of figure 1, classes belonging to a module have been grouped together. the complete uml class diagram is available online at [cit] ."
"medium distance. for medium distance targets ( figures 12(a), 12(b) and 12(c)), all lmh users employed a combination of one and two fingers. they usually started with two fingers (medium speed curve) and, once near the target, switched to one finger (low speed curve). some users switched to one finger prematurely (figure 12(a) ). some were accurate with two fingers (figure 12(b) ). we observed some pointer overshooting with two fingers, followed by one-finger fix (figure 12(c) )."
"table ii presents the examined properties of each system, produced by cilla. the table shows the number of examined css files, total lines of embedded and external css code, number of css rules, number of css selectors, number of ignored selectors (pseudo elements/classes), number of css declaration properties, number of ignored properties (belonging to a pseudo selector), and the number of dom states examined. table iii shows the results of our evaluation produced by cilla. for each experimental object, the table shows the detected number of undefined classes (uc), unmatched selectors (us), ineffective selectors (is), unused selectors (us+is), unused declaration properties (up), the percentage of unused selectors, and the percentage of unused properties. we calculate the percentage of unused selectors (% (us+is)) and unused properties (%up) as follows:"
"the implementation illustrated above relies on java. besides the fact that it represents a very popular, solid and re-liable programming language, the choice was also motivated by the intention to foster the use of asp in new scenarios, and in particular in the mobile one; android is by far the most widespread mobile platform, and its development and deployment models heavily rely on java. however, as already stated, the abstract architecture of embasp can be made concrete by means of other object-oriented programming languages. a thorough discussion about different language implementations is out of the scope of this work; however, we briefly discuss in the following some interesting possible approaches."
"with different languages that lack such features, the mapping mechanism can still be implemented with a simulation via inheritance and polymorphism and applying typical software engineering patterns [cit] . as a matter of example, one possible implementation can be accomplished using the prototype design pattern, that results well-suited to our purposes, as it allows to \"specify the kinds of objects to create using a prototypical instance, and create new objects by copying this prototype\" [cit] . such pattern can be the key to simulate the dynamical loading of classes in languages that do not support it natively, as it happens with c++. indeed, the run-time environment can make use of it in order to automatically create an instance of each class when it's loaded, and then register the instance with a prototype managerin our case, represented by the aspmapper component. all classes that in java (or similar languages) would make use of reflection and annotations, can be defined by extending a properly defined prototype class and then specify how to map predicates and terms. moreover, a class aspmapper would still be needed, with a behaviour quite similar to the java case."
"the traditional mouse and touchpad provide very comfortable pointing experience with standard sized displays, with high accuracy and speed, and minimal physical and cognitive load. modern operating systems provide dynamically variable cd gain, which enables accuracy at low speeds and accelerated pointer movement at higher speeds [cit] . as a result, the mouse and touchpad have become ubiquitous pointing devices for normal displays. however, implicit granularity control, while being very useful on standard screens, fails to scale up to the diverse pointing conditions on lhrds [cit] ."
"from traditional clusters to modern cloud and fog systems, monitoring has always been an important aspect of distributed system analysis and management. different approaches and tools have been applied at different levels of the distributed environment, often combining their efforts. this creates a vast ecosystem of monitoring tools that we try to summarize in this section by detailing the most recent work."
"additional interaction techniques can give users explicit granularity controls to change the acceleration curve. for example, windows os allows users to apply a multiplier (\"pointer speed\" in the control panel) to the acceleration curve [cit], but this is a global setting not intended for dynamic use during tasks. other examples, such as nancel [cit] and dasiyici [cit], attempt to accommodate the multiscale environment by allowing users to manually alternate between predefined curves by a mode switching mechanism. their studies indicate that the interaction with the switching mechanism created a cognitive and a physical barrier that negatively affected performance."
"as future work, we want to evaluate the traffic bottlenecks that this kind of system can experience in an environment with real gateways that route traffic between regions. we also want to add new plugins to the tool and explore the possibility of using complex algorithms to raise alarms and detect anomalies directly inside the agent. to facilitate the management of the fmone pipelines by the user, we would like to add a web ui, allowing the user to easily select the placement of the different back-ends and monitoring agents of the pipeline. as a parallel line of work, we want to explore methods that can define regions within a fog infrastructure, depending on the connectivity between its components."
"furthermore, our lmh technique improves on jellinek's mechanism for explicit switching by using modern multitouch touchpad devices to incorporate explicit controls into a single mode device. while most interaction techniques for lhrds focus on multimode interactions, the issues associated with the process of switching between modes and the two modes interfering with each other are likely to remain. lmh addresses this issue by cleanly integrating the multiscale nature of the interaction technique into a single mode of interaction. thus, we hope to minimize cognitive barriers that would normally occur during the transitions between different modes of interaction."
"based on the css specification [cit], we calculate the overall specificity weight (sw) of a selector s composed of different selector types as follows: concatenate(a, b, c, d) where a, b, c, and d are calculated as follows:"
"for example, eye-gaze and head tracking techniques allow coarse positioning, which can be combined with traditional mouse or touchpad for precise target acquisition [cit] . ray-casting is improved when precise positioning is added. specifically, vogel and balakrishnan [cit] introduce an interaction technique that allows switching between absolute pointing for coarse positioning and relative pointing for precision. conversely, [cit] enable switching between absolute pointing with a pen for precision and relative pointing with high cd gain for coarse positioning. [cit] captured natural body movements in a rotating chair to enable coarse positioning and combined it with precise positioning using a traditional mouse. [cit] combined eye tracking with precise pointer stick positioning. [cit] introduced a multifinger gestural input with multitouch touchpad for interaction with large displays. the input technique enables coarse and fine positioning by mapping the left half of the touchpad to the entire screen and right half to a small rectangular area previously positioned using the left half (analogous to overview + detail). effectively this approach allows direct manual control of the scale of interaction. users switch between the two parts of the touch surface to alternate between scales."
"solutions, in this context, are actually workouts suggestions to the user. the optimize predicate is of arity 3, and third argument is supposed to express the \"importance\" of the statement (the higher the number, the more the importance). in this example, the asp code models that: (i) the user wants (preference level: 3) to maximize the number of favourite activities to perform, and provides an order ( \"running\" first, then \"walking\" and finally \"on_bicycle\"); (ii) if more than one admissible workout is found featuring the same favourite activities, she wants to minimize the total time spent exercising (preference level: 2); also, (iii) if there are workouts that have the same favourite activities and the same time, she wants to minimize the total number of activities (preference level: 1)."
"in the former, only one aggregated metric is sent to the central region and, in the latter, everything is stored within the monitored region. the flexible monitoring model that fmone provides, through the aggregation, filtering, and geoaware storage of the metrics, enables an optimal placement of the monitoring workflow, eliminating any impact on the performance of the applications."
a class attribute value c attached to a dom element is said be defined if and only if there exists at least one css selector that matches c.
"other potential designs that achieve these design goals are possible. for example, a pressure sensitive touchpad could vary the acceleration curve multiplier with finger pressure, called pressure pointing. this design has very similar characteristics to multifinger pointing. pressure-pointing could conflict with the existing use of pressure sensitive clicking."
"monitoring such an infrastructure poses several challenges. firstly, nodes may join and leave the network frequently (e.g., consider a mobile device that can leave the carrier's coverage area, shutdown, or fail). the monitoring services should be able to handle this type of behavior. secondly, job scheduling, resource provisioning, and allocation mechanisms require monitoring metrics with very low and predictable latency to make fast decisions. however, the round-trip time between the central data center and the edge devices in a geographically distributed infrastructure could be long. this creates the need for keeping the monitoring data at the edge, following the same paradigm as fog applications, something that is not easily achieved with current monitoring solutions. additionally, and as it will be shown later in our experiments, the constrained bandwidth of devices at the edge is another important factor in the monitoring process. the monitored information sent outside the local network should be kept to a minimum, in order to leave the available bandwidth to processes that need to use it intensively. finally, if monitoring metrics are going to be kept at the edge, we need to consider how they are going to be ingested and stored by the devices (e.g., key-value stores, time series database, and message queue). this depends, firstly, on the amount of measurements and, secondly, on the computation and storage capacity of the devices hosting these back-ends. consequently, we should be able to change the metrics backend in a flexible way."
"at this point, the reasoning process can start; since for android we provide only the asynchronous execution mode, a callback object is in charge of fetching the output when the asp system has done (lines 39-40)."
"we show next an asp program p sudoku such that the answer sets of p sudoku ∪ f correspond to the solutions of the sudoku schema at hand; note that, in general, wellfounded sudoku instances have only one solution, and thus p sudoku ∪ f will have a single answer set."
"single mode interaction techniques with explicit granularity control allow users to maintain the same mental model for the interaction while providing the ability to explicitly change pointing characteristics of the input device. extending the single interaction mode to accommodate multiscale targeting, by adding explicit controls, has the potential to not only provide high speed and accuracy but also eliminate the need to readjust to different pointing modes when interacting with lhrds [cit] ."
"the acceleration curve is applied to the average of the velocity vectors of all the fingers contacting the touchpad as follows. whenever fingers move on the touchpad, tuio reports each finger independently with its own velocity vector ( figure 5) . lmh computes the overall control velocity vector advances in human-computer interaction 7 as the average of the velocity vectors of all fingers at the current timepoint (see (1) ). then, lmh computes the cd gain on the magnitude of the control velocity vector using the pointer acceleration curve (figure 4 ) and applies the multiplier (l, m, or h) depending on the number of fingers currently contacting the touchpad as reported by tuio (see (3) ). the cd gain is then applied to the control velocity vector to compute the display pointer velocity vector, which is used to displace the pointer on the screen. consider"
"similarly, for the desktop platform we developed a desk-tophandler and a desktopservice, which generalizes the usage of an asp reasoner on the desktop platform, allowing both synchronous and asynchronous execution modes."
"in this paper we presented a framework for embedding asp reasoning and modeling capabilities into external systems. the fully abstract architecture makes the framework general enough to be adapted to a wide range of scenarios; indeed, it can be implemented in any programming language, grounded to different platforms, and can make use of different asp solvers. we herein presented an actual java implementation and two specialized libraries for embedding dlv on android applications and clingo on any java-based desktop application. the framework has been tested within some university courses featuring asp topics, for implementing a set of applications, ranging from aibased games to educative apps; it proved to be an effective set of tools and interoperability mechanisms able to ease the development of asp-based applications, in both educational and real-world contexts."
"in a within-subjects design, each participant performed all target tasks with all four interaction techniques. eight undergraduate students participated in the experiment (three male and five female) between 19 and 23 8"
"in this work, we present a framework for the integration of asp in external systems for generic applications; it consists of an abstract architecture, implementable in a programming language of choice, that easily allows for proper specializations to different platforms and asp reasoners."
"eventually, once the computation is over, from within the callback function the output can be retrieved directly in form of java objects. for instance, in our case an inner class mycallback implements the interface callback:"
"guessandcheckers is a native mobile application that works as an helper for users that play \"live\" games of the (italian) checkers (i.e., by means of physical board and pieces). the app, that runs on android, can help a player at any time: by means of the device camera a picture of the board is taken, and the information about the current status of the game is properly inferred thanks to opencv 9, an open source computer vision and machine learning software; an asp-based artificial intelligence module then suggests the move."
"when users want to launch an fmone pipeline, they have to specify the monitoring workflow throughout the fog infrastructure. first, they choose a set of regions that the pipeline is going to take care of. afterwards, they have to specify for each of the regions the and ℎ intervals together with the inplugin, midplugin, and outplugin that they want to use for the fmone monitors. moreover, additional parameters can be passed to the plugins if it is needed. for example, if we want to push metrics to a kafka queue we need to give as a parameter the name of the kafka broker and the topic we want to publish to. this process enables the extraction of metrics at different levels of the system, their processing, and their storage in the chosen backends in a flexible way. lastly, the user can optionally provide one or more back-end types (e.g., mongodb, cassandra, and influxdb) that will be created as part of the pipeline."
"among the single-finger techniques, participants preferred m over h, since it was easier for precise positioning due to the lower acceleration multiplier, although they described it as somewhat uncomfortable. for both h and m, all users verbally expressed frustration when using them, especially for h. small target acquisition was challenging due to high cd gains, and participants said that precise control required significant careful effort. however, the worst technique according to participants was l, because of the large amount of clutching required for long distance target acquisition. with l, all participants said they had no difficulty acquiring targets, but the numerous clutching for long distance pointer movements caused them fatigue, although, for short distances, they preferred l due to its low cd gain which makes it easy to select small targets."
"the application features a \"parent area\", that allows parents to monitor child's achievements and to express some preferences, such as desired express directions in order to grant/forbid access to some games or educational areas."
"rules r1 − r4 guess the number for each cell, ensuring that each cell is filled exactly one number (symbol ); note that the guessed values for the positions complete the extension of the predicate cell for which some values have been already provided in f . rules r5 and r6 check that a number does not occur more than once in the same row or column, respectively; rules r7 and r8, finally, ensure that two different cells in the same block don't have the same number."
"on the questionnaire, participants were also asked to rate the difficulty of each technique on a scale from 1 to 7, with higher number indicating greater difficulty. while the results of the questionnaire resonate with the results of the informal interviews, the numerical results are not statistically significant ( figure 13 ). it is interesting that participants rated l and h similarly and rated them both as more difficult than m and lmh. this indicates that users find it difficult to use techniques that either are inaccurate (h) or require too much clutching (l). they preferred the more balanced techniques."
"to evaluate the accuracy of reported unused css selectors (rq1), we measure precision, recall, and f-measure as follows: precision is the rate of detected unused selectors found by the tool that are correct:"
"as a use case, we will refer to an application for solving sudoku puzzles. we will report the code related to the emb-asp usage; the complete code is available online [cit] . notably, thanks to the annotation-guided mapping, the aspbased aspects can be separated from the java coding: the programmer does not even necessarily need to be aware of asp."
"this module includes specific classes for the management of input and output to asp solvers. in particular, aspinput-program extends inputprogram to the asp case. in addition, since the \"result\" of an asp solver execution consists of answer sets, the output class has been extended by the answersets class that is composed by a set of answerset objects."
"in this paper, we provide new insights into fog monitoring, presenting an innovative, flexible monitoring solution, named fmone, to address fog requirements. fmone is a lightweight, installation-free, and user-adjustable monitoring tool, specially designed to monitor a fog system. it relies on a container orchestration system to build monitoring pipelines that adapt to the distinct features of a fog infrastructure. our contributions are as follows:"
"infrastructure. telecommunication companies, also called telcos, are one of the key actors in the fog computing paradigm [cit] . the fog allows them to take advantage of their geographically distributed infrastructure, made of small smart devices like gateways, smart phones, or any other kind of device with storage and computing power. this infrastructure can be used as the bare metal that hosts services in closely delimited regions, such as cities or districts, placing them closer to where they will actually be used."
"to assess the effectiveness and real-world relevance of our approach, we have conducted a case study following guidelines from runeson and höst [cit] . our evaluation addresses the following research questions: rq1 what is the overall accuracy of cilla in detecting unused css code? rq2 how does cilla's detection rate compare to existing approaches? rq3 what percentage of css code is typically unused in online web-based systems?"
"a diagram of the setup is depicted in figure 6 for a better comprehension. note that this is a simulated scenario and there are conditions not included, like routing the traffic through gateways, which we plan to evaluate in future work. hardware heterogeneity also comes to mind, but it is important to note that fmone can allocate its agents in any device with a docker daemon, which has been proved to work even in small single-board devices [cit] ."
"resilience. lastly, we test the resilience of all the fmone parts. the following three scenarios are laid out where we introduce a series of failures that can happen in an unstable scenario such as fog: (ii) we then shut down the mongodb instance. since the agents do not have a back-end to publish to, they will also exit with an error code. we measure the time interval for the whole pipeline to go up again."
"(iii) finally we shut down all the containers of the pipeline and the node containing the marathon engine. marathon is the foundation in which the fmone operation is based on. however, it is also resilient by means of a zookeeper instance [cit] and is able to relaunch itself again by choosing a new leader. the pipeline is automatically restored afterwards. again, this time interval is measured."
"we implemented two libraries derived from embasp, allowing the embedding of asp reasoning modules handled by dlv and clingo, from within android and desktop applications, respectively."
iv. our approach our overall approach is based on dynamic analysis in which we automatically drive a given web application and infer the runtime relationship between the css rules and dom elements of the navigated states. we use this inferred relational knowledge to spot unused css selectors.
"threats to validity. to minimize selection bias, seven of our fifteen experimental objects were chosen randomly. we acknowledge the fact that more studies are required to draw more general conclusions. however, we believe the systems are representative of css code present in deployed web applications."
"for measuring the rate of false positives and false negatives, we randomly select a sample of the entire rule set from the css code and check them manually against the output. manual checking of css rules is a time intensive task done by the authors of the paper. therefore, we acknowledge that it could be error-prone and biased towards our judgment, although we made every effort to mitigate these threats. the main reason behind selecting samples instead of the entire sets has been constraining the effort and time needed to manually form a baseline, since there are thousands of selectors and declaration properties along with hundreds of dom states in the systems, as shown in table ii . increasing the sample size or selecting other samples could change the assessment, positively or negatively. as a remedy for this threat, we created a script that randomly selects the samples. with respect to reliability of our results, cilla and all the fifteen web-based systems are publicly available, making the case study replicable. one threat with using online web applications in empirical studies is that they might change over time, making exact replications challenging. that is why we have also included two open source systems in the study."
"our technique operates in three steps to detect unused selectors and undefined class values. in the first step, we execute each selector against all dom states to differentiate unmatched selectors (us). in the second step, the matched selectors (as us) and dom elements are analyzed to detect the set of ineffective selectors (is). the output of our technique as far as the total unused selectors is concerned is the set us [ is. there is also the set of class values (uc) present on dom elements that are not defined in css code (not shown on the diagram). in the last step, we use the matched selectors to spot these undefined class values. each of these steps is further described in the following subsections."
"we simulate a fog scenario with virtual machines hosted in the grid5000 testbed [cit] to evaluate fmone at large scale. grid5000 is a testbed that provides access to a large amount of computing resources. it is highly customizable and offers a wide range of tools for reproducible experiments. in fact the repositories with the scripted experiments are publicly available (https://github.com/brandonage/execo-utilities-g5k, https:// github.com/brandonage/execo-g5k-benchmarks). we use vagrant together with the vagrant-g5k (https://github.com/ msimonin/vagrant-g5k) plugin specific to this testbed to provision the vms. grid5000 has nine sites available with several machines each. we launched 78 vms in the rennes site, with 4 cores and 10 gb of ram and we configured the testbed infrastructure as follows."
"(iii) between regions the communication bandwidth is restricted to 4 mbps and to emulate communication latency, we introduce a delay of 50 ms in all sent packets through the traffic control utility (https://linux.die.net/man/8/tc [cit] ). bandwidth was chosen based on the statistics of speed testing in the european union (http://testmy .net/rank/countrycode.up/ [cit] ) and taking into account the fact that many of the fog devices use technologies with variable speeds such as 4g/3g."
"control-display (cd) gain specifies the multiplier between the speed with which the user moves the control device and the resulting speed of the pointer on the display. [cit] identify the usable range of fixed cd gain values, depending on targeting characteristics. gain must be large enough to enable movement across the entire screen without clutching yet small enough to enable precise pointing to small targets. unfortunately, for large high-resolution displays the usable range is empty, causing fixed cd gain to perform poorly on large displays [cit], thus necessitating variable cd gain techniques such as pointer acceleration."
"our work can be enhanced and extended in several ways. improving the algorithms and implementation to further reduce the number of false positives and conducting larger case studies to obtain more empirical data form part of our future work. other directions we will pursue are exploration of suitable techniques for analyzing pseudo classes and elements. further, we will investigate how the analysis results can be used for detecting antipatterns and 'smells' in css code and suggesting refactoring steps for improving code quality and maintainability."
"one of the fundamental w3c standards for developing web applications is cascading style sheets (css) [cit] . css is a language for defining the presentation semantics of html elements, including their positioning, layout, colour, and fonts. the main driving force behind adopting css has been the separation of structure from presentation. although this separation of concerns helps a web application's evolution as far as the structure and content is concerned, the css code itself is not easily maintainable [cit] ."
"in this section we describe some asp-based applications developed by means of embasp for educational purposes, and, in particular, in the context of a university course that covers asp topics; it is worth noting that such applications have been developed by some of the course attendants, i.e., undergraduate students. the educational aspect here is two-folded. the most relevant is the engagement of university (under)graduate students in asp capabilities, in order to make them able to take advantage from it when solving problem and designing solutions, in the broadest sense. furthermore, asp looks well-fitted for the use in the development of educational/training software, as, for instance, the dlvedu app introduced below; a deeper study of such aspects, however, is out of the scope of the present work."
"more in detail, the app constantly detects the current user activity (running, walking, cycling, etc.) and (at a customizable frequency) stores some information (activity type, timestamps, calories burned up to the present time, etc.). activity detection is performed by means of the google activity recognition apis [cit], a de-facto standard on android, thus relying on these for the accuracy of detection. as already mentioned, the user might ask, at any time, for a suggestion about a plan for the rest of the day; the reasoning module hence prepares a (set of) proper workout plans complying with the very personal goals and preferences previously expressed."
"(iv) fmone agent: this is the docker container responsible for monitoring each of the devices. we divide a monitoring job into three fundamental tasks: collect, filter, and publish metrics. their responsibilities are divided into three plugins:"
"suppose that the nodes and the arcs are represented by a set f of facts with predicates node (unary) and arc (binary), respectively. then, the following asp program allows us to determine the admissible ways of coloring the given graph."
"the user has to register all its annotated classes to the aspmapper, although classes involved in input translation are automatically detected. if the classes intended for the translation are not annotated or not correctly annotated, an exception is raised. other problems might occur if once that the solver output is returned, the user asks for a translation into objects of not annotated classes: in this case a warning is raised and the request is ignored."
"(i) we describe two different fog monitoring use cases inspired by the literature (section 2) from where we can draw a set of requirements that a fog monitoring tool should have. (ii) these requirements are listed and explained in order to provide a clear mapping with fmone design choices (section 3). (iii) we present the design and architecture of fmone (section 4) based on these requirements. we evaluate it in a simulated fog environment, verifying that it is able to meet the above-mentioned requirements (section 5). (iv) we review the state of the art in monitoring solutions (section 6) and provide a comparison with fmone in terms of functionality."
in the following we show the use of the specialized java libraries generated via embasp for developing android applications; we report some considerations about programming languages different from java at the end of the section.
"(iii) flexible back-ends: users should be able to choose a back-end for their metrics depending on several factors, like the number of elements to be monitored or the machine in which this back-end is going to be installed. it is evident that if a large amount of metrics needs to be stored, more scalable technologies like kafka should be used for the ingestion. on the other hand, if we are going to store fewer metrics on a low resource device, we could use lightweight databases like sqlite, which has already been used in some iot prototypes [cit] . in any case and since a fog environment is very diverse, this should be flexible and not limited to one technology. it is not mandatory for a pipeline to have a back-end, since an fmone agent can always take advantage of an existing one."
"traffic and fulfills requirement r8, for example, publish cpu load values that are above a certain threshold. a more complex logic can be used, from simple aggregates, such as averaging metrics for a time window, to a machine learning classifier that decides whether or not a metric should be published. (c) outplugin: it dumps all the filtered data by the midplugin to a back-end every ℎ seconds. this back-end can be any kind of storage as long as it is supported by an outplugin implementation. a file, a document database such as mongodb, and a key-value store like cassandra are some examples of back-ends. another target can be a message queue, from which another fmone agent can pull, aggregate/filter the metrics, and push them to a different location. this is part of the requirement r3."
"techniques that allow continuous pointing without switching from one type of interaction to another can be dubbed as single mode techniques. single mode techniques maintain the same mental perception of the interaction technique even when the cd gain is changed. these techniques include ray-casting [cit], touchpads [cit], mice [6, [cit], pointing sticks [cit], head and body tracking [cit] as well as hand tilting [cit] ."
"with multimode techniques for continuous pointing, users switch between modes of interaction to handle different targeting conditions. different modalities are separately exploited to independently support either fast or accurate pointing."
"(ii) two fmone agents are deployed in each node. one is going to collect 30 different types of metrics from the disk, memory, network, and cpu usage at the host level. the second one will collect 24 different metrics about the resource usage of each of the five containers running in that node. (iv) we measure the performance of the ycsb clients with the following fmone pipelines:"
"this paper makes three contributions. first, we identify a new class of interaction techniques for lhrds that combine implicit and explicit pointer acceleration controls within the same mode of interaction. second, we propose a new interaction technique design within this new class, called multifinger pointing, that allows users to dynamically choose from up to five acceleration curves while manipulating the pointer by simply changing the number of fingers in contact with the multitouch touchpad. our implementation, lmh, supports switching between three curve multipliers. third, we find through an empirical study that the combination of implicit and explicit controls in the form of multifinger lmh improved overall performance when compared to just implicit control (l, m, or h), enabling both fast long distance movement and accurate small target refinement. participants found it easy to use and preferred it. participants appropriately used the ability to explicitly switch pointer acceleration curves, depending on targeting conditions, to efficiently perform multiscale tasks on a large high-resolution display. we hope this work spurs on new designs for singlemode interaction techniques that cleanly integrate multicurve pointer acceleration into existing interfaces for large high-resolution display systems."
"definition 1 (matched selector) let ⌃ denote the set of dom states, then a css selector s is said to be matched if and only if there exists at least one dom element e 2 ⌃ so that it is selectable by s."
"for each consecutive new dom state, cilla first examines all the css sources defined in that particular state. any new css code that is identified is parsed and added to the map before the analysis is carried out for that particular state."
"moreover, this module also contains classes extending op-tiondescriptor to implement specific options of the solver at hand. for instance, the class dlvfilter is a utility class representing the filter option of dlv."
"we conduct a study to explore how users leverage this ability to dynamically switch acceleration curves as they manipulate the pointer and to compare the performance tradeoffs of multicurves against the standard single-curve technique. in the study, we test a 3-finger implementation so as to examine significant scalability while also maintaining a tractable study design. one finger has the lowest acceleration curve multiplier (l) which offers optimal cd gain range for small regions and targets; two fingers have a greater multiplier (m) that offers cd gain range for medium distances and targets; finally, three fingers produce the highest (h) multiplier and cd gain range that is aimed at coarse long distance movements. for convenience we refer to this specific implementation as lmh."
"for what the asynchronous mode is concerned, the class service depends from the interface callback, since once the reasoning service has terminated, the result of the computation is returned back via a class callback."
"there is a wide range of customization possibilities in this setting: thanks to the modeling capabilities and the declarative nature of asp, adding new features to dlvfit, such as new exercises or new kind of preferences, is straightforward, and sums up to adding a few lines to the logic program. it is also worth noting that the asp program is dynamically built, thus providing the developer (and, in turn, the final user) with great customization and flexibility capabilities. indeed, we plan to actually take advantage from this in the future versions of the prototype, contemplating a higher number of rules and sub-programs to be dynamically fed to dlv."
"(vii) plugin-based monitoring (r7). different technologies run together in this kind of infrastructure and we might need to monitor them at different levels. for instance, we might want to monitor a mongo database, a docker container and a kafka queue which are part of the chat system of the abovementioned online gaming paas, taking into account the fact that each one will need different kinds of metrics. the deployment agent should have plugins that enable the extraction of these diverse metrics by specifying the types of component that need to be monitored."
"our main design goal is to enable users to dynamically alter pointer acceleration curve properties in a fluid way, minimizing the physical and cognitive barriers associated with manual switching. our aim is to exploit the usability of implicit pointer acceleration curves but scale up the usable range of cd gains by also enabling explicit modification of the acceleration curve. the interaction mechanisms for these should be cleanly integrated, and so we explore the use of a single mode of interaction. we also seek to explore if this can be designed in a way that easily fits into existing technologies and usage."
"answer set programming (asp) [1, 2, 9, 10, 18, 23, 24 ] is a purely declarative formalism for knowledge representation and reasoning developed in the field of logic programming and nonmonotonic reasoning. the language of asp is based on rules, allowing (in general) for both disjunction in rule heads and nonmonotonic negation in the body."
"the logic program used takes as \"input\" (i.e., a set of facts as instances of proper predicates): optimize(o, w, p) the specific optimization operation(s) that the user wants to perform; each direction is assigned a weight (w ) and a preference order (p )."
"however, it seems unlikely that there exists a single acceleration curve that performs well over the diverse range of tasks on large displays. casiez and roussel [cit] investigated various pointer acceleration curves used in common operating systems. they found that the default curve in osx improved performance for small targets but reduced performance for large targets when compared to the curves used in windows and xorg. similarly, [cit] studied the utility of an optimized pointer acceleration curve for large high-resolution displays and concluded that such an acceleration curve performed poorly with targets of size less than 7 mm. large displays require a large range of cd gains, thus requiring a sharply exaggerated acceleration curve that is overly sensitive to minor speed changes and difficult for users to control [cit] ."
"the results of our empirical evaluation, on several opensource and industrial web applications, clearly point to the ubiquity of the problem: on average we found 60% unused selectors and 52% unused properties. our results also demonstrate the efficacy of the approach in automatically detecting unused css code (100% recall and 80-100% precision)."
"the popular turn-based connect four game is played on a vertical 7*6 rectangular board, where two opponents drop their disks with the aim of creating a line of four, either horizontally, vertically, or diagonally."
"dlvedu is an educational android app for children, that integrates well-established mobile technologies, such as voice or drawn text recognition, with the modeling capabilities of asp. in particular, it is able to guide the child throughout the learning tasks, by proposing a series of educational games, and developing a personalized educational path. the games are divided into four macro-areas: logic, numeric-mathematical, memory, and verbal language. the usage of asp allows the application to adapt to the game experiences fulfilled by the user, her formative gap, and the obtained improvements."
"for each selectable dom element, the algorithm starts by retrieving all the corresponding css selectors that matched that particular element (line 4). then it sorts the list of matched selectors (line 5) according to their calculated specificity weight in decreasing order."
"once all the properties are annotated, we can retrieve the set of ineffective (or effective) selectors. the ineffec-tiveselectors procedure iterates over all the annotated rules and selectors (lines 20-21). if none of the properties of a selector is marked as effective (lines [cit], then that selector is added to the set of ineffective selectors (line 28)."
"core module. the core module defines the basic components of the framework . two different execution modes can be made available: synchronous or asynchronous. while in the synchronous mode any call to the execution of the asp solver is blocking (i.e., the caller waits until the reasoning task is completed), in asynchronous mode the call is non-blocking: a callback component notifies the caller once the reasoning task is completed. the result of the execution (i.e., the output of the asp system) is handled by the output component, in both modes."
"what we witness in the evaluation is that some unused rules are a result of reusing the entire css rule set of a parent website, while perhaps only a small subset is needed. others are a result of copy-paste from other resources. cilla can tell web developers exactly what the used subset is so that the remaining obsolete rules can be avoided. in addition, our technique can assist in understanding the relationship between css rules and dom elements at runtime. for instance, cilla provides all dom elements affected by each selector, as well as all selectors affecting each element. this relational information can be valuable during program comprehension and refactoring tasks. another interesting area where cilla could support web developers in is crossbrowser compatibility [cit] by providing an overview of how dom elements are linked to css rules in different browsers."
"in this section, we present two motivating scenarios by broadening the ideas presented in the literature [cit] : a telecommunication company that needs to keep track of the usage of all their devices at the edge and a provider of a platform as a service (paas) for multiplayer games that needs to monitor the different elements of the platform. these scenarios will illustrate the challenges that arise when the monitoring workflow has to be changed from a cloud environment (centralized, homogeneous, and stable) to a fog environment (decentralized, heterogeneous, and unstable)."
"after more than twenty years of research, the theoretical properties of asp are well understood and the solving technology, as evidenced by the availability of a number of robust and efficient systems [cit], is mature for practical applications: asp has been increasingly employed in many different domains, and also used for the development of industrial-level and enterprise applications [cit] . notably, this is spreading asp teaching in universities worldwide, and, interestingly, is moving the focus from a strict theoretical scope to more practical aspects. moreover, it makes clear the need for proper tools and interoperability mechanisms that ease the development of asp-based applications, in both educational and real-world contexts."
"what implicit versus explicit switching behaviors do users exhibit during pointing tasks of varying scale characteristics? the target acquisition patterns show that users consistently utilized explicit curve switching in lmh in a way that is similar to the designed optimal usage pattern, beginning with two and three finger swiping for coarse pointing, followed by single finger precise refinement. only one user did not consistently use all three curves."
"(vi) geo-aware (r6). a fog platform has an inherent geodistributed structure. nodes are divided into regions depending on their location, which leads to better connectivity between them through local connections and fewer hops across the network. this creates the need of deploying a set of monitoring agents and their back-ends on the same region, in order to leverage these local connections and avoid unnecessary latency in the monitoring workflow. it can also be used to get more detailed data of problematic regions where we need to zoom into a specific technology that is only found on that region. an example of this workflow would be to deploy specialized monitoring agents with a specific plugin in one region hosting a particular set of technologies (e.g., an agent that monitors the number of messages ingested by a kafka queue in one region)."
"there is also the possibility of creating one backend or more for that pipeline. these back-ends are also implemented as docker containers and the most popular database vendors already provide them. the users can choose the one that best fits their use cases, satisfying requirement r3."
"in this paper, we propose a technique that automatically (1) checks css code against different dom states and their elements to infer an understanding of the runtime relationship between the two; (2) detects unmatched and ineffective rules, overridden declaration properties, and undefined class values. we have implemented the technique in an open source tool called cilla. the results of our evaluation show that cilla has a high precision and recall rate of retrieving unused css code. our empirical study conducted on fifteen web-based systems indicates that, on average, 60% of css selectors is unused in deployed web applications."
"these issues introduce research questions about the inherent tradeoffs between the use of single or multiple pointer acceleration curves on large displays and how to effectively combine the advantages of an efficient implicit acceleration curve with the power of explicit curve switching. multiple curves can potentially enable a broader range of multiscale targeting conditions on large displays. however, these benefits can only be realized with an efficient mechanism for curve switching."
"similarly, clingodesktopservice is a specific version tailored for the clingo reasoner on the desktop platform, extending the desktopservice with proper functions needed to invoke clingo. in this case, different versions of the solver for several desktop oses were already available online [cit] ."
"our proposed technique design, called multifinger pointing, is to dynamically alter acceleration curve properties by 6 advances in human-computer interaction simply varying the number of fingers contacting the surface of the touchpad while manipulating the pointer. specifically, the number of fingers can be used to control a multiplier applied to the pointer acceleration curve during pointer manipulation."
"this technique, called multifinger pointing, can allow users to dynamically select up to five different pointer acceleration curves by using one to five fingers to move the pointer. the number of fingers adjusts a multiplier applied to a standard acceleration curve. thus, users can easily shift the range of cd gains that the acceleration curve offers. with up to five curves, this technique scales well beyond simple singleor dual-curve techniques, supporting very large displays and diverse targeting tasks. however, it may be impractical to scale beyond three fingers and curves."
"moreover, our framework is not specifically bound to a single or specific solver; rather, it can be easily extended for dealing with different solvers; in addition, it allows to build applications that can run different solvers, and different instances, at the same time; none of the mentioned systems exposes this feature. finally, to our knowledge, the specialization of embasp for dlv on android has been the first actual attempt to port asp solvers to mobile systems reported in literature; indeed, the preliminary version of embasp was originally explicitly tailored to the mobile scenario [cit] ."
"so far, traditional cloud architectures have been used to provide the infrastructure for these previously mentioned applications. however, with this approach all the data generated has to be moved from the edge of the network to a data center [cit] . this can create an important bottleneck in the network, specially if this data center is located in a distant region. in addition, cloud providers have their computing assets spread throughout few specific locations [cit], making it difficult to host the applications close to the edge. finally, the response time is expected to be high and unpredictable, as the round time trip of the communication impedes real-time responses."
"each component in the core module has been implemented by means of an homonymous class or interface. in particular, the handler class collects inputprogram and optiondescriptor objects communicated by the user."
"other useful future work would include examining how the multifinger pointing technique would scale up to even higher resolution displays and additional acceleration curves. it would also be interesting to conduct longer-term studies in more complex tasks (e.g., [cit] ) to go beyond any effects of novelty and reveal more prominent performance insights into how users utilize implicit and explicit pointer granularity controls."
"in the following, we briefly introduce the use of asp as a tool for knowledge representation and reasoning, and show how its fully declarative nature allows to encode a large variety of problems via simple and elegant logic programs."
"as we have seen, css provides different selector types to target dom elements and each of these types carries a different weight of importance. the specificity weight of a selector is the sum of all its selector type's weights. the more specific a selector points to a dom element, the higher its specificity weight."
"by means of the annotation-guided mapping, the initial schema can be expressed in forms of java objects. to this extent, we define the class cell, aimed at representing a single cell of the sudoku schema, as follows: it is worth noticing how the class has been annotated by two custom annotations, as introduced above. thanks to these annotations the aspmapper will be able to map cell objects into strings properly recognizable from the asp solver as logic facts of the form cell(row, column, v alue)."
"we close the paper with a set of conclusions on the challenges of fog computing and monitoring, together with future research work in this field (section 7)."
"in terms of jellinek's gearbox metaphor, our lmh technique would be analogous to a drivetrain with continuously variable transmission (cvt) and a differential with dynamically selectable ratios. it combines speed-based cd gain change (pointer acceleration) with the ability to instantaneously amplify the acceleration multipliers by varying the number of fingers touching the surface of the touchpad. metaphorically speaking, the cvt is analogous to the pointer acceleration curve, while the differential ratio is analogous to the acceleration multiplier determined by the number of fingers. thus, lmh users potentially benefit from both implicit (acceleration curve) and explicit (switching between different curves) granularity control as in figure 3 ."
"an asp program p is a finite set of safe rules. an atom, a literal, a rule, or a program is ground if no variables appear in it. accordingly with the database terminology, a predicate occurring only in facts is referred to as an edb predicate, all others as idb predicates; the set of facts of p is denoted by edb(p)."
"tweety is an open source framework for experimenting on logical aspects of artificial intelligence; it consists of a set of java libraries that allow to make use of several knowledge representation systems supporting different logic formalisms, ranging from classical logics, over logic programming and computational models for argumentation, to probabilistic modelling approaches, including asp. tweety and embasp cover a wide range of applications, and the use is very similar: at the bottom line, both provide libraries to incorporate proper calls to external declarative systems from within \"traditional\" applications. currently, tweety implementation is already very rich, covering a wide range of kr formalisms, yet looking less general, as the more abstract level is conceived as a coherent structure of java libraries; also, it currently misses the mobile focus. embasp is mainly focused on fostering the use of asp in the widest range of contexts, as evidenced by the specialization for the mobile setting; nevertheless, the framework core is very abstract, and has been conceived in order to create libraries for different programming languages, platforms and formalisms."
"we repeat the experiment 10 times for each scenario. the mean time and its standard deviation are shown in table 2 . we can see how an agent can recover quickly and lose only about 7 seconds of monitoring information. there is more overhead for a failure in a back-end, since these containers have more complex deployment times than the lightweight fmone agent. additionally, a failure in a back-end also affects all the other agents, as previously explained. nonetheless, the maximum delay we observed was 58 seconds with a mean of 27 seconds for the 10 experiments. the most costly situation to recover is when marathon fails. in this case the system has to elect a new marathon instance and relaunch the fmone pipeline. the system recovered in 96 seconds on average. remember that this is an extreme situation, since the marathon instance does not have to be running in any of the fog nodes which are unstable and constrained devices. the idea is to have all the core components like the mesos master, marathon, and the fmone framework in a stable location from where the whole fog infrastructure can be coordinated."
"(ii) pipeline: it represents a workflow of fmone agents that can optionally communicate with each other and/or dump their metrics to one or more backends. users can define their own pipelines. for the purpose of this paper, we have defined three pipelines, as we will see in the experiments section (section 5). nonetheless, an interactive layer such as a dashboard or a console could be added to fmone, allowing the user to examine the different regions, the devices assigned to them and facilitating the definition of new pipelines. once the pipeline has been defined, it will be translated by fmone into a marathon application group (https://mesosphere.github.io/marathon/docs/ application-groups.html), a json file that establishes an order for the containers to be deployed. figure 3 includes a description of the conceptual parts that constitute the pipeline concept. each pipeline takes care of one or more regions of the fog, as the geodistribution of its devices is one of its distinctive characteristics. in this way users can build different pipelines with different configurations for its monitoring agents, depending on the physical location of the devices. it also facilitates placing them in problematic regions or near their back-ends. a pipeline can have one or more back-ends, which are hosted in one of the regions. monitoring agents in a pipeline can store their metrics in one of these back-ends, which can be an existing one (previously launched in another pipeline) or a new one, which is created. in each of the regions covered by the pipeline, one or more fmone agents are launched. for the fmone agents a set of inplugin, midplugin, and outplugin has to be chosen. details about the plugins will be provided later in this same section. figure 4 shows an example of a pipeline. as explained in figure 3, a pipeline takes care of monitoring several regions, in this case regions a, b, and c. each region has a different number of fmone agents, whose plugins can be chosen by the user. in region a all the metrics are dumped into an existing back-end 2 in region d. region b has 4 agents, three of which perform monitoring tasks and another one aggregates the metrics extracted by the other three. the aggregation is stored in back-end 2. the same pattern is used in region c, but this time we store the full detail of the metrics in back-end 1, hosted in the same region, while another agent aggregates the metrics and store them in back-end 2. this means that metrics can be stored in several back-ends at different granularities. all these data can be queried from the intelligence in the central cloud or by actors at the edge. as previously mentioned, these pipelines enable to change the monitoring workflow depending on the user needs, aggregating metrics, and storing them at different levels (e.g., gateways or iot devices with storage), fulfilling requirement r2."
"scalability and performance. the results in table ii show that our approach is scalable to deployed industrial web applications consisting of tens of thousands of css loc and hundreds of dom states. on average, it took cilla 22 minutes in total (including state exploration and css analysis) to analyze 7k lines of css code and 100 dom states. the results indicate that both scalability and performance of cilla are acceptable."
"as a sample implementation of this approach, we design an interaction mechanism that seeks to minimize the effort required for explicit switching among curves. to minimize cognitive and physical effort, implicit pointer manipulation and explicit multicurve selection are incorporated cleanly into a single mode of interaction. specifically, the design leverages the multitouch capabilities of modern touchpad devices to enable selection of different pointer acceleration curves by simply altering the number of fingers in contact with the capacitive surface of the touchpad while manipulating the pointer (as depicted in figure 1 )."
"the examples below have been implemented adhering to the \"guess&check\" (gc) paradigm [cit], one of the most common asp programming methodology. in summary, a gc program features 2 modules:"
"multiple studies show [3, 5, 6 ] that automatic dynamic cd gain adjustment (i.e., pointer acceleration) improves pointing performance by dynamically adjusting cd gain depending on the physical movement speed. as a result, lower speeds decrease cd gain for precision, while higher speeds increase the gain for faster pointer movements. this provides users with implicit granularity control for relative pointing devices and defines a pointer acceleration curve that relates control speed to display speed (see figure 1, e.g., pointer acceleration curves)."
"(i) fmone: this is the general framework that coordinates the monitoring process across the whole fog infrastructure. (a) high availability: marathon achieves high availability through zookeeper [cit] . this means that there are several marathon instances available and, in case that the current leading instance fails, a new leader will be elected, achieving 100% uptime. additionally, it takes care of relaunching any docker container that fails or relocating it if its current machine fails. this enables requirement r5. (b) constraints: it allows us to specify a criteria to control in which nodes the fmone containers should be executed. an example would be to assign a label to the fog nodes depending on the region they belong to and then constraining the deployment of a set of containers to a specific region. as long as the different resources in the fog infrastructure are correctly labeled, it will allow us to have a geo-aware control of the fog infrastructure, by using fmone to launch monitoring agents in specific regions only. this fulfills requirement r6. (c) adding nodes: through the above-mentioned constraints, we can also define that in each host an fmone agent container must be active. as soon as a node joins the system, the framework will start monitoring it without the operator intervention. therefore, we can meet requirement r4. remember that constraints are part of the marathon api (https://mesosphere.github.io/marathon/docs/ constraints.html [cit] ) and that fmone uses them to achieve these features."
"cloud is a well-known, established technology with a clear business model behind it [cit] . cloud architectures support general-purpose services at different levels, relying on centralized data centers and infrastructures. fog is expected to provide different services and applications at the edge of the network, reducing the latency between the client and the application to support a specific commercial activity, by means of a decentralized infrastructure."
"in summary, fog iaas needs to be monitored at the same level as its cloud counterpart. however, the particular characteristics of the devices in this infrastructure make it necessary to incorporate new functionalities to fog monitoring tools. these functionalities are not present in previous centralized and more homogeneous environments."
"dlvfit is a health app that aims at suggesting the owner of a mobile device the \"best\" way to achieve some fitness goals. the app lets the user express her own goals and preferences in a very customizable ways along many combinable dimensions: calories to burn, time to spend, differentiation over several physical activities, time constraints, etc. then, it monitors her actual activity throughout the day and, upon request, it computes one or more plans meant, if accomplished, to make her meet the aforementioned goals the way she would have preferred."
"let us think of a user that designed (or has been given) a proper logic program p to solve a sudoku puzzle and has also an initial schema. we assume that the initial schema is well-formed i.e. the complete schema solution exists and is unique. for instance, p can correspond to the logic program presented in section 2.3, so that, coupled with a set of facts f representing the given initial schema, allows to obtain the only admissible solution (i.e., a single answer set). it is worth remembering that, in case of less usual sudoku schemata featuring multiple solutions, the asp program features multiple answer sets, one-to-one corresponding to such solutions."
"(1) centralized: we sent all the metrics to the kafka queue in the central region. this could be considered as the baseline and the paradigm followed by most monitoring solutions, where everything is stored in a single back-end. note that the nodes that host the cassandra cluster used for the evaluation of ycsb and the ones that host the kafka cluster are different. otherwise we will create an obvious overhead in the database operations. (2) aggregated: we have an fmone agent in each region that is going to aggregate each type of metric for all the hosts of the same region, before sending them to the central location. (3) regionaldb: all the agents of one region are going to store their metrics in a mongodb backend, which resides in that same region."
"(v) in addition, to compare the fmone pipeline workflow with existing centralized solutions, we measure the performance of a prometheus [cit] we expect a drop in ycsb client performance for the centralized fmone pipeline and the prometheus scenario, since the limited bandwidth available to the device when communicating with the central region will have to be shared between the ycsb operations and the metrics sent by the monitors. we execute all of the workloads that are available by default. the characteristics of the different workloads are depicted in table 1 . note that since there are connection constraints between the ycsb clients and the central region where the cassandra database resides, we expect a low number of operations per second. the clients execute each workload until they complete 1000 operations and this is repeated three times. the results can be seen in figure 7 . the operations per second throughput are averaged for the 65 nodes that host the ycbs clients and the three executions. notice how workload e has a low number of ops/sec since read short range is an expensive operation. as expected, there is an improvement in the performance on all workloads with respect to the centralized prometheus approach, reaching as much as 8% for some cases. it is important to note that these are the results for an emulated scenario in which we use the linux traffic control utility, while, in a real scenario, this effect would be further exacerbated by the metrics generated by thousands of nodes and the additional stress on the backbone of the network [cit] . the aggregated and regionaldb approaches have similar performance as the traffic going out of each region is reduced to the minimum, thanks to the aggregation and flexible back-ends features of fmone."
"the venn diagram of figure 3 depicts the total landscape of css selector coverage and what our approach targets. set as represents all the css selectors present in a web application. us is the set of unmatched selectors, i.e., selectors with no dom element counterparts. the set as us contains all the matched selectors, from which the set is encompasses all the ineffective ones, i.e., the selectors that are matched but have no effect on the dom elements. me is the set of all matched and effective selectors. ideally, after each development and maintenance cycle, the css code should only contain the set of selectors in me."
"usage. the monitoring process should be lightweight and nonintrusive, specially in resourcepoor iot devices. the aim of this experiment is to prove the requirement of nonintrusiveness of fmone. we monitor the performance of the ycsb clients in each region in two situations: without any monitoring at all and with the previously introduced fmone regionaldb pipeline, where the metrics from the host and from 15 additional containers per host are dumped to a mongodb in their same region. again, metrics are gathered and published every second and the ycsb clients perform requests until they complete 1000 operations. the results are depicted in figure 8, averaged for the 65 nodes that host the ycbs clients. the overhead is minimal with a maximum drop of 1.2%. the fact that the agent can keep the metrics in memory before publishing them enables us to transfer the data in bulk, using resources in a more effective way. this, together with the fact that the fmone agent is a lightweight container, means low impact on performance. we also show in figure 9 the resource usage of the nodes hosting the different back-ends for the metrics. we mentioned that a fog environment has heterogeneous hardware. one of our requirements was deploying different back-ends depending on the computational power of the device that is going to host it. in this experiment we use different back-ends and show their different resource usage profiles. the objective is to motivate why back-ends should be chosen depending on the computational capabilities of workflow. light back-ends can be used for fog devices with limited resources and more powerful and complex ones for upper layers near the cloud."
"during the implementation and testing phases of cilla, we used two web applications with unused rule sets that were well-known to us. however, in the evaluation phase, our goal is to assess the effectiveness of the approach on real-world web applications. our study includes fifteen web-based systems in total. two are open source, namely, phormer, which is a photo gallery written in php, and igloo, a simple company website taken from the book 'pro css for high traffic websites' [cit] . seven of the systems are randomly selected using the random link generator provided by yahoo! [cit], which has also been used in other studies [cit] . the randomly selected web systems include becker, equus, pte, vanities, lcn, employeesolutions, and sync. [cit] table i shows each system's id, name, and resource. the characteristics of these systems in terms of their size and complexity is shown in table ii ."
"in our first experiment, we want to evaluate the impact on performance of an fmone pipeline compared to a centralized approach. we configure the following scenario to do this:"
"(ii) augment with explicit switching among multiple (several) pointer acceleration curves to enable scalability. (iii) apply a single mode of interaction for both of the above, so as to enable the most rapid and fluid switching mechanism possible."
"[3-col] as a first example, let us consider the wellknown problem of 3-colorability, which consists of the assignment of three colors to the nodes of a graph in such a way that adjacent nodes always have different colors. this problem is known to be np-complete."
"in the following, we first briefly introduce three applications; then, in order to further clarify the embasp use, especially in the mobile setting, we describe the dlvfit android app more in detail."
"applications. the first direct application of our technique is in css code maintenance. cilla can be incorporated into web development cycles through automated nightly runs. the reported unused code can, for instance, be deleted to maintain a clean code base and decrease the processing load on the browser. specifically, mobile web applications could greatly benefit by avoiding the download of unnecessary css code to decrease bandwidth usage and user-perceived latency."
"it is worth noting that the framework design is intended to ease and guide the generation of suitable libraries for the use of specific solvers on particular platforms; resulting applications manage asp solvers as \"black boxes\". on the one hand, this might lead to issues arising from users demanding for a more interactive white-box usage; on the other hand, this made us able to keep a clean design that grants an intuitive usage and an architecture which is general and easily adaptable to different platforms and reasoners. the resulting libraries can hence be used in order to effectively embed asp reasoning modules, handled by the asp system(s) at hand, within any kind of application developed for the targeted platforms. in addition, as already discussed above, the framework is meant to give developers the possibility to work separately on asp-based modules and on the applications that makes use of them, thus keeping things simple when developing complex applications. additional specific advantages/disadvantages might arise depending on the programming language chosen for deploying libraries and on the target platform; special features, indeed, can make implementation, and in turn extensions and usage, easier or more difficult, to different extents. we will briefly discuss these issues later on."
"up #cssp roperties #ignoredcssp roperties ⇥ 100) table iv shows descriptive statistics of the percentages of detected ineffective selectors (% is), unused selectors (% (us+is)), unused properties (% up), and reduction in css code size if the unused code were to be removed (% red.)."
"(iii) flexible back-ends (r3). there are several implications that must be considered when storing monitoring metrics. some nodes closer to the edge are more unreliable, in the sense that they can disappear from the network at any moment. therefore, metrics should be stored on more reliable nodes. also the volume and speed of the monitoring process can change depending on what elements are being monitored, and from how many nodes. for example, for a small amount of monitored components, a mysql database might be enough. on the other hand, for an intensive log monitoring task on a large geographical area of nodes, we might need a more scalable database, like cassandra. this requires a flexible back-end in both type and location for the monitored data. additionally, there may be several back-ends for the same data at more than one of the levels presented in figure 1 ."
"(v) resilience (r5). the nodes in the fog can be faulty, specially considering that they can have limited resources. if a monitoring agent fails, the system should be able to relaunch it. also, if the host of an important point of the monitoring workflow (e.g., the back-end) disappears, we should be able to reallocate that point to another host nearby."
"overall, lmh was well balanced, nearly satisfying the goal of being as fast as all single-curve techniques at all distances. its only failing was to fall slightly behind l at the shortest distances due to the unnecessary use of multiple fingers in those cases. this perhaps indicates that there is still some cognitive struggle in explicitly selecting among multiple curves. it seems natural that, with more training and experience, users could overcome this problem by simply recognizing that a single finger is sufficient for short distance operations. considering l's extremely poor performance at long distances, lmh is the best overall choice. m is the next most balanced but is slower than lmh for short and long distances."
"does the addition of multiple acceleration curves into a single mode of interaction enable users to improve performance over a single acceleration curve? this study confirms that users were able to use three pointer acceleration curves in a way that improved their overall performance over using only any one of the curves. that is, users exploited explicit curve switching (among three curves) to greatly expand the range of cd gains available to them in order to efficiently accomplish multiscale tasks on a lhrd. users utilized the higher multipliers (h) for fast coarse positioning over long distances and the lowest multiplier (l) for fine positioning over small nearby targets. thus, while the singlecurve techniques exhibited the expected tradeoff at different distances, lmh broke the tradeoff. importantly, the lmh technique enabled the fastest long distance selection times, while also matching the accuracy (low error rates) of the low speed curve (l). this combination of explicit and implicit control of pointer granularity yielded better overall results than strictly implicit control. the only exception was slightly slower performance than l alone for the shortest distance targets (discussed later)."
"technological advancements of the last years are making a connected world a reality [cit] ihs markit report, more than 20 billion devices are used worldwide to generate, process and store data [cit] . this is causing a ubiquitous phenomenon; we can find sensors everywhere, clusters of processors, and almost unlimited storage. this interconnected network of devices is nowadays known as the internet of things (iot) [cit] . these devices are currently generating about 2.5 quintillion bytes of data daily [cit] . applications from different domains, such as smart cities, healthcare, or connected vehicles [cit], can make use of this data."
"the main design challenge with an interaction technique that features dynamically switchable pointer granularity settings is to ameliorate the cognitive and physical obstacles introduced by the mechanism of switching as well as the mental dissonance associated with the process of switching between different modes of interactions. this raises a question of what interaction mechanism to use that would minimize the cognitive and physical efforts while allowing on-demand, dynamic, and instantaneous switching of pointer granularity."
"challenges and motivation. our motivating example is a very simple example of the kind of css code that is present on the web. the code snippet is showing the properties that are applied on the elements of the two dom states at runtime in bold. this clear indication is, however, not readily available when developers write or maintain css code. even in this simple example, with only two dom states and a few lines of css code, it is not trivial to understand how the rules are applied to the dom elements. having to work on a web project with thousands of lines of css code and hundreds of dom states can, thus, be very challenging. our aim in this work is to provide a technique that can automatically provide the developer with information on css rule usage."
"in our setting, we make use of such feature so that it is possible to translate facts into strings and vice-versa via two custom annotations, defined according to the following syntax:"
"however, we found two main problems in the results that lead to potential future work. these two problems indicate that there is still some vestige of physical and cognitive barriers to explicit switching in the lmh multifinger pointing technique. first, as a physical barrier, users tended to perform slightly more clutching than was necessary. it seems that some users occasionally clutched when explicitly switching curves, which may have slowed their overall performance. second, as a cognitive barrier, there was some use of suboptimal strategies in the patterns of explicit switching. for example, some users attempted to unnecessarily use two fingers for short distance targets, which explains the slightly slower performance of lmh over l for those tasks. also, one user avoided using three fingers even for long distance targets. there was some variance in the timing of the explicit switching from h to m to l to hone in on targets, causing some users to undershoot or overshoot. yet, these problems do not appear to be fundamental and could possibly be overcome simply with more training and experience by the users, or refinements to the design."
"we have implemented our css analysis technique in an open source tool called cilla. 2 we use cssparser [cit] to parse css source code and transform the rules into a dom 2 style tree [cit] . for automating the dom state exploration phase, we use crawljax [cit], a dynamic web crawler capable of detecting dom state changes of ajax-based web applications."
"our results show that ineffective selectors are present in web applications and they can form up to 71.4% of the total number of unused selectors. unmatched selectors, however, constitute the largest portion of unused code. there are also a number of undefined class values in html code. the highest number reported in our study is 450 for lenovo. as shown in table iv, css file size can be reduced by up to 58% merely by eliminating unused selectors and properties."
"fog devices can frequently join and leave the infrastructure, as stated in our requirements section. for instance, a mobile node that joins the network, or a device that is switched on, has to be included again in the monitoring process. we need to assess how soon agents can be up and running on a device. we measure the time in seconds that it takes to deploy fmone agents in the following configurations: 1 in one node, 2 in one node, 10 in 5 nodes, and 30 in 15 nodes. when using a docker container, the node needs to pull down the image from a repository if it has not been done yet. we perform the test in both situations, where the node pulls and does not pull the image. the results are shown in figure 10 with the average deployment time and standard deviation for 5 rounds of experiments. as expected, a node that runs fmone for the first time introduces some overhead when downloading the image. this overhead will be higher the more agents we deploy at the same time, due to the time needed to download the image. the maximum is 34 seconds for 30 agents and 15 hosts. bear in mind that this situation, where the node has to pull the docker image, only happens once and it takes only 2.5 seconds to deploy them once the nodes already have the docker image."
"single mode interaction techniques with explicit granularity control have had surprisingly little attention in the research community. [cit], jellinek and card [cit] introduced a modified mouse (manual mouse) that allowed users to change cd gain on the fly by pressing hardware buttons. in their study [cit], they compared manually accelerated and constant gain mice. unlike our experiment, jellinek's manual mouse enabled users to choose among fixed cd gains instead of acceleration curve multipliers. their metaphor for the manual mouse was a manual gearbox with which drivers explicitly select their own gears, in contrast to an automatic gearbox that implicitly selects gears based on the speed of the vehicle. although manual selection of cd gains provides support for a multiscale environment within a single interaction technique, jellinek's implementation suffered from a crude method of switching between different cd gain values."
"for instance, adaptive ray-casting techniques [cit] allow variable cd gain to better match targeting conditions. with such techniques the cd gain changes depending on the hand movement characteristics and, as a result, provides faster and more precise continuous pointing experience. while the single mental model of ray-casting is preserved, the pointing performance improves."
"let p be a program. the herbrand universe and the herbrand base of p are defined in the standard way and denoted by up and bp, respectively."
"while both synchronous and asynchronous modes are provided in the desktop setting, we stick to the asynchronous one on android: indeed, mobile users are familiar with apps featuring constantly reactive graphic interfaces, and according to this native asynchronous execution policy, we want to discourage a blocking execution."
"moreover, the module features an aspmapper class, that acts like a translator, providing proper means for a two-way translation between strings recognizable by the asp solver at hand and java objects directly employable within the application. the aspmapper is intended at translating asp input and output from and to objects: thus has a dependency from aspinputprogram and answersets classes."
"the dlvfit android app was the first application making use of the framework; it was conceived as a proof of concept, in order to show the framework features and capabilities. to our knowledge, it is also the first mobile app natively running an asp solver."
"in a postexperiment questionnaire, all participants preferred lmh most out of the four techniques. in verbal feedback and discussion, they cited lmh's adaptive nature. they appreciated the ability to slow down near the targets by lifting extra fingers. some said that \"speed\" switching required no noticeable effort and was performed \"without thinking about it. \" when asked to describe their target acquisition strategies, all of them mentioned switching to the \"slow speed\" when approaching a target. several participants described lmh as \"fun\" and \"engaging. \" seven out of the eight participants said that they would use lmh if they had a display as large as the one used during the experiment. five participants said that they found lmh intuitive after the first dozen clicks. the other three also described it as intuitive, although during the first several minutes of using lmh they had the opposite opinion."
"to address rq3, we run cilla on all fifteen experimental objects and calculate the percentage of the css code that is found to be unused."
"current industrial tools for analyzing css code are mainly static analyzers concerned with either standard conformity, such as w3c css validator [cit], or code formatting and optimizations, such as csstidy [cit], cssclean [cit], and css lint [cit] . a few industrial tools exist, such as dust-me selectors (dms) [cit] and cssess [cit], which target css rule analysis. all such tools are, however, still immature. in particular, they produce a high rate of false positives and false negatives (as shown and discussed in sections vi-vii)."
"let us suppose that a set of facts f is given, representing the schema to be completed; in particular, a binary predicate pos encodes possible position coordinates; symbol is a unary predicate encoding possible symbols (numbers); facts of the form sameblock(x1,y1,x2,y2) state that two positions (x1, y1) and (x2, y2) are within the same block; facts of the form cell(x,y,n) represent that a position (x, y) is filled with symbol n."
"cascading and specificity. the cascading notion in css ultimately determines which properties will be applied to a selectable dom element. the cascading order is based on two main concepts: specificity and location. when there is a competition on an element from two or more css rules for the same property (e.g., color), the language applies these methods to determine which rule takes precedence."
"most of components in the herein presented java implementation have been accomplished thanks to features that are typical of any object-oriented language, such as inheritance and polymorphism. the unique exception is represented by the aspmapper component, implemented by means of java peculiar features, such as annotations and reflection. in case of other languages that feature similar constructs, such as c# 8, the approach can resemble the herein presented java implementation."
"when the state exploration phase is done, the tool iterates over the entire css rule entries in the map and reports about the findings. the first entry in the report consists of the total number of (1) examined css resources, rules, selectors, and properties (2)"
"2) unused css code: figure 7 depicts a bar plot of the percentages of unused css selector and declaration properties for all the fifteen systems. the numbers on each bar represent the number of unused entities. in order to obtain the results shown in figure 7 (i.e., remain unchanged). we depict the results in figure 6 obtained from six systems with high numbers of dom states. the percentage of unused selectors oscillates when we increase the number of explored dom states from 10 to 50. after 50, the graph stabilizes, indicating that the process of detecting unused selectors has reached a steady state. the numbers in figure 7 represent the percentages of unused selectors taken after stabilization. going back to rq3, the high percentages reported in figure 7 clearly indicate that there is a huge amount of unused css code in online web applications. as shown in table iv, on average, there are 59.67% unused css selectors and 52.07% unused declaration properties. staples has the highest unused percentages: 92% selectors and 90% declaration properties."
"(i) to simulate the applications and components that run inside fog devices and that will be monitored by our agents, we launch 5 dummy docker containers per vm that just sleep for a long period of time."
"in order to illustrate the use of the framework, we present here an actual java implementation; in addition, we introduce two specialized libraries for dlv [cit] and clingo [cit], two state-of-the-art asp systems, on mobile and desktop platforms, respectively. furthermore, we show some applications developed in an educational context, that prove the effectiveness of the framework."
"the method startreasoning is in charge of actually managing the reasoning: in our case, it is invoked in response to a \"click\" event that is generated when the user asks for the solution. lines 19-32 create an inputprogram object that is filled with cell objects representing the initial schema, which is then served to the handler; lines 34-37 provide it with the sudoku encoding. it could be loaded, for instance, by means of a utility function that retrieves it from the android resources folder, which, within android applications, is typically meant for containing images, sounds, files and resources in general 7 ."
"the most efficient use of lmh requires switching curves depending on the targeting conditions. for quick long distance targeting, high speeds should be used to reach the general area of the target; once the pointer is close to the target, lower speeds should be used for accurate target acquisition. here we analyze whether users employ lmh in this ideal fashion. to analyze the usage patterns of lmh, we recorded the path of the pointer as it is moved between targets and the number of fingers touching the surface of the touchpad at each timepoint along the path. with this data we can visualize the patterns of lmh usage (figure 12 ) for various target distances. short distance. for the short distance targets, participants mostly used one-finger mode (low speed curve). however, in 18% of cases, 5 out of 8 participants used more than one finger, which explains the difference in performance between l and lmh for short distances. postexperiment discussion revealed that only one participant knowingly used two fingers for the short distance targets. the participant incorrectly believed that with two fingers it would be faster to acquire the targets."
"despite these challenges, css analysis has not received much attention from the research community and there is currently a lack of solid tools and techniques to support its comprehension and maintenance. to the best of our knowledge, analyzing css code from a maintenance perspective has not been addressed in the literature."
"in clingo4, the scripting languages lua and python enable a form of control over the computational tasks of the embedded solver clingo, with the main purpose of supporting also dynamic and incremental reasoning; on the other hand, embasp, similarly to the java wrapper and jdlv, acts like a versatile \"wrapper\" wherewith the developers can interact with the solver. however, differently from the java wrapper, embasp features a mapper that, in the java implementation, makes use of annotations, a form of metadata that can be examined at runtime, thus allowing an easy mapping of input/output to java objects; and differently from jdlv, that uses jpa annotations for defining how java classes map to relations similarly to orm frameworks, embasp straightforwardly uses custom annotations, almost effortless to define, to deal with the mapping."
control. some single mode techniques change the cd gain depending on the physical input from the user. the objective of implicit granularity control of the input device is to determine the targeting condition through the physical input from the user and then provide suitable cd gain for the inferred targeting condition.
"the framework features explicit mechanisms for two-way translations between strings recognizable by asp solvers and objects in the programming language at hand, directly employable within applications. this gives developers the possibility to work separately on asp-based modules and on applications that makes use of them, and keeps things simple when developing complex applications. let us think, for instance, of a scenario in which different figures are involved, such as android/java developers and krr experts. both figures can take advantage from the fact that the knowledge base and the reasoning modules can be designed and developed independently from the rest of the java-based application."
"as for the android platform, we developed an android-handler that handles the execution of an androidservice, which provides facilities to manage the execution of an asp reasoner on the android platform."
"[ramsey] the ramsey number r(k, m) is the least integer n such that, no matter how we color the arcs of the complete graph (clique) with n nodes using two colors, say red and blue, there is a red clique with k nodes (a red k-clique) or a blue clique with m nodes (a blue m-clique). ramsey numbers exist for all pairs of positive integers k and m [cit] . similarly to what already described above, let f be the collection of facts for input predicates node (unary) and edge (binary), encoding a complete graph with n nodes; then, the following asp program p r (3, 4) allows to determine whether a given integer n is the ramsey number r (3, 4), knowing that no integer smaller than n is r (3, 4) ."
"by applying this algorithm to our motivating example, #news, .sports, and #news span are identified to be effective, since they all have at least one effective property. this leaves .latest as ineffective."
"(iv) elasticity (r4). nodes are being continuously added and removed from the network. for example, users can lease their mobile devices as a computing asset [cit] for telcos. a fog monitoring solution should be able to detect any node that becomes part of the infrastructure and start monitoring the device and its components as soon as possible. also, the whole monitoring process should not be hindered by the disappearance of any node in the infrastructure."
"the general architecture of embasp is depicted in figure 1 : it defines an abstract framework to be implemented in some object-oriented programming language. due to its abstract nature, figure 1 just reports the general dependencies among the main modules. nevertheless, each concrete implementation might require specific dependencies among the inner components of each module, as can be observed in figure 2, which is related to a concrete java implementation and will be discussed hereafter."
from this use case we can draw the conclusion that an already complex system like paas can be really difficult to monitor if we add an extra layer of complexity like fog computing. a monitoring tool for such an environment should facilitate this process to the user as much as possible.
"(viii) nonintrusive (r8). some devices at the edge infrastructure are resource-poor. consequently, the monitoring process should interfere with the normal functioning of these already constrained devices as less as possible. especially, the amount of monitored information sent over the network must be kept to a minimum (as bandwidth may be limited). this is specially true when we have demanding applications, like online gaming and running on the nodes. (ix) hardware and operating system agnostic (r9). a fog infrastructure can consist of normal nodes, mobile phones, gateways or any other kind of devices that can provide some kind of computation power. the monitoring tool should be able to run on these broad spectrum of technologies, independently of the operating system or hardware underneath."
"the connect4 application allows a user to play the game (also known as four-in-a-row ) against an asp-based artificial player. notably, the declarative nature of asp, its expressive power, and the possibility to compose programs by selecting proper rules, allowed to design and implement different ais, ranging from the most powerful one, that implements advanced techniques for the perfect play, to the simplest one, that relies on some classical heuristic strategies. furthermore, by using embasp, two different versions of the same app have been built: one for android, making use of dlv, and one for java-enabled desktop platforms, making use of clingo."
"for the purposes of the experiment, we implement a specific version of multifinger pointing, called lmh. lmh allows users to choose any one of three pointer acceleration curve multipliers at any given time, by applying one, two, or three fingers to the surface of the touchpad. users can add and release fingers during the course of finger movement to dynamically switch curves. when multiple fingers are applied, lmh uses the average speed of all contacting fingers. this captures the overall hand movement better than simply capturing the speed of any one finger."
"dlvandroidservice is a specific version of androidservice for the execution of dlv on android. it is worth noting that dlv was not available for android; furthermore, it is natively implemented in c++, while the standard development process on android is based on java. to this end, dlv has been on purpose rebuilt using the ndk (native development kit) 4, and has been linked to the java code using the jni (java native interface) 5 . this grants the access to the apis provided by the android ndk, and in turn accedes to the dlv exposed functionalities directly from the java code of an android application."
"(i) installation-free (r1). as previously mentioned, there is a clear need of a dynamic, heterogeneous infrastructure, such as the ones from telcos. the node heterogeneity would hinder an installation process with different dependencies based on their architecture. in addition, nodes can join the network at any time, creating the need for installation-free monitors that need to be up and running as soon as possible."
"techniques with no granularity control provide only a fixed cd gain. absolute pointing techniques such as ray-casting [cit] essentially provide a cd gain of 1. ray-casting allows users to interact with the display from any distance with simple wrist movements in the way that laser pointers are used. however, since users can change their distance to the display to vary their pointing accuracy, this might be considered a variable cd gain with explicit control (distance from display). the advantages of ray-casting include a natural interaction metaphor, low cognitive load, and freedom of movement in the display space. however, this interaction technique fails to provide high precision pointing [cit], especially with displays that feature high pixel density. the fatigue associated with manipulating the device results in involuntary hand tremors, which in turn reduces pointing accuracy."
"however, more recently, convolutional neural networks (cnns) got popularity for segmentation problems since they learn the features directly from the input data. in the case of retinal vessel segmentation, cnns surpass traditional handcrafted approaches [cit] . however, the problem of blurred output images and false positives around indistinct tiny vessel branches is persistent in these methods. the main reason behind this limitation is use of a unified loss function in a pixel-wise manner to segment both thin and thick vessels. this led to blurred thin vessels resulting in non acceptable segmentation maps during binarization of generated probability maps. to address these issues a novel approach for retinal vessel segmentation using generative adversarial networks (gan) has been proposed in this research. basically, a patch-based discriminator is utilized to learn inconsistencies and sharp edges of high-resolution blood vessels. additionally, a loss term is integrated within the main objective function to learn low-frequency edges. we show that the proposed method is able to effectively segment out thick and thin vascular pixels form non-vascular pixels on two benchmark datasets namely drive [cit] and stare [cit] . our results show a significant boost in the performance as compare to the state-of-the-art methods."
"since this paper focuses on the classical bbo algorithm, biogeography-based optimization (bbo) [cit] and is an extension of biogeography theory to evolutionary algorithm [cit] ，which is based on the mathematical model of biological species distribution and migration [cit] . the bbo has demonstrated good performance on various unconstrained and constrained benchmark functions [cit] . further, it has been applied to real world optimization problems, including sensor selection [cit], power system optimization [cit], etc."
"we apply our algorithms to the selection of optimal architectures for power generation and distribution in a passenger aircraft. fig. 1c illustrates a sample architecture in the form of a single-line diagram, a simplified notation for three-phase power systems [cit] . typically, aircraft electric power system (eps) components include power sources, such as the left and right generators (l/r-gen) and the auxiliary power units (apu) in fig. 1c . the generators power the buses and their loads (not shown in fig. 1c ). ac power is converted to dc power by rectifier units (tru). a bus power control unit monitors the availability of power sources and configures a set of switches, denoted as contactors, such that essential buses remain powered even in the presence of failures."
"for the immunohistochemical staining, the pretreatment of paraffin-embedded tissue samples was conducted as described before [cit] . after heat repairing of antigens in 0.01 m citrate buffer aqueous solution at 95°c for 6~8 min, the nonspecific antigens were blocked with 5% goat serum at 37°c for 30 min. then, the sections were exposed to rabbit anti-cd31 primary antibody (1:100, abcam, cambridge, uk) at 4°c overnight. after 3 washes with pbs, the sections were incubated with goat anti-rabbit secondary antibody at 37°c for 30 min, developed and counterstained with 3, 3'-diaminobenzidine tetrahydrochloride (dab) solution and hematoxylin, respectively. the positive areas stained with brown color were observed under an optical microscope."
"for the drive dataset, the proposed model achieves 0.9562, 0.9824, 0.7746 and 0.9753 for acc, sp, se and au c respectively, where the model achieves better results for acc, sp and au c as compared to the all current state-of-theart unsupervised, supervised and deep learning techniques. however orlando [cit] outperforms all the methods in terms of se as shown in table 1, the only se norm is not conclusive. in contrast, the performance of the proposed model is much better than all the compared methods."
"the angiogenesis of all three groups was systematically investigated. gross observation provides direct evidence of angiogenesis at the early implantation stage. as the implantation time and vessel density increased, advanced testing methods were needed to count the number of the newly formed blood vessels. in this study, newly formed blood vessels were specially marked by immunochemical staining for cd31 (figure 6 ), a type i integral membrane glycoprotein expressed by endothelial cells early in vascular formation [cit] . with the maturation of the blood vessels, another cell layer composed of smooth muscle cells occurred around the endothelial layer, and this layer expresses α-sma ( figure 5 ). compared with cs, the knitted mesh mechanically supports the 3d porous microstructure of the collagen sponge, facilitating the transmission of oxygen and nutrients and the infiltration of cells and blood vessels. in addition, the proper mechanical strength can also change regularly according to the requirement of neotissue and new blood vessel growth. although the mechanical properties of adm are better than those of pmcs, the angiogenesis results are not good. the histology result showed that the infiltration of fibroblasts and vessels was mainly distributed around the scaffolds (figure 3) . the microstructures of adm showed the collapsed hole and tousle structure with burliness fiberboard tier upon tier. the high mechanical strength of adm made it relatively difficult to deform, which blocked the tissue regeneration. further evidence of angiogenesis was provided by rt-qpcr, which was used to investigate the expressions of cd31, α-sma, vegf and pdgf-bb at the gene and protein levels ( figure 7 ). among these key factor related to vascular development, vegf can promote vascular development and maturation with a tiny dose; pdgf-bb, often released by endothelia cells at the sprouting tip of forming capillaries, is not involved in the initial vessel formation but in the neovessel stability and functionalization by inducing anastomoses and mediating pericyte recruitment [cit] . the results of rt-qpcr indicate that the pmcs group exhibited the highest mrna expression of vegf and pdgf-bb, gradually increasing from weeks 1 to 4 and decreasing in week 8. the results of molecular biological detections for cd31 and α-sma also demonstrated that the pmcs group had a much higher level of angiogenesis than the other two groups. all these results confirm that moderate mechanical properties and proper physical structure have a basic and critical influence on the rapid angiogenesis of dermal equivalents. only the proper mechanical strength and aperture could promote the tissue and cell ingrowth. both cs and adm have homogeneous holes and compositions, but the pmcs has a variable structure with a mesh positioned at one end that produces a 3d mode with a graded aperture in which the holes from the mesh side to the collagen side are gradually enlarged. this is in line with the general laws of the material-induced regeneration of tissue and cells."
"the mutation model is another important feature in the classical bbo, which realizes mutation through a uniformly random function to probabilistically replace sivs by randomly generating new sivs in a solution. this situation can much better avoid search prematurity. however, the random function doesn't adapt to the vm running environment in real."
"in this paper, the improved migration model is used to represent the migration feature. the linear function in bbo is basic migration model in bbo, which can be used to share sivs between habitats. however, the linear model is the theoretical model with ideal condition. when there are more or less species in one habitat, the change rate of immigration and emigration will trend to the steady state. otherwise, the change rate is fierce in the real virtualization environment. so this situation can be analogized in vmcp."
"overall, we infer that ilp-ar turns out to be preferable when we aim to a coarser estimation of the capability (and limitations) of an architecture template and a platform library in terms of reliability. on the other hand, ilp-mr makes it easier to incorporate domain-specific knowledge, since a designer can customize the techniques adopted to improve reliability at each iteration. moreover, ilp-mr becomes the preferred choice, especially for larger problem instances, when we can estimate the number of redundant paths needed to satisfy the requirement as early as possible, or when we are willing to pay for a longer execution time to incrementally fine tune the reliability of the design."
"for the sake of conciseness, the formal proof of the theorems in this paper will be omitted. informally, we observe that, since the number of components in t is finite, the ilp-mr routine, based on algorithms 1 and 2, will terminate. moreover, because relanalysis implements an exact reliability analysis method, if a final architecture is found, then it will satisfy all the requirements, being only subject to the rounding error due to the ilp solver and to relanalysis. on the other hand, because all available components will be eventually used to increase the reliability, if ilp-mr terminates with unfeasible, then there is no architecture, obtained from the given template, which is able to satisfy all the constraints."
"the migration resource overhead value of imbbo(1) reach 14004.37 through 1000 iterations. the cpu extra utilization and transfer times in source and destination pms are considered in this process. finally, the imbbo(1) algorithm can reduce the resource overhead in migration process compared others. fig.3(a)-(c) show the comparisons of the three algorithms on synthetic data. the imbbo algorithm rapidly finds the best solutions. the blue cure presents the bbo. the green cure is the imbbo(2). the sky blue cure is the gsa. however, the red cure is the imbbo(1)."
"based on theorem 2, (7) can provide \"optimistic\" estimations, but the bound to such optimism can be explicitly estimated for a given graph template t ."
"this work was partially supported by ibm and united technologies corporation (utc) via the icyphy consortium, and by terraswarm, one of six centers of starnet, a semiconductor research corporation program sponsored by marco and darpa. (iii) we compare the performance of the two approaches and demonstrate their effectiveness on the design of safetycritical industrial-scale architectures for aircraft electric power distribution."
"deep learning has influenced image analysis in various important application areas including remote-sensing, autonomous vehicles, and specially medical [cit] . usually, diagnostic analysis and treatment which covers medical disorders, diabetic retinopathy, and glaucoma have been carried on retinal vessels of opthalmologic fundus images [cit] . morphological attributes and retinal vascular structures including vascular tree patterns, vessels thickness, color, density, crookedness, and relative angles are the key features used in the diagnostic process by ophthalmologists [cit] . thickness and visibility of retinal vessels are the core attributes for diabetic retinopathy analysis [cit] . however, conventional and manual approaches for vascular analysis are time-consuming and prone to human error. therefore, computer-assisted detection of retinal vessels is inevitable to segment out the retinal vessels from fundus images and mainly categorized into three approaches: unsupervised learning, supervised learning, and deep learning."
"to extract low level features, proposed model followed inspiration form unet [cit] and segnet [cit] . the generative network is comprised of encoder and decoder networks. encoder network extracts the hidden features and reduces the size of the input image whereas the decoder network reconstructs and up-sample at each stage till the last layer. each input image passes through the entire generator network and skip connections between encoder and decoder network acts as bridge to semantic gap between the feature maps of the encoder and decoder prior to fusion."
the tissue samples were prepared and embedded in paraffin. the paraffin blocks were all sectioned at a thickness of 5 μm and stained by hematoxylin & eosin (he) staining and masson trichrome staining. the stained sections were visualized under an optical microscope.
"in the synthetic data, there are 500 physical machines in one data center. the initial habitat is set with 500. the number of virtual machines is 200. next, the initial power consumption is 500*162w and the detail result information is presented in table ⅲ of synthetic data. the improved migration rate model and mutation rate model are adopted in imbbo (1) . meanwhile, the improved migration rate model and constant probability value of mutation model are adopted in imbbo (2) . finally, the imbbo(1) algorithm only needs 49 active servers to support these vms running."
"recently, many researchers and institutes have focused on virtual machine consolidation problem, which can effectively reduce the energy consumption in one data center by consolidating vms to pms and shut down the idle ones. however, most methods just consider how to consolidate some vms to some pms, it will result in the unreasonable resource contention because vmc is a dynamic process. for the cloud providers, a good vmc scheme should maximize resource utilization and minimize power consumption and others."
"the m presents the habitat size and the n presets a number of independent variables called suitability index variables, which simulate the habitat number and features in habitat. so it is proportional to m*n. in addition, the space complexity is m*n, which presents the matrix size in imbbo."
"reliability constraints. a typical reliability requirement prescribes that the failure probability of a sink, i.e. the probability that a sink gets disconnected from a source because of failures, should be less than a desired threshold. therefore, to formulate a reliability constraint, we need to compute the probability of composite failure events in the system, starting from the failure probabilities of the components. specifically, we denote as system failure r i an event in which there is no possible connection between any of the available sources and a sink i, i.e. when the functional link f i breaks, and as reliability level r i the probability of r i . practically, the above notion of failure models the interruption of any information or energy transfer to an essential portion of the system. we assume that when a component fails it cannot be recovered, and the adjacent links are no longer usable. moreover, failures in different components are independent."
"a novel multi-objective optimization algorithm, imbbo, is presented. the key factors of migration and mutation model in imbbo are modified in order to preferably adapt to the vmc problem. meanwhile, based on synthetic and real vms running data, we compare the proposed imbbo with gravitational search algorithm (gsa), thus explaining the advantage of our method."
"in a typical cyber-physical system (cps) architecture, software components running on a hardware computing platform are connected in feedback with physical processes to form a large, distributed, control system subject to tight cost, safety and reliability constraints. a major obstacle to the development of model-based design tools for these systems is the heterogeneity of the requirements, often expressed using different mathematical formalisms, and hard to account for at once. it is, therefore, desirable to devise abstractions that enable scalable co-design and optimization of complex cps architectures for several, possible conflicting concerns, while guaranteeing design correctness and fault tolerance."
"let p i be the event that component i fails (self-induced failure). then, the event r i of a system failure affecting component i can be recursively computed as follows"
"in order to solve the above objective optimization problem, a novel multi-objective vmc algorithm named imbbo (improved bbo) is proposed based on improving the classical bbo algorithm. the major contributions of this paper are as follows."
"traditional safety and reliability assessment is based on a set of methods that are hard to incorporate into automatic design exploration and optimization frameworks. in addition to the complexity of exact network reliability analysis, which is an np-hard problem [cit], techniques such as fault tree analysis (fta) or reliability block diagrams (rbd) often rely on a set of system abstractions, which are hardly interoperable with system design flows [cit] . for instance, in fta, causal chains leading to some failure are depicted as a tree, inherently describing a hierarchical breakdown. however, in fta, decomposition into modules mostly relates to the hierarchy of failure influences rather than to the actual system architecture. therefore, the integration of fault trees with other system design models is not directly possible."
supervised approaches intend to assign a class label to each pixel of the image. these approaches can be based on either traditional machine learning or deep learning. the former utilizes classifiers that learn decision boundaries on handcrafted features e.g. support vector machine (svm) and k-nearest neighbor classifier (knn).
"collagen scaffold, plga-collagen scaffold and adm are three types of dermal substitutes composed of collagen with different mechanical properties and microstructure characteristics. what type of role does the biomechanics and microstructure play in wound healing? in this study, we chose these three types of scaffolds mentioned above to represent the low (cs), intermediate (pmcs), and high (adm) mechanical property scaffolds to determine which one is most suitable for dermal regeneration. pmcs was prepared by incorporating the plga knitted mesh (plgam) into collagen scaffold (cs) as a hybrid scaffold to study the function of the mechanical properties and microstructure in tissue regeneration. young's modulus as a physical quantity of the ability to resist deformation of solid material was measured to illustrate the mechanical properties. the hypothesis that tissue regeneration and angiogenesis is possibly dependent on porous microstructure and suitable mechanical properties of scaffolds was tested by investigating the angiogenic and regenerative potentials of the three scaffolds implanted subcutaneously in a rat model. the study protocol was approved by institutional review board at the zhejiang university."
"the proposed model is trained and evaluated on two publicly available datasets include drive [cit], stare [cit] . we followed the same evaluation metrics and protocols to conduct a fair evaluation with the reported state-of-the-art methods. accuracy (acc), sensitivity (se) and specificity (sp) are used as a benchmark for quantitative evaluation and area under the receiving operating curve (au c) is used for the qualitative evaluation."
adversarial learning approach combines generator and discriminator networks in such a way that conditional input provides a head start to the overall learning process. generator network g expects noise and conditional input sample and learn to generate the synthetic retinal map. discriminator network d takes two sets of input: conditional input and a generated synthetic map (fake sample) and conditional input and actual segmentation map (/real sample/ground truth). segmentation maps generated from generator network are divided into rectangular patches and discriminator tries to discriminate each patch. this patch based discriminator is deployed to discriminate between actual or synthetically generated segmentation maps as shown in fig. 1 .
"the different configuration coefficient will be used to verify this imbbo algorithm. some parameter values of imbbo algorithm and gsa are defined in table i . in order to getting average and exact results, each set is optimized by running independently 20 times and the average value is reported for each result. the gravitational search algorithm (gsa) will view as the comparing algorithm for verifying the performance of imbbo algorithm. meanwhile, in order to keep the performance, two algorithms use the same set of initial data."
"after being sectioned with a razor blade and coated with platinum, the morphologies of adm, cs and pmcs were observed using sem (philips xl30, eindhoven, the netherlands). the accelerating voltage was set at 1.0 kv. to determine the mean pore size of the scaffolds, images of cross-sectioned surfaces from cs and pmcs were analyzed as described previously [cit] . three sem images of each scaffold were taken randomly. in each, ten apparent pores were selected and measured with a ruler. the long and minor axes of pores in a perpendicular direction were measured, and the average was taken as the mean pore size."
"since the traditional vmc scheme just focus on how to minimize the consolidation, the similarity among vms resource utilization isn't considered. more vms may be migrated to the same pm that will extremely result in the unstable state of pms and other vms. in additional, vms migration not only occupies the network bandwidth, but also the cpu utilization in source and destination. pms may cause extra overhead such as more cpu cycle, the high memory operation and network transfer. therefore, these factors should be considered in vmc in order to save power consumption."
"in future, we will focus on parallelization of imbbo and research re-configuration migration and mutation model in real problem. the appropriate adjustment also should be used to solve the different objective function in order to avoid shortcoming of premature convergence. (2)"
"recently, some extension researches also have been presented. typically, haiping ma [cit] improved the classical bbo algorithm and analyzes the equilibrium of migration models. haiping ma and dan simon [cit] discussed migration models using markov theory and blended bbo for constrained optimization. in real, bbo maps these factors as suitability index variable (siv) and habitat suitability index (hsi) to mathematical solution space in order to find the optimal solution of a certain problem."
"in this section, experiment environment will be described and the results will be evaluated by using two different instance types, including synthetic instance and real vm running instance. then, many experiments comparing with gsa algorithm are carried out to verify the performance of our algorithm from different aspects. finally, we analyze and discuss the experimental results in detail."
"in the synthetic data: servers are based on homogeneous framework. synthetic data is configured according to the reference [cit] . the items in dataset independently follow the normal distribution, which has been adopted in previous researches, including cpu demands generated with n(0.15,0.05), memory demands generated with n(0.10,0.08) and network demands generated with n(0.03,0.01). in the real-world data: sixteen types of instance ( table ⅱ. ) referring to amazon ec2 are also used to simulate in virtual cloud laboratory system, which are divided into three categories including general, computing optimized and memory optimized. since the reference just contains cpu and memory except network, the network parameter will be set through simulation or get by using the data collection tools which are developed by shell (linux) or c# (windows)."
"the ilp-ar algorithm replaces exact reliability computations with an approximate algebra that allows encoding a reliability requirement into a number of linear constraints on boolean variables, which is polynomial in the size of the template. in section iv-a we introduce the approximate reliability algebra, which estimates the correct order of magnitude for the failure probabilities of all the components in an architecture by leveraging the fact that components with the highest failure probability tend to dominate the overall failure probability. then, in section iv-b we report the main results on correctness and complexity of ilp-ar."
"where e x,z and e x,y are loss functions for generator g and discriminator d respectively. to handle the problem of blurred outputs caused by l 2 norm reported in literature, we integrated l 1 norm in the main objective function to capture low and high-frequency components of the fundus retinal images. l 1 norm can be formulated as:"
"1. the vmc problem is formulated as a multi-objective optimization problem, which includes the novel minimizing server power consumption, achieving better loading balance based on vms' resource correlation, and reducing migration resource overhead considering in source and destination pms."
the aim goal of this paper is to study the vmcp through multi-objective optimization which is based on imbbo. to formulate this problem we will deal with it in the mathematical forms:
"meanwhile, many vms are placed in the same pm may greatly increase the probability of resource contention. finally, some vms should be migrated to other pms in order to maintain the running performance. from this, the improved cosine migration rate model can be illustrated in eq. (6)"
"the ilp modulo reliability (ilp-mr) algorithm avoids the expensive generation of symbolic reliability constraints by adapting the ilp modulo theory approach [cit] to reliability computations, as summarized in algorithm 1. ilp-mr receives as inputs: the library of components l, together with their attributes c, w and p, the template t, and the set of requirements, including interconnection and reliability constraints. to simplify, we assume that r is the worst case failure probability over a set of nodes of interest, for which the same reliability requirement r * must be satisfied."
"we have introduced, characterized and demonstrated two efficient ilp-based algorithms for the optimal selection of cyber-physical system architectures subject to safety and reliability constraints. as a future work, we plan to further investigate the generalization of the presented approaches to support a broader category of systems (e.g. power grids, communication networks) and design concerns (e.g. impact of system dynamics and transients)."
"for all 3 groups, no evident inflammatory reaction, e.g., infection, fistula and fibrous capsule, was observed in the implantation and adjacent sites at different time points. at week 1, the implants had integrated into the surrounding tissues, and several blood vessels could be observed around the implants in pmcs and cs. this process was not obvious in adm. adm and pmcs maintained a similar shape and area with the first state. meanwhile, the residual area of cs reached nearly 70% of the previous area."
"since the ea algorithms may lead to premature convergence (local optimization), including the bbo, the different strategies also should be adapted to avoid it. the appropriate parameters adjustment and the different situation analysis should be pre-done for the different objective functions."
"before being implanted in animal experiments, the scaffolds were sterilized with 70% ethanol for 30 min and then washed completely with sterilized phosphate buffer solution (pbs) 5 times, each for 15 min."
"full-thickness skin defects resulting from injury, burns and nonhealing ulcers represent a significant clinical problem that is far from being solved. at present, the main treatments for skin defects are transplantations such as autografts, allografts and xenografts to ensure wound closure as soon as possible [cit] . tissue engineering of whole skin represents an equally attractive and ambitious novel approach [cit] . many skin tissue engineering methods have been developed for the treatment of full-thickness skin defects [cit] . although tissue engineering has a significant potential to provide alternative approaches for skin regeneration, several problems have hampered progress in translating technological advances to clinical reality [cit] . the skin split-thickness autograft has been widely used but frequently leads to scarring in full-thickness wounds because of the lack of dermis. therefore, dermal reconstruction or regeneration is the critical issue to reduce scar formation and improve the quality of wound healing; this has encouraged the initiation and development of dermal substitutes [cit] . several products have been manufactured and used clinically. however, none of them were able to perfectly regenerate skin for various reasons, including unsuitable physical or mechanical properties and inappropriate microstructures that hinder the cellular infiltration, proliferation and differentiation [cit] . the ideal substitutes should have suitable properties that effectively induce rapid angiogenesis, a significant challenge in the field of tissue engineering and regenerative medicine, which is one of the crucial premises to promote regeneration and decrease the risk of infection [cit] ."
"at each iteration of ilp-mr, if the optimal architecture satisfies the reliability constraints, it is returned as the final solution. otherwise, learncons estimates the number of redundant paths needed to achieve the desired reliability and suggests a set of strategies to implement the required paths by augmenting the original optimization problem with a set of interconnection constraints. this constraint learning function is, therefore, instrumental to efficiently converge towards a reliable architecture, while minimizing the number of calls to relanalysis. we provide details about this function in the following section."
"to enforce additional paths, addpath uses the walk indicator matrix of t, defined below. we can then require at least k additional connections of components of type t i, belonging to the set π i of the partition π of t, to a sink v via at least one path of length n − i + 1 by enforcing"
"to investigate collagen deposition, masson trichrome staining was performed, and the results are illustrated in figure 4 . in all the groups, the quantity of collagen deposition increased with the prolongation of the implantation time. at week 1, many blue collagen fibrils were observed near to the mesh side in the pmcs group, while a few fibrils were distributed around the adm and cs groups. at week 2, compared with the collagen deposition throughout the pmcs, there were some fibrils positioned in the scaffolds near to the surfaces in the other groups. at week 4, more collagen fibrils appeared in the pmcs group, and the fibrils were more homogenous and ordered than those of other groups. from weeks 4 to 8, more abundant and ordered collagen deposition was observed in the pmcs group, while the cs and adm groups revealed a heterogeneous collagen distribution."
"the second objective eq. (2) is the load balancing based on minimization correlation of resource utilization among pms in the data center. the phenomenon shows that the load balancing is the similarity average resource utilization among pms. since the resources competition will result unstable resource condition among pms and vms, the reasonable solution is that these vms with minimum similarity of resource utilization should be consolidated into same pm. in addition, the configuration parameters,  cpu, mem  and  net, are the weight coefficient, which is satisfied with constraint condition also should be considered in process. the parameter  ex is the proportionality coefficient that the vm migration process may extra occupy the cpu cycle."
"in this paper, the vmcp is formulated as a multi-objective optimization problem, which contains three configurable objectives, that is, server power consumption minimization, load balancing minimization based on resource utilization similarity and reducing migration resource overhead. a novel multi-objective optimization algorithm named imbbo is proposed to solve vmcp based on the classical bbo. the migration and mutation model was improved, which can better meet the actual situation of vmcp. the experiment results show that the imbbo can improve the performance of optimal solutions and convergence characteristic by comparing with gsa by using the synthetic and real vms running data."
"in the experiment, two types of experiment data are used to verify the performance of imbbo algorithm which is compared with gsa. our experiment environment is based on the real vm running environment for ecust virtual cloud laboratory system. meanwhile, imbbo algorithm will be deployed on monitor server. the virtual cloud laboratory system was developed on openstack. this system will provide basic experiment courses for students. the cluster is composed of a numbers of dell poweredge r730 which is viewed as the computing node. each server consists 2 physical cpus (intele5-2650 v3 2.3ghz, 25m) and 256gb main memory."
"the rational selection and collocation of biomaterials help design biomimetic drt. incorporation of the plga knitted mesh into a collagen/chitosan porous scaffold with lyophilization formed a reinforced hybrid scaffold with excellent mechanical property and a suitable microstructure. when embedded into the dorsal subcutaneous pockets, pmcs can resist the contraction, induce cell infiltration and neo-tissue formation, and promote blood vessel ingrowth compared with cs. adm has a high mechanical property that is large enough to resist contraction, but without a porous structure and elastic strain capacity, the angiogenesis that induces tissue regeneration is delayed. hence, the ideal scaffolds not only require the proper mechanical strength to maintain their structure but also require an elasticity change to accommodate the extension of neo-tissue. this indicates that suitable mechanical functioning could be one of the most crucial characteristics for maintaining the shape and microstructure of the constructs to facilitate cell ingrowth and functionalization. in summary, the mechanical properties and microstructure, which play an important role in accelerating angiogenesis and inducing in situ tissue regeneration, are the most basic characteristics of engineered scaffolds."
"angiogenesis at the gene level was further characterized by quantifying several critical factors, such as cd31, α-sma, vegf and pdgf-bb, using rt-qpcr. generally, the trends of gene expression in the 3 groups fluctuate regardless of the types of genes, whereas the pmcs group always retained a higher gene expression level at different time points. when compared within the different implantation periods, the cd31, α-sma, vegf and pdgf-bb mrna expressions of the pmcs group were always significantly higher than those of other groups (figure 7 )."
"for experiment, the imbbo(1) algorithm just needs 19 active servers to support these vms running. the imbbo(1) can reduce the power consumption from 150*162w to 4045.58. that is, it saves 83.36% of power. in addition, the similarity evaluation value of imbbo (1) is -0.0891, which is greatly smaller than others in reducing resource utilization competition. meanwhile, the migration resource overhead value of imbbo(1) reach 3559.00 through 1000 iterations. fig.3(d)-(f) show the comparisons of the three algorithms on real vm running environment. these curve styles are the same as the synthetic instance. after the 1000 iterations, all of these curves approximately trend to line. since a great number of vms run in real data center environment, the appropriate running time and convergence time are the much more important parameter index."
"interconnection constraints. interconnection requirements originate from the composition rules in l and are used to enforce legal connections among components. for example, let d, l, and b be subsets of v . then, we can prescribe that there exists at least (most) one connection from a node in l and a node in d as follows:"
"the improved descending function in this section, which is compared to the random function in bbo, is a more excellent strategy. with the increasing numbers of iteration, more excellent elitisms will be contained in next generation. the system will more tend to be a stable state. that is, the probability of mutation should be descended in iterations. the eq. (7) represents the improved mutation model."
"based on the definitions above, we cast the architecture selection problem as an optimization problem. given a library l and a template t, our goal is to derive a configuration that satisfies a set of interconnection and reliability requirements, while minimizing the cost and the complexity (number of components) of the overall network. the set of boolean variables e will then include our decision variables. based on the final assignment over e, some of the edges and nodes in t will be selected to generate an optimal architecture; unnecessary nodes and edges will instead be pruned away to minimize the overall cost. in the following, we provide example formulations for the objective function and the requirements in terms of boolean arithmetic constraints."
"the similarity evaluation value of imbbo (1) is -0.4351, which is greatly smaller than others. this situation demonstrates that the algorithm can gain better performance and achieve the smallest similarity among virtual machines in order to reduce resource competition."
"interconnection requirements as the ones above originate linear arithmetic constraints in the decision variables, or include logical operations (conjunctions and disjunctions) that can be linearized with standard techniques [cit] ."
"in the real vm running environment, we set 80 vms which simulate 80 students to attend class in virtual cloud laboratory system. the initial configuration needs 150 servers. the experiment results are described in table ⅲ of real vm running data."
"(i) we provide a general graph representation of an architecture that allows an efficient casting of the design problem as an integer linear program (ilp), capable of modeling a variety of system requirements, such as connectivity, safety, reliability and energy balance;"
"another set of interconnection constraints can be used to enforce conservation laws or balance equations in physical systems, e.g. requiring that the maximum power provided by a source in t is greater than or equal to the maximum power required by the connected sinks. let d be a node in the graph, which is neither a source nor a sink. let b be the set of direct predecessors of d, and l be the set of its direct successors. then, the balance equation at the terminals of d can be written as"
"two weeks after implantation, more blood vessels grew into the implants and the color of the implants turned to light red in the pmcs and cs groups. several blood vessels could be observed around the adm group, but the surrounding tissue was not close. in the pmcs group, the mesh loops became indistinguishable, which possibly resulted from the neotissue ingrowth induced by the scaffolds. the residual areas of adm and pmcs changed little, while cs underwent obvious contraction."
"where c i is the cost of component i,c ij is the cost of the switch on edge e ij, and δ i is a binary variable equal to one if the component is instantiated in a configuration and zero otherwise. we express δ i in terms of the edge variables as"
"where e ji is j th -row, i th -column element of the adjacency matrix e of t . in other words, component i fails when either a failure is generated in itself, or when failures are induced through its predecessors. a symbolic constraint for r i can then be generated using (5) to enumerate all possible failure events while traversing t from node i to the sources. however, such an exact computation, based on the enumeration of all possible component failure events, has exponential complexity on a fixed graph configuration [cit] . the problem is further exacerbated when compiling a symbolic expression for a reconfigurable graph, since, in general, enumerating all possible configurations has also exponential complexity. to overcome this issue, we propose two approaches to the solution of the optimal architecture selection problem, namely, ilp-mr and ilp-ar, which we detail in the following sections."
the infiltration of blood vessels into the implants was evaluated morphologically by examining the critical factors relative to the process of angiogenesis such as cd31 and α-sma by immunohistochemistry.
"in conclusion, the imbbo algorithm shows the better performance both in reducing power consumption, achieving good load balancing (minimizing similarity values among vms) and decreasing the migration resource overhead. the reason is that imbbo algorithm considers the special migration model and dynamic consolidation judgment strategy. at the same time, the mutation rate trends to much more stabilization with the iteration. besides, the different configuration parameters may display the different performance in the experiment. these parameters need to be adjusted according to the real vm running situation. finally, the gsa algorithm just focuses on finding the massive particles. the correlation of iteration doesn't been contained through the iteration."
"where η n−i+1 and η * n−i+1 are the walk indicator matrices, respectively, for t (decision variables) and the current architecture g * . the constraint (6) can be converted into an equivalent set of linear constraints in the elements of e (edge variables) by using standard linearization techniques. the following result summarizes the properties of the ilp-mr approach."
"since these experiments are arranged based on homogeneous server architecture, some extra situations and different configuration parameters haven't been detailedly considered in process. it may exist others factors to influence the process of optimization. besides, the emergency situation in some vms also is the reason to affect optimization process."
"adm matrix, which is rich in predominantly type-i collagen, is increasingly used in reconstructive surgery applications [cit] . therefore, the in vivo influence of this type of hybrid scaffold on inductive regeneration and angiogenesis has been widely investigated in skin tissue engineering. compared with artificial polymers, adm materials (adms) as naturally derived biomaterials have excellent mechanical properties, but the pores are too small to sustain the vascularization and facilitate cell infiltration [cit] ."
"where the parameter  is the slope value, it is defined by 0.01 in experiment. in addition, the parameter  will be used to decide the degree of mutation rate in a time period. next, the parameter  is the rectify coefficient, it is defined by 0.001 in experiment. the fig.1(b) illustrates the improved mutation rate cure in a single habitat."
"connectivity properties can be expressed by using constraints as the ones in (2) and (3). for instance, we can prescribe that any rectifier must be directly connected to only one ac bus, and that all dc buses that are connected to a load or another dc bus must be connected to at least one rectifier to receive power from an ac bus. power-flow constraints are used to enforce that the total power provided by the generators in each operating condition is greater than or equal to the total power required by the connected loads, by using expressions as in (4) . finally, a reliability constraint prescribes that the probability that a load gets unpowered because of failures should be less than a desired threshold. a functional link will then consist of the set of paths from any generator to the load. moreover, since our template supports only reduced paths, we use an edge between two nodes of the same type as a shorthand notation to indicate two redundant components: if v i and v j, with v i ∼ v j, are connected by an edge, then any direct predecessor of v i is also a direct predecessor of v j and vice versa."
em is the elasticity modulus; f is the mechanical force applied to the sample; a0 is the original cross-sectioned area; δl is the change amount by which the length of sample; and l0 is the original length of the sample.
"adm is prepared from human dermal tissue by removing the cells and epidermis. adm has been widely used for wound regeneration because of its excellent biocompatibility and mechanical properties that mimic human acellular dermis through its ability to naturally interface with host tissues with minimal tissue response [cit] . however, its high mechanical property is based on a small and collapsed aperture which could be observed on figure 1c ."
"the scaffolds embedded subcutaneously in the rats for 1, 2, 4, and 8 weeks were harvested to isolate rna for the rt-spcr analysis. each sample was dissolved in 1 ml of trizol reagent (invitrogen, ca, usa), and rna was isolated. the content and purity of rna were measured with an ultraviolet spectrophotometer after dissolving rna in depc-h 2 o solution. rna conversion to cdna was carried out with an m-mlv reverse transcriptase cdna synthesis kit (promega, wi, usa). rt-spcr was amplified for cd31, vegf, pdgf-bb and α-sma and the calibrator reference gene (glyceraldehyde-3-phosphate dehydrogenase, gapdh). the reaction was performed with iq table 1 ."
"in this paper, we propose an optimization-based methodology for the selection of cps architectures whose reliability is a function of the interconnection structure. our goal is to minimize the overall system cost (e.g. number and weight of components) while guaranteeing that an upper bound on system failure probability is met. our contributions can be summarized as follows:"
"to characterize the newly formed blood vessels in the implants, α-sma, cd31 (a marker of endothelial cells around the blood vessels) immunohistochemical staining was performed, and the results are illustrated in figure 5 and figure 6 . this indicates that the number of blood vessels increased to some extent for all the groups with the development of the implantation time, and their expression levels were influenced heavily by the type of implant. in week 1, the blood vessels were chiefly distributed around the implants, particularly in the pmcs group, and the density was significantly higher than that in cs and adm. at week 2, the blood vessel densities of pmcs increased, respectively, and the values are also significantly higher than those of cs and adm. at 4 and 8 weeks after implantation, the pmcs and cs groups displayed strong positive staining, while some blank fields could be found in the adm group. the number of blood vessels in the pmcs group continued to increase and was higher than that of other groups with a significant difference at the different time points."
"the rest of this paper is organized as follows. section ⅱ discusses related works. section ⅲ formulates the vmc as a multi-objective optimization problem. section ⅳ introduces the proposed algorithm, including its main process and key strategies. the simulation results are presented in sectionⅴ. and sectionⅵ is the conclusion."
"informally, the result follows from the fact that, for each type of components, ilp-ar attempts to determine the degree of redundancy needed to meet the reliability requirement. therefore, if ilp-ar returns unfeasible, assuming that the interconnection constraints are feasible, then we can conclude that t does not provide enough redundancy to satisfy the reliability constraints. on the other hand, when ilp-ar provides an optimal topology, the solution will satisfy the reliability requirement with an approximation error which is worst case bounded by (8) ."
"dermal regeneration scaffolds regulate the tissue and cells' regeneration as an external force during the process of extension and growth of cells in the material. these materials are capable of not only inducing tissue regeneration and vascularization but can also inhibit the hyperplasia. studies have proven that collagen-based scaffolds possess many interesting properties but also that they possess weak mechanical properties for skin tissue engineering applications [cit] . many approaches, such as physical/chemical crosslinking, have been developed to improve the biomechanical function of scaffolds, and the results seem obvious [cit] . some researchers have reported that a knitted mesh from artificial polymers is a good way to provide collagen-based scaffolds with excellent mechanical properties, and this type of hybrid scaffold has been successfully applied to repair cartilage, ligaments, tendons, blood vessels, etc. [cit] . in our previous study, we found that mesh-reinforced collagen-chitosan scaffolds can resist contraction and promote cell infiltration, neotissue formation, and blood vessel ingrowth more effectively than collagen-chitosan scaffolds [cit] . based on these studies, we developed poly (l-lactide-co-glycolide) (plga) yarns knitted into mesh-reinforced collagen scaffolds to enhance the collagen scaffolds' mechanical properties. for skin tissue engineering, several types of meshes knitted by plga or poly(lactic acid-co-caprolactone) (placl) were integrated with collagen to fabricate dermal substitutes, and the primary results illustrated that using the knitted mesh as a \"skeleton\" improved the mechanical strength of the hybrid scaffolds and inhibited wound contraction [cit] ."
"to detect high-frequency components and thin vessels, patch-based discriminator network is proposed which penalize each patch and discriminate real or generated synthesized segmentation maps. this kind of approach treats each individual rectangular patch as a stand-alone image and results in the probability map on each patch. the final result against an image is obtained by averaging all patch based results. patch based discriminator processes input as a markov random field by assuming independence among all patches. the small size of the patch allows fast convergence of network and results in high-resolution segmentation maps."
"(ii) we propose two algorithms to decrease the complexity of exhaustively enumerating all failure cases on all possible graph configurations, i.e. integer-linear programming modulo reliability (ilp-mr) and integer-linear programming with approximate reliability (ilp-ar). ilp-mr lazily combines an ilp solver with a background exact reliability analysis routine. the solver iteratively provides candidate configurations that are analyzed and accordingly modified, only when needed, to satisfy the reliability requirements. conversely, ilp-ar eagerly generates monolithic problem instances in polynomial time using approximate reliability computations that can still generate estimations to the correct order of magnitude, and with an explicit theoretical bound on the approximation error;"
"to test the scalability of both the approaches, we designed eps architectures with an increasing number of components. in table ii, we report on the execution time of the ilp-mr approach using algorithm 2 (at the top) in comparison with the one obtained by a lazier approach, which only adds one additional path at each iteration between the load and a component with a minimal degree of redundancy. the dramatic reduction in time spent for reliability analysis (e.g., more than one day versus 3 min for a 50-node architecture) shows the advantage of using the analysis results to infer the number of required redundant paths, as proposed in algorithm 2. when this inference is feasible, ilp-mr outperforms ilp-ar (see solver times in table iii) for architectures with more than 40 nodes. on the other hand, as evident from table iii, once the optimization problem is generated for a given template, ilp-ar is more competitive for smaller architectures. yet, problems with several thousands of constraints, and including a realistic number of generators (normally less than 10), can still be formulated and solved in a few hours. we also observe that, because of the sparsity of the eps adjacency matrix, in this case study, it was possible to reduce the number of generated constraints, which is always smaller than the asymptotic estimation in section iv."
"presence of lesions and cotton wools in fundoscopic images mainly affect the local features and the thick vessels. other challenges are the presence of central reflex vessels and low contrasts. to address four types of challenges (central reflex vessels, cotton wools, low contrast, and lesions) in segmentation of fundoscopic images, the proposed model is able to segment out the retinal vessels in these challenging scenarios. by integrating l 1 norm in the main objective function, the generator network is able to detect low contrast and thin retinal vessels as shown in fig. 2 . the generator network learns the low contrast vessels whereas the discriminator network forces the model to learn the non-vessel pixels too by predicting a zero score. in this way, the entire model learns the structure and appearance of vessels simultaneously and the model is able to address the central reflex vessel problem. patch based discriminator network allows the model to capture thin vessels in the presence of lesions and cotton wools. in summary, the proposed generative adversarial network can effectively address the main challenging cases by learning generator and discriminator network alternatively and integrating a custom loss term."
"in this section, the brief classical bbo algorithm will be introduced which includes the theory and the important features. then, the improvement cosine migration rate model and mutation rate model will be discussed in detail. finally, the process of imbbo will be described by the pseudo-codes."
"all the animal experimental procedures were carried out under the zhejiang university animal care and use committee. adult sprague-dawley (sd) rats, male, aged 2 months and weighing 200 ± 8 g, were purchased from the experimental animal centre of zhejiang university. after the hair on the backs of the rats was trimmed and removed, the rats were anesthetized by an intraperitoneal injection of 3% pentobarbital sodium solution (sigma) at a dosage of 1.0 ml/kg. after local disinfection with 2.5% povidone iodine solution, the sterilized adm, pmcs and cs scaffolds (diameter 2.0 cm) were embedded into the subcutaneous tissue pockets of rats. at intervals of 1, 2, 4, and 8 weeks after the operation, the rats were killed with a lethal dose of pentobarbital sodium solution, and then the rat skin was incised and turned open along the midline on the back to expose the embedded objects. pictures of the implants were taken, and the tissue specimens were obtained and kept in 10% neutral formalin aqueous solution and liquid nitrogen for histopathological observation and molecular biological detection, respectively. at each time point, six parallel scaffolds were set for each group."
"a novel generative adversarial network based deep learning model has been proposed, that can potentially address segmentation of retinal blood vessels in fundoscopic images. training the generator network to learn small transitions in thin vessels and allowing the patch based discriminator to discriminate vascular and non-vascular pixels. results on publicly available datasets showed that the proposed model is competitive with current state-of-the-art techniques. averaging the patch based results over small patches of fundoscopic image and integration of additional loss term into the main objective function leverage and enhances the effectiveness of the proposed model. the model has the potential to probe the different patch sizes so that the influence of patch-based discriminator on segmentation performance can be better analyzed."
"the implants were harvested at different times for histological analysis to evaluate the tissue response to the three different types of scaffolds, and the results of he staining are shown in figure 3 . at week 1, the implants were easily distinguished from the host tissues in all the groups. inflammatory cells such as granulocytes and macrophages were observed at this stage. in the pmcs group, fibroblasts grew into the implants from the mesh side rapidly, and abundant ecm deposition was observed in the center of pmcs (figures 3a, d, g) . in the adm and cs groups, the infiltration of fibroblasts was mainly distributed around the scaffold with a thin ecm secretion ( figures 3a, b) . additionally, newly formed blood vessels in the pmcs were observed, while most areas in the center of the cs and adm did not exhibit cell or capillary infiltration ( figures 3a, b, c) . two weeks after implantation, the implants were closely integrated with the adjacent tissues, except in the adm group. the number of granulocytes decreased, and the number of macrophages increased in all three groups. the three-dimensional structures of pmcs were almost filled by cells, ecm and newly formed capillaries (figures 3d, g), which was not clear in the other two groups. meanwhile, blank areas without cell ingrowth remained in the cs and adm groups ( figures 3h, i ). at week 4, newly formed tissue became more abundant, and the number of macrophages showed down-regulation in all the groups. the degradation of the three types of scaffolds was more obvious than before, and their outlines were difficult to discriminate from the host tissues. especially in the pmcs group, the newly formed tissue integrated well with the surrounding tissues without an obvious gap ( figures 3d, g) . from weeks 4 to 8, cs and pmcs degraded completely, while some residual components remained in the adm group ( figures 3g-i )."
"tissue engineering has proven to be one of the most promising alternative therapies for wound healing and tissue regeneration [cit] . in this study, scaffold degradation and in situ tissue regeneration can be observed simultaneously. in the in vivo microenvironment, the degradation of scaffolds is a complex process involving hydrolyzation and biodegradation. cs required no less than 4 weeks to be assimilated by the host tissue, in contrast to adm and pmcs [cit] . in the pmcs group, the outlines of plga fibers remained relatively intact at weeks 1 and 2, displayed obvious degradation phenomena with fiber fracturing and crushing, and almost degraded completely with a little residue surrounded by macrophages and neotissue at week 8, which resulted in a stronger resistance to contraction in the early stage than that of cs (figure 3a, d, g) . almost 8 weeks later, the outline of the adm was still visible with he ( figure 3c, f, i ) and masson's staining ( figure 4c, f, i ). fibroblast infiltration and collagen secretion in the 3 types of implants were visualized well by he and masson's staining. from week 1 to 2, the pmcs group promoted the fastest cell infiltration and ecm formation, especially in the portion near the mesh side, which resulted from the rapid angiogenesis described above. abundant and ordered collagen deposition was closely related to the up-regulation of fibroblast function in the local microenvironment. it has been reported that cell activity is mainly regulated by the specific surface area (ssa) in the scaffold, from which the space distribution is determined by the geometrical structure of the constructs [cit] . in the in vivo complex biomechanical environment, the existing knitted mesh in the pmcs can maintain a geometrical microstructure and ssa distribution suitable for fibroblast activity to reach a higher level of tissue regeneration induced by the hybrid scaffold. scaffolds should possess suitable mechanical properties to maintain 3d porous structures for tissue ingrowth and provide temporary mechanical support until the regenerated tissue can support mechanical loads."
"an ilp is solved in a loop with a reliability analysis routine. solveilp generates minimum cost architectures for the given set of interconnection constraints. the relanal-ysis routine computes the probability of composite failure events at critical nodes, starting from the failure probabilities of the components, a problem known as k-terminal reliability problem in the literature [cit] . to do so, we implement a modified depth-first search algorithm to traverse the graph g from node i (root) to the source nodes (leaves), by applying a path enumeration method, and by turning event relations as (5) into probability expressions. however, any other exact reliability analysis method for directed graphs can also be used [cit] . although the k-terminal reliability problem is nphard, the key idea is to solve it only when needed, i.e. a small number of times, and possibly on smaller graph instances."
"the overall ilp-ar approach is illustrated in algorithm 3. to implement genilp-ar, we use the approximate algebra to capture all the reliability requirements. while (7) is a nonlinear expression, a linear encoding of the same constraint can be obtained as follows"
"where e lj di is the edge from node l j to node d i, and the inequality turns into an equality when one and only one connection is admitted. moreover, we can state that if there exists an interconnection from any node in l to a node d j in d, then d j must be connected to at least one node in b, using a constraint of the form:"
"the main process of imbbo is similar as bbo. the pseudo-codes are as in fig.2 . further, in order to preferably satisfy the vmcp and performance, the improved migration model, including immigration and emigration rate calculation, is modified in line 7 and 9. in addition, the improved mutation model is also modified in line 18."
"the se reduction obtained by both the standard lms algorithm and the modified algorithm as applied to a staalgorithm h sults have shown that the standard lms algorithm removes the noise from the signal, the next section. the timing analyzer has showed or t 17 m noi tionary signal composed by 3 frequencies, corrupted by a random gaussian noise, with signal to noise ratio of 5.86 db were studied. both algorithms used 16 bit fixed point representation for data and filter coefficients [cit] . the frequency spectrum of the original signal, standard lms, and modified lms filter are given in figure 19 . the modified lms used a dynamic learning rate coefficient α based on a heuristic function formerly proposed by widrow [cit], and consisted of 1/n decaying function, coefficients were approximated by a piecewise linear curve, starting from the value 0.1 down to 0.001 (in about 1000 aster converthe standard lms used a static learning rate with the best performances obtained by setting the µ parameter equal . the two algorithms reported noise attenuation ater than 40 db and 36 db respectively. as can be n from the two learning characteristics in figure 20 steps). this heuristic function achieved a f gence, and les gradient noise. it has proved to be effective when applied to stationary signals. on the other hand to 0.05 gre see rithms nd noises showed similar simulation results. the adaptive noise filtering was implemented using a 16 bit 2's complement fixed point representation for samples and weights. as it can be seen in figure 5, the floor planned design required 1776 slices (logic blocks) of 3072 available (about 57%), and allowed a running clock frequency of 50 mhz (with a non optimized, fully automatic place & route process). it would require 2750 slices (89%) and would run at less than 25 mhz (due mainly to routing congestion). the assembly file used for the simulation is given in appendix a. the assembly code is provided elsewhere [cit] ."
"over the past ten years, the vehicular propagation channel has attracted the interest of numerous researchers provided that new frequency bands have been allocated for vehicular systems [cit] . nevertheless, the vehicle-to-vehicle (v2v) communications channel presents special peculiarities with regard to the fixed-to-mobile (f2m) scenarios, that is, the considerable mobility of both the transmitter (tx) and receiver (rx) terminals and the interacting objects such as reflectors and/or scatterers [cit] . in addition, the low elevation of the antennas determines a significant obstruction probability of tx-rx links, particularly in urban environments with dense road traffic. also, the propagation mechanisms in the dedicated short-range communications (dsrc) band at 5.9 ghz are very different from those occurring in the traditional cellular communications bands, that is, from 1 to 2 ghz. therefore, as a part of the narrowband channel characterization, accurate models for the small-scale fading distribution in v2v propagation channels are essential to develop, evaluate, and validate new protocols and system architecture configurations [cit] ."
"the architecture of the adaptive noise filtering based on the modified lms algorithm is shown in figure 8 . it was designed to implement 32 tap adaptive noise filter in a medium density fpga device. it has a modular and scalable structure composed by 8 parallel stages, each one capable of executing 1 to 4 multiply and accumulate (mac) operations and weights update. by controlling the number of operation performed by each block it is possible to implemen range from 8 to 32. in the first case, by exploiting max mum parallelism, the filter is capable of processing a data sample per clock cycle. in the other cases 2 to 4 clock cycles are requested. some fpga's internal ram blocks were used to implement the tap delays and to store weights coefficients. each weights update block is mainly composed by an adder/subtractor accumulator. the weights update coefficients δ i are computed by a separated block, which also handles the learning rate update function, following the above mentioned heuristic algorithm, and implements its multiplication with the error signal. by slightly modifying this unit, a more sophisticated adaptive function, can be easily obtained, thus enhancing the performances of the adaptive noise filtering for non stationary signals."
"the mean standard deviation, the skewness, and the kurtosis [cit] of the parameters of such distributions in logarithmic units are summarized in table 2 . the skewness is an estimation of the asymmetry of the probability distribution and it is defined aŝ1"
"as discussed in the previous sections, we have shown the differences between the dsp processors and fpgas. as far as power and cost are considered, dsp processors in general have lower power consumption, which makes them suitable for battery powered applications. these applications can be done on audio applications. these voice applications are very straight forward and do not require sophisticated pipeline and parallel moves. audio applications can be different filter applications. these are used especially in the voice transmission lines and cell phones. when it comes to the high frequency applications, dsp processors have some restrictions on their part when they are compared to the fpgas. in high speed applications, fpga's are much faster than the dsp processors. when it comes to high speed applications, the dsp boards have some limitations when compared to the fpgas. fpgas can offer more channels, and thus when cost per channel is considered because fpgas can offer more channels, the cost per channel is lower than the dsp's. also the partitioning of the fpga's can offer more throughputs as compared to dsp processors. thus fpgas can handle multiple tasks when their controls and finite state machines are configured correctly."
"a total of 8 routes were measured in different environments around the city of valencia, spain. a map describing all the routes is depicted in figure 1, and two photos of highway and urban environments, respectively, are illustrated in figure 2 . also, more details about the measurements are presented in table 1 . routes #1-#3 correspond to highway with heavy road traffic density and two or three lanes in each direction. both sides of the highways are open areas. routes #4-#8 are urban streets with several conditions. route #5 corresponds to narrow streets of the city of valencia with one lane, whereas routes #4 and #6-#8 are wide streets with at least three lanes in each direction."
"fpgas are ideally suited for the implementation of adaptive filters. however, there are several issues that need to be addressed. when performing software simulations of adaptive filters, calculations are normally carried out with floating point precision. unfortunately, the resources required of an fpga to perform floating point arithmetic are normally too large to be justified. a the filter tap itself. numerous techniques have been devised to efficiently calculate the convolution when the filter's coefficients are fixed in advan nother concern is operation ce. for an r time, these ugh computing floating point arithmetic in fpga is d with the inclusion of costly in terms of decidecimal places is adefor a given algorithm to s only four bits. for simple convolution, then dividing the output adaptive filter whose coefficients change ove methods will not work or need to be modified significantly [cit] . the reconfigurable filter tap is the most important issue for high performance adaptive filter architecture, and as such it will be discussed at length."
"the results derived from the application of the k-s test have shown that the nakagami-, rice, weibull, and -distributions can match satisfactorily the empirical distribution associated with the measurements. nevertheless, the -distribution exhibits a better fit compared to the 7 other distributions, making its use interesting to model the small-scale fading in v2v channels, where the large channel variations due to the mobility of tx and rx terminals, together with the other interacting objects, make it difficult to separate both the small-and large-scale fading. it is also worth noting that the weibull distribution provides the best match for the lower tails of the empirical distribution."
"as a consequence the weights are updated using a factor proportional to the error and the sign of the current reference noise sample, instead of its value. this implies that weights can be updated by using an addition (or subtraction) instead of a multiplication. this simplified algorithm requires only n multiplications and 2n additions. however the simplification of the weights update rule usually results in worse learning performances, i.e. in a slower adaptation ca learning α has been used. generally this can be done by updating it with an adaptive rule, or, by using a heuristic function. simulations of the above mentioned method shows that a dynamic learning rate gives an advantage not only in the learning characteristics, but also in the accuracy of the final solution (in term of improvement of the signal to noise ratio of the steady state solution). the product αe i is used to update all weights; only one additional multiplication is required."
"compared the [n-k] index of the coefficient indicates th produce equivalent output only when the don't change with time. this means architecture is used, the lms algorithm will not con verge differently from the direct implementation i [cit] . the change needed was to account for the weights as shown in equation er actually aditional lms gh, due to the sposed form fir, when quired."
"a g come with a hardware cost. the flexibility comes with a great number of gates, which means more silicon area, more routing and higher power consumption. dsp processors are highly efficient for common dsp tasks, but the dsp typically takes only a tiny fraction of the silicon area, which is dedicated for computation purposes. most of the area is designated for instruction codes and data moving. in high performance signal processing applications like video processing, fpgas can take highly parallel architectures and offer much higher throughput as compared to dsp processors. as a result fpga's overall energy consumption may be significantly lower than dsp processors, in spite of the fact that their chip level power consumption is often higher. dsp processors can consume 2-3 watts, while the fpgas can consume in the order of 10 watts. the pipeline technique, more computation area and w channels at the sam channel is significantly less in the fpga's [cit] . dsps are specialized forms of microprocessor, while the fpga's are form of highly configurable hardware. in the past, the usage of dsps has been nearly ubiquitous, but with the needs of many applications outstripping the processing capabilities (mips) of dsps, the use of fpgas has become very prevalent. it has generally come to be expected that all software, (dsp code is considered a type of software) will contain some bugs and that the best can be done is to minimize them. common dsp software bugs are caused because of, failure of interrupts to completely restore processor state upon completion, non-uniform assumptions regarding processor resources by multiple engineers simultaneously developing and integrating disparate functions, blocking of critical interrupt by another interrupt or by an uninterruptible process, undetected corruption or non-initialization of pointers, failing to properly initialize or disable circular buffering addressing modes, memory leaks, the gradual consumption of available volatile memory due to failure of a thread to release all memory when finished, dependency of dsp routines on specific memory arrangements of variables, use of special dsp \"core mode\" instruction options in core, conflict or excessive latency between peripheral accesses, such as dma, serial ports, l1, l2, and external sdram memories, corrupted stack or semaphores, subroutine execution times dependent on input data or configuration, mixture of \"c\" or high-level language subroutines with assembly language subroutines, and pipeline restrictions of some assembly instructions [cit] . both fpga and dsp implementation routes offer the option of using third party implementation for common signal processing algorithms, interfaces and protocols. each offers the ability to reuse existing ip in the future designs. fpga's are more native implementation for more dsp algorithms. figures 21 and 22 give the block diagrams of the dsp and fpga respectively. motorola dsp56300 series can only do one arithmetic computation and two move instructions at a time. however, in the case of fpgas, each task can be computed by its own configurable core and designated input and output interface."
where μ is the learning rate parameter; its purpose is to control the speed of the adaptation process. the lms rithm i onvergent in the mean square provided in equation (10) .
"the dsp system consists of two analog-to-digital (a/d) converters, and two digital-to-analog converters (d/a) converters. the dsp56303evm evolution module is used to provide and control the dsp56300 processor, the two a/d converters, and the two d/a converters. the left analog input sig sired in t sig-5 m ms to compute the f nal x(t) consists of the de pu nal s(n) plus a white noise signal w(n). the left analog input signal x(t) is first digitized using the a/d converter on the evaluation board. dsp processor executes the adaptive filter algorithm to process the left digitized input signal x(n), the left and right output signals y 1 (n) and y 2 (n) will be generated. the left output signal y 1 (n) is the error signal. the right output signal y 2 (n) is the filtered version of the left digitized input signal x(n), which is an estimate of the desired input signal s(n). the two d/a converters on the evaluation board are then used to convert the left and right digital output signals y 1 (n) and y 2 (n) to the left and right analog output signals y 1 (t) and y 2 (t)."
"adaptive noise filtering techniques are applied to low frequency like voice signals, and high frequency signals such as video streams, modulated data, and multiplexed data coming from an array of sensors. unfortunately in all high frequency and high speed applications, a software implementation of the adaptive noise filtering usually doesn't meet the required processing speed, unless a high end dsp processor is used. a convenient solution can be represented by a dedicated hardware implementation using a field programmable gate array (fpga). in this case the limiting factor is represented by a number of"
"altho possible, it is usually accomplishe custom floating point units, which are logic resources. therefore, a small number of floating point units can be used in the entire design, and must be shared between processes. this does not take full advantage of the parallelization that is possible with fpgas and is therefore not the most efficient method. all calculation should therefore be mapped into fixed point only, but this can introduce some errors. the main errors in dsp include adc quantization error, coefficient quantization error, overflow error caused impermissible word length, and round off error. the other three issues will be addressed later."
t was corrupted by a higher frequency sinusoid and random gaussian noise with a signal to noise ratio of 5.86 db. the input signal can be seen in figure 17 . a direct form fir filter of length 32 is used to filter the input signal. the adaptive is trained with the lms algorithm with a learning rate
"adaptive filters have the ability to adjust their own parameters and coefficients automatically. hence, their design requires little or no prior knowledge of the input signal or noise characteristics of the system. adaptive filters have two inputs, x(n) and d(n), which are usually correlated in some manner. figure 1 gives the basic concept of the adaptive filter. the filter's output y(n), which is computed with the parameter estimates, is compared with the input signal d(n). the resulting prediction error e(n) is fed back through a parameter adaption algorithm that produces a new estimate for the parameters and as the next input sample is received, a new prediction error can be generated. the adaptive filter features minimum prediction error. two aspects of the adaptive filter are its internal structure and adaptation algorithm. its internal structure can be either that of a nonrecursive (fir) filter or that of a recursive (iir) filter. an adaptation algorithm can be divided into two major classes; gradient algorithms and nongradient algorithms. a gradient algorithm is used to adjust the parameters of the fir filter. the least mean square (lms) algorithm is the most widely applied gradient algorithm. this adjusts the filter's parameters to minimize the mean-square error between the filter's output y(n) and the desired response input d(n) [cit] . when an adaptive filter is implemented on the dsp56300 processer, address pointer to mimic fifo (first-in-firstout)-like shifting of the ram data, modulo addressing capability to provide wrap around data buffers, multiply/accumulate (mac) instruction top both multiply two operands and add the product to a third operand in a single instruction cycle, data move in parallel with the mac instructions to keep the multiplier running at 100% capacity and repeat next instruction (rep) to provide compact filter code are being used by the processor. the processor's capability to perform modulo addressing allows an address register (rn) value to be incremented (or decremented) and yet remain within an address range of size l, where l is defined by a lower and an upper boundary. for the adaptive fir filter, l number of coefficients (taps). the value l-1 is stored in the processor's modifier register (mn). the upper address boundary is calculated by the processor and is not stored in a register. when modulo addressing is used, the address register (rn) points to a modulo data buffer located in x-memory and/or y-memory. the address pointer (rn) is not required to point at the lower address boundary; it can point anywhere within the defined modulo address range l. if the address pointer increments past the upper address boundary (base address plus l-1 plus 1), it will wrap around to the base address. modulo register m1 is programmed to the value ntaps-1 (modulo ntaps). address register r1 is programmed to point to the state variable modulo buffer located in x-memory. modulo register m4 is programmed to the value ntaps-1. address register r4 is programmed to point to the coefficient buffer located in y-memory. given that the fir filter algorithm has been executing for some time and is ready to process the input sample x(n) in the data alu input register x0, the address in r4 is the base address (lower boundary) of the coefficient buffer. the address in r1 is m, where m is greater than or equal to the lower boundary of x-memory address and less than or equal to the upper boundary of x-memory address. the x-memory map for the filter states, the y-memory map for the coefficients, and the contents of the processor's a and b accumulators and data alu input registers x0, x1, y0 and y1 are shown in the figure 2 . the clr instruction clears the a-accu-tim y1 and the error sample e(n) to the data in mulator and simultaneously moves the input sample x(n) from the data alu's input register x0 to the x-memory location pointed to by address register r1, and moves the first coefficient from the y-memory location pointed to by address register r4 to the data alu's input register y0. both address registers r1 and r4 are automatically incremented by one at the end of the clr instruction (post-incremented). the rep instruction regulates execution of ntaps-1 iteration of the mac instruction. the mac instruction multiplies the filter state variable x0 by the coefficient in y0, adds the product to the a-accumulator and simultaneously moves the next state variable from the x-memory location pointed to by the address register r1 to the input register x0, and moves the next coefficient from the y-memory location pointed to by address register r4 to input register y0. both address registers r1 and r4 are automatically incremented by one at the end of the mac instruction (post-incremented)."
"the matching in the lower tails of the estimated to the experimental distributions affects significantly the wireless performance parameters, such as the bit error rate (ber) and the outage probability [cit] . therefore, in addition to the k-s test we need to assess the behaviour of the distribution specifically in the lower tails by using a procedure which evaluates the mismatch between the estimated and the experimental cdfs."
"a suitable compromise for dealing with the loss of precision when transitioning from a floating point to a fixedpoint representation is to keep a limited number of mal digits. normally, two to three quate, but the number required converge must be found through experimentation. when performing software simulations of a digital filter for example, it is determined that two decimal places is sufficient for accurate data processing. this can easily be obtained by multiplying the filter's coefficients by 100 and truncating to an integer value. dividing the output by 100 recovers the anticipated value. since multiplying and dividing be powers of two can be done easily in hardware by shifting bits, a power of two can be used to simplify the process. in this case, one would multiply by 128, which would require seven extra bits in hardware. if it is determined that three decimal digits are needed, then ten extra bits would be needed in hardware, while one decimal digit require multiplying by a preset scale and by the same scale has no effect on the calculation. for a more complex algorithm, there are several modifications that are required for this scheme to work [cit] . the first change needed to maintain the original algorithm's consistency requires dividing by a scale constant any time and previously scaled values are multiplied together. consider, for example, the values a and b and the scale constant s, the scaled integer values are represented by a s  and b s  . to multiply theses values requires dividing by s to correct for the s 2 term that would be introduced and recover the scaled product"
"the algorithm for adaptive filtering were coded in matlab experimented to determine optimal parameters such the learning rate for the lms algorithm. after the para ters have been determined, algorithms were coded for xilinx in vhdl language."
the training algorithms for the adaptive filter need some minor modifications in order to converge for a fixedpoint implementation. changes to the lms weight update equation were discussed in the previous section.
"the total driven distance for each route and the maximum tx-rx separation distance are shown in table 1 . it is worth noting that both tx and rx were moving in convoy, that is, in the same direction, during the measurement acquisition. the maximum tx-rx separation between both vehicles ranged from 72.2 m (in route #5) to 440.53 m (in route #1)."
"once the estimated parameters of the rayleigh, nakagami-, weibull, rice, and -distributions were calculated, we have compared such distributions to the experimental distribution in each window."
"ultipliers. moreover experimental data showed that the modified algorithm achieves the same or even better performan the standard lms version. there are many possi ost ir) digital filter, whose coefficients are iteratively updated multiplications required by the adaptive noise cancellation algorithm. by using a novel modified version of the lms algorithm, the proposed implementation allows the use of a reduced number of hardware m ces than ble implementations for an adaptive noise filter, but the m widely used employs a finite impulse response (f using the lms algorithm. the algorithm is described in equations (24) to (26), leading to the evaluation of the fir output, the error, and the weights update."
"here, c ss and c nn denote the covariance matrices of signal and noise, respectively. notice that if x has a non-zero mean vector, μ, equation e becomes:"
"the continuous analog signal was sampled at a rate of twice the highest frequency present in the spectrum of the sampled analog signal in order to accurately recreate the analog audio signal from the discrete samples. the analog audio signal was mixed with noise using a sum block which is bound to occur when the audio signal passes through the channel. the noise however, first low pass passed filter using a finite impulse response filter to make it finite in bandwidth. fir noise filter was observed to have little or no significant effect on the signal with noise. the information bearing signal is a sine wave of sample cycles 055 . 0 is shown in figure 12 . the noise picked p by the secondary microphone is the input for the adapu tive filter as shown in figure 13 . the noise that corrupts the sine wave is a low pass filtered version of the noise. the sum of the filtered noise and the information bearing signal is the desired signal for the adaptive filter. the noise corrupting the information bearing signal is a filtered version of noise as shown in the figure 14 . figure 15 shows that the adaptive filter converges and follows the desired filter response. the filtered noise should be completely subtracted from the signal noise combination and the error signal should only have the original signal. the results can be seen in figures 12 to 16 ."
"adaptive noise filters have been implemented on dsps and fpgas. motorola dsp56303 has been used for dsp platform, while xilinx spartan iii boards are used to implement fpga adaptive noise filtering. matlab simulink has been used to test the effectiveness and correctness of the adaptive filters before hardware implementation."
"in this section, the parameters of the nakagami-, weibull, rice, and -distributions are estimated. the goodness-offit of these distributions is calculated by using the k-s test [cit] . then, the assessment of the matching of the previous distributions to the experimental distribution is carried out for the lower tails of the distribution, using the difference between the experimental and estimated cdfs."
"on the other hand, the smallest values of the standard deviation correspond to the parameter for the weibull distribution with a maximum of 3.12 db in route #2."
"a few years ago, the -distribution was proposed to model the small-scale fading amplitude in inhomogeneous scattering field environments [cit] . nevertheless, to the best of the authors' knowledge, the -distribution has not been used to estimate the small-scale distribution in v2v communications yet. this paper estimates the parameters of the rayleigh, rice, weibull, nakagami-, and -distributions to analyze the small-scale fading in v2v from a narrowband channel measurements campaign designed specifically for that purpose. the parameters of the distributions considered have been estimated by statistical inference. the results derived from the application of the k-s statistical test have shown that the nakagami-, weibull, rice, and -distributions can match satisfactorily the experimental (empirical) distribution. as a novelty, our analysis shows that the -distribution exhibits a better fit compared to the other distributions, making its use interesting to model the smallscale fading in v2v channels, where it is difficult to separate both the small-and large-scale fading effects as a consequence of the large channel variations."
"s discussed in the previous chapters, the concept of the adaptive noise filtering applications can be implemented in both dsp processors like motorola dsp56300 series and also in the field programmable gate array such as xilinx spartan iii boards. in high performance signal processing applications, fpgas have several advantages over high end dsp processors. literature survey has showed that high-end fpgas have a huge throughput advantage over high performance dsp processors for certain types of signal processing applications. fpgas use highly flexible architectures that can be greatest advantage over regular dsp processors. however, fp as ith more gates fpgas can process more e time. thus power consumption per a"
"during the execution of the filter algorithm, address register r4 is post incremented to a total of ntaps es; once in conjunction with the clr instruction and ntaps-1 times (due to the rep instruction) in conjunction with the mac instruction. since the modulus for r4 is ntaps and r4 is incremented ntaps times, the address value in r4 wraps around and points to the coefficient buffer's lower boundary location [cit] . also address register r1 is post incremented to a total ntaps times; once in conjunction with the clr instruction and ntaps-1 times (due to the rep instruction) in conjunction with the mac instruction. also at the beginning of the algorithm, the input sample x(n) is moved from the data alu input register x0 to the x-memory location pointed to by r1. since the modulus for r1 is ntaps and r1is incremented ntaps times, the address value in r1 wraps around and points to the state variable buffer's x-memory location m. the macr instruction calculates the final tap of the filter algorithm and performs convergent rounding of the result. the data move portion of this instruction loads the input sample x(n) into the b-accumulator. at the end of the macr instruction, the accumulator contains the filter output sample y(n) as shown in figure 3 ."
"in this work, an extensive analysis of the small-scale fading distribution in v2v channels has been performed. the analysis is based on a narrowband measurement campaign carried out in both highway and urban environments. the parameters of the rayleigh, rice, weibull, nakagami-, and -distributions have been estimated from the measurements by statistical inference. the goodness-of-fit of the theoretical distributions to the empirical distribution has been evaluated using the k-s statistical test. in all the highway routes assessed, the mean of the rician -parameter exceeds 10 db, which corresponds to situations with dominant mpcs. on the contrary, the results show that the mean of the rician -parameter ranges from 4.21 to 8.82 db in urban environments."
"tool ise vements visually that is proposed by das, the adaptive filt lee and das has been compares thro (see figure 9) the target simulink model is responsible for code generation where as the host simulink model is responsible for testing. the host drives the target model with heavy wavelet noisy test data consisting of 4096 samples generated from wnoise function in matlab. matlab's fda is used for designing the bandpass filter to color the no source. a colored gaussian noise is then added to the input test signal. this noisy signal and the reference noise are inputs to the terminal of the lms filter simulink block. iltering of heavy sine noisy signal consisting of 4096 samples per frame. figure 11 shows the comparison between the das proposal of the wiener filter and the lee's wiener filter proposal in the signal to noise ratio aspect. as it can be seen from the figure 11 the performance for the das proposal is higher than the lee's wiener filter. the improved adaptive wiener filter provides snr improvement from 2.5 to 4 db as compared to lee's adaptive wiener filter."
the efficient realization of complex algorithms on fpgas requires a familiarity with their specific architectures. the modifications needed to implement an algorithm on an fpga and also the specific architectures for adaptive filtering and their advantages are given below.
"likewise, division must be corrected with a subsequent multiplication. it should now be evident why a power of two is chosen for the scale constant, since multiplication and division by power of two results in simple bit shifting. addition and subtraction require no additional adjustment. the aforementioned procedure must be applied with caution, however, and does not work in all circumstances. while it is perfectly legal to apply to the convolution operation of a filter, it may need to be tailored for certain aspects of a given algorithm. consider the tap-weight adaptation equation for the lms algorithm in equation (9)."
"according to our study, the final conclusion is that for simple audio applications like adaptive noise cancelling, motorola dsp56300 is more beneficial, because the requirements for audio applications are met with dsp processors. also they are more power efficient and can devices. but when adaptive in high speed applications cal signal processbe used for battery powered noise filtering is considered like video streaming and multiplexed array signals, fpga's are offering a faster approach and thus they are more suitable for high frequency applications."
"in the above equations, x i is a vector containing the reference noise samples, d i is the primary input signal, w i is the filter weights vector at the i th iteration, and e i is the error signal. the µ coefficient is often empirically chosen to optimize the learning rate of the lms algorithm. the hardware implementation of the algorithm in an fpga device is not trivial, since the fir filter has not constant coefficients, so multipliers cannot be synthesized by using a look-up table (lut) based approach. this however, should be straightforward in fpga architecture. multipliers with changing inputs instead need to be built by using a significantly greater number of internal logic resources (either elementary logic blocks or embedded multipliers). in an nth order filter the algorithm requires at least 2n multiplications and 2n additions. note the factor 2µ that is usually chosen to be a power of two in order to be executed by shifting. this makes it impractical for fully parallel hardware implehe value of n grows. this mentation of the algorithm as t is due to the huge number of m der to reduce the complexity of weights update expression (equation as pability of the filter. to overcome this weakness, and significantly improve the characteristics, a dynamic learning rate coefficient t an adaptive filter whose order can iultipliers required. in orthe algorithm, the (26)) is simplified in equation (27)."
"specifically, the learning rate µ and all other constants should be multiplied by the scale factor. when µ is adju rm in equation (11) . with µ modifica sted it takes the fo tion weight update equation (11) can be modified as in equation (12) . figure 6 r structure is shown in figure 6 and the output y at any time n is given by equation (13), where nodes b and c are described respectively. the direc rmined by the depth of the output adder pendent on the filter's order. the transposed f ier a e other hand, has a delay of only one multipl der, regardless of the filter length. it is therefore a ntageous to use the transposed form for fpga impl mentation to achieve maximum bandw shows the direct and figure 7 shows the transposed fir structures for a three tap filter. the relevant nodes have been labeled a, b and c for a data flow analysis. each filter has three coefficients, and are labeled h 0 [n], h 1 [n] and h 2 [n] . the coefficients' subscript denotes the relevant filter tap, and the n subscript represents the time index, which is required since adaptive filters adjust their coefficients at every time instance."
"speed is one of the most important concepts that determine the computation time and also it is one of the most important concepts in the market. in the adaptive filters the parameters are updated with the each iteration and after the each iteration the error between the input and the desired signal get smaller. after some number of iterations the error becomes zero and the desired signal is achieved. according to the specifications from the manufacturer manuals, motorola dsp56300 series has a cpu clock of 100 mhz, but this speed depend on the instruction fetch, computation speed and also the speed of th audio codec runs on 24.57 mhz, this clock speed is determined by an external crystal. in the other hand xilinx spartan 3 has the maximum clock frequency of 125 mhz, but this speed can be reduced because of the number of instruction ns, gates and the congestion on the routing of the signals. both of the modified adaptive noise filtering applications take about 200-250 iterations to cancel the noise and achieve the desired signal. in the motorola dsp processor case because of the actual clock speed being lower, causality conditions and the speed limitation that is coming from the audio codec part of e board, the running time is 20 mhz. e clock to be faster."
"e peripherals. on the dsp56303evm board the th of the modified lms algorithm in the case of the fpga's the running speed is around 50 mhz. this due to discussions from the previous section, which is fpga's flexibility and reconfigurable gates allows for th"
"in the future, the adaptive noise filtering can be implemented on high frequency applications, such as noise removal from video streaming and noise removal from multiplexed data arrays. these applications may be applied first to fpgas with verilog hdl or vhdl. after application has been verified, hardware code can be converted to a net list and thru synopsys a custom asic design can created. the asic design and fpga design may be compared in the aspect of cost, power, architecture, noise removal and speed. these comparisons would be helping us to provide us a more educated choice for future applications."
"we train multiple classifiers on different sets of lexical and syntactic features after performing feature selection based on univariate regression between each feature and the outcome. we also perform dimensionality reduction by randomized principal component analysis (pca). after evaluating a number of classification models available in pythons sklearn package, we report the results from the logistic regression classifier, with l2 penalization, and inverse of regularization strength set to 0.1, which obtain the best results. although we have experimented with a variety of feature combinations which are included in the appendix, we discuss selected classifiers (for meaningful comparisons) as well as the best-performing ones."
"(a) (b) ( c) figure 3 . boundary area percentage examples using 10% (a), 25% (b) and 75% (c) of the lesion area for analysis (white region). (this is the same lesion as in figure 1 )."
"to train and test the classifiers for the stage 1 and 2, we use the differential language analysis toolkit 2 (dlatk), a python package developed for social media text analysis. lexical features are extracted in dlatk and syntax-based features are separately generated and imported into dlatk feature sets."
"normalized frequency distribution over 1-, 2-and 3-grams liwc: frequency distribution of categories from the linguistic inquiry and (liwc) [cit] . anew: weighted frequency distribution of the dominance score in the affective norms in english words (anew) [cit] . word2vec topics: we use word clusters built on top of word2vec, by preoţiuc [cit] ."
"we describe a computational linguistic approach to identify the locus of the author's control in their social media writing. utilizing its psychological underpinnings, we create an annotated dataset of 4000 sentences, labeled with control relevance and of internal and external control. we show that identifying control is largely dependent on syntactic information for control-relevance and lexical information for internal/external control. from the nlp standpoint, this suggests that solely using bag-of-word features may not be sufficient for predicting specific psychological outcomes. we have also distinguished our work against dominance and thematic roles, which may be the closest approximations to loc; however these concepts do not completely translate into each other."
"(1) start with the lesion border (2) compute the distance transform using 8-connected distance to get pixel distance from the lesion boundary inside of the skin lesion (3) starting with a distance of 0 (lesion boundary), determine the lesion boundary region with distance less than or equal to the current distance (4) compute the area of the resulting lesion boundary region (5) retain the lesion boundary region if its area is greater than the boundary area percentage (6) otherwise, increment the distance by 1 and repeat steps (2)- (6) steps (5) and (6) show that the process continues until the area percentage criterion has been satisfied. figure 3 shows examples of boundary area percentages of 10%, 25% and 75% of the lesion area; only the areas shown in white are used for feature calculations; the remaining lesion interior is excluded from feature calculations."
"the relatively low diagnostic accuracy for discrimination of malignant melanoma demonstrates the uncertainty involved in skin lesion analysis. this low diagnostic accuracy is an important medical problem for society, since the incidence of cutaneous melanoma, 161,790 [cit], vs. 140,860 [cit] . in more recent advances, techniques such as deep learning have shown potential using pixel blocks from lesion images with disease labels for significantly improved skin lesion discrimination, achieving an area under the diagnostic roc curve of approximately 0.91, exceeding the performance of dermatologists [cit] . even with the improvements in computer-assisted techniques for skin lesion classification, learning and application of specific features for skin lesion discrimination is important for clinicians and clinician training. therefore, the investigation of computer-assisted techniques for the determination and analysis of specific skin lesion features is useful in the dermatology community. in this research, we focus on automatic detection of melanoma colors."
"building a useful computational model requires labeled training data. we labeled the facebook dataset using three trained annotators pursuing a master's program in psychology, to construct the first public corpus annotated with control. we asked the annotators to determine whether the author of the sentence is in control (internal control) or being controlled by others or circumstances (external control). to ensure quality work, we provided examples corresponding to each point on the scale. the examples provided to the annotators are also provided in the annotation scheme described in the appendix."
"future work on fuzzy logic-based color analysis will extend this research to larger data sets and explore combinations of percentile-based features, e.g., decile-based calculated features, as well as features based on various radii."
"the color clustering ratio is based on using both melanoma and benign training set of images for populating a cumulative relative color histogram, probabilistic labeling of the relative color histogram bins as melanoma or benign colors with melanoma and benign color region growing within the histogram for final melanoma and benign color bin assignments. the clustering ratio feature is computed over the boundary region of interest to determine clustering of melanoma colors. for each melanoma pixel, i.e., for each pixel that maps as a melanoma color, the sum of eight-connected pixels that map as melanoma colors is calculated and added to the cumulative sum for all melanoma pixels in the boundary region. this total is divided by the total of all eight-connected neighboring pixels for all melanoma pixels in the region. applying the color clustering ratio to skin lesion discrimination in dermoscopy images yielded true positive, true negative, and overall classification rates as high as 87.7%, 74.9%, and 81.3%, respectively, for the 75% boundary area percentage case [cit] . the fuzzy logic-based approach improves the skin lesion discrimination capability over the crisp clustering ratio. the fuzzy logic-based approach achieves the true positive highest results in the outer 25% of the lesion, which is consistent with the color clustering ratio results obtained for clinical images [cit] ."
"(1) start with the lesion border (2) compute the distance transform using 8-connected distance to get pixel distance from the lesion boundary inside of the skin lesion (3) starting with a distance of 0 (lesion boundary), determine the lesion boundary region with distance less than or equal to the current distance (4) compute the area of the resulting lesion boundary region (5) retain the lesion boundary region if its area is greater than the boundary area percentage (6) otherwise, increment the distance by 1 and repeat steps (2)- (6) steps (5) and (6) show that the process continues until the area percentage criterion has been satisfied. figure 3 shows examples of boundary area percentages of 10%, 25% and 75% of the lesion area; only the areas shown in white are used for feature calculations; the remaining lesion interior is excluded from feature calculations."
"-control-relevant imperative sentences were frequently misclassified as control-irrelevant, e.g. \"have a good day everyone\"; \"find jesus.\"; \"stay focused!\" -internal loc sentences with modals and 'get' were misclassified as external loc, e.g., \"finally got a best man!\"; \"join me and you can save money on all you purchase here.\""
v includes all neighbors of pixels within the skin lesion that have a non-zero membership value in b for the specified lesion class. the fuzzy clustering ratio for a skin lesion is given as
"they built 200 open-ended word clusters by applying spectral clustering to the word-to-word similarity matrix from the neural embeddings mikolov et. [cit] . pronouns: we clustered all pronouns (except for possessives) into 1st-, 2nd-, and 3rd-person regardless of their syntactic role."
"diagnosis of early-stage melanomas is challenging, especially for melanomas which have an atypical presentation [cit] . dermoscopy is an imaging modality that uses a simple hand-held device that eliminates surface glare and magnifies structures invisible to the naked eye. although dermoscopy significantly improves diagnostic accuracy compared to naked-eye examination alone [cit], average dermoscopic melanoma sensitivity for dermatologists and others seeing dermatology patients in three recent reader studies was only 71-85% and specificity 54-71% [cit] . non-specialist sensitivity and specificity is even lower; when compared to dermatologists, non-specialists were found to have a sensitivity for melanoma of 54% and a specificity of 73% [cit] ."
"imaging of skin lesions is anticipated to have an important role as a diagnostic aid for malignant melanoma. in this research, the red, green and blue (rgb) color space of dermoscopy images was used to analyze skin lesions. in the rgb space each dimension (r, g and b) typically uses 8-bits per pixel, providing 256 possible values for each dimension. thus, there are a total of 256 3 or 16,777,216 possible colors in this color space."
"where x is the bin frequency in the secondary histogram and f is the empirically determined bin frequency count for full membership in the benign fuzzy set b. f is set as the frequency count given that 5% of the secondary histogram of the benign skin lesions have frequency f or greater. increased bin frequency (up to f) will be reflected as increased membership value in the specified class of skin lesion. figure 2 shows a representative secondary histogram with the frequency count f labeled in (a) and the trapezoidal membership function generated in (b). the horizontal axis provides the frequency of occurrence (x) that a bin is populated over all benign images of the training set. the vertical axis in figure 2a gives the number of bins with x \"hits\" per bin over the training set of benign images."
"internal locus of control has been argued to be important for mental and physical health, and to characterize well-run (\"empowered\") work teams; we hope that the model presented here can be used-with appropriate consent and privacy-for unobtrusive monitoring of loc in many therapy and work settings."
"several empirical relationships were utilized to determine non-skin color, pixels in deep shadow, direct flash reflection and exogenous marks are eliminated. an existing dermatology image database was used under the supervision and guidance of a dermatologist to derive the skin pixel finder used in this research. the skin pixel finder has been applied to skin lesion analysis in other research [cit] . to approximate the surrounding skin color, the distance function is used twice. first, a 10-pixel wide area surrounding the lesion is removed from consideration, to allow for possible inaccuracy in border determination. then the distance function is used to determine an annular region just outside the 10-pixel-wide region; the area of this outer annular region is 20% of the skin lesion size [cit], which is the area used for surrounding skin calculation. if the lesion extends close to the image edge, the remaining non-lesion pixels within this annular region are used rather than the standard 20% of lesion size. the pixels determined by this technique are used to determine the average surrounding skin color (r skin, g skin, b skin ) pixel values."
"the results found in the current study have relevance for clinicians. first, our results confirm results from a previous study that found that white or hypopigmented areas in the outer three deciles of the lesion were needed for the best discrimination of melanoma vs. benign [cit] . second, we have confirmed the clinical impression that colors within the lesion periphery, specifically the outer lesion deciles, are important in describing benign lesions, since a circumferential \"hazy border\", with coloration intermediate between that of skin and lesion, implies a benign lesion [cit] and a \"pink rim\" is a sign of a melanoma [cit] . third, the patches or clusters of color that matter most are larger than a single pixel but still relatively small, in the single-digit range, as in the \"peppering\" of early malignancies [cit] . thus a fuzzy logic-based description of lesion colors offers relevance to clinical descriptions of malignant melanoma."
"in this paper, we focus on stage 1 and stage 2 of annotation, which distinguish control-relevant sentences and classify them as either internal or external control. we evaluate the reliability of the annotations by computing percent-agreement. the highest pairwise inter-annotator agreement between annotators is 90.2% for stage 1 and 79% for stage 2."
"the results found in the current study have relevance for clinicians. first, our results confirm results from a previous study that found that white or hypopigmented areas in the outer three deciles of the lesion were needed for the best discrimination of melanoma vs. benign [cit] . second, we have confirmed the clinical impression that colors within the lesion periphery, specifically the outer lesion deciles, are important in describing benign lesions, since a circumferential \"hazy border\", with coloration intermediate between that of skin and lesion, implies a benign lesion [cit] and a \"pink rim\" is a sign of a melanoma [cit] . third, the patches or clusters of color that matter most are larger than a single pixel but still relatively small, in the single-digit range, as in the \"peppering\" of early malignancies [cit] . thus a fuzzy logic-based description of lesion colors offers relevance to clinical descriptions of malignant melanoma."
"helpful, despite their linguistic and semantic simplicity. among the syntax-based features, using the verb categories of sv and svos provide the second best f1 score (levin's classes are better than our classes), and adding self-reference ratio to verb categories generates the best results. interestingly, pos-ngrams perform on-par with the lexical ngrams, suggesting that the encoded syntactic information in pos-sequences is just as good as lexical information in ngrams with considerably less sparsity and more efficient computation."
"we operationalize the key aspects of locus of control described by existing psychology theories * masoud rouhizadeh & kokil jaidka co-lead this work. [cit] to identify an external locus of control when authors express that they feel controlled by other people or the environment. on the other hand, authors communicate an internal locus of control when they ascribe the control of their decisions and circumstances to themselves. this study poses the question: do writers rely more on content than syntax to convey their control? following its psychological underpinnings, we expect that overall lexical choice, self-reference, and certain verb categories would be indicative features for modeling author's locus of control. our study attempts to validate these assumptions by answering two research questions:"
the remaining sections of the paper include: (1) an overview of dermoscopy image acquisition; (2) the fuzzy logic technique for color histogram analysis and color feature determination; (3) an overview of the lesion region analysis; (4) the automated approach for melanoma detection; (5) experiments performed; (6) results and discussion and (7) conclusions.
"the best results for classifying internal vs. external control are in table 1b . unlike control relevance, ngrams are the most helpful features here and pronouns and verb category features result in a poor performance. liwc, anew, and w2v do not noticeably improve the results when combined with ngrams, but adding verb categories and self-reference ratio to this combination creates the best result. this suggests that identifying internal/external control benefits from an ensemble of syntactic, semantic and lexical features and simple lexical features are more helpful than word embeddings. similar to control relevance, posngrams are helpful here although they are not as good as lexical ngrams. table 2 shows the most predictive verb categories (with subject or subject-object arguments) based on logistic regression coefficient. we see that not only does the magnitude of the relationship change, but it is possible that the direction can completely change (positive values are control relevance in table 2a and external control in table 2b . interestingly, we see that verbs for audio/visual activities, demand, and start/end of processes, are more frequently used in control-related contexts, whereas using love and like verbs is an informative signal that a given sentence is not related to control. in addition, verbs of cognition, missing, feeling, and hope are positively associated with being controlled by others or the environment, whereas verbs indicating attempt are correlated with author's control of the situation. although not complete, these semantic categories are intuitively well-associated with the psychological theory of the control concept. in the appendix, we also identify the pos ngram patterns which are significantly correlated with both, control relevance and internal vs. external control."
"differential language analyses identified interesting associations between verbs categories and control. we found that audio/visual verbs tend to occur significantly more in the control-related text, and some emotion verbs such as \"miss\" or \"feel\" are correlated with lack of control of the surroundings. a caveat of using language-based models is that their association with traits is correlational, not causal. furthermore, the differences in platform affordances [cit] c) and diachronic drifts in language use over time [cit] ) imply that language models to identify control may need domain adaptation before they are applied on other corpora, language from other time periods and other social media platforms, as well as posthoc domain adaptation before they can scale to measure community traits [cit] ."
"let l denote the set of pixel locations within the skin lesion region of interest with relative colors roi rel(x,y) that map into m, formally"
the level of confidence of being in a skin lesion class b is given by the fuzzy clustering ratio feature. the fuzzy clustering ratio feature is computed as follows. let m denote the set of relative color values that map into relative color histogram bins labeled with
"existing psychological theories are mainly based on self-expressed and self-perceived locus of control in questionnaires, but they may be susceptible to self-report biases. instead, we demonstrate that some of these constructs can reliably be extracted from language samples, such as social media posts, which are unsolicited selfexpressions of control."
"language is a form of action, and written occurrences constitute a performance of power and control [cit] . research in natural language processing has long focused on distilling the constituents of writing that conveys authority [cit], dominance [cit], dogmatism [cit], expertise [cit] and politeness [cit] . these studies have shown how authors' use of certain lexical and syntactic patterns achieve specific rhetorical effects. in this study, we contribute to the growing literature with an analysis of how individuals express control, and compare the insights and predictive power obtained from both, lexical and syntactic features."
"we report the results for prediction performance of the main feature in table 1a . we see that lexical ngrams are moderately effective, whereas liwc, anew, and w2v features are not helpful. on the other hand, pronouns appear to be very table 1 : precision, recall and f1-measure for predicting control-relevance and internal/external control (10-fold cv) from lexical and syntactic features."
"relative color provides many advantages when used for analyzing skin lesions. first, differences resulting from variation in ambient light and their respected digitization errors are reduced when relative color is used. second, errors in digitizing images from different film types or different film processing techniques can be minimized by relative color. third, the mammalian visual system operates in a manner similar to the relative color technique. finally, skin color varies among persons and relative color tries to minimize this variation and produce a skin-tone indifferent system."
"imaging of skin lesions is anticipated to have an important role as a diagnostic aid for malignant melanoma. in this research, the red, green and blue (rgb) color space of dermoscopy images was used to analyze skin lesions. in the rgb space each dimension (r, g and b) typically uses 8-bits per pixel, providing 256 possible values for each dimension. thus, there are a total of 256 3 or 16,777,216 possible colors in this color space."
"the assymetric unit of the title salt compound contains two molecules of thiourea (a and b), three cations of in the crystal, the components are linked by a combination of thirteen n-h···cl, five n-h···s, one s-h···cl and two s-h···s hydrogen bonds into a three-dimensional structure. (fig. 2 and table. 1)."
"1. the specification does not have types. this is not a problem for tlc, since it constructs states on the fly and hence dynamically computes types. in the symbolic case, one can use type synthesis [cit] or the untyped smt encoding [cit] . 2. direct translation of next to smt would produce a monolithic formula, e.g., it would not analyze produce and consume as independent actions. this is in sharp contrast to translation of imperative programs, in which variable assignments allow a model checker to focus only on the local state changes."
"since propositions 5 and 6 hold, it would suffice to consider the set branches(φ), together with an assignment strategy, to generate symbolic transitions. however, it is often the case that, for two distinct branches br 1 and br 2, the same assignments in a are chosen, that is, the intersections br 1 ∩a and br 2 ∩a are the same. we show that one can reduce the number of considered symbolic transitions, by analyzing how various branches intersect a."
"the aim of this paper is to describe the details of our lens finding effort, present the sample of newly discovered lens candidates, discuss the relative performance of crowdsourcing and of the yattalens software and to suggest strategies for future searches. the structure of this work is as follows. in sect. 2, we describe the data used for the lens search, including the training set for crowdsourcing. in sect. 3, we describe the setup of the crowdsourcing experiment and the algorithm used for the analysis of the classifications from the citizen scientist volunteers. in sect. 4, we show our results, including the sample of candidates found with yattalens and highlighting interesting lens candidates of different types. in sect. 5, we discuss the merits and limitations of the two lens finding strategies. we conclude in sect. 6. all images are oriented with north up and east left."
"we then visually inspected the resulting sample with the purpose of refining the candidate classification. nine co-authors of this paper assigned to each candidate an integer score from 0 to 3, to indicate the likelihood of the subject being a strong lens. we used the following scoring convention:"
"if and only if a is a covering set for φ. it is easy to restrict coverings to the minimal coverings. to do this, we define the set of collocated labels, denoted colloc(φ), as"
"the 'optimal' threshold flux of the foreground-subtracted images was instead set to the minimum between the 90%-ile of the residual image and the flux corresponding to ten times the sky background fluctuation. we made this choice to highlight features close to the background noise level. as an example, we show the aforementioned four versions of the rgb images of a known lens, sl2sj021411−040502, in fig. 1 ."
"additionally, in order to ensure consistency in grading criteria across the whole sample and among different graders, we proposed the following algorithm for assigning scores."
"in addition to the documentation, the 'field guide' and the tutorial, we relied on training images to help volunteers sharpen their classification skills. participants were shown subjects, to be graded, interleaved with training images of lenses (known or simulated), known as 'sims' for simplicity, or of non-lens galaxies, referred to as 'duds'. they were not told whether a subject was a training one until after they submitted their classification, when a positive or negative feedback message was displayed, depending on whether they guessed the correct nature of the subject (lens or non-lens) or not (with some exceptions, described later). training images were interleaved in the classification stream with a frequency of one in three for the first 15 subjects shown, reducing to one in five for the next 25 subjects and then settling to a rate of one in ten as volunteers became more experienced. the sims and duds were randomly served throughout the experiment to each registered volunteer. as the number of sims was 50% higher than the duds in the training subject pool, the sims were shown with correspondingly higher frequency than the duds. we describe in detail the properties of the training images in subsection 3.1."
"one of the main goals of this experiment was to extend the sample of known lenses to higher lens redshifts. in fig. 6, we plot a histogram with the distribution in photo-z of our grade a and b lens candidates, together with candidates from our previous searches and compared to the lens redshift distribution from other surveys. the sugohi sample consists now of 324 highly probable (grade a or b) lens candidates. this is comparable to strong lens samples found in the dark energy survey [cit] b, discovered 438 previously unknown lens candidates, one of which is also in our sample) and in the kilo-degree survey (kids: [cit] b, presented ∼ 300 new candidates, not shown in figure 6 due to a lack of published redshifts, 15 of which are also in our sample). most notably, 41 of our lenses have photo-zs larger than 0.8, which is more than any other survey."
", then by the induction hypothesis, we have that m γ(φ 1 )∧γ(slice(φ 2, s ∪t )). therefore, we have m slice(φ, s ∪t )."
"we know that m γ(φ 1 ) ∨ γ(φ 2 ). without lost of generality, assume that m γ(φ 1 ). applying the induction hypothesis, there exists a branch br 1 of φ 1 such that m γ(slice(φ 1, br 1 )). by lemma 5, we know that br 1 is also a branch of φ. because γ(slice(φ,"
the final strip height measured in the experiments and predicted by fea after the 4 th forming station is compared in figure 9 (a). the overall height of the strip edge is similar in magnitude between the experiments and the fea. however while the experiments show three local peaks the fea only predicts two.
"the background source was modelled with an elliptical sérsic light distribution. its half-light radius, sérsic index and axis ratio were drawn from uniform distributions in the ranges 0.2 − 3.0, 0.5 − 2.0 and 0.4 − 1.0, respectively, and its position angle was randomly oriented. we assigned source magnitudes in g, r, i bands to those of objects randomly drawn from the cfhtlens photometric catalogue [cit] . the source position distribution was drawn from the following axi-symmetric distribution:"
", as a prior for the probability of the subject being a lens before the new classification is read. the posterior probability of the subject being a lens after the k + 1th classification then becomes:"
"in the coming years, the volume of data available for lens finding purposes will increase greatly, as the euclid space telescope 11 and the large synoptic survey telescope 12 (lsst) are each planned to cover areas of the sky that are more than a factor of thirty larger than that scanned in our study. scaling up space warps to such data volumes will be challenging: a much larger number of volunteers and/or a higher number of classification per volunteer will be needed. we can aim to improve the efficiency of our search by modifying the definition of the parent sample of subjects passed to the volunteers. for instance, machine learning-based methods could potentially be used to preselect large samples of possible lenses that are then refined in a visual inspection step via crowdsourcing. nevertheless, our experiment shows how crowdsourcing is a very powerful tool for finding lenses, delivering samples of lens candidates with relatively high purity and completeness, and we expect it to play 11 https://euclid-ec.org/ 12 https://lsst. [cit] s. yattalens can produce samples of roughly one grade c lens candidate or better per square degree in hsc-like data, and is therefore confirmed to be a valid tool for finding galaxy-scale lenses in a semi-automated way, provided that high completeness is not a critical requirement."
"on the other side of the spectrum are model checkers that require little user effort to run. indeed, tla + users debug their specifications with tlc [cit] . beyond simple debugging, tlc found serious bugs in specifications of distributed algorithms [cit] . although tlc contains remarkable engineering solutions, its core techniques enumerate reachable states and inevitably suffer from state explosion."
"thus, the implication holds. proof. we can prove this by induction on the structure of ψ: we conclude that the lemma holds for any propositional formula ψ in nnf."
"every tla + specification declares a certain finite set of variables, which may appear in the formulas contained therein. let φ be an α-tla + expression. we assume, for the purposes of our analysis, that φ is associated with some finite set vars (φ), which is a subset of vars, containing all of the variables that appear in φ (and possibly additional ones). this is the set of variables declared by the specification in which γ(φ) appears."
"in the previous section, we also reported the discovery of a number of highly probable lenses that were flagged by volunteers in the 'talk' section of space warps, including some that were missed by both the volunteer classification data and yattalens. based on the numbers reported in table 1, with as many as 11 grade a and 86 grade b lens candidates found among 264 inspected candidates, one could be inclined to conclude that the 'talk' section provides much purer samples compared to the analysis of classification data. however, those numbers are misleading: 264 were the candidates that were deemed sufficiently interesting to be included in the final grading step, and were selected after sorting through thousands of subjects flagged by volunteers. the effective purity of this lens search method is therefore much lower than table 1 suggests."
"+ has rich syntax [cit], which cannot be defined in this paper. to focus only on the expressions that are essential for finding assignments in a formula, we define abstract syntax for tla + formulas below. in our syntax, the essential operators such as conjunctions and disjunctions are included explicitly, while the other nonessential operators are hidden under the star expression ."
"while it is both desirable and plausible that future improvements in the development of lens finding algorithms will lead to higher purity and completeness in lens searches, a currently viable and very powerful approach to lens finding is crowdsourcing, harnessing the skill and adaptability of human pattern recognition. with crowdsourcing, lens candidates are distributed among a large number of trained volunteers for visual inspection. [cit], cfht-ls) . in this work, we use crowdsourcing and the tools developed by the space warps team to look for strong lenses in 442 square degrees of imaging data collected from the hsc survey."
"instead of enumerating states, software model checkers run sat and smt solvers in the background to reason about computations symbolically. to name a few, cbmc [cit] and cpachecker [cit] implement bounded model checking [cit] and cegar [cit] . domain-specific tools bymc and cubicle prove properties of parameterized distributed algorithms with smt [cit] ."
"the original and foreground-subtracted data were used to make two sets of rgb images, with different colour schemes. images with the light from the foreground galaxy subtracted are shown on the right, while original images are on the left. the images at the top and bottom row were created with the 'standard' and 'optimal' colour schemes, respectively."
"the rationale for the third point is to take into account the fact that a) groups and clusters are more likely to be lenses, due to their high mass concentration, and b) often produce non-trivial image configurations which might be penalised during the fourth step. finally, we averaged the scores of all nine graders, and assigned a final grade as follows:"
"the steps described above led us to a sample of ∼ 300, 000 galaxies. from these, we removed 70 [cit], as well as a few hundred galaxies already inspected and identified as possible lenses (grade c candidates in our notation) in the aforementioned studies. many of the known lenses were used for training purposes, as will be explained in subsection 3.1. [cit] search covered the same area as the present study, but targeted exclusively ∼ 43, 000 luminous red galaxies with spectra from the baryonic oscillation spectroscopic survey. only about half of those galaxies belong to the sample used for our study, while the remaining half was excluded because of our stellar mass limit. finally, we removed from the sample ∼ 4, 000 galaxies used as negative (i.e. non-lens) training subjects 2 . the selection of these objects will be described in subsection 3.1. the final sample size consisted of 288, 109 subjects."
"where n lens is the number of sims the volunteer classified and n lens the number of times they classified these sims as lenses. given that 'lens' and 'not' are the only two possible choices, the probability of the same volunteer wrongly classifying a lens as a non-lens is"
"we have introduced a technique to compute symbolic transitions of a tla + specification by finding expressions that can be interpreted as assignments. importantly, we designed the technique with soundness in mind. detailed proofs can be found in the report [cit] . we believe that our results can be used as a first preprocessing step when developing a symbolic model checker or a type checker for tla + . as in the case of tlc, one can find tla + specifications, for which no assignment strategy exists. however, tla + users are systematically checking their specifications with tlc, in order to find simple errors. hence, most of the benchmarks already come in a form compatible with tlc. thus, we expect our approach to also work in practice. based on these ideas, we are currently developing a symbolic model checker for tla + . methods, pp. 54-66. [cit] a extended definitions table 8 gives us a full formal definition of the function depth which maps an expression φ in our α-tla + abstract syntax to a natural number. proofs of propositions 5 and 6 are based on induction on the depth of an expression φ in the α-tla + abstract syntax. ::"
"the most difficult aspect of lens finding through visual inspection is distinguishing true lenses, intrinsically rare, from nonlens galaxies with lens-like features such as spiral arms or, more generally, tangentially elongated components with different colours from those of the main galaxy body. the latter are much more common than the former, so any inaccuracy in the classification has typically a large impact on the purity of a sample of lens candidates. in order to maximise opportunities for volunteers to learn how to differentiate between the two categories, we designed our duds training set by including exclusively non-lens objects bearing some degree of resemblance to strong lenses."
"we also added to the sample 264 outstanding candidates flagged by volunteers on the 'talk' section of the space warps website, which we browsed on a roughly daily basis, quickly inspecting subjects with recent comments (typically on the order of a few tens each day). this last subsample is by no means complete (we did not systematically inspect all subjects flagged by the volunteers) and has a large overlap with the set of probable lenses produced by the classification algorithm. nevertheless, we included it in order to make sure that potentially interesting candidates would not get lost. although most of the candidates inspected in this way turned out not to be lenses, this step still proved to be useful, because it enabled the discovery of a few lenses that would have otherwise been missed (as we will show later)."
"we split the lens training sample in two categories: an 'easy' subsample, consisting of objects showing fairly obvious strong lens features, and a 'hard' subsample, consisting of less trivial lenses to identify visually. after classifying an easy lens, volunteers received a positive feedback message (\"congratulations! you spotted a simulated lens\") or a negative one (\"oops, you missed a simulated lens. better luck next time!\"), depending on whether they correctly identified it as a lens or not. for hard lenses, we used a different feedback rule: a correct identification still triggered a positive feedback message (\"congratulations! you've found a simulated lens. this was a tough one to spot, well done for finding it.\"), but no feedback message was provided in case of misidentification, in order not to discourage volunteers with unrealistic expectations (often the lensed images in these hard sims were impossible to see at all). the implementation of two levels of feedback is a novelty of this study, compared to previous space warps experiments."
"1. identify the images that could be lensed counterparts of each other 2. depending on the image multiplicity and configuration, assign an initial score as follows: -einstein rings, sets of four or more images, sets consisting of at least one arc and a counter-image: 3 points. -sets of three or two images, single arcs: 2 points. 3. if the lens is a clear group or cluster, add an extra point up to a maximum provisional score of 3. if artefacts are present, then multiple images may not preserve surface brightness, may show mismatch of colours or may have the wrong orientation or curvature around the lens galaxy. 5. make sure that the final score is reasonable, given the definitions outlined above."
the highest residual density was found 0.82 a from s4 and the deepest hole 0.74 a from s4. figures fig. 1 . molecular view of the title compound showing the atom-labeling scheme. displacement ellipsoids are drawn at the 50% probability level. h atoms are represented as small spheres of arbitrary radii.
"training subjects served three different purposes. the first was helping volunteers to learn how to distinguish lenses from nonlenses, as discussed above. the second purpose was to keep volunteers alert through pop-up boxes that give real-time feedback on their classifications of the training images: given that the fraction of galaxies that are strong lenses in real data is very low, showing long uninterrupted sequences of subjects could have led volunteers to adopt a default non-lens classification, which could have resulted in the mis-classification of the rare, but extremely valuable, real lenses. the third purpose was allowing us to evaluate the accuracy of the classifications by volunteers, so as to adjust the weight of their scores in the calculation of the lens probabilities of subjects (more details will follow in subsection 3.2). in order to serve these functions properly, it was crucial for training subjects to be as indistinguishable as possible from real ones. this required having a large number of them, so that volunteers could always be shown training images never seen previously 7 . we prepared images of thousands of training subjects of two classes: lenses and non-lens impostors."
the title compound was synthetized at ambient temperature by a mixture of 5 mmol s of thiourea and 5 mmol of hcl in ethanolic solution. the solution was slowly evaporated until solvent completely dried and white crystalline salt was obtained.
"the two most important quantities that define the performance of a lens finding method are completeness and purity. the former is the fraction of strong lenses among the ones present in the surveyed sample that are recovered by the method, while the latter is the fraction of objects among the ones labelled as strong lenses that are indeed lenses. unfortunately, it is very difficult to determine either of them in an absolute sense: it would require to apply our lens finding methods to a large complete sample of real lenses and to a large sample of galaxies representative of our survey the nature of which is known exactly. we can however evaluate the relative performance between our two methods, crowdsourcing and yattalens. the data reported in table 1 shows clearly how the former outperformed the latter both in terms of completeness, with roughly twice the number of lens candidates with grade c or higher, and purity, with 40% of the inspected candidates having grade c or higher, against only 5% for yattalens. the comparison is not entirely fair: first of all, yattalens correctly identified most of the 52 known lenses used in the training sample, which have been excluded from the summary of table 1 and would have otherwise increased the completeness of yattalens [cit] . secondly, 3, 800 duds initially found by yattalens were removed from the subject sample and only shown to the volunteers as training images, making their classification job slightly easier. however, given the relatively good performance of the volunteers on the training subjects, with only ∼ 10% of the duds being classified as lenses (see fig. 5 ), the purity of the sample produced by the volunteers would only have changed by a small margin by including the duds."
"since the labels are unique, we write lab( :: ψ) to refer to the expression label and expr( ) to refer to the expression that is labeled with . we refer to the set of all subexpressions of φ by sub(φ). see [cit] for a formal definition."
"the existing frf technology is still limited in regard to the geometries that can be formed and wrinkling was found to be a critical defect. it is believed that wrinkling in frf can be minimised by an advanced tool and process design. in this paper, a new prototyping frf equipment will be introduced together with a new tool concept. this promises production of profiles with both variable width and depth at low tooling cost."
"where curly brackets denote ensembles over all volunteers who have classified the subject and the classification c k can take the values 'lens' or 'not'. using bayes' theorem, the posterior probability of a subject being a lens given the data is"
"à á2ch 4 n 2 s, contains two molecules of thiourea, three (diaminomethylidene)sulfonium cations and three chloride anions. the crystal packing is stabilized by n-há á ácl, n-há á ás, s-há á ácl and s-há á ás hydrogen bonds, forming a three-dimensional network."
"we searched for suitable galaxies among a sample of ∼ 6, 600 lens candidates identified by yattalens, which we ran over the whole sample of subjects before starting the crowdsourcing experiment. details on the lens search with the yattalens algorithm will be given in subsection 4.2. upon visual inspection, a subset of ∼ 3, 800 galaxies were identified as unambiguous non-lenses and deemed suitable for training purposes. these were used as our sample of duds. we show examples of duds in fig. 3 . in order to double the number of available duds, we also included in the training set versions of the original duds rotated by 180 degrees."
"fig. 4: smt formulas that are constructed when checking the executions up to length 4: using the action next (left), and using symbolic transitions (right). the gray formulas are excluded from the smt context during the exploration. figure 4 shows the smt formulas that are constructed by a bounded model checker when exploring executions up to length 4. (for the sake of space, we omit the formulas that check property violation.) on one hand, the monolithic encoding that uses only next has to keep all the formulas in the smt context. on the other hand, by incrementally checking satisfiability of the smt context, the model checker can discover that some formulas -for example, t 2 0,1 and t 3 1,2 -lead to unsatisfiability and prune them from the smt context. similar approach improves efficiency of bounded model checking c programs [cit] [ch. 16], hence, we expect it to be effective for the verification of tla + specifications too."
"in our example, tlc evaluates the actions produce and consume independently and assigns variables as prescribed by these formulas. as tlc is explicit, for each state, it produces at most 2 2 5 successors in produce as well as in consume."
"the space warps -hsc crowdsourcing experiment was launched on april 27, 2018. it saw the participation of ∼ 6, 000 volunteers, who carried out 2.5 million classifications over a period of two months. with the goal of assessing the degree of involvement of the volunteers, we show in fig. 4 the distribution in the number of classified subjects per user, n seen . this is a declining function of n seen, typical of crowdsourcing experiments: while most volunteers classified less than twenty subjects, nearly 20% of them contributed each with at least a hundred classifications. it is thanks to these highly committed volunteers that the vast majority of the classifications needed for our analysis was gathered."
"but br i is a branch and br i \\ t is its proper subset, so m[br i \\ t ] boolform(φ i ) and consequently, m[s ] boolform(φ), for any proper subset s ⊂ br . therefore, br is a branch of φ."
we evaluate the likelihood based on past performance of the volunteer on training subjects. we approximate the probability of the volunteer correctly classifying a lens subject with the rate at which they did so on training subjects:
"given that in the process the strip edge mostly undergoes compressive deformation only the minimum principal strains evaluated in both the experiments and fea are presented in figure 9 (b). according to figure 9 (b) the fea shows slightly higher compressive strains compared to the experiments in most parts of the flange region. this suggests that in the fea the material holds the compressive deformation, i.e., a higher magnitude of compressive strain is permanent. in the experiments the material releases the compression most likely in form of wrinkling of the flange which may explain the higher wrinkling tendency observed in the experiments compared to the numerical model (figure 9(a) ). the numerical results for the wrinkling pattern in the edge after the fourth forming step and the minimum principal strain is compared between the forming with and without the use of finger rolls in figure 10 (a) and (b) respectively. figure 10(a) clearly indicates that the finger rolls play a major role in postponing the initiation of wrinkles and lead to less wrinkling defect. in figure 10( that even though the part is symmetric in longitudinal direction there is a non-symmetric distribution of minimum principal strain. the minimum principal strain is higher if finger rolls are used which indicates that the finger rolls increase the level of permanent compressive strain in the material; this leads to reduced wrinkling. without the finger rolls the material tends to release more compressive strain by developing more wrinkles. this suggest that wrinkling issues in flexible roll forming may be minimised by a smart tool design and by optimising the forming sequence. figure 10 . (a) wrinkling pattern with and without finger rolls (b) distribution of the minimum principal strain after each pass"
"let us now consider a subject for which k classifications from an equal number of volunteers have been gathered. if a k + 1-th classification is collected, we can use the posterior probability of the subject being a lens after the first k classifications,"
"implementation and evaluation. based on the theory presented in this paper, we have implemented a procedure to find assignment strategies and their corresponding symbolic transitions from tla + specifications, or report that none exist. it uses z3 as the background smt solver."
"a schematic of the new 3d roll forming prototyping equipment is shown in figure 3 . the pre-cut blank is placed between two clamps where the top clamp has the features of the inner surface of the part to be formed such as the corner radius, the profile length and width. the clamping force is provided by a set of hydraulic cylinders attached to the main frame which is called \"sled\". the sled which carries the blank sheet and clamps can be moved back and forth on the mill bed by a lead screw attached to a servo controlled motor. the forming tool is mounted on two hexapods that consist of six leadscrews which are driven by six separate servo motors. the tool holder which carries the forming tool is placed on the hexapod plate providing a six degrees of freedom tool motion. according to this new machine concept, the same tool set can be used for several roll forming steps which enables an infinitely selection of bending angles and significantly reduces tooling cost. the part shown in figure 1 (a) has a uniform width and a variable height. therefore the pre-cut blank has a non-uniform width which can be obtained by unfolding the 3d model given in figure 1 (a). the symmetry half of the pre-cut blank is shown in figure 4 and the same blank was used for the finite element analysis. the profile web has three different curvatures that are followed by the forming tool to form the flange ( figure 5 )."
"the are two subcases: br ) ). by lemma 7 we have that br is a branch of φ 2 in the first case and that br is a branch of φ 3 in the second case. from the induction hypothesis, it is easy to show that m 1 is also a model of γ (φ). in conclusion, we have that the theorem is true for all depth (φ), or for all logical formula φ."
"we obtained cutouts around ∼ 300, 000 massive galaxies selected with the criteria listed above and served them for inspection to a team of citizen scientist volunteers, together with training images consisting of known lenses, simulated lenses and non-lens galaxies, via the space warps platform. the volunteers were asked to simply label each image as either a lens or a nonlens. after collecting multiple classifications for each galaxy in the sample, we combined them, in a bayesian framework, to obtain a probability for an object to be a strong lens. the science team then visually inspected the most likely lens candidates and classified them with more stringent criteria. [cit] ) . by merging the crowdsourced lens candidates with those obtained by yattalens, we were able to discover a sample of 143 high probability lens candidates. most of these very promising candidates were successfully identified as such by the citizens."
"we introduce a technique to statically label expressions in a tla + formula φ as assignments to the variables from a set v, while fulfilling the following:"
"we know that m is a model of both γ(slice(φ 1, s )) and γ(slice(φ 2, s )). by the induction hypothesis, we have that m is a model of both γ(slice(φ 1, s ∪t )) and γ(slice(φ 2, s ∪ t )). therefore, we know that m is a model of γ(slice(φ, s ∪ t )) since"
"in reality, the sled which carries the clamps and the pre-cut blanks moves back and forth while the tool changes its position and angle to form the flange according to the forming sequence. however in the finite element model, the pre-cut blank is fixed in space and the forming tool moves in sequence over the blank to form the strip to the desired shape as shown in figure 7(a) . the numerical analysis was performed using the commercial software package copra® fea rf [cit] which is based on msc marc and uses an implicit solver. the rolls and clamps were modelled as solid bodies while the strip was discretised with full integration, hexahedral, type 7 arbitrarily distorted brick elements. only the transversal half of the strip was modelled due to symmetry as shown in figure 4 and one element through the thickness was used to save computational time. a refined mesh was used in the bending region and frictionless contact was assumed. material data for the model was obtained from the tensile test. the elastic material behaviour was defined assuming a poisson's ratio of 0.3 and a young`s modulus of 200gpa. isotropic hardening and von mises yield criterion were used in combination with the plastic component of the trues tress strain curve given in figure 2 to define plastic material behaviour [cit] ."
a new prototyping facility for roll forming variable width and depth components was introduced and was successfully simulated with copra® fea rf. the high strength steel that was formed showed a high tendency for wrinkling and a newly introduced roll cluster showed good potential to postpone the initiation of wrinkles. higher magnitude of compressive strain attributed to the less sever wrinkling and vice versa. introduction of the finger rolls reduced the number wrinkling peaks from 6 to 2. in addition the numerical control of the roll movement can be used to increase the accuracy of the part by changing the tool path or bending angle.
"the first assignment strategy a 1 seems to be better than a 2 because a 1 generates fewer symbolic transitions than a 2 . however, in this paper, we do not introduce any metric, by which we could compare assignment strategies. in the implementation, we use any strategy found by the smt solver."
the profile considered for this investigation is shown in figure 1 standard tensile tests were carried out according to astm e8/e8m [cit] with bone shaped samples oriented along the rolling direction of the strip. an instron 5967 with a 30kn load cell was used for the tests and the test speed was 0.025 s -1 giving a strain rate of 0.001 −1 . the true stress-true strain curve of the dp780 is given in figure 2 . table 1 presents the material parameters which were obtained by fitting the hollomon's power law to the true stress-effective plastic strain curve of the material.
"notice that if γ (s ) is the empty set, there is no model for γ (φ). since m is a model of γ (φ), we know that γ (s ) is not the empty set and therefore, there ex-"
"the flower pattern shown in figure 6 (a) shows the forming sequence while figure 6 (b) gives a schematic of the tool used in the first four forming passes. this consists of three bottom rolls (b1, b2 and b3) and two top rolls (t1 and t2). the two top rolls are called finger rolls due to their shape and the part will be formed in fea with and without finger rolls to understand their effect on the material flow. the centres of roll b1, b2 and t1 are located in one plane and the right part of figure 6 (a) indicates their position on the flange during forming. the tool has to follow a path given by the web of the profile as shown in figure 5 to generate the component geometry."
"although the boss galaxies used as lenses are labelled as red, a substantial fraction of them are late-type galaxies, i.e. they exhibit spiral arms, disks or rings. simulations with a latetype galaxy as a lens are more difficult to recognise, because the colours of the lensed images is often similar to those of starforming regions in the lens galaxy. nevertheless, we allowed late-type galaxies as lenses in the training sample, as we did not want to bias the volunteers against this class of objects."
"in conclusion, we have that the theorem is true for all depth (φ), or for all logical formula φ. proof. denote by s the union br 1 ∪ · · · ∪ br k . we prove the lemma by induction on the structure of φ:"
"in general, finding assignments and slicing a formula into symbolic transitions is not as easy as in our example, because of quantifiers and if-then-else complicating matters. in this paper, we present our solution, demonstrate its soundness and report on preliminary experiments."
"the are two subcases: m γ (φ 1 ∧ φ 2 ), or m γ (¬φ 1 ∧ φ 3 ). in both cases, the arguments are similar to the conjunction case."
"loosely based on estimates from past lens searches in cfht-ls data. although our hsc data is slightly better than cfht-ls both in terms of image quality and depth, which should correspond in principle to a higher fraction of lenses, we do not expect that to make a significant difference. this is because, as we will clarify later, we designed our experiment so that the final posterior probability of a subject is always dominated by the likelihood and not by the prior."
"we can use this set to reason about minimal coverings: a minimal covering may contain, per variable, no more than one label from each pair of collocated assignments to that variable. we describe these labels by using the sets"
"as our goal is to reason about the side-effects of variable assignments, the remainder of this section looks at how we can achieve this with the help of branches."
"to simplify the analysis, we force r to be injective, when it is restricted to labs(φ). otherwise we could always construct an injective function from r, which respects the required inequalities. the formula we therefore consider is as follows:"
"we carried out a crowdsourced lens search over 442 square degrees of data from the hsc survey. the search was carried out on a sample of ∼ 300, 000 galaxies with photometric redshift between 0.2 and 1.2 and a stellar mass larger than 10 11.2 m . almost 6, 000 citizen volunteers participated in the crowdsourcing experiment, named space warps -hsc. we collected ∼ 2.5 million classifications, which we then analysed with an algorithm developed in past editions of space warps. in parallel, we searched for lenses in the same sample of galaxies using the automated lens finding method yattalens. from the two searches combined, we found 143 highly probable (grade a or b) new lens candidates, in an area that already included 70 known lenses. compared to yattalens, crowdsourcing was by far the most successful lens finding method, both in terms of completeness and purity. we found lenses of a variety of kinds, including lenses with a compact source, with a red source, group-scale lenses and lens candidates with highly asymmetric configurations."
"we ran yattalens on the sample of ∼ 300, 000 galaxies obtained by applying the stellar mass and photometric redshift cuts described in subsection 2.1 to the s16a internal data release catalogue of hsc. more than 90% of the subjects were discarded at the arc detection step. of the remaining ∼ 22, 000, 6, 779 were flagged as possible lens candidates by yattalens and the rest was discarded on the basis of the lens model not providing a good fit. we then visually inspected the sample of lens candidates with the purpose of identifying non-lenses erroneously classified by yattalens that could be used for training purposes. the ∼ 3, 800 galaxies that made up the duds were drawn entirely from this sample."
"tla is a general language introduced by leslie lamport for specifying temporal behavior of computer systems. it was later extended to tla + [cit], which provides the user with a concrete syntax for writing expressions over sets, functions, integers, sequences, etc. tla + does not fix a model of computation, and thus it found applications in the design of concurrent and distributed systems, e.g., see [cit] ."
"in the above equations, i, r, g indicate the flux in each band, while i cut, r cut and g cut are threshold values: pixels with higher flux than these thresholds are assigned the maximum allowed intensity, 255. in a similar vein, pixels with negative flux are given 0 intensity. in the first colour scheme, dubbed 'standard', we fixed i cut, r cut and g cut for all images, set to sufficiently low values so as to make it possible to detect objects with surface brightness close to the background noise level. this choice resulted in images with consistent colours for different targets, though often with saturated centres, especially for the brightest galaxies. we also adopted a second colour scheme, named 'optimal', where we used a dynamical definition of the threshold flux for each subject: in the original image (i.e. without foreground subtraction), this was set to the 99%-ile of the pixel values in each band, so that the main galaxy, typically the brightest object in the cutout, was not saturated, except for the very central pixels."
"finally, we point out how roughly half of our grade c lens candidates consist of systems with either a single visible arc, the counter-image of which, if present, is very close to the centre fig. 8 . set of eight disk galaxy lens candidates discovered in our sample. fig. 9 . two lens candidates with strongly lensed red sources. and not detected, or a double in a highly asymmetric configuration. it is very difficult to determine whether such candidates are lenses or not using photometric data alone, but, given their abundance (a few hundred in the whole sample), they constitute a very interesting category of lenses: even if only a fraction of them turned out to be real lenses, they would end up dominating the lens population. this is not surprising, but is to be expected from the simple geometrical arguments that we made when describing our procedure for simulating lenses, in subsection 3.1.1: the area in the source plane that gets mapped into highly asymmetric image configurations is larger than the area corresponding to configurations close to symmetric 10 ."
roll forming is a well-known process for the manufacturing of components with uniform cross section for applications in the building and automotive industry [cit] . it allows the forming of hard to form materials to complex geometries with minimum shape defects. as a result roll formed sections from ultra high strength steel (uhss) and advanced high strength steel (ahss) are increasingly used in automotive industry for structural and crash components [cit] . while roll forming is limited to long profiles of continuous cross section the recently developed flexible roll forming (frf) technology enables forming of profiles with variability in width and depth over the length of the part [cit] . in frf some rolls stay stationary similar to the conventional roll forming process while others provide a translational and rotational movement to achieve variability in cross section shape. the two major shape defects in frf are web warping and wrinkling in the flange [cit] . web warping has been extensively studied by ona [cit] who showed that a lack of permanent deformation in the transition zones of frf profiles of variable width led to increased web warping. [cit] developed an analytical model to predict web warping defects in frf profiles of variable width. larranaga [cit] introduced a local heating system while [cit] developed a blank holder system to minimise web warping defects. only a few studies investigated wrinkling in frf [cit] . so far the wrinkling defect has only been studied for profiles of variable width and has been related to an excessive stretching of the strip edge before it enters the compression zone [cit] . however the cause for wrinkling may differ depending on the part shape and currently there are no guidelines for minimising wrinkling defects in frf.
"most of the lens candidates missed by yattalens were discarded at the arc detection step, for various reasons: their lensed images are either point-like (as two of the candidates shown in fig. 7), or too red (as in the two cases shown in fig. 9 ), or consist of arcs that are considered too faint or too far from the lens galaxy by yattalens. in principle, we could adjust the settings of yattalens to be able to detect such lenses in future searches, although most likely by paying a penalty in terms of purity."
"our crowdsourcing project, named \"space warps -hsc\", was hosted on the zooniverse 4 platform. the setup of the experiment followed largely that of previous space warps efforts, with minor modifications. [cit] for further details. upon landing on the space warps website 5, volunteers were presented with two main options: reading a brief documentation on the basic concepts of gravitational lensing, or moving directly to the image classification phase. the documentation included various examples of typical strong lens configurations, as well as false positives: non-lens galaxies with lens-like features such as spiral arms or star-forming rings and typical image artefacts. during the image classification phase, volunteers were shown sets of four images of individual subjects, of the kind of fig. 1, and asked to decide whether the subject showed any signs of gravitational lensing, in which case they were asked to click on the image, or else proceed to the next subject. on the side of the classification interface, a 'field guide' with a summary of various lens types and common impostors was always available for volunteers to consult. users accessing the image classification interface for the first time were guided through a brief tutorial, which summarised the goals of the crowdsourcing experiment, the basics of gravitational lensing and the classification submission procedure."
"where θ s is the radial distance between the source centroid and the centre of the image. the above distribution is approximately linear in θ s at small radii, as one would expect for sources uniformly distributed in the plane of the sky, but then peaks at θ ein /4 and turns off exponentially at large radii. the rationale for this choice was to down-weight the number of lenses with a very asymmetric image configuration, which correspond to values θ s close to the radial caustic of the sie lens (i.e. the largest allowed value of θ s for a source to be strongly lensed), at an angular scale ≈ θ ein . sources close to the radial caustic are lensed into a main image, subject to minimal lensing distortion, and a very faint (usually practically invisible) counter-image close to the centre. these systems are strong lenses from a formal point of view, but in practice are hard to identify as such. they would dominate the simulated lens population if we assumed a strictly uniform spatial distribution of sources, hence the alternative distribution of eq. 2. not all lens-source pairs generated this way were strong lenses: in ∼ 13% of cases the source fell outside the radial caustic. such systems were simply removed from the sample. among the remaining simulations, most showed clear signatures of strong lensing (e.g. multiple images and/or arcs). for some, however, it was difficult to identify them as lenses from visual inspection alone. we decided to include in the training sample all strong lenses, regardless of how obvious their lens nature was, so that volunteers would have the opportunity to learn how to identify lenses in the broadest possible range of configurations. this choice could also allow us to carry out a quantitative analysis of the completeness of crowdsourced lens finding as a function of a variety of lens parameters, although that is beyond the scope of this work."
"since any label that is both in br, a branch of φ k, and cand(v, φ) is in cand(v, φ k ), we see that b ∩ a ∩ cand(v, φ k ) is also nonempty. by the induction hypothesis for φ k, this tells us that"
"below, we show that the branches and slices induced by them naturally decompose a tla + formula. let φ be an α-tla + expression and γ (φ) its corresponding tla + formula. then, the following holds:"
"volunteers also had the opportunity to discuss the nature of individual subjects on the 'talk' (i.e. forum) section of the space warps website: after deciding the class of a subject, clicking on the \"done & talk\" button would submit the classification and prompt the volunteer to a dedicated forum page, where they could leave comments on it, ask questions and view any previous related discussion. volunteers did not have the possibility of changing their classification once it was submitted, so the main purposes of this forum tool was to give them a chance to bring the attention on specific subjects and ask for opinions from other volunteers or experts. this helped the volunteers in building a better understanding of gravitational lensing, as well as creating a sense of community. we regularly browsed 'talk' to answer questions and to look for outstanding subjects highlighted by volunteers."
"because m γ(slice(φ, s )) and γ(slice(φ, s )) ⇔ γ(slice(φ 1, s ))∨γ(slice(φ 2, s )), we know that m is a model of either γ(slice(φ 1, s )) or γ(slice(φ 2, s )). if m is a model of γ(slice(φ 1, s )), by the induction hypothesis we have m is a model of γ(slice(φ 1, s ∪t )). if m is a model ofγ(slice(φ 2, s )), by the induction hypothesis we have m is a model of γ(slice(φ 2, s ∪ t )). therefore, we know that m is a model of γ(slice(φ, s ∪ t )) since"
"the boundary conditions applied on the pre-cut blank are shown in figure 7(b) . an x lock boundary condition was applied on the nodes along the symmetric centre which fixes the material in x direction to represent the transversal symmetry of the pre-cut blank. a y lock boundary condition was applied on nine bottom nodes along the symmetric centre at three different locations; this was to hold the pre-cut blank in the vertical plane while forming. the longitudinal movement of the strip is restricted by the z lock boundary condition which is applied on four nodes at the end of the strip, towards the symmetric centre (see figure 7 ). in addition to that, to define the tool movement a tool path was introduced in the finite element model which has same contour as the web of the profile shown in"
"where, p i t,a indicates the consumed power at time, t by the a-th appliance, while h, t, a bf and a ts denote the sets of all households, time steps, buffered and time shifting appliances respectively."
"ability of scaling up or down the resources to meet the workload is one of the most desired cloud computing advantages. however, this great advantage can lead to service failures, if it is not well implemented or if a maximum response time is not agreed upon in advance. a web application developer hosting services on a cloud may see how the response time steadily increases with the usage of application -because the scaling-up of resources is not so quick by the cloud. on the other hand, scaling must be limited by some threshold. this threshold would stop the continuous increase in the allocation of resources to prevent the cloud provider from a denial of service attack. in either case, the customer could be billed for service that they did not want. existing service level agreements determine quality of service requirements, but not in terms of response time with workload variations. hence, the scaling remains a big challenge to the cloud providers."
"cloud computing has tremendous business potential, still has many implicit limitations that could restrict its usefulness. a number of technical and business issues put boundaries on the adoption and growth of cloud computing. service level agreements become most complex, when offering applications as a service. the current state of slas in cloud computing is inadequate and there is a need for fine-grained specifications with enhanced performance parameters than simply the resource availability. cloud computing is ready to successfully host the most typical web applications with great cost savings, but the applications with (a) strict latency requirements (b) large datasets, (c) strict needs for availability, are critical to the success of cloud migration. even with the evolution of slas, all-purpose clouds might not be willing to offer guarantees for the most demanding applications hence, the compensation models for violation of the sla is also very crucial. in order to offer best migrated services, consideration of specific performance requirements is highly important for each deployment and service model. this however, offers an excellent opportunity for clouds targeted and designed for particular workloads. the issues discussed in the paper are certainly of great concern for migration of services and hence the work will be a guiding path to the present day migration scenario."
"an additional issue is that although slycat provides the data management for retrieving and viewing multiple videos using synchronized playback, screen real estate limits viewing to a handful of videos at a time. what we really need is a new model that will abstractly show video similarities over time, so that we can see the shifting relationships between video results without having to view them all."
"an integrated approach for day-ahead congestion management is formulated that combines the advantages of marketbased indirect control and active power curtailment based direct control methodologies. the proposed approach avoids the shortcomings of the indirect and direct control methods, which makes it well-suited for congestion management in residential lv networks."
"the simulation is performed with a time step of 15 minutes. the agent-based simulation platform is developed and coordinated in matlab environment. the appliance response optimization problems in eq. (2)-(11) are solved using matlab optimization toolbox [cit] . based on the load profiles, the power flow calculation is performed using the epri distribution system simulator, opendss [cit] . the optimization problem associated with graceful degradation in eq. (14) - (23) is modeled using open-source matlab toolbox, yalmip [cit] . yalmip supports rapid prototyping of optimization problems and uses an external optimization solver. in this research, the gurobi optimization solver is used for solving the mixed-integer problem discussed in section iii-b [cit] ."
"2) market-based price adjustment: as shown in fig. 1, the ta requests flexibility from the aggregator, when a congestion is anticipated. in such a case, the aggregator tries to alleviate the peak loads through iterative changes in the price. in each iteration, the households react by reoptimizing the appliances based on eq. (4) - (11) . the iterative process aims to find a suitable adjusted price that reduces the peak loads to the agreed upon level with the dso. this adjusted price signal p t,new is based on the following equation:"
"the efficacy of the integrated approach is investigated through simulations for 55 households in the modified ieee european lv test network. simulation results show an association between the dynamic price and the occurrences of congestion, since flexible loads aim to reduce the energy cost as much as possible by shifting to times of low prices. the market-controlled dr is an efficient method to tackle the most of hte congestion which occurs, as the total monthly congestion duration can be lowered up to 62%. for safe operation of the network, the other 38% of the congestion has to be reduced, even if the price adjustment does not always succeed to procure sufficient flexibility. in such a case, a significant reduction (94%) in the congestion duration is achieved by complementing the market-controlled dr with a graceful degradation approach. since the response of the appliances following the curtailment is explicitly modeled, the magnitude of the supplied energy remains of the same order. based on the monthly simulation in an intel core i7 computer with 8gb of ram, the integrated approach requires a simulation time of approximately 21 minutes compared to 14 minutes for the uncontrolled case."
"interactions are as important as the representations in designing the interface. representations should be linked to explore relationships across abstraction levels. at the ensemble level, the visual elements themselves can be used as affordances within the interface, executing operations over sets of simulations or acting as filters to enable rapid reduction in visual clutter. at the level of individual runs, each simulation can be linked to both ensemble level groups and to individual variable values. as information foraging 2 leads the analyst to a reduced set of simulations, group operations can be used to retrieve and compare sets of result variables."
"snapshots of the simulation results are presented for typical winter days in the netherlands. first, the impact of dynamic pricing scheme on residential demand is discussed. next, the impacts of the market-based dr and graceful degradation methods are explained separately, followed by the effects of applying the integrated approach. subsequently, the monthly performance of the mechanisms is analyzed in terms of the overall efficiency of managing the congestion. fig. 4 illustrates the impact of the dynamic price on the aggregated network demand. the initial load profile represents the combined projected day-ahead profile of the households. each ha adjusts its initial day-ahead profile considering the same dynamic price sent by the aggregator. the resulting network load is illustrated by the price-adjusted profile. as discussed in section iii, the has aim to minimize the energy cost by optimizing the daily energy usage of the devices. to do so, flexible devices are mostly scheduled during the times of lower price. therefore, the peak loads in the adjusted network load profile are related to the valleys in the profile of the dayahead price and vice versa. thus, a lower domestic energy cost is realized by introducing the dynamic price at the expense of larger peaks from an lv-network perspective."
"interesting media can be pinned in the view to perform comparisons ( figure 7 ). for multiple pinned videos, video synch enables shared group controls for synchronized playback and singlestepping of all visible videos."
"3) household response: since the curtailment request indicates the time and amount of curtailment for each house, the ha first determines the devices that need to be switched off at the designated time steps. for each house, i, this can be mathematically formulated as the following optimization problem, (26) where, t i curt denotes the set of time steps in which curtailment is required and n(a ts ∪a bf ) represents total number of available time shifting and buffer appliances in house, i. the decision variable, v i t,a is a binary variable and equals to 1, when a particular appliance needs to be switched off."
"the modeling of the integrated approach of day-ahead congestion management consists of two parts, the market-based demand response modeling and the modeling of the graceful degradation. both of the options require the use of household load data on the appliance level, as the level of flexibility differs depending on the type of appliance. in order to generate the load curves of the appliances in a household, a bottom-up markov chain monte carlo approach to the household load modeling is applied [cit] . based on this approach the preferential household load curve can be determined comprising of different devices. subsequently the household load is subjected to the dynamic price from the aggregator. how the appliances in the household react to a dynamic price is discussed in the next section."
"cloud platforms can also be classified based upon the underlying infrastructure deployment model as public, private, community, or hybrid clouds [cit] . different infrastructure deployment models are distinguished by their architecture, location of the datacenter where the cloud is realized, and also in accordance with the customer needs."
"in order to resolve congestion, the dso requests flexibility from the market. the aggregator, as the demand flexibility coordinator, determines if the requested flexibility can be delivered by adjusting the day-ahead price. flexible loads of the end-users are influenced by varying the price levels, with an objective of reducing the peak load. along with the change in price, the resulting adjusted load profiles are illustrated for two representative days in fig. 5 . in order to take advantage of the decreasing price at the late evening of the first day, evs start charging almost simultaneously and results in overloading the transformer. similar situation occurs in the evening of the second day, when the operation of the hps coincide with the conventional residential peak loads."
"to compare members of an ensemble, we need to find sets of variables shared by all the runs, such as a multi-variate set of scalar outputs, or a finite element mesh of the final time step in the simulation. we will refer to these variables as shared attribute spaces. although it might seem that simulation ensembles could be easily constructed to simply generate common result sets, even outputs as simple as time series data may require transformations prior to comparison. for instance, the set of time series may not all span identical time intervals, sequences may contain differing numbers of values, or values may not be sampled at corresponding times. consequently, the attribute space is a transformation of the data that is constructed to facilitate comparisons. for more complex output types, attribute spaces based on statistical measures or extracted features are necessary to reduce the dimensionality of the results."
"the overall effect of the methods is depicted in the monthly load duration curve in fig. 7 . the adjusted network load profiles demonstrate a considerable shift from the original price-based profile. however, loads are largely shifted in time in order to keep the energy consumption of the same order."
"slycat models are each tailored to address specific results types, so different model types can provide complementary perspectives of data features when analyzing the same data set. insights gained from each model combine to create a more complete mental model of the underlying problem domain."
"where, v − and v + are the negative sequence and positive sequence voltage at the poc respectively. according to the en50160 standard, the 10-minute average rms value of vuf must be limited to 2% for the 95% of the time in a week [cit] . in this work, the sensitivity of vuf at the end of a feeder to the changes in power consumption at the pocs is determined to limit voltage unbalance resulting from active power curtailment. the sensitivity of i-th poc, (δ(vuf) i /δp i ) is estimated by the changes in vuf at the end of the feeder due to a change in power consumption at the same poc."
"the effects of various levels of s target on the performance have been tabulated in table ii. a higher target loading level results in a narrower window of flexibility for graceful degradation. consequently, the duration of congestion remains higher. on the other hand, a lower value of s target provides a greater margin for curtailment decisions. since the graceful degradation is formulated in order to minimize the number of curtailment locations, a lower s target does not result in a higher amount of curtailed power. thus, the resulting duration of congestion is largely mitigated while maintaining the total supply."
"depending on the simulation code and the problem being modeled, results data typically include multiple variables saved in a series of temporal snapshots. the variables can consist of scalar or vector values, or they can be a set of values arranged into more complex structures, such as volumes, surfaces, matrices, graphs, or trees. with the growth of in situ visualization, which renders images and videos as part of the simulation, we are seeing media-based results increasingly among the outputs. these results combine decreased capabilities for post-simulation control of views, variable selections, and visualization parameters with increased data fidelity, especially temporal fidelity, during image rendering."
"editor: theresa-marie rhyne theresamarierhyne@ gmail.com aspects of ensemble analysis that make it difficult: the complex nature of the results data, the difficulty of defining analysis approaches that reveal high-level relationships between runs or between variables, and finding suitable visual representations at various levels of abstraction. figure 1 . parameter space model of a 15k run ensemble to study material fracturing, filtered by velocity value on left, showing in situ generated images of an anomalous run, which have been retrieved from a remote computing system."
"a modified european lv test network is used for the case study [cit] . the test network consists of 55 households equipped with single phase connections. a 250 kva, 11kv/0.416kv transformer is used to supply the lv fig. 3 . sensitivity of the voltage unbalance factor at the furthest end of the feeder to the changes of household load."
"currently, the application of multiple slycat models to analyze the same ensemble is a manual operation. each model must be independently created and operations are not shared between models. given the power of combining multiple perspectives on a shared problem, we are working on mechanisms to link selection or visibility between models. the challenge is to do this in a web-based framework."
"the market-based demand response reduces the duration of congestion by almost 62% by shifting the load through price adjustments. the load shifting behavior results in a higher supplied energy compared to the graceful degradation and the integrated approach. however, the adjusted price may not always tackle the congestion, since it depends on the available flexibility. this is reflected in the magnitude of the peak load being the same for both the price-based and the market-controlled dr."
"in the latest report of european network and information security agency (enisa) [cit], it has been suggested that the level of this risk would be considerably lower if the encryption is applied to data at rest. this can be highly significant for the cloud computing environment."
"in figure 6, we can see at a glance that there are only two significant input parameters, velocity and density_1, which are negatively correlated with most of the output metrics (i.e. axis-aligned minimum and maximum stress values at the punch tip). the scatterplot provides the individual-run level of abstraction with each point representing a run. this view shows the correspondence between the bar chart's high-level relationships and each simulation run. the coordinate space is highly abstract, with the x-axis representing a metavariable that is the sum of all the input parameters and the y-axis a metavariable of all the scalar outputs. the x and y coordinates for each run are computed as the weighted sums of input and output values, respectively. runs whose variable relationships exactly match the cca weightings will lie along the diagonal. runs whose variable relationships do not match will appear off the diagonal, such as the two white-colored outliers in figure 6 . this view can be used detect runs whose variable values or relationships are outliers or anomalous. because the point coordinates depend upon all variables simultaneously, this view reveals outliers resulting from interactions between multiple variables, which would not be discovered by simple thresholding."
"one of the main features of any cloud computing service is the level of abstraction from the underlying physical infrastructure. the end user is unaware about the location of the hardware. the only source of information the user has about these servers is the hardware specifications provided by the cloud provider for each type of service. moreover, the hardware metrics could not have the same meaning in a cloud server as they did in a traditional server, because the users may be sharing computing and i/o resources at a given instance of physical processor. users expect always the same performance for the same money, but this could simply not be true as the performance depends on various factors and most of them are not under the control of the end user. in fact, this is currently a main concern that enterprise customers have about cloud computing, according to a survey by idc [cit] . cloud computing's economic benefits are based on the ability to increase the usage level of the infrastructure through multitenancy, but it is not clear that one user's activity will not compromise another user's application performance. on top of that, the latency to the datacenter, where the server is hosted, along with other network performance parameters, could vary as a function of the time of day, the particular location of the current servers, and the competing traffic in the communication links. therefore, the performance might not be as per expectations and could fluctuate. this variance in performance may cause a problem if the customer is unable to predict these variations, their magnitude, and duration -as the price remains deterministic."
"contrary to the iterative process of the market-based control, graceful degradation sends the curtailment signals based on the instantaneous flexibility at each time step. although peak loads are largely curtailed, the rebound effect of the loads results in congestion at other time steps. since the price levels are not adjusted, the households react to the curtailment requests by shifting the loads to relatively lower price intervals. thus, application of the graceful degradation alone is associated with risks of congestion at more time steps, especially when the daily price levels do not vary much."
"implementing approaches and ideas developed in response to this goal, we demonstrate the analysis of a 15k run material fracturing study using slycat, our ensemble analysis system."
"network from the mv bus. the transformer comprises of a delta/grounded-wye connection. the resistance and reactance of the windings are 0.4% and 4% of the base values at the mv side respectively. an additional lv feeder with an aggregated peak load of 100 kw, is assumed at the mv/lv substation."
"(vuf) t denotes the voltage unbalance factor at the end of the feeder at time t. constraint (23) ensures that the aggregated curtailed power at the pocs do not result in a violation of the maximum allowable unbalance factor of 2%. the optimization problem, as defined by eq. (14) - (23) contains both integer and continuous variables and is therefore solved using a mixed-integer programming method."
"we currently have models providing correlation analysis, parameter exploration, and time series clustering. correlation analysis looks exclusively at scalar-valued table data and addresses the tasks of sensitivity analysis and outlier identification. parameter exploration combines table data and media data, facilitating tasks of parameter optimization and outlier finding. time series clustering operates on tables and temporal sequences, targeting the tasks of result partitioning and outlier detection. because we want to map results back to input parameters, table data is a component of all slycat model types."
"how do we construct a mental model of such a diverse set of results and relate that back to the input parameters? to understand the ensemble as a whole, we need analysis methods that summarize the predominant behaviors of the overall group. this includes relationships between sets of runs, individual runs relative to the majority, correlations between variables, and how set membership or all of the preceding relationships change over time. the analysis techniques selected are highly dependent on the type of results data being compared and the type of relationships being evaluated. for example, slycat uses hierarchical, agglomerative clustering for discovering set relationships within time series results, and canonical correlation analysis 1 for correlating relationships between input variables and scalar outputs."
"although for scientists, each simulation run is a model of real-world phenomena given certain conditions, we use the term model to refer to our modeling of the ensemble data, not the physics. within slycat, a model encapsulates an attribute space, an analysis method, a set of visual representations, and a set of interactions between those representations."
"the distributed nature of the cloud model, necessarily involves more transits of data over networks. thus, data handling offers new challenging security risks. the confidentiality of the data must be assured whether it is at rest (i.e. data stored in the cloud) or in transit (i.e. to and from the cloud). it would be desirable to provide a closed box execution environment, where the integrity and confidentiality of the data could be verified by its owner. while encryption is an answer to securely storing data in the cloud, it does not fit well with cloud-based processing. in most cases, the data has to be decrypted at some time when it is inside the cloud. some operations would be impossible with encrypted data and, also the computations with the encrypted data would consume more computing resources, hence increasing the expenses."
"where τ is the time shift of the appliance, τ min a and τ max a the limits of the maximum allowable time shift and a ts the set of all time shifting appliances. for the optimization of the time shifting appliances the time shift τ is the decision variable rather than the power at each time instant. the limits of τ for the dishwasher as well as the washing machine are determined from a pilot project [cit] . the only curtailable appliance considered from a market perspective is the pv. for the curtailment the optimization problem for the pv can simply be stated as:"
"the threat of a malicious insider with a privileged role (e.g. a system administrator) is inherent to any outsourced computation model. abuse by insiders could impact and damage the customer's brand, reputation, or directly damage the customer. although this kind of attack can in a traditional (i.e., non-cloud) computing infrastructure, still customers have a lot of concerns for the cloud provider. cloud customers should conduct a comprehensive assessment of access privileges (i.e. stating who will have access to their data and what level of access they will have) as well as transparency measures. availability of an additional trust system will be of great use as this will significantly reduce the risk of user's blind trust [cit] ."
"filters are the ensemble level representation (figure 1), showing either a set of buttons, if the variable values are discrete as in latin hypercube sampling, or a range slider. summary information is only displayed if a filter is active. filters are predominantly scatterplot controls to reduce visual clutter. the individual-simulation-level abstraction embodied in the scatterplot is the core representation. we chose scatterplots because we wanted to interact with media from large numbers of runs, and points use very few pixels. hovering over individual points (or selecting groups of points to enable a group interface) retrieves images and movies from remote data stores, providing rapid viewing of media outputs that are linked to parameter information. although overplotting can be an issue with this representation, the scatterplot axes and point color-coding are interactively selectable, so the plot can be manipulated to expose interesting runs. since outliers are often targets for drill-down, their unique position or color can be used to further down-select runs."
"by incorporating both direct and indirect control, the developed approach provides a reliable platform for network congestion management. future research on this topic will be directed to the integration of voltage control methods in unbalanced residential networks. the aggregator functionality can be further upgraded by including suitable learning techniques to have better insights into the residential energy usage, unlocking more accurate price adjustments. the agent interaction model needs further modification to expand the approach to multiple aggregators. during the dispatch the predefined programs may not hold due to the uncertainties with load and local generation, and can lead to deviations which may require more or less peak reduction. the inclusion of this uncertainty within the evaluation of the proposed scheme should be further investigated to ensure a more robust and reliable operation."
"the overall performance of the integrated approach is evaluated in terms of the congestion duration, maximum network load and total network supply for a simulation time window of one month. these performance indicators reveal the efficiency of the integrated approach for resolving the congestion while maintaining the reliability of supply. table i summarizes the comparative performance among the market-control, graceful degradation and integrated approach. a significant reduction in the duration of the congestion is realized by the integrated approach with a slight drop in the total network supply. the reduced supply can be explained by the part of the curtailed load, which is shifted to the next day and hence is not considered within the simulation time frame."
"the load profiles are subsequently forwarded to the ta for a network congestion analysis. in case congestion is expected, the ta requests the aggregator for flexibility during the time when the congestion occurs. following the request, the aggregator runs an iterative process to try and procure the desired flexibility from the has by adjusting the dynamic price. an adjusted price is communicated to the has, who in turn send back an adjusted load profile. in case the price adjustment fails to reduce the network load to the associated limits, the ta overrules the market and runs the process of graceful degradation to determine curtailment plan for the next day. the curtailment plan includes the information of the time, location and amount of active power curtailment in the network. based on the developed plan, the ta sends the curtailment signal to the has directly. finally, related has reoptimize their appliances based on the imposed curtailment constraints."
"the table at the bottom provides drill-down to variable values within individual runs. all three views are linked by selected variable (bar chart row, scatterplot color, and table column). the scatterplot and table are linked by selected run (point and table row). these interactions facilitate searching for patterns and relationships, such as identifying that the anomalous points in figure 6 represent simulations with extreme values at the punch tip, one in a maximum stress variable, and the other in a minimum stress variable."
"some cloud based applications, such as gmail, had a great success; but as the diversity of the offerings grows, it increase the reluctance to trust the sensitive data and services on offsite computers. at the enterprise level, in a number of cases, the decision makers keep rejecting a move to the cloud [cit] . at present most organizations are only willing to outsource applications that involve less sensitive information. according to a survey [cit] of more than 500 chief executives and it managers of 17 countries, \"trust is more with the existing internal systems over cloud-based systems due to the fear about security threats and loss of control of data and systems\". those who agree to move to the cloud still demand third party risk assessments or at least ask the cloud providers: initially, the cloud computing platforms appear to be attractive for their low-cost development and deployment capabilities. but as the cloud platforms are used for actual production environments, they require enterprise-level slas. maximizing systemic qualities require integrating the development of these qualities into the design process of large-scale architectures. for cloud computing, the focus of systemic qualities is different from the host-based, clientserver models and web-based models of the past. in some ways the challenge to achieve systemic qualities is more complex. on the other hand, if these architectures are properly designed from the beginning, this can lead to the perfect systemic qualities. each of these obstacles can be classified as the technical or business issues, both of these aspects must be taken care of, for smooth migration to the cloud environment."
"where a is the set of all appliances within a single household, n t the number of time steps in a day and p t the price at time t and p t,a the power of appliance a at time t. the appliance scheduling which needs to be optimized covers two groups of appliances: time shifting and buffered appliances [cit] . for the different groups of appliances it can be assumed that there energy use is independent of one another, allowing eq. (2) to be rewritten as:"
"the last challenge is the visual representation. representations of ensemble data are closely tied to the level of abstraction being shown, ranging from highly abstract to concrete. we define four levels of abstraction for viewing an ensemble: the entire ensemble, sets of runs, individual runs, and individual simulation results. a representation at the ensemble level shows trends or behaviors that are exhibited by most of the runs, including relationships between variables and their values. relationships may include variable correlations, spatial distributions of values, spatial co-occurrences, or temporal shifts in any of the preceding. abstractions for sets of runs display group relationships, such as similarities between clusters of responses. at the level of individual runs, the abstraction explicitly represents runs as separate entities in the visualization. similarities and differences in runs relative to the broader group can reveal the robustness of solution spaces, outliers, or anomalies. finally, the ability to drill-down to the level of an individual run and quantitatively examine its unique input and output data is important for understanding anomalies."
"the public cloud infrastructure is made available to the general public or a large industry group and is generally owned by an organization selling cloud services. resources are dynamically provisioned on a fine-grained, self-service basis over the internet, via web applications/web services. the third-party provider is responsible for provisioning of the resources as well usage billing."
"due to redundancy in the saas and paas configurations, we can increase reliability. with cloud computing systems like applogic offering highly reliable storage as part of the package, many customers are tempted to skip backup. but, data loss and the resulting unplanned downtime can result not just from failures in the cloud platform, but also from software bugs, human error, or malfeasance such as hacking. if you don't have a backup, you'll be down a long time -and this applies equally to cloud and non-cloud solutions. the advantage of cloud solutions is that there is usually an inexpensive and large storage facility coupled with the cloud computing offering, which gives convenient backups. in cloud computing facilities are identical and transactions are mirrored almost instantly, making interruption of service related to hardware problems or data issues virtually impossible. if one of these facilities goes down (e.g. because of a natural disaster), the applications will run on the other site with minimal interruption."
"it is not enough to cluster temporal results in the time series model, we want to understand how differences in the input parameters are related to changes in the responses. although we could have stepped through color-coding the line plots by each of the input variables to discover the relationship between initial velocity and cell deaths, a more efficient approach is to use the canonical correlation analysis (cca) model to explore relationships between variables."
"the process of graceful degradation is coordinated by the ta and is activated only when the aggregator fails to reduce the peak loads through price adjustments. in order to relieve network congestion, graceful degradation limits the consumption level at certain households. the day-ahead curtailment plan is developed based on a target loading level, the voltage unbalance, the available non-firm capacity and the household preferences represented by the maximum frequency and duration of curtailment. upon receiving a curtailment request from the ta, the households optimize their appliances to adhere to the prescribed loading level supplied by the ta."
"by highlighting one of the two dark blue subtrees (clicking on a sparkline selects its subtree members in both the line plot and the table), we can see that although the two groups initially have similar plots, the highlighted set of dark blue runs levels off with fewer overall deaths. as the dendrogram shows, the highlighted runs more closely match the light blue group (whose subtree is immediately above the highlighted group) than the non-highlighted dark blue set. by interactively manipulating the view, we can determine that there are only four unique groups of responses. we use the highlighted rows in the table to identify group members through their id number."
"since residential customers are connected in single phase, active power curtailment at pocs may lead to voltage unbalance in the network. the degree of unbalance is measured by the voltage unbalance factor, vuf and becomes more pronounced along the length of a lv feeder from the substation. as shown in eq. (13), the voltage unbalance factor, vuf is calculated from the symmetrical components of the voltage at the poc [cit] ."
"for the active power curtailment when the graceful degradation is activated, the maximum duration and the frequency of curtailment per household are assumed randomly distributed between 1-1.5 hours and 1 or 2 times per day respectively. the flexible loads, i.e., ev, heat pump, washing machine, dishwasher and freezer are considered to comprise of the non-firm capacity available at each poc."
"with the growing implementation of advanced ict infrastructure and the increasing share of distributed energy resources (ders) and flexible appliances in residential distribution networks, market-controlled operation involving smallscale prosumers has been drawing a significant research interest in recent years. attempts have been made to incorporate the flexibility of residential loads through transactive energy frameworks with varied scopes and aims [cit] . local flexibility oriented market-based mechanisms have been shown to be one of the most promising alternatives for ancillary services like overall system balancing [cit] and network congestion management [cit] . however, in a marketcontrolled distribution network with a dominant share of price-responsive loads, the load of individual households tends to coincide with each other. while the coincidence among the households is generally 0.2 [cit], with price-responsive loads this can easily increase up to 0.6. this results in a higher peak load of the network, and consequently in overloading of network assets [cit] ."
"the dendrogram can also be used to find outliers and anomalous runs. in figure 5, two light blue outliers appear in the dendrogram above the three subtrees that we previously discussed. in figure 4, the highlighted anomalous run in orange initially aligns with the orange group, then transitions to more closely match the behaviors of the white group. the dendrogram shows it as a lone run (fourth sparkline from the bottom) and links it with the white group's subtree below."
"as discussed in section ii, the market-based control involves interactions between the aggregator and the household, where the households respond to the dynamic price sent by the aggregator by adjusting the initial load profile. the price is later altered, if a congestion is expected. first, the response of the devices to the price signals is elaborated, followed by an explanation of the adjustment of price levels."
"where p adj is the incremental price adjustment, s t is the system apparent power at time t and s rated is the rated power. after the updating of the price the system apparent power s t is recalculated with the household load profiles adjusted to the new price p t,new . if the adjustment in the peak load is not sufficient, the aggregator adjusts the price again according to eq. (12) until the required peak reduction is reached or the required price difference becomes uneconomical from the aggregator's point of view. the dso only pays a limited amount to the aggregator to reduce the peak load as it has the backup option of obtaining peak reduction through graceful degradation. the change in peak load generates a sub-optimal load profile from the market perspective. the aggregator has to bear the cost for this sub-optimal load profile while it gets compensation from the dso. if the dso compensation is not sufficient, the aggregator reverts to its original market price as the aggregator needs to be able to generate benefits for the consumer in order to have a viable business model. not reverting back to the original price increases the net energy cost (energy cost plus the dso incentive) above the original level."
"for the simulation of the congestion management approaches, the household load first needs to be determined. as the household load will be assessed differently for different types of devices, the flexible devices need to be modeled separately. the residential loads are therefore subdivided into seven categories: base load, ev, pv, heat pump, washing machine, dishwasher and refrigerator. the base load consists of all noncontrollable devices (tv, microwave, etc.). the required ev charge is modeled based on the driving distance, arrival and departure time [cit] . the charging rate is assumed to range between the 3 and 8 kw. for the pv, the rated power varies between the 2 and 5.5 kw per household, facing in a southwestern to southeastern direction. the heat pumps have a rated power of 0.7 kw to 2 kw, with an additional 2 kw resistive heating element. the washing machine, dishwasher and refrigerator have a rated power in the range of 0.6-2 kw, 0.5-1.11 kw and 0.035-0.140 kw respectively. the original switching times of these appliances are modeled based on reported user behavior. a constant power factor of 0.95 has been used for the analyses."
"looking at the individual-run-level representation in the line plot view (figure 4, upper right), where each line represents cell deaths for a single run, we see that there are four or five distinct groups. the red lines represent runs with the smallest initial velocities and the dark blue lines are those with the largest. we see that cell death, and hence fracturing, increases with the initial velocity of the punch. changing to the parameter space model to view videos from each group, we discover that a high-speed impact stretches and tears the plate cells, leading to earlier and greater numbers of cell deaths, while a slower impact does less cell damage (sometimes not even penetrating the plate), leading to fewer and later cell deaths (see figure 7 )."
"in case of paas, a development platform is provided in the cloud environment. this may include a development toolkit and a number of supported programming languages to build higher level services."
"once the ha determines the switching actions of the devices, by applying the appliance level optimization as defined by eq. (2) - (11), the optimization is performed once again with an additional constraint for each type of the devices, as shown in eq. (27) and (28) ."
"although the present initiatives are too slow to have high efficiency, which limits their commercial utilization in the present scenario. in addition to the topology of the cloud network, the geographic location of the data also matters in some cases. knowing the data location is fundamental to secure it, as there might be important differences between regulatory policies in different countries. cloud computing customers must tackle this issue by understanding the regulatory requirements for every country, they will be operating in. not only the data location, but the path followed by data also matters. according to forrester's \"cloud privacy heat map\" [cit], this is really hard for an application operator to deploy applications at a minimum \"distance\" from the user."
"the slycat architecture integrates data management into our models. typically, results consist of many files per run, distributed across multiple directories, stored on a remote file system. analysis of even a single result variable across all runs can entail significant data management issues due to the size, locality, and organization of the results data. minimizing data movement helps to mitigate these issues. analysis is performed in parallel by remotely launching jobs through a slycat agent on the same hpc where the ensemble was created, as shown in figure 2 . this reduces the scale of data moved from the hpc to that of a much smaller set of analysis artifacts and a handful of results from selected sets of runs. results, such as images or movies, are interactively pulled from the hpc and viewed remotely on the analysts' desktops via commodity web clients. prior to beginning this work, we interviewed analysts at sandia to evaluate workflows, analysis needs, and available tools. 1 slycat resulted from our identification of an unmet need for ensemble analysis. our choices in targeted results data types, analysis tasks, and visual interfaces were made in response to requests from, and in collaboration with, our users."
"the previous two models have provided very abstract representations of simulation results. although image and movie outputs provide familiar representations of volumetric data, without the context of the associated simulation parameters and the ability to simultaneously view and compare multiple instances, media results rapidly become incomprehensible. the parameter space model provides this context. like cca, the parameter space model operates on table data, but it additionally provides filtering and remote access to in situ generated media results, along with group-based operations for interacting with them. the shared attribute space is the parameter table combined with media types. this model can be used for tasks such as parameter optimization, result partitioning, and identifying outliers. other than gathering summary statistics for the variables, no analysis is done by the system. instead, this model is an exploratory interface to assist the user in finding patterns."
"in this research, an integrated solution for day-ahead congestion management is defined by incorporating an advanced market-based mechanism with a graceful degradation method based on flexibility contracts. specifically, a sensitivity-based approach is adopted to consider the impact of curtailment on the voltage unbalance factor at the end of the feeder. the integrated approach combines the advantages of the direct and indirect control methods and ensures a more reliable operation of the network. the approach makes use of the distributed intelligence within an agent-based environment. a detailed case study is performed on a modified ieee european lv test network in order to investigate the efficiency of the proposed approach. the main contributions of the paper are as follows:"
"2) development of curtailment plan: the curtailment plan corresponds to the location, time step and requested amount of active power curtailment at each poc. from the perspective of ta, the decision making algorithm can be formulated as an optimization problem, that minimizes the required curtailment, as shown in eq. (14) . (14) subject to,"
"traditional scientific visualization systems provide tools to explore and compare no more than a handful of simulation results simultaneously. rather than scaling in the number of results, these systems have been designed to scale relative to problem size, focusing on interactive visualization of extremely large results data. however, there are some important types of questions that cannot be answered with just a few simulations. in particular, sensitivity analysis, uncertainty quantification, and parameter studies all rely on generating a collection of simulation runs, typically referred to as an ensemble. an ensemble is a set of related runs, each of which is a sample within a shared problem space. ensemble data is typically large, consisting of paired sets of input parameters and simulation results. the inclusion of the input parameters (or a set of independent variables) is a key distinction that separates ensemble research from research into techniques for comparing collections of results data."
"our goal in ensemble analysis is to enable an understanding of the ensemble without requiring the user to view each simulation's data. this is analogous to using text analysis to understand the content of a set of documents without reading them all. beyond scalability issues, there are three"
"to better evaluate whether the dark and light blue runs form distinct groups, we switch to the ensemble-level abstraction (figure 4, upper left), which uses agglomerative hierarchical clustering. we chose this analysis method because it does not require prior knowledge of the number of clusters, and the dendrogram representation because it mirrors the technique. the dendrogram is initially only drawn to a few levels, with most of the tree hidden in subtree icons, which are the purple triangles annotated with the number of the hidden leaves. each subtree or leaf node is followed by a sparkline, 12 which is an exemplar of the plot shape for runs in that subtree. dendrogram components act as controls, not only in the display of the tree itself, but also in the line plot and the table ( figure 5 )."
"the cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load-balancing between clouds)."
"cca operates on tables of scalar values, providing correlations between two sets of multivariate data, such as between sets of input parameters and output metrics, which form the shared attribute space. we selected this analysis method to perform sensitivity analysis because it maps well to the duality of our data space and provides a many-to-many analysis of the variables, i.e. it simultaneously correlates all the inputs to all the outputs (see our earlier paper for more detail). we chose a bar chart for the representation because we wanted to encode both the magnitude and type of correlation so they could be understood pre-attentively. although the weight values are written in the central column, they are also encoded in the bar length. color and orientation of the bars redundantly encode the positive or negative correlation between rows of variables, with shared color between input and output variable rows representing positive correlations and differing color showing negative correlations. variables are sorted in order of decreasing significance (weight) in each of the input and output sets."
"the cloud infrastructure is operated solely for an organization in case of the private clouds. however, it may be managed by the organization itself or a third party and may exist on premise or off premise."
"the sensitivity of vuf at the end of the feeder to the changes in power consumption is estimated by 40 simulations for each household. in each round of simulation, the vuf at the end of the feeder is recorded, upon varying the power consumption at each household between 100 w and 4 kw with an increment of 100 w in each step. while varying the load of the selected house, the power consumption of the remaining houses is kept fixed. the process is repeated for 200 randomly selected samples from the generated load profiles. fig. 3 illustrates the effect of each household on the sensitivity by means of boxplots. the unbalance becomes more prominent along the feeder and reaches the maximum at its end. it is important to note that the effect of the load at a particular house varies within a very small range. therefore, the 95 th percentile value of the resulting sensitivity for each household is considered in the subsequent analyses."
"the integrated approach utilizes a hierarchical architecture, equipped with agent-based distributed intelligence. a detailed bottom-up markov chain monte carlo approach is used to model the price-responsive household appliances. day-ahead household profiles are generated by scheduling the flexible appliances based on the day-ahead price in order to minimize the residential energy cost. if network congestion is expected, the aggregator tries to procure flexibility from the end-users by adjusting the day-ahead price. in case the price-adjustment cannot influence the end-users to shift their loads up to the desired margin, the dso takes over and selects suitable locations for active power curtailment. to this end, the sensitivity of the voltage unbalance factor to the changes in residential demand is taken into account. this provides a better distribution of curtailment among the phases and prevents a higher voltage unbalance following active power curtailment."
"where, c i t is the decision variables that denotes the amount of curtailment at i-th household; u i t is binary in nature and assumes a value of 1 when i-th household is selected for curtailment; z i t is a binary dummy variable that helps restricting the frequency of curtailment; n h indicates the total number of household in the network and is given by the number of elements in the set of all households, h."
"we use slycat to analyze a solid mechanics problem, consisting of a 15k run ensemble. this ensemble explores changes in material responses as a punch impacts a plate (shown in figure 3 are the simulation and in situ codes, respectively, used to generate the ensemble. given the large number of simulation runs, we did not save volumetric results, choosing to save in situ generated outputs instead. for each of the 15k runs, the ensemble consists of 8 input parameters and 38 outputs (12 scalar results, 16 variables changing over time, 6 event-triggered images, and 4 movies of 1000 frames each). although the total data size was small, only about a terabyte, the complexity is derived from the sheer number and variety of results. this ensemble is used in generating all the examples in the remainder of the paper."
"for instance, the transformer is overloaded by 3.5% and 1.8% at 01:00 and 01:15 hours respectively on the first day. in order to resolve the congestion at these consecutive time steps based on available flexibility, the ta sends a curtailment request of 5.13 kw and of 4.49 kw to house no. 48. an additional request for 4.04 kw at 01:00 hours is sent to house no. 39, due to the higher required peak reduction. the has respond to the curtailment request by optimizing the available devices with the new constraints. the adjusted network load profile reveals fewer peaks as the loads are mostly shifted away from the peak times."
"outages of a service become a major worry when customers have stored all of their information in the cloud and might need it at anytime. given that the customer management interfaces of public clouds are accessible via internet, there is an increased risk of failure when compared to traditional services. in fact, there are more weak points in the chain of elements needed to access the information or application. for instance, web browser vulnerabilities can lead to service delivery failures. a feasible means to obtain a high degree of availability would be using multiple cloud computing providers. cloud providers are well aware of these risks and today provide more information about the current state of the system, as this is something that customers are demanding. salesforce.com [cit] for instance shows the real-time average response time for a server transaction at trust.salesforce.com. amazon has implemented a service dashboard that displays basic availability and status history."
"2) interaction: the interaction among the agents is graphically presented by means of a uml sequence diagram in fig. 1 . the process of the integrated approach starts with the das preparing the projected consumption based on the dayahead price supplied by the aggregator. the ha combines the device profiles into the day-ahead household profile and sends it to the aggregator. the day-ahead load profile of house, i comprises of the flexible load, p i flex and uncontrollable base load, p i base . since the flexible load comprises of buffered and time shifting appliances, p i flex is given by,"
"with increases in the availability and scale of high performance computing (hpc) platforms, ensembles are increasingly being used to evaluate and understand computational models. the trend has been towards growth in the numbers of runs within an ensemble, so scalability is a key requirement for designing systems to explore and interpret ensemble results. in slycat, 1 an ensemble visualization and analysis system that we are developing at sandia national laboratories, we routinely work with ensembles containing thousands of runs. in the last year or two, we have started seeing ensembles with tens of thousands of runs."
"upon receiving the flexibility request from the ta, the aggregator increases the price levels during the times of peak demand. the off-peak price level is further reduced in order to maintain a constant daily average price. the has respond to the new price levels by re-optimizing the flexible loads. the effect of the increase in the price level during the peak is reflected in the load profile, as the maximum load is reduced to 0.99 p.u. from 1.03 p.u. flexible loads are shifted in time and thereby keeping the total energy consumption of 4.84 mwh fixed in all of the cases. fig. 6 presents the resulting load profile when only graceful degradation is applied. simultaneous charging of evs is seen to cause overloading at the midnight of the first day. the coincidence of the flexible loads with usual evening peak results in further congestion in the evening of both of the simulated days. upon violation of the thermal rating, the ta selects the most suitable houses and limits their network access in terms of the connection capacity. the curtailment amount is determined considering the target loading, s target, available flexibility of the households and the maximum duration and frequency of curtailment dictated by the flexible capacity contract."
"each household has multiple types of appliances which will react differently to a change in price. four different types of appliances can be distinguished: non-controllable appliances, time shifting appliances, buffer appliances and curtailable appliances [cit] . the main flexible appliances (ev, pv, heat pump, washing machine, fridge, and dishwasher) in the household are characterized as one of these types of loads. the other appliances are considered to be non-controllable and will not be affected by the dynamic price. the household tries to limit the cost of energy by changing the scheduled operation based on the dynamic price. this can be expressed by the following optimization problem:"
"constraints in eq. (15) and (16) limit the curtailed power in each household within the contractual non-firm capacity, p i nonfirm and curtailable power, p i t,curt at time, t respectively. the curtailable power refers to the instantaneous available non-firm capacity. constraints described by eq. (18) - (21) ensure that the maximum allowable frequency (f i max ) is not violated; similarly, constraint (22) limits the duration of curtailment per household within the predefined limit ( i max ). constraint (17) is imposed to limit the total curtailment within the desired margin of c min t and c max t . the margins indicate the minimum and maximum levels of curtailment and are calculated based on the thermal rating and the target loading level respectively."
"since cell death is tightly coupled with fracturing behavior, we want to understand how cell death evolves over time. we use slycat's time series model to analyze a temporal result containing running totals for the number of dead cells, sampled at each time step (figure 4) . note that the lines are color-coded by initial velocity values, which we chose because the results of analyzing the parameter table with slycat's canonical correlation analysis (cca) model showed that this input parameter has the strongest correlation with output metrics for stresses (see below)."
"the choice of representation for a particular result type is also tied to the mental model of the user community. for instance, circuit simulations produce sequences of voltage and current values over time as outputs. electrical analysts want to see and compare waveform shapes as graphs of amplitude over time, so at least one of the abstraction levels should present the data in this familiar form."
"many re-sequencing projects complemented with publicly available variant data sets are on the scale of several thousands of genomes. tasks such as variant calling, data processing and management, and storing the genome data are most commonly implemented by experienced bioinformaticians within the research groups. the billions of variants from such large-scale projects offer a challenge in extracting candidate variants from specific families or groups specific to a research project. furthermore, the presence of additional samples or controls within the cohort is an invaluable resource that allows users to filter out the common variants. specifically, for non-human data sets there are no or few population frequency databases like 1000g genomes project [cit], exac [cit], and gnomad (https://doi.org/10.1101/531210) to greatly reduce the number of potential causal variants for further downstream analysis. however, the extraction of candidate variants from the entire variant database to a manageable subset still requires the assistance of bioinformaticians in every project associated with the data set. for example, consider a research group with \"n\" geneticists having more than one family to study from the samples within the data set. and each researcher requiring to analyze or filter variants in \"x\" different ways per family. this requires the bioinformatician to perform \"n*x\" filtering tasks per family which is daunting and repetitive. a central installation of webgqt on institutional servers deployed with user data sets would enable geneticists, clinicians or researchers with life science backgrounds to filter for candidate disease variants without requiring computational skills or installing r or webgqt."
"using messaging in an unreliable way requires very few lines of code, using it in a reliable way takes much more time and thoughts. all the unexpected behaviors should be taken into consideration and handled properly, most of the code needed is about exception handling: the problem get even more complicated if a given application need to support multiple protocol and/or multiple programming languages yielding to code duplication and bugs multiplication."
"the phenotype file page is used to input the ped file to query the gqt indexed variant files. an sqlite sample database of the ped file is created. the sample database has a single table with sample name and sample position. the sample database created on the pedigree file (ped) complements gqt for rapid and complex queries based on the individual's phenotype/ancestries/ relationships, provided in the phenotype file. by clicking on the \"browse\" button the user uploads a ped file (.ped extension) and can visualize the ped file as a data table by clicking \"view samples\" (supplementary figure 1a) . this gives a summary of the number of cases, controls or carriers used in the study and the \"phenotype\" column in the data table can be used to review the affection status of the individuals (supplementary figure 1b) . subsequently, by clicking the \"çreatedb\" button, a sample database that describes the samples in the target bcf file is created. once the sample database is created, the subset of variants that meet the user criteria can be quickly identified by choosing one of the analysis modules. an example of performing variant filtering with a dominant module is shown in supplementary figure 2a with the progress bar displayed at the bottom of the page."
the interest for messaging as an integration middleware increased both inside cern (from the computer center to the accelerators control) and outside (from low level monitoring to experiments dashboards).
"in summary, webgqt is a web application built with gqt tools with an emphasis on filtering for candidate disease-causing variants from larger cohorts of samples. we expect that webgqt suits the need of research/clinical groups that require a common platform for storing, management, and filtering variant data large-scale genomic data sets with many samples. overall, webgqt serves as a useful tool for disease genomics e.g., for candidate disease variant filtering among families, pedigrees or larger cohorts, as well as filtering population specific variants in short time. the availability of webgqt as a web application, stand-alone installation on user computer, and a stand-alone installation on local servers of a research group allows the user to protect or share sensitive data."
here are the proposed changes which address the three areas identified in order to provide a better messaging service for wlcg: dedicated messaging services also have a positive impact on security:
"to evaluate the efficiency of webgqt, we compared the execution time of webgqt with canvasdb, an existing tool/ database on 1000 genomes data that has 4.4 billion variants from 1,092 whole genomes from 26 populations. the task is to find the population-specific variants i.e. variants present in at least 10% of the individuals in the target population and at most 1% in all the other individuals from the other populations. previously, canvasdb has reported the execution times applying the same filtering strategy on the phase 1 data set with 4.4 billion variants and 1,092 individuals [cit] . here, we have implemented webgqt with the same strategy on phase 1 data set to allow direct comparison with canvasdb. the filtered variants count and execution times have been reported in table 1 . for the majority of the populations, the execution time of webgqt on the phase 1 data set ranged from 1 to 6 min while it took more than 30 min to find ibs, yri, and lwk population specific variants. canvasdb completed the same task in the time range of 29 min to 20 hrs. on the phase 1 data set, the time required for the population-specific variant filtering by webgqt has been always several times less than canvasdb ( table 1 ). in addition, we have also implemented webgqt on the phase 3 data set, which has nearly twice the number of variants (8.4 billion) and more than twice the number of individuals (2,504) in phase 1 data set. strikingly, the execution times of webgqt for population-specific variant filtering on the much larger phase 3 data set are in the time range of 3 to 10 min, except for lwk which took 13 min to query and write the results to gui. the maximum time taken by webgqt for both querying and writing to disk is several times less than the minimum query time taken by canvasdb on 78% (11/14) of the phase 1 queries and 100% (14/14) on phase 3 population queries, which signifies that the performance of webgqt is increased with increased cohort sizes. ( table 1) . the conversion of vcf output to tabular format is implemented with vcfr package [cit] within webgqt."
"second, webgqt requires a ped file that defines the sample information, phenotypes, population groups, case-control status, and relationships of the samples in the target vcf/bcf file. the ped file should contain the tab-separated fields of individualid, phenotype, sex, and population in the first line to perform pedigree analysis, case-control studies, and population comparisons ( supplementary table 1 ). the individualid and phenotype fields are mandatory for analysis in pedigree and casecontrol modules. the \"individualid\" column represents the sample names in the target vcf/bcf file. the \"phenotype\" column represents the affection status of the samples. the phenotype coding for the samples in the ped file is as follows: 0 represents samples to exclude from the analysis; 1 represents unaffected parent or unaffected offspring/siblings or other unaffected individuals (controls); 2 represents affected offspring (cases); and 3 represents affected or carrier parents. individualid, population, and sex fields are mandatory for population modules while other optional fields can be present. sex is coded as 1 for males, 2 for females, and na if unavailable. when ped files are uploaded, the interface allows users to review the sample relations and affection statuses from the data table before creating the sample database. a sample database has to be created to enable rapid variant filtering of cohorts with different sample attributes in the associated vcf/bcf file. the sample database created in the ped file complements the gqt index files created in the original vcf or binary vcf (bcf) file to identify the compressed target sample bitmaps and retrieve the genotype bitmaps in vcf format."
the architectures presented in this paper (services: independent brokers; software: components to be assembled) can support this multidimensional growth in the use of messaging for wlcg.
"here, a web server for gqt (webgqt) is developed to support inheritance model-based filtering among a family or a group of individuals in the study cohort utilizing the gqt index files to reduce computation time. specifically, webgqt is developed with an emphasis to provide a web server that can be utilized as a common platform across members of a research group without requiring independent access to the large variant databases. we present a graphical user-interface (gui) application to identify candidate disease-causing variants according to user-defined inheritance patterns that include dominant, recessive, de novo inheritance models, population comparisons, and sample-based filtering from large cohorts. the pedigree-based studies and case-control studies aim to identify rare disease-causing variants that are present in affected individuals and absent in un-affected individuals. the population-based studies filter for variants with differential mafs among two populations. we present several use-cases available with webgqt, representing mendelian inheritance models, case-control studies, and population studies that can be implemented on human and other model species. this is validated by using webgqt on the data from the study [cit], which identified the causal variant segregating in a recessive inheritance pattern. the key features include:"
"in contrast to providing the software only as a stand-alone application or only as a web application, we provide webgqt in both forms. the web application provides a resource for the 1000 genome project phase 3 data set via the web interface that allows users to query the 1000 genomes data set by uploading the ped file defining their phenotypes of interest or population groups or by uploading custom data sets. alternatively, the users can install their own version of webgqt as an r package on local computer (linux/mac os) and can query by deploying or uploading custom gqt indexed data sets where the data will be available only to the user. webgqt can also be deployed on a local or remote server with their genomic variant data to allow multiple users within the group to query the variants by only uploading only the project specific ped file. this allows users within the group to filter variant data without installing r or the webgqt application or without requiring for independent database creation. further, webgqt installation on local servers can be accessed or limited only to the users with the web-address to the application."
"for the population module, the ped file should have the population groups and sex specified with the fields population and sex, respectively. this module filters the variants between two populations defined in the ped file by allele frequencies or individual count or by sample."
"webgqt is implemented using r shiny server to provide a graphical user interface to query variants stored in gqt index files. gqt utilizes sqlite database to query the samples and phenotypes with the gqt indexed variant files. the webgqt installations and analyses were performed on an ubuntu 18.04.1 64-bit system, with 16 cpus and 80 gb ram with 700 gb ephemeral hard disk and gqt version 5.5.29 and r version 3.4.4 (https://vm1138.kaj.pouta.csc.fi/webgqt/). webgqt can be installed as an r package on linux/mac os operating computers as well as on local servers to serve within a group of users. the web application is available for download and installation as an r package with instructions at https://github. com/mehararumilli/webgqt."
message queue (mq): file-system based message queue with a simple yet robust api which supports concurrent access messaging transfer agent (mta): program that can transfer messages between a broker and a message queue (all combinations)
"a messaging service could be provided by a machine that already provides other services such as web services or database services. after all, we can see a messaging broker as just one more daemon to enable on a server. at the other end of the scale, a single messaging service could have tens of dedicated servers, running nothing else than the broker daemon. scaling up may require more powerful hardware and/or increased network capacity. it all depends on the application requirements."
"it started as a prototype to evaluate its usefulness in some parts of the grid middleware, like monitoring, with a relative low requirements in terms of security, scalability and reliability."
"gqt indexes the standard vcf file with the command \"gqt convert bcf -i input.vcf/input.bcf\" to allow \"sample-centric\" variant filtering which is used as input by webgqt. gqt indexing creates three metadata files; a compressed index (.gqt), a compressed summary of the variant metadata (.bim), and a variant id file (.vid) file, which are only a fraction of the source vcf/bcf, and thus saves storage space and improves performance query over the existing methods. the conversion pre-process efficiently indexes and compresses the variant data in vcf/bcf format as described in the original study [cit] . creating gqt index files is a one-time pre-processing step on the target vcf/bcf file upon which numerous queries can be performed by multiple users from different projects."
"in the results \" table\" panel, the user can apply sorting of individual variant columns to inspect the results or can download the results from the download buttons (supplementary figure 2c) . download vcf button retrieves the output in the standard vcf (.vcf) format, which can be used with any third-party tools compatible with vcf input, e.g. bcftools for custom filtering, vep for gene annotation. \"download table\" generates a text format (.xls) file of variants which has a similar format to the variants displayed in the table pane in the results page. this file can be analyzed in excel to further fine-tune the results."
messaging enables loosely coupled architectures where producers and consumers do not need to know about each other; it enables asynchronous communications and offers high flexibility to client applications.
"it is important to note that the data uploaded to our webserver is temporarily available only to the user during the user session and is not retained or distributed or stored on our servers. however, it is the user responsibility to comply with the data security and privacy while uploading sensitive or clinical human data. we encourage researchers to copy and modify the code to suit their specific research needs and local installations and also welcome the contribution of users and experts in future development of the app. the development of the app is ongoing, and we intend to improve on the speed and analysis modules."
egi: this is the service which is operated by egi and used by the grid operational tools. atlas-ddm: this service is used by the atlas experiment for several applications like the ddm tracer and other operational tools; it consists of 2 uncoupled production brokers plus 1 testing broker. the two production brokers are running apache activemq and the test broker is running both activemq and activemq apollo [cit] . dashboard: this service is used by the dashboard team for their monitoring application; it consists of 2 uncoupled production brokers plus 1 testing broker. like the atlas service the two production brokers are running apache activemq and the test broker is running both apache activemq and apache activemq apollo.
"4.3. loosely connecting the services that need to exchange messages with a monolithic service, any client can connect to any broker and potentially receive or send any message (provided that security allows it). once we use different services, how can messages been shared between applications? we recommend to use shovels for this. these are dedicated applications that do nothing but copying messages from one broker to another. they scale very well: if you need more throughout, just throw more shovels at it. by shoveling only the messages that need to be shoveled (e.g. with filtering), they minimize the load on the network and on the brokers. they are also compatible with the proposed enhanced security since, from the broker point of view, they appear just as normal clients that need to be authenticated and authorized."
"we can use lego tm bricks concept: small, robust and flexible components which can be combined into complete solutions addressing the most common use cases. the simple bricks which suit messaging needs are:"
"beyond this, we foresee that the same kinds of services and components will be used outside of the grid. for instance, at cern, the agile infrastructure project is about to use a dedicated service running activemq apollo and rabbitmq."
"a lot of research and investigations have been conducted by the cern messaging team [cit] and the emi messaging product team [cit] . the requirements have been analyzed, several messaging technologies have been evaluated and the service architecture has been defined in order to satisfy the foreseen increase in demand."
"in the future, we expect many more applications using messaging and therefore we expect both the number of brokers and the number of dedicated services to grow significantly."
"webgqt brings the power and performance of gqt to a wider audience, including researchers from bio/medical field and clinicians without bioinformatic expertise, by replacing the command-line interface with an intuitive web gui. this includes pre-built modules to perform customized filtering analysis for mendelian disease and population studies. some previously published methods like gemini [cit] and canvasdb [cit] have similar filtering modules to webgqt. however, the major distinctive feature of webgqt is its use of gqt on the backend, which uses an \"sample-centric\" strategy for indexing and mining large data sets and the word-aligned hybrid (wah) bitmap indices strategy for data compression that maximizes performance over all other existing methods [cit] . like variant filtering tools that require pre-processing of the variant data, webgqt also requires pre-processed gqt index files which is the bottleneck for attaining better performance involving big data sets. the indexing offers webgqt an advantage in exploring massive data sets of thousands to millions of genomes while similar tools such as canvasdb [cit] are limited to tens of thousands of samples from whole-exome experiments, which are 1% to 2% of the genome. as shown in the results, webgqt outperformed canvasdb in the most complex population specific queries on the 1000 genomes data set with twice the number of samples compared to the canvasdb data set. webgqt can also be used to query non-human data sets. the case-study demonstrates the applicability of webgqt on other model species by identifying a recessively inherited known diseasecausing mutation, while the existing companion method gemini solely supports human variation. in the case-study, the use of 329 internal controls by webgqt allowed to filter out the common variants which not only emphasizes the importance of including internal control cohorts in the absence of population frequency but also highlights the scalability of webgqt with many samples in the vcf. the scalability is further evident by implementing webgqt on 2,504 genomes from the 1000 genomes data set. in addition, the various filtering modules across pedigree analysis and case-control studies, allows the user to apply expert knowledge to query the variant database in different possible ways. in the absence of the parent's genomes of the affected individual, casecontrol module facilitates to filter for specific genotypes among the affected individuals contrasting with healthy controls."
all the applications use stomp (mainly) or openwire [cit] (rarely) to access the brokers that run either activemq 5.x or activemq apollo (that should become activemq 6).
"better reliability must also come from the messaging clients themselves, it is not enough to be implemented on the server side. designing and writing a solid message client application is not straightforward and error handling is often neglected."
"to support this evaluation effort, egi operated a small cluster of four tightly coupled brokers running apache activemq [cit] : two at cern, one at auth and one at srce [cit] ."
"when multiple machines are used for the same service, we recommend to have them completely independent. this may put some constraints on the message clients (that may have to contact several brokers to send or receive the messages they want) but this provides true linear scalability. in the typical grid use case (many distributed producers, very few consumers), this is very easy to achieve as illustrated in figure 1:"
"the first step in using webgqt is to select the type of variant database to perform the query. webgqt is deployed with 1000 genomes phase 3 variant data set and the user can query the phase 3 data set by choosing \"1000 genomes\" as the input data set. this provides a platform to query 2,504 individuals from 26 populations, simply by choosing the desired samples in subsequent steps for a specific query. to use webgqt on a custom data set through the web server, the user is either required to upload the gqt index files by clicking the \"upload vcf\" button or to deploy the application with a default data set on their personal computer or server. the latter functionality allows the provision of web services within research groups and helps clinicians to explore patient data through their local installation on their own servers or computers. the \"upload vcf\" page also shows the instructions to prepare the input gqt index files on the target vcf/bcf."
"exome sequencing, genome sequencing, and gene panel sequencing methods have become the de facto methods for studying the heritability of human and non-human genetic diseases involving small pedigrees to large-scale population cohorts. the current wealth of sequenced genomes produces billions of genotypes in the variant call format (vcf), requiring publicly available programs to effectively and rapidly filter candidate variants for personalized disease and population genomics. there are tools to analyze large vcf files, such as gemini [cit], canvasdb [cit], vcf-miner [cit] and varsifter [cit], which have specific pros and cons in terms of large file handling, query time, graphical user interface (gui), memory requirement, and application to non-human genomes when compared with each other. a recent study has shown that vcf-explorer [cit], a gui program, has out-performed the standard queries in speed. on the other hand, genotype query tools (gqt), a command line software [cit], was developed to query and scale-up to the billions of loci from the uk 100,000 genomes project [cit] and the expected millions of microbial, plant and animal genomes [cit] using a word-aligned hybrid (wah) compressed bitmap index. the gqt algorithm uses a samplecentric indexing strategy that is orders of magnitude faster than existing methods to query genotypes, phenotypes, and familial relationships. the 1000 genome project consortium has extended to 2,504 individual genomes in phase 3 and gqt has become an integral tool in the 1000 genomes project to expedite such massive data sets [cit] . additionally, users use the gqt command line interface to filter inherited variants among small pedigrees, case-control variant filtering, and comparing variants among different cohorts. while gqt is available to a wide audience as command line software, the difficulty in constructing queries faced by non-it or non-bioinformatics researchers has limited its applicability among many users."
"messaging is a dynamic technology and will in parallel also change. activemq is getting to the end of its life-cycle while other broker software like activemq apollo, rabbitmq [cit] and qpid [cit] get more and more solid for the future. in terms of protocols, amqp usage will very likely grow now that it has matured (the 1-0 [cit] )."
this effort lead to the wlcg messaging roadmap [cit] that aims to address the combined requirements of potential applications using messaging in the wlcg context. this roadmap lists main areas for improvement as well as concrete recommendations.
"webgqt utilizes gqt indexed files as the variant database and serves as a webtool to filter candidate variants more rapidly by the user criteria. webgqt is implemented using the r shiny server, which provides a graphical user interface (gui), and variant database queries are performed using gqt in the backend, with the filtered variants rendered as data tables in the gui. a new installation of webgqt can be done on a personal computer using rstudio as an r package. alternatively, the users can host their own version of webgqt on a local or remote server with nginx http server to act as front-end proxy to shiny server. additionally, we host the application on our institutional server where users can explore the features and upload the data for analysis. each data upload creates a new instance and is only viewable to the specific user during a single browser session. in order to allow multiple users to access the application, the data upload size per user is limited to 50gb. an overview of the webgqt system architecture is shown in figure 1a . the webgqt application has been extensively tested on chrome, firefox, and safari browser environments. the workflow for implementing webgqt is shown in figures 1b and 2 . in the first step, the user selects the default data set or uploads the variant database. subsequently, a ped file with sample relationships or affection status is uploaded, and a sample database is created. in the final step, a filtering module is chosen and analyzed to obtain the variant results ( figure 1b) . the gui provides documentation for each module about data preparation, input file format specifications and parameters to be used in each module. the gui of webgqt is shown in figure 2, illustrating the workflow assuming a dominant inheritance mode of filtering."
the best way to manage these components is to use supervisors [cit] . like in erlang otp it is possible to declare hierarchies of supervisors and workers to build services. hierarchies are useful and important because supervisors can be configured to handle children failures in a custom way.
future usage of messaging in wlcg should follow guidelines and recommendations for a more sustainable messaging service. it is fundamentally important to be broker and protocol agnostic when making usage of messaging; it is required in order to have the flexibility to switch easily between different technologies and avoid to remain stuck on a legacy technology.
applications using messaging should have a solid architecture in order to offer redundancy and reliability. it often happens that reliability is expected from the broker side but not reflected on the client side. making full confidence on the broker side is a common mistake on integrating an application with messaging.
"1. scale-up to efficiently query the stored genotypes in gqt index files from the upcoming millions of genomes [cit] 2. rapid filtering of massive data sets relative to applications such as canvasdb [cit] 3. mendelian inheritance disease models for pedigree-based studies, case-control studies, and complex population comparisons 4. control for minor allele-frequencies (maf) within control cohorts 5. cross-species support (unlike few other tools that are limited to only human data sets) [cit] ) and 6. the flexibility of running as either a web service or a standalone webserver for use within and between research groups."
"variant count: the variant count module allows users to count the variants of selected genotypes (hom_alt, het, het hom_alt) in a specific sample selected from the dropdown list. this operation is orders of magnitude faster compared to the other modules as it only gives the count of the variants instead of writing the output to vcf file."
"security: security was not a priority in the beginning, the situation changed and it has now to be taken into account. scalability: more and more applications used or wanted to use messaging, yielding to a major increase in capacity requirements. the monolithic model which was in use could not easily scale. different applications may require different broker tunings and in order to get a very good performance, one may have to select the best technology combination for each use case. availability/reliability: although the four egi brokers were part of a kind of cluster, the whole service needed to be stopped during some interventions (e.g. software upgrades or major configuration changes). in addition, some applications had single point of failures (e.g. apache camel [cit] to aggregate messages): the unavailability of one machine stopped the application."
"establishing the causality of the variant as disease-causing variant requires hypothesizing analysis strategies with different modes of inheritance across pedigree, case-control, and population modules in the gui. each module is constructed with independent analysis models for studying with familial cases or trios or non-familial case-control samples or group level comparisons."
"webgqt is a gqt database dependent web server and, therefore, requires pre-indexed gqt files that serve as a bottleneck to attain the best query performance among largescale genome projects. enabling native vcf support will require to perform the indexing of the vcf file on the server hosting the application which will consume hours of computational time especially for large-scale data sets and, therefore, disabled. webgqt is not a variant annotation tool and also does not require gene annotations in the input vcf/bcf. if an annotation exists, it is utilized only to summarize the filtered variants across the gene features. this unrequired gene annotation extends the applicability of webgqt to any model species and the user can opt to upload annovar [cit] or snpeff [cit] pre-annotated variant data in vcf format for any species. the current version of webgqt does not natively support the popular vep annotations, in which case the filtered variant summary across gene features are missing, although variant filtering can still be performed. furthermore, webgqt does not support the variant data in gvcf format. while gvcf is popularly used to store variant and non-variant information from wgs samples, it is not widely supported for downstream analysis and also restricts the merging of targeted or exome samples with the genome sequencing data sets."
machine. svm is a supervised learning classifier. the cost estimation function in svm gives special emphasis on avoiding overfitting by looking for global optima. this leads to an optimum hyperplane. that is why it is also known to be a large margin classifier. by default svm is a binary classifier. but it can be utilized for multiclass classification by using one-vs-all method.
svm learns by applying structural risk minimization principle [cit] . as a classifier svm shows low bias and high variance [cit] . one of the highlights of svm is its ability to apply kernel methods to implicitly map data into the required dot product space.
"a subtree is considered as a constituent sub tree from a node in the original tree. it consists of all of its descendants from the current node in the original tree. of a tree the total number of subtrees can be generated are much lesser than the substructures generated using sst and pt. figure 1 (a) shows some of sts of sentence \"presence of beta 2-m. \""
"support vector machines (svm) is one of the most efficient and popular machine learning approaches which are being used in ner and other related tasks. it is basically a binary classifier which finds separating hyperplane with a large margin. that is why it is also known as a large margin classifier. here the data (in ner task-words) is represented in the form of feature vectors. the similarity between two words is computed as the dot product between the corresponding vectors. the features used are mostly binary, which takes value one when that feature satisfies for the current word and zero otherwise. in the ner task most commonly used features are the current and surrounding words, suffix and prefix information of the words, and so on. a previous word feature can take any value from the lexicon (all unique words in the corpus) which designates the previous word. in this way only to the feature \"previous word\", a range of binary features in feature space is included (total number of unique words). similarly binary representation is there for other features also. this way, the total number of features for the ner task becomes huge. so the computation of dot product between the vectors is a tedious task."
"the scientific world journal and \"i-named entity\" (for rest of ne words). and the words which fall outside these classes are annotated as \" \" [cit] ."
"seeing the advantage of kernel techniques over extensive feature based model, lots of kernel techniques have been developed over the years. string kernel [cit], tree kernel [cit], clustering based kernel [cit], kernel-based reranking [cit], dependency tree kernel [cit], tagging kernel [cit], and so on are very few among all the kernel methods proposed for various text analysis tasks. unfortunately all the kernels proposed for various text processing tasks are not applicable in the ner task."
most of the existing ner systems which are built using feature vectors use word syntactic and shallow parse information for classification. an entity name in different contexts can designate different entity classes. therefore the information of semantic correlations also plays a major role in classification of an entity.
"alternatively, the similarity between the words can be measured by using an appropriate kernel function. a kernel function helps in establishing an implicit relation between two words by mapping them into an alternative dimensional feature space. this saves time and effort of explicitly selecting features into the feature set for improving the overall efficiency. in the literature we found that various kernel functions have been proposed and used in various text processing tasks. most of these techniques compute the distance between two sequences by finding the number of 2 the scientific world journal similar subsequences. in string kernel the distance between two words is computed by measuring the similarity in character sequences [cit] . the tree kernel finds the distance between two trees by finding the similarity between their tree fragments [cit] . several variations of tree kernel are there, among which subtree kernel, subset tree kernel, and partial tree kernel are popular [cit] ."
"in this paper we have proposed a ner task-specific kernel function. the proposed kernel function is basically a modification of the existing tree kernel. but it is capable of handling the complete ner task, which the conventional tree kernel is incapable of doing. the proposed kernel is achieving a reasonable accuracy without the use of any domain-specific knowledge. the performance of the system can be improved further by using proper domain-specific information. the task can also be viewed as a two-phase process as done in the literature and the kernel can be applied in both phases separately. these lead the future scopes of work using this kernel."
in general sst kernel shows better accuracy than st and pt kernels. the pt shows lower accuracy than the sst but shows a better accuracy on dependency structures [cit] .
"in the literature we observe that the use of task-specific kernel for ner task is limited. string kernel finds similarity between two words by computing character similarity and therefore not much effective in ner task. tree kernel considers a sentence as an entity and finds a tree representation (e.g., parse tree) of the sentence in order to find the similarity. but in ner task the distance between the individual words has to be computed. therefore the conventional tree kernel is not directly applicable in this task. we observe that in a few ner systems the tree kernel has been applied for boundary detection or reannotation. there the task is viewed as a twophase process; the first phase identifies the class of the words and the second phase detects the boundary. in those tasks, for entity identification other machine learning classifiers like crf are used and the tree kernel is used for named entity boundary detection [cit] or reannotation [cit] ."
among all the classifiers it has been found in the literature that svm is more efficient as a classifier. the ease and efficiency in implementation of kernel techniques into the classifier is the most attractive part of svm [cit] . ner task has much been explored for many decades. there have also been language independent ner systems which use svm as classifier [cit] .
named entity recognition (ner) is a task of identifying the named entities (ne) from texts. in a text nes are the pivotal elements to further contextualise the data. that is why ner has huge application in various text mining tasks.
"by minimizing the parameter ⃗ along with the cost error, it finds a maximum possible distance of the hyperplane to its support vectors. this leads to the optimum hyperplane [cit] ."
"a semantic parse structure using parts-of-speech information is built on a complete sentence. therefore a tree kernel is subjected to find the distance between two trees, that is, two sentences. but to perform ner task the distance between words has to be found rather than the sentences. when sst, st, and pt are used in the kernel space for ner task, the kernel space fills with redundant fragments. this is because consecutive words belonging to the same sentence would share similar fragments. that is why the existing tree kernels are not applicable in task like ne identification. moreover this leads to performance degradation and longer execution time in ner task."
"the sl decreases the structural redundancy of the constituent trees in the kernel space. figure 3 shows the constituent trees of sliding windows with markers. the (a) and (b) in figure 2 show the sliding windows for words \"presence\" and \"of. \" the constituent subset trees generated at the same nodes of original example tree mentioned in figure 2 show dissimilarity with respect to the words belonging to the same tree. subtrees (a-1) and (b-1) (in figure 3) are different even though they start from the same node \"np\" which is the left child of node \"s\" in source example tree (shown in figure 1 ). the tree (b-1) also includes word \"2-m\" as leaf node and its preterminal which (a-1) does not have. and (b-1) has cw marker on the word \"of \" as preterminal, whereas (a-1) has it on word \"presence. \""
in this paper we have proposed a novel kernel function for the ner task which is tangled to do both named entity identification and named entity boundary detection. this kernel will execute the complete ner task on its own. the design of the kernel is motivated by the tree kernel.
"similar to the tree kernel, the proposed kernel also uses a tree representation to the sentences. but instead of considering the whole tree we focus on the subtrees where the target word and its surrounding words are present. here the target word is represented by a tree fragment where only the target and its surrounding words are present. to these fragments we also include additional marker nodes as parents of the target words. we name the proposed kernel as the \"sliding tree kernel. \""
"for this similarity function using dot product space to work, the data has to be normalized to 1 and has to be mapped to the dot product space. this requires a mapping function ℎ :"
"data. in table 4 [cit] data. zhou and su [cit] achieved the best accuracy ( -score of 72.55) using the data. in that system they have used extensive domain knowledge like name alias resolution, cascaded ne resolution, abbreviation detection and so on. without the domain knowledge the accuracy of the system is 64.1%. although our system is not achieving a higher accuracy but without using any domain knowledge it shows a competent performance."
this makes sl a task-specific kernel for tasks like ner. it easily identifies the word with generated constituent trees and generalizes for an optimum hyperplane to be found. the distances between the trees are calculated by counting the common substructure of sl trees. as in tree kernel the above kernel space is mapped in the vector spaces of . then the distance between the words is calculated using the dot product similarity measure. in ner task it shows a competent ability and efficiency both in word identification and boundary detection.
ssts are more generalized than the st structures. but it has been constrained that an sst should not break any grammatical rule. that is why the tree substructures of sst may or may not contain the tree leaves but it has to have its entire preterminal nodes in candidate trees. the associated kernel function measures the similarity between two trees by counting the number of their common subparts. the above kernel space is then converted to equivalent vectors of using the mapping function. and the distance between trees are found by the classifier into the dot product space using the kernel function. by this a kernel function identifies wether tree subparts (common to both trees) belong to the feature space that we intend to generate [cit] .
"3.3. tree kernel. tree kernels are intended kernel functions for semantic parse trees. it identifies trees in the form of tree fragments. this way it can identify trees without explicitly considering the entire feature space. tree fragments are nothing but the different substructures of a tree, where each terminal of the grammar is associated with the tree leaves. in the tree kernel a syntactic parse structure is considered. the popular tree kernel methods used are subtrees (sts), subset trees (ssts), and partial trees (pts) kernel."
"looking into the various literatures, tree kernel has never been used to do complete ner task. it has always been partially used for ner subtasks like named entity boundary detection or reannotation."
"the data points fetched to the classifier are in the form of feature patterns. these feature patterns are used to generalize the data points. but defining an explicit feature set to the problem can be huge and tedious, whereas a kernel function helps in implicitly generalizing the data points with a new set of features, that is, in another dimension."
"ner task is the primary task in biomedical domain. many systems using various supervised learning classifiers, like hidden markov model [cit], maximum entropy [cit], conditional random field [cit], svm [cit], and so forth, have been proposed. a supervised learning classifier for ner task learns its classification model using a training data. [cit] data."
"2) histology: examination of the histology slides indicated no evidence of tissue damage in the majority of specimens. no evidence of acute inflammation or short-term healing was apparent in prostate or liver, which would have been suggestive of an acute injury from the needle steering. in the kidney slides, inflammatory cells were present. in some slides, these cells were organized along narrow, nearly linear regions, which were likely locations of the needle tracks (see fig. 12 ). neutrophils were not apparent in kidney sections, indicating a lack of evidence for suppurative infection caused by the needle trauma. no signs of major or chronic damage were noticed."
"this time, the reason is that the recursion laws provide an effective truth-preserving translation from all pal-formulas with dynamic modalities into the s5 base language, while for that static fragment, pal is a conservative extension of s5. there is an obstacle. update semantics recurses on the structure of propositional formulas viewed as updates, whereas pal does not recurse on inner structure of announcements !ϕ, but on post-conditions ψ for modalities [!ϕ]ψ. hence, we enrich pal with 'conversational programs' built from actions !ϕ by standard operations of union and sequential composition. the following translation can then be defined:"
"thus, in the study of consequence relations, implicit and explicit approaches live in harmony, and we can often perform a gestalt switch from one to the other. such switches also suggest precise mathematical system translations in our earlier sense."
"the main contribution of this work is to mitigate the impact of perching by controlling the pose change speed with a tilt-rotor-based airframe. since the non-tilt approach only changes its pose as fast as possible to prevent falling by slippage, the impact could be high enough to damage the wall or the drone itself. therefore, experimental tests are conducted to verify the performance of the mechanism and control algorithm by measuring the angular speed of the pose change and comparing it with that of a non-tilt approach. in the experiment, we set the target angular speed to 1.0, 3.0, and 5.0 deg/s for the proposed approach, and measure the angular speed with an imu installed for flight control. the results are shown in fig. 13 . in the non-tilt approach, the drone just changes body pose until it sticks to the wall with the maximum pose change speed. however, the proposed approach is able to control its pose speed to follow the target speed after the intersection zone (phase 2). as for the soft landing, the data log includes the state of the perching from normal flight to pose change. while the pose is changed very rapidly during phases 1 and 2, the average angular speed is 0.98, 2.91, and 4.82 deg/s at the final state (phase 3), which is very close to the target speed of 1.0, 3.0, and 5.0 deg/s, respectively, as shown in fig. 13 and table 1 . however, the non-tilt approach changes its pose within 1.5 seconds and the average angular speed is about 84.72 deg/s, which is significantly higher than the soft landing approach. consequently, the experimental tests show that the proposed system allows the drone to perch on a vertical wall with a desired angular velocity and guarantees a soft landing. in order to verify the effect of reduced perching impact, we installed an accelerometer on the drone and target wall, compared acceleration data, and calculated the impact for both approaches. regarding the non-tilt approach, when the front part of the drone firstly contacts the wall, low level of impact force occurs as shown in fig. 14(a) . within 1.2 seconds, a drone completes perching process by changing its pitch angle and the rest of its body bumps into the wall. the second contact causes high level of impact as shown in fig. 14(a) . the calculated impulse of the first and second contact was 0.90 and 4.76 kg·m/s, respectively."
"for needle shape analysis, our custom matlab program fits a circle to the needle reconstruction, eliminating outliers greater than 1.5 standard deviations away from the mean until the result converges. if convergence does not occur before all outliers are removed, the program will fit a straight line to the data. the program returns the circle or line equation and a root-meansquared standard error associated with the fit."
"this section describes the node representation of the map. we first explain the node map and how obstacles are represented in it. later, we describe how path planning is done on the node map."
"this may not be an easy paper to classify qua style or results, but we hope that the reader will benefit from looking at logical system design in our broad manner."
"the shortest path calculated by p3dx is shown in figure 9a . once p3dx starts its navigation, two new obstacles are placed as shown in figure 9b . p3dx updates the map with new obstacle shown in figure 9c . it plans a new path towards the goal shown in figure 9d . p3dx shares the new obstacle information with turtlebot and tracks the confidence of the obstacle (figure 9e ) which decays with time, shown in figure 9f . as explained earlier in section 3, when obstacle 1 is observed again at the same position, its confidence is reset to one. this is shown in figure 9g in which p3dx resets the confidence threshold to one and this information is also shared with turtlebot. figure 9h shows that one more obstacle 2 observed by p3dx and its information is also shared with turtlebot. figure 9i,j shown the resetting of obstacle's confidence values upon observing it again. the map updated by p3dx is shown in figure 8e ."
"but first it may seem time for a choice. is intuitionistic logic or epistemic logic better or deeper as an analysis of information and knowledge? should we prefer one over the other? many philosophers think this way, but we feel that this adversarial attitude is not very productive, and it also runs counter to known mathematical facts about system connections (for a similar, but more general criticism, cf. [cit] )."
"fails in the domain of physical quantum phenomena. there are of course reasons for this failure: measurements disturb the current state of a physical system -but this is left implicit in quantum logic. there is a long tradition of research in this area, which has resulted in an extensive algebraic and modal theory of quantum logics."
"in this section, we review the studies pertinent to the framework of our proposed system architecture, namely afarls. the scope of the studies focuses on knn, elm and src algorithms, together with their respective enhancements in order to facilitate the understanding of our analytic model."
"the stability and generalization performance of oselm can be improved in a similar way as performed for elm by utilizing the regularized least squares to replace the pseudoinverse. we refer it to as coselm. in coselm, given the initial data"
"to address this issue, this paper studies the design of reliable wireless backhaul networks under correlated failures with focus on rain fading. we consider green-field topology design and brown-field topology upgrade scenarios with the objective to minimize the total cost of wireless links added to meet the target reliability requirement in the presence of correlated link failures. we propose a new model to formulate the spatial correlation using pairwise joint probability distribution of rain attenuation between different links. this model is applied to consider the linkwise correlation along individual paths, as well as the correlation among the multiple redundant paths from the source to the destination node of a traffic flow. we formulate the problem as a quadratic integer program, which is np-hard, and develop a heuristic algorithm to find near-optimal solutions. performance evaluation shows that correlation-aware design improves the resiliency under rain disturbance at a slightly increased cost."
"for the attitude control, two pid controllers are used for controlling f t (3, 4) and γ, respectively. the pose errorθ is calculated as follows:θ"
"large breathing effects are evident in liver, and to a lesser extent in kidney. during many needle insertion procedures, it may not be possible to place the patient on a breath hold, so breathing forces should be considered in future needle steering development."
"here is a major explicit way of taking knowledge and information seriously. we add modal operators for knowledge to propositional logic, and study the laws of the resulting epistemic logics on top of classical logic. these conservative operator ex-tensions of classical logical systems have interesting structure and modeling power, also for notions beyond knowledge, such as belief. hence epistemic logic is used in many disciplines: philosophy, linguistics, computer science, and economics."
"another benefit of this representation is that there is no need to maintain a global map. instead, only the dictionary e shown in equation (1) needs to be maintained. although e may have a large number of nodes, only those edges whose value is 1, i.e., the edges over which there is an obstacle needs to be communicated to the robots. this saves a lot of communication bandwidth and ensures fast communication with the robots."
"the wall-climbing system should be thoroughly prepared for falling accidents by system failures because an accident caused by system failure of a wall-climbing robot is potentially very dangerous causing damage to the robot and harming pedestrians. in order to overcome these limitations, the concept of a wall-climbing robot based on a drone platform, called caros (climbing aerial robot system), has been proposed and developed before [cit] . this robot platform can fly like commercial drones, as well as perch on a wall by changing its pose, and climb the wall. traditional wall-climbing technologies such as magnetic [cit], a suction module [cit], adhesive material [cit], mechanical claws [cit], a microspine [cit], pneumatic-adhesion [cit], and tether-supported climbing methods [cit] are generally dependent on the material or the shape of the surface. the original motivation for developing caros was to create a wall-climbing robot that is unaffected by the wall condition like surface material or shape. wall-climbing mechanism using the friction force generated by normal force to the wall could be the solution because the force naturally pushes the robot to the wall and the friction exists almost everywhere."
"previous needle steering experiments in ex vivo tissue include: studying needle-tissue interaction forces for bevel-tip needles in chicken breast [cit], steering custom needles with large stainlesssteel tips in cadaveric brain tissue [cit], developing and testing an integrated active cannula and ablation device in ex vivo bovine liver [cit] . our preliminary work characterized the effects of various parameters on needle curvature in ex vivo goat liver [cit] . in vivo forces associated with needle insertion have been measured for human prostate [cit], porcine liver [cit], and both porcine liver and kidney [cit] . however, the needles used in these experiments were conventional stainless steel needles. to date, the authors are not aware of any prior work to characterize steerable needle insertion in living tissue."
"as we said when motivating epistemic models, getting information by shrinking a range of options is a common idea in many disciplines, that works for information flow by being told or through observation. we can call this hard information because of its irrevocable character: the update step eliminates all counter-examples."
"we have also suggested that, even when two implicit and explicit logics can be mutually embedded under translation, intensional differences may remain. here we encountered a general issue in the foundations of logic. there is no generally accepted criterion for when two (presentations of) logical systems should count as the same."
"explicit logics need not have unique implicit companions, there may be more matchings. indeed, the more striking implicit counterpart to dynamic epistemic logics may well be another logical paradigm, that we will discuss now, raising new issues."
"in recent years, there has been a surge of autonomous service robots used for cleaning, surveillance, entertainment, and object delivery at hospitals, warehouses, and public places. generally, multiple mobile robots are used as they have several advantages. multiple robots can cover larger areas and execute work in parallel. failure of one robot does not bring down the entire system. these mobile robots need to navigate from one place to another to provide services like cleaning, or object delivery. to do this, they are equipped with exteroceptive sensors like laser range finders and cameras to perceive the external world. the robots have software modules to process the data collected from these sensors for path planning, localization, mapping, and obstacle avoidance [cit] . the environments in which these robots operate are often dynamic. for example, although there are fixed elements in the map like walls, the positions of other objects like furniture are not absolute and may change in the map with time. some paths may be blocked temporarily due to cleaning or repair operations."
"this paper proposed a method in which multiple robots share information about the locally encountered obstacles in the path. this allows robots to get timely information about remote locations in the map without having to explicitly visit those areas. to overcome the problems of correspondence problem in different maps build by the robots, the paths of the map were presented as nodes. the robots only need to share if certain edges are blocked. this allows to convey the information to different robots by consuming minimum communication bandwidth. the transient nature of temporary obstacles was modeled through which the robots can keep a track of which obstacle might have been removed. to this end, a decay function was proposed whose parameters can be calculated for different types of environments. experiment results confirm that, in the long run in large environments employing multiple robots, the proposed method can improve the efficiency of the system in terms of shorter distance traveled by the robots, and shorter planning time by eliminating path re-planning. the proposed method requires networking between the robots which is the only overhead. however, most of the buildings already have facilities of wireless networks, and robots can use pre-existing infrastructure to their benefit. the proposed method can easily be extended to outdoor robots and automobiles like cars which have navigation systems installed and useful real-time information about different paths can be provided."
"a similar principle for updating conditional beliefs axiomatizes the system completely. there is also a recursion law for safe belief under public announcement, which is even simpler. the following equivalence holds on plausibility models:"
"it is also important to initially set the parameters of the decay curve to model the transience of obstacles. the parameters of the decay curve depends on factors like the size of the service area, number of service robots deployed, and diversification of their service locations. for example, if the service area is small, there is a high probability for one of the robots to observe (or update, or remove) an obstacle. in such case, a small value of threshold time (t th ) and threshold confidence (c th ) will suffice. moreover, if a shortest path is notified to be blocked, a robot with no time-critical task at hand can traverse the shortest path to confirm the obstacle's existence. for example, in case of figure 4f, the shortest path is blocked by obstacle 'a'. however, a robot with less time-critical task can take the shortest path to confirm the existence of the obstacle and accordingly notify other robots. the initial values can be set empirically, and later the appropriate values can be set by statistically analyzing (for ex. clustering) the time of existence of the obstacles at a particular location. in the proposed scheme, an obstacle is assumed to be removed if its confidence falls below a threshold. if it still exists, the first robot to observe it suffers from re-planning the path, however, since the information is shared, the subsequent robots benefit ultimately. the threshold values can also be dynamic, i.e., threshold increased or decreased, depending upon the nature of obstacles at specific times."
"there is a perception limit to each robot due to the limitation of the sensors, and the robots are only aware of the changes in their own local vicinity. hence, if a path at a remote location in the map is blocked, robots may not know it until they explicitly visit that location. traditionally, if one robot has discovered that a path is blocked, this knowledge is only local to itself and it can plan a new path to the goal location. however, other robots do not have this information, and they plan their paths without considering the new obstacle. in a large environment in which multiple robots navigate long distances, this limitation severely affects the system's performance, as more and more robots spend time in re-planning and navigating a new path from the blocked location to the goal. this paper proposes a knowledge sharing system to solve the aforementioned problem, in which, if a robot finds a new obstacle in one of the paths of the map, it notifies other robots about the obstacle enabling them to consider the new information while planning their paths. such notification is possible in a sensor network which enables robots to communicate with each other. this idea is graphically shown in figure 1, in which robot r1 finds one of the paths blocked and communicates this information to other robots on the same network. the other robots thus have a timely information to consider while planning their paths. this is thus, a symbiotic navigation scheme in which robots help each other by providing timely information of the newly encountered obstacles to plan optimal paths. however, sharing the new obstacle information is difficult as each robot maintains its own map and there is a problem of correspondence of features. moreover, different robots may maintain different types of maps, and the transience of the new obstacle in the map needs to be modeled. this work provides solutions for these problems, and discusses how the proposed method increases the efficiency of multi-robot navigation."
"where the similarity i is based on the euclidean distance metric, (x i, y i ) and (x i,ỹ i ) is the true and the estimated positions of the i th test fingerprint, p 1 and p 2 are the penalties associated to wrong building and floor estimations, b i is 1 for wrong building estimations and 0 otherwise, f i is the absolute difference between the true floor and the estimated floor. p 1 and p 2 are set to 50 and 4 meters respectively. to evaluate the classification accuracy in predicting the building and the floor successfully, we define hit rate and success rate. the building hit rate corresponds to the percentage of the test fingerprints whose building is correctly predicted. the floor success rate corresponds to the percentage of the test fingerprints whose building and floor are correctly predicted. if we ignore the correctness of the building identification, the floor success rate becomes the floor hit rate that corresponds to the percentage of the test fingerprints whose floor is correctly predicted. hence, we consider the best localization metric is the one in which its classification rate (highest hit rate or success rate) is the highest, and then followed by the highest mean positioning accuracy (lowest error, error * or error pn )."
"a natural addition to the heartland of logic are notions of knowledge and information for agents, that have been part of the discipline from ancient times until today, [cit] . in what follows we do not need intricate contemporary logics for epistemology, [cit], interesting and innovative though these are. [cit] s."
"logics of belief change recursion laws exist for belief changes under a wide variety of soft events representing different levels of trust or acceptance for new information, [cit] . an area where this variety makes special sense is learning theory, [cit] : different update rules induce different policies for reaching true belief in the limit. the handbook [cit] has details on the landscape of modal logics for belief change, plus connections with agm-style postulational approaches."
"but one can also borrow ideas inside one stance. for instance, intuitionistic logic started from the proof-theoretic bhk interpretation of the logical constants, which met up with semantics only afterwards. could a similar proof-theoretic analysis apply to dynamic semantics, a major implicit paradigm for information dynamics?"
"choosing locally co-existence means that both implicit and explicit stances have intrinsic value, but even so, particular areas may bring further reasons for using one rather than the other. for instance, are there favored stances in human cognition?"
"on the contrary, in our proposed approach with the pose change speed of 5 deg/s, the impulse level of the first contact is the same with the non-tilt approach; however, the impulse from the second contact was significantly decreased to 0.05 kg·m/s. fig. 15 shows an exemplary tail-down perching process using a tilt-rotor mechanism and the video of overall experiments is available at https://youtu.be/ioh9mveiwas/."
"update s by removing s(1). 11: end while 12: solution: s * based on the above equation and knowing that z k is used to choose path q k, the penalty function is given by c 1,2,...,m z 1 z 2 ...z m . using this model makes the optimization problem computationally prohibitive due to the multiplication of m variables. therefore, we use a simplified version of the cost that sums all possible path pairs as follows:"
"few people today see the epistemic modality as a conclusive analysis for the full philosophical notion of knowledge. but even so, this system is a perfect fit for another basic notion, the 'semantic information' that an agent has at her disposal, cf."
"since it was difficult to apply the tilt mechanism using the normal propeller of general drones due to its large diameter, we applied edf (electric ducted fan) units with small diameter. but it is disadvantageous that the energy efficiency of the edf unit is lower than that of the normal propeller propulsion system. furthermore, the structure with tilt mechanism cause an intersection zone, and consequently a specific control strategy is necessary and various pose states may not be possible. for these reasons, in the future, we need to develop a better platform structure that has enough space for tilting a large propeller and minimize the interference of airflow between two tilt units depending on various tilt angles."
"video of the experiment can be downloaded from the link provided in the supplementary materials section. figure 9 shows the various stages of the experiment. in the experiment, the start and goal locations of the robot are indicated in figure 9a . both the robots had the grid map of the environment beforehand. initially, p3dx was programmed to navigate towards the goal and come back to its starting position. until then, turtlebot robot was programmed to remain stationary. table 1 ."
"where arg min is achieving a global minimum value of the phase rotation factors. furthermore, the computation complexity of the c-pts method is considered high because finding the optimum phase rotation factor needs to examine w v-1 operations. in addition, the transmitter should send (log2w v-1 ) bits as the side information (si) to the receiver to regain the original data. therefore, the c-pts technique relies on the segmentation scheme type, the number of the subblocks (v), the phase weighting factors, and the number of the different phase weighting factors (w) [cit] ."
"while the tilting process is performed, a pid (proportional integral derivative)-based flight controller still operates in the same manner as normal flight status. therefore, if slippage is detected by measuring the angular acceleration from an imu (inertia measurement unit), the thrust level at the front side naturally increases. when the ratio of the front thrust to rear thrust is over a specific threshold, the drone stops tilting and proceeds to the next step. the friction coefficient is approximately estimated by the maximum tilting angle by (2) ."
"n eedle-based procedures are commonly used for medical diagnosis and intervention when there exists a nearly straight-line path between an entry point and subsurface target. examples of needle-based procedures include biopsy, ablation, and brachytherapy. for many procedures, accurate needle placement is critical for acquiring diagnostic samples and providing accurate delivery of therapy. however, tissue deformation can cause a straight needle to deviate from a desired path. robotic needle steering is a promising technique that allows for correction of a needle path inside tissue, and may also enable new procedures currently not feasible with straight needles."
"the maximum and mean insertion forces for liver are comparable with previous experiments for needle insertions into ex vivo liver [cit] . higher insertion and lower retraction force for the stainless steel needles could be due to a higher coefficient of friction for stainless steel needles. all needle shafts were ground by hand to ensure even surface texture between the needles; however, this did not necessarily equalize the coefficient of friction. although one might expect higher insertion and retraction forces for needles with larger curvature and bent tips, no such correlation existed."
"rain attenuation on different links exhibits strong spatial correlation, which is modeled by combining the independent white gaussian signals for each link with a spatial correlation matrix r. the element (i, j) of the correlation matrix, denoted by r ij, shows the correlation coefficient between links e i and e j and is calculated as"
"the history of logic has themes running a spectrum from description of ontological structures in the world to elucidating patterns in inferential or communicative behavior. the mathematical foundational era added the methodology of formal systems with semantic notions of truth and validity and matching proof calculi. this modus operandi is standard fare, enshrined in the major systems of the field."
"2) force analysis: the maximum insertion force, mean insertion force, maximum retraction force, and mean retraction force were extracted from the force data (see fig. 8 ). statistical analysis was performed using the method described previously, and the results are shown in table ii . needle type did not have a significant effect on force. insertions into the prostate had higher maximum and mean force than insertions into kidney or liver. maximum and mean insertion forces were higher for larger diameter needles and for stainless steel needles. the maximum and mean retraction forces for prostate were lower than for liver and the mean retraction force for liver was significantly higher than kidney as well as prostate. stainless steel needles had lower maximum and mean retraction forces than nitinol needles."
"truth maker semantics our second example shows our contrast at work in a very recent development. `truth maker semantics' has been touted as a hyper-intensional paradigm springing the bounds of standard modal logic, cf. [cit] and related papers. in our terms, truth maker semantics is an implicit approach to describing metaphysical (or, in some intended applications, epistemic) structure, changing the meanings of the logical constants, and defining new notions of consequence based on these. so, it makes sense to look for a translation from truth maker logic into an explicit companion, namely, a standard modal logic over the same class of models."
"orthogonal frequency division multiplexing (ofdm) has become a reliable modulation technique for highspeed data rate frameworks. the ofdm system overcomes to the other multicarrier systems by some distinctive features, for example, high system capacity, efficient bandwidth utilization, and robustness against multipath fading [cit] . consequently, the ofdm system considered by the numerous communication systems such as the broadcast radio access network (bran) [cit], the 4g mobile communication systems for both long-termevaluation (lte) standard [cit], and is chosen as one of the candidates for the next generation (5g) data transmission system [cit] ."
"since the tilt-rotor mechanism has only single degree of freedom with a fixed wheel drive aligned with the same direction, the drone platform is optimized for only vertical climbing. for lateral movement on the wall, there must be a mechanism that can change driving direction of the drone body or additional tilting mechanism is required for other axes. considering the limited payload of the drone platform, it is challenging to simplify and implement the mechanisms for an improved design. therefore, it is also a future work to design a tilt-rotor-based wall-climbing platform that allows omnidirectional movement."
"a lateral incision 5 cm below the twelfth rib was made to access the kidney. due to the unique canine anatomy, kidney stabilization within the abdominal cavity was not possible. rather, the kidney was carefully pulled out of the incision and stabilized with surgical towels. a straight cone-tip needle was inserted in the lower pole of kidney, along the longitudinal axis of the organ. evans blue dye was used to mark the insertion sites to aid in histology postmortem. during insertion, force data were collected. after insertion, a cbct scan was taken and the needle was retracted, while recording the force data. this process was repeated for the bevel and bent tip needles."
"on the other hand, the output ofdm signal is obtained by superposition of the n subcarriers with the samples of the baseband signal. hence, when the phases of these samples are in high consistency, some of these samples might be added together, and the instantaneous power of these samples rises significantly to become much higher than the mean power of the signal. this fluctuation of the signal is named papr, and it is defined the maximum peak power of the ofdm signal divided by the mean power [cit] . the papr is measured in decibel (db), and it can be written by"
the new obstacle information is stored in a queue and updated when the robot has reached its goal. this 'lazy update' ensures smooth navigation and robots only update relevant information.
"section ii introduces the basic mechanisms and strategies for vertical soft landing and wall-climbing. section iii elaborates the structure of the prototype drone. finally, in order to show the feasibility of the proposed mechanism, wall-perching and wall-climbing tests are performed and the results of the experimental tests are shown in section iv."
"but earlier reservations apply: despite the translations, us and pal seem intuitively different. for instance, recall our notion of 'procedural information' in intuitionistic models. viewing pal as a logic of inquiry, a generalized semantics of 'protocol models' with restricted temporal histories of updates makes sense, [cit] . this natural change in models changes the laws of pal, and it blocks the translation of pal into s5. however, it is unclear if protocol models make sense in dynamic semantics."
"take new proposition letters p + and p -for each atomic proposition letter p. now, for each propositional formula ϕ, we recursively extend this double set-up as follows, closely following the above truth definition: we summarize a few strands that occurred in the preceding sections."
"thus, both epistemic logic and intuitionistic logic have dynamic extensions having to do with inquiry, and these can be developed in both explicit and implicit styles."
"theoretically, if the friction coefficient is 1, the tilting angle can be 90 degrees. however, since this aerial drone platform is based on an x-configuration quadrotor (see fig. 4 (a)), the airflow from front thrusters is obstructed by the rear structure of the drone with 90 degrees tilting angle, as shown in fig. 4(b) . then it is difficult to deal with accidental slippage with limited angular speed of the tilt-rotor mechanism. therefore, as in fig. 4 (c), the tilting angle should not be 90 degrees, but rather about 35 to 45 degrees practically, which is also related to the pose change process."
"though stabilization process is not affected by the obstruction of airflow, obstruction is inevitable during pose change process because of the x-configuration design. in this paper, if the air obstruction occurs at a specific pose of the drone, we define it as an intersection zone. the range of the intersection zone is determined by θ as shown in fig. 4"
"after the experiments, the animal was covered in a heated blanket to raise body temperature, as some heat was lost during surgery, and was taken off anesthesia and ventilation when appropriate. the animal was monitored daily for seven days until euthanasia was required for histological analysis."
"the prostate was sectioned entirely into ten specimens. ten specimens were obtained from the upper pole of the kidney, where the needles were inserted. ten specimens were also obtained from both the left and right medial lobes of the liver, mostly near the regions where the needles were inserted. the specimens were processed into slides using standard histologic techniques with one slide from each specimen. the slides were stained with hematoxylin and eosin to better distinguish cellular components. fig. 6 shows an example of original 2-d x-ray images and the corresponding stereo reconstruction. the curve-fitting program fit a circle, rather than a line, to the needle shaft for 20.5% of the straight needles, for 53.9% of the bevel tip needles, and 84.6% of pre-bent needles. with the exception of insertions of stainless steel needles into prostate, needle curvature increased with the amount of needle tip asymmetry (see fig. 7 ). the minimum radius of curvature was 23.92 cm for straight needles, 16.45 cm for bevel needles, and 5.62 cm for prebent needles. a kruskal-wallis nonparametric analysis of variance test was used to determine the significance between groups for curvature (see table ii ). a p-value of less than 0.05 indicates that a group has statistically significant differences; however, we note that due to unequal variance of some data groups, the kruskalwallis tests have limited modeling power. to further identify which groups were significantly different from each other, a post-hoc scheffe test was performed using the kruskal-wallis test statistics. there is significantly more curvature in prebent tip needles than bevel and straight needles; there is also significantly more curvature for insertions into the kidney than insertions into liver or prostate. nitinol needles had more curvature than stainless steel needles. finally, planes were also fit to the needle data to characterize planar deviation. the mean-squared error for fitting a plane to ex vivo needle insertions ranged from 6.05e-5 to 0.212 mm."
"to show the impact of the online incremental data on the system performance, we randomly select 10000 samples from the training dataset as the online incremental data to reflect the environmental dynamics. in other words, we set up an initial model in afarls with the initial training data of 9937 samples during offline training phase. [cit] incremental data. [cit] samples. figure 4 (a) depicts the influence of the online incremental data to the system performance in the online localization phase, after every update to the model. as shown, the initial localization performance is the worst. after gradually refining the model with the online incremental data, the performance improves because the newly updated model can reflect the current indoor environment better. this tells us that afarls can improve and update itself along the timeline by utilizing the incremental learning method for a lifelong and high-performance running."
"steerable needles were inserted into ex vivo and in vivo liver, kidney, and prostate to study the 3-d shape of the needle, insertion forces, and repeatability over multiple insertions."
"hence, coselm is a special case of oselm, whereas oselm is a sepcial case of elm. the extension of elm to coselm trumps the traditional supervised batch learning elm, because it achieves better generalization performance and is adaptive to changes."
"remark this brief exposition of intuitionistic logic does not do justice to its deep connections with proof theory, universal algebra, and category theory, or to the many surprising effects of working in mathematical theories on top of a weaker base logic. see the encyclopedic source [cit] for a wide-ranging exposition."
"this challenges simple views of how intuitionistic and epistemic logic connect. the epistemic logic for semantic information is s5, and the fact that the gödel translation takes us into s4 reflects a view of intuitionistic models as temporal processes of inquiry. thus, an explicit counterpart to intuitionistic logic needs a temporal version of dynamic epistemic logic. indeed, temporal `protocol models' with designated admissible histories satisfying constraints on inquiry, [cit], model procedural information in long-term processes of inquiry or learning beyond local dynamic steps."
"in the remainder of this section, we go into more depth on the foundational issue of how the two views of dynamics are related, and show new issues that emerge."
"dynamic logics of belief change enrich the original language with informational events and attitude changes, but then work conservatively with a classical consequence relation, explaining deviant features of non-standard consequence by attitude and information change through the recursion laws for the new dynamic operators. in the following section, we evaluate this difference in approaches."
"we have seen that non-monotonic consequence relations can be translated faithfully into a classical logic with operators for attitudes and informational events. but as before, this does not identify the two perspectives: one can still have advantages over the other. for instance, implicit approaches focus on structural rules, which are a natural high-level vantage point allowing for elegant theory. on the other hand, an explicit dynamic approach provides an emancipation of informational events in problem solving that is of interest per se, as it adds new events beyond inference."
"translating truth maker logic into modal information logic the models just described and their modal logic are an explicit companion to truth maker logic. and the connection is very close. here is a two-component recipe for translating from implicit truthmaker logic into explicit modal logic, where the simultaneous use of variants + and -is a standard trick in reducing three-valued logic to classical logics."
"three types of needles were used in the experiments: steerable nitinol bevel tip and prebent tip needles, and stainless steel cone tip needles (see fig. 2 ). the cone tip needles were used as a control to compare steerable needles to needles with straight trajectories, such as needles used in current clinical practice. each needle type was inserted into all three organs. the needle diameters and degree of asymmetry were selected based on initial experiments in cadaveric canine livers, kidneys, and prostates to identify needles with a range of curvatures and sufficient repeatability. a needle diameter of 0.74 mm was chosen for insertions into the liver and the prostate, and a needle diameter of 0.58 mm was chosen for insertions into the kidney. a cone tip of 30"
"but there is a richer repertoire of epistemic notions available on this models. for instance, on binary world-independent orderings, a good addition is 'safe belief', a standard modality intermediate in strength between knowledge and belief:"
"this 'meaning loading' of the classical operators makes the intuitionistic laws for negation and implication deviate from classical logic. now earlier points become precise. famously, this semantics invalidates the law of excluded middle ϕ ∨ ¬ϕ, as this disjunction fails at states where ϕ is not yet verified though it will later become so. these deviations from classical logic are informative in telling us implicitly about properties of knowledge. failure of excluded middle says that agents cannot decide everything a priori. thus meaning loading makes the remaining validities informative (they now say something new), and more mysteriously, it packs information in the absence of classical laws -like dogs that do not bark in the night-time."
"from translating to merging finally, moving away from reduction, a weaker but still significant contact between explicit and implicit logics is compatibility. can such systems be merged in meaningful ways? intuitionistic modal logics have long existed, and hybrids of explicit and implicit logics keep emerging, as we will see later on. often this juxtaposition seems routine, but hybrids can also be natural."
"thus, basic epistemic logic is a conservative extension of classical logic, and the same holds for variations like s4 or s4.2 that encode other intuitions concerning knowledge, [cit] . more intricate laws hold for modalities of common or distributed knowledge in groups, but again these will not be not needed here."
"also, pal update has a natural extension to dynamic-epistemic logics with much more drastic model changes modeling the dynamics of partly private information, and it is unclear if this richer dynamics has any role in a dynamic semantics."
"belief change under soft information but richer belief models also support new transformations. in addition to hard information, there is soft information, when we take a signal as serious, but not infallible. its mechanism is not eliminating worlds, but changing plausibility order. a widely used soft update is 'radical upgrade':"
"already in gödel's seminal [cit], there is a faithful translation from intuitionistic logic into the modal logic s4 whose underlying intuition follows the present knowledge perspective. we now look at this connection to see what it achieves."
"i must leave this matter of finer intensional differences open here, but will return to the issue of comparing dynamic components by drawing in the logic pal."
"teasing out the hidden actions intuitionistic models represent a process of inquiry, with endpoints as final stages where we know the truth about all proposition letters. what are the implicit steps in the background of such a process taking us from node to node? moves from one state to a successor come in two kinds."
"to minimize the total deployment cost and penalty function, formulated in eqs. (10) and (15), respectively, we define the objective of td-cf optimization problem as a convex combination of (10) and (15):"
"this sounds like a plea for taking informational actions seriously, as we did in the preceding section. but this time, they are treated, not by adding new operators, but implicitly, by loading the meanings of classical vocabulary with dynamic features."
"in this paper, the papr and computational complexity performance of the three ordinary segmentation schemes are analyzed and simulated with two scenarios. furthermore, a new segmentation scheme named transpose-pts (t-pts) scheme is introduced to improve the papr reduction performance without increasing the complexity of the system."
"on the other hand, the position is estimated from elm-c based on the fingerprints in which the corresponding coordinates belong to the predicted floor from the adaptive sub-dictionary. src sparsely represents parts of the nonzero elementary features of the fingerprints to an adaptive sparse dictionary that has much lower density for every floor, because of the high-dimensional feature of the fingerprints is compressible. for the floor level associated to f classified as the class of c, f should be approximately represented by the sparse dictionary of the c-th floor. in other words, for f that cannot be approximately represented by a floor's sparse dictionary, it is considered not belonging to this floor. since the performance of knn can be improved by introducing a weighting scheme for the nearest neighbors, we develop wsrc working with knn based on the sorensen distance metric with powed data representation to strengthen the classification results as it is insufficient to distinguish the rss fingerprint through the residual alone. the noteworthy feature of wsrc is that it emphasizes close neighbors more heavily from the adaptive sub-dictionary, similar to the basic idea of wknn. rather than being based on the distances to the query, wsrc is based on the residual to the query. at first, the corresponding residual to each floor is converted to weightages. based on the k neighbors, the frequency to each floor's vote is counted, so that the majority vote of the neighbors is considered as the metric for the classification accuracy. the primary effect of the weighting scheme on the wsrc is that its decision rule on the neighbors has less significance on the majority vote than the neighbors that have the smallest residuals. as a result, f is assigned to one of the possible floors in which the rule is not merely based on the smallest residual to the query, but also the most frequent among its k nearest neighbors. wsrc classifies f to one of the possible m classes of the floors of the problem at hand given as in (33) ."
"all needles were fabricated from solid wires. cone tips were ground using a multiprep system from allied high tech products, inc., where a custom fixture held the needle at a specific angle. the needles were held at 15"
"transfer of ideas different stances on the same thing facilitate creative borrowing, since their agendas may differ. for instance, epistemic logic has a rich tradition of multi-agent and group knowledge. intuitionistic logic can then profit from the same ideas, creating accounts of mathematics closer to research as a social activity, cf. [cit] ."
"in this paper, we proposed the cost-efficient design of wireless network topology capable of providing high reliability under rain disturbance. in such conditions, link failures are highly correlated and the reliability requirements cannot be guaranteed by simply assuming uncorrelated failures. to address this issue, we developed a new model to consider the spatial correlation of rain attenuation among links on each path, and among different paths from the source to the gateway node. the evaluation results of the proposed topology design approach show that considering failure correlation significantly improves the network reliability performance under heavy rain at a slightly increased cost. as a port of future work, we will develop tailored online adaptation approaches to meet the reliability requirements under rain disturbance."
"2) animal preparation and surgical procedure: the canine was sedated, placed on a warm water circulating blanket, and then placed under full intravenous anesthesia and mechanical ventilation. throughout the procedure, heart rate, end-tidal co2, and body temperature were monitored. the perineum, skin above the kidneys, and upper abdomen were shaved and surgically prepped before making incisions. after needle insertions for each organ, the surgical site was cleaned, all surgical towels were removed and counted, and the incision was sutured."
"we have identified a significant methodological contrast running through logic, between implicit and explicit stances. we use the word 'stance' here, and not 'system', because we do not identify logic with a family of formal systems. some logical systems can indeed be called implicit or explicit, but the contrast as we have discussed it also applies to broader workings habits in logical analysis."
"throughout, we take explicit and implicit approaches seriously as equally natural stances, and we discuss new logical questions suggested by their co-existence. our final conclusion from all this will be that the interplay of the two stances needs to be grasped and appreciated, as it raises many new points and open problems concerning the architecture of logic, while it also has philosophical repercussions."
"highlighting the distinction, consider the fact that we do not know the answer to every question, and maybe never will. this showed as follows in intuitionistic logic."
"our discussion so far centered on the statics and dynamics of knowledge. however, the implicit/explicit contrast applies just as well to logics of belief, perhaps the more important attitude in agency. the case of belief shows interesting new features and suggests new comparisons between implicit and explicit logic systems. we start with belief dynamics in explicit style, moving to implicit counterparts later."
"2) force analysis: fig. 10 shows force profiles for insertions into the prostate, and insertion and retraction in the liver and kidney. the maximum and mean insertion and retraction forces were extracted from the force data and are shown in fig. 11 . insertion forces are higher for the larger diameter needles and forces in the liver and prostate are higher than in the kidney. in addition, cone-tip needles have higher insertion forces than bevel-tip needles, and retractions in the liver have lower retraction forces than the kidney. for the in vivo experiments, there is not enough data to perform statistical tests."
"in this modus operandi, in contrast with epistemic logic, there is no separate syntax for knowledge or information -but old logical constants are re-interpreted, making negation and implication sensitive to the information structure of new models with an inclusion order that is absent in models for classical logic. in particular, an intuitionistic negation ¬ϕ says that the formula ϕ is not just 'not true', but that it will never become true at any further stage along the inclusion ordering. also, failure of classical definability equivalences leads to fine-structure for classical notions like implication, which can now be viewed in several non-equivalent ways."
"where p k s is the success probability of path q k, i.e., the probability that all links along that path are available. to compute the exact value of the success probability, the joint probability distribution of their corresponding attenuation a i, ..., a i+l, should be known. however, only the pairwise joint probability distribution of rain attenuation between different links has been formulated in the literature (see eq. (5)). therefore, we approximate the success probability p k s using pairwise distribution of all consecutive link pairs included in the path:"
"this style of reasoning deviates from classical logic. in particular, it is 'non-monotonic': a conclusion c may follow from a premise p in this sense, but it may fail to follow from the extended set of premises p, q. for, the maximal models within the set of models for the conjunction p ∧ q need not be maximal among the models of p. making it explicit can we provide alternative explicit accounts leaving the notion of consequence standard, while adding vocabulary to bring out the origins of the new consequence notions? of course, we need a guiding semantic perspective for doing so, and this will depend on the precise motivation for the new consequence relation."
"the work described in this paper was carried out with the support of the kista 5g transport lab (k5) project funded by vinnova, ericsson ab, and cost action 15127 recodis."
"as mentioned before, since the mav has a limited payload and operation time, it is important to minimize the number of actuators and other mechanical parts (components) that support them. therefore, the tilt-rotor mechanism should be implemented using the minimum number of actuators. in the proposed perching process, the thrusters are classified into two groups in terms of their role. one group is for stabilization, which acts as a hinge support for pose change, and the other is a group of thrusters at the opposite side, by which the drone generates the torque for pose change. in order to minimize the platform weight and complexity of control, the manipulation plan for the tilt mechanism is based on this grouping."
"the proposed technique was tested in the simulation environment shown in figure 4 using the matlab software and robotics toolkit [cit] . d* algorithm [cit] was chosen for path planning, however, any other algorithm can also be chosen. a grid based navigation is chosen with one unit cost for forward, back, left, and right movement, whereas, for diagonal movement the cost is √ 2 units. in figure 4, s and g represents the start and goal locations of the robot, respectively. figure 4a shows the d* path from start to goal location when there are no obstacles in the map. figure 4b shows a new obstacle (marked a) in the map found by another robot and communicated to others. the planned path considering this informed obstacle is shown. similarly, figure 4c shows the path of the robot which has been informed about both the obstacles marked a and b. for comparison with the traditional method, consider figure 4d which shows a new obstacle marked c in the map. if no other robot has found this obstacle, the robot starts navigation and stops at location s . the path shown in blue color in figure 4d is not accessible due to the obstacle. the robot then has to plan a new path from the location s to g which is shown in figure 4e ."
"but live disciplines are not finished fields but advancing quests. logic has a growing agenda, including the study of information, knowledge, belief, action, agency, and other key topics in philosophical logic or computational logic. how are such topics to be brought into the scope of the established mathematical methodology? there are both modifications and extensions of classical logic for these purposes, and the aim of this paper is to point at two main lines, representing a sort of watershed."
"with this single illustration, we hope the reader has grasped the methodological point we are after -and in later sections, we will now explore the 'implicit' versus 'explicit' contrast in other cases, adding more depth to what it involves."
"the implementation of the internet of things (iot) is in a wide diversity of fields ranging from smart infrastructure, healthcare applications, industrial automation to real-time monitoring and tracking, etc. the iot-based object localization and tracking are considered as one of the recent active and immense developments of iot applications [cit] . nonetheless, the localization and tracking system is not new. since the first satellite navigation system was studied by the u.s. [cit] . currently, the outdoor geolocation of an object is obtained from the global positioning system (gps) which uses four or more satellites. undeniably, the mature technology of the outdoor positioning system that relies on the gps has had a tremendous impact on users' everyday lives. examples include navigation, tracking, mapping and so forth. however, the gps does not work well in indoor environments because it requires line-of-sight (los) measurement [cit] . hence, the precision of around 50 meters achieved by the gps for the non-line-of-sight (nlos) of reference objects in a complex indoor environment is very limited to commercial applications [cit] . to resolve these limitations, various signals include wifi, bluetooth, rfid, ultrasound, light and magnetic field have been investigated for iot-based indoor positioning system (ips)."
"we show this with two examples, from the philosophy of physics and from metaphysics. again, these raise new issues of their own that we will only touch upon."
"a timestamp (t i ) is maintained for an ith obstacle representing the time at which the obstacle was last seen. if an obstacle has recently been added to the map and a short time has elapsed since its addition, then the probability that it has not been removed is high. on the contrary, if a lot of time has elapsed since the addition of the obstacle, the probability that it still exists in the map is less."
"we found natural translations between dynamic semantics and dynamic-epistemic logics. still, implicit and explicit approaches do not collapse, and again we might be content with creating merges. either way, the realm of information dynamics seems a rich source for our explicit/implicit contrast, raising interesting issues of its own. dynamic logics of soft information"
"generating an adaptive sub-dictionary associated to f is an important step for src to reduce the overall computation burden. if the discriminative criterion is satisfied, the adaptive sub-dictionary is used by src to predict the floor and is fed to the elm-c regressor to estimate the position. otherwise, it is used by elm-c to predict the position. first, we alter the usage of coselm from classifying f to the location labels to where the user belongs, to clustering a group of relatively closely associated training fingerprints with the corresponding reference locations. instead of having one set of the location labels as the final output, coselm clusters q pairs of position labels corresponding to the respective top q highest confident scores in s 1 and s 2 . in order to enhance the probability of the classification rate, we increase the size of the cluster group by establishing a new larger cluster of the coordinates from the database whereby either their longitude or latitude labels match to the q pairs of the position labels. if the discriminative criterion is satisfied, the relevant fingerprints, p associated to these coordinates are extracted from the database. otherwise, we further reduce the size of the cluster by extracting only the relevant fingerprints p, in which the corresponding coordinates have the floor that matches the predicted floor determined from coselm. although the size of the newly constructed subset has been greatly reduced compared to the original database, the direct application of the src-based classification to predict the floor level and the direct application of elm-c based regression to estimate the position are time-consuming. hence, it is appropriate to construct a fixed size of k, which is the smaller subset of the extracted training samples. to address this issue, we utilize the knn-based classification as the adaptive subdictionary selection strategy."
"in the last decade, a number of needle steering techniques have been developed, along with associated mechanical models of needle-tissue interaction, path-planning algorithms, control strategies, and experiments to characterize parameters that affect needle steering in artificial and ex vivo tissue [cit] . complete needle steering systems, with integrated robotic devices, planners, and image-guided control, are also being developed and validated [cit] . however, to become a clinically relevant technology, the usability of steerable needles to perform treatments in biological tissue must be demonstrated. no previous studies have measured steerable needle behavior in in vivo tissue, and our understanding of the repeatability of steerable needle insertion is far from complete."
"the heart of the dynamic logic is then a compositional analysis of post-conditions for the key actions via recursion laws. this leads to conservative extensions of the base logic, though some dynamic-epistemic systems force redesign of the base language, while some recent semantics no longer support all-out reduction. many further notions can be treated in this style, such as changes in agents' beliefs, inferences, agenda issues, or preferences -where one often changes the ordering of a current model rather than eliminating worlds. moreover, extended dynamic logics also deal with public and private events in rich multi-agent scenarios such as games."
"in the c-pts technique, there are three common kinds of segmentation schemes including interleaving segmentation (il-pts), adjacent segmentation (ad-pts), and pseudo-random segmentation (pr-pts) [cit] . fig. 2 shows the three conventional segmentation schemes. the segmentation schemes must fulfill the follows: i. all the subblocks must be equivalent in size. ii. each subblock must have n/v active subcarriers, and the other locations should set to zeros iii. each subcarrier must assign only one time inside the subblocks. iv. the subblocks must be non-overlapping with each other. fig. 2 ordinary partitioning schemes [cit] in the il-pts scheme, the subcarriers are assigned with equally spaced of v locations inside each subblock. the ad-pts scheme allots the sequential subcarriers within each subblock, successively. however, the pr-pts scheme assigns the subcarriers within the subblocks."
"where f a is the required thrust force for wall-climbing, θ a is the tilt angle with respect to the wall, and f f is the friction force caused by the normal force to the wall. f f is directly proportional to the friction coefficient µ. (8) is plotted in fig. 7 and it shows that the required thrust force drastically decreases as the tilt angle θ a decreases in the low friction coefficient environment. the plot in fig. 7 assumes that the mass of a drone is 3 kg same as that of the prototype drone. therefore the required force for hovering state is calculated as 29.42 n to support the 3 kg weight drone. depending on the friction coefficient, there is a range of tilt angle that requires small thrust force and a specific optimal angle with minimum required thrust force. due to the reduced required thrust force, the energy efficiency can increase. section iv includes the experimental tests to validate this simulation."
"in order to realize the rotation of the tilt axis, a continuous rotation servo is employed; its weight is about 55 g, the stall torque is about 10 kg/cm, and operation speed is 300 deg/s. combined with a 72/14 ratio reduction pulleys, the tilting axis rotates with maximum torque of 51 kgf·cm and angular speed of 58 deg/s, which satisfies the required specification. pulleybelt drive systems could minimize a backlash problem which can cause an unstable body structure with vibration or angular error. for the same reason, a pulley-belt is also applied to the rotary encoder measuring tilting angles as shown in fig. 8(b) . using servomotors with angular speed of 300 deg/s and a 72/14 ratio pulley system, the axis for tilting can rotate with volume 7, 2019 angular speed of 58 deg/s. the time for rotation of the remaining 90 degrees takes only 1.5 seconds. fig. 9 shows a block diagram of the electrical components of the control system, sensors, and actuators. in order to implement real-time flight control loop of the main mcu, the control system of the drone consists of two atmega2560-based 8-bit mcus (micro controller units). as a main controller, one mcu is used for the control of all actuators such as thrust control for flight, tilt control, and wall-climbing drive control. the other auxiliary mcu for acquiring most sensor data is installed on the drone. the drone has two infrared distance-measuring sensors, two crash sensors, and two rotary encoders for the tilt angles. the data acquired from the auxiliary mcu is transmitted to the main mcu via serial communication. the only data that the main mcu directly obtains is imu and remote control data for a corresponding flight control loop with fast update interval."
"we could just add the actions of pal to intuitionistic logic, [cit] . but can we be more implicit about actions, without putting them explicitly into the syntax?"
"but there is more than mere transposing of concerns from dynamic-epistemic logic. the tree structure of intuitionistic models registers two notions of semantic information on a par, a distinction also found in epistemic inquiry with long-term scenarios in learning theory, [cit] :"
together with the s5-laws for epistemic logic plus simple axioms for boolean compounds after update this gives a complete axiomatization for pal. another interesting law demonstrating the dynamics of pal governs iterated updates:
"intelligence, and it has since entered other fields. in particular, the consequence notion of circumscription [cit] says that, in problem solving or related tasks, the following inferences are allowed:"
"equation (10a) expresses the total cost of links in the network, whose minimization is the objective of our approach. constraint (10b) enforces that the selected paths do not share any common link, and constraint (10c) ensures the reliability requirement. it should be noted that we use ln (.) operator to derive a linear constraint in (10c)."
"regarding the range of the intersection zone, θ r should be close to 90 degrees as the rear thruster direction aligns with the direction of the airflow of thrusters at the front side and prevents the sidewall of the rear thruster from directly blocking the airflow. under this condition, smaller tilting angle of forward thrusters, θ f, is better as long as the sticking force to the wall is sufficient. otherwise, large tilt angle leads to obstruction of the airflow, which can lead to falling by slippage."
"although the ofdm systems have many advantages, the high papr value is considered a major drawback restricts the system in the real applications, because the system has non-linearity devices such as high power amplifiers (hpa) [cit] . the conventional solution to restrain the high papr is to use hpas with large linear scope, but these power amplifiers are typically costly and lead to the increase of system complexity [cit] . therefore, many techniques have been introduced to limit the high papr as a successful arrangement without additional costs such as clipping and filtering [cit], peak windowing [cit], selective mapping (slm) [cit], and partial-transmitsequences (pts) [cit] . among these techniques, the pts method is considered an efficient algorithm to alleviate the high papr value, whereas its computational complexity is considered relatively high. the basic concept of the pts method is segmentation the input symbol into the subsets and rotated them with a group of the phase weighting factors before combining the subsets again and transmission to the receiver. consequently, the pts methods depended on two stages for its operation; the partitioning scheme and the weighting rotation factors [cit] ."
"how we can get knowledge matters, not just what is the case. while endpoints record eventual factual information states, the branching tree structure of intuitionistic models, both its available and its missing intermediate stages, encodes further non-trivial information: viz. agents' knowledge about the process of inquiry."
"consider p as the parameter that denotes the number of updating times of the chunks of data presented for the online sequential learning. in general, the solution of oselm after p + 1 times of incremental learning iŝ"
"the three ordinary segmentations schemes have different papr performance based the subcarriers correlations within the subblocks [cit] . the il-pts scheme records the worse papr lessening capacity among the partitioning schemes, because of the large peak correlation between its subcarriers. the papr reduction capacity of the pr-pts is considered the best among ordinary schemes because its random pattern reduces the peak correlation among the subcarriers. however, the ad-pts scheme has papr performance lower than pr-pts and higher than il-pts based on the subcarriers correlations."
"the canine was euthanized on the eighth day following the procedure. the left and right medial lobes of the liver, the left kidney, and the prostate were immediately harvested and placed in a 10% formalin solution. signs of needle entry were not evident grossly on the prostate or kidney. faint traces of evans blue dye were noticeable on the liver; however, no needle injury could be seen. the organs were removed from the solution after seven days and sectioned into specimens for histological analysis."
"we switch between these two modes all the time, while staying inside the same medium of communication. there may be local cognitive preferences between going explicit or implicit, but we doubt they can be justified in a sweeping manner."
"model update here is a system making the dynamic actions behind basic epistemic logic explicit by representing informational action as model change. the simplest case of such a change occurs with a public announcement or a similar public event !ϕ that produces hard information, where one learns with total reliability that ϕ is the case. this eliminates all worlds in the current model where ϕ is false:"
"as for laws of reasoning, the modal logic of information has interesting validities, but details are not relevant here. one principle that fails though is associativity:"
"since coselm is the crucial factor of our proposed model to determine the overall performance, we investigate the critical parameters that can affect its performance in this section with the limited training fingerprints in the positive data representation based on the eu zenodo database. we start by training the classifier to identify the floor, while training the regressor to estimate the position. both the classifier and the regressor are based on elm. after that, the performance of the classification rate and the positioning accuracy using four different activation functions are evaluated. these activation functions"
"recursion axioms reduce formulas with dynamic operators to static base formulas, so the extension of our classical base logic is conservative in the usual explicit style."
"another strategy to overcome air obstruction in the intersection zone is to decrease the duration in the intersection zone by rapidly increasing the angle of the forward thrusters to the wall at the moment of entering the intersection zone, as described in fig. 5 ."
"as illustrated in table 8, afarls performs very well as expected in a single building environment. this is because the test bed is small. when the environment is enlarged to include additional new buildings (the second and third buildings), the performance declines slightly. it should also be noted that these results are very similar to the results we have obtained in the previous section when we train the system with all the training data fully prepared in advance during the offline training phase. in fact, preparing all the data in advance is labor-cost and time-consuming. it is very hard to collect all the required training data ahead of time. therefore, afalrs makes use of the online sequential learning method to suit the way the new training data arrives without sacrificing the accuracy and it adapts itself to a new environment without retraining a new model with all the training data."
"this work uses a particular needle steering technique in which thin, flexible needle shafts curve during insertion due to tip forces induced by needle tip asymmetry. our needle tips have either a simple bevel or a bevel combined with a permanent bend of the needle shaft very close to the tip. control of the 6-d pose of the needle tip can be achieved by various patterns of needle insertion and rotation (spin about the needle's long axis), where rotation reorients the direction of asymmetry. in these experiments, we focus on the degree and repeatability of needle curvature, as well as needle insertion force."
"as with intuitionistic logic, these new meanings for old operators result in deviations from classical logic. in particular, conjunction is no longer commutative, reflecting the typical order dependence of dynamic acts. facts about the informational process are now encoded in the logic of the logical operators in this system. this encoding becomes even more pronounced with a new dynamic consequence saying that, after processing the successive premises, the conclusion has no further effect: dynamic semantics keeps the actions implicit, while giving the old language of propositional logic richer dynamic meanings supporting a new notion of consequence, with a technical theory that differs from standard propositional logic."
"the proposed scheme brings several merits in multi-robot path planning and navigation, nevertheless, certain issues remains to be addressed. for example, the proposed method does not transfer the exact dimensions of the obstacle. a blocked path which cannot be traversed by one robot might actually be traversable by another robot of narrow dimensions. the proposed scheme does not take the dimensions of the robots and obstacle geometry into consideration. although it is possible to also incorporate such information, it has not been done in our experiments to save communication bandwidth and computation."
"for insertions into the prostate, the canine was placed in a supine position on the imaging table, with the hind legs pulled in toward the torso, and the robot was mounted on a movable cart in front of the perineum [see fig. 5(a) ]. after each needle insertion, the needle was detached from the robot and the cart was moved to allow the c-arm to rotate around the canine. for insertions into the kidney, the animal was placed in a lateral position to expose the left kidney [see fig. 5(b) ]. for insertions into the liver, the animal was placed in the supine position [see fig. 5(c) ]. the robot was mounted directly on the imaging table for kidney and liver experiments. all needles and robotic components touching the animal were chemically sterilized prior to the procedure and other components were covered with sterile plastic covers."
"this is like pal-style learning by elimination of worlds. but in other intuitionistic steps, like the one in m1, there is no such elimination of endpoints, and we merely get more proposition letters true at the next stage. one might explain this move as a new type of informational action, namely, 'awareness raising' #ϕ that some fact ϕ is the case, where awareness involves syntactic in addition to semantic information."
"remark there are well-known analogies between non-monotonic consequence and conditionals in the style of lewis [cit] . instead of [!ϕ]bψ, this might favor conditional belief b ψ ϕ pre-encoding effects of learning that ψ. the two versions are not quite the same, as update !ψ restricts a model to its ψ-worlds, while a conditional can still look at ¬ψ-worlds when evaluating ϕ. but these details need not concern us here."
"for flight stability, a general mav (micro aerial vehicle) has design principles and constraints. first, most drones consist of a main body at the center, support arms connecting thrusters with the main body, and landing gears. to maximize flight stability, the main body is located at the center of the drone to ensure that the center of mass is located at the center of the drone and to minimize the moment of inertia. second, from the main body, support arms are arranged radially or symmetrically. the connecting material should be rigid and should not transfer vibration. if it is not rigid, the thrust force will not be fully delivered to the main body, and as a result a feedback controller such as the pid controller will not work well because of incorrect feedback information. third, the parts containing heavy components such as the battery, electric devices, and landing gears should be placed near the center of gravity as long as they do not hinder pitch or roll motion. this condition increases flight stability and helps pose change with small thruster force. under these design constraints of the typical mav structure, there are practical difficulties in applying the tiltrotor mechanism. in particular, the propeller's large diameter makes applying a tilt mechanism harder in that the propeller size increases in proportion to the payload according to the momentum theory or actuator disk theory. in order to carry 2,500 to 3,000 g of payload with a moderate flight time, the required propeller diameter is about 10 to 12 inches [cit] . like this, as a normal propeller requires large diameter and volume for tilting, a duct fan unit can be considered as an alternative. most importantly, it can save space because of its smaller outside diameter of about 70 mm. the overall layout of the main body frame is based on a general x-configuration quadrotor. two thrusters are connected with a tilt arm and rotate together along the tilt axis as shown in fig. 8(a) ."
"the following is a recursive map tr from propositional formulas ϕ to modal formulas tr(ϕ)(q), where q is a fresh proposition letter (note the clause for conjunction):"
"we have seen how the implicit/explicit contrast runs through both static and dynamic logics for knowledge and belief, as well as for logics for consequence relations."
one significant factor in multi-robot communication is the amount of data exchanged. the proposed node representation of the path enables that minimum communication is sufficient to convey meaningful messages. a lazy update of obstacle information by robots ensures that the robots only stop to update the relevant information which affects the current navigation towards their goals.
3) cbct needle segmentation and needle shape analysis: the scans from the philips xperct were converted into dicom format and imported to matlab for analysis. a custom mat-lab program was written for semiautomated analysis of cbct scans where the user specifies threshold levels for segmentation and defines regions containing the needle. the program then automatically fits either circles or lines to the 3-d data as described in section ii-d3.
"s5 can define model changes in ambient sets q using the formulas tr(ϕ) as indicated, and this process even simulates the working of us in a recursive manner."
"the ever growing mobile network traffic demand requires reliable backhaul solutions to connect different users to the core network providing high capacity and high reliability. as a flexible networking technology, wireless systems continue to play an important part in backhauling of next generation of mobile network referred to as 5g [cit] . 5g technology is expected to support high reliability performance, such as the connection availability in order of 99.999 % for certain use cases [cit], which corresponds to 5.256 minutes mean downtime per year. meeting this stringent reliability requirement is challenging due to the nature of wireless communications susceptible to random fading of wireless channels, including multipath fading and rain fading [cit] . thus, the evaluation and improvement of network reliability in the presence of failures becomes essential for the design and maintenance of future highly reliable wireless networks."
"next, how can we do belief revision in an alternative implicit style? one line runs via dynamic semantics, with new meanings for linguistic expressions such as epistemic modals, [cit] . all our earlier points apply, but we will not pursue them"
"in large-scale highly dynamic indoor environments, the outstanding wifi-based ips should offer advantages of not merely the high positioning accuracy, but also the lifelong performance running, together with the fast and feasible site survey. in this paper, we have proposed afarls to offer these advantages in the large-scale indoor environments and have validated the performance through extensive experimentation. the results show that afarls can offer realtime performance with high accuracy, and leverage online incremental data with a varying size to update the out-of-date model without retraining a new model. this novel adaptive wi-fi indoor localization model inherits the advantages from the original elm, src and knn algorithms in order to tackle their respective drawbacks which render their practical applications in ips. future improvement can be focused on filtering algorithms and more advanced elm algorithms to enhance the performance and reduce the memory size of the reference database by storing only the reliable training fingerprints collected through the popular collaborative or crowdsourced method."
"indeed, it has been claimed that natural language conveys much information implicitly, perhaps for ease of coding. implicit logics would then model this reality directly, whereas explicit logics of information and agency are outside theorists' views of language. but this does not fit the facts. natural language is a medium where both stances occur, in the guise of one might call participating versus observing modes. a key feature to keep in mind here is the universality of language."
"θ f tilt angle for front thrusters in drone body frame (deg) θ r tilt angle for rear thrusters in drone body frame (deg) θ tilt angle with respect to the wall for front thrusters during stabilization (deg) θ a tilt angle with respect to the wall during wallclimbing (deg) α pitch angle of a drone with respect to the wall (deg) f t (1, 2) thrust force of front thrusters (n) f t (3, 4) thrust force of rear thrusters (n) f f friction force between the wall and a drone (n) f a required thrust force for wall-climbing (n) f w the drone's weight (n) m the mass of the drone (g) µ static frictional coefficient"
"to see this, transform s5-formulas ϕ into their normal form nf(ϕ) of modal depth 1. system identity now an earlier issue returns. do the preceding results say that us is the same system as s5? our translations reduce valid consequence both ways, which is enough for the standard notion of system equivalence. but the intuitive novelty of us is that it does something more: it can express the dynamics of model change."
"abstract-inherent vulnerability of wireless backhauling to random fluctuations of the wireless channel complicates the design of reliable backhaul networks. in the presence of such disturbances, network reliability can be improved by providing redundant paths between given source and destination. many studies deal with modifying and designing the network topology to meet the reliability requirements in a cost-efficient manner. however, these studies ignore the correlation among link failures, such as those caused by rain. consequently, the resulting topology design solutions may fail to satisfy the network reliability requirements under correlated failure scenarios."
"qualitatively, both surgeons conducting the experiment noticed a difference between tip-asymmetric needle steering and straight needle insertions. for insertions into the prostate, it was clear that the prebent needle was following a curved path; however, due to tissue inhomogeneity and possible needle shape changes from detaching the needle from the robot, there was no significant curvature of the prebent tip needle shaft after insertion. with a thicker needle, or by using a straight introduction needle prior to insertion into the prostate, we might observe better needle curvature of a steerable needle. additionally, haptic feedback from trial insertions with brachytherapy needles was critical to identifying prostate tissue. thus, teleoperation for robotic needle steering could benefit from haptic cues."
"a perching experiment according to the tilt angle to the wall, θ, was conducted with our prototype drone where the friction coefficient between drone and the wall was about 0.5 to 0.7. the tilt angle ranges from 0 (no-tilt case) to 90 degrees, with increments of 10 degrees. as shown in fig. 12, if the tilt angle is not sufficient, the perching fails. on the other hand, when the tilt angle is too excessive, it fails during the intersection zone of pose change, although stabilization is successful. with a full-tilting angle, it failed 10 times out of 10 trials as shown in fig. 12 ."
"existing backhaul networks typically form a tree topology to ensure connectivity in a cost-efficient manner [cit] . however, tree topologies are very sensitive to any kind of link degradation (such as rain attenuation and blockage) as they do not provide any redundant paths, which reduces the network reliability performance. mesh topologies combined with efficient survivable routing algorithms alleviate this problem by providing alternative communication paths [cit], at the expense of higher deployment cost. knowing that 25 − 50% of total network cost belongs to backhaul [cit], it is of utmost importance to develop cost-efficient backhaul network design approaches capable of guaranteeing high reliability."
"as mentioned before, during pose change, there is an intersection zone where the airflow of the thrusters at the front side could be obstructed by the structures of the thrusters. the effect of decreasing thrust force caused by air passage obstruction is verified by an experimental test as shown in fig. 11 . in the experiment, a drone is installed on a linear guide, on which the drone moves freely along the guide rail. the thrust force of right thrusters in fig. 11(a) is set to 16 n equal to the hovering or stabilization level, and the tilt angle of the other thrusters, θ r, confronting airflow of right thrusters is varied from zero to 90 degrees. we used a force gauge to measure tensile force caused by thrust force of right thrusters to verify the effect of the air passage obstruction. fig. 11(b) shows the result of experimental tests. the result shows that as the tilt angle becomes close to 90 degrees, the tensile force is decreased. on the contrary, the closer θ r gets to zero, the less air obstruction is caused and consequently the net thrust force increases. to overcome this platform limitation, we separated the pose change process into three phases depending on the intersection zone as in fig. 5 ."
"(a) (b) (c) the threshold confidence c th was set to 0.45, the threshold time t th to 60 s, and t z was set to 150 s. from equation (4), the value of the degree of the decay curve n was calculated to 0.652."
"either way, seeing the contrast reveals patterns running through the field of logic, and it suggests new questions of a conceptual and technical nature. we have shown this in a number of concrete instances of system design and in translations between systems. so, seeing the contrast means work to be done, and in fact, we see it as a force toward a better understanding of the coherence of logic, both in its systems and working habits. moreover, we have pointed out in various illustrations that the contrast has philosophical consequences, since it undercuts sweeping ideological views that are tacitly based on taking one stance while ignoring the other."
"c) needle insertions in liver: access to the liver was obtained through a 10-cm midline laparotomy. a finochietto rib spreader was used to retract the wound edges and the lobes of the liver were stabilized using surgical towels. due to the deep chest of the canine, some lobes of the liver were not accessible; we performed one set of needle insertions in the right medial lobe and then two in the left medial lobe. a straight cone-tip needle was inserted in the left medial lobe while force data were collected. evans blue dye was used to mark the insertion sites to aid in histology postmortem. after insertion, a cbct scan was taken and the needle was retracted, while recording force data. the needle was then repositioned to the right medial lobe and the process was repeated twice. the same procedure was executed for the steerable bevel and bent tip needles, resulting in a total of nine insertions, three for each needle type. table iii summarizes the types of needles used for the in vivo experiment. the insertion speed of all needles ranged from 0.5 to 1.5 cm/s and the retraction speed for kidney and liver was 0.5 cm/s."
"one line of enriching classical logic adds new operators for new notions to be analy- we will discuss a sequence of illustrations displaying the contrast, and analyze what makes it tick. we set the scene by recalling some key facts about two well-known systems: epistemic logic and intuitionistic logic, presented with a focus on information and knowledge. after that we discuss less standard cases such as logics of information update, default reasoning, games, quantum mechanics, and truth making."
"this part of the complexity is because of the comparison the ptss in order to select the best ofdm signal, and it can be written as"
c. link failure probability model link failure probability parameter abstracts the ability of the physical layer of wireless systems to reliably transmit packets [cit] . a link is considered to fail when its received power falls below a certain threshold necessary to achieve a certain bit error rate (ber) that is required to maintain a given quality of service level.
"the in vivo results for needle curvature show that prebent tip needles exhibit the most curvature, and bevel-tip needles (without additional modification [cit] ) do not exhibit enough curvature to be clinically relevant. the tendency for larger diameter needles to have higher maximum and mean insertion forces could be due to larger needle surface area. higher insertion forces for cone-tip needles than bevel-tip needles could be due to a higher coefficient of friction for stainless steel needles. this effect may not have been seen in bent-tip needles as interaction forces at the needle tip could have much larger than friction forces [cit] . the lower retraction forces in liver as compared to kidney were a surprising result as in the ex vivo case, the reverse is true. the effects of blood perfusion on tissue friction for each organ could explain this result and would need further study."
"excluded middle ϕ ∨int ¬int ϕ was not valid -where indices highlight the fact that the failure occurs on the intuitionistic understanding of negation and disjunctionthough special cases of this principle may, and do, remain valid. in contrast with this, the law of excluded middle is unrestrictedly valid in epistemic logic, but it"
"where s ij denotes the spatial correlation between links e i and e j, and s ii refers to the spatial correlation of link e i with itself."
"many researchers have designed various types of drone platforms [cit] along with their control strategies [cit] . also, they have developed tilt-rotor-based drones, some of which focus on the attitude and position control of the drone [cit], and others apply tilt-rotor mechanism to wall-climbing robots [cit] . although the tilt mechanism for drones has high potential to broaden the application area, only a few commercial racing drones use tilt mechanism to increase the speed [cit] . because the payload of drones is very limited, an additional mechanism could decrease the operation time and the energy efficiency. considering these limitations, we aim to develop a tilt-rotor-based wallclimbing drone using a minimum number of actuators. using only two actuators, the drone is designed to perch on the wall with gentle motion and climb the wall with higher energy efficiency than the previous caros."
"4) tissue health assessment: tissue health was assessed through blood work and histological analysis. blood samples (10 ml) were taken several days prior to the procedure (baseline), immediately after the procedure (day 0), and on the first, third, and seventh day following the procedure. a complete blood count analysis, including white blood cells (wbc), red blood cells (rbc), hemoglobin (hgb), hematocrit, and platelets, was performed on each sample. additional tests for liver function (bilirubin, aspartate aminotransferase (ast), alanine aminotransferase, gamma-glutamyl transferase, lactate dehydrogenase, and albumin) and kidney function (creatinine and blood urea nitrogen) were also performed. we note that when the canine arrived from the supplier, his front right paw was swollen from a previously acquired infection. antibiotics were administered two days prior to the procedure."
"the reason is that, unlike in truth maker semantics, we do not demand existence of all suprema in our partial orders. the modal logic of information structures is axiomatizable, but a major open problem is whether it is decidable. it is known that logics with associative modalities can often encode undecidable word problems, which might be a warning sign for impossible worlds as suprema in truth making."
"the c-pts strategy has been viewed as the significant probabilistic scenario to decrease the high papr pattern in ofdm framework. however, the computational complexity is the prominent drawback of the c-pts method, because the system should perform a comprehensive search to select the optimum phase factor."
"the t-pts algorithm leads to diminishing the correlation among the subcarriers within the subblocks. moreover, the superiority of the ad-pts method is due to the new scheme does not organize the order of subcarriers inside the subblock successively. therefore, t-pts can accomplish papr reduction performance more prominently than both ad-pts and il-pts."
"in this section, we modify the (td-if) problem, formulated in (10) to take into account the spatial correlation of rain attenuation. to do so, the first step is to modify the path failure probability p k f to consider the spatial correlation of the rain. then, we consider spatial correlation between different paths by adding extra cost for selecting correlated paths."
where w represents the number of the different phase rotation factors. the optimum phase weighting factor which achieves the minimum papr is obtained by 12
"where α is a mathematical constant, set to e. withp, knn based on the sorensen distance metric is performed to generate the adaptive sub-dictionary that contains o in which the rss values are positive. for the sake of clarity, we refer to these steps, starting from the output of coselm to knn, as q-by-k clustering. after q-by-k clustering, we can perform the src-based classification to identify the floor by using the adaptive sub-dictionary that contains the fingerprints, o, provided that the discriminative criterion is satisfied. otherwise, the floor is directly determined from coselm."
"then the preceding intuition becomes the following truth definition: this extends classical propositional logic: the base clauses are standard, with one operator clause added. epistemic accessibility ~ is often taken to be an equivalence relation -but we can vary on this if needed. the resulting logic is s5 for each single agent, without non-trivial bridge axioms relating knowledge of different agents."
"now it is easy to show that, for models m whose domain is the set s, for us. e.g., the logic of pal extended with conversational programs that allow finite iterations is non-axiomatizable and not arithmetically definable, [cit] . so, dynamic semantics for discourse rather than sentences might run the same complexity risk."
"b. elm elm is proposed based on a single-hidden layer feedforward neural network (slfn) architecture [cit], and the latter extended to the generalized feedforward network. elm itself is a supervised batch learning method. its notable merit lies in it randomly selects the hidden node parameters (input weights and bias), and then it determines only the output weights. for n arbitrary distinct data samples,"
"the rest of the paper is organized as follows. section 2 deals with obstacle information on node representation of the path. section 2.2 explains path planning on node map. section 3 explains the obstacle removal and update in the map with lazy update scheme. results are shown in section 4, with simulation results shown in section 4.1, and results in real environment shown in section 4.2. results are discussed in section 5 along with the limitations of the proposed method. finally, section 6 concludes the paper."
"in this paper, we explore the capabilities, challenges, and clinical relevance of tip-asymmetric needle steering in prostate, kidney, and liver. we present the first known work demonstrating needle steering in in vivo tissue (inside an intact, living organism) and conduct extensive needle insertion experiments in ex vivo tissue (whole organs removed from an organism) to allow for statistical analysis of the effects of various parameters on tip-steerable needle behavior. needle behavior includes needle curvature from 3-d needle reconstructions (ct or stereo x-ray) and insertion forces. we compare steerable needles to straight stainless steel needles and present a potential clinical application of needle steering for the treatment of large tumors with overlapping ablations."
"an example of a cbct reconstruction for a needle insertion into liver with a fit curve is shown in fig. 9 . in this case, a 10.4-cm circle was fit to the needle. table iii summarizes the curve fitting and the root-meansquared error associated with the fit. the program fit circles, rather than lines, to 20% of the straight needles, 20% of the bevel tip needles, and 80% of the prebent needles. for all insertions, the errors of fitting are on the order of the needle diameter."
"the curves given by equation (3) have the desired characteristic that for higher values of n, the curve flattens out more and delays confidence decay until the threshold time (t th ), and after that it decays quickly to zero in t z time. for a given c th, t th, and t z, the value of the degree of the curve (n) can be found by solving equation (3) as,"
"1) blood work: most of the blood test results remained in the normal range through day 7. the wbc was high at baseline, most likely due to the existing paw infection, and peaked on day 1. by day 7, the wbc returned to a normal level. rbc, hgb, albumin, and ast levels were abnormal following the surgery, but returned to normal levels by day 7. in general, the blood test results were consistent with expected responses to surgery, and not consistent with additional trauma that may have occurred from needle insertion."
"further examples in this epistemic line can be found by moving from information flow to agency and games: in the monograph [cit], implicit logic games and explicit game logics are naturally entangled strands throughout. but once one sees the contrast, it applies to any part of logic whatsoever, not just information and agency."
"the annotations say that the two branches of m2 may be viewed as public announcements of which endpoints, viewed as classical valuations, the process can get to."
"where θ d is the target pitch angle of the drone andθ is the drone's current pitch angle estimated from the imu. then, the control values of the rear thrust level δ t (3, 4) and pitch offset angle δ γ are calculated based on the pose error (θ) as follows:"
"the second view need not be superior to the first, but its very existence undermines strong conclusions arising from looking at consequence in only one stance."
"we consider a network that is partially or completely affected by rain. depending on the weather conditions, each link may experience different channel attenuation: multipath fading in clear sky or rain attenuation during rain. let us denote the rain fading on link e i by a i . a i follows a lognormal distribution [cit] :"
"where l is the body length of the drone. however, as the pitch angle of the body α decreases, the thrust force may not generate the torque for pose change, since t a approaches to 0 when α approaches to 0. we define pitch offset angle γ as the additional tilt angle for rear thrusters to generate the torque for pose change, as described in fig. 6(b) . we determine γ in the attitude control mechanism for the third phase. depending on the angle γ, the torque can be generated as follows:"
"so, now we have encountered two major research paradigms in the field of logic, both meant to take information and knowledge seriously -but doing so in very different ways. let us highlight the major differences that showed in the above:"
"but for the purposes of this paper, we will just stipulate that both are based on stable interesting sets of intuitions, both have generated a rich mathematical theory, and both seem bona fide instances of a logical modus operandi in system design."
"going beyond what we know. but beliefs can be wrong, and new information can lead to correction and learning. one trigger for belief revision are the earlier public announcements. here is the recursion law governing the matching model changes:"
"knowing the specific characteristics of rain attenuation, this paper proposes a new model to consider the spatial correlation under rain, and investigates the related topology design prob-lem. the objective is to add links in the backhaul network such that their total cost is minimized and the required reliability under rain disturbances is satisfied. we address this problem in several steps of increasing complexity. we first formulate an integer linear program (ilp) of the problem aimed at minimizing the total link cost under reliability constraints for a given pair of source and destination nodes. then, we modify the formulation to include the spatial correlation of rain attenuation. to do so, we propose a new model to consider correlation based on the joint failure probability distribution between different links under rain. our model considers two correlation aspects, i) link-wise, which deals with correlation between links along a path and computes the path failure probability using pairwise joint probability distribution of rain attenuation between different links along that path and, ii) path-wise, which deals with spatial correlation among multiple paths and generates a path correlation matrix using pairwise link correlation coefficients. we formulate the topology design problem considering both link-and path-wise correlation as a quadratic integer program (qip), which is np-hard. hence, we develop an efficient heuristic algorithm based on the continuous relaxation of the problem, which is shown to find near-optimal solutions."
"dynamic semantics comes in many forms. we will use veltman's propositional update logic us for a comparison with the explicit pal approach. here, on a universe of information states (in the simplest case, sets of valuations), each propositional formula ϕ induces a state transformation [[ϕ] by the following recursion:"
"dynamic epistemic logic makes the actions explicit, provides them with explicit recursion laws, extends the old base language while retaining the old meanings for it, and in all this, it still works with standard consequence."
"considering that the pitch and yaw angle in the body frame is very small in a moderate flight state, the drone has two sonar sensors measuring the distance to the wall, d l, d r, and calculating the approaching angle to the wall, τ, as shown in fig. 10(a) . when the drone detects a specific distance of about 10 to 150 cm from the wall, the landing assistant system is activated to make the drone's heading normal to the wall by decreasing the approaching angle and maintaining the altitude from the ground. this self-alignment system is working in the high-level flight controller for yaw motion control and thrust level control by using z-axis acceleration and altitude information h, as illustrated in fig. 10(a) ."
"the systems presented here are explicit in a double sense. not only do events and acts that usually stay in the background of logical systems become first-class objects of study, but also, dynamic logics for knowledge and belief have explicit syntax and laws for these actions. the new structure is not put into the meanings of the original language, and so we get conservative extensions of earlier static logics, although sometimes there is a need for some redesign of the original static vocabulary."
"having introduced our explicit/implicit contrast for two well-known logics, we now move to more recent developments and see where it leads. we start by noting that inquiry lies at the heart of both epistemic and intuitionistic logic. clearly, knowledge and information do not function in isolation, but in an ongoing dynamic process of informational action, or in a social setting, interaction between agents."
this work assumes that the robots of the multi-robot system are on the same network and can communicate with each other and to a central server. each robot is also assigned a unique robot-id (r id ).
"another purpose of the tilt-mechanism is to climb the vertical wall in that the tilt mechanism helps the drone stick to the wall with low friction coefficient by mitigating the dependency on the friction force generated by the normal force to the wall surface. however, in the wall-climbing test, it was found that friction force was readily decreased as the contact surface is worn out from rubbing or stained with contaminant like fine dust. for these reasons, it could be reasonable to consider very low frictional coefficient in real world application for thrust-assisted wall-climbing mechanism. therefore, other future work is to develop and optimize the wall-climbing system for very low frictional coefficient using a tilt mechanism in terms of energy efficiency, climbing performance, and safety."
"implicit logics might give the reasoning with, and explicit ones reasoning about. but while this seems appealing, it does not quite fit. for instance, epistemic logic with operators can also be used in first-person mode, and on the other hand, say, dynamic semantics has also been applied to third-person discourse."
"for all the keys (edges) in e whose value is 1, i.e., edges who are blocked by new obstacles, following information about the obstacle is maintained in a tuple:"
"here. instead, we show how our contrast can also take us, perhaps surprisingly, to an area of implicit logic that seems quite different from those discussed so far."
"however, considering that the wall-sticking mechanism is based on the friction force at contact points, which is caused by the normal force to the wall generated by the thrust force for flight, the friction coefficient is a very important factor and the energy efficiency is greatly affected by the unknown friction coefficient. due to the uncertainty of the surface condition, for example, the irregular shape or various contaminants, ideal condition with a high friction coefficient cannot be assured. for the same reason, when caros perches on the wall, assuming a low friction coefficient, the pose of the drone should be swiftly changed with large thrust force to maximize the normal force to the wall. from the viewpoint of protecting the wall and the drone, impact from high speed for perching is not favorable."
"after passing the intersection zone, assuming the first contact point is a hinge support, the thrust force f t (3, 4) causes a torque and the drone's body leans toward the wall. the torque at the first contact point a, t a, is expressed as follows (see fig. 6 (a)):"
"future work includes exploring new needle designs to improve curvature and expand applicability of needle steering to other organs. additionally, needle steering systems will need to be integrated seamlessly with existing medical imaging technology and the user interface will need to become more intuitive and natural, perhaps incorporating haptic feedback with collaborative human-robot needle control to improve needle placement."
"if we see one stance on a topic, we can usually find a dual one. thus our contrast becomes a force for logical system design. we saw this with dynamic semantics of questions, which suggested an explicit companion logic of issue modifying events. and conversely, explicit logics of belief change suggested new implicit notions of consequence in the area of non-monotonic logic."
"this minimal information is enough to convey turtlebot that there are two obstacles between nodes n 5 and n 10, and between nodes n 12 and n 13 as shown in figure 8c . moreover, it is also conveyed that the first obstacle is not traversable (indicated by value of 1), while the other is traversable (indicated by value of −1)."
"also, the computational complexity of the pr-pts and ad-pts schemes are equaled. this can attribute of that the pr-pts and ad-pts should implement all the stages of the ifft to transmit the subblocks from the frequency-domain into the time-domain. in contrast, the il-pts scheme has low computation complexity when using cooly-tukey ifft algorithm [cit] . because of the periodic transition of the subcarriers, the il-pts scheme does not perform all the ifft stages to transform the subblock into the time-domain. accordingly, the number of addition and multiplication operations of the il-pts scheme are fewer than that of the other ordinary schemes."
"we have noted the existence of our contrast, but we have not offered an explanation of why it is there. it has been suggested by readers of this paper that the background may lie in some well-known distinctions in logic and philosophy. one is that between reasoning with, from an internal firstperson perspective, and reasoning about, from an external third-person perspective."
"by introducing different weights to every nearest neighbor, knn is developed to wknn. the weighting scale depends on the distance between the test sample and the nearest neighbor. the primary effect of the weighting scheme is that its decision rule on the k nearest neighbors has less significance on the majority vote than the closer neighbors. as a result, the pattern y is assigned to one of the possible m classes in which the rule is not merely based on the closest distance to the query, but also the most frequent among its k nearest neighbors, as shown in (3)."
"where k p γ, k i γ, k d γ are coefficients for the proportional, integral, and derivative terms for pitch offset angle control, respectively."
"2) experimental procedure: five sets of canine organs (liver, two kidneys, and prostate from each animal) were used in these experiments. for insertions into the liver (left lateral lobe) and the prostate, each needle (cone, bevel, and bent) was inserted three times in different locations, resulting in a total of nine needle insertions per organ. for insertions into the kidney, the cone, and bevel needles were inserted into one kidney, three times each, and the bent needles were inserted in the other kidney, three times each, resulting in a total of six insertions in one kidney and three in the other, for each kidney pair. this was done to minimize overlap between individual needle tracks, due the relatively small size of the kidneys. two prostates were unusable due to small size and toughness, so a total of 27 needle insertions into prostate were recorded. insertion speed was 0.5 cm/s for all needle insertions. 4 . in vivo experimental setup. the robot was attached to a 6-dof passive arm, mounted either on the surgical table or a cart. the animal was placed under a c-arm cbct scanner."
"problem in src due to the np-hard problem of the former [cit] . with the optimal v, the characteristic function δ i (v) in src generates a new vector that only possesses nonzero coefficients inv associated to the respective c-th class. if it belongs to the class c, the residual of the class c, r c ("
"for cone-and bevel-tip needles, there was almost no curvature of the needle shaft. for bent needles, the insertion into the prostate had no curvature; however, needle insertions into the kidney and liver had radii of curvature ranging from 10.4 to 14.2 cm. as with ex vivo needle insertions, planes were fit to the data. the mean-squared error of the planar fitting ranged from 0.073 to 0.416 mm."
"wall-climbing robot technology has drawn constant attention in the field of mechanical engineering and robotics society due to its numerous possible applications. recently, some companies have launched commercial wall-climbing robots for cleaning walls [cit] or solar panels [cit] . however, wall-climbing technology still has limitations in that it is impossible to move along a separated façade or an irregular surface. the most important requirement for a wall-climbing robot for practical use is the stability to adhere to the wall and the reliability to prevent falling accidents. one promising solution for this is to apply normal force to the wall. for the inspection of civil structures exposed to harsh environmental conditions such as strong winds, this approach allows an aerial robot to stably cling to the wall [cit] . when the drone fails to attach to the wall, this mechanism can help the drone re-attach to the wall by generating a pushing force [cit] ."
"while the preceding analysis may seem just technical, well-known positions based on non-standard logics may come under pressure by an explicit-style reanalysis. in particular, existence of different consequence relations on a par has led to the thesis of logical pluralism, a view that logic should acknowledge competing views on the nature of logical consequence, and perhaps also other core notions of the field, [cit] . but in our view, this grand conclusion depends on taking the implicit methodology for granted. on a dynamic explicit re-analysis as presented here, the competition between consequence relations disappears, and we get compatible extensions of classical logic without any commitment to competition."
"each robot of a multi-robot system has a slam (simultaneous localization and mapping) [cit] unit which builds a map and localizes itself in it. the robots also update the positions of new entities in the map. the new entities could be the temporary obstacles, or new permanent features and both needs to be updated in the map for correct path planning. a robot estimates the absolute position (x obs, y obs ) of an obstacle in its map along with the uncertainty (σ obs ) associated with it. this information about the new obstacle (x obs, y obs, σ obs ) can directly be shared with other robots, however, there are certain disadvantages in doing so:"
"after a pose change, in order for the airframe to stay on the wall with minimum thrust force, the drone enters the wallclimbing mode. under the thrust level same to the hovering state, it adjusts the direction of all thrusters with the estimated friction coefficient from the stabilization process. according to the relationship between the friction coefficient and the required tilting angle in fig. 3, the direction of thrusters is calculated. another purpose of the tilt mechanism is to climb the vertical wall efficiently. the tilt mechanism helps the drone stick to the wall with a low friction coefficient by mitigating the dependency on the friction force generated by the normal force to the wall surface. the relationship between the required thrust force and the friction coefficient can be represented as follows:"
"what these two examples suggest is a more demanding criterion of system identity: equality or difference in 'natural generalizations'. but is there a formal basis to this, or would the criterion merely concern our current powers of imagination?"
"we have given some new examples, and no doubt much more can be proved about translating between implicit and explicit logics. even so, there is no automatic algorithm for turning one sort of system into the other. finding illuminating counterparts as we have done is an art rather than a science, and it may well remain so."
"mutual interpretability is a significant notion of equivalence that allows for much transfer of information, so we should always see if it occurs, but it need not be the last word. in fact, one vexing problem that makes it hard to judge how good this notion is has to do with a scarcity of negative results. there are no general methods showing non-translatability between logical systems. perhaps, in the end, there is too much translatability in the realm of logics, and a finer sieve is needed."
"to calculate the papr value accurately, the baseband ofdm signal is sampled at the nyquist rate. the oversampling operation is done by embedding (l-1) n zeros between the samples of the baseband ofdm signal [cit], where (l) denotes the oversampling factor, so that x(n) can be written as"
"from the results in simulation and real experiments, it is evident that sharing obstacle information can bring efficiency in robot navigation in a multi-robot system. in fact, this scheme attempts to mimic the symbiotic behavior of people in large environments. people often share such information with other people at different levels of abstraction, viz. 'hey, they blocked the way in front of the library', enabling other people to use this information to plan an alternate path. this knowledge sharing is particularly useful in large maps. notice that, in all cases, the first robot to find a new obstacle blocking a path needs to re-plan a new route to the goal in both traditional and the proposed navigation method. however, in the proposed method, the subsequent robots benefit from information shared by other robots for better path planning. moreover, each robot updates the obstacle information whenever it is observed again and shares the information with other robots."
"in this paper, the c-pts strategy for reducing the high papr value in ofdm system is analyzed in terms of partitioning schemes and computational complexity. furthermore, new partitioning scheme named t-pts is introduced to improve the papr reduction performance better than that two types of the ordinary subblock partitioning schemes, ad-pts, and il-pts. also, the computational complexity of the partitioning schemes was analyzed, and the numerical calculations of the pr-pts, ad-pts, and t-pts recorded the same number of the addition and multiplication operations. however, the il-pts scheme recorded less value of the computational complexity compared with the other schemes. therefore, the proposed scheme, t-pts can reduce the papr reduction performance better than that of the ad-pts without increasing the computational complexity."
"to mitigate this problem, a novel transformable drone platform is proposed in this paper, where the direction of thrusters can be changed and the ratio between the normal force and the ascending force can be adjusted using tiltrotors. consequently, the drone less relies on a wall sticking condition and it does not need to change its pose with high angular speed to prevent slippage."
"this section presents the experimental results and discussions. the results are obtained by running the experiments on a pc, which has an intel core i7-4700mq cpu at 2.40ghz and 8gb ram. two large-scale publicly available wi-fi crowdsourced fingerprint datasets are used to verify the actual effect of the system model proposed in this paper in a multifloor single building indoor environment and a realistic multifloor multi-building indoor environment. the datasets are the eu zenodo database and the ujiindoorloc database. [cit] at tampere university of technology. it covers a five-floor building that contains 822 rooms in total and has about 22570m 2 of footprint area of about 208m length and 108m width. the database consists of a total of 4648 wi-fi fingerprints and the corresponding reference locations. these locations contain the floor levels, the longitude and the latitude coordinates (in meters). from the database, it is split into the training set and the testing set. the former and the latter contains 697 and 3951 wifi fingerprints respectively. each row of the wifi fingerprint is represented by a 992-column vector of mac addresses which contains the default value of +100dbm for those non-heard aps. if the mac address is received, then it has a negative level of the rss value (in dbm). all the measurements were reported from 21 user devices which could support both the 2.4ghz and 5ghz frequency range. [cit] at universitat jaume i. it was reported from more than 20 different users and 25 android devices. it covers three buildings with four or five floors. the total cover footprint area is almost 108703m 2 . the database contains 21048 wifi fingerprints with the corresponding labels of the building, the floor and the real-world longitude and latitude coordinates (in meters) collected at pre-defined locations. furthermore, the database is split into the training set and the validation set. [cit] 7 training fingerprints and the latter contains 1111 validation fingerprints. since the publicly available ujiindoorloc does not provide the testing samples which are made available only for the competitors at evaal [cit], we use the validation data as the testing data. each row of the wifi fingerprints is represented by a 520-column vector of mac addresses which contains the default value of +100dbm for those non-heard aps. if the mac address is received, then it has a negative level of the rss value (in dbm)."
"some other interesting effects noticed in the data include tissue inhomogeneity, tissue relaxation [cit], and delayed capsule puncture [cit] . breathing effects are also present, especially for insertions into liver. these effects increase with insertion depth. the forces due to breathing during maximum insertion of the needle had an average amplitude of 0.452 n (0.153 sd) for liver and 0.193 n (0.061 sd) for kidney. these values were found by fitting a sinusoid with the same frequency as the ventilator to the insertion force data for all needle insertions and averaging across organ groups. breathing effects are negligible for prostate."
"here h is called the hidden layer output vector. the notable merit of the batch elm algorithm is the random generation of the hidden parameters of w i and b i without tuning during training. therefore, (10) becomes a linear system, and the β is obtained by solving the following least squares problem to minimize the error between t j and s j ."
"a) needle insertions in prostate: a small incision (2 cm) was made through the tough skin of the canine's perineum to allow for needle insertions. a transrectal ultrasound probe was used along with cbct scans and trial insertions with brachytherapy needles to locate the prostate and align the robot. a straight cone-tip needle was inserted robotically into the prostate and insertion forces were recorded. then, the needle was detached from the robot, the robot was pulled aside, and a cbct scan was taken. after the scan, the needle was removed manually and this process was repeated for the bevel and bent tip needles for a total of three needle insertions. retraction forces were not measured."
"the first explicit companion to all this seems the dynamic measurement logic of baltag and smets, cf. [cit] . their system of 'quantum pdl' has dynamic modalities for measurement actions that satisfy perspicuous laws mirroring physical quantum facts, but it remains squarely based on classical logic. in doing so, it explains all the deviant features of quantum logic in a uniform manner as properties of a small frag- this brief exposition may not do justice to explicit quantum dynamic logic, but suffice it to say that this new approach placing measurement actions and quantum information flow at center stage is more than just logic-internal system redesign."
"inquiry and questions a current innovation in dynamic semantics is inquisitive semantics for natural language, [cit], where formulas get richer 'inquisitive meanings' reflecting their role in, not just conveying information, but also in directing discourse. the resulting logic is a non-classical intermediate logic related to medvedev's logic of problems from the intuitionistic tradition. our analysis then suggests the design of an explicit counterpart. such dynamic-epistemic logics of inquiry -not tied to natural language, but closer to epistemology and learning theory -involve explicit acts of 'issue management', where questions and related actions modify current issue structures on top of epistemic models, [cit] ."
"despite employing knn at the very beginning to determine the optimal k nearest neighbors from the database, we propose it after coselm. this can be explained in terms of the reduced computational costs when knn works with clustering algorithms such as the k-means clustering approach [cit] and the support vector machine-based clustering approach [cit] . instead of using the initial large database, knn figures out the nearest neighbors from a cluster which contains fewer samples which are more correlated to each other. since coselm has demonstrated promising features in computational speed and learning new data continuously, we adopt coselm as the clustering algorithm for knn to generate the adaptive sub-dictionary which contains the optimal k nearest fingerprints, o to f . as pointed out previously, the recent developed knn based on the sorensen distance metric with powed data representation generates the best result compared to the traditional knn. thus, we adopt knn that is based on this configuration. first, the highest rss value, rss max, is identified by considering the fingerprints, p and f . then, we convert the rss values of p according to (32) and returnp."
"we performed experiments to evaluate the differences of needle curvature and force in ex vivo and in vivo liver, kidneys, and prostate for various needle types. we compared straight needles to tip-asymmetric bevel and prebent needles, and found that steerable needles do not seem to damage tissue more than straight needles. in addition, insertion and retraction forces may even be lower for steerable needles than for straight needles. the minimum radius of curvature was 5.23 cm for ex vivo organs and 10.4 cm for in vivo tissue. this difference could be due to the increased stiffness and increased effect of inhomogeneity due to perfused blood vessels in vivo, and it indicates a need to study the effects of blood perfusion on tissue properties."
"in one direction, given an implicit non-standard notion of consequence, one can tease out informational or other events motivating it, and write their explicit dynamic logic. this style of analysis, backed up by mathematical representation theorems, replaces non-standard deviation from classical logic into dynamic extension of classical logic. explicit events behind non-standard notions of consequence are sometimes easy to find, as in the above analysis of circumscription, but there is no automatic method for this art -and there are unresolved challenges concerning major substructural logics, [cit] . in particular, no explicit-style dynamic reanalysis seems to be known so far for linear logic, whose non-classical notion of consequence is primarily based on proof-theoretic resource intuitions. different structural rules will then encode differences in the underlying process of drawing a conclusion. thus, we generate new notions of consequence, and more would arise by using other mixtures of knowledge, belief and update actions."
"kinds that work together in many natural scenarios. acts of inference matter -but equally important are acts of observation, and of communication. such actions, or other events that embody them, are studied in current dynamic-epistemic logics, by adding an explicit vocabulary for the core actions to existing logical systems, and then analyzing the major laws of knowledge change, cf. the broad study [cit] ."
"where k p t, k i t, k d t are coefficients for the proportional, integral, and derivative terms for thruster control, respectively, and"
"the core of the performance evaluation considers the training time, testing time, training accuracy and testing accuracy for the results. throughout the experimentation process, the time is reported on average by running it ten times, whereas the positioning accuracy for the analysis of a predicted position (longitude and latitude) per test fingerprint uses the similarity function (in meters) in (38) based on the euclidean distance. to analyze n test fingerprints from the validation database, three accuracy metrics (in meters) are defined for evaluating the performance of the results. first, the error metric in (36) considers the two dimensional (2d) mean positioning accuracy of the test fingerprints regardless the correctness of building and floor identification. on the other hand, the error * metric considers the 2d mean positioning accuracy of the test fingerprints whose building and floor are correctly matched. last, the error pn metric in (37) considers the 2d [cit] ."
"let us define a th as the rain attenuation threshold corresponding to a link failure event. therefore, the link failure probability of link e i is equal to the probability that its attenuation denoted by a i is higher than a th :"
"in the proposed knowledge sharing scheme, the turtlebot already has the information of the two new obstacles with their confidence values beforehand, shown in figure 9k . hence, when turtlebot is instructed to go to the goal location, it plans a path considering the shared information from p3dx robot. the path planned by turtlebot considering the shared obstacle information is shown in figure 9l . notice that the confidence of obstacle 1 is reset when it is observed again (figure 9m ) and turtlebot shares this information with p3dx. similarly, obstacle 2 confidence is updated (figure 9n ) when observed again. turtlebot thus reaches its goal in an efficient manner considering the shared information about obstacles from p3dx (figure 9o ). in the absence of the proposed scheme, turtlebot would have relied on the old map information and would have to stop at the first obstacle and then re-plan a new path to the goal. this was avoided in the proposed method. although the experiment environment was small, if we consider a large environment, it is evident that sharing obstacle information would save more time and navigation. this would also turn into battery power efficiency of the robots. the values of obstacle confidence at different times are summarized in table 1 ."
"the rest of the paper is organized as follows. section ii presents the considered system model. the problem formulation considering independent failures is described in section iii. in section vi, the topology design formulation is modified for correlated failure scenarios and the heuristic algorithm for link addition is presented. the performance of our proposed approach is evaluated in section v and concluding remarks are given in section vi."
"to help discriminate reliable cell-state hierarchies from noisy rearrangements, sincell implements two approaches: i) a strategy relying on a gene resampling procedure and ii) an algorithm based on random cell substitution with in silico-generated cell replicates."
"the technique of transcranial electrical stimulation (tes) relies on the application of weak electrical currents on the scalp. these currents can have different spatiotemporal patterns. in the last 18 years, transcranial direct-current (dc), alternatingcurrent (ac), and random-noise current stimulation (commonly known as tdcs, tacs, and trns, respectively) have been used to neuromodulate different regions of the brain and their associated functions [cit] . despite the high number of published studies (especially for tdcs) [cit], the neural mechanisms underlying tes and the rationale behind the choice of a this article is part of the topical collection on neuromodulation * javier márquez-ruiz jmarquez@upo.es specific tes modality, as well as the selection of the specific stimulation parameters (duration, polarity, intensity, frequency, or location), remain unclear."
"the main goal of using adf is to make diffusion process higher in homogeneous regions and slow down the process where the gradient of the image is higher, which corresponds to the edges of the image. the model can be represented by equation 1."
"the images used in this research are taken from international skin imaging collaboration (isic) archive [cit] . images in this database includes cancer types like benign and malignant melanocytes, nevi and basal cell carcinomas, seborrheic keratosis etc. which are confirmed by biopsy. from this data set we have used 300 images with different skin cancer history, phototype, size, subtypes, localization and patient age. the ground truth segmented images are also provided in this data set."
"in post-processing, different morphological operations can be applied to extract the foreground data from background pixels. in morphological opening by reconstruction, the background objects are removed using a structuring element and the image is reconstructed. the morphological closing by reconstruction follows this step in which the remaining foreground object is enhanced and any spots left in the image are removed."
"a dermatoscope is a device which is used to capture different skin conditions. the device uses a magnifier which is illuminated by a light input which helps in capturing the color and microstructures of skin lesions more accurately. the images obtained from a dermatoscope are called dermoscopy images. currently, most of the computer automated systems use the dermoscopy images for analyzing and diagnosing skin cancer patients. however, these methods are still in their preliminary stage to be used by dermatologists in a clinical environment [cit] . in recent years, the use of smartphone cameras to promote health care and assist in early detection of the skin lesions. various applications in smartphones have potential to use these images for review by a dermatologist or automated detection of the skin lesions [cit] ."
"visual results for five selected samples are presented in figure 1, the selection of these samples is done on the basis of different characteristics of image quality and skin lesion, which is the cancerous part in the image. sample-1 shows the benign malignant which is localized at the abdomen, it has uniform lesion for cancerous part. in sample-4 dysplastic nevus was diagnosed and localized at the upper limb, it has uniform lesion only at the boundary of the foreground of image. sample-3 also diagnosed dysplstic nevus but localized at the breast and it has a nonuniform lesion as the right side of the foreground of the image is more affected as compare to left side, as shown in figure 1(a) . in sample-2 melanoma was diagnosed and located at the pubis, this image has bad illumination due to poor quality of image as it is dark at the corners, as shown in figure 1(a), also skin lesion in this sample is covered by body hair. sample-5 is diagnosed as benign malignant and located at the upper extremities of body, it has rough skin lesion with degraded image resolution and illumination. the statistical results for sample images are presented in table 1 ."
human and animal rights and informed consent this article does not contain any studies with human or animal subjects performed by any of the authors.
"once cell replicates have been generated, a sincell algorithm performs a random replacement of a given number \"n\" cells in the original gene expression matrix with a randomly selected set of in-silico replicates \"s\" times. for each set of substitutions \"s\", a new connected graph of cells is assessed using the same method as for the hierarchy being tested. in each \"s\", the similarity between the resulting graph and the original one is assessed as the spearman rank correlation between the two graphs of the shortest distance for all pairs of cells. the distribution of spearman rank correlation values of all replacements may be interpreted as the distribution of similarities between hierarchies that would be obtained from stochastic perturbations of a proportion of cells. a distribution with a high median and small variance would indicate a well-supported cell-state hierarchy. on the contrary, a distribution with a low median of similarities and/or a wide variance would indicate a hierarchy very sensitive to changes in the data, and therefore not well statistically supported."
"the process starts by reading the image then applying gray scale conversion on it. in pre-processing stage, the image will be filtered by adf, as explained in section-3. a 2-dimensional fast fourier transform (fft) is applied over image; which takes the image from the spatial domain in to the frequency domain. the reason to apply fft is that frequency domain is computationally faster as compare to spatial domain therefore most filtering methods are applied in frequency domain. in frequency domain image is decomposed into its sinusoidal components which makes it easier to target a specific frequency component. fft is calculated as given in equation 8."
"in conclusion, the future of tes and its successful utilization in basic research and clinical interventions in human subjects partially rely on the correct use of animal models and on the extent that these animal studies can provide answers to fundamental questions about the mechanisms underlying tes effects in the human brain. to this end, tes experiments in animals should be carefully designed to maximize opportunities for applying discoveries to the treatment of human disease."
"results from animal studies suggest an extensive margin of improvement in the application of non-invasive tes. considering the current state of the art, it is not an exaggeration to affirm that the success of this technique in human application for basic research and clinical treatments will rely to a large extent on the findings we will make for optimizing and refining stimulation protocols in animal models."
animal models are being used to study possible benefits of tdcs for treating a number of neurological and neuropsychiatric disorders that result from brain injury or pathology.
"where and represent the pixels belongs to segmented foreground of ground truth image and proposed method image respectively, and is logical and and or operators respectively. sensitivity shows the performance of an algorithm on basis of number of tp pixels identified, which is similar to the correct estimation of the foreground of the image. specificity is equivalent to performance measurement based on correct estimation of tn pixels, which is similar to the percentage of the correct estimation for background of the image. accuracy is the calculation of overall correct estimation of both tp and tn pixels, which means it will calculate the percentage of overall correct segmentation of both foreground and background of the image [cit] . sensitivity, specificity and accuracy is calculated as shown in equation 15-17."
"mst algorithms usually provide a single mst solution. however, in theory there could be multiple, equally optimal solutions. this could happen if several equally parsimonious paths exist or if two or more edges have the same lengths [cit] : \"in such situations, only one of the possible shortest paths is arbitrarily selected during tree construction, which has the potential to significantly affect the structure of the tree. in light of this issue, it has been appreciated that the solution to the mst problem is better considered a minimum spanning network (msn), in which all mst solutions are combined into a single graph that demonstrates all equally parsimonious paths\". nevertheless, in our case the probability of finding several pairs of individual cells with the exact same gene expression profile distance is virtually zero. therefore, we think this type of potential instability is negligible."
"in order to focus on the cancerous part of the image, low pass gaussian filter is applied over the image with cutoff frequency of 40, this will further smooth the image by ignoring pixels which are non-cancerous. gaussian kernel is shown in equation 9, which will be convolved with the ( )."
where is the number of objects detected in the image having only black pixels and is the selected reference object which has highest number of pixels. distance of all the objects from reference object is calculated by equation 12.
"the distribution of the expression levels of a gene can be described by a measure of variability such as the variance or the coefficient of variation. it is known that the expected variation is dependent on the mean expression values of the gene [cit] . based on this, we can simulate a stochastic fluctuation of the expression of a gene by perturbing the observed level in a given cell with an error term whose magnitude is consistent with the mean-variance relationship observed in the data. by doing that for all genes from an individual cell c i, we can produce an in silico replicate of it."
"e. note on the use of the spearman rank correlation between the shortest distance for all pairs of cells as a similarity measure between two graphs. this way of comparing graphs is taken from a similar approach used to compare phylogenetic trees [cit] or against positions within a multiple sequence alignment [cit] . to provide evidence of its use as a valid similarity measure between graphs we assessed the distribution of spearman rank correlation values obtained between a reference graph and a population of 100 hierarchies obtained when 100 random sets of a given percentage of genes are subsampled 100 times. we did this for increasing percentages of subsampled genes: 5%, 25%, 50%, 75% and 95%. supplementary figure 2 shows these distributions reflecting the expected behavior, that is: the more representative the subsampling (higher number of sampled genes) the higher the similarity of the resulting hierarchies with the reference."
"in both cases, late timepoints lead to hierarchies with a high median while early time points had a lower median and a higher variance. results suggest that at early time points homogeneity of cell states is high, leading to hierarchies more sensitive to perturbations of the data and therefore less statistically supported. however, late timepoints showed hierarchies more robust to both gene subsampling and replacement with in-silico replicates, reflecting a marked heterogeneity in cell states. indeed, a gradient can be observed in both panels (from 0 to 24, 48 and 72h) suggesting that heterogeneity in cell-states increased as a function of time."
"in the first iteration, the two clusters with the lowest distance are connected forming a cluster of size 2. in a new iteration, distances among clusters are recomputed and a new connection is added between the next two clusters with the shortest distance. the distance between a cluster of size higher than one and another cluster is the lowest distance between any of their constituent cells. the process is repeated until there are no cells unconnected."
"in the first iteration, a connection between two clusters a and b is added if a is among the closest k nearest clusters of b and b is among the closest k nearest clusters of a. this process is iterated until there are no unconnected cells. as with sst, the distance between a cluster of size higher than one and another cluster is the lowest distance between any of their constituent cells."
"funding this work was supported by grants from the spanish mineco-feder [cit] -53820-p) to jmr and from the us national institutes of health (rf1 mh114269) to jfm and jmr. dr. sánchez león reports grants from fpu13/04858, during the conduct of the study."
"gene expression levels detected by single-cell rna seq are subject to stochastic factors both technical and biological. this means that, if it were possible to profile the same cell in the same cell-state (or, more realistically, a population of individual cells in a highly homogeneous state) multiple times, the detected expression levels of a gene would randomly fluctuate within a distribution of values. in the ideal scenario where that distribution was known for each gene, individual cell replicates could be produced in silico, leading to variations in gene expression levels similar to what would be obtained from in vivo replicates. the generation of in silico replicates would then permit testing the reproducibility of the cell-state hierarchy upon random replacement of a fraction of the original cells with the replicates."
"where is the variance which is set to 40. after convolution, the inverse fft will be applied to obtain the filtered image . to ignore the non-affected part of the image, pixel manipulation is applied to the ground pixels. all the pixels having value greater than 155 will be assigned 255 which will turn the non-cancerous part of the image into white."
"in summary, animal models provide an excellent opportunity to directly measure different parameters related to the strength of externally applied electrical fields during tes, helping to guide the design of future stimulation protocols by constraining the assumptions and validating the predictions of computational models."
"in non-linear filtering methods all the pixels of the image are processed by a similar function which can merge different regions information. in the segmentation of skin cancer, the precise detection of the cancer boundary is extremely important therefore these linear filtering techniques are not preferred. adf solves this problem by increasing the spatial conduction in more homogeneous regions of the image and avoids major alteration of the signal along region boundaries. adf carries two main attributes, which includes causality and piecewise smoothing. causality means no fictitious information should be induced while passing from coarser to finer scale and piecewise smoothing is a process in which intraregional smoothing should occur preferentially as compare to interregional. both attributes can preserve edges of skin lesion in smoothed image thus improving the process of segmentation [cit] ."
"the main objective of this review is to summarize studies that have used animal models to improve stimulation protocols and to develop tes-based applications for enhancing normal brain function, and for treating a variety of neurological disorders. these studies in animal models have helped bridge a number of gaps in our knowledge of tes, such as basic mechanisms mediating its effects, the definition of safety limits, and the experimental validation of computational models. in the final section, we will discuss existing limitations for translating animal-based results to human studies and how we could possibly minimize them."
"in order to evaluate the performance of proposed technique, statistical methods based on dice similarity coefficients (dsc), jaccard similarity coefficients (jsc), sensitivity, specificity and accuracy is used. these parameters will evaluate the performance on the basis of number of same pixels identified between ground truth image and image which is segmented by the proposed method. these parameters use the terms true positive (tp), true negative (tn), false positive (fp) and false negative (fn). these terms are mainly used to analyze the performance of a segmented image. ground truth image will be a reference image in which the segmented part also referred to as foreground or black pixels and remaining pixels are considered to be the background or white pixels. in this scenario tp is equivalent to number of pixels correctly segmented as foreground, tn is the number of pixels segmented correctly as background, fp is number of pixels segmented falsely as foreground and fn is equivalent to number of pixels falsely segmented as background."
"in conclusion, therapeutically oriented studies addressing the applicability of tes in animal models of brain dysfunction contribute to our understanding of disease progression and recovery. in addition, these animal studies provide an opportunity to elucidate the mechanisms underlying the beneficial effects of tes, and to design systematic experiments for assessing the efficacy of different stimulation protocols for treating brain pathology."
"in contrast to the minimum spanning tree (mst) algorithm (that minimizes the total sum of the weights of any possible spanning tree), the sst algorithm prioritizes the highest similarities between any two groups of cells, proceeding in an agglomerative way that represents intermediate cell states. in some cases, mst and sst can lead to the same graph."
"(ftp://ftp.ncbi.nlm.nih.gov/geo/series/gse52nnn/gse52529/suppl/gse52529_fpkm_matrix.txt.gz). following [cit] and the vignette of its associated bioconductor package monocle (http://www.bioconductor.org/packages/devel/bioc/html/monocle.html), the expression matrix is restricted to 575 genes differentially expressed between cells from time 0 and the ensemble of cells of times 24, 48 and 72 hours of differentiation. here, we analyze each time-point separately and evaluate the statistical evidence of cell-state heterogeneity within them."
"four cell-state hierarchies were assessed for each timepoint separately (0, 24, 48 and 72h) on their logtransformed fpkm values using the first two dimensions of a dimensionality reduction with independent component analysis (ica) and a minimum spanning tree (mst). to evaluate the statistical support of the arrangements obtained, two sincell algorithms were applied: i) a gene resampling procedure and ii) a random cell substitution with in silico-generated cell replicates. supplementary figure 1a represents the distribution of similarities between a reference cell-state hierarchy and the 100 hierarchies obtained when a random set of 50% of genes are subsampled 100 times. supplementary figure 1b represents the distribution of similarities between a reference cell-state hierarchy and the 100 hierarchies obtained when 100% of the cells are replaced by a randomly chosen in silico replicate 100 times. here we used a uniform distribution as described in section 3.b.2.i)"
"image segmentation is a method used in image processing to partition an image into different regions and extract meaningful information. in dermoscopy images, segmentation can be used to identify the boundary of the skin lesions that may lead to cancer. various image features are extracted from the segmented image that can be used to classify the image as cancerous or non-cancerous. several image segmentation techniques can be used to extract the boundary of the skin lesions like thresholding, k-means clustering, and region growing, etc. thresholding method to segment an image uses a threshold value to convert an image from grayscale into binary. in otsu's method [cit], this threshold value is selected by maximizing the variance of the image. in adaptive thresholding, the threshold value for every pixel of the image is selected based on its surrounding pixel values. this adaptive method can help overcome the varying light conditions in the input image and provides a better conversion from gray-scale image to binary image."
"our proposed method is divided in to three main stages. first stage is pre-processing which includes smoothing of skin lesion in image. in the second stage, adaptive thresholding and morphological operations are applied. in the final stage, segmentation is performed."
"this algorithm performs a random subsampling of a given number \"n\" of genes \"s\" times from the original gene expression matrix. for each subsampling, a new connected graph of cells is computed using the same method as for the hierarchy being tested. in each subsampling, the similarity between the resulting graph and the original one is assessed by the spearman rank correlation between the two graphs of the shortest distance for all pairs of cells. the distribution of spearman rank correlation values of all subsamplings can be interpreted as the distribution of similarities between hierarchies that would be obtained from small changes in the data. a distribution with a high median and small variance would indicate a well-supported cell-state hierarchy. on the contrary, a distribution with a low median of similarities and/or a wide variance would indicate a hierarchy very sensitive to changes in the data, and therefore not well statistically supported."
"various methods to automate the segmentation of melanoma skin lesions have been proposed so far, otsu's method of image thresholding which is based on maximization of image variance is a frequently used method for skin lesion segmentation [cit] . in all the fixed thresholding techniques, a cutoff value is selected based on which the image is converted from gray-scale into binary."
"initially described by nitsche and paulus in humans [cit], modulatory effects of tdcs on motor function have been successfully reproduced in anesthetized mice showing increased or decreased amplitude of motor-evoked potentials in response to anodal or cathodal tdcs in m1 [cit] . a recent study in anesthetized rats, using tacs over the hindlimb area of the motor cortex, highlighted the importance of the cortical excitation/inhibition balance for determining whether tacs ultimately leads to increases or decreases in cortical excitability [cit] . besides the modulation of cortical neurons, tdcs applied over the sensory-motor cortex can also have an impact on the excitability of sub-cortical motor systems, which has been demonstrated by recording neck-muscle emg and descending volleys from the surface of the spinal cord in response to electrical stimulation of the red nucleus, medial longitudinal fascicle, and the pyramidal tract in anesthetized cats and rats [cit] . interestingly, the long-lasting effects of anodal and cathodal tdcs on sub-cortical neurons resulted, respectively, in facilitation and depression of evoked motor responses in the cat [cit], whereas the opposite modulation has been observed in the rat [cit] ."
"other metrics to compare graphs that could have been used include e.g.: i) the symmetric difference of two sets of edges, i.e. the set of edges present in either of the sets but not in their intersection. this distance evaluates the topology of the graphs, but does not consider distances between nodes. and ii) least moves, i.e. the minimum number of changes that graph a needs to become graph b. this distance is useful to measure difference between topologies, and weighted edges can be used giving different values to the actions of resizing, removing or adding an edge [cit] for a discussion on this metrics). however, we decided to use our approach based on rank correlations because it is less sensitive to small variations in the topology as far as they occur in the vicinity (i.e. they don't translate in large path distances between the affected nodes)."
"in this last part of the algorithm segmentation is applied on basis of boundary conditions. all the foreground parts of the image are referred to as objects. the largest object, which is also referred to as the reference object, is found by calculating the number of black pixel values in all objects and selecting the maximum one, as shown in equation 11."
"first, the mean m and variance v of all genes in the original gene expression matrix is computed. genes are assigned to classes according to the deciles of the mean to which they belong. next, for a given gene g, a variance v is randomly chosen from the set of variances within the class of the gene. then, a random value drawn from a uniform distribution u(0,v) of mean zero and variance v is added to the expression value of a gene g in a cell c. by perturbing all genes in a reference cell c in this way, we obtain an in silico replicate c'. repeating the process n times, n stochastic replicates are generated for each original cell."
"although tes is delivered with low-intensity currents in humans, concerns may arise about the technique when considering its safety limits, especially when protocols aim to increase intensity or duration of the applied electrical field, the number of stimulating sessions, or when studies are performed in susceptible individuals (e.g., children) [cit] . nevertheless, only a few studies have addressed the issue of safety in human trials, mainly using computational models [cit], or by assessing behavioral changes [cit], the occurrence of skin erythema [cit], and injury-related alterations of the blood-brain barrier or cerebral tissue detected by mri [cit] . however, the establishment of safety parameters for tes requires characterization of a dose-response curve, determination of a density threshold for histological damage, and the impact of electrical fields on molecular markers that mediate neuroinflammatory processes [cit] ."
"clearly, tes can have an impact on a variety of brain functions, even if the exact way tes is able to modulate behavior and enhance performance is not yet fully understood. animal models provide an optimal approach to explore the use of tes for boosting specific brain functions and to start elucidating the underlying cortical and sub-cortical neuronal mechanisms. although recent findings encourage the use of tes to improve different neuronal processes and treat different neurological disorders linked to impaired brain function in human subjects [cit], animal studies are necessary in order to discard possible adverse effects."
"morphological operations like opening, erosion and dilations are applied on binary image to preserve the information of the foreground part of the image, erode and enlarge the boundary of foreground pixels respectively."
"this sincell algorithm is particularly suited to evaluate associations with gene set collections such as those from the molecular signatures database (msigdb) of the broad institute (http://www.broadinstitute.org/gsea/msigdb/collections.jsp), gene lists representing gene ontology terms of functional pathways, and in general, any gene set collections that might be of particular interest for the user."
"in this work, we used adf in pre-processing stage for smoothing and to enhance the skin lesion then adaptive thresholding is applied to segment the skin lesions, which is also the motivation of this work as these both methods are not used in the skin lesion segmentation. the breakdown of this paper is as follows, section-2 presents the literature review, in section-3 adf is explained, section-4 and 5 present proposed methodology and experimental results and analysis respectively and paper is concluded in last section."
"in conclusion, animal models constitute a valuable approach for defining safety limits allowing for a systematic and in-depth analysis of tes-related changes in the brain. despite the low number of currently published papers focused on assessing safety thresholds for different types of non-invasive brain stimulation, the growing interest in human tes presumably will boost new animal studies in the near future."
"sometimes due to non-uniform illumination conditions, image background becomes uneven which can make the segmentation task difficult. generally adaptive thresholding filtering is used to differentiate the foreground of the image, which is the cancerous part, from the background of the image. it will segment the image by assigning a foreground value to all pixel having intensity less than a threshold and for the rest of the pixels a background value is assigned to secure a binary image. there are several methods to set the threshold value, in our case best result are achieved by using locally derived threshold based on mean [cit] . to improve the results, local threshold is assigned values of (mean-c), where c is a constant. a window ( ) is defined and mean is calculated as shown in equation 10 ."
melanoma is a type of skin cancer that originates from small pigments on the skin of subjects. the pigments contain cells that called melanocytes. fatality rates caused by melanoma skin lesions are the highest amongst skin cancer patients. melanoma skins cancer has been on the rise for the past few year [cit] . the dermatologists need to screen every patient thoroughly for possible infection. image processing techniques can help the dermatologists by automatically filtering the melanoma affected patients.
"we present here two graph-building algorithms that can be used to infer the progression through a continuum of intermediate cell states: the maximum similarity spanning tree and the iterative mutual clustering graph (imc). both algorithms start from a cell-to-cell distance matrix as an input, and compute a connected graph where nodes represent cells and edges represent their kinship as intermediate cell-states. the weight of an edge connecting two cells corresponds to the original distance between them. the algorithms start with all nodes unconnected, treating them as clusters of size 1."
"will be applied on each pixel after calculating the difference with respect to its four neighbors in east, west, north and south directions, as shown in equation 4-7."
"in conduction operator which controls the threshold value; if then pixels are considered as part of homogenous or interior region so higher smoothing should be done, if then pixels are considered to be part of edges so they should be less blurred. in this research, we used the value and conduction operator used is as in equation 2, because the best results were achieved with these parameters."
"where ( ) is the image pixel at position ( ) and ( ) is the frequency component of image at ( ), n and m are image dimensions. after fft the zero frequency component of the image is shifted to the center of the image spectrum, which brings the average brightness values of the image in the center."
"the use of animal models for exploring tes effects and associated underlying basic mechanisms has undergone a resurgence in recent years that parallels the increased interest in the application of this methodology to treat and enhance human brain function. as emphasized in this review, animal models provide an invaluable scientific instrument to understand the mechanisms implicated in tes, to define safety limits through the detection of lesion thresholds and the impact on molecular mediators involved in brain injury, to validate computational models, to inspire the development of more effective stimulation protocols, to enhance normal brain function, and to explore new therapeutic applications. much of the progress in addressing these fundamental questions has come from studies that have taken advantage of animal models to apply tes in combination with invasive neuroscientific tools that are difficult to implement in human subjects, like electrophysiological recording of neural activity, fluorescent and two-photon imaging, or optogenetic manipulation."
"the possibility of enhancing brain function in healthy subjects through the application of tes is enticing. indeed, it has been demonstrated that tes can improve visual contrast perception [cit], spatial tactile acuity [cit], motor performance [cit], and learning and memory processes [cit] in human subjects. animal models offer a unique opportunity to explore new stimulation protocols aimed to boost brain function, particularly those that are unconventional and may be associated with higher risk. in this regard, animal models have shown tesinduced enhancement of sensory-motor learning, and memory processes, constituting a \"proving ground\" for future applications in human subjects."
"in this paper, we presented a novel and simple methodology to segment the skin lesion in which adf is used initially to secure clean skin lesion in the image, then binary image is generated by adaptive thresholding and final segmented image is obtained by boundary conditions. the proposed methodology tested over 300 images taken from isic archive and averaged results based on statistical parameters are presented and compared with existing techniques. the proposed method achieved higher accuracy as compared to benchmark techniques. visual results based on different skin lesions are also presented here in which higher accuracy of segmentation for the proposed method can be observed. however, a degraded segmentation performance of the proposed technique is observed if skin lesion is non-uniform on the edges. this issue can be solved by increasing the limit in pixel manipulation step which can affect other constraints in segmentation hence this issue can be addressed in further research work."
"the animal-based validation of present tes protocols, together with other experimental approaches proposing a more unconventional application of tes, are of highest priority for the future application of the technique in human subjects."
"sincell implements an algorithm to evaluate this association. first, a new cell-state hierarchy is assessed where only the expression levels of the genes in a given functional gene set are considered. second, the similarity of that hierarchy with the reference hierarchy (the one assessed on the initial gene expression matrix) is calculated. the similarity between the two hierarchies is computed as the spearman rank correlation between the two graphs of the shortest distance for all pairs of cells. third, an empirical p-value of the observed similarity between the two hierarchies is provided. the empirical p-value is derived from a distribution of similarities resulting from random samplings of gene sets of the same size."
"the proposed method segmented all the sample images similar to the ground truth images, except for sample-3, in which only right side of the lesion is segmented, shown in figure 1 (f). in this sample image, the left part of the lesion was ignored due to pixel manipulation, same can be verified from statistical analysis in table 1, the jsc and sensitivity are 75.6% and 75.5% respectively and overall accuracy is also the lowest which is 94.3% as compared to other sample images. sample-1 and 4 secured the highest accuracy as the skin lesion was uniform and image quality was better therefore segmented part is almost similar to ground truth image and secured highest dsc, jsc and accuracy, as shown in table 1 . in sample-2, skin lesion was under body hair and due to bad illumination there were black spots in the image corners but this proposed technique even segmented this image with accuracy of 95.2%. this proposed technique ignored the hair and other objects in image by boundary condition implementation, as shown in figure 1(e), because the distance of these objects from skin lesion was more than 200 pixels, distance measurement can be seen by blue lines in figure 1 (e) for sampe-2. for sample-5, the image quality and resolution was degraded but due to adf and gaussian filtering, a better image is restored for segmentation, shown in figure 1(b) . non skin lesion parts at the corner of this sample image is ignored by same boundary condition, as shown in figure 1 (e). inner portion of skin lesion for sample-2 and 5 got ignored due to morphological functions and adaptive thresholding but it did not affect the overall segmentation results due to efficient boundary condition implementation as given in equation 11 and 12 of the proposed methodology. overall it can be stated that the proposed methodology performed better segmentation for all conditions except when skin lesion is non-uniform at the boundaries and this method ignored the less affected part of the skin lesion."
"once a cell-state hierarchy has been assessed and its statistical support checked, the next step is interpreting the hierarchy in functional terms. sincell allows different graphical representations that can help interpret the hierarchies in terms of the features of the samples (e.g. differentiation time) or the expression levels of markers of interest. in this section, we propose an analytical approach to test whether the cell-state hierarchy associates with a given functional gene set, that is: whether the relative similarities among the individual cells in the hierarchy are driven by the expression levels of a subset of genes with a common functional feature."
if the distance of object from the reference object is greater than 200 pixels than that object is ignored and remaining objects will be considered as the part of the cancer or foreground of the image. calculate the boundaries of the selected objects and exact segment of the affected part is obtained by mapping it on original image.
"pre-processing is an important step in every image segmentation technique. various image processing techniques can be used in the pre-processing stage to help remove redundant data from the input image. image smoothing and denoising is one of the essential pre-processing methods which is used to remove additive noise from the image that is introduced during the image capturing process. different linear and non-linear filtering methods to de-noise the images have been suggested. in linear filtering approach, e.g. averaging, gaussian filtering etc., every image pixel is assigned a value based on a linear weighted relation with the surrounding pixels. gaussian filtering uses a gaussian function to transform the image pixels and smooth the image for segmentation. most of the linear filtering approaches end up in the removal of the image noise but degrade the edge information as well. anisotropic diffusion filtering (adf) is a non-linear image smoothing and denoising method in which image quality is not degraded after the removal of noise and edge details are also kept accurately for further processing in the segmentation phase [cit] . the method ensures the fine details in the image are kept while removing the noise using causality and smoothing parameters."
"the results obtained are very promising, providing a platform to start performing network intrusion detection on a live network traffic link in a near future, a very important future step."
"f igur e 2 intra-volume motion estimation and correction. the volume is divided into two groups with different motion states, which are then interpolated for motion estimation and reconstruction. this allows us to improve the temporal motion correction sensitivity to tr/2. the figure shows the case with eight total slices acquired with mb 5 2. therefore, for each motion state, the four slices are acquired in two excitations as indicated by the index underneath the slabs"
maintaining the security of computer networks is a crucial task and plays an important role in keeping the users data safe and the network a safe place to work. such task can be accomplished by network intrusion detection system (ids) e.g. snort [cit] .
a model is a formal description of biological knowledge and its quantitative formulation using mathematical or computational tools. an ontology based description of these tools would clarify to biologists the value of the model results. however very few attempts in these directions have been made for modeling in physics and engineering [cit] . we expect that the increasing interest in mathematical modeling in life science will push toward an increasing interest in this aspect in a near future.
"to achieve motion-robust reconstruction, accurate estimation of motions between the rf-encoded volumes is essential. however, the rf-encodings can introduce significant image contrast differences between the rf-encoded volumes, which lead to inaccurate registration. for example, the first rfencoded volume with p phase on the left-most sub-slice will appear to have a bulk volume shift to the right in the slice direction, whereas the last rf-encoding volume with p phase on the right-most sub-slice will look like being shifted to the left. to overcome this, motion parameters with 6 degrees of freedom (three translations and three rotations) for each diffusion-weighted rf-encoded volume are estimated relative to a corresponding b 5 0 s/mm 2 (b0) volume with the same rf-encoding, acquired at the beginning of the scan. here, it is assumed that there is no motion between the b0 rf-encoded volumes at the start of the scan. based on our simulations, sub-voxel subject movements along the slab-encoding direction also create significant contrast differences even among imaging volumes acquired with the same rf-encoding, leading to registration errors. to address this undesirable interaction of motion and rfencoding, an iterative motion estimation method is developed, as illustrated in figure 1b . here, motion estimates are iteratively updated by registering the target rf-encoded diffusion volumes to the updated reference b0 volumes, created using the motion parameters estimated from the previous iteration. specifically, the reference b0 volumes are regenerated at each iteration by passing a high-resolution b0 volume and the current motion estimates through a forward rf-encoding model. the high-resolution b0 volume is reconstructed by gslider from the rf-encoded b0 volumes acquired at the start of the scan before motion estimation. as the iteration progresses, the reference b0 volumes should get f igur e 1 a, the reconstruction framework of the proposed motion-corrected gslider (mc-gslider) including data preprocessing steps, (b) iterative motion estimation and (c) motion-aware reconstruction. in the iterative motion estimation process, the updated reference b0 images are obtained by passing a high-resolution b0 volume (created from a b0 gslider acquisition at the beginning of the scan, assumed motion-free) through a gslider-encoding simulation that incorporates motion information estimated from the previous iteration closer to the motion states of the target diffusion volumes, which should improve the accuracy of the motion estimates."
"diffusion magnetic resonance imaging (dmri) has been demonstrated to be an important tool to study tissue microstructure. 1, 2 it has been widely used to assess structural brain connectivity by reconstructing white matter fiber tracts, 3 and is also capable of probing neural tissue abnormalities in clinical settings to detect and characterize numerous neurological and psychiatric disorders. 4 however, a major challenge for dmri is its low spatial resolution, typically at 1.5-2 mm isotropic, which compromises its ability to provide details of the brain's fine-scale structures, especially at the tissue interfaces such as at gray and white matter boundaries. 5, 6 the difficulties in achieving high-resolution dmri originate from (i) the inherent low signal-to-noise ratio (snr), (ii) the distortion/blurring artifacts of epi or spiral readout typically used in dmri, and (iii) the sensitivity to motion during long dmri scans. recent developments toward mitigating these issues are described below."
"in cdc, complement (c1q) binds to the fc of ig at the surface of the opsonized cell (cancer cell or vaccine cell), the complement cascade is activated, the membrane attack complex (mac) is formed in the cell membrane and the cell is killed. in adcc, fc receptor gamma of natural killer cell (nk) binds to the fc of ig at the surface of the opsonized cell (cancer cell or vaccine cell), nk kills the cell (cancer cell or vaccine cell)."
"among others, we have tested the examples of sect. 3, a distributed syn flood and a distributed port-scan. all these intrusions were successfully described using nemode and valid gecode and adaptive search code was produced and then executed in order to validate the code and ensure that it could indeed find the desired network intrusions."
"in this section, we briefly review the basic theory of gslider-sms, based on which we will develop mc-gslider. the original gslider-sms combines simultaneous multi-slab acquisition with a novel rf slab encoding that is similar to hadamard encoding. 43 several rf slab-encoded volumes are acquired in consecutive trs and combined to reconstruct a single high-resolution, thin-slice volume. the rf encodings are designed with slice-phase \"dithering\", such that a different sub-slice within each slab undergoes a p phase modulation for each encoding. this slice phase-dithering not only provides highly independent encoding basis for reconstruction (similar to hadamard), but also creates high-snr images in each slab-encoded volume for navigation-free phase correction. during the reconstruction, the simultaneously acquired slabs are first unaliased using leak-block slice-grappa. 44 the phase variations due to physiological motion are then estimated and removed using the low frequency phase of each rf encoded data estimated from the center of the k-space. finally, the corrected slab-volumes can be used to reconstruct the final thin-slice volume using standard linear reconstruction with tikhonov regularization 22 :"
"constraint programming (cp) is a declarative programming paradigm consisting in the formulation of a solution to a problem specified as a constraint satisfaction problem (csp) [cit], in which a number of variables are introduced, with wellspecified domains and which describe the state of the system. a set of relations, called constraints, is then imposed on the variables which make up the problem. these constraints are understood to have to hold true for a particular set of bindings for the variables, resulting in a solution to the csp."
"the port-scan attack can be described by a set of constraints between two network packets, the packet that initiates the tcp/ip connection and the packet that closes it, which closes the connection in a very short time after the tcp connection had been initiated. in order to detect if the network is under such attack we monitor if there are many of these sets of networks packet to appear on a network traffic in a short period of time."
"mc-gslider corrects for the inter-diffusion-direction bmatrix rotation by using the averaged parameters across the gslider encoding volumes in each diffusion direction, while ignoring the intra-direction rotation differences. such interdirection strategy has been proven effective for correcting most of the errors from b-matrix rotation, 38 because motion typically varies slowly within the acquisition of one diffusion direction, and the amount of rotation is relatively small compared with the change of diffusion gradient for different directions. in summary, the mc-gslider provides motion-robust high isotropic resolution dmri with a temporal motion correction rate of 2 s per frame. the method was shown to provide accurate imaging of fine-scale brain structures in the presence of large subject movements. such technique should improve the robustness of sub-millimeter isotropic dmri and pave way for its more widespread usage in neuroscientific and clinical settings."
"a comparison of reconstruction performance for single-and double-basis gslider encoding, with and without motion was conducted. only through-plane translation was considered because in-plane motions do not affect the conditioning of the reconstruction. psfs were calculated based on equation 4, where the impulse signal was first passed through the single or double basis encoding (a encode 5 dat), for both motion-present and motion-free cases. tikhonov regularization was used for the reconstruction:"
"the correspondence between the packet and its payload is achieved by matching the packet number, i, which is the first variable in the tuple representing the packets and the i th position of the array containing the payloads."
"distributed network attacks are very popular among hacker communities, since they are driven from several places and easily elude the detection mechanisms, since these attacks originate from several places at once, being difficult to identify the network traffic as an attack."
"for the distributed port-scan attack, we created a log file composed of 400 tcp network packets while a computer was being under a distributed port-scan attack, being used as the network traffic. the used signature describes 52 tcp packet variables."
"we still need to model more network situations as a csp to better evaluate the performance of the system. we also need to better evaluate the the work presented in this paper by comparing the obtained results with systems like snort. also, we have plans to implement new back-end detection mechanisms using different constraint programming paradigms."
"an interaction between two entities is a complex action which eventually ends in a status change of one or both entities. in the immune system, interactions can be specific (adaptive immunity) or non specific (innate immunity). specific interactions characterize the immune adaptive response and comprise a specific recognition phase between two entities, the antigen receptor and an antigen. these interactions involve the recognition of an antigen by:"
"although snort provides some basic mechanisms which allow the writing of rules that spread over several network packets, such as the stream4 or flow preprocessors, they do so in a very limited and counter-intuitive way, not allowing the description of more complex relations between packets, such as the temporal distance between two packets."
"the proposed reconstruction framework includes corrupted data rejection, physiological motion correction, eddy current (ec)-induced distortion correction, bulk motion estimation, and motion-aware reconstruction as shown in figure 1a . details of the various components are outlined below."
"in our recent work, we proposed the generalized slice dithered enhanced resolution-simultaneous multislice (gslider-sms) technique 22 to achieve high-resolution dmri, with high snr and low distortion. this technique combines blipped-caipi controlled aliasing with a slab radio frequency (rf) encoding for snr-efficient, navigator-free, simultaneous multi-slab acquisition. the acquisition also uses the zooppa approach for low-distortion ss-epi, and for the first time achieved in vivo whole-brain dmri at 600-800 lm isotropic resolution range. in this study, we extend the gslider-sms acquisition/reconstruction framework to create a motion-robust, navigator-free, high-resolution dmri technique termed motion-corrected gslider (mc-gslider), that mitigates both (i) the background phase corruption from physiologic and bulk motion, and (ii) the actual bulk inplane and through-plane motion effects. to our knowledge, even though there are some existing methods 14 correcting for in-plane motion for multi-slab diffusion acquisition, no method has been previously proposed to retrospectively correct for the actual 3d bulk motion with or without the use of navigators. in mc-gslider, a motion-aware reconstruction with spatially varying regularization is developed to optimize the conditioning of the image reconstruction under difficult through-plane motion cases. additionally, a navigator-free approach for intra-volume motion estimation and correction is proposed to achieve motion correction at a high temporal resolution of 2 s. simulations and in vivo experiments demonstrated the ability of mc-gslider to remove motion artifacts and recover detailed brain structure for dmri at 860 lm isotropic voxel size in the presence of motion ranging from sub-millimeter to several centimeters."
"to examine the benefits of motion correction, motioncorrupted cases were reconstructed using a recon generated from a encode with and without motion information. the psf error, defined as the normalized root mean squared error (nrmse) of the input impulse signal and the corresponding impulse response, was calculated for each case. the snr was also calculated using equation 5, where the snrs of the double-basis cases were normalized by ffiffi ffi 2 p to compensate for the doubled acquisition time to provide a fair comparison."
"whereas usually ontologies led to knowledge databases, in what follows, we adopted another approach in which concepts for an ontology of cell and molecule interaction were generated starting from an agent based model (abm), the catania mouse model (cmm for short) and its computer implementation, the simtriplex simulator [cit] . simtriplex simulates the immune system response elicited by the triplex vaccine [cit] against mammary carcinoma. this effort provides a solid infrastructure that is useful to overcome the semantic ambiguities that arise between biologists and mathematicians, physicists, and computer scientists, when they interact in such a multidisciplinary field."
"most of the work in the area of intrusion detection systems consists in the development of faster detection methods [cit], but there is also some work focused on how the network signatures are described and detected, such as in the work [cit], where the authors present a declarative approach to specify intrusion signatures which are represented as a specialized graph, allowing the description of signatures that spread across several network packets."
"in the case of the distributed syn flood attack, we created a log file composed of 100 tcp network packets while a computer was being under a distributed syn flood attack, which was used as the network traffic. the signature used to model the problem was composed by 30 tcp network packet variables. table 1 presents the time(user time, in milliseconds) required to find the desired network situations for the attacks presented in is present work, using gecode and adaptive search on a x86 architecture and also using adaptive search on the cell/be architecture. the number of network packet variables used to model the signatures and the number of network packets used to search the intrusions are also presented. the times presented are the average of 128 runs."
"as a gold standard dataset, diffusion-weighted gslider data were collected on an anthropomorphic head phantom (phantoms.martinos.org) using the following imaging parameters: the acquired data were reconstructed to high-resolution using standard gslider reconstruction. rigid motion transformations with 6 degrees of freedom were applied to the highresolution data to mimic subject movements at a temporal resolution of tr/2. by passing these transformed data through the forward rf encoding model, double-basis rfencoded data with motion can be simulated. spatially uncorrelated gaussian noise was then added to the k-space data (snr 5 10) to mimic in vivo noise conditions. finally, the proposed motion estimation and motion-aware reconstruction were applied to this simulated data to obtain high-resolution motion-corrected reconstruction. the conventional gslider reconstruction was also implemented for comparison."
"nemode is composed by a compiler, which reads a nemode program and parses it into a semantic model. then, based on that semantic model, it is generated code for each of the available back-ends in system. after all recognizers have been generated, each generated back-end receives as input the network traffic and produces a valid solution, if the intrusion described as a nemode program exists on the network traffic that was given as input to each back-end detection mechanism."
"where p is the coordinate in the observed space and p 0 in the desired undistorted space, d p denotes the voxel displacement field given ec-induced off resonance field eðqþ, q represents ec parameters. this correction only includes in-plane transformation and interpolation, thereby preserves rf-encoding information and provides reliable data to feed into the motion estimation and motion-aware reconstruction process."
"the cfa maps of the motion-corrupted data, reconstructed using the conventional gslider show marked deviations from f igur e 9 a, estimated subject movements during a deliberate motion scan. motion parameters were estimated at a temporal rate of 2 s across the 34-min scan needed to acquire 860 micron data with 44 diffusion encodings, using double-basis rf-encoding. b, reconstructed images during large motion periods, using the conventional gslider and mc-gslider. zoomed-ins of the images are shown on the bottom right of each image, which clearly indicating the ability of mc-gslider to recover detailed brain structures that were lost to motion f igur e 10 colored-fa results of motion-corrupted data obtained through the conventional gslider (middle row) and mc-gslider (bottom row), along with results from the reference data with negligible motion (top row). spurious structures that are present in the conventional gslider reconstruction (yellow arrows), likely from unintentional signal mixing between neighboring slices, are removed by mc-gslider the reference, even after co-registration. these deviations are highlighted in the zoomed-in panels with yellow arrows, where the incorrectly added structures are likely caused by signal mixing from neighboring slices. in comparison, the cfa maps of the motion-corrupted data reconstructed with mc-gslider is in much better alignment with the reference, indicating the ability of the proposed method to obtain highisotropic resolution even in presence of large subject movements. minor differences between the mc-gslider and the reference still remain, likely from residual displacements between these two separate scans even after co-registration and the residual errors in the correction."
estimated motion parameters are incorporated into a motionaware reconstruction where artifact-free high-resolution volume of each diffusion direction can be obtained by solving the following minimization problem:
"so far, we have worked with some simple network intrusion signatures: (1) a distributed port-scan attack, and (2) a distributed syn flood attack. all of these intrusion patterns can be described using nemode and the generated code was successful in finding the desired situations in the network traffic logs."
"to increase the temporal resolution of our motion estimation and achieve good reconstruction in rapid motion cases, motion parameters will be estimated at two rather than one time-point per rf-encoded volume (tr/2, 2 s). diffusion images are usually acquired in a slab-interleaved manner to minimize artifacts due to spin history or cross-talk, therefore, whole-brain coverage (with slice gaps) is achieved every tr/ 2. for motion estimation, the data in each tr/2 slab group are assumed to have the same motion states, and are interpolated to obtain full volumes to allow for motion estimation (figure 2 ). with intra-volume motion correction, the motion transformation matrix t in equation 3 will now transfer the high-resolution volume into 23n rf motion states, and the sampling matrix d will be altered to select encoded slabs for each group."
"problems in propagation-based [cit] solvers are described by stating constraints over each variable that composes the problem, which states what values are allowed to be assigned to each variable, then, the constraint solver will propagate all the constraints and reduce the domain of each network variables in order to satisfy all the constraints and instantiate the variables that compose the problem with valid results, thus reaching a solution to the initial problem."
"to mitigate the distortion/blurring of epi acquisitions in dmri, 24 several in-plane segmented techniques have been developed. [cit] these techniques can achieve excellent mitigation at a cost of longer scan time and increased motion sensitivity. on the other hand, a combined use of in-plane parallel imaging and reduced field-of-view (fov) acquisition [cit] has also been developed, which provides good, albeit more limited distortion/blurring mitigation, without the need to resort to multi-shot imaging. recently, zooppa 32 has enabled whole-brain imaging with reduced distortions through sagittal acquisition with phase-encoding along the head-foot direction. 34 another major issue in high-resolution dmri is its motion sensitivity during the long scan time, which causes image degradation and resolution loss. for single-shot epi (ss-epi), the diffusion-weighted images can be co-registered to correct for inter-volume motion. however, for multi-shot and multi-slab acquisitions, intra-volume subject movements between different shots or slab-encodings can cause severe artifacts and blurring. several methods have been proposed to address this problem, including several navigator-based motion correction 14, 25, [cit] and self-navigated or navigationfree motion correction approaches. 35, [cit] among the navigation-free techniques, augmented multiplexed sensitivity encoding (amuse) 42 incorporates motion correction into parallel imaging reconstruction, which reduces the effects of both microscopic and macroscopic motion in highresolution multi-shot dmri data. nonetheless, most methods including amuse do not consider the effect of throughplane motion, which would lead to large signal dropout or incorrect mixture of anatomy from different slices."
"where y are the physiological-motion and ec-corrected slabencoded volumes; x denotes the desired high-resolution volume to be reconstructed; t is the rigid-motion transformation matrix, 48 which transfers the high-resolution volume into n m motion states based on the n m sets of estimated motion parameters, where n m is the same as the gslider encoding factor n rf for inter-volume motion correction; a is the gslider encoding matrix which encodes high-resolution volumes with different motion into n m 3n rf rf-encoded volumes; d represents the sampling matrix selecting n rf encoded volumes with the corresponding motion, and k is the regularization parameter. note that in-plane motions (both translations and rotations) do not affect the conditioning of equation 3, as each rf-encoded volume is fully encoded in-plane. however, through-plane motions can decrease the orthogonality of the slice-dithered rf-encodings and lead to an ill-posed reconstruction. additionally, rotational and translational motions can cause spatially varying degree of through-plane motion, which leads to spatial variability in the conditioning of the reconstruction. to mitigate the potential of an ill-posed reconstruction, two repetitions of a full n rf rf-encoded gslider data are acquired consecutively (i.e., \"double-basis\" 5 2 3 n rf ) for each diffusion direction to provide more encoding information for motion robustness."
"in order to use the constraint programming mechanism to perform network intrusion detection, there is the need to model the desired signature as a constraint satisfaction problem (csp). a csp which models a network situation is composed by a set of variables, v, representing the network packets involved in the description of the network situation; the domain of the network packet variables, d; and a set of constraints, c, which relates the variables in order to describe the network situation. we call such a csp a network csp. on a network csp, each network packet variable is a tuple of integer variables, 19 variables for tcp/ip 1 packets and 12 variables for udp packets 2, representing the significant fields of a network packet necessary to model the intrusion signatures used in our experiments."
"where d i is an impulse vector at the i th position along slice direction, and psf i is the corresponding impulse response. the snr of the i th voxel was calculated as follows,"
"to constrain the reconstruction in ill-posed spatial locations while avoiding over-regularizing the well-posed regions, spatially varying regularization (k) is used. in particular, a tailored 3d k map is used to reconstruct the motion-corrupted dataset of each diffusion encoding. this map is tailored to the specific motions that occur during that timeframe, and reflects the spatially varying conditioning of the reconstruction. to capture the spatially varying conditioning, a high-resolution image with all voxel values set to one is transformed with the estimated motion parameters and encoded by rf encoding to obtain motion-corrupted low-resolution images. then, we take the difference map between the high-resolution image, and the corrupted image reconstructed from the low-resolution data by means of equation 3 with a very small amount of regularization ( figure 1c) . such difference map can be computed quickly and a scaled version of which is used to generate the desired 3d k map for each diffusion encoding. upper and lower thresholds are applied to the resulting k maps to constrain the reconstructions to be in a reasonable snr and point spread function tradeoff range according to the theoretical analysis that will be outlined below in the method section."
"before bulk motion estimation and correction, several preprocessing steps are needed: (a) corrupted data rejection: when large bulk motion occurs during the application of diffusion gradients, significant, unrecoverable signal loss can happen. such corrupted data are detected by comparing the image intensity of each acquired slab with a threshold, which is determined by the average value of image intensity of that slab across all diffusion directions. any slab with signal intensity lower than the threshold would be regarded as corrupted data and this slab would be rejected. (b) physiological motion correction: is then performed by removing the low frequency phase of each rf encoded data estimated from the center of the k-space. (c) subsequently, ec-induced image distortion is corrected. this distortion is diffusion encoding dependent and consists of in-plane shears, stretches and translations along the phase encoding direction. 45, 46 the conventional gslider-sms corrects for ec-induced distortion on the reconstructed high-resolution images, assuming that there is no difference of head position and ecinduced distortion between different slab-encoded volumes of the same diffusion direction. both assumptions could be problematic because subject movements can occur between the slab-encoded volumes during the same diffusion direction (in 20-25 s), and the different slab-encoded volumes can experience different ecs due to the leftover eddy currents from the previous diffusion encoding gradients, which is especially true when tr is short. to correct for the distortions across different rf-encoded volumes without jeopardizing the encoding information along the slice direction, we estimate the ec-induced field on the acquired rf-encoded volumes, and correct for the distortions before the motionaware reconstruction by applying the in-plane transformation in the observed space as follows 47 :"
"another challenge in mc-gslider is to solve an ill-posed reconstruction caused by compromised/reduced rf-encoding basis set. in particular, through-plane motion changes the rf-encoding along slice direction, making the actual encoding basis less orthogonal. moreover, the necessary data rejection of corrupted data with large signal loss can also lead to reduced encoding basis. therefore, double-basis acquisition and regularized reconstruction were used to improve the conditioning of the reconstruction without sacrificing snr efficiency. a motion-aware spatially adaptive regularization on gðxþ was developed and used to constrain the reconstruction in ill-posed spatial locations while avoiding over-regularizing the well-posed regions (figure 8 ). mc-gslider performed well for various ranges of motion as illustrated in figure 7, and the efficacy of reconstruction was not compromised in presence of larger motion range. this is because the 3d encoding a matrix consists of periodic rf encoding bases across different slabs, so for the motion larger than the slab thickness, it is actually the remainder of the motion divided by the slab thickness that is affecting the conditioning of the reconstruction."
"there are certain aspects that should be verified in order to maintain the security of the users data, as well as the quality and integrity of the services provided by a computer network. being able to describe these aspects, together with a verification that they are met can be considered as an network intrusion detection task."
"in the x86 architecture, the port-scan performs better in gecode than on adaptive-search, as for the syn flood-attack, the opposite is verified. this shows that in the x86 architecture gecode performs better on more complex, but when the complexity of problems decrease, adaptive search performs better."
"1. based on the network intrusion signatures 2. based on anomaly detection on network intrusion detection systems based on signatures, the network attacks are described using their signatures, particular properties of network packets used to achieve the desired intrusion or attack, which are then looked in the network traffic. intrusion detection systems based on anomaly detection, tries to understand the normal behavior of the systems by modeling its behavior using statistical methods and/or data mining approaches. the network behavior is then monitored, and if considered anomalous according the network model, the network is probably under some kind of attack. nemode uses use an approach based on signatures."
"in this work we describe how to model two distributed network attacks in nemode, a system for network intrusion detection based on constraint programming, allowing an intuitive an expressive way to describe such situation due to the possibility of the specification of relations between several network packets in an easy way."
"the catania mouse model (cmm) has been developed using unified modeling language (uml) http://www. uml.org/. uml is a diagramming language or notation to specify, visualize and document different types of models and object oriented software systems. uml helps in visualizing design and in communication. we used umbrello uml modeller http://uml.sourceforge. net, an open source tool that allows to manage and create uml based models. uml was selected because it is a widely-used system for the representation of objects and their relationships. moreover the umbrello tool was used to export the cmm classes into extensible markup language/resource description format (xml/rdf), in order to create the concepts of cmm-ontology. xml was developed by the w3c http://www.w3.org. the current standard for the xml schema language is controlled by the xml schema working group of the w3c. xml is a good candidate to share ontologies because of the significance of the web and web-based applications [cit] . it is clear that the web is rapidly becoming the primary method for the exchange of information and data, and that xml is currently the leading candidate for a generic language for the exchange of semi-structured objects. from the model to the cmm-ontology main concepts cmm-ontology concepts were generated from the model cmm. they provide a semantic standardization of the knowledge in the biological modeling field. they are used to identify the main biological entities used in the model as well as their interactions. we focus on two main types of concept: the concepts of identification and the concepts of interaction. these concepts bridge the gap between molecular component ontology and cellular component ontology. it is expected that they will allow scientists to easily identify the main biological entities they use, to model any given biological scenario."
"a distributed syn flood attack happens when several attackers, from several places, join forces to drive a distributed attack in order to initiate more tcp/ip connections than the server can handle and then ignoring the replies from the server, forcing the server to have a large number of half open connections in standby, which leads the service to stop when this number reach the limit of number of connections. this attack can be detected if a large number of connections is made from a single machine to other in a very short time interval. listing 3 shows how a syn flood attack can be described using nemode."
"all back-ends available in the system work in parallel, each one producing a solution to the problem. in a final step, the best solution produced is selected, which is simply the first solution to be produced. fig. 1 represents the architecture of the system and how the data flows between each component."
"two different tikhonov regularization terms were evaluated: (a) l 2 of image value x, and (b) l 2 of gðxþ, where g is the gradient operator. the impulse response was calculated by means of equation 4, using the reconstruction matrix in equation 6 for (a) and the following equation 7 for (b), respectively."
"one major challenge in incorporating motion correction into the gslider reconstruction was obtaining accurate motion parameters. the rf-encodings in gslider can introduce image contrast differences between different rf-encoded volumes, and sub-voxel movements along the slice direction can lead to contrast differences even between volumes that use the same rf-encoding, which further complicates motion estimation. the proposed iterative motion estimation addresses these problems by iteratively updating the reference b0 volumes to have the same motion-states and rf-encodings as the target diffusion-weighted volumes. its effectiveness was demonstrated in phantom simulations, where the mean estimation error was reduced from approximately 0.2 to 0.1 mm&degrees for motion scenario of 62 mm&degrees range ( figure 6a ). based on our analysis of the effect of motion estimation error on mc-gslider reconstruction ( figure 5 ), such reduction in motion estimation error will significantly reduce reconstruction error and enable better effective resolution."
"the experimental results described in sect. 4 shows that the performance varies in a great scale depending on the problem and the recognizer. table 1 shows that the port-scan takes longer to be recognized than the distributed syn flood. this behavior is explained by the complexity of the problems, which is directly related to the way the problems are modeled, since the port-scan is modeled by using more variables than the syn flood attack, also, the constraints used to model the port-scan are more complex than the ones used in the syn flood, making the port-scan much more complex than the syn flood attack."
"finally, by sweeping through the regularization parameter (k) values, a plot of psf errors versus 1/snr (similar to the l-curve) was also generated for each case."
"in all three motion cases, image reconstructed by the conventional gslider shows severe artifacts and blurring, resulting in large errors. most of these artifacts and errors are successfully removed by mc-gslider. additional simulation results for a range of simulated motion up to 16 mm&degrees can be found in figure 7, where mcgslider shows consistent improvement over the conventional gslider. in addition, as the motion range increases, the nrmses of mc-gslider is not increasing as much as the conventional gslider, indicating the efficacy of mcgslider in presence of large motion. figure 8 shows a comparison of mc-gslider performance when reconstruction was performed with and without spatially adaptive regularization, for a simulated data with 62 mm&degrees linear-motion profile across all six rigidmotion parameters. when a single small k value is used (figure 8, left), severe jagged-artifacts along the slice-direction (left-right) and signal dropout in the ill-posed regions are present. on the other hand, if a single large k is applied to improve the conditioning (figure 8, center), unnecessary blurring can occur in the well-posed regions, compromising the effective resolution. through adaptive k (figure 8, right), artifacts and signal dropout can be reduced in the ill-posed regions while spatial resolution is maintained in other regions. figure 9 and 10 show in vivo results from 860 lm data acquired with 44 diffusion encodings. figure 9a shows the estimated motion parameters at a temporal resolution of 2.1 s across the 34-min scan, where large in-plane and throughplane motions can be observed in the blue and red timeblocks, respectively. the top-row of figure 9b shows reconstruction results from the conventional gslider at these large motion time-points, where significant blurring is observed. the bottom-row shows reconstruction results from mcgslider, where the blurring is satisfactorily mitigated to allow for the recovery of detailed structures around the cortex as shown in the zoomed panels. figure 10 shows the color- double-basis encodings), reconstructed with mc-gslider using small k, large k, and spatially adaptive k. three orthogonal planes of the reconstructed images are shown along with the corresponding error maps (3 2). the ill-posed regions reconstructed using the small k and the well-posed regions with unnecessary blurring using the large k are indicated by the yellow and blue arrows, respectively. the k map for the adaptive regularization case is also shown on the far right coded fractional anisotropy (cfa) maps from: top-row) reference acquisition with negligible motion, center-row) motioncorrupted acquisition with the conventional gslider reconstruction, bottom-row) motion-corrupted acquisition with mc-gslider reconstruction. three orthogonal views, along with corresponding zoomed-in panels are shown."
"a port-scan is an important step before a network attack, scanning for available services on a given computer, allowing to discover the potential vulnerabilities of the victim, helping the attackers to better the attack. a distributed port-scan is made from several computers, geographically distributed, all scanning the services of a single computer."
"to validate the proposed motion-correction framework, phantom, and in vivo motion experiments were performed. data were acquired on the mgh-usc 3t connectom scanner with a maximum gradient strength of 300 mt/m and a maximum slew rate of 200 t/m/s, 49-51 using a custom-built 64-channel phased-array coil."
"in cmm, the interactions have been modeled using component diagrams. they show the components (either component technologies or sections of the system which are clearly distinguishable) and the artifacts they are made of, such as source code files, or relational database tables. components can have interfaces (i.e. abstract classes with operations) that allow associations between components."
"as for the syn flood in the cell/be a.s. version it presents a worst performance, which could be explained by signature used to model the problem, which is much simpler than the port-scan attack. also, the network packets that make part of the attack are much closer together in the network packet window, making the search of solution much simpler. do to these two facts, the problem gets much less complex, not being able to take full advantage of the cell/be architecture."
"biology is a knowledge-based discipline. many predictions and interpretations of biological data are made by comparing the data against existing knowledge. traditionally, the knowledge base in biology has resided within the heads of experienced scientists who have devoted much study and became experts in their particular domain. this approach worked well in the past, when considerable effort was needed to tease a few new data out of biological experiments. however, this situation is changing rapidly, and biology is moving fast toward the virtuous circle of other disciplines: from data to quantitative modeling and back to data. models are usually developed by mathematicians, physicists, and computer scientists to translate qualitative or semiquantitative biological knowledge into a quantitative approach [cit] ."
"using equations 4 and 5 we performed snr versus psf analysis for mc-gslider to (i) compare the performance of single versus double basis rf-encoding in the presence of motion, (ii) evaluate the effects of different regularizations used in a recon, and (iii) characterize the effects of motion estimation errors on the reconstruction. these analyses, which are outlined below, were performed for 5x gslider encoding (n rf 5 5), but can be easily adapted and applied to other gslider encoding paradigms."
"in this paper we presented a first attempt, generated from a model, at defining an integrated molecular -cellular ontology to be used in modeling biological problems. as the overall goal of this approach is to use a standardized approach to describe biological entities we plan to adapt in the future this methodology to the most widely used software tool in this field, i.e. protégé http://protege.stanford.edu). work in this direction is in progress and results will be published in due course."
"the consequence of the \"molecule_cell_interaction\" in these two instances, that results in the killing (by cdc or adcc) of the opsonized cancer cell or vaccine cell involves new instances of concepts defined above:"
"the development of ontologies for molecular and cellular biology information, and the sharing of those ontologies within the bioinformatics community, are central problems in bioinformatics. if the bioinformatics community is to share ontologies effectively, ontologies must be exchanged in a form that uses standardized syntax and semantics. for this reason, while the initial motivation of our study was to present an ontology for the cmm, the paradigm we show here has wider applications as it bridges the molecule ontology with cell ontology (figure 1 ). this is achieved by defining, at the same time, interactions in terms of cellular and molecular components of a biological system."
"an ontology describes basic concepts in a domain and defines relations among them. basic building blocks of ontology design include concepts and their instances; properties of each concept describing various features and attributes of the concept (slots, sometimes called roles or properties); restrictions on slots (facets, sometimes called role restrictions). an ontology provides a common vocabulary for researchers who need to share information in the domain and allows to build knowledge databases. ontologies are widely used in biology and medicine and several important ontology systems have been established. they contribute to a precise and exhaustive way to access bio-information and define concepts in a precise and rigorous way [cit] . interestingly, despite or because of the complexity of the immune response, imgt-ontology, the first ontology for immunogenetics and immunoinformatics, is also conceptually one of the more advanced biological ontologies [cit], on which has been built imgt®, the international immunogenetics information system® http:// www.imgt.org [cit] ."
"the domain of the network packet variables, d, are the values actually seen on the network traffic window, which is a set of tuples of 19 integer values (for the tcp variables) and 12 integer values (for the udp variables), each tuple representing a network packet actually observed on the traffic window and each integer value represents each field relevant to intrusion detection. the packets payload is stored separately in an array containing the payload of all packets seen on the traffic window."
"the nemode intrusion detection system is a declarative system that provides a domain specific language, following the constraint programming methodologies, enabling an easy and very descriptive way to describe the intrusion signatures that spread across several network packets by allowing to state of constraints over network entities and express relations across several network packets."
"this work shows that we can easily describe distributed network attacks or network situations, which spread across several network packets, using a declarative approach, and, from that single description, generate several constraint programming based network situation recognizers, using different constraint programming paradigms, which actually detect the desired intrusions, if they exist in the network traffic."
"by combining gslider encoding with sms, tr can be effectively shortened. furthermore, by combining gslider-sms with zoomed imaging using inferior saturation pulse and in-plane acceleration, the epi readout time can be shortened, thus reducing in-plane distortions without the need for multi-shot acquisition. this integrated approach provides high resolution data with high snr efficiency as well as the ability to achieve navigator-free physiological motion correction. however, because several rf slab-encoded volumes are combined to reconstruct each high-resolution diffusionweighted image, gslider-sms can still be sensitive to macroscopic subject motions across these slab-encoded volumes. for typical imaging at 600-800 lm with n rf 5 5 slabencoding volumes, each acquired with mb 5 2 and tr of 4-5 s, the motion sensitivity time frame is n rf 3tr 5 20-25 s."
"other important efforts are underway to link models at different scales by means of markup languages (i.e. xml). cellml project is one of this http://www.cellml. org. the cellml language is an open standard based on the xml markup language. cellml is being developed by the auckland bioengineering institute at the university of auckland and affiliated research groups. the purpose of cellml is to store and exchange computerbased mathematical models. cellml allows scientists to share models even if they are using different modeling tools. it also enables them to reuse components from one model in another, thus accelerating model development."
"network intrusion detection systems are very important and one of the first lines of defense against network attacks or other type of malicious access, which constantly monitors the network traffic looking for anomalies or undesirable communications in order to keep the network a safe place."
"with this work, we also proved that we can easily describe and detect signatures which spread across several network packets, something which is hard to achieve in systems like snort. although the intrusions mentioned in this work can be detected with other intrusion detection systems, they are modeled/described with out relating several network packets, usually being described by specifying a set properties over a single network packet, which could lead to a large number of false positives."
"attempts in that direction are, at the moment, based on the use of markup languages [cit], i.e. xml, but a general framework is still to come. in modeling other pathologies [cit] we experienced that using an ontology driven approach, itself generated from a model, resulted in speeding up the process of model construction as well as clarifying the biologist needs regarding model definion. we believe that this is a powerful methodology."
"while the performance of adaptive search cell/be version the x86 version can't be directly compared, it's possible to conclude that more complex problems takes advantage of the cell/be architecture and the performance doesn't degrades as much when the complexity increases."
"where r 2 low is the variance of the acquired slab data, and j is the index that counts through the different spatial positions along psf i vector and the second dimension of a recon ."
"the comprehensive motion correction in mc-gslider is possible due to the use of gslider rf-encoding in the slice direction, which allows unaliased, high-snr images to be reconstructed directly from each slab-encoded acquisition. in addition to enabling high-temporal, navigation-free motion estimation, this feature also uncouples the motion correction problem from the parallel imaging and k-space reconstruction, making the motion correction much more tractable. with each slab-encoded volume fully encoded in-plane, all in-plane motion corrections can be performed directly between the slab-encoded volumes without effecting the conditioning of the overall image reconstruction. furthermore, through-plane motion correction can also be incorporated directly into the gslider reconstruction, where the use of regularization and double-basis encoding have helped ensure that the reconstruction remain well-posed, even in the presence of large motions. the incorporation of motion correction into gslider's linear reconstruction also enables full characterization of snr versus resolution tradeoff for the motion-corrected reconstruction, as well as aiding in the development of motion-aware spatially varying regularization which further improves reconstruction performance."
"because of this reason, the motion range of 62 mm as investigated in the snr&psf analysis and phantom simulations is a representative ill-conditioned motion range at the slab thickness of 4.3 mm used in the human studies. future work will explore the incorporation of regularizations that also leverage joint information across diffusion encodings, such as the ones previously used to denoise and accelerate gslider acquisitions (through interlaced subsampling of the rf-encoding basis). [cit] this should further improve mc-gslider reconstruction and enable motion-robust reconstruction even in the case of single-basis encoding. data reacquisition 26, 62 may also be applied to further mitigate the ill-posed reconstruction problem. for patient populations that are prone to frequent/rapid motions, such as pediatric patients, intra-volume motions could degrade the image quality of mc-gslider. therefore, intra-volume motion correction was developed, where each rf-encoded volume was separated naturally into the two slab-interleaved groups, each with whole imaging volume coverage to achieve motion estimation every tr/2. a faster rate of motion estimation should be feasible through rearranging the slab acquisition order, to create slab groups that span across the whole imaging volume at a faster rate. however, there is a natural tradeoff between temporal resolution and accuracy of such intra-volume motion correction approach. the use of temporal smoothing/denoising across the estimated motion parameters could potentially mitigate this issue. note that rapid motion could also cause significant spin-history changes to create spatially varying signal modulations that are currently not accounted for in mc-gslider, which could also lead to reconstruction artifacts. a more sophisticated data rejection scheme to exclude large motion volumes and/or additional regularization/modeling could be explored to mitigate this issue."
"to eliminate semantic confusion between biology and other disciplines, it is necessary to have a list of the most important and frequently used concepts coherently defined so that involved people could use such a set of definitions to create new models and software, to provide an exact, semantic specification of the concepts used in an existing schema and to curate and annotate existing database entries consistently. we notice here that it is important to understand that semantic ambiguities also can arise between human experts. however, in the course of a conversation usually enough background knowledge and context is available so that semantic ambiguities are most often faster resolved than even consciously recognized. this is possible because of our intelligent capabilities which computers, programs and databases, at least for the near future, fall yet short of."
"in cmm-ontology, the concepts of interaction and their instances, generated from the model, are the following (figure 3 ): 1. the \"molecule_molecule_interaction\" concept. if the molecule is a soluble immunoglobulin (ig) specific for an antigen, and if the other molecule encountered is that antigen (ag), ig binds to ag and forms an immunocomplex (that can be captured by a macrophage). that instance of the \"molecule_molecule_interaction\" concept is"
"2. the \"cell_molecule_interaction\" concept. if the cell is a b lymphocyte, a macrophage or a dendritic cell, and if the molecule is an antigen, the cell can internalize the native antigen, process it and present it as peptide bound to mhc-ii (pmhc-ii) protein at the cell surface. the cell becomes a professional antigen presenting cell (or apc). three instances can therefore be defined:"
"detecting network intrusions with constraints consists on identifying a set of network packets in the network traffic, which identify and makes proof of the desired network signature attack, matching the desired network signature described through the use of constraints stated over a set of network packet variables, thus describing relations between several network packets."
"mc-gslider is an integrated approach developed to provide efficient acquisition and reconstruction for sub-millimeter isotropic diffusion imaging in presence of subject movements ranging from sub-millimeter to several centimeters. this self-navigated approach was demonstrated in vivo to provide motion estimation and correction at a high temporal rate of 2 s, to recover fine-detailed brain structures. the reconstruction framework has been designed to mitigate various sources of motion-related artifacts: starting with the commonly addressed background phase-corruption from physiologic and bulk motion, to the more demanding correction of the bulk in-plane and through-plane motions themselves. to our knowledge, this is the first technique that has been proposed to retrospectively correct for these 3d bulk motions in multi-slab diffusion acquisitions. in addition to these motions, mc-gslider also corrects for the subtler differences in ec-induced image distortions across slabencoding shots."
"a wireless local area network (wlan) offers connectivity and internet access for wireless enabled clients within its coverage area. for example, ieee 802.11 [cit] (commonly known as wi-fi) is widely deployed, and it is also used for localizing wi-fi enabled devices. the fingerprinting approach is often used in wi-fi positioning systems [cit] . it is based on received signal strength indication (rssi) measurements in the localization area. positioning systems using wi-fi are considered cost effective and practical solutions for indoor location tracking and estimation [cit] ."
"in mobile networks, such as gsm, many techniques are used to estimate client position. contrarily to the satellites that are continuously moving around the globe, base stations (bs) of the mobile networks have fixed geographical positions. in addition, each bs broadcasts its cell-id and location area identifier (lai) to the mobiles within its coverage area. therefore, each mobile can approximate its own position using the geographical coordinates of its serving base station in cell-id method [cit] ."
"hybrid positioning techniques are proposed to satisfy the increasing need for positioning accuracy. they make use of the collaboration between multiple radio access technologies existing in the same geographical area. for instance, assisted-gps uses information from the mobile network to improve positioning accuracy in gps. location-based services are based on the knowledge of user position. they include emergency, informational, tracking, entertainment and advertising services. lbs are useful for service providers and for the clients. accuracy improvements increase user satisfaction, and make the localization services more robust and efficient."
"ad hoc on-demand distance vector (aodv) routing [cit] adopts both a modified on-demand broadcast route discovery approach used in dsr [cit] and the concept of destination sequence number adopted from destination-sequenced distance-vector routing (dsdv) [cit] . when a source node wants to send a packet to some destination and does not have a valid route to that destination, it initiates a path discovery process and broadcasts a route request (rreq) message to its neighbors. the neighbors in turn forward the request to their neighbors until the rreq message reaches the destination or an intermediate node that has an up-to-date route to the destination. figure 3 illustrates the propagation of the broadcast rreqs in an ad hoc network."
"ad hoc routing protocols may generally be categorized into proactive and reactive according to their routing strategy [cit] . proactive protocols require that nodes in a wireless ad hoc network should keep track of routes to all possible destinations so that when a packet needs to be forwarded, the route is already known and can be used immediately. any changes in topology are propagated through the network, so that all nodes know of those changes in topology. examples include \"destinationsequenced distance-vector\" (dsdv) routing [cit], on-demand protocols only attempt to build routes when desired by the source node so that the network topology is detected as needed (on-demand). when a node wants to send packets to some destination but has no routes to the destination, it initiates a route discovery process within the network. once a route is established, it is maintained by a route maintenance procedure until the destination becomes inaccessible or until the route is no longer needed. examples include \"ad hoc on-demand distance vector routing\" (aodv) [cit], \"dynamic source routing\" (dsr) [cit] . proactive protocols have the advantage that new communications with arbitrary destinations experience minimal delay, but suffer the disadvantage of the additional control overhead to update routing information at all nodes. to cope with this shortcoming, reactive protocols adopt the inverse approach by finding a route to a destination only when needed. reactive protocols often consume much less bandwidth than proactive protocols [cit], but they will typically experience a long delay for discovering a route to a destination prior to the actual communication. however, because reactive routing protocols need to broadcast route requests, they may also generate excessive traffic if route discovery is required frequently."
"in this paper, we describe the main positioning techniques used in satellite networks like gps, in mobile networks, such as gsm, and in wireless local area networks such as wi-fi. the coexistence of several wireless radio access technologies in the same area allows the introduction of hybrid positioning systems, and promotes the diversification of position dependant services. we explain some of the hybrid localization techniques that coordinate information received from different radio access technologies in order to improve positioning accuracy. such improvements increase user satisfaction, and make lbs more robust and efficient. we also classify these services into several categories."
"the wide deployment of radio access technologies and the increasing development of wireless-enabled devices are promoting the extensive use of lbs. as mentioned in the previous sections, numerous positioning techniques are used to estimate client position. however, the main parameter for lbs efficiency is accuracy of the positioning technique. indeed, users are more satisfied when the estimated position is closer to the real geographical position, and when the probability of erroneous estimations is reduced."
"position dependant services are useful for mobile mapping, deformation monitoring and many civil engineering applications [cit] . they have revolutionized navigation (on land, in the air and at sea) and intelligent transportation system by increasing their safety and efficiency. however, user location privacy security poses a potentially grave threat [cit] . in fact, it is possible to access user location information anytime and anywhere. therefore, many privacy protection methods are introduced to deal with the contradiction between location privacy protection and quality of service in lbs. some of them protect user id information by hiding the true id when requesting the service. other methods do not submit the exact location to the server, but they send a region containing user exact position."
"numerous geolocation technologies are used to estimate client (person or object) geographical position. the large diversification of existing wireless radio access technologies (rat) and the increasing number of wirelessenabled devices are promoting the extensive application of location based services (lbs) [cit] . position-dependant services include: emergency services such as rescue response [cit] and security alerts, entertainment services like mobile gaming [cit], medical applications [cit] and a wide variety of other applications."
"once the distances between visible satellites and the gps receiver are measured, client position is estimated via the trilateration method, commonly known as triangulation [cit] . three distance measurements are required to perform position estimation. in fact, the estimated position is the intersection of three spheres having the satellites as centers and the calculated distances as radii."
"if there is space in the incremental update packet, then those entries whose sequence number has changed may be included. when the network is relatively stable, incremental updates are sent to avoid extra traffic and full dumps are relatively infrequent. in a fast-changing network, incremental packets can grow large so full dumps will be more frequent. each route update packet, in addition to the routing table information, also contains a unique sequence number assigned by the transmitter. the route labeled with the highest (i.e. most recent) sequence number is used. if two routes have the same sequence number then the route with the best metric (i.e. shortest route) is used. based on past history, the stations estimate the settling time of routes. the stations delay the transmission of a routing update by settling time so as to eliminate those updates that would occur if a better route were found very soon."
"in the first phase, rssi measurements are done for each point in the positioning map (under different network conditions). at the end of this phase, positioning database is created. it contains mean rssi values for every point in the grid [cit] . however, the second phase performs online rssi measurements for signals received from the neighboring access points. the positioning entity compares live rssi measurements with the values stored in the database. therefore, client position is estimated as the entry (x, y) in the database that best matches the actual measurements [cit] . accuracy of rssi-based positioning techniques in wi-fi networks depends on the number of access points involved in the localization problem."
"in this work, comparison of different routing protocols based on the energy is studied with simulations. networks of 20, 50, 75 and 100 nodes are generated, where nodes roam in an area of 1000 by 1000. network traffic is generated by cbr source, where the source and the destination of a session are chosen randomly. the duration of the simulation is 500 seconds. in the figure 7, we performed simulation using 20 nodes after adding energy model to the network. if the node contains more energy, the node will be in green color. if the node contains medium energy, it will be changed to yellow color. if the node contains no energy, it will be changed to red color."
"in wireless local area networks, such as wi-fi, rssi fingerprinting technique depends on the number of access points involved in the localization process. the validity of mean rssi measurements stored in the database also affects positioning accuracy. we exploit the coexistence of personal area networks (pan) along with wi-fi in the same geographical area to improve positioning accuracy. in fact, rssi measurements are done for all the existing wireless technologies, and results are stored in the positioning database. therefore, wi-fi fingerprinting uses additional rssi information from wireless pans existing in the same area such as bluetooth and zigbee networks."
"in aodv, route which has not been used for a period of expire time will be deleted. the set of expire time is important since short expire time may lead to the deletion of still valid route and long expire time will give more non-fresh routes. thirdly, it is about rrep. the destination of dsr could send rreq more than once for the same rreq without considering whether the same rreq had been replied just a second ago for other routes."
"wi-fi positioning techniques are similar to those used in mobile networks. however, the most common technique used to localize a client in wi-fi networks is based on rssi measurements. in the remainder of this section, we classify wi-fi positioning techniques into several categories, and we describe the basics of rssi-based localization methods."
"angle of arrival (aoa) measurements [cit] of several radio links between the base stations and the mobile are also used to estimate client position. hence, user position is approximated according to these angle measurements and using information about base stations geographical coordinates. time of arrival (toa) [cit] requires synchronization between the different network elements (i.e., base stations and mobile stations). the time difference between bursts sent by the mobile are converted into distance. hence, trilateration is used to estimate client position. other methods use received signal strength measurements to localize mobile stations. for example, received signal power is converted into distance via propagation models or empirical models. in addition, the fingerprinting method [cit] compares rss measurements with the values stored in a database for specific points in the localization map in order to approximate client position."
"time difference of arrival (tdoa) [cit] technique is inspired by toa. indeed, in toa, the positioning entity measures signal propagation time from the emitter to the receiver. time measurement is converted into distance that is used to estimate client position. however, tdoa technique requires the simultaneous transmission (for each base station) of two signals having different frequencies. these signals will reach the receiver at different times. therefore, the time difference is measured and converted into distance. once we have three distance measurements, trilateration is used to estimate client position. enhanced observed time difference (e-otd) [cit] requires synchronization between network entities (base stations and mobiles). each base station broadcasts messages within its coverage area. a mobile station compares the relative times of arrival of these messages to estimate its distance from each visible base station."
"toa [cit] and tdoa perform time measurements to calculate the distance between wi-fi client and access points. hence, three distance measurements are required to estimate user position via trilateration [cit] . such methods belong to the category of time-based positioning techniques, and they require time synchronization between network entities. in cell-id category, users scan the received radio beacons to estimate the closer access point. they use either predefined radio propagation models or experimental fingerprinting data to estimate user position."
"olsr is designed to work in a completely distributed manner and does not depend on any central entity. the protocol does not require reliable transmission of control messages: each node sends control messages periodically, and can therefore sustain a reasonable loss of some such messages. also, olsr does not require sequenced delivery of messages. each control message contains a sequence number which is incremented for each message. thus the recipient of a control message can, if required, easily identify which information is more recent -even if messages have been re-ordered while in transmission."
"location-based services are related to the position of the user making the request. they are classified [cit] as emergency services (e.g. security alerts, public safety and query of the nearest hospital), informational services (i.e., news, sports, stocks and query of the nearest hotel or cinema), tracking services (like asset/fleet/logistic monitoring or person tracking), entertainment services (for example: locating a friend and gaming) and advertising services (such as announcements or invitation messages broadcasted by the shops to the nearby mobile clients). moreover, future applications of lbs include support to the studies on climate change, seismology and oceanography."
"when the destination node receives the route request, it appends its address to the route record and returns it to the source node within a new route reply message. if the destination already has a route to the source, it can use that route to send the reply; otherwise, it can use the route in the route request message to send the reply. the first case is for situations where a network might be using unidirectional links and so it might not be possible to send the reply using the same route taken by the route request message. if symmetric links are not supported, the destination node may initiate its own route discovery message to the source node and piggyback the route reply on the new route request message. figure 6 shows the transmission of route record back to the source node. route maintenance uses route error messages and acknowledgement messages."
"in this work, we have compared performance of routing protocols based on the energy level and proposed method is useful to save the energy in mobile ad hoc network. here, nodes which are having less energy will not forward the rreq packet. by using this method, we can decrease the energy consumption."
"in the future, we can reduce the energy consumption by using the same energy efficient path instead of finding the route again. the future scope of this paper is to implement the same concept in other protocols also."
"in aodv, each node maintains its own sequence number and a broadcast id. each rreq message contains the sequence numbers of the source and destination nodes and is uniquely identified by the source node's address and a broadcast id. aodv utilizes destination sequence numbers to ensure loopfree routing and use of up-to-date route information. intermediate nodes can reply to the rreq message only if they have a route to the destination whose destination sequence number is greater or equal to that contained in the rreq message. so that a reverse path can be set up, each intermediate node records the address of the neighbor from which it received the first copy of the rreq message, and additional copies of the same rreq message are discarded. once the rreq message reaches the destination (or an intermediate node with a fresh route) the destination (or the intermediate node) responds by sending a route reply (rrep) packet back to the neighbor from which it first received the rreq message. as the rrep message is routed back along the reverse path, nodes along this path set up forward path entries in their routing tables (figure 4) ."
"the advantage of having more than one route entry per destination is that it can provide backup routes when there is a break on the current route. it means that there is no need to send a rreq again to search for the route. on the other side, if the network has high mobility, the freshness of the backup route could be a problem and reinitiate a new rreq should be a better choice. with the above analysis, it is found that aodv protocol should perform better in high mobility ad hoc network."
"in the figure 11, we are comparing total remaining energy of basic routing protocols with modified routing protocols. total remaining energy in aodv is 0.17% when the numbers of nodes are 20. after implementing the proposed method remaining energy in modified aodv is 2.13%. the energy consumption is reduced after implementing the proposed method. in dsr routing protocol, total remaining energy is 0.12% when the numbers of nodes are 20. in modified dsr, remaining energy is 0.14%. so, after implementing method energy consumption is decreased. the figure 12 shows the comparison of aodv, dsr with modified protocols based on energy."
"toa technique is very frequently used for positioning in mobile networks. however, it requires synchronization between mobiles and base stations. in addition, toa is restricted by multipath and non-line-of-sight propagation problems. hence, this positioning technique is assisted by additional information from aoa technique. angle measurements are performed by the serving base station using antenna arrays. the usage of toa assisted aoa technique improves positioning accuracy, especially in bad propagation environments [cit] ."
"in addition, multiple route entry could be store in the route table for the same destination in the same source node. that is, a source node has multiple route entries for one destination. while, in aodv, only one route reply can be sent by the destination and only one route entry per destination is stored in the route table."
"there are a few differences between dsr and aodv routing protocols [cit] . firstly, compared with dsr, the source node of aodv only knows the route to the destination, but in dsr, source node knows the route to intermediate node also. on the other hand, because in dsr, each data packet has to take the whole route information in the header, it costs large overhead which will waste data rate. [cit] another difference between dsr and aodv is the usage of timer. in the dsr protocol, there is no timer used for the validation of routes. stale routes could be used for routing. in aodv, timer is used for the freshness of a route. in dsr, with stale route, it is possible that the route is not validated. it will cause the loss the packets before source node is notified that the route is invalid. on the other hand, if the route is still valid, route overhead is saved for route discovery process."
"if a node detects a link failure when forwarding data packets, it creates a route error message and sends it to the source of the data packets. the route error message contains the address of the node that generates the error and the next hop that is unreachable. when the source node receives the route error message, it removes all routes from its route cache that have the address of the node in error. it may initiate a route discovery for a new route if needed. in addition to route error message, acknowledgements are used to verify the correct operation of links. to reduce the route search overhead, an important optimization is allowing an intermediate node to send a route reply to the source node if it already has an up to date route to the destination."
"in this paper, we surveyed the major positioning techniques used in wireless networks. global positioning system is the most common technology for outdoor locating services. localization techniques in mobile networks, such as global system for mobile communications, perform time, angle or signal power measurements to estimate user position. in wi-fi networks, received signal strength indication fingerprinting approximates user position within the coverage area. accuracy depends on the number of access points involved in the positioning process."
"when a node detects a link failure or a change in neighborhood, a route maintenance procedure is invoked: if a source node moves, it can restart the route discovery procedure to find a new route to the destination. if a node along the route moves so that it is no longer contactable, its upstream neighbor sends a link failure notification message to each of its active upstream neighbors. these nodes in turn forward the link failure notification to their upstream neighbors until the link failure notification reaches the source node."
"aoa method uses directional antennas to measure the angle of arrival of signals transmitted by the clients. hence, client position is estimated via the geometry of triangles in angle-based positioning techniques. however, the most common positioning techniques in wi-fi networks are based on rssi measurements [cit] . some of them are based on propagation models [cit] to translate signal power into distance. other methods use empirical models, and they store rssi measurements in a positioning database. therefore, localization methods in wi-fi are classified into four main categories: cell-id, time, rssi and angle. fig. 3 illustrates the classification of wi-fi positioning techniques. received signal strength indication measurements are quantified levels that reflect the real power of a received wireless signal. when propagating in free space, the transmitted radio frequency signal is subject to degradation due to attenuation, reflection, diffraction and scattering. indeed, several propagation models formulate signal strength degradation as a function of the traveled distance and the transmission frequency. for instance, hata-okumura [cit] model approximates path loss (pl) according to the distance between emitter and receiver, antenna characteristics and transmission frequency. hence, rssi measurements are compared with theoretical values of the received power (calculated using propagation models) in order to find the distance traveled by the signal. three distance measurements are required to estimate wi-fi-enabled client position via trilateration. in fact, it is the intersection of three circles having the access points as centers and the calculated distances as radii."
"the wide usage of position dependant services increases the need for more accurate position estimation techniques. due to the limitations of positioning methods that use data from one single rat, hybrid positioning techniques are proposed to increase accuracy. they make use of the collaboration between different wireless access networks existing in the same geographical area, such as gps and gsm, to exchange additional position related data."
"in wireless networks, the knowledge of user geographical position allows the introduction of numerous position dependant applications. these applications are known as location-based services, and they are useful for service providers as well as for mobile clients."
"optimized link state routing (olsr) is a routing protocol used for mobile ad-hoc networks (manet). it is a best-effort proactive protocol. proactive protocols are characterized by all nodes maintaining routes to all destinations at all times through the periodic exchange of protocol messages. this gives them the advantage of having pre-computed routes available when needed and to propagate topology changes in bulk updates to many nodes. olsr performs hop-by-hop routing, where each node uses its most recent topology information for routing. olsr is highly focused on reducing the protocol overhead. as a result, information about qos-related state is not propagated throughout the network. but with the rising popularity of multimedia applications and the potential commercial usage of manets, qos support in ad-hoc networks has become a very critical issue and a range of qos signalling and routing protocols have been proposed."
gps consists of a network of 24 satellites in six different 12-hour orbital paths spaced so that at least five are in view from every point on the globe [cit] . satellites serve as reference points when estimating client position. they continuously broadcast signals containing information about their own position and direction. distance between satellite and receiver is determined by precisely measuring the time it takes a signal to travel from the satellite to the receivers antenna.
the rest of the paper is organized as follows: in (ii) we explain the principles behind positioning techniques used in satellite and mobile networks. wi-fi localization methods are reported in (iii). we describe hybrid positioning systems in (iv). section (v) contains a classification of lbs. concluding remarks are given in section (vi).
"localization techniques in mobile networks can be classified in two categories: network-based and clientbased. in network-based positioning techniques, the network collects necessary information to estimate client position. time, angle or distance measurements performed by the base stations are usually forwarded to a positioning server deployed in the network. the required information when estimating user position is stored in the positioning database. thus, positioning server has information about the positions of all the users in the system. however, client-based localization techniques are characterized by the absence of a centralized positioning entity. in fact, each client performs time, angle, power or distance measurements locally. thus, it approximates its own position using local measurements and information broadcasted from the base stations. fig. 1 shows a qualitative comparison of the positioning techniques described in this section. performance criteria used for localization techniques comparison are accuracy and coverage. a positioning technique is better when it has lower accuracy error (distance between the estimated position and the real geographical position) and greater coverage. traditionally, a wi-fi network provides internet access to wireless-enabled clients located within its coverage area. in addition, it allows interconnectivity between wireless devices existing in the same network. recently, wi-fi networks are having additional applications. for example, we can benefit of the coexistence of several radio access technologies in the same geographical area. heterogeneous networks offer the possibility to steer user sessions preferentially to a given radio access technology, such as wi-fi or universal mobile terrestrial radio access system (umts), according to service type [cit] and network load. moreover, the wide deployment of wi-fi networks allows the introduction of numerous location-based services."
"reactive protocols take a lazy approach to routing. in contrast to proactive routing protocols, all up-to-date routes are not maintained at every node, but instead the routes are created as and when required. when a source wants to send to a destination, it invokes the route discovery mechanisms to find the path to the destination. in this section several typical reactive (on-demand) routing protocols [cit] are introduced."
"other positioning techniques based on rssi measurements use empirical models to estimate user position. instead of approximating the distance between wi-fi clients and access points, the localization area is divided into smaller parts using a grid. each point of the grid receives several wi-fi signals from the neighboring access points. rssi measurements are performed under different conditions (e.g. time, interference, network load) in order to increase positioning accuracy. if n is the number of wifi access points, an n-tuple (rssi 1, rssi 2, ..., rssi n ) containing mean rssi values is created for each point (x, y) in the map. such positioning technique is called rssi fingerprinting, and it occurs in two phases: an offline fingerprinting phase and an online positioning phase."
input: t : the r-tree index; p: the number of buckets output: b: the constructed buckets 1 begin 2 find the first level in t with more than p nodes;
"proof. see appendix b, available in the online supplemental material. in order to find an effective transformation, we assign tokens into different groups by considering the frequency of each token, which is the total number of its appearance in all records in the dataset. it is obvious that tokens with similar frequency should not be put into the same group. the reason is that for two records x and y, such tokens will be treated as same ones and the value of minðv i ½x; v i ½y þ will not increase. as the sum of all token frequencies is constant, we should make the total frequency of tokens in each group nearly the same to make a larger value of equation (2) . t u"
"in each step, we can select the buckets to merge based on the idea of gradient descent: we try to merge each bucket with its neighbors and adopt the selection which can minimize above objective function. this method will run n à p steps. for each step, there will be oðn 2 þ trials of merge operation. thus the time complexity of this strategy is oðn 3 þ."
"moreover, as in many situations it is not required to return the exact knn results, we also propose an approximate knn search algorithm which is much faster than the exact algorithm and with high recall at the same time. to reach this goal, we devise an iterative estimator to model the data distribution. we evaluate our proposed methods using four widely used datasets, on both memory and disk based settings. experimental results show that our framework significantly outperforms state-of-the-art methods."
"the typical query supported by the r-tree is range query: given a hyper-rectangle, it retrieves all data entries that overlap with the region of this query. but it can also efficiently support knn search. we will talk about it later in section 5."
"the index construction time is shown in fig. 13 . actually the index construction time of both transformation and multitree is significantly less than flamingo. this is because these two methods do not need to build inverted lists. meanwhile, transformation has comparable index construction time with multitree. for our method, the main cost of this process is to map the original records into representative vectors and generate dual transformation."
"existing approaches employed the filter-and-verify framework for set similarity search and join. they generated signatures from original records and organized them into inverted lists. as such signatures can be used to deduce a bound of similarity, they make use of these signatures to filter out dissimilar records. however, scanning the inverted lists can be an expensive operation since there are many redundant information in the inverted lists. for a record with l tokens, it will appear in l inverted lists. and for a given query q, the filter cost will be dominated by the average length of all records in the collection, which will result in poor scalability. as set similarity metrics are relatively light weighted, the filter cost could even counteract the benefits of filtering out dissimilar records."
"next we report the results about indexing. here we focus on two issues: index size and index construction time. the index sizes of different methods are shown in table 5 . the index size of our method is significantly smaller than that of flamingo. this is because we just store the data in leaf nodes of r-tree and do not build inverted lists. in this way, the index size only relies on the cardinality of dataset rather than the length of records. among all methods, multitree has the smallest index size. the reason is that multitree maps each record into a numerical value and constructs a b þ -tree like index on them. however, there might be significant loss of useful information in this process. compared with multitree, the index size of our method is only slightly larger but our method achieves much better search performance."
"next we compare our dualtrans with state-of-the-art methods. for each dataset, we randomly select 10,000 records from it as query and report the average query time. we also also show some min-max error bars to assess the stability of results. for all baseline methods, we try our best to tune the parameters according to the descriptions in previous studies and report their best results. here transformation is the dualtrans method in fig. 5, which has the best performance."
"complexity. next we analyze the complexity of algorithm 1. we first need to traverse the set record with average length \" l in u, sort s, and then traverse each token in s. during traversing each token, we need to find the group with the minimum total frequency which costs log ðmþ using priority queue. thus the total time complexity is oðjuj á \" l þ jsj á ðlog jsj þ log mþþ."
"in this section, we propose a transformation based framework to support knn set similarity search. we first transform all set records into representative vectors with fixed length which can capture their key characteristics related to set similarity. then we can deduce an upper bound of set similarity between two records by leveraging the distance between them after transformation. as the length of representative vectors is much smaller than that of original set records, calculating such distance is a rather light-weighted operation. we first introduce the transformation based framework in section 4.1. we then prove that finding the optimal transformation is np-hard and propose an efficient greedy algorithm to generate the transformation in section 4.2. finally we introduce how to organize the records into existing r-tree index in section 4.3."
"after constructing the index, we could accelerate the search progress in the same way just as what is mentioned before by utilizing algorithm 2. the only difference is that we need to replace the transformation distance and query-node minimum transformation distance with the new distance as is shown in definition 4. based on lemma 1, we could deduce that the multiple-transformation distance is also an lower bound of jaccard distance as is demonstrated in lemma 3."
"where l is the operation of concatenation. and we call u v the multiple transformation operation. therefore, with the help of joint representative vector, we could apply multiple transformations pruning techniques."
"we also show the performance of approx disk-based settings. besides its good efficiency, the recall rate is also promising as is shown in table 4 . furthermore, approx can save a great number of i/o cost as it only needs to load the buckets with overlapping one by one and does not need to traverse the r-tree index."
"next we talk about how to generate the buckets. as shown in theorem 3, finding the optimal partition of buckets is np-hard. one way to make an approximation is to adopt the idea similar to clustering algorithms such as hierarchical clustering. the basic idea is to regard each record as a bucket, i.e., starting with n buckets, and in each step we merge two buckets into one until we obtain p buckets (see fig. 4 ). here the objective function is:"
"many statistical measures used in education are experimental research designs that require strict scientific methodologies that cannot be implemented in educational institutions without violating legal policies or severely disturbing the learning environment and associated instructional climate and vital to instruction. the time has come for education to provide its own scientific field and subsequent measures based on its own rigor and grounded in the foundation of longstanding educational research, fundamental educational theory, and innovations in qualitative, quantitative, and mixed methods research designs native to the specifics of pedagogy and andragogy. this paper provides a definition for the establishment of the field of eduscience and a comprehensive statistical test for that field that is specifically designed for use in education [cit] ."
"we can see that it can be very expensive to apply above method on the data collection since the value of n can be very large. the reason is that we need to construct the buckets from scratch without any prior knowledge. recall that we have already built the r-tree index which tries to minimize the overlap between the nodes. therefore, the mbrs of r-tree can be a good starting point for constructing the buckets since we need to minimize the total perimeter of the buckets. based on this observation, we then propose an iterative approach to construct the buckets by leveraging the existing r-tree index."
"definition 2 (transformation distance). given two set records x, y and a specified transformation v, we define the transformation distance transdistðv; x; y þ between them w.r.t representative vectors, which is:"
"to address above issues, we propose a transformation based framework to efficiently support knn set similarity search. motivated by the work of word embedding [cit] in the area of natural language processing, we transform all set records with variant lengths to representative vectors with fixed length. by carefully devising the transformation, we can guarantee that the representative vectors of similar records will be close to each other. we first provide a metric to evaluate the quality of transformations. as achieving optimal transformation is np-hard, we devise a greedy algorithm to generate high quality transformation with low processing time. next we use a r-tree to index all the representative vectors. due to the properties of r-tree, our work can efficiently support both memory and disk based settings. then we propose an efficient knn search algorithm by leveraging the property of r-tree to prune dissimilar records in batch. we further propose a dual-transformation based algorithm to capture more information from the original set records so as to achieve better pruning power."
"we first evaluate the effectiveness of our transformation techniques. to this end, we propose three methods: randomtrans is the method that randomly groups the tokens into m groups to generate the transformation. singletran is the method that uses one transformation generated by the greedy grouping mechanism. dualtrans is the dualtransformation based method."
"the process of designing instruments for the purposes of assessment and evaluation is called \"psychometrics\". psychometrics is broadly defined as the science of psychological assessment [cit] . the tri-squared test pioneered by the author, factors into the research design a unique event-based \"inventive investigative instrument\". this is the core of the trichotomous-squared test. the entire procedure is grounded in the qualitative outcomes that are inputted as trichotomous categorical variables based on the inventive investigative instrument. the specific assessment of the vari-ables is completely dependent upon the outcomes determined by the researcher's instrument. the creation, production, and deployment of the trichotomous inventive investigative instru-ment requires that the research investigator adopt the role of a \"trichotomous psychometrician\". a \"trichotomous psycho-metrician\" is an educational scientist that uses trichotomous-based psychometrics to develop a qualitative inventive investi-gative instrument specifically designed capture qualitative re-sponses during a specific event. a description of the entire tri-squared research process follows and is described in detail to provide the reader of the precise steps undertaken in the process of developing, designing, and ultimately implementing an in-ventive investigative instrument [cit] ."
"we also show the result of the approximate knn algorithm approx in fig. 8 . the performance of approx significantly outperforms minhash. the reason is that approx only needs to visit the bucket and can avoid traversing the index. besides, the search space is also much smaller as we only need to perform incremental search on the buckets. the reason is that the hash index of minhash distributes messier than that of approx, and minhash needs to access more nodes than approx to get the same candidate records. at the same time, we report the recall of approx (app) and minhash (mh) for each value of k in table 4 . it ic computed by dividing the number of correct top-k records by k. as the top-k results are not unique, we will treat any result with correct similarity as valid. we can see that except for the efficiency of approx, the overall recall rate is also better. for example, on dataset live-journal the average recall rate of approx is 0.742 while that of minhash is 0.464. the main reason is that minhash specifies the order of the dimensions. however, this order could cause some false negative. for instance, if the min-hash results of record are same as that of query records but different from the first index, though they tend to be similar in high possibility, the record will be accessed later than other records with the same first index which might be dissimilar."
"create the first transformation g: the total time is oðjuj á \" l þ jsj á ðlog jsj þ log mþþ. split each group with around jsj m tokens in g: it needs to call the function greedy grouping mechanism m times. the total time is oðjsj þ jsj á log mþ. traverse k and assign tokens with minimum total frequency to h min . the total time is oðm 2 log m þ jsjþ."
we first get a global dictionary of tokens and sort all tokens by frequency as is shown on top of fig. 1 . the way to group all tokens is according to algorithm 1. the final result of grouping are shown at the bottom of fig. 1 .
"we use four real world datasets to evaluate our proposed techniques: 1) dblp 1 : a collection of titles and authors from dblp computer science bibliography. the goal of this collection is to provide real bibliography that is based on real scenarios. it could be used for query reformulation or other types of search research. we tokenized the records in this dataset following previous study, that is we split all records based on non-alphanumeric characters. 2) pubmed 2 : a dataset of basic information of biomedical literature from pubmed in xml format. we select the abstract part of the datasets and divide the abstracts into tokens based on spaces. 3) livejournal 3 : the dataset contains a list of user group memberships. each line contains a user identifier followed by a group identifier (separated by a tab), implying that the user is a member of the group. 4) kosarak 4 : the dataset is provided by ferenc bodon and contains anonymous click-stream data of a hungarian on-line news portal. the detailed statistical information is shown in table 3 . we evaluate our disk-based algorithms on the pubmed dataset, which is much larger than the other datasets. and the other three datasets are used to evaluate the in-memory algorithms. we implemented our methods with c++ and compiled using gcc 4.9.4 with -o3 flag. we obtain the source codes of all baseline methods from the authors which are also implemented with c++. all the experiments were run on a ubuntu server machine with 2.40 ghz intel(r) xeon e52653 cpu with 16 cores and 32 gb memory."
"xg, which is the number of tokens in x that belong to group i. then we can define the transformation distance between two records by looking at the representative vectors. we claim that it serves as an upper bound of the jaccard similarity as is shown in lemma 1."
next we will first have a brief introduction about the properties of the r-tree index. the r-tree is a height balanced index structure for multi-dimensional data [cit] . each node in r-tree is corresponding to a minimum bounding rectangle (mbr) which denotes the area covered by it. there are two kinds of nodes: an internal node consists of entries pointing to child nodes in the next level of tree; while a leaf node consists of data entries.
"an instrument was developed and deployed as a pilot study to collect data that was analyzed as a foundation on determining the dimensions and strategies that would constitute a larger sample. the larger sample will be used to collect data that would lead to testing for reliability and validity of the instrument that would lead to development of the comprehensive conceptual framework for faculty as academic leaders. after approval of the university irb the initial survey design was cross-sectional analysis instrument designed to study faculty shared governance and academic freedom in terms of faculty collegiality from the university of california. the pilot study survey was redesigned after collaborative discussion and input from the hbcu research, evaluation, and planning office (who also disseminated the survey electronically) out of this the pilot study likert scale instrument was developed and deployed via cover letter to academic institutions and professional organizations. the research instrument (delivered online) was as follows:"
"here we define the size of a node as that of a disk page. then we can obtain the fan out of the index, which is the number of entries that fit in a node. the value of fan out can be set between the maximum and some value that is no larger than the ceiling of maximum divided by 2. the actual fan out of the root is at least 2. therefore, our framework can naturally support both in-memory and disk settings. and the r-tree also supports insert and update operations. similar to b-tree, such operations eliminate nodes with underflow and reinsert their entries, and also split overflow nodes. fig. 2 shows an example of r-tree that indexes the vectors in table 2 ."
"the next problem becomes how to propose an effective transformation. previous approaches use one token as the unit for filtering and build inverted list for each token. then there will be jsj inverted lists, where s is the global dictionary of all tokens in the collection. the basic idea is to regard each token as a signature for deciding the similarity. then a straight forward way is to represent each record as a jsj-dimension vector. however, obviously this is not a good choice due to the large value of jsj. to reduce the size, we can divide all tokens into m groups. and we use v i ½x to denote the ith dimension of record x. and the cardinality of v i ½x is correspondingly the value of the ith dimension of the representative vector."
we propose a transformation based framework to support the problem of knn set similarity search for practical instances. we devise an effective greedy algorithm to transform set records to fixed length vectors and index them with a r-tree structure. we propose an efficient knn search algorithm by leveraging the properties of r-tree. we further devise a dual-representation strategy to enhance the filtering power.
"with above techniques, each set record is represented with an m-dimension vector. then we index them with an r-tree index. for this application scenario, we do not to worry about the influence of curse-of-dimensionality. the reason is that for our problem, as we treat all the set records as bags of tokens, even if every record is modeled as an d-dimensional data where d is the size of global dictionary of tokens, we only need to look at the non-zero dimensions when calculating jaccard similarity. therefore, the data sparsity problem which occurs in many complex data objects (e.g., sequence, image, video) mentioned in the previous paper, will not seriously harm the performance of jaccard based similarity search."
"in this paper, we use jaccard as the metric to evaluate the similarity between two set records. given two records x and y, the jaccard similarity between them is defined as"
"where aðu 0 þ is the area for mbr of the remaining records covered by nodes which do not belong to any bucket so far. the process of iterative bucket construction is shown in algorithm 4 (a running example is shown in appendix h, available in the online supplemental material). we first locate at the first level of t with more than p nodes (line 2). in step i, we initialize the bucket b i with the node n l having left-most mbr (line 4). next, we try to add remaining nodes one by one from left to right into b i and look at the value of equation (11) . that is, we calculate the values of çðb i þ by adding each remaining node to b i . for each time, we add the node that leads to the smallest çðb i þ value to b i, and repeat. if no node addition reduces the value of çðb i þ, the step i finishes and the current bucket becomes b i in the results (line 8). when there are p à à1 buckets constructed, we group all remaining nodes into the last bucket and stop the algorithm (line 9)."
"actually, we would get two dissimilar transformations from algorithm 3 (a running example is shown in appendix h, available in the online supplemental material). moreover, based on lemma 4, the filtering power of our dualtransformation is stronger than that of single transformation."
"the term \"trichotomy\" is defined in trichotomy-squared in the following manner: \"trichotomy\": is pronounced [\"trahykot-uhmee\"], spelled \"trichotomy\", and is a noun with the plural written form \"trichotomies\". \"trichotomy\" has the following threefold definition: 1) separation or division into three distinct parts, kinds, groups, units, etc.; 2) subdivision or classification of some whole into equal sections of three or \"trifold segmentation\"; and 3) categorization or division into three mutually exclusive, opposed, or contradictory groups, for example-\"a trichotomy between thought, emotions, and action\" [cit] ."
"in this paper, we study the problem of knn set similarity search. we propose a transformation based framework to transform set records to fixed length vectors so as to map similar records closer to each other. we then index the representative vectors using r-tree index and devise efficient search algorithms. we also propose approximate algorithm to accelerate the knn search process. experimental results show that our exact algorithm significantly outperforms state-of-the-art methods on both memory and disk settings. and our approximate algorithm is more efficient and with high recall rate at the same time. for the future work, we plan to extend our techniques to support knn problem in other field, such as spatial and temporal data management. besides, we would also like to conduct more probabilistic analysis on the basis of our transformation based techniques."
"s et similarity search is a fundamental operation in a variety of applications, such as data cleaning [cit], data integration [cit], web search [cit], near duplicate detection [cit] and bioinformatics etc. there is a long stream of research on the problem of set similarity search. given a collection of set records, a query and a similarity function, the algorithm will return all the set records that are similarity with the query. there are many metrics to measure the similarity between two sets, such as overlap, jaccard, cosine and dice. in this paper we use the widely applied jaccard to quantify the similarity between two sets, but our proposed techniques can be easily extended to other set-based similarity functions. previous approaches require users to specify a threshold of similarity. however, in many scenarios it is rather difficult to specify such a threshold. for example, if a user types in the keywords \"new york, restaurant, steak\" in a search engine, he or she may intend to find a restaurant that serves steak. usually, the users will pay more attention for the results which rank in the front, say the top five ones. in this case, if we use thresholdbased search instead of knn similarity search, it is difficult to find the results that are more attractive for users."
"to address this problem, we propose a transformation based framework that eliminates redundancy in the index structure. moreover, its performance is independent from the length of records. the basic idea is that for each record x 2 u, we transform it into a representative vector v½x with fixed length m. we guarantee such a transformation can reflect necessary information regarding the similarity. then we can deduce a bound of set similarity from the distance between representative vectors."
"distance). given a record q and a node n, the minimum transformation distance between v½q and n, denoted as mindistðv; q; nþ, is the distance between the vector and the nearest plane of hyper rectangle of b n ."
"we have an important observation on the nodes in an r-tree index regarding the transformation distance. given a representative vector v½q and a node n in the r-tree, we can deduce a minimum transformation distance between v½q and all records in the subtree rooted by n. this can be realized using the properties of mbr of node n, which covers all the records in the subtree. then if the minimum transformation distance between v½q and the mbr of n is larger than ub r, we can prune the records in the subtree rooted by n in batch."
"to date faculty at all levels are struggling to face the dynamic challenges brought on by education in the 21st century. the challenges and social change demand a reconceptualization of education process to emphasize entrepreneurship and leadership throughout the academy. the new conceptualization of teaching and learning innovation requires a reflection on how data is processed and implemented. currently, institutions are required to demonstrate learner knowledge, skills, and dispositions as a part of institutional assessment. thus, the reconceptualization process needs a coherent statistical framework that serves as a guide towards more effective evaluation of teaching that can be used locally, regionally, nationally, and internationally. the aim of this paper is to explore strategies and concepts towards establish a development of an ideal statistical framework that would lead to increased awareness, motivation and empowerment to conduct research on their teaching by faculty at all levels of education."
"although multiple-transformation distance can improve filter power, the overall performance could deteriorate with a large number of transformations. the overhead comes from several aspects: first, we need to store joint representative vector into the index. a larger number of transformations will result in extra space overhead. second, as specified in equation (8), we need to calculate the query-node minimum transformation distance multiple times. third, it is difficult to construct multiple transformations with high quality. if two transformations are very similar, the lower bound deduced from them will also be very close. in this case, there will not be improvement of filter power. based on these considerations, we propose a dualtransformation framework which utilizes only two transformations. then the problem becomes how to construct them. regarding the third concern above, it is better to construct two dissimilar transformation. here the definition of \"similar\" is: for two similar transformations, records mapped into the same group under one transformation are in a great possibility to be mapped into the same group under another. then we proposed a metric to measure the similarity between two transformations."
"the following survey is designed to assess faculty attitudes and perceptions related to shared governance and leadership in higher institutions. the results will be used to develop means of improving institutional and faculty professional development. your participation is voluntary and all answers are anonymous. if you choose to participate in the survey, you may withdraw your consent at any time. please place and (x) in the box or space that best represents your response to the stated question. if you have any questions about the survey or any specific questions, please contact: dr. masila mutisya (919-530-7689)."
please place and (x) in the box that best represents your response to the statement. your participation is helping to improve shared governance for your institution. thank you the summative comprehensive tri-squared formula probability distribution.
"for approximate knn set similarity search, we extend min-hash algorithm [cit] to do approximate knn set search, denoted as minhash as the baseline method of our approx algorithm. in minhash, we first utilize min-hash algorithm to transform the original set records into hashed vectors. then we utilize the similar approach proposed in approx to incrementally search for the approximate knn result."
"based on above observation, we then propose a greedy group mechanism considering the total token frequency f i of each group g i . the detailed process is shown in algorithm 1. we first traverse all the records in u and obtain the global dictionary of tokens; then we sort all tokens in the descent order of token frequency (line 2). the total frequency of each group is initialized as 0 (line 3). next for each token in the global dictionary, we assign it to the group with minimum total frequency (line 5). if there is a tie, we will assign it to the group with smaller subscription. after assigning a token, we will update the total frequency of the assigned group and the current group with minimum frequency (line 6). finally we return all the groups. traverse u, get the global dictionary of tokens s sorted by token frequency; 3 initialize the total frequency of each group as 0; 4 for each token t 2 s do 5"
"the next problem becomes how to partition all datasets into p buckets. according to the principle of minskew, we should try to minimize the difference between buckets. to this end, we use the perimeter, i.e., total length of planes to evaluate the quality of bucket b i as is shown in equation (10):"
"the next problem becomes how to efficiently locate the leaf nodes containing knn results. there are two key points towards this goal. first, we should prune dissimilar records in batch by taking advantage of r-tree index. second, we should avoid visiting dissimilar leaf nodes which will involve many unnecessary verifications."
"similarly, the multiple-transformation distance can be used in the r-tree index to prune dissimilar records in batch. on the basis of query-node minimum transformation distance, we define a similar mechanism under the multiple transformations in definition 5. since we get the maximum individual transformation distance among all transformations to calculate the query-node minimum multiple-transformation distance, it is obvious that this distance has almost the same property but no less than the individual query-node minimum transformation distance. it is easy to see that the query-node minimum multiple-transformation distance can be a tighter bound than that in definition 3. we prove it in lemma 4. proof. see appendix f, available in the online supplemental material. t u"
"in this section, we propose an approximate knn algorithm which does not return the exact knn results but runs much faster. we first introduce the idea of distribution aware approximation of knn results in section 6.1. we then talk about how to make the partition by leveraging the our r-tree index in section 6.2."
"to reach this goal, the first problem is how to construct index and perform filtering with the help of multiple transformations. the basic idea is that given a set of transformations v, for each record x 2 u under transformation v 2 v, we could generate different representative vector v½x individually. we give an order to v and assign each transformation in v with a number, thus we use v i to represent the ith transformation. then we define the joint representative vector under v by concatenating vectors generated by different transformations v i ½x into one vector:"
"our proposed methods can also support disk-based settings. unlike the in-memory scenario, all the index and data will be located on disk. here we evaluate the proposed methods using pubmed dataset, which is much larger than the other three datasets."
"the tri-squared statistical model was used to analyze data to determine the attitudes and perceptions of faculty as leaders. many statistical measures used in education are experimental research designs that require strict scientific methodologies that cannot be implemented in educational institutions without violating legal policies or severely disturbing the learning environment and associated instructional climate that is vital to instruction. to promote the previously mentioned efforts towards empowering faculty in the areas of: social justice, empowerment, and environmental equity novel statistical measures and methods are required that are specifically designed for education and educational environmental needs. the time has come for education to provide its own scientific field and subsequent measures based in its own rigor and grounded in the foundation of longstanding educational research, fundamental educational theory, and innovations in qualitative, quantitative, and mixed methods research designs native to the specifics of pedagogy and andragogy [cit] ."
"thus, there is a need for a framework that helps faculty to rapidly determine the outcome of their efforts and supports the intent to work collaboratively with other faculty. the trisquared statistical methodology meets this need. by using this statistical framework, faculty will begin to develop effective ways to meet the need for change by emphasizing teaching and learning process that promotes problem-solving approach and holds the teacher and the learner more accountable and responsible as critical thinkers."
"here in this work, we assume all the set records are multi-sets, which means that duplicate elements in each set are allowed. next we give the formal problem definition in definition 1."
"actually, from the aspects of performance, dualtrans outperforms singletran obviously. this is because for dualtrans, we construct two orthogonal transformations to calculate upper bound individually whose quantity relation differs with specific nodes or records to enlarge pruning power. the only extra cost for dualtrans, compared to singletran, is that the number of comparisons in each region of r-tree is doubled than that in singletran. this would slightly increase the filter cost but will definitely result in greater filter power, i.e., much smaller number of verifications. from fig. 6, we also find that the pruning power of dualtrans exceeds that of singletran because of our ingenious design for multiple transformation."
finally we introduce the process of constructing the index structure. we first generate the global token dictionary and divide them into m groups with algorithm 1. we then transform each record in the dataset using the generated groups by computing v i ½x with the tokens of x in group i. next we adopt state-of-the-art method [cit] to index all the m-dimensional vectors into the r-tree structure.
"in the phase of index construction, we first create the transformation set v discussed above, and then map each record x into multiple representative vector u v ½x and index them with an r-tree index."
"the basic idea of performing knn search on a collection of set records is as following: we maintain a priority queue r to keep the current k promising results. let ub r denote the largest jaccard distance between the records in r to the query q. obviously ub r is an upper bound of the jaccard distance for knn results to the query. in other words, we can prune an object if its jaccarddistance to the query is no smaller than ub r . here for r we maintain ub r that is an upper bound of jaccard distance for knn results to the query. when searching on the r-tree index, we use the transformation distance to filter out dissimilar records; when performing verification, we use the real jaccard distance. every time when jaccard distance is updated, we will update ub r at the same time. with the help of index structure, we can accelerate above process by avoiding a large portion of dissimilar records. given the query q, we first transform it into an m-dimension vector v½q. next we traverse the r-tree index in a top down manner and find all leaf nodes that might contain candidates with the help of ub r . we then verify all the records in such nodes to update r in a similar way."
"with a single transformation, we can only reveal a facet of the features in a set record. therefore, the pruning power could be weaken due to loss of information. to address this problem, we discuss the way of utilizing multiple independent transformations to excavate more information of data distribution."
we also design an approximate knn search algorithm by leveraging the statistical information to model data distribution. then we build an iterative estimator to improve the search performance. we conduct an extensive set of experiments on several real world datasets. experimental results show that our framework outperforms state-of-the-art methods on both memory and disk based settings. the rest of the paper is organized as following. we discuss related work in section 2. we formalize the problem definition in section 3. we introduce the transformation mechanism and indexing techniques in section 4. we propose the exact knn search algorithm and pruning strategies in section 5. we introduce the iterative estimation techniques and approximate knn algorithm in section 6. we provide experimental results in section 7. finally the conclusion is made in section 8.
"in this section, we introduce the exact knn search algorithm based on the index structure. we first propose a knn algorithm which can prune dissimilar records in batch by leveraging the property of r-tree index in section 5.1. we then further improve the filter power by extending the current transformation in section 5.2 and devise an optimized search algorithm in section 5.3."
"the magnitude of the nmp behavior for the physical shift, r nmp,cop, is the distance the subject's cop, again computed with a 10-sample moving average filter applied, travels in the direction opposite the weight shift. larger values of r nmp,cop indicate more forceful shift initiation [cit] . the cop's position at the reaction instant, referred to herein as the initial cop position, serves as the reference position from which r nmp,cop is computed as"
"on some benchmark tests, the gcpso has shown an excellent performance of locating the minimal of a space after unimodal with only a small amount of particles. e steps to be followed for the gcpso are shown in algorithm 7."
"is is also a clustering algorithm slightly modified from the k-means algorithm. in centroid calculation instead of calculating the mean value, the median value is considered. is algorithm significantly reduces the error since there is no squared operation as in the calculation of the euclidean distance."
cop data were calculated from vertical reaction force data obtained from the balance boards. each balance board consists of force transducers that provide vertical force information for each of the four quadrants of the device [cit] . the mediolateral cop location for a subject standing on two balance boards can be computed as
"e search ability is increased by the social part. is will improve the random search in the area around the gbest position. e random vector and diameter of the search area are r and ρ(t), respectively. e range of the random vector lies between 0 and 1. e diameter of the search area can be updated using the following equation:"
"e exploration and exploitation in pso are based on the inertia weight. [cit], has no inertia weight. [cit], shi and eberhart introduced the concept of inertia weight by adding constant inertia weight. ey stated that a significant inertia weight facilitates a global search, while a small inertia weight facilitates a local search [cit] . is enhances the convergence rate and reduces the number of iterations. inertia weight less than 1, in general, improves the results. e used method improves the convergence rate and saves the time taken and some iterations."
"where the terms #successes and #failures are defined as the number of consecutive successes and failures, respectively. e threshold parameters sc and fc are determined empirically. since it is hard to obtain a better value in only a few iterations in a high-dimensional search space, the recommended values are thus sc � 15 and fc � 5."
"(2) find the cumulative distributive function; from that, find the cdf according to gray levels. (1) select the cluster centers. let them be \"c.\" (2) calculate the euclidean distance. (3) take each and every pixel and assign them into the appropriate cluster if the euclidean distance is minimum between the cluster and pixel. (4) once the segregation is completed for all the pixels, recalculate the new cluster center using the following formula: (1) select the random cluster centers. let the number of cluster centers be \"c.\" (2) calculate the euclidean distance. (3) take each and every pixel and assign them into the appropriate cluster if the euclidean distance is minimum between the cluster and pixel. (4) once the segregation is completed for all the pixels, recalculate the new cluster center using the median value instead of using a squared formula. (5) repeat the steps from 2 to 4 for some number of iterations or until a certain condition is encountered. computational and mathematical methods in medicine sample images iwpso gcpso figure 9 : comparative results of the false negative rate value. figure 11 : resultant images after preprocessing."
"filtering center of pressure feedback in mediolateral weight shifting potentially be confusing if people observe that their displayed cop initially moves opposite to the intended shift direction. in normal practice, such confusion is mitigated at least somewhat by filtering the displayed vfb. this filtering is generally performed to reduce feedback noise and is achieved through the use of a smoothing function applied to the raw cop measures provided by the force plate. the presence of smoothing is likely to obscure some of the nmp behavior in the vfb, thereby making the displayed cop path appear to be more nearly monotonic."
"e graphical view of the comparison of the true positive rate, true negative rate, false positive rate, and false negative rate for the algorithms used is shown in figures 6-9 . it is proved that the true positive and true negative rates are high and false positive and false negative rates are low for the gcpso algorithm."
"this study examined weight shifting performance in terms of the metrics time-to-target and magnitude of nmp behavior [cit] . with the introduction of filtered vfb, the motion of the cop marker visually presented to the subject generally differs from his/her actual measured cop motion. therefore, there are two relevant values for each metric-one quantifying the physical shift performance by means of the actual cop motion and the other quantifying the performance visually fed back to the subject by means of the filtered cop marker's motion. all four metrics were log-transformed to improve normality."
"e segmentation accuracy was measured using the true positive rate, true negative rate, false positive rate, and false negative rate by comparing the results from the algorithm with manual segmentation results. e practical results of the kmeans clustering segmentation algorithm are shown in table 2 ."
"in the preprocessing stage, a comparison was done between the performance of median, adaptive median, and mean filters. e ssi and smpi values are shown in table 1 and figures 4 and 5 . from the results, it is evident that the adaptive median filter has accurate characteristics than the mean and median filters for medical image segmentation."
"many studies have demonstrated that the performance of risk models can be improved by using many machine-learning based methods including regularization, hyper-parameter optimization, and ensembling algorithms [cit] . in this study, we aim to develop a good permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for third-party components of this work must be honored. for all other uses, contact the owner/author(s). risk model for classifying business delinquency by jointly and comprehensively exploring the effect of the above-mentioned modelimproving algorithms."
"these results show that differences in vfb delay and visually displayed nmp behavior result in significant differences in both shift speed and shift initiation force. without separating these two elements, though, one cannot ascertain the individual effects of these two vfb characteristics on shift performance. since vfb delay is known to have significant effects on static balance control [cit], one might expect it to be the dominant vfb characteristic in the present examination of dynamic weight-shifting balance. also, the duration of the nmp behavior is a portion of the total shift time (average of 0.64 s or 62 ± 12%) confined to the beginning of the shift, which could limit its effect on balance control compared to delay, which occurs over the entire duration of the shift. if delay is the dominant characteristic, the attenuation of displayed nmp behavior might have either complementary (increasing shift speed and nmp behavior) or opposite (decreasing shift speed and nmp behavior) effects, only less significant ones, with complementary effects being more likely. future work might attempt to separate out the effects by constraining the cop vfb to move exclusively toward the target after it appears, thereby eliminating all nmp behavior without generating additional time delay. without some manipulation of the vfb, though, this approach would result in a step increase in the displayed cop velocity from zero once the measured cop passes through the initial shift position. such an unexpected discontinuity in cop velocity could lead to reduced reliance on the vfb, so one possible manipulation would be an anticipatory motion of the cop vfb toward the target once that actual cop began moving in that direction after the peak in nmp behavior."
"for all vfb conditions, the display components of the vfb were the same. one-dimensional direct cop vfb was provided by a green circular marker placed on a grey planar background denoting the subject's (filtered) mediolateral cop location (fig 2) . the horizontal position of the marker denoted this cop location, while the vertical position of the marker was fixed to the horizontal axis at the middle of the display."
"few studies have fully reported specifications for filters applied to the provided vfb [cit] . while one type of filtering (addition of time delay to vfb) has been shown to alter static balance performance [cit] through smoothing that decreases the difference between the cop and the estimated cog position, the effects of filtering on dynamic balance performance have not been previously explored. in this study, moving average filters were applied to cop vfb in the context of mediolateral weight shifting. use of this simple type of filter provides a basis for adjusting vfb in a straightforward manner, while potentially affecting balance performance. applying a simple moving average filter to the cop data smooths it, reducing the magnitude of the visually displayed nmp behavior and thereby making the visually displayed cop position more similar to that of the actual cog position early in a weight shift. such filtering, however, naturally results in vfb delay, which can cause greater differences between the cop vfb and the actual cog positions later in a shift. candidate approaches to reduce this difference without the addition of vfb delay are postulated in the discussion."
"e two main types are small-cell lung carcinoma and nonsmall-cell lung carcinoma [cit] . long-period tobacco smoking is the primary factor for 85% of lung cancers [cit] . about 10-15% of cases occur in people who have never smoked but due to air pollution, secondhand smoking, asbestos, and radon gas. computer tomography (ct) and radiographs are the conventional methods to detect the presence of lung cancer."
"this study demonstrated that increases in filtering applied to cop feedback provided during a mediolateral weight-shifting balance task resulted in significantly increased shift initiation force and decreased physical shift times. as such, filtering of vfb in a weight-shifting task should be selected and reported accordingly in future work. these results show that simple filtering could be used to adjust vfb for balance rehabilitation tasks in a clinical setting in order to potentially optimize balance performance."
"in this paper, we aim at developing and improving the risk modeling via the widely used machine-learning based algorithms including regularization, hyper-parameter optimization, and ensembling simultaneously. the results show that the optimal hyper-parameter settings for the base classifiers are lr without regularization, kn n by using 9 nearest neighbors, dt by setting the level of the tree to be 7, and an n with three hidden layers. the optimal model we get for classifying business delinquency is through bagging on kn n with k valued 9, which reaches the average accuracy, auc, recall, and f1 score valued 0.90, 0.93, 0.82, and 0.89, respectively. although different conclusions may be obtained because of various dataset used in the future, the study methodology provided by us is a good reference for studies that aiming to improve risk modeling using machine-learning based algorithms."
"(1) assume the input matrix \"a\" which has m rows and n columns. (1) obtain the histogram for the input image and find the probability mass function."
"logistic regression (lr) is a widely used technique for binary classification because of its strong interpretability and competitive performance. moreover, regularized lr, which leads to a substantial decrease in variance and prediction error, outperforms lr in some studies. two commonly used versions of lr include l1-regularized lr and l2-regularized lr, with the former version penalizes the l1 norm of the coefficients while the latter version penalizes the l2 norm of the coefficients [cit] . decision tree (dt ) is another widely acceptable binary classification technique but different settings of its hyper-parameters could largely affect its performance [cit] . similarly, in an artificial neural network (an n ) and k-nearest neighbor (kn n ), careful tuning of the hyper-parameters can improve their performance [cit] . moreover, model ensembling is another effective way to improve the model performance [cit] ."
"spatial processing to preserve the edge detail and to eliminate nonimpulsive noise by the adaptive median filter plays a vital role. e small structure in the image and edges are retained by the adaptive median filter. in the adaptive median filter, the window size varies with respect to each pixel."
"because a single force plate is a convenient way to instrument balance measurement, most studies involving dynamic balance activities have recorded and examined the cop rather than the cog during weight shifting. likewise, when vfb is provided in these studies, it is based on the cop, not the cog. under these circumstances, the presence of the nmp behavior could non-minimum phase behavior. as the subject shifts his/her position from the center to the blue target region on the left, his/her cop first moves in the opposite direction due to the force required to initiate the weight shift, referenced in this and a previous study as nmp behavior. the red trace is not shown as a part of this study's vfb. doi:10.1371/journal.pone.0151393.g001"
"two variables change whenever a person's weight is shifted. one is center of gravity (cog), which is defined as the vertical projection of the person's center of mass onto the ground [cit] . the second is cop, which is the location of the resultant force of the vertical support forces present in the person's two legs [cit] . explicit measurement of cog information requires motion capture equipment, whereas cop information can be captured using a single force plate. the movements of the cog and cop during weight shifting are highly correlated except for a relatively brief period at the start of the shift during which the cop exhibits a non-minimum phase (nmp) behavior signified by its movement in a direction opposite that of the weight shift [cit] (fig 1) . the mechanical forces that are generated by torques applied through the hips and ankles and are needed to initiate a shift of weight result in this nmp behavior, thus causing the cop to follow a biphasic path, first opposite and then aligned with the direction of the weight shift. in contrast, the cog position typically changes monotonically in the same direction throughout the shift."
"(5) select all the elements listed by the mask and sort them in ascending order. (6) take the median value (center element) from the sorted array and replace the element a(1, 1) by the median value (7) slide the mask to the next element. (8) repeat the steps from 4 to 7 until all the elements of matrix \"a\" are replaced by their corresponding median value."
"optimization. pso is a metaheuristic algorithm used efficiently in medical image analysis [cit] . it mimics the social behavior of the birds searching for food [cit] . e fundamental idea of pso is sharing and communicating the information. in this approach, each particle has initial position and velocity. based on the fitness value, the velocity and position are updated. e relevant two equations in pso to update the position and velocity are as follows [cit] :"
"the wehab system [cit] was employed by a researcher to lead all subjects through the research protocol. this system consists of a computer running the wehab software, two nintendo wii balance boards for data collection, one nintendo wii remote for researcher input, and two webcams for collecting visual data. balance board data were collected at a sampling rate of 63.4 ± 1.4 hz. apart from the use of a small desktop computer instead of a laptop computer, the wehab setup used in this study was identical to that from previous work [cit] . while being led through each balance task, each subject stood with one foot on each of the two nintendo wii balance boards placed with 31.8 cm separation between their centers. a chair located directly behind the balance boards provided the place for the subject to rest between activities without the subject needing to move his/her feet. during the study, a researcher sat in a nearby chair, read instructions to the subject, and used a nintendo wii remote to advance through each activity. a 27-inch monitor providing vfb was located a horizontal distance of 134 cm away from the front of the balance boards, and its height was adjusted to the eye level of each subject by the researcher."
"biofeedback in standing balance has been shown to result in improved efficacy and learning in balance therapy [cit], though the relationship between biofeedback and balance performance is not fully understood [cit] . the use of center of pressure (cop) information to provide visual feedback (vfb) has been shown to improve balance control in healthy adults [cit], and previous work has examined the effects of vfb design [cit] on balance performance in healthy subjects. for individuals post-stroke, mediolateral balance specifically has been shown to be particularly indicative of balance performance [cit] . in the rehabilitation of stroke, static balance tasks like quiet standing can be performed with or without vfb, while the dynamic balance task of weight shifting, commonly used in both clinical rehabilitation [cit] and balance assessment [cit], typically involves a subject being guided through a series of weight shifts with vfb in order to place his/her cop within a displayed target region [7, [cit] . one motivation for performing mediolateral weight shifting tasks is that they allow information (such as time-to-target and path-planning data) to be collected from post-stroke dynamic balance performance, which is beyond the scope of static balance tasks [cit] ."
"elderly subjects have been shown to complete weight shifting tasks more slowly and with greater difficulty than young adults [cit] . stroke patients, who are often advanced in years, are therefore likely to demonstrate differing results from younger subjects. as such, examination of an elderly subject pool would provide additional insight into vfb design in clinical applications. additionally, a previous study showed no significant effects of vfb delay on static balance performance in stroke patients [cit] . these differing results compared to those obtained from both young healthy [cit] and elderly subjects [cit] indicate that clinical subjects may be affected less by vfb delay. such differences may be due to clinical subjects lacking the ability to utilize vfb to the same extent as healthy subjects and instead relying primarily on visual information from the environment as a result of cognitive deficits or the increased mental and physical effort necessary to simply remain standing. this idea is not necessarily inconsistent with previous work showing that stroke patients exhibit greater dependence on visual information in controlling their balance [cit], which may be a result of impaired proprioception. in terms of a dynamic weight-shifting balance task incorporating changes in visually displayed nmp behavior, no previous work has examined the effects of filtering in dynamic weight shifting for clinical subjects. indeed, due to this reduced effect of vfb delay on clinical subjects, such subjects could potentially see greater impact from the differences in nmp behavior provided in weight-shifting vfb and merit examination in a future study."
"as shown by the r nmp,cop metric, additional filtering of cop vfb significantly increased the physical shift initiation force applied by subjects, with the cop 34 vfb condition resulting in an average of 12% more and the cop 58 vfb condition resulting in an average of 23% more cop nmp behavior compared to the cop 10 vfb condition. in terms of actual force, this corresponds to an average of 4.8n additional force for the cop 34 vfb condition and 12.9n additional force for the cop 58 vfb condition. these increases in shift initiation force are consistent with the reduced cop shift time associated with increased vfb filtering. the effect of vfb condition on shift initiation force is likely caused by more than just the difference of attenuation in visually displayed nmp behavior."
"these significant effects of vfb filtering for young healthy subjects could potentially translate to differences for stroke patients. as such, modulation of vfb filtering could be used to provide subtle direction to patients performing a weight-shifting balance task. for example, lower functioning patients could be eased into performing weight-shifting balance activities with minimal vfb filtering to start, while higher functioning patients could be encouraged to shift with more vigor through increasing vfb filtering. for patients with hemiparesis, the magnitude of filtering could even be adjusted depending on the direction of the shift, thereby encouraging more forceful and quicker shifts to one direction compared to the other. the use of this filtering would likely be best applied on a subject-by-subject basis, with individual patient deficits and balance capabilities informing the therapist's selection of the extent of vfb filtering."
"e ct images data used to support the findings of this study have been deposited in the lungct-diagnosis repository (doi.org/10.7937/k9/tcia.2015.a6v7jiwx). various methods accuracy (%) pso, ga, and svm algorithm [cit] 89.50 k-nn classification using ga [cit] 90.00 projected gcpso method 95.81"
"after each trial, including the demonstration trial, the subject was instructed to sit down for 45 s of rest to mitigate any potential physical fatigue effects. during this rest time, subjects sat in a provided chair and rested until the break was complete. in order to gauge any physical or mental fatigue effects that were present despite these breaks, subjects were asked afterwards whether they felt tired during the study. with each subject completing four trials (68 lateral weight shifts) for each of four vfb conditions, the total duration of each session, including rest, was about 38 minutes on average."
"image enhancement is the technique which is used to improve the image quality. for better understanding and analysis, it is mandatory to enhance the contrast of medical images. e conventional method used for this operation is histogram equalization. a minor adjustment on the intensity of image pixels is done in this method. each pixel is mapped to intensity proportional to its rank in the surrounding pixels. e steps followed for histogram equalization are given in algorithm 3 [cit] ."
"where f r and f l are the total forces present on the right and left balance boards, respectively, and d x is a multiplier that converts the vertical force ratio into a mediolateral distance. in this study, a value of 0.159 m (half the distance between the two feet) was used for d x . from this equation, a subject's cop can range from −d x to d x, where a value of 0 represents equal weight distribution between both legs. in quiet standing, the cog and cop trajectories share an average value [cit] and are inphase [cit] . previous studies have used cop data from quiet standing to estimate the cog through use of a low-pass filter [cit] . in a dynamic weight-shifting balance task, the difference between the cog and the cop is more readily apparent, and this low-pass filter method does not produce accurate cog trajectories [cit] . this inaccuracy motivated the use of a model-based vfb condition (cop mod ) in the present study in the form of an estimate of the mediolateral cog position computed by numerically integrating the dynamic equation of motion for a parallelogram model (fig 3),"
"statistical analysis was performed through linear mixed models using sas version 9.4 (sas inc., cary, nc). subject and vfb sequence were specified as random factors. initial analysis of the effects of target location (center vs. offset) were analyzed; based on these results, the final analysis was performed separately on both center and offset target data subsets. the following main effects were examined:"
"this study examines vfb provided with significantly different magnitudes of moving average filtering, as well as a model-based filter, in order to examine the effects on both the provided vfb and the resulting subject balance performance. it is hypothesized that increasing filter magnitude results in increased time required for the vfb to shift to each target and reduced magnitude of the nmp behavior displayed in vfb, resulting in significant differences in physical shift performance. the objective is to develop a more fundamental understanding of vfb implementation in dynamic balance tasks, as well as to inform the design of future vfb systems."
"while feedback conditions after the first ordinal position (first vfb provided to each subject) resulted in significantly increased t cop and decreased r nmp,cop values, no significant differences were seen for t vfb and r nmp,vfb due to ordinal position. this order effect may be explained by subjects becoming more adept at efficiently shifting based on the provided feedback, resulting in less effort spent as shown by slower and less forceful physical shifts without significantly affecting the feedback that was provided during those shifts with the various filters. this behavior was likely a result of the feedback progressing to each subsequent target location based on feedback shift time (t vfb ), not physical shift time (t cop ) and is consistent with previous studies of dynamic balance tasks [cit] that demonstrated improvement over time, indicating the presence of learning."
"this study incorporated data from 30 healthy subjects, who consisted of 7 males and 23 females, ranging from 18 to 21 years of age (body mass 63.9 ± 11.3 kg; height 170.7 ± 8.0 cm; this form indicates mean ± standard deviation, as it does for all data presented subsequently). all reported having no visual or balance impairments that could have led to difficulty in performing the weight-shifting tasks in this study; however, independent confirmation of their balance capabilities was not obtained."
"e gcpso focuses on a new particle which deals with the current best position in the region. in this task, this particle is treated as a member of the swarm, and the velocity update equation for this new particle is given as follows [cit] :"
"e diagnosis is confirmed by biopsy which is usually performed by bronchoscopy or ct scan. e cause of cancer-related death among men is mainly due to lung cancer. hence, it is essential to determine a new robust method to diagnose the lung cancer at an earlier stage [cit] . for the present study, 20 lung image samples and four algorithms have been taken for analysis. it was proved that the combination of adaptive median filter, adaptive histogram equalization, and guaranteed convergence particle swarm optimization-(gcpso-) based algorithm has more accurate results among others. (1) assume the input matrix \"a\" which has m rows and n columns."
"where x i are the pixels, v j are the cluster centers, ‖x i − v j ‖ is the euclidean distance between x i and v j, c i is the number of data points for the ith cluster, and c is the number of cluster centers [cit] . e steps followed for k-means clustering are given in algorithm 4."
e comparative evaluation based on the accuracy of the segmentation is shown in table 7 and figure 10 . e results indicate that the gcpso-based technique has the highest average value of accuracy than the other methods.
e success of pso relies on the fitness function. e following fitness function has been used for the present study: where n is the number of clusters. e steps followed for the particle swarm optimization are shown in algorithm 6.
"in an earlier research, lung cancer detection was done using pso, genetic optimization, and svm algorithm with the gabor filter and produced an accuracy of 89.5% [cit] . e method to detect lung cancer by means of k-nn classification using the genetic algorithm produced a maximum accuracy of 90% [cit] . e comparative results with respect to the above-said methods are shown in table 8 ."
e resultant images after segmentation using k-median clustering are shown in figure 13 . e resultant images after segmentation using the pso algorithm are shown in figure 14 .
"e clusters formed by this method are more compact. as an alternate, this approach uses the lloyd-style iteration. e steps followed for k-median clustering are given in algorithm 5 [cit] ."
e simplest and conventional method in cluster analysis is the k-means clustering algorithm. is algorithm segregates the given dataset into two or more clusters [cit] . e accuracy of this method completely depends on the selection of the cluster center. it is mandatory to select the optimum cluster center to get a better result. e euclidean distance is the general measure to segregate the dataset [cit] . pixels are assigned to an individual cluster based on the euclidean distance. e objective function used in this algorithm is
"the best base classifiers along with their hyper-parameter setting are lr-0 (i.e, lr without regularization), kn n -9 (i.e., k valued 9), dt -7 (i.e., 'max_depth' valued 7), and an n -4 (i.e., contains three hidden layers and having units valued 50, 25, and 13 respectively in each layer). figure 1 shows the result of the best classifiers as well as their ensemble models. the post-fix bag and boo denote bagging and boosting, respectively. in lr, bagging and boosting cannot significantly improve the model performance with respect to accuracy, recall, and f1 score. quite unexpectedly, boosting on lr even hurt the auc by a large extent. in kn n, both bagging and boosting are beneficial when considering accuracy, recall, and f1 score measures. on the contrary, boosting on kn n decrease auc significantly. bagging on dt outperforms base dt as it produces significantly higher auc and marginally higher accuracy. however, boosting significantly decrease auc of dt . compared with base an n, boosting on an n is beneficial in terms of accuracy, recall, and f1 score while it hurt auc significantly. bagging on an n can significantly decrease accuracy, recall, and f1 score, indicating that an n is not a good base classifier to be bagged on. by comparing all the aforementioned results, we conclude that kn n -9-bag (i.e., bagging on the base classifier kn n with k valued 9) is the optimal risk model in our study with average accuracy, auc, recall, and f1 score valued 0.90, 0.93, 0.82, and 0.89, respectively."
"the vfb indicator results demonstrate that the different filters and the smoothing they provided were sufficient to provide distinct vfb conditions in terms of both vfb delay and visually displayed nmp behavior, which increased and decreased, respectively, as the size of the moving average filter window increased. the filters' attenuation of visually displayed nmp behavior early in a weight shift could be desirable since it reduces the discrepancy between the desired and actual initial movement of the visual indicator. the relatively large vfb delay that accompanies this attenuation is problematic, though. throughout the latter course of the weight shift, the delay creates a discrepancy between the visual indicator's position and the subject's own sense of his/her progress toward completing the shift. previous studies have shown that the addition of vfb delay in static balance, wherein it decreases the difference between cop and estimated cog position [cit] (the same is true only early in a weight shift, with the opposite occurring later), leads to a decrease in subject reliance on the vfb in young healthy subjects [cit] and increased sway in elderly healthy subjects [cit] . taken together, these results suggest that significant vfb delay is likely undesirable for weight shifting activities. potentially reducing subject reliance on the vfb that is essentially required to define the task they are asked to perform would be counter-productive unless an alternative form of feedback, such as auditory, was being emphasized instead. in most cases then, the magnitude of vfb filtering for weight shifting tasks should be selected based on the effects of filtering on actual weight-shifting performance discussed below."
"subjects experienced the four vfb conditions in one of four sequences (cop 10 34, and cop 10 ) generated using a latin square. a sequence was randomly assigned to each subject, with some constraints applied to ensure appropriate distribution of the sequences across the subjects. at the beginning of testing with each of the four vfb conditions a subject experienced in his/her assigned sequence, visual instructions were provided on the monitor as follows: \"quickly shift your weight to each blue target when it appears. targets will alternate between center and a randomized side. time will start once you are centered for 3 s.\" in all cases, a trial began only after the subject had maintained his/her weight in a neutral stance for 3 s (as assisted by vfb). for each vfb condition, the first trial was a short demonstration consisting of eight shifts (four shifts to offset target positions, each followed by a shift back to a central target corresponding to neutral stance) accompanied by the appropriate vfb. this demonstration trial served to ensure that the subject was familiar with the current vfb condition, as consistent with prior studies [cit], before performing the actual weight-shifting balance task in trials two through four."
e resultant images after preprocessing are shown in figures 11(a) and 11(b) . e resultant images after segmentation using k-means clustering are shown in figure 12 .
"all subjects gave their informed consent prior to inclusion in the study, and this research received approval from the notre dame institutional review board (protocol id: 12-06-375)."
e presence of spatial noise is mainly due to the data acquisition process. e neighborhood mean value is measured for each and every pixel and is replaced by the corresponding mean value. is process is repeated for every pixel in the image [cit] . all the pixels in the digital image are modified by sliding the operator over the entire range of pixels. e steps followed for the average filter are given in algorithm 2.
"the dataset used in this study contains the financial information of 9500 [cit] with the delinquent rate valued 49.69%. we randomly select 80% of the data as the training set and use the rest as the testing set. the dimensionality of the features is reduced to 100 via hierarchical variable clustering in the data preprocessing stage. four base classifiers including kn n, lr, dt and an n are developed by using regularization and hyper-parameter optimization algorithms. to be specific, lr is regularized by using both l1 and l2-regularization. in kn n, the hyper-parameter 'k', denoting the number of nearest neighbors, is tuned by taking a series of values ranging from 3 to 13. in dt, we tuned 'max_depth', which denotes the maximum level of the tree structure, by using different values ranging from 5 to 15. in an n, the hyper-parameter 'layer_size', representing the number of hidden layers with units in each layer, is tuned by taking a series of values of '50', '50_25', '50_25_13', and '50_25_13_6'. for example, the value of '50_25_13' means there are three hidden layers in the an n and the number of units in each layer is 50, 25, and 13, respectively. area under the receiver operating characteristic curve (auc of roc), recall, and f1 score are used as evaluation metrics based on testing set by repeating 10-fold cross validation 10 times [cit] . after building the base classifiers as accurate as possible, two model ensembling techniques (including bagging and boosting) are performed to examine whether the performance can be further improved or not. -isbn: 978-1-4503-6251-1 kennesaw, ga, usa, april 18-20, 2019"
"lung cancer, also known as lung carcinoma, is a malignant tumor characterized by uncontrolled growth of the cell in tissues of the lung. it is mandatory to treat this to avoid spreading its growth by metastasis to other parts of the body. most cancers that start in the lung are carcinomas."
"computational and mathematical methods in medicineeradicate the incidence of noise content and to improve the image quality before an examination [cit] . is part of work is known as preprocessing. in the preprocessing stage, noise removal and contrast enhancement are two primary steps. in the present study, the performance results of median, adaptive median, and average filters to isolate the presence of speckle noise have been compared. e coding for the same has been implemented using matlab. furthermore, the image quality and visual appearance are improved by adaptive histogram equalization. e second stage of work is segmentation. is stage consists of applying five methods, namely, k-means, k-median, particle swarm optimization (pso), inertia-weighted particle swarm optimization (iwpso), and gcpso. e tumor portion was extracted from the segmented results of the above-said five methods and compared with manual extraction. e results show that the gcpso-based segmentation has more accuracy than the others. figure 1 depicts the process of operation for the present study."
"in this study, various optimization algorithms have been evaluated to detect the tumor. medical images often need preprocessing before being subjected to statistical analysis. e adaptive median filter has better results than median and mean filters because the speckle suppression index and speckle and mean preservation index values are lower for the adaptive median filter. comparing the five algorithms, the accuracy of the tumor extraction is improved in gcpso with the highest accuracy of 95.8079%, and it obtained above 90% of precision in all the 20 images. it is more accurate when compared to the previous method which had an accuracy of 90% in 4 out of 10 datasets only. in future studies, the use of more number of optimization algorithms will be included to improve the accuracy."
certain performance measures are used to evaluate the results obtained from medical image segmentation. e list of performance measures used to assess the filter operation is shown in figure 2 [cit] . let i f be the image after noise reduction and i 0 be the noisy image.
"generally, time series forecasting involves the prediction of future values of data based on discovering the pattern in the historical data series and extrapolating that pattern into the future. time series forecasting is a widely discussed issue and its applications appear in various fields of business and engineering [cit] . the reason is that prediction of future events is crucial for many kinds of planning and decision-making processes. applications regarding time series data can be easily found in the literature, such as wind energy forecasting [cit], water resource management [cit], traffic accident prediction [cit], and cash flow forecasting in construction projects [cit] . hence, it is not surprising that time series analyses and predictions are on the rise among researchers."
"in the task of forecasting usd/twd exchange rate series, although the anfis model delivers the smallest error in the training data set, its performance on the testing data set is undesirable. moreover, the flsvr tsp yields the best outcome since its rmse and mae for the testing data are 0.36 and 0.29. the experimental results have shown that a hybridization of the ls-svr and fa algorithms can deliver more superior performance compared with other benchmark approach. the fa algorithm has autonomously identified the most appropriate values of the ls-svr's tuning parameters as well as the embedding dimension. this eliminates the tedious effort for setting the model parameters and also enhances the model prediction performance."
"motor control, in general, requires the fine-tuning of an agent's forces [cit] ). for example, balancing a pole requires very small adjustments in the neighborhood of the equilibrium position. another example concerns how people unintentionally coordinate their actions as when they walk at the same pace. a fascinating laboratory example concerns two persons sitting in two rocking chairs with different natural rocking frequencies [cit] . the experiment shows that individuals spontaneously coordinate their rocking frequencies in patterns closely resembling classical coupled oscillator dynamics. this illustrates well the importance of continuous adjustments in motion events. if the adjustments were not continuous, it would be difficult to achieve coordinated movements."
"adverbs typically exhibit ordinal properties as well. for example, the adverb \"softly\" in \"victoria hits oscar softly\" selects a sub-region of the force space where the value on the dimension of the strength of the force is relatively low (an ordering property). if monotonicity is assumed, then the value on the corresponding result dimension, i.e., pain, will also be low. so the inference in this case is that oscar will only feel a mild pain (if any at all)."
"primary road speed (for both directions) was constant at 50 km/hr; daytime and good weather conditions, which allow good visibility, were adopted in this scenario."
"the data used in the analysis are gap-acceptance observations (driver decisions) collected from driving simulator experiments, in which the virtual environment has been built with the aim to reproduce a real three-leg priority intersection located in a sub-urban area near venice (fig. 1) . a high level of detail in the threedimensional representation of the real context has allowed to create a realistic virtual environment."
"among the ai methods, the least squares support vector regression (ls-svr) is an advanced machine learning technique for solving regression analysis [cit] . this method has been proved to possess many advanced features [cit] . in the ls-svr's training process, a least squares cost function is proposed to obtain a linear set of equations in the dual space. consequently, to derive the solution, it is required to solve a set of linear equations, instead of the quadratic programming as in the standard svm. moreover, this linear system can be efficiently solved by iterative methods such as conjugate gradient."
the study is part of a broader project where we analyse laughter using a multi-layered scheme and propose a semantic/ pragmatic account of the meaning and effects of laughter. the focus of the current study is the positioning of laughter in relation to its laughable.
"the chromosomes, and the annotations, are constructed by passing their genomic co-ordinates, and some optional secondary data, separately as tab-delimited text files similar to the commonly used bioinformatics file format, bed (browser extensible data). a single r function, namesake of the package, is used for constructing a variety of different plots (discussed in the next section) by changing its various arguments values. the simplest annotation plot, however, can be constructed by using the following command:"
"visualizing differential expression analysis results from a breast cancer study data [cit] . chromosomes in black strand depict average expression change(logfc) per loci where red and blue regions signify down-regulation and up-regulation respectively. the mean of normalized expression of the two groups, for the same genes, is shown in adjacent (control and case)."
"swarm intelligence is a design framework based on social insect behavior [cit] . social insects such as ants, bees, firefly, and wasps are unique in the way these simple individuals cooperate to accomplish complex, difficult tasks. this cooperation is distributed among the entire population, without any centralized control. each individual simply follows a small set of rules influenced by locally available information. this emergent behavior results in great achievements that no single member could complete by themselves [cit] . firefly algorithm (fa) is one of the recent swarm intelligence methods which was based on the flashing patterns and behavior of tropical fireflies [cit] . according to previous works, the algorithm is very efficient and can outperform conventional algorithms in solving many optimization problems [cit] ."
"in this study driving simulator experiments have been designed with the aim to measure the effects of some of these variables. based on these data, models which include driving styles variables and other variables commonly used in gap-acceptance studies (time interval size, driver's gender) have been estimated and validated. the comparative analysis between these two kinds of models has been carried out using the so-called roc (receiver operating characteristic) curve analysis."
"the target entity of the event is called the patient and the result vector describes the changes of the properties of the patient. if there is an entity that generates the action vector it is called the agent. there exist, however, events without agents, for example events of falling, drowning, dying, growing and raining."
"-contrary to popular belief, only 30% of laughters occur immediately after the laughable. laughters frequently start during the laughable (more so with \"speaker\" laughter than \"audience\" laughter)."
"first of all, the three constraints clearly contribute to the buffering of working models from the details of perceptual inputs and to generate robust predictions. monotonicity drastically simplifies causal inference, requiring only ordinal information on causes to infer ordinal properties of effects. monotonicity helps to extrapolate and guide the prediction of causation in the working model. continuity ensures that small changes in a neighborhood of effects are associated with small changes in causes. it thus predicts that observed variations in effects should be related to similar causes. finally, convexity ensures that it is enough to know two event mappings to know what is in between them. thus convexity helps to interpolate -for example to predict unspecified properties of the event. one could summarize by saying that the three properties ensure ordinal, topological and geometrical robustness to working models -and that they enable robust inference about causation, control of action and generalization. for example, in a narrative that only specifies a few snapshots of a story, the three constraints can help the reader or listener to fill in missing information."
"for performance comparison, root mean square error (rmse) and mean absolute error (mae) for training and testing data sets are calculated. the forecasting results obtained from the bpnn, anfis, esim approaches, and the proposed flsvr tsp are provided in table 3 . it is observable that the flsvr tsp has achieved a significant improvement in terms of prediction accuracy. the prediction errors of testing data sets yielded by the newly developed model are smaller than that obtained by other ai approaches. this means that flsvr tsp has better generalization property and it has successfully diminished the problem of over-fitting. in prediction of mackey glass series, rmse, and mae of the flsvr tsp for testing data are 0.005 and 0.004, respectively. the anfis shows relatively good forecasting result while performance of the esim is poor. in the case of the water flow series, the flsvr tsp and the esim outperform the bpnn and the anfis. however, the flsvr tsp prediction is slightly better than that of the esim. herein, rmse and mae of the flsvr tsp for testing data of the water flow series are 10.33 and 8.35."
"-more specifically, laughter-laughable alignment may vary depending on at least the source of the laughable (self or partner) and whether it is speech laugh or laughter bouts."
"we analyzed a portion of the duel corpus [cit] ) the corpus consists of 30 dyads (10 per language)/ 24 hours of natural, faceto-face, loosely task-directed dialogue in french, mandarin chinese and german. each dyad conversed in three tasks which in total lasted around 45 minutes. the three tasks used were:"
"we coded whether the laughables are described events, meta-linguistic, or exophoric events. in our corpus described events are the commonest (92% in french and 89% in chinese), followed by exophoric laughables (7% in french and 10%). metalinguistic (1% in both languages) laughables are rare, so we grouped them with described events in the current analysis. on average, there are more self-produced than partnerproduced laughables, supporting the idea that speakers laugh more often than the audience. interestingly, 3% of the laughables are jointly produced (one person finishing the other's sentence, or both saying roughly the same thing at the same time) (see (8)). with the former two categories, we also coded whether the laughable is produced by the laugher or her partner, which allow us to compare our results with studies of \"speaker\" or \"audience\" laughter."
"moreover, the back propagation neural network (bpnn), the adaptive network based fuzzy inference aystem (anfis) [cit], and the evolutionary support vector machine inference model (esim) [cit] are used for result comparison. for the bpnn, it is needed to specify the number of hidden layers and hidden neurons. for the anfis, the type of membership function and the number of membership functions for each input are required for constructing the prediction model. the determination of these parameters is often carried out by repetitive trial-and-error tuning processes. in this study, for each time series, we select the model configuration that yields the smallest prediction error of validating data. it is noticed that the embedding dimensions ( ) for the bpnn, anfis, and esim models are calculated by the fnn approach [cit] . using this approach, the embedding dimensions for the mackey-glass series, the water flow series, and the usd/twd exchange rate series are 3, 3, and 4, respectively. meanwhile, in the flsvr tsp, the fa automatically identifies the optimal embedding dimensions. the optimal tuning parameters of the flsvr tsp for these three time series are shown in the table 2 ."
"on the surface, their findings in speakers and signers are similar: speakers do not stop midsentence to insert a laugh, and signers do not laugh while signing a sentence. however, this \"similarity\" may be a difference in disguise. we have shown that speakers frequently overlap laughter and speech. if it were indeed true that signers do not laugh while signing, it raises the question why speech laughter is common for speakers but rare for signers. [cit] hypothesised that the placement of laughter in dialogue is controlled by a higher linguistic ordered structure, where laughter is secondary to language. therefore, even when the two don't occur in competing channels, e.g., for signers, laughter still only occurs at phrase boundaries."
"our experiment explored how subjects (making a right turn maneuver from the minor street) select gaps presented in the same order as observed in the real situation. the driving experiments have been conducted at the 9 2 transportation laboratory of the department of civil, environmental and architectural engineering (university of padova). the simulation system used is a fixed-base driving simulator produced by stsoftware®. it includes a realistic cockpit, three networked computers, five high definition screens; the system is also equipped with a dolby surround® sound system. this simulator configuration allows to produce realistic virtual views of the road network and of the surrounding context (fig. 2) . in the following sections a description of the experiments tasks is presented."
"our study provides the first systematic analysis of laughables, and demonstrates the existence of a corpus, the duel corpus [cit] b) in which less than a third of the laughs immediately follow their referents. instead, the laugh can occur before, during or after the laughable with wide time ranges. in addition, laughter does \"interrupt\" speech: we frequently start laughing in the middle of an utterance of the interlocutor or of ourselves (often speech-laugh). our results challenge the assumption that what laughter follows is what it is about, and thus question previous claims based on this assumption."
"in some cases, such a discontinuous phase transition may mark a cognitive separation between two different events, like handling or destroying something, cooking or burning, bending or breaking. there is an asymmetry here, however. we can bend a twig to a varying degree, but when we break it, this results in a single state that is no longer amenable to a degree of quantification. therefore, a more fruitful cognitive perspective seems to consist in distinguishing between continuous and discontinuous events."
"a good example is when we have to account for the influence of counterforces. mental models of events often contain counterforces such as supporting surfaces or friction, but their presence is in general not expressed. important questions are then when and how counterforces can be expressed linguistically."
"the experiments performed dealt with a relatively simple situation: vehicles on the main road traveled at constant speed, so the gaps between vehicles remained constant once vehicles were created and drivers had to drive adopting their usual behavior, making a right turn maneuver from the minor street if considered safe."
"the continuity and convexity constraints thus let us infer that when we are near a typical event then similar actions produce similar results, under the implicit ceteris paribus assumption."
"studies have been carried out to demonstrate the excellent generalization, prediction accuracy, and fast computation of the ls-svr [cit] . since time series forecasting can be formulated as a regression analysis problem, it is very potential to apply the ls-svr to tackle the problem at hand. nevertheless, the implementation of the ls-svr requires an appropriate setting of its tuning parameters, namely, the regularization parameter and the kernel function parameter. improper specification of these tuning parameters can significantly degrade the performance of the machine learning technique."
"several logit models of gap-acceptance behavior were specified and estimated in this study using the hielow® program; in which, as expected, the acceptance probability increases with the interval size is. gender has an influence in the sense that male drivers seem to accept smaller intervals than female drivers. also driving styles affect the probability of acceptance: if the driver anxiety increases, the probability of acceptance decreases and if the angry component increases, the probability of acceptance increases. some examples of these gapacceptance behaviors are reported in fig. 3, separately for male and female drivers. table 4 shows the results corresponding to the best one, indicated as ga l . the ga l model includes, as explanatory variables, the size is of the time interval (gap or lag), the gender of the driver (represented by a dummy g, which takes the value of one in the case of a male and zero in the case of a female driver), the driver's values of angry/hostile (h) and anxious (a) driving styles, as derived from the questionnaire. the other driving styles (reckless/careless and patient/careful) are not included in the model because they are not significant."
"the chromosomes store the information required to produce an entire living organism in its gigantic structural component, called the dna, constituting sequence of four nucleotide bases. the dramatic decrease in the cost of genome sequencing, in recent years, has rapidly increased the amount of genome being sequenced and deposited to the public repositories. after sequencing, researchers are primarily interested to annotate chromosomal elements (like genes) to construct a physical map of the genome elucidating the structural boundaries of its annotated elements. beside defining the structure, annotations can provide meaningful insights into understanding the genomic variations within and between species through genome comparisons."
where and are the solution to the linear system (6) . the kernel function that is often utilized is radial basis function (rbf) kernel. description of rbf kernel is given as follows:
"the predictive ability of the two models has been tested by means of the roc curve analysis [cit], a method used in various research fields for evaluating and comparing the discriminatory power of models having binary outputs [cit], including logit and fuzzy models [cit] . few examples are found in the transportation case [cit] ."
"another difference is that verbs expressing continuous events the process are, in general, reversible, which is often represented by verb pairs such as push-pull, heat-cool, expand-shrink, etc. in contrast verbs for discontinuous transitions are not reversible: a stick that has been broken cannot be made \"unbroken\" and a child who is born cannot be \"unborn.\""
"currently, genomes are usually viewed in interactive web-based genome browser applications like jbrowse [cit] . there is, however, a paucity of non-web based independent tools capable of generating publication-ready interactive visualization and annotation of genomes. chromomap, developed as an r [cit] package, is capable of generating interactive graphics visualization of entire chromosomes or chromosomal regions of any living organism facilitating annotation of thousands of chromosomal elements on a single plot. the humongous genome size of certain species, including our own, poses practical graphical challenges of displaying whole chromosomes within the available canvas space suitable for publications. the chromomap, in order to surmount this restriction, render chromosomes as a continuous composition of loci. each locus, consisting of a specific genomic range determined algorithmically based on chromosome length, displays the detailed information about the annotation in that region as a tooltip, thereby, imparting interactivity to the plot."
on our event model the distinction comes out very naturally: manner verbs refer to force vectors of events while result verbs refer to result vectors. another way of expressing this is to say that the manner/result distinction is basically a cause/effect distinction: manner verbs refer to causes and result verbs to effects.
"as explained, an ordering is essentially a one-dimensional structure. in contrast, the space of forces will often have more dimensions. the constraint nevertheless applies, because a force vector will select a direction, and along this dimension, forces can be ordered according to their strength. and if the counterforce is constant, the application of such a directed force should also select a direction in the result space. thereby, the relation between forces applied in a specific direction and the resulting effects becomes a relation between ordered structures, and the constraint applies: stronger forces applied in the selected direction lead to stronger results in the enforced direction."
"motion events are fundamental for our understanding of the physical world. yet there are systematic discrepancies between our intuition of motion and its physical reality [cit] . these discrepancies reveal how we conceptualize events. in particular, naïve intuitions of motion tend to follow the scholastic notion of impetus developed by medieval natural philosophers in the aristotelian tradition. the impetus is thought of as keeping the object moving after it has separated from the source of the force. according to the conception, objects slow down and finally stop because the impetus gradually diminishes [cit] . by ignoring the counter-forces of resistance, this view preserves a simple monotonic relation between impetus and speed: a diminishing speed is explained by the diminishing impetus. even when individuals consider counter-forces such as friction, these are in turn interpreted as having a monotonic effect: more effort generates greater results, but greater resistance reduces results. [cit] has labeled such monotonicity properties \"ohm's phenomenological primitive\" claiming that this primitive is a basic building block of our intuitions about physical reality."
"some individual objects, in particular people and places, are given names in language. in contrast individual events are seldom represented in language. exceptions are mainly historical events, e.g., the flood, the french revolution, the second world war."
"the participant laughed immediately (up to 3s) after his/her own complete utterance short confirmation participan's laughter immediately (up to 3s) after a confirming 'mm', 'i see' or something comparable by himself or his conversational partner laughter participant's laughter after a conversational partner's laughter. with an interval of less than 3s before utterance participant's laughter after a short pause (at least 3s) in conversation, but immediately (up to 500ms) before an utterance by him/herself situation laughter occurring during a pause in conversation (at least 3s), not followed by any utterance. the laughter is attibuted to the general situation and not to an utterance ment that immediately precedes laughter is the actual trigger for a laugh, and it is not \"amusing\" in itself (i.e. it is a \"banal comment\"), it doesn't necessarily entail that the laughable is not humourous. the funniness might arise from the \"banal comment\" in relation to the previous utterance, the context of the interaction, shared experiences between the speakers, world knowledge and cultural conventions. for example, in (1) \"what's funny\" resides in the implicit content that the utterance refers to. in (2), the preceding utterance is funny only in relation to the context."
"a general rule seems to be that counterforces are made explicit if and only if they prevent, obstruct or diminish the result expected from the application of the force under ordinary circumstances, or if they have the capacity to do so and therefore need to be overcome [cit] . linguistically, such breaks of expectation are typically expressed by 'but' constructions (gärdenfors, 1994) : \"i pushed (manner) the coffin as hard as i could, but it did not move (result).\" sometimes, also facilitating forces can be expressed when they amplify the result beyond what is normally expected. in both cases, the additional forces move the result vector away from the prototype."
"human cognition does not seem to be plagued by the frame problem that has dogged the symbolic approach to ai, i.e., the problem how to describe the effects of actions succinctly without having to represent also all the trivial side effects or obvious noneffects of those actions. rather, framing is a basic condition for perception and knowledge as such [cit] . apparently, humans use certain general principles to automatically infer (non-)consequences of actions, and those then no longer need to be represented explicitly. we believe that in this paper, we have identified three such principles, monotonicity, continuity and convexity."
"moreover, the three properties allow some natural forms of aggregation of elementary events. monotonicity allows the reduction of multiple causal mappings into composite ones. for example the overarching event that somebody dies because he eats too much is a composition of the monotonicity of eating more leads to an increase in weight, which leads to increased coronary problems, which in turn leads to a higher probability of dying. thus composition of events is facilitated by monotonicity. convexity leads to a different types of aggregation, based on the creation of (convex) categories of events such as walking or running [cit] -as long as motion falls within a convex category, no changes in the motion component will be detected across elementary events. continuity entails that even if a new event is initiated, many features of the previous event, such as the location of objects, do not change drastically [cit] ."
"to enhance the similarity of the virtual scenario with the real one, the sequence of vehicle arrivals observed in the field has been reproduced at a point placed 500 meters upstream of the intersection (generation point). the three-dimension rendering software created the vehicle stream at the generation point out of the view of the test driver, and removed them from the simulation at the destination point, which was placed downstream of the intersection sufficiently far away, so that drivers were not able to see vehicles at the time when they were removed from the simulation."
"as we shall elaborate in section \"applications, \" verbs expressing continuous events can be modified by adverbs that serve to express the degree of the effort or force applied. no such modification, however, is possible for verbs that express discontinuous phase transitions. such verbs typically express the transition between only two possible states, that before the force is applied and that resulting from its application. before you smash the window, it is intact, and after you have smashed it, it is broken. neither state is considered to be amenable to a degree of quantification. this is a binary situation. in contrast, for continuous events, a continuum of possible degrees and states is assumed."
"our analysis mainly focuses on the distinction between self and partner produced laughables, and between speech laugh and laughter bouts, presented separately below. due to space constraints, the effect of the rest of the tiers are not discussed."
"as regards the convexity constraint, gärdenfors (2000 gärdenfors (, 2014 proposes that concepts are represented by convex regions in conceptual spaces. in particular actions can be represented as convex regions of patterns in force space (gärdenfors, 2007; gärdenfors [cit] ) . now suppose that you hear or read the sentence \"victoria hit oscar.\" since you have no further information about the event, you interpret \"hit\" as a prototypical instance of hitting, i.e., close to the center of convex and continuous region of force space (or more generally action space) representing hitting. and from previous experience you know that this center is mapped into the interior of a region of pain space, for instance close to its center. in other words, you infer that hitting oscar, or any other person for that matter, with a prototypical force, i.e., one located at the center of the force region, causes him pain. since the pain region is also convex, its center prototypical area is located away from its boundary. then, if the mapping from the force to the pain region is continuous, you assume that when oscar is hit by victoria the force is close to the prototypical one and you therefore conclude that the result will also be close to the center that part of the pain region."
"in this article, we invoke general mathematical structures [cit] in order to develop this geometric perspective by analyzing three important properties of event mappings that constrain the structure of the event and support fundamental cognitive uses of event concepts. we focus on three cognitive processes that are crucially involved in thinking about events: causal thinking, control of action and learning by generalization. these cognitive processes are supported by three corresponding mathematical properties: monotonicity (that we relate to qualitative causal thinking); continuity (that plays a key role in our activities of action control); and convexity (that facilitates generalization and the categorization of events). we argue that these constraints play a central role in the 'working model' of an event [cit] . in mathematical terms, monotonicity rests on an order structure, continuity is a topological notion depending on a notion of proximity, and convexity is a geometric concept, utilizing the notion of betweenness."
"also, many optimization problems appear to be nonmonotonic in nature. in economic optimization, the profit achieved is the product of the price per unit and the number of units sold. the latter, however, typically is a decreasing function of the price per unit. thus, non-monotonicity here results because a monotonically increasing function is multiplied by a decreasing one. that is, in order to see the monotonicity, we need to break up the effect into more elementary constituents."
"this section dedicates in describing the proposed prediction model, named as flsvr tsp, in detail. the establishment of the model (see figure 2 ) is accomplished by a fusion of the ls-svr and fa algorithms. the flsvr tsp employs the ls-svr as the supervised learning algorithm for mining the implicit patterns in the series. furthermore, the fa, an evolutionary optimization algorithm, is utilized to automatically identify the optimal values of tuning parameters. the construction of the prediction model is dependent on a set of tuning parameters. the embedding dimension ( ) is needed in the state reconstruction process. the regularization parameter ( ) and the kernel function parameter ( ) are required for the flsvr tsp ."
"3. border control: one participant plays the role of a traveller attempting to pass through the border control of an imagined country, and is interviewed by an officer. the traveller has a personal situation that disfavours him/her in this interview. the officer asks questions that are general as well as specific. in addition, the traveller happens to be a parent-in-law of the officer."
"studies about laughter in interaction have been mainly focused on the acoustic or perceptual features, and often observations of the events preceding to it have been the base for claims concerning what laughter is about. [cit] ) made a claim that has been subsequently adopted in much of the literature: laughter is, for the most part, not related to humour, because it is found to most frequently follow banal comments. similar reasoning has been adopted by several other studies on the kind of situations that elicit laughter. the deduction process in these studies rely on an important yet untested assumption: what laughter follows is what it is about. our paper investigates this assumption. we first briefly discuss previous studies on laughter in interaction; we then argue for a semantic/pragmatic account in which we treat laughter as a gestural event anaphora referring to a laughable. we present a corpus study of laughables and evaluate our results against previous proposals."
"(3) state reconstruction. with the embedding dimension being specified, the time series is transformed to the input matrix and the desired output vector (see (1)). after being transformed, the data is used for the ls-svr's training process. the fa searching. the regularization parameter ( ) controls the penalty imposed to data points that deviate from the regression function. meanwhile, the kernel parameter ( ) affects the smoothness of the regression function. it is worth noticing that proper setting of these tuning parameters is required to ensure desirable performance of the prediction model."
"our account suggests that resolving the laughable is crucial for deriving the content of a laughter event. we hypothesize that laughter is not always adjacent to its laughable. rather, the sequential distribution between laughter and laughable is somewhat free, illustrated in figure 2 . we hypothesize that laughter can occur before, during and after the laughable, and that it is possible for intervening materials to occur between a laughter event and its laughable. in more detail, we make the following hypotheses in relation to our research questions: q1: does laughter always follow its laughable?"
"the aim of the current study was to deepen the little research available on the relation between laughter, laughable and speech in natural conversation, starting from the observation of their temporal sequence and alignment. we investigated three questions: whether laughter always follows, or at least is adjacent to its laughable, as is commonly assumed; whether this sequential alignment differ depending on differeht \"types\" of laughters; and whether laughter always punctuates speech. our main findings are:"
"monotonicity can also support the reverse inference process. it has been observed that \"an effect is attributed to the one of its possible causes with which, over time, it covaries\" [cit] . when needing to identify causal factors among multiple potential ones, monotonicity can provide a powerful selection criterion, that captures an ordinal covariation principle. for example, the tides have been observed as long as humans have existed, but it was only when the monotonic correlations to the moon's position and distances was discovered that we understood the force vectors causing the tides."
"we suggest that our three constraints may characterize the internal structure of working models and enrich the notion of what constitutes a \"predictive error, \" that is, a surprise. we thus consider our proposal as complementary to theories of event segmentation."
"(5) fa searching. the fa automatically explores the various combinations of the tuning parameters ( and ). at each generation, the optimizer carries out the mutation, crossover, and selection processes to guide the population to the optimal solution. by evaluating the fitness of each individual, the algorithm discards inferior combinations of and, and allows robust combinations of these parameters to be passed on the next generations."
"again, the constraint of continuity extends well beyond the domain of physical events. in some types of phenomena, like most economic ones, assumptions of continuity are very common. for example, most market events commonly satisfy the constraint of continuity. however, there may be cases in which straightforward analysis can be challenging due to the heterogeneity of the factors involved. for example, historical events present a mixture of continuity and discontinuity along all dimensions of social structure. events such as revolutions [cit] are highly multidimensional events where cascading transformations of many dimensions, located in different moments, coexist with the continuity of other domains. indeed, the choice of the scope of discontinuities is often used to draw the boundaries of the event itself [cit] . it is striking that most of the time such complex events are expressed through their projection on simple (and often metaphoric) representations in which the continuity constraintor its violation -carries important structural information about the most salient dimensions. these metaphoric representations usually project these complex multidimensional events on simple physical domains where prototypical notions of continuity and discontinuity are better understood (e.g., the fall off the roman empire). the use and abuse of such metaphorical projections provides, in our view, strong support to the cognitive cogency of the continuity constraint and its physical prototypical sources."
"second, violations of three constraints provide a clear source of surprise that may lead to the update of a working model. imagine that i am telling you that i made a trip from venice to paris. then, for example, going to lyon and then back to milan before flying to paris would violate monotonicity. similarly, a long stopover at milan would create a loss of continuity. violations of convexity can also generate surprise. for example, if i am mentioning a trip from venice to paris, it would be surprising to find that the trip goes through moscow."
"(6) fitness evaluation. in the flsvr tsp, in order to determine the optimal set of tuning parameters, the following objective function is used in the step of fitness function evaluation:"
"2. laughter-laughable alignment may differ depending on the different \"types\" of laughable and laughter. specifically, laughters about a partner-produced laughable (audience laughter) start later than those about a self-produced laughable (speaker laughter). speech laughs occur earlier than laughter bouts, and overlaps more with the laughable."
"the mathematical structure of a topology turns a set into a space. a set simply is the collection of its elements, and as such it is structureless and amorphous. a topology on a set introduces a notion of nearness or vicinity. a topology on a set distinguishes certain of its subsets as \"open.\" this is meant to indicate that an open set is a neighborhood for all its members, in the sense that for each element, it also contains the elements surrounding it."
"when throwing an object, the distance achieved is a nonmonotonic function of the horizontal angle. but this does not contradict our general principle that the effect is a monotonic function of the force applied. for the same angle, the distance increases as a function of the force with which the object is thrown."
"the dataset obtained from the collected data contained a total of 4,384 decisions (gap/lag acceptances and rejections), where 1,914 gaps/lags correspond to acceptances (right turn maneuver completed). the average number of decisions per drivers' approach to the minor street stop line was 2.29 (during a test the same driver approached the intersection about 10 times and each driver made at least four tests during the experiment). a summary of the data collected during the experiment is shown in table 2 . with reference to mdsi, a summary of measures (based on a six level scale of assessment) of test drivers' driving style is presented in table 3 . the identification/estimation of both fuzzy and logit models has been carried out using the stratified holdout approach [cit] . the full dataset has been divided in a calibration dataset (70% of data) and a validation dataset (30% of data), to identify/calibrate the models and evaluate their performances, respectively. this procedure allows to measure correctly the predictive capabilities of the models, because it is well known that using the same data for estimation and validation could lead to an optimistic evaluation of model performance. data were randomly sampled from the full dataset, maintaining approximately the same proportion of output classes (acceptance and rejection), type of intervals (gap and lag), and gender of drivers (male and female)."
"the sample of participants was composed by 39 drivers, approximately 80% males and 20% females. drivers were students, staff of the university and other people having the following characteristics:"
"in which, as expected, the acceptance probability increases with the interval size is. gender has an influence in the sense that male drivers seem to accept smaller intervals than female drivers. also driving styles affect the probability of acceptance: if the driver anxiety increases, the probability of acceptance decreases and if the angry component increases, the probability of acceptance increases. some examples of these gap-acceptance behaviors are reported in fig. 3, separately for male and female drivers."
"the three qualitative constraints that we have presented here are organizing principles that govern the mapping from causes to results. they allow general and robust inferences independently of the counter-forces that are present and of the initial conditions of the action. each qualitative constraint leads to a different type of inference, extrapolation in the case of monotonicity, interpolation for convexity, and effect control for continuity. we view them as principles that considerably improve the cognitive economy of our causal thinking. like all cognitive simplification strategies, they have their limitations, and there do exist cases where they don't apply. we expect, however, that such cases require a higher mental effort than those where the principles apply. for instance, boomeranging is a very surprising phenomenon."
"in future work, we will study to what extent laughter-laughable alignment differs by the function/effect of laughter, and what the limit is for the \"free\" alignment. this work may be useful for dialogue systems which allows a computer agent to generate laughter at appropriate times depending on the type and location of the laughable."
"the three mathematical structures that we have presented and their corresponding constraints on the mapping from force to result vectors generate a lot of inferences about events and event categories. consequently, the structures that depend on events will be constrained by these inferences. we therefore believe that investigations of cognitive representations of events will be very fruitful in a range of domains."
"the notion of continuity in definition 2 is purely qualitative, basically saying that points that are mapped onto nearby points in y are also nearby in x. in order to make it quantitative, one needs a metric, that is, a function that assigns to any two elements of a set their distance. this brings us into the realm of geometry."
"event segmentation theory [cit] explains event segmentation as the result of updates in working models which respond to increases in predictive error of a current working model. according to the model, event boundaries correspond to such update activity. this entails that as long as the content of the working model does not violate the developments in the world, an event is continued."
"our results discredit the method of inferring what the laughter is about by looking at the elements that immediately precede or follow it. therefore, previous conclusions using this method should be revisited [cit] . one such conclusion is that because they follow \"banal comments\", laughter is mostly about not about funny stimuli. we have shown that the logic does not hold, as very often, those preceding \"banal comments\" are not the laughables. and even if they are, the \"funniness\" or incongruity may reside between the laughable and something else, e.g., the context of occurrence, world knowledge, cultural norms, experiences, informational and intentional states shared between interlocutors. for example, in the following exchange, the exchange seems rather banal, but in fact, they are laughing about the exophoric situation that they are acting. exactly what proportion of laughables contain funny incongruity is a topic for further research. for now, our results questions the validity of existing proposals on this score."
"the fuzzy gap-acceptance model (ga f ) has been determined from experimental data using fispro, an opensource software available for free on the internet [cit] . the membership functions of the premise and consequence fuzzy sets are identified based on k-means algorithm results, and the rules of inference with the socalled fpa (fast prototype algorithm [cit] )."
"(1) input data. the flsvr tsp takes a univariate time series as its input. the data can be recorded at regular time interval, for example, daily, monthly, and quarterly, and so forth. the whole data set is divided into training set, validating set and testing set. in our study, the ratio of the validating set to the training set is 1/5."
"next we come to the principle of continuity. many verbs represent force or result vectors of events in which there is a continuous mapping between the two types of vector, for example, \"heat, \" \"stretch\" and \"press.\" as will be explained in section \"manner and result verbs, \" there are verbs that express violation of such continuity such as \"cut\" or \"break\" [cit] . such verbs typically express the transition between only two possible states, that before the force is applied and that resulting from its application. a notable difference is that verbs expressing continuous events can be modified by adverbs that serve to express the degree of the effort or force applied. no such modification, however, is possible for verbs that express discontinuous phase transitions. for example one can \"press strongly\" (until something breaks), but one cannot \"break strongly.\" of course, such transition verbs can be modified by adjectives addressing other dimensions. for instance, one can \"break the window intentionally, \" but already \"breaking the window violently\" requires some special context to be a viable expression."
"psychological research on events has dedicated much attention to how complex flows of activities are segmented into simpler event entities by readers, observers or even direct participants [cit] . our three constraints do not (and cannot) constitute a theory of such event segmentation, as we are here concerned with the structure of elementary, \"atomic\" events rather than with more complex event architectures. however, the three constraints may indirectly contribute to a better understanding of the segmentation process."
"monotonicity also enables qualitative causal inferences. first of all, it allows making purely ordinal inferences about the relationship between causes and effects. for example, while different individuals may react with different intensity to a medical treatment, making quantitative predictions difficult, one can still predict that larger doses of the treatment will have larger effects, independently of the person that is treated. mill (1843) called this form of causal inference \"the method of concomitant variations.\""
"in addition to our earlier work on event structure, we have in this paper introduced three general cognitive constraints on event mappings and shown how they can be translated into well-known general mathematical properties of mappings. the mathematical properties have broad implications for how we reason about events and for the semantics of natural language. our approach unifies several areas since it brings out what is common to causal thinking, control of action and learning by generalization."
"our general thesis for cognitive processes dealing with events is that they are fundamentally guided by some formal principles that are based on general mathematical structures. the structures that we identify here are monotonicity (order), continuity (topology), and convexity (geometry). we shall now describe those structures in turn and explain how they lead to principles that constrain the mapping between forces and results in the event model. [cit] ."
"the mapping between the force space and the result space is what characterizes the event. for example, pushing a cupboard sometimes results in the cupboard moving, sometimes not; shooting a moose sometimes kills it, sometimes not. in such cases the mapping between the force vector and the result vector qualifies two different events."
"the three constraints involve different mathematical structures, as will be described in detail in the next section. therefore, neither of them can be derived from the others. here are also some simple examples that they are independent of each other when constraining events. periodic motions are recurrent and therefore not monotonic (although their speed will typically depend monotonically on the force exerted), but they are usually continuous. the example of a boomerang just mentioned is a more exotic case. -result verbs usually describe non-continuous effects, called phase transitions in physics. at the critical point, exerting just a little more force may break a window. monotonicity holds, since utilizing a stronger force on another window will also break it. convexity also applies in the sense that when two force magnitudes each suffice to destroy a window, any intermediate one will do as well. if one stretches a spring, the elastic energy acquired will depend on the force exerted within a certain range, and in that range, convexity holds. when the force exceeds a critical threshold, however, instead of an elastic, we get a plastic deformation, and monotonicity ceases to hold. more abstractly, while we may still interpolate within a tested range of validity of a phenomenon, where convexity holds, we may not be able to extrapolate beyond that range, and a prediction based on a monotonicity assumption outside that range may not be valid. whether or not a discontinuous phase transition occurs in such situations, that is, whether continuity holds or fails, is another matter."
"in linguistics, it is common to distinguish between manner verbs and result verbs (see e.g., [cit] . according to rappaport hovav and levin (2010, p. 21 ) \"manner verbs specify as part of their meaning a manner of carrying out an action, while result verbs specify the coming about of a result state.\" result verbs group together verbs describing motion with verbs that describe property changes because of the tendency to give the same linguistic construction to a changing entity as to a moving one: both involve changes of properties, which the manner verbs do not. the distinction is supposed to be exhaustive [cit] : any particular use of a verb is either a manner verb or a result verb (but not both)."
"notice that not all speech laughs overlap with the laughable, suggesting that often, laughter that co-occurs with speech is not about the cooccurring speech (47.8% in french and 30% in chinese). in the following example, speaker b says that she'll take the bigger bedroom, and laughs. speaker a joins the laughter but starts a new utterance."
"in section \"a two-vector representation of events, \" we briefly summarize the building blocks of a geometric model of events, the space representing the forces generating the events, and the result space representing changes induced by such forces. an event is specified by the mapping between these spaces. in section \"three qualitative principles for events, \" we describe the three properties that constrain such a mapping. these constraints are supported by research on naive physics, qualitative causal reasoning, action perception, psychology of motor control, and cognitive linguistics. in section \"mathematical correspondences of the constraints, \" we define the mathematical correspondences to the constraints. sections \"event structure and semantics\" and \"manner and result verbs\" explore the implications of such constraints for semantics. our paper does not directly focus on the problem of event segmentation that has been in focus in much of research on events (e.g., [cit] ) . however, we discuss the relevance of the three constraints for this area in section \"contributions to working models and segmentation.\" we conclude by a discussion that relates our approach to some other accounts of events."
"while monotonicity is easily defined in one dimension, it is a more problematic notion when multiple dimensions are involved. changes in the force vector may involve different directions of change in different dimensions and then monotonicity may not hold. either process is monotonic, but since they take place along different dimensions, they are not naturally combined into a single monotonic process. consequently, the monotonicity constraint can be expected to apply only when the changes go in the same direction. in the case of causal inference, this common direction is called \"qualitative synergy\" [cit] ."
"we found that laughters about a partnerproduced laughable start later than those about a self-produced laughable, but still the average starting time is before the end of the laughable. with partner-produced laughables, the average gap between the end of laughable and start of laughter is -0.02s in french and -0.3s in chinese, while with self-produced laughables, the average gap is -0.7s in french and -1.3s in chinese."
"therefore, the purpose of this study is to fuse the ls-svr and fa techniques to establish a new hybrid ai model for prediction of time series. our research goal is to build a model that possesses the capability of delivering accurate as well as operating autonomously without human interference. the second section of this paper reviews the methods needed to accomplish the research objective. in the third section, the framework of the proposed flsvr tsp is described in detail. the fourth section demonstrates the experimental results. conclusion on our study is mentioned in the final section."
"in our data sample (summarized in table2), laughter is very frequent, constituting 17% of the conversation duration in french and 7.2% in chinese. each laughable is \"laughed about\" more than once (1.7 times in french and 1.4 times in chinese)."
"it has been suggested (notably by provine) that laughter bouts almost never (0.1%) disrupt phrases but punctuate them [cit] . he explains this finding on the basis of an organic constraint: laughter and speech share the same vocal apparatus and speech has \"priority access\". curiously enough, provine has always excluded speech-laughs from his investigations, without any justification. a more recent study on laughter in deaf asl signers [cit] showed that signers rarely laugh during their own utterances, where no competition for the same channel of expression is present. provine and emmory conclude that the punctuation effect of laughter holds even for signers, and possibly is not a simple physical constraint that determines the placement of laughter in dialogues, but due to a higher order linguistic ordered structure [cit] ."
"the fuzzy output variable \"acceptance\" is defuzzified using the centroid method [cit] obtaining an \"acceptance index\" of a certain gap/lag. using the \"acceptance index\", it is possible to build \"acceptance curves\" that allow to use the model as predictive tool (and to validate it over the validation sample). when a gap/lag of a certain size has an acceptance index greater than or equal to the 0.5 threshold, it is considered \"acceptable\", otherwise it is considered \"unacceptable\"."
"also the other performance metrics obtained by both models in correspondence of the cut-off value of 0.5 (table 5) are similar, and the two models are equivalent in terms of accuracy."
"we investigated whether laughter occurs at utterance-medial positions when one party is speaking, and when the partner is speaking. does laughter interrupt partners' utterances? yes. we found that 51.8% of laughter bouts in french and 56.7% of laughter bouts in chinese start during the partner's utterances (not necessarily laughables), for example:"
"the basic idea of roc curve analysis may be explained by considering an experiment with only two possible outcomes, 1 and 0, that are denoted as positive and negative outcomes. in the ga l and ga f models the two outcomes are the acceptance (positive) and the rejection (negative) of a certain gap/lag, therefore four cases are possible: the discriminatory power of the models increases as both tpr and tnr increase. the roc curve describes the relationship between tpr, also called \"sensitivity\", and (1-tnr), also called \"1-specificity\", for all possible classification thresholds. since the \"1-specificity\" is the fpr, the roc curve describes the relationship between the \"percentage of hits\" and the \"percentage of false alarms\" obtained with the model."
"notably, constructing a predictive model for time series forecasting is a challenging task. it is because real-world time series data are often characterized by nonlinearity, being nonstationary, and irregularity [cit] . random noise and effect of unidentified factors are the main causes that degrade the prediction accuracy. moreover, in most cases, the underlying model that generates the series is unknown and the process of discovering such model is oftentimes hindered by the stochastic nature of the time-dependent data [cit] . particularly, for each time series, determination of a suitable embedding dimension is also of major concern [cit] . therefore, these challenges necessitate the development of advanced approaches."
"our view of the role of causality in event structure is much closer to the one that has emerged in cognitive semantics. like our approach, this tradition puts causality inside the event. it also emphasizes the paradigmatic function of two thematic roles, agent and patient, which are clearly connected to our two spaces [cit] ) . [cit] presents a geometrical model that shares many features with ours. however, a difference from our approach is that croft does not develop the force aspect of the event [cit] b) . we believe that by taking into consideration the three constraints that have been analyzed in this paper, linguists will be able to explain several semantic aspects related to events, in particular in relation to the semantics of verbs. this may help to reconcile the two lives of events."
"convexity turns out to be of great importance whenever a problem of categorization or classification of events is at stake also in complex action domains such as those object of the legal domain. the domain of law provides important examples of application of convexity principles to the interpretation of events that do not have just a physical nature. for example, in the contract law, it is often important to determine which type of event happened, in order to know whether a contract has been fulfilled or not. in all these cases, defining a convex range of performances (acts and acts outcomes attributes) within which a given performance happens determines the attribution of the 1 although the notion of convexity can be generalized to non-metric spaces, as long as betweenness can be defined. event to one type or the other (for a classical law school case, see frigaliment vs. bns)."
"we argue for a different explanation. assuming speech laughter data (laughter that overlaps utterances) were not excluded in the asl study as they were in spoken dialogue studies, in deaf signers, since the laughter is perceived only visually and involves marked facial movements, it would interfere with the perception of the message conveyed by language. in sign languages, body and face movements constitute important communicative elements at all linguistic levels from phonology to morphology, semantics, syntax and prosody [cit] . despite the fact that emotional facial expressions can overlap with linguistic facial movements [cit] ), a laugh, implying a significant alteration of facial configuration (see identification of a laughter episode) could be excessively disruptive for the message aimed to be conveyed. while in verbal language the laughter signal can be completely fused in the speech as a paralinguistic feature [cit] and used in a sophisticated manner to enrich and facilitate communication, [cit] report that not even from an acoustic perspective is laughter secondary to speech: when co-occurring the laugh indeed does not resemble the speech spectral patterns nor does the speech resemble the laughter ones, but together they create a new idiosyncratic pattern. laughter is fully meaningful and communicative in itself, universally across cultures, and the emotional components that it carries are not secondary to speech or trivial."
"two main issues in such debates are lexical aspect (aktionsart) and causality. we have not discussed aspect [cit], because it does not involve directly the mapping between the action and the effect spaces. however, we suggest that the typology of aspects follows more coherently from an ordinal treatment of time as a succession of (sub)events rather than by locating events on a euclidean time line. indeed, considering aspects in terms of bounds over the timeline (open/closed induces the telic non-telic distinction, while point vs. interval induces the instant/duration distinction) generates an anomaly: semelfactives, which are simultaneously non telic and instantaneous, would have to correspond to \"open\" (unbounded) points, which don't exist in the standard topology of the line. on the other side, considering only ordered sequences of discrete time points allows to define duration as multiple points vs. instantaneous as single point, and telic vs. non telic as having or not having an end point. in this case a semelfactive just corresponds to a point which is not an end point [cit] for a more detailed analysis of aspect)."
"when the monotonicity property can be assumed to hold, we can profile an event by either a manner or result verb without much loss of information. for example, when we say we pushed a table the default assumption is that some effect was produced, proportional to the intensity of the force exerted. conversely, when using a result verb the default assumption will be that a force in the same direction was exerted by the agent (\"i moved the table to the next room\"). however, when such an assumption breaks down, we need to provide a more complete profile of the event, typically by introducing one more verb or suitable modifiers."
"we hypothesize that laughter can occur before, during or after the laughable; laughter and laughable should not have a one-to-one relationship: one laughable can be the referent of several laughter events."
"the time boundaries were marked, the content (whether verbal or not) was annotated and an index was assigned in order to map laughter (or multiple laughters) and laughable. laughables were classified according to three main categories: described, metalinguistic and exhophoric event. reliability of type assignement was assessed by having a masters student as a second coder for 10% of the material observed. percentage agreements between the two coders for french and chinese averaged 92.5% with a krippendorff α [cit] of 0.77."
"after these general remarks about the role of events in semantics, let us now turn to the role of monotonicity, continuity and convexity in semantics."
"even though the mental model of an event may be complex, a sentence normally captures only certain features of a construal generated from a particular focus on the event [cit], ch. 3; [cit], ch. 3, for a survey). by analogy with the visual process -where we can only focus our attention on some features of the visual field -a construal focuses only on certain parts of an event. the sentences \"victoria hits oscar\" and \"oscar is hit by victoria\" describe the same event with the aid of two different construals, where victoria and oscar, respectively, are put in focus. gärdenfors (2014, p. 177) proposes that a construal of an event contains as least one vector (force or result) and one object (patient or agent). then he puts forward the thesis (ibid., p. 178) that a (declarative) sentence typically expresses a construal of an event."
"obviously, not every relation between cause and effect is monotonic. we only claim that the relation between the exerted force and its effect is typically monotonic. to understand this better, let us discuss the example of rotations. here, the motion is obviously non-monotonic. the only monotonic relation is that between the exerted force and the angular velocity. in fact, the mental effort involved in the processing of rotations is an increasing (approximately linear) function of the rotation angle, see e.g., [cit] . no such phenomenon seems to occur for translation movements. this indicates that the non-monotonic nature of rotations as opposed to that of translations requires an increased mental effort -which, by the way, is monotonically increasing itself."
"when the outcome of an action is very hard to predict given ignorance of counter-forces and approximate understanding of causal laws, the effect of actions can still be understood in qualitative terms. a very general constraint is that, whatever counter-forces and other forces are present in a given situation, increasing the action force will also increase (or at least not decrease) the magnitude of the effect. for example the effects of pushing an object may vary widely depending on the friction of the surface. nevertheless, pushing the object harder means that it moves further (or at least does not move in the other direction). this ordinal principle implies a sort of ceteris paribus thinking: other counter-forces can be of varying magnitude and direction, but no matter what they are, ceteris paribus, the change of effect is predictable from the change of the action force."
"where is the kernel function parameter. when the rbf kernel is used, there are two tuning parameters (, ) that are needed to be determined in ls-svr. the regularization parameter ( ) controls the penalty imposed to data points that deviate from the regression function. meanwhile, the kernel parameter ( ) influences the smoothness of the regression function. it is worth noticing that proper setting of these tuning parameters is required to achieve desirable performance of the prediction model."
"considering the entire range of threshold values, ga l slightly outperforms ga f, as revealed by the comparison of the auc metric (table 5 ). nevertheless the cut-off value which maximizes the performances of both models is 0.5, in correspondence of which the two line are indistinguishable, as can be seen in the squared box of fig. 6 ."
"to investigate the time alignment between laughter and laughable, we calculated \"start of laughter minus start of laughable\", \"end of laughter minus end of laughable\", and \"start of laughter minus end of laughable\". if laughter always follow the laughable, all three measurements should be above zero. this was not the case. in both chinese and french, on average, laughter starts during rather than after the laughable, and finishes af-ter the laughable. in general, laughs in chinese are more likely to overlap with the laughable than in french. the distribution varies over a wide range. table 3 summarizes the gaps between the boundaries of laughter and laughable, and figure 3 plots specifically the gap between the end of the laughable and the start of laughter. they show that it is common for laughs to start before, during and after the laughable. when a laugh has no overlap with its laughable, they are not always adjacent to each other (average utterance duration is under 2 seconds while the gap can be up to 10 seconds). in the following example, the first two instances of speech laugh refer to a laughable in a later utterance."
"this thesis about sentences implies that at least an agent or a patient (expressed by a noun phrase) and a force vector or a result vector (expressed by a verb phrase) are included in what is expressed. thus the two main components noun phrase and verb phrase have to be present in a linguistic description of an event. the upshot is that given the event model and the thesis about construals, sentences are indeed central semantic units."
"starting from the consideration that the time interval size between vehicles on the primary street is the most important factor affecting gap-acceptance behavior (as widely reported in literature), and considering that drivers evaluate this variable in subjective terms, in this work we consider time interval as a fuzzy variable; other fuzzy variables are driver's angry/hostile and anxious driving styles. the gender of the driver is an objective factor and therefore it has been treated as a crisp variable in the model."
"the fuzzy system knowledge base so obtained was characterized by five triangular fuzzy sets in the domain of the time interval size, by six triangular fuzzy sets in the domain of the variables \"angry/hostile\" and \"anxious\" driving styles and by two \"singletons\" in the domain of the crisp variable \"gender\" (fig. 4) . fuzzy sets used for \"angry/hostile\" and \"anxious\" variables have been chosen to represent the verbal scale used by the drivers to judge their own driving style, maintaining the same level of aggregation. unfortunately, the characteristics of driving styles observed for the sample drivers are not representative of all the available combinations (table 3 ), but they are concentrated on a limited portion of the domain. as a result the rules could not be developed from data for some combinations of the input variables, in particular for high values of driving style variables."
the brightness of an individual firefly can be defined similarly to the fitness value in the genetic algorithm [cit] . the light intensity ( ) varies according to the inverse square law as follows:
"this paper has presented a novel hybrid ai model, named as the flsvr tsp, to assist decision-makers in dealing with time series forecasting. the flsvr tsp was developed by a fusion of the ls-svr and fa techniques. the ls-svr is employed to infer the input/output mapping function of time series data. meanwhile, the fa searching algorithm is utilized to identify the most appropriate set of tuning parameters. this mechanism eliminates the need of expertise or trialand-error process in parameter setting. moreover, simulation and performance comparison, for simulated and real-world time series data, have proven the aptitude of the flsvr tsp . these facts demonstrate the strong potential of the proposed model as an alternative for time series forecasting. the future direction of the current work may include improving the current model for solving multistep-ahead time series prediction and applying the hybrid intelligent model to forecast other real-world time series."
then we apply continuity to our transformation from force to result: the second constraint that small changes in the force vector lead to small changes of the result vector corresponds mathematically to that the mapping from forces to results is continuous.
"the fa is a stochastic, natureinspired, and metaheuristic algorithm that can find both the global optima and the local optima simultaneously and effectively [cit] . the flashing lights of fireflies are an amazing sight in the summer sky in tropical and temperate regions. the pattern of flashes is often unique for a particular species. in essence, each firefly is attracted to brighter ones as it randomly explores while searching for prey."
"where and denote predicted and actual value for output th. in addition, is the number of data points. the fitness function, in essence, represents the trade-off between model generalization and model complexity. it is worth noticing that well-fitting of the training set may reflect the model complexity. however, complex model tends to suffer from over-fitting [cit] . thus, incorporating the error of the validating data can help identify the model that features the balance of minimizing training error and generalization property."
"while notions of events in philosophy and science may diverge, they all agree that events involve participants and relations between them [cit] . thus events are relational concepts [cit] ). yet the nature of such relations has not been systematically analyzed."
"the continuity constraint says that small changes in force should lead to small changes of the result. the constraint is an expression of our sense of control. when we want to change the effect only by a small degree, we can achieve that by applying a sufficiently small change of effort. when we turn the steering wheel a little more to the right, we expect the car also to move a little more to the right, and not to make a u-turn. when we hit the tennis ball a little harder, it will fly a little faster and further, but not move wide out of the court. and when we boil the egg a little longer, it will become somewhat harder. large changes in force could still lead to small changes of the result, but, expressing it in qualitative terms, the continuity constraint forbids that small force changes produce large effect changes."
"similarly, [cit] ) used exclusively timing parameters -i.e., what precedes and what follows the laugh (within a threshold of 3s) -to distinguish 6 different contexts (see table 1) for laughter occurrence to support claims about situations that elicit laughter."
"while the three properties are presented in a specific model of events, they also apply to different accounts as long as they rely on some type of geometric representation. [cit], the causal models of wolff (2007 wolff (, 2008 [cit] . for example, the monotonicity constraint should apply whenever forces are analyzed in their causal role."
"to correctly reproduce real traffic flow conditions, field observations were collected during peak-hour periods (7.00-9.00 a.m.) through video camera recorder. the videos were processed using an application software that allows the user to record the primary vehicle arrival time at the conflict point (c9-2 in fig. 1 ) together with the vehicle category (e.g. car, van, truck). the data were organized in a database and then processed using a software procedure that extracts the following gap-acceptance information: time interval size and category of major street vehicle closing the interval."
"from the acceptance curves provided in fig. 5, some trends regarding the relationships among the changes in driving styles and the effects on gap-acceptance behavior are shown. for female drivers a reduction in the anxiety value (compared to the mean value) produces a reduction of the size of the minimum accepted intervals. for male drivers, an increase of the anxiety produces an increase in the size of minimum accepted intervals. these trends are similar to the results obtained with the ga l model."
"3. comparing chinese and french, the majority of the patterns are similar, except that in chinese, laughs are more likely to overlap with the laughable than in french. this provides an initial indication that while certain aspects of laughter behaviour are influenced by culture/language, generally we use laughter similarly in interaction. 3 4. laughter does interrupt speech: we often laugh when others are speaking (half of all laughter bouts) and occasionally we insert stand-alone laughters mid-sentence (less than 10%). moreover, very frequently laughter overlaps speech (around 40% of all laughters)."
"for the main analysis, we include in our analysis both laughter and speech laughter [cit] . in the current study we restrict our observations about the aspects pertaining to the form, to the contextual distribution and positioning of a laugh in relation to others' laughter, the laughable and laugher's herself speech."
"inferable from its name, the polyploidy feature allows to construct sets of chromosomes simultaneously on the same plot. this is achieved by simply passing co-ordinates files separately for each set of chromosomes and setting the argument ploidy equivalent to the number of sets being passed. each set of chromosomes are rendered independent of each other and, hence, can take sets of chromosomes that differ in size and numbers. an interesting plausible application of this feature is to visualize cross-species chromosome sets that can be advantageous for their comparative studies. figure 1 describes the use of this feature to annotate orthologous genes in rice and maize conspicuously depicting the notable difference in their genome sizes and chromosome numbers. polyploidy visualizations can be achieved by using the following command: figure 1 visualizing orthologs in rice and maize using chromomap. five randomly selected sets of orthologous genes, shown as distinct colors, were fetched from ensembl plants database [cit] . the interactive version of the plot (supplied in supplementary files) shows each gene identifier name on hover. each locus, in this case, shows a single gene since it's the only gene annotated in that locus range."
"monotonicity applies also to non-physical types of events, of a more social or moral nature. for example, more unfair offers will cause more resentment. again, monotonicity makes it easy to compose a chain of causal structures: more greed will cause more unfair offers that will cause more resentment, which can be contracted to \"more greed will cause more resentment.\""
"the relatively free alignment between laughter and speech seems analogous at a first approximation to the relation between manual gesture and speech [cit] . we propose to consider laughter as a verbal gesture, having an independent channel from speech, with which it communicates through an interface."
"given the unknown nature of the counter-forces, the direction of effect need not be the same as the direction of the force vector. for example an unknown side wind may effect the direction of the movement of a motorboat, so that the direction of the movement driving force of the motor may become oblique in relation to the driving force of the motor direction of the movement [cit] . still the general monotonicity constraint is valid: if the force of the boat motor is increased, the speed of the boat will increase."
"this abstract common structure of events in language, physical thinking and planning can be naturally interpreted in geometric terms. the structure can be modeled as a mapping between vector spaces. the action and the result components of the event can then be represented as vectors in the two spaces."
"in the case of events, the convexity preserving constraint allows to generalize over sets of events and thus to form event categories. the convexity preserving constraint therefore has a fundamental cognitive property: it allows moving from categories of action and categories of result to categories of events. more precisely an event category is a convexity-preserving mapping from categories of action to categories of result. for example, in a \"jump\" event, both action and result are implied. on the force side, a pattern of forces with a vertical component (and gravity as a counterforce). on the result side, a movement with a vertical component. convexity in this case implies that given two jumps, a jump which is between those in terms of force exerted and direction of the movement will be also called a jump. of course, events may in general imply too complex mappings between force and result to be captured by a single word, in which case they can be expressed by composing a force and a result expression, like in \"driving into town.\""
"the paper is organized as follows. section 2 is dedicated to a brief description of the laboratory experimental design. section 3 describes the estimation of the proposed logit model, section 4 describes the identification of the proposed fuzzy model. section 5 deals with the comparative roc curve analysis of the two types of gapacceptance models. concluding remarks are presented in section 6."
"we consider as the laughable the event which, after appraisal, produces a positive psychological shift in the laugher. we distinguish three different kinds of laughable types: described events, metalinguistic stimuli and exophoric events. we also mark whether they originated from the laugher him/herself or by the partner."
"recently, there has been a strong growth of models of causal reasoning in terms of bayesian networks. this approach has normative value, but it makes too strong claims about people's ability to reason about probability distributions and to compute bayesian updates. [cit] claims that \"people represent the qualitative structure of causal systems without accurately representing all quantitative details\" [cit] . in support of this, he notes that causal reasoning makes use of the monotonicity property: \"on the flip side of the qualitative/quantitative divide, we do have access to some quantitative knowledge. we know that stepping harder on the accelerator pedal turns the wheels more than stepping lightly\" [cit] argue that people represent more aspects of the mechanisms of causality than the bayesian formalism can encompass. the upshot is that we do not consider the bayesian approach to be cognitively realistic. our point is rather that the use of the general principles of monotonicity, continuity and convexity obviates the need for much of bayesian reasoning."
"our general theoretical hypothesis is this: events as such can be arbitrarily complicated. to get a mental grip on them, we break them up into more elementary constituents, and these constituents are assumed to satisfy some general structural constraints. to express those basic constituents, we have appropriate verbs at our disposal. those verbs model a mapping from a force space into a result space. these spaces can often be assumed to be vector spaces or to have at least properties similar to vector spaces. this metaphor also applies to social interactions where we are confronted with intentions additionally to physical forces. to understand events and apply verbs in situations that interpolate or extrapolate from known ones and to predict the outcome of small variations in the input, we utilize structural constraints. let us now be more specific."
"1. dream apartment: the participants are told that they are to share a large open-plan apartment, and will receive a large amount of money to furnish and decorate it. they discuss the layout, furnishing and decoration decisions;"
"when we talk about events, we clearly imply a relational structure. for example, linguists resort to a thematic role structure where the basic roles are agent and patient [cit] . when we think about everyday physical phenomena, we conceive of events as relations between causes and effects. when we make plans, we consider different actions and their consequences. these three perspectives reveal different aspects of the same underlying structure, which is an asymmetric relation between two entities."
"moreover, if causal relationships are monotonic, inferences over whole chains of cause-effect relationships can be made, allowing assessments of indirect causation. [cit] suggests two operators that allow to chain causal relations. one is reduction: two chained positive monotonic relations can be reduced to a single one. the other is inversion: a chain of a positive and a negative monotonic relation reduces to a negative one. these operations are easily illustrated. when you press the accelerator pedal of your car harder, the engine operates at a higher speed, and as a result, the car runs faster. these two actions, that of the driver and that of the engine, get reduced to the single one of accelerating the car. in contrast, in order to see inversion, when you want to brake, you step on the brake pedal, and the brakes then decelerate the car. thus, there is a positive action, activating the brake, and a negative one, reducing the speed of the car, and the two actions are reduced to the single negative one of braking the car. as both operators reduce a chain of two relations to a single one, they can be used iteratively to reduce even longer chains of relations."
"if you want to express a counterforce, it will function as a second cause and should be represented as a force vector. according to our account of events, you then need another event expression where the counterforce is the force vector. consequently, linguistic expressions of counterforces will in general involve embedded clauses in a sentence that expresses the main event."
"a second way to express counterforces is to use modal verbs. again, modal verbs involve another verb expressing a primary force (or the result of such a force). many modal verbs relate to a counterforce. for example, in \"i let the dog come in, \" \"let\" expresses the removal of a counterforce (by opening a door). or in \"the dog may not come in, \" the presence of a potential counterforce is made explicit."
"over the recent years, there has been increasing efforts dedicated in establishing ai based models to predict realworld time-dependent data. various ai approaches such as the artificial neural network (ann), adaptive network based fuzzy inference system (anfis), support vector machine 2 applied computational intelligence and soft computing (svm), and least squares support vector machine (svm) approaches have been applied to cope with time series prediction in various domains [cit] . these previous works have illustrated that application of these techniques, as a solution to the challenges of time series problems, is not only feasible but also very effective."
"in this work data collected from laboratory experiments of driving behavior (questionnaire and driving simulator sessions) have been used to develop a fuzzy model and a logit model of gap-acceptance behavior at priority intersections. laboratory experiments allowed to observe and record information about explanatory variables not detectable from direct observations (on site), and to include them in models with the aim to better describe, understand and simulate driver's choices. on the other hand the use of a fuzzy model allowed to overtake problems concerning non-homogeneous explanatory variables and uncertain and imprecise information on the system. the proposed models have included driving styles variables (in particular \"angry/hostile\" and \"anxious\"), in addition to some variables commonly used in gap-acceptance studies (time interval size and driver's gender)."
"the element-associated numeric data, such as gene expression values, can be visualized effectively as heatmaps. as each locus represents a specific range of base pairs, multiple elements can possibly be annotated in its range. chromomap uses a data aggregation algorithm, provided as average or sum functions, to assign data values to each locus. hence, for the locus encompassing more than one elements, aggregate data value will be assigned to it. the actual data values, however, can still be viewed in tooltip for each of the element. the differential expression analysis results from a breast cancer study from geo(gene expression omnibus) [cit], shown in figure 4, utilizes this feature in combination of other features of chromomap. the chromosome strands show the average expression change(logfc) per loci, while the mean expression (normalized) values from the two groups (control and case) has been shown in adjacent."
"we shall also speak of a typical event when the center of a region in force space is mapped to the center of a region in effect space. we recall here that we assume that these regions are convex, and therefore their centers tend to be sufficiently far away from their boundaries."
"this enables us to formulate conversational rules of the form 'if a laughs and pleasantness is set to k, then reset pleasantness to k + θ(α)', where α is a parameter corresponding to arousal."
"in this section, the newly developed flsvr tsp is applied to forecast three time series: the mackey-glass series, the daily water discharge at palo verde drain (http://waterdata.usgs.gov/), and the monthly usd/twd exchange rate (http://fx.sauder.ubc.ca/data.html). the mackey-glass chaotic time series is defined by (19) [cit] . herein, the parameter is set to be 17. in our study, 500 data cases are generated in which 400 cases are used for training and validating process. the rest of the data is used for testing the model as follows:"
"we conclude by a few notes on how the geometric approach relates to some of the problems concerning event representation that are discussed in the contemporary philosophical, linguistic and psychological debate."
"the fa algorithm uses the following three idealized rules: (1) all fireflies are unisex, so each firefly is attracted to other fireflies regardless of their sex, (2) the attractiveness of a firefly is proportional to its brightness and decreases as the distance increases. a firefly moves randomly if no other firefly is brighter, and (3) the brightness of a firefly is affected or determined by the landscape of the objective function [cit] . the fa algorithm can be illustrated in pseudocode 1."
"the first and second arguments, respectively, taking the co-ordinates of the chromosomes and the annotations. the generated plot, as viewed in rstudio's viewer pane, offers interactivity to the users to view deeply into a locus allowing them to explore their annotations of interest. users can export the plot either as a static image or as a standalone html file encompassing the interactive plot capable of including as supplementary data in publications. in addition, the plots can be included in shiny applications, or embedded in rmarkdown documents. the rest of the paper illuminates the user about some of the most prominent features offered by the package illustrating, with examples. its application in research field."
"a special case of causation occurs as a result of the intentions of an agent. the distinction between non-intentional and intentional sometimes shows up in result verbs. the classical case is kill versus murder. the latter is intentional, while the former is undetermined with respect to intentionality. thus, murder cannot occur with non-intentional agents. some events involving intentions, such as those describe by give, buy and sell, can be construed from either of two perspectives: physical causation or intentional causation leading to the fulfillment of a goal. such a situation can still be expressed with the aid of a single verb, since the fulfillment of the intention presupposes a physical action, for example the transition of an object from an agent to a recipient."
"in studies of vehicular gap-acceptance behavior, the choice to accept or reject a gap of a certain size is generally considered the result of a driver decision process which includes, as inputs, subjective estimates of a set of explanatory variables, given specific objective factors. these subjective evaluations are usually affected by a high degree of uncertainty, which can be properly treated both by classical probabilistic models, e.g. logit [cit] and by fuzzy system theory [cit] ."
"while the two-spaces model of events is meant to capture mostly qualitative features of event representations, it can provide more precise and testable propositions (and approximate modeling strategies). for example, experimental studies suggest that individuals can perform intuitive addition of force vectors when observing how simultaneous sources of force affect a patient trajectory [cit] . classical experiments based on michotte's \"launching\" paradigm [cit] show that the attribution of causality in an event depends on the angle of the trajectory of a hit object b with a hitting object a -which demonstrates that individuals decide whether an animation captures a single (causal) event or two disjoint ones on the ground of the mapping of forces to trajectories. recent research has shown the perception of such mapping to be remarkably accurate, and a good predictor of \"causal impression\" [cit] . in all these cases, simple two-spaces models of events provide good approximate predictions of individual perception of causal events and the satisfaction/violation of such predictions affects the evaluation of what constitutes an event. this paper further specifies relevant constraints on elementary event representation."
"generally, in time series prediction, the historical time series are transformed into high dimensional space to facilitate the exploration of implicit pattern lying in the series. this process of transformation, widely known as state reconstruction [cit], is dependent on the embedding dimension ( ). equation (1) illustrates the state reconstruction process for one-step-ahead forecasting in which the original time series (with element) is transformed into an input matrix of the size ( − )-by-and an output matrix of the size ( − )-by-1:"
"the daily water flow data set consists of 273 data cases of daily water discharge (cubic feet per second) at palo verde outfall drain, from 1/1/2011 to 9/30/2011 (see figure 3) . the number of data cases used for testing is 30. the monthly usd/twd exchange rate includes 260 records from 1/1990 to 8/2011 (see figure 4) . in the experiment, 36 data cases are utilized for testing process. for these two time series, onestep-ahead prediction is carried out."
"definition 4: a subset c of a space s is called convex, if whenever x, y are in c, then also all points between x and y are contained in c."
"while interactivity allow the users to lookup in detail about the annotations, the chromomap also provide the feature of associating hyperlinks for each annotation on the plot. hence, when the genes are viewed in tooltips, users can directly click on them and land to the web page describing its details. moreover, the users can use the labelling option to display labels on the plot generating a static visualization of the plot."
"the continuity constraint is more demanding in terms of structure, since it requires more than ordering relations (see next section). this structural cost comes with a benefit since it allows finer predictions of the causal effects of actions. in particular, this makes the effects of our actions quantitatively controlled. this is clearly important for learning many motor skills where by fine-tuning our actions we want to modify the effect to a precise degree. monotonicity alone would not be sufficient here, because that would tell us only that a stronger effort would lead to a stronger result, but not help us in predicting to what extent we need to increase our effort in order to achieve a precise degree of increase of the effect."
"an even stronger demand on the cognitive structure of events is to impose a distance measure, i.e., a metric on the spaces involved in the mapping. this allows us to define betweenness and convexity (see convexity) on the spaces. the convexity preserving constraint on the event mapping f can then be formulated as: if the force vector z is between force vectors x and y, in the sense that they all point in the same direction, but the strength of z is between those of x and y, then the result f(z) gärdenfors, 2000, ch. 3; gärdenfors, 2001) . from a cognitive point of view, convexity is closely related to generalization: it allows us to make inferences over whole regions from a limited number of observations. in general, feedback control mechanisms presume that the mapping from actions to result is convexity preserving. for example, when you are shooting with a cannon you can control the horizontal and vertical direction of the cannon. if you have tried to settings x and y for the cannon and observed the landing points f(x) and f(y) of the projectile, you presume that an intermediary setting will lead to an intermediary result. similar principles apply to many cases of motor control. [cit] showed subjects movies of a person lifting objects of different weights. the subjects could not see the object themselves but only the movement patterns of the person doing the lifting. nevertheless, the subjects were very accurate in predicting the weights of the object. this would not be possible, unless the mapping from forces to results (object being lifted) is convexity preserving: from the movement patterns, the subject could infer the forces exerted by the person lifting the box and then (implicitly) infer that intermediary forces corresponded to intermediate weights of the boxes."
"a central construct of psychological models of event segmentation is the working model. event segmentation models submit that individuals hold at each moment (and for a given time scale) [cit] define as the working model of the event. working models of an event support the systematic effort of observers and participants to predict the future in response to current perception. thus, working models need to be quite coarse and stable models of the event, a kind of static snapshots that are robust enough to remain buffered from the variations associated with the fine details of sensorial inputs [cit] ."
"in (17), tr and va denotes the training error and validating error, respectively. the training and validating errors herein are root mean squared error (rmse) calculated as follows:"
"during each test the same driver approached the intersection about 10 times (10 runs) and each driver made at least 4 tests during the experiment. before starting the test, drivers were subjected to a learning task in the simulator to make them familiar with it."
"all observations relate to the right turn movement from a minor street controlled by a \"yield\" sign. many parameters characterizing driver behavior provided by the simulator were recorded, such as: for the purpose of this study, only major stream vehicles arrival time at the conflict point and test driver's arrival and departure time at the stop line were considered. the data were organized in a database and then processed using a software procedure that allows to extract gap-acceptance information for each driver."
(2) tuning parameter initialization. the aforementioned tuning parameters of the model are randomly generated within the range of lower and upper boundaries (see table 1 ) in the following manner:
"for annotating elements on the chromosomes, chromomap provides the choice of two annotation algorithms, viz. point-annotation and segment-annotation, differing in how annotations are visualized on the plot. the point annotation algorithm ignores the size of the element, and, annotate the element on a single locus. the segment annotation algorithm, on the other hand, consider the size of the element visualizing the annotation as a segment. this can be advantageous when visualizing chromosomal regions, like gene, annotating its structural elements (figure 3)."
"thus, a monotonic map from x to y preserves the ordering. our first constraint that larger forces lead to larger results corresponds mathematically to that the mapping from forces to results is monotonic."
"betweenness can be defined for many non-metric spaces, for example for all graphs. an ordering structure also leads to a concept of betweenness, as will be discussed in a moment."
"the corpus is transcribed in the target language and glossed in english. disfluency, laughter, and exclamations are annotated. the current paper presents analysis of laughter in two dyads in french and chinese (3 tasks x 2 pairs x 2 languages)."
"the chaining of causes plays an important role in the structure of events that involve instruments. actions involve in many cases instruments, and it is not uncommon to represent the full event through a compressed version of it. for example, we can define an event as \"the hammer broke the window\" although we understand that some agent broke the window through the instrumental use of a hammer. we submit that such typical compression of instrumental events is made possible by the chaining of causal effects which is enabled by monotonicity. by transferring its force to the instruments, the agent endows the instrument of a kind of derived agency, which allows the construal of a reduced representation of the event itself."
"the analysis of roc curve is useful in those cases in which the best cut-off value of the variable of interest is not known a priori by the researcher. in the gap-acceptance case the best cut-off value is expected to be in correspondence of an acceptance probability equal to 0.5 for the ga l model and a value of the acceptance index equal to 0.5 for the ga f model. the results confirm these assumptions, as shown in fig. 6 . the solid line represents the ga l model, while the dashed line the ga f model."
"while the continuity constraint is a very general organizing principle, it is important to realize that it does not always apply. in fact, the cases where it does not apply need a treatment of their own. consider the example where you are stretching a rubber band. the initial results of increases in force will be continuous changes in length, but eventually a critical point will be reached where the band breaks. in more general terms, we see here a discontinuous phase transition occurring when an obstructing counterforce is suddenly overcome, and a qualitatively new result is achieved. thus, at or near the transition point, a very small change of effort produces a large result, and the mapping from force to effect is discontinuous. the monotonicity constraint, at least when not interpreted in the strict sense, i.e., that strictly larger forces lead to strictly larger results, will still apply. monotonicity is a qualitative principle that neither needs continuity nor a quantitative substrate as provided by a metric."
"many instances of visualizing annotations, including the one shown in figure 1, require depicting annotations of groups of elements as opposed to individual ones. to visually differentiate between groups, chromomap allows users to assign groups, for each element being passed, as an additional data column in input file. figure 2 depicts another example of polyploidy and group annotation where the paralogous and homeologous genes in allopolyploid wheat genome are visualized."
"in this and in the following sections, we describe in detail the case-study in order to illustrate the collected data and the methods by which we derived information from the data. the analysis has been developed following three steps:"
"the laugh episode included the last comment by a speaker if it occurred within an estimated 1 s preceding the onset of the initial laughter. a laugh episode was terminated if an estimated 1 s passed without speaker or audience laughter, or if either the speaker or the audience spoke.\". they found that \"only about 10-20% of episodes were estimated by the observers to be humorous\" [cit], and thus derived the conclusion which is now widely adopted in the literature: laughter is, for the most part, not related to humour but about social interaction. an additional conclusion based on this study is that laughter never interrupts speech but \"punctuates\" it occurring exclusively at phrase boundaries."
"as technology advances, computers or machines with human emotion are no longer an unreachable dream. a system that understands human emotions can provide more personalized services and be extended to more applications. professor rosalind picard stated that affective computing explores the topic about computing that relates to, arises from, or influences emotions. this concept creates a new computing system or idea of human-machine interface design which can realize, recognize, and utilize human emotions. the concept above coincides with \"technology derives from humanity. \" today, in the twenty-first century, human's demand for hightech product is no longer merely some specific functionality. humanity is an essential element for successful product. the design focusing on affectivity will become the mainstream."
"open access this article is distributed under the terms of the creative commons attribution 4.0 international license (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the creative commons license, and indicate if changes were made."
"here we presented a study that applied gsa within the context of in silico prediction of pharmacological toxicity. the 378 target of gsa is the latest version of the in silico model of an isolated cardiac cell [cit], cipaord, which was developed 379 under the cipa initiative and incorporates dynamic herg-drug interactions [cit] . our analysis explored the effects of a 380 large population of virtual drugs on the seven major cardiac ion-channel currents thought to be important in regulation of 381"
"penzias and wilson's discovery of the cosmic microwave background of the universe offers an example where a serendipitous discovery results from outsourcing one's range of attention. burke associated penzias' issue with peebles' research, and expected that dicke could suggest relevant questions. and indeed, during the conversation with dicke, the question is the interference characterized by such-and-such pattern? entered penzias' range of attention, soon followed by the conditional if the interference is characterized by such-and-such pattern, then it is caused by a 'big bang' event, which was a consequence of peebles' research. a positive answer to the former question yielded (by modus ponens) a complete answer to penzias and wilson's main research question (what is the source of the interference pattern recorded by our horn antenna?). as we also mentioned, this discovery could also be described from the standpoint of dicke and his team. in particular, circulating the preprint of peebles's paper played, for dicke and peebles, the same role as talking to burke for penzias and wilson. for both teams, the phone call between penzias and dicke resulted in the same modus ponens inference to the validation of the 'big bang' model, one team providing the antecedent, and the other the conditional, and the phone call between penzias and dicke completing the inference."
"holmes' bam reflects an outdated view of the material substrate of memory. at the same time, it is clearly a sort of cam, and displays a cogent outlook on the functional organization of memory. a bam-cam distributed architecture creates a host of potential associations, contingent on one's past experience. for instance, someone who has first encountered a given shade of red through the red diamonds of a particular card set would associate that shade with a particular rhombus shape."
"in table 10 . in the simulations, the reported median values of parameters were used. the known tdp categories and maximum effective free therapeutic concentrations for the validation drugs ( [cit] ) 532 are listed in table 11 . the figures (figure 10, 11, 12) show values of the derived features extracted from the simulation of virtual drugs 536 generated for the sobol sensitivity analysis against individual input parameters. tdp risk classification of \"cipa drugs\" based on transmural dispersion of repolarization 550"
"eliciting […] information by questioning can be viewed as one possible recall procedure. at the same time, it can be generalized so as to become a common model of several different kinds of information-gathering activities, deductive as well as inductive. this partially shared model for recall and intelligent inquiry may perhaps serve as an explication for the link between memory and intelligence. for another thing, our general idea should not surprise any sherlock holmes fan. in some cases, the great detective has to carry out an observation or even an experiment to answer the question. more frequently, all he has to do is to perform an anamnesis and recall certain items of information which he already had been given and which typically had been recorded in the story or novel for the use of the readers, too, or which are so elementary that any intelligent reader is expected to know them. [cit] that the \"link between intelligence and memory\" should be construed as the ability to use memory strategically. since memory relies on associations, the suggestion entails the existence of 'strategic associations'. this notion may seem somewhat of an oxymoron, if 'association' is understood as 'free' or 'spontaneous' but the notion has an empirically grounded interpretation. we will develop further this interpretation in sect. 1.3 and argue that it vindicates some of holmes' ideas on memory, to which we now turn."
"in this section, we present an illustrative example to demonstrate the capability of our evaluation approach. in this case, we organize five experts to evaluate 4 types of wireless sensor network. the experts provide comprehensive performance numerical evaluations about the 4 types of wireless sensor network in different circumstance."
"429 surprisingly, the recently introduced dynamic-herg block parameters v half and ku [cit], which are measured using 430 challenging experimental protocols [cit], show minor influence on ead genesis (table 5 ) when compared to other 431 tested inputs. these parameters have relatively small contribution to the variance of all the tested derived metrics 432 compared to other influential input parameters (figure 1, 2, and data in the supplemental material). in cases where the 433 majority of the primary regulating parameters are similar between drugs, accounting for changes in the modestly 434 influential parameters can allow for improved predictions. we observe that a surrogate of qn et estimated by inclusion of 435 the v half parameter allows for improved prediction of few intermediate risk drugs (figure 5) . similarly, when classifying 436"
"a non-deductive problem can be solved by strengthening the premises enough to warrant a deductive proof for a solution from the strengthened premises. and the best deductive strategy to derive a solution is the one that yields the simplest deductive proof. subsequently, insights into the best deductive strategy for deriving a solution to a non-deductive problem from a set of premises strengthened with some information, are also insights into how to actually extend the set of premises for solving this nondeductive problem. these insights guide information-seeking, and the discovery of a solution. furthermore, they are not the result of some irrational intuition: insofar as they are grounded in logic, they support the claim that a logic of discovery is possible. 3 in order to make the notion of strategy formally precise, m. b. and j. hintikka specified a utility function for an inquirer with bounded cognitive resources [working memory [cit] and attention [cit] ] and a subsequent preference for lower cognitive loads. unfortunately, those utilities cannot in general support strategic reasoning without hypercomputation, that is, solving problems that are not computable by turing machines [cit] ]. after the death of m. b. hintikka, j. hintikka and his collaborators sidestepped the issue, and bumped off utilities. specifically, they characterized the relation between deduction and interrogation with the resources of first-order semantics and proof theory alone [cit] for informal expositions]. in a nutshell, if inquirer has the same preferences as a theorem prover, she tries to minimize the number of cases she reasons about, and the number of individuals to consider in those cases. expressing those preferences with utility functions becomes redundant, and the game-theoretic analysis of strategy selection is substituted with a proof-theoretic one, which is appropriately resource-sensitive."
"[the] partially shared model [of the 'sherlock holmes sense of deduction'] for recall and intelligent inquiry may perhaps serve as an explication for the link between memory and intelligence. [cit] the hintikkas phrased their conclusion cautiously, because they reconstructed only what cognitive psychology refers to as 'analytic' thought processes. in order to provide an explication for the link between intelligence and memory, their analysis would have required an extension to associative processes. this extension was unfortunately never realized. after the demise of m. b. hintikka [cit], j. hintikka and his collaborators developed what became the interrogative model of inquiry (imi) with an exclusive focus on the relations between theorem-proving and information-seeking. in the process, the naturalistic motivations and ambitions of the original approach were almost entirely forgotten."
"in the second experiment, the performance evaluation related to the adopted statistical analysis feature set as well as its combination with approximated lfe contours is summarized in table 3 . in this table, we abbreviate the adopted statistical analysis feature set as γ. moreover, the combination of the adopted statistical analysis feature set and approximated log energy contours are represented by γ, . with the, the experimental result reveals that the identification rate of sadness is decreased slightly by 3.2%, but the identification rates of disgust, joy, and neutral are enhanced by 8.7%, 5.7%, and 5.2%, respectively. the γ, 3 feature set achieves the best total identification rate 82.3%, which is 2.26% higher than merely using statistical analysis feature set γ. the confusion matrix corresponding to the optimal feature set γ, 3 is given in table 4 ."
"we have built our argument on a cost-benefit analysis of distributed knowledge rather than on a semantic one, on the grounds that the former is sensitive to the resource-boundedness of real-life agents. cost-benefit analyses capture the down-toearth reality of laboratory life. for instance, in order to rediscover doroshkevich and novikov's analytic result, and to build a radiotelescope, dicke's team had to face salary and infrastructure costs. these expenses could have been saved with a better management of the knowledge distributed in the astrophysics community of the time. again, our suggestion that these considerations are relevant to epistemology is not a novelty, and is similar to theories of inquiry proposed by american pragmatists (peirce, dewey, or more recently levi). this thread could be further pursued into a refined analysis of distributed knowledge, considering that on the one hand pragmatic theories of inquiry and on the other hand sociology of organizations and management research overlap in both subject-matter and methodology."
"(6) fault tolerance the sensor of network often useless because of environment or power source exhausts. so the hardware and software have the very strong fault tolerance in the wireless sensor networks, which guarantee high reliability in the system. when hardware or software in the network has breakdown, the system could be automatic control or automatic restructuring."
"to generate the pca bases of approximated speech feature contours, a training speech database is required so that eigenvectors of the approximated speech feature contours can be found. the goal of pca is to search a linear combination of the original bases that maximizes the total variance of training approximated speech feature contours. by selecting the top principle bases or eigenvectors, pca projection matrix is capable of representing approximated speech feature contour accurately with lower dimensions of projection coefficient vector."
"the trust performance in wireless sensor networks should include four facts [cit] . （7）communication characteristic in the wsn, node behavior includes control order or the transmission of application. when the malicious node maybe discard or distort the data packet, the selfish node possibly discard data packet which needs to retransmit because of saving energy. the node's correspondence behavior has been observed to distinguish malicious node or the selfish node. also the communication characteristic is one of major performance in trust."
"our line of argument parallels two recent trends in mainstream analytic epistemology, namely the debate on extended cognition and the burgeoning discussion of the relation between memory and distributed cognition. 2 we have however resisted the temptation to draw explicit parallels with those trends. our reason is twofold. first, the hintikkas' concern with the relation between memory and inquiry predates those debates by nearly thirty years. therefore, attempting to put the former in the perspective of the latter would incur a risk of misidentification and incorrect charaterization of the hintikkas' project. second, the notion of 'distributed knowledge' investigated in these trends is considerably richer than the one we investigate and interpret within our relatively simple formal model. this is again intentional, since this model is intended to be as conservative an extension as possible of the hintikkas' original model. whether this simple model can contribute to the debate about richer notions of 'extended' and 'distributed knowledge' is therefore an open question and a topic for further research. we will suggest in conclusion some directions this research could take."
"the last remark suggests some possible relation with the recent epistemological trends that we mentioned at the end of our introduction. the notion of distributed knowledge that we have investigated has explicit ties with the formal (semantic) [cit] s. we argued that, modulo the hintikkas' model, the informal notion of distributed knowledge introduced by sociologists of organizations can solve some of the difficulties associated with the formal notion (in particular the vexing problem of logical omniscience). what remains to be determined is the relation with a third and more recent understanding of 'distributed' knowledge as e.g., characterized by k. michaelian and j. sutton, namely \"distributed across heterogeneous systems combining neural, bodily, social, and technological resources.\" [cit] . our argument has covered in part the 'neural' and 'social' aspects of distribution but did not consider either the 'bodily' or 'technological' ones. we must admit that we have little to say about the former. however, the relation we just alluded to between algorithms for recommender systems and strategies favoring serendipitous discoveries may foreshadow a possible extension of our argument to the technological dimension."
"in this paper, we outline the extension of the hintikkas' approach from analytic to associative processes. we vindicate the suggestion that it can illuminate \"the link between intelligence and memory\" by showing that memory supports strategic associations. the relevant concept of strategy (behavioral strategies) is borrowed from game theory, and is thus well-defined and mathematically precise. furthermore, our appeal to this concept is grounded in computational models of human memory, rather than conceptual analysis alone. and insofar as these models are supported by empirical data, we can claim a measure of empirical adequacy for this concept of strategy. [cit] . we argued that the imi suffers from shortcomings that the hintikkas' original (naturalistic) interpretation can overcome. and we pointed out that their interpretation anticipated on empirical research that has since then clarified how the architecture of memory supports creative thinking and problem-solving."
"where, z is class value vector， while c w is represented weight for cth kind index. repeating the above step, we can obtain the gray evaluation weight matrix and the evaluation index value of other wireless sensor network."
"the \"cipa drugs\" were also calssified based on the transmural dispersion of repolarizations as shown in figure 19 ., as in our previous study [cit] . uniformly spaced samples in this 3d 555 space were selected to generate a set of virtual drugs. the virtual drugs were simulated and outputs of the in silico 556 models were visualized for presence or absence of eads (figure 20) . the region in green in the figure 20 denotes the 557 region of the examined parameteric space where eads were not observed in the cipaord model, while the region in red 558 denotes the region with presence of eads in the model. the surface separating the ead+ and ead− regions is shown 559"
"tsoukas' analysis of distributed knowledge is influenced by f. hayek's critics of neoclassical economics, in particular the assumption that agents perform economic calculations from full and correct information. hayek's argument has a parallel in epistemology, due to the assumption that the information sets of agents are closed under logical consequence, also known as the problem of logical omniscience. 11 [cit] . the hypothesis of an expanding universe, together with einstein's special relativity, entails the existence of a cosmic microwave background. if we apply the semantic concept 11 for instance, gerbrandy notes that: \"possible world semantics give rise to a rather distorted notion of belief and knowledge. in particular, […] the belief (and knowledge) of an agent is closed under logical consequence. this does not conform at all to the situation as it actually is: people do not always see the consequences of their beliefs, and know that other people are limited in the same way. […] this mismatch between the semantics and the concept of knowledge and belief is known as 'the problem of logical omniscience'.\" [cit] gerbrandy defends the idealizations of formal semantics with the remark that information is closed under logical consequence. j. [cit] s [cit], also defended the view that it is best understood as a logic of information [cit], ch. 1-3). of knowledge, the existence of the cosmic microwave background is thus 'known' by everyone in the community, but also known by everyone to be known by everyone, etc., and is thus not only distributed knowledge, but common knowledge. by contrast, less-than-logically omniscient agents do not expect others to draw the consequences of what they know, and sometimes interact with others in order to obtain assistance figuring out the consequences of what they know. tsoukas' informal account captures this as, respectively, \"normative expectations\" and \"dispositions […] formed in the course of past socializations\". therefore, strategies for co-opting the inferential and associative processes of other members of an epistemic community that we described in sect. 2.3 are consistent with tsoukas' account of how knowledge is distributed and accessed in an epistemic community of less-than-logically omniscient agents."
"the input parameters are assumed to be random variables that uncorrelated and mutually independent. the functional 123 decomposition can be translated into a variance decomposition. this allows to quantify the variance contribution to the 124 total output of individual parameters and the parameter interactions,"
"the organizational problem firms face is the utilization of knowledge which is not, and cannot be, known by a single-agent. even more importantly, no single agent can fully specify in advance what kind of practical knowledge is going to be relevant. firms, therefore, are distributed knowledge systems in a strong sense: they are decentered systems, lacking an overseeing 'mind'. the knowledge they need to draw upon is inherently indeterminate and continually emerging; it is not self-contained. individuals' stock of knowledge consists of (a) role-related normative expectations; (b) dispositions, which have been formed in the course of past socializations; and (c) local knowledge of particular circumstances of time and place. a firm has greater-or-lesser control over normative expectations, but very limited control over the other two. at any point in time, a firm's knowledge is the indeterminate outcome of individuals attempting to manage the inevitable tensions between normative expectations, dispositions, and local contexts. [cit] substituting 'firms' with 'academic communities' in the above passage depicts a familiar picture. [cit], expectations that ivy-league princeton astronomers (dicke's team) would build a horn antenna to validate their analytical results are \"role-related normative expectations\". the habit of talking to colleagues (penzias and burke) and of circulating preprints of important papers (dicke and peebles) are \"dispositions […] formed in the course of past socializations\". finally, the result published by doroshkevich and novikov, known to their editors and translators (but lost to penzias, wilson, burke, dicke and his team), as well as the content of the observations made by penzias and wilson and peebles' analytic results, are all examples of \"local knowledge\". notice that tsoukas' forms of \"stock knowledge\" overlap: the circulation of peebles' preprint can be seen as the result of enforcing several \"role-related normative expectations\" (showcasing princeton's research leadership and disseminating results for the benefit of the community), instantiating a disposition (as noted), and inducing local knowledge (burke's, later beneficial to penzias)."
"b. gray sort of evaluation gray sort of evaluation, as fuzzy and uncertainty, is a variety or concept which could be compared. since to describe the gray sort, the gray value and whiten weight value should be confirm. however we are unaware of nicety gray value, which include probable range. what is called whiten weight function is presented probability or preference degree of evaluation index, which belong to a certain gray sort in one value range. based on gray theory, the 5 evaluation gray sort in evaluation of performance for wsn, corresponding to evaluation degree:"
"in single-agent (decision-theoretic) reconstruction of information seeking behavior, shifts in agent's attention must be represented as depending on nature's state. if some of these shifts are not deterministic, they must be represented as lotteries over possible states of nature, that is, chance events. in a multi-agent (game-theoretic) reconstruction, it becomes possible to represent how inquirer can 'manipulate chance', that is, deliberately introduce lotteries in her information-gathering behavior. for instance, one could reconstruct penzias' conversation with burke, as part of a behavioral strategy, based on some heuristics. for instance: for every colleague that penzias would meet, and who would have some extensive knowledge in astrophysics, even outside penzias and wilson's field, randomize between mentioning the interference, or not. intuitively, the weights of the lottery would represent how orthogonal preferences are weighed against one another (here: the opportunity of gaining insights from informal conversations, vs. the risk of being perceived as work-obsessed and boring). 9 the perspective on serendipity typically adopted by sociologists of science and information scholars is implicitly grounded in decision theory. from this perspective, 'luck' is mapped to chance events in the state of nature, while 'sagacity' is mapped to reasoning and decision-making processes. by contrast, a reconstruction explicitly grounded in game theory further constrains 'luck', making it part of the strategy selection process, to which is also mapped 'sagacity'. then, at least in multi-agent cases, there is a rather precise interpretation of the notion that serendipity holds a middle ground between sagacity and luck. in the next section, we suggest that this game-theoretic interpretation for the multi-agent case is also relevant to the single-agent case."
"(1)energy validity energy validity of the wireless sensor networks is the number of processable requests, under the condition that the energy of network is limit."
"in this case, we use ahp method to determine weights. we stratify factors involved in decision making by establishing hierarchy structure which includes target layer, rule layer and project layer to construct a model of hierarchical analysis. we establish judgment matrix, mark low-layer elements dominated by elements of the target layer u with 12"
"to address the problem, we automatically generated several classification metrics for the cipa datasets as linear 297 combinations of the 5 normalized direct features sbikr, bin al, bin a, bical, and v half . note that the parameters for 298"
"the rest of this paper is organized as follows. section 2 introduces the overview of the system. the proposed speech feature set for emotion identification is illustrated in section 3. section 4 describes the emotion classifier used in this paper. next, section 5 gives the experimental results. conclusions are finally drawn in section 6. figure 1 illustrates the proposed system block diagram. the proposed emotion identification system can be divided into two main parts, feature extraction and emotion classifier. in the feature extraction, we extract all the acoustical features from both of training and testing speeches. the acoustical features comprise silence/active, voiced/unvoiced, pitch, log frame energy (lfe), subband power, spectral centroid, spectral bandwidth, spectral flatness, mfccs [cit], and ratio of spectral flatness to spectral centroid (rss) [cit] . the statistical analysis features are generated from the aforementioned frame-based acoustical features. the chosen feature is important for classification task [cit] . a feature selection procedure chooses the effective statistical analysis features to form the selected feature set. the selected statistical analysis feature set as well as the selected approximated speech feature contours is combined to form the proposed feature set. in the emotion classifier, a multiclass svm is used as the classifier to identify the emotion class of a speech utterance."
"of special interest are cases where the solution involves memory and attention, and hinges on the ability to recall relevant information and associate it to the premises of the problem under consideration. and in fact these cases are within the purview of the approach that m. b. and j. hintikka had initially laid down:"
"in hintkka's terminology, a conditional such as the above is called a conclusiveness condition. more formally, a conditional if a, then b is a conclusiveness condition for a question q if b is one of the potential answers to q. a simple heuristic principle for answering some question q is to search one's background knowledge for conclusiveness conditions for q and ask yes-no questions about their antecedent. in some contexts, this heuristic principle can be implemented mechanically [cit], pp. 16-17) . but in the case of the discovery of the cosmic microwave background, penzias and wilson followed another heuristic. namely, they outsourced the process of forming associations between answers to the question they were investigating, and the knowledge available at the time among astronomers, when they talked to burke. similarly, they outsourced the inferential process when penzias talked to dicke on the phone. this heuristics illustrates why serendipity can be conceived of as the product of both luck and sagacity. as we said, 'sagacity' can be here equated with the ability to form strategic anticipations about the inferential consequences of some 'small' answers, together with some conclusiveness conditions retrieved from one's memory. in the case of penzias and wilson, the process of forming such anticipations was partially implemented by someone else (dicke), and penzias and wilson's sagacity was actually to rely on someone else's sagacity."
the cell was stimulated for 200 beats with additional block of maximum conductance of ikr by 85%. the 85% block was 95 selected since almost half of the population of virtual drugs (across the entire parametric space observed) resulted in ead 96 development in the model simulations.
"[f]requently, all [sherlock holmes] has to do is to perform an anamnesis and recall certain items of information which he already had been given […] or which are so elementary that any intelligent reader is expected to know them. [cit] the hintikkas' argument goes as follows: (1) the reconstruction of the 'sherlock holmes sense of deduction' provides an explication of how deductive anticipations guide information-seeking through observation; (2) by the above remark, recalling information sometimes plays the same role as observation in holmesian inquiry; (3) subsequently:"
"when they substituted game theory with meta-mathematics, hintikka and his collaborators prevented their reconstruction of interrogative inquiry to collapse into conceptual contradiction. but they also threw the proverbial baby with the bathwater, leaving implicit in their formal results any constraint that would result from inquirer's bounded cognitive resources. [cit] that those bounds deeply impact the interpretation of the formal results expressing the relation between interrogation and deduction, in particular when instrumental questions cannot be obtained mechanically from the premises (cf. also appendix)."
"the hintikkas' approach was an analytic (or conceptual) model of inquiry, scientific or otherwise, whose ambition was to make discovery amenable to logical and strategic inquiry. we have vindicated their approach and argued that the non-analytic component of discovery, that is, analogical processes, is also amenable to the same analysis. the hintikkas' approach could also be a starting point to computational models of discovery. we have suggested [cit] could capture some of the dynamics of an agent's range of attention. the picture we have given of this dynamics in sect. 1 of the present article is more sophisticated. since it supervenes on memory, each input in a data stream can elicit recollections which may in turn suggest new patterns."
"it is however unclear whether a multi-agent computational model would improve upon the conceptual analysis of multi-agent inquiry captured by the hintikkas' analytic model. for instance, the analytic model is not predictive. hence, a computational model could not serve as a proxy for empirical validation of the model's predictions through simulations. however, the model could be used to explore ways to generate 'serendipitous' recommendations in a network of agents. a possible application would be computer-assisted collaborative inquiry learning, where artificial agents could assist human learners by making recommendations based on anticipations of the learners' strategies. the practical implementation of this application could draw inspiration from algorithms developed for recommender systems that make unexpected but useful recommendations based on the user's preferences. these algorithms have been recently shown to outperform other methods including those that introduce random variations [cit], 2015) ."
"as a tribute to the hintikkas and sherlock holmes, we propose that our argument is a vindication of the sherlock holmes sense of memory. the ideas credited to holmes about memory are grounded in ancient rhetorics, but with a modicum of interpretive charity they dovetail the description of memory from cognitive neuroscience and cognitive psychology. however, our interest lies beyond holmes' inquiries, which primarily illustrate the single-agent case. our reconstruction of strategic associations permits us to consider strategies for accessing information distributed across the memory of a single epistemic agent on par with strategies for accessing informa-tion distributed across a network of epistemic agents: when cooperation prevails, they belong to the same continuum. 1 the paper builds up to the conclusion just expressed in three steps. section 1 presents the hintikkas' reconstruction of the strategic role of deduction (1.1), then holmes' conception of memory in the light of cognitive neuroscience (1.2), and combines them to obtain a characterization of the strategic role of memory for the single-agent case (1.3). section 2 introduces the multi-agent case with a non-fictional example, namely the (serendipitous) discovery of the cosmic microwave background (2.1) that we reconstruct following the hintikkas' approach (2.2) in order to obtain a game-theoretic characterization of the information-seeking strategies it illustrates (2.3). section 3 introduces the concept of distributed knowledge (3.1) and proposes that strategies for accessing information distributed in a community are continuous with strategies for accessing the distributed content of one's memory (3.2). we conclude with suggestions for further research. an appendix collects examples of deductive and non-deductive problems illustrating the relations between deduction, research questions, and recall."
"cognitive outsourcing of the type just described vindicates the view that serendipitous discoveries fall somewhere between sagacity and luck without reducing to either. in a case like the discovery of the cosmic microwave background, it is possible to describe inquiring agents as almost literally 'betting' on the ability of informants to provide additional information-be it explicit or implicit, where the latter includes the products of both memory recalls and deductive inferences (cf. sect. 1.1). formally, this betting process is captured by the game-theoretic notion of behavioral strategy. a (complete) pure strategy for a given player in a given game is a function assigning an action to that player, for every position it is that player's turn to play. for instance, a strategy for player white in the game of chess assigns one move for white, to every possible sequence of moves whose last element is a move made by black. a behavioral strategy is a sequence of lotteries over moves, equivalent, in our game of chess, to pick a move at random among those that white can play after each particular sequence whose last element is a move made by black. white can bet symmetrically (or 'equivocate') with an equal chance to play any of the moves allowed after black's; or she may have a particular bias for some moves at some position, and thus a greater chance to play some moves than others at that position. in the limit, the strongest possible bias recommends exactly one move: from a game-theoretic standpoint, pure strategies are degenerate cases of behavioral strategies."
"since the memory of each individual is also distributed, and \"contains information that was never explicitly stored there\" (cf. 1.3), there is a continuity between the means by which members of a community tap into knowledge distributed in that community and the means by which individual members of the community access the implicit content of their own memory. so far, we have seen that accessing knowledge distributed in the community involves behavioral strategies (lotteries over informants). we will now argue that behavioral strategies also capture formally the process of accessing implicit information distributed in one's memory."
"in this study, we proposed a model to evaluate performance for wsn by using the data of integrated performance evaluation index. the model, based on gray theory, had a better performance than conventional evaluation method. its advantage was that the whiten ability was much bigger than the linear discriminant evaluation model. gray theory-based evaluation method which also use the ahp to deside the weight of evaluation index, has a great potential for evaluation of wsn design and practical applications."
"holmes' view of memory is revealed in a study in scarlet. having casually mentioned that the earth revolves around the sun, watson is first surprised to find out that holmes ignored that fact, and then baffled when the consulting detective proclaims that he will do his best to forget everything about it. holmes then offers the following clarification:"
"to see why, think of ann's situation: if she is married, then because she is looking at george, she is the married person looking at an unmarried one; on the other hand, if she is not married, because jack is looking at her, he is the married person looking at an unmarried one. either way, the conclusion follows, so the information is already implicit in the first two sentences. [cit] levesque proposed his problem as a special case of a general problem for artificial intelligence (how to program computers to process implicit information and draw explicit conclusions). [cit], have earlier discussed as fully disjunctive processing of problem components. that is, one must consider the disjuncts of ann's marital status, which is the implicit information, in order to derive the correct conclusion. if ann is married, then the answer is \"yes\" because she would be looking at george who is unmarried. if ann is not married, then the answer is still \"yes\" because jack, who is married, would be looking at ann. the correct solution to this problem can be derived only by using a disjunctive strategy. [cit], the inspiration for the controlled experiment carried by toplak and stanovich, proposed that \"thinking through disjunctions\" amounts to exploring the branches of a decision tree. toplak and stanovich further decomposed the process in the married problem as: (1) raising an instrumental question (is ann married?); (2) no answer being available, reasoning by cases; and: (3) drawing the same conclusion in the two cases that need be considered. the process is represented in the decision tree below, assuming that the domain of discourse is a subset of the domain of persons: the \"deductive strategy\" is possible here because, whatever the answer a2.x to q2 is, the answer a1.x to q1 is the same. there is no need to actually obtain additional information."
"the scientific world journal 7 transform. pca projection coefficients provide an efficient feature representation of the approximated speech feature contours. for pca projection coefficients of approximated log frame energy contour, 44.39% average identification rate can be achieved. after integrating pca projection coefficients with the selected statistical analysis feature set, the average identification rate coefficients are increased from 80% to 82.26%. this result demonstrates the effectiveness of the proposed pca projection coefficients generated from approximated speech feature contours. in the future, the effectiveness of other different fourier coefficients can be exploited. moreover, wrapper selection and linear discriminant analysis may further increase the performance."
"as ap d50, ap d90, peakca, and cad90 have been shown to provide the best classification when varying the 407 computational model of interest [cit] ."
"uncertainties in model input parameters that are higly influential (e.g., as revealed by gsa) result, therefore, in lower 450 confidence in the predicted risk, while errors in estimating less influential model parameters are better tolerated by risk 451"
"depending on the context, these associations may just as well draw their attention to features of the environment relevant to their current task or divert it towards irrelevant ones. one can cultivate associations, as recommended by the method of loci or holmes' method. then, particular visualizations create patterns of co-activation in a bam-cam. the relation between the functional-level description (the locus an individual memory is kept in, and its relations with other loci and memories) and a material-level description (the patterns of co-activation of cliques of neurons) is more subtle than the one-one correspondence suggested by holmes' bam description. but holmes' recommendation to keep \"a large assortment [of tools in] perfect order\" has nonetheless a possible description at both levels. furthermore, it offers a first interpretation of the notion of strategic associations."
"so far, we have only considered probabilistic weights that result from pro-active (ex ante) decisions and choices, made prior to encountering particular problem-solving situations, and based on anticipations of what such situations might bring. however, the architecture of memory also supports reactive (in medias res or post hoc) associations in situations that one has not fully anticipated. let us first recall gabora's examples, which are both cases of creative thinking and problem-solving, and consider how gabora characterizes the neural correlate of creative insight in such situations:"
"in this paper, we have argued that means for accessing information distributed in the brain (associative memory) and in a community (distributed knowledge) belong to a continuum of strategies. in the light of hindsight, this should be no surprise. knowledge distributed within a community is also distributed across the associative memories of the individual members of the community. hence, strategies for accessing distributed knowledge should be expected to scale up strategies for accessing the content of one's individual memory. we illustrated this point with the example of a strategy leading to a serendipitous discovery, that scaled up strategies usually conducive of creative solutions in the single-agent case. the conceptual payoff of homing in on this example was the precise characterization of the relevant notion of 'strategy'. specifically, we showed that it is captured by the notion of behavioral strategy at the multi-agent level. we then came full circle, and justified our early identification of the method of loci as a strategy, by arguing that it supports real-time behavioral strategies for recall in the single-agent case."
"in the previous sections, we demonstrated that popular derived metrics for classification (i.e., ap d90, qn et) could be 293 well recapitulated by linear regressions constructed on direct features. in this section, we further explore the matter by 294 tackling the following question: what are the odds that a randomly generated metric based on the linear combination of 295 direct features would perform well in classification of tdp risk?"
"finally, the extremely low frequency components in (3) are transformed to time-domain by inverse fourier transform. the resynthesized time-domain signal is the proposed approximated speech feature contour."
"extracting appropriate speech features is an important issue in emotion identification. features can be used to describe the emotion characteristics of speech. suitable feature set can effectively increase the performance of classifiers. for emotional speech identification, most systems partition speech waveforms into segments called frames, and a classifier is trained using features extracted from frames. these features are frame-based speech features, which usually have an excellent performance for highly nonstationary signals. to further enhance the frame-based features, statistical analysis such as ratio, mean, and standard deviation of frame-based features usually creates more reliable speech features. in addition to statistical analysis of frame-based features, this paper also considers the approximated speech feature contours."
"it should be noted that gsa results are highly dependent on explored parametric space. here, we evaluate the that accurate classification of the ranolazine drug to low risk category requires a feature that is at least moderately 417 sensitive to variations in block of late sodium current, since the drug appears to be a pure herg and sodium channel 418 blocker. our gsa results (figures 1 and 2 drug-induced block of non-herg ion channels for 28 cipa compounds at their eftpc based on the measurements from the in vitro assay [cit] . a: stacked bar chart of six ion channel current block values for each of the 28 drugs.b: a swarm plot of block values of six ion channel currents categorized into high, medium, and low risk groups."
"calculation of the overall score of the a-type wireless sensor network using formula(12) repeat the above steps, we can get the integrated score. the integrated score of the a-type wireless sensor network is 7.268. the integrated scores of the b-type, c-type and d-type are 7.845, 6.935 and 6.646."
the number of sensitivity indices in (7) grow exponentially with k and typically only sensitivity indices of up to order two (s1 i and s2 i ) and the total-effect indices (st i ) are estimated [cit] . the total-effect index
"concentrations [cit], where model parameter estimates are inherently less accurate. however, it is important to emphasize 448 that the relative contributions of drug-induced modulation of ion-channels on output features differ significantly."
"the figure 3 shows a plot of the st index against the µ * statistic from the morris method. the observed strong 233 correlation is in agreement with previous analyses comparing elementary effects and variance-based methods [cit] . given 234 the results, the morris method was able to rank the relative influence of parameters with similar accuracy of the more 235 computationally expensive variance-based method. however, as discussed earlier, the variance-based sensitivity measures 236 provide also additional information on parameter interactions. 237"
"we perform gsa explicitly with respect to b current,drug rather than ic 50,current, c drug, and h. in this study, we refer to 66 the parameters of the block of in a, in al, ical, iks, ik1 and ito as bin a, bin al, bical, biks, bik1, and bito, 67 respectively. equation (1) is used in classification of real compounds. 68 sampling virtual drug population 69 the population of virtual drugs is created through monte carlo sampling from a high-dimensional (10-d) input 70 parametric space. the parametric space represent changes in model parameters used to describe drug binding and blocks 71 of ionic currents. basic cycle length (bcl) of cell pacing in the simulations was also considered as a parameter for gsa. 72"
"in y due to the second-order term x 2 2 . hence, even in the absence of parameter interactions, the model-derived metrics 250 can be nonlinear with respect to some inputs. we further examined the fitting of linear regressions of direct features 251 (assuming absence of nonlinearity and parameter interactions) to approximate the different model-derived metrics. the 252 goodness of fit was reported as r 2 values ( as linear combinations of the inputs, i.e., of the direct features. moreover, the small differences between the s1 i and 256 the r 2 indicate minor contributions of higher-order terms of individual inputs on variation of most of the derived features 257 except for diastolicca and peakv m. we also report the s1 i metric and r 2 values obtained for the m cell model in 258 regional sensitivity analysis figure 5a, b, c. the performance 288 measure of the classifiers reported as confusion matrices in figure 5d, randomly generated linear metrics 292"
"in the previous subsection, statistical analysis of frame-based acoustical features is performed. this subsection provides another point of view to extract emotion related information, that is, the temporal shape of a feature contour. as the detailed shape information may be merely aroused from the various phone pronunciations rather than the various emotions, an approximated speech feature contour is proposed. figure 2 depicts the flowchart to obtain the approximated speech feature contour."
"the results of our monte carlo filtering analysis ( table 5 and figure 4 ) indicate that the generation of eads, which 422 are thought to be cellular precursors of tdp genesis [cit], are most sensitive to variations in block of ical and to the figure 4 ). our analysis shows that several of the derived features proposed previously are 426 sensitive to different model inputs than those for eads (see figure 1 ). for example, the qn et metric is sensitive to 427 variations in block of sodium currents and block of sbikr for the endo cell model. in contrast, the bical and sbikr 428 parameters are the most influential for regulating eads."
"wireless sensor networks (wsns) are special networks comprising a large number of energy-constrained sensor nodes, which are densely deployed throughout physical space for observing surrounding ambient conditions [cit] . wsn can be widely used such as military, medical and industrial purpose. and the wireless sensor networks have presented interesting and challenging issues [cit] . inorder to optimize network service performance,designs the new network mechanism, quantification analysis and evaluate wsn is necessary. while the evaluation approach include game theory, ahp, topsis and so on [cit] ."
"the presented approximated speech feature contours rely on extracting extremely low frequency components to speech feature contours. to extract these frequency components, the symmetric property of discrete fourier spectrum is also required to be taken into account. assume lowestfrequency fourier coefficients (frequency bins) are to be extracted; the frequency component extraction is achieved by converting in (2) into (3):"
"[…] leakage of information from outside the problem domain occurs in a manner that is a priori unpredictable because it involves unearthing associations that exist due to the overlap of items in memory that may not have been previously noticed. because memory is distributed, coarse-coded, and content-addressable, items encoded previously to neurds are superficially different from the present situation yet share aspects of its deep structure. therefore, the recruitment of neurds may foster associations that are seemingly irrelevant yet potentially vital to the creative idea. [cit] in gabora's example of the beanbag chair (mentioned in sect. 1.3), the typical features of a chair include 'having a back' and 'having legs' and the atypical features '[adapting/not adapting] to one's body'. in a state of focused attention, there is a high probability of recalling examples of other objects sharing some of the first features, like benches and armchairs. in a state of defocused attention, associations with, and recall of, objects that share atypical features of a chair, such as a beanbag, become more probable. the same holds, mutatis mutandis, for fence posts and decommissioned skis. contextual focus is for the most part under the control of the subject, who can let her attention drift, or on the contrary re-focus it. since alterations of contextual focus affect recall in real-time, they are tantamount to selecting particular behavioral strategies for recall. 12 this description makes conspicuous that, from a strategic standpoint, defocusing one's attention and randomizing one's sources of information in an epistemic community belong to a continuum of strategies for accessing distributed knowledge."
"[…] those neural cliques that respond to atypical features of the situation, and thus that are activated in associative but not analytic thought, are referred to as neurds."
"the second step of the analysis targeted the small portion of metrics that were able to classify correctly at least 11 out 317 of the 12 drugs of the training set. the selected metrics were further tested for classifying the merged training-validation 318 set from cipa, again via multiclass logistic regression. note that we refit logistic regression (12) to the merged dataset 319 instead of reusing coefficients γ i obtained for the training set to quantify the ability of the metrics to separate full set of 320 drugs into correct categories. figure 6b and c show the frequencies of correctly predicted drugs from the merged 321 training-validation dataset, but when accounting only for the selected metrics generated from group a that were able to 322 classify correctly at least 11 of the 12 training drugs. note that the peakca and ap d90 based metrics classified correctly 323 less than 11 training drugs, but were included in these subpanels just for comparision. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 no. correct classified drugs table 6 . a: frequency distribution of the randomly generated metrics that classify a given number of cipa training drugs correctly. b: frequency distribution of the subset of randomly generated metrics that classify a given number of cipa training and validation drugs correctly and all of the cipa training drugs. c: frequency distribution of the subset of randomly generated metrics that classify a given number of cipa training and validation drugs correctly and at least 11 of the 12 cipa training drugs. labels qn et, ap d90, and peakca point to accuracy of these metrics for comparison. in order to estimate the confidence intervals, lhs was repeated for 100 different random seeds. changes of these two parameters were taken into consideration. we also compared the obtained thresholds for eads to 334 the thresholds considering drug-induced changes of all the seven ion channel currents measured from the in vitro assays 335 and characterized by the 9 parameters reported in table 2 . the results are summarized in the table 7 . our ead analysis 336"
"to convert these frame-based features into more effective features, statistical analysis is applied to each of them. this paper adopts three statistical analysis types, ratio, mean, and standard deviation. the ratio analysis is applied to silence/active and voiced/unvoiced to calculate the silence ratio and voiced ratio, respectively. as for the remaining features, both mean and standard deviation analysis are performed."
"by applying the defnition of whiten weight function, we can define the performance evaluation index kj u which belong to eth gray sort. herein, the gray evaluation coefficient of ith wsn, which belong to eth evaluation gray sort, is defined as"
"iks, ik1, and ito channels were not accounted for developing the metric as most of the cipa compounds had minor 299 effects on these channels."
"in this paper, parameter is chosen as 1, 2, and 3. taking log energy feature, for example, figures 3, 4, and 5 exemplify the approximated log energy contours of an angry, a bored, and a sad speech utterances, respectively."
"since the wireless sensor networks' usability is influenced by its performance immediately, we need to consider how to evaluation of performance for the wirelsss sensor network. the following is the key performance and trust performance of wireless sensor network, as shown in fig 1 [cit] ."
"for a \"skillful workman\" whose trade (like holmes') is a particular brand of problemsolving, the \"most perfect order\" is the one that fosters useful associations facilitating said problem-solving. in some cases, expert knowledge is the right tool for that trade. holmes' extensive readings about past criminal cases have informed him about the kind of evidence found at a variety of crime scenes. subsequently, he has made a special study of cigar ashes and written a monograph on the topic; 5 and he learned how to identify a newspaper clipping by its paper and the type of its letters. this expert knowledge sensitizes holmes to features of his environment that he can identify while others cannot. for instance, holmes spots in a study in scarlet scattered trichinopoly ashes on the floor of the abandoned house of lauriston garden, and later identifies the murderer upon finding the same ashes in his hotel room. in the hound of the baskervilles, holmes peruses words cut up from a newspaper and glued together to compose an anonymous letter, and identifies the shape of their type and the quality of the paper they are printed on, characteristic of the times. together with other indications, the finding suggests a complete method for identifying the sender. 6 m. b. and j. hintikka define the range of attention of inquirer as the set of yesno questions that inquirer is ready to raise in a given interrogative game [cit] . with this parameter, the role of a well-ordered 'brain-attic' in holmesian inquiry can readily be interpreted. for instance, in the above examples, expert knowledge is instrumental in improving holmes' range of attention, allowing him to devise problem-solving strategies. holmes' method exploits the features of associative memory: associations draw his attention to certain details that in turn trigger other associations and inferences, etc. this creates a feedback loop between attention (and perception) and associative memory that plays a major part in holmesian deduction. this feedback loop is critical to fully understand what m. b. and j. hintikka bill as the main task of the \"holmesian logician\":"
"(4) sensation precision the sensation precision of wireless sensor networks is the precision of sensation information from receiver. while the precision of sensor, method of information process, communication protocol of network affect the sensation precision in abundance."
"tdp. gsa provided a systematic understanding of the model input/output relationships and allowed for the identification 382 of the most influential parameters that regulate model-derived features of interest. in the future, the knowledge gained 383 from gsa could help improve the model structure and potentially guide the design of novel effective experiments to 384 increase reliability of model-driven predicted risk."
"an easy non-deductive problem. a non-deductive variant of the married problem is easily obtained, assuming the same premises, but substituting the principal question with: is an unmarried person looking at a married person? this variant cannot be solved without actually obtaining answers to instrumental questions. however, there is an optimal interrogative strategy to solve it, displayed below, under the slightly stronger assumption that the domain of discourse is the subset of the domain of persons that contains only jack, ann and george: the strategy is optimal, because it does not ask more questions than necessary, and is better than the strategy selecting q3 before q2. indeed, asking q2 warrants an answer to q1 in three out of four cases, while q3 does so only conditionally upon receiving the answer \"ann\" to q2, and provides redundant information in the other cases. the problem is 'easy' insofar as q2 and q3 are expressed with vocabulary occurring in the premises in q1 alone, and could thus be generated mechanically from them."
"besides the several primary factors, there are other trust factors to influence network. for instance, a node which has a higher trust value, is chosen to carries out the duty probability higher. then the electricity consumes quickly, therefore the electrical energy is one of consideration factors in trust value computation. and node's usability is also one of factors in trust value computation."
"(8) password mechanism in the majority of applications, the trust management is supplement method based on the cryptology mechanism, which enhances the network security. at the same time, the password mechanism also becomes one of main consideration factors in many trust evaluation index."
"variance-based analysis to be most sensitive to bin al, sbikr, bin a, and bical with contributions to the output variation of 30%, 26%, 17%, 227 and 10%, respectively. bical had the strongest impact on the variability of peakca concentrations with an s1 index of 228 around 0.6. the difference between the st and s1 indices in figure 2b shows the impact of higher-order indices. small 229 differences between s1 and st for several derived metrics such as ap d90, qn et, peakca suggest minor influence of 230 parameter interactions. the s1 and st sensitivity indices obtained for all the derived features (table 3 ) across different 231 cell types are reported in the supplemental material."
"the data acquisition is main application in wsns. according to application, sensor node needs to gather data through transmitting data to processing server for information, and the gather data is to be analysis and recorded by the server. in order to reduce the number of information flow, the transmission energy consume, memory demand in the network, data aggregation is applied frequently in upload processing of information, sometimes also be called data fusion. moreover the trust management usually uses to enhance data fault-tolerant ability, distinguish error message, enhances the upload data the accuracy and so on."
"(5) extendibility the extendibility of wireless sensor networks is the expansion limit which can be found from the number of sensor, covering field, life cycle, time delay, sensation precision. if the level of extendibility is assignment, the wireless sensor networks should provide the mechanism and method to support the level of extendibility."
"another promising thread of investigation would be the implementation of our theoretical model into a multi-agent computational model. in this model, an agent would be characterized by a memory representing the agent's background knowledge and a range of attention comprising the presuppositions of questions she is ready to ask. the memory would be represented so as to capture probabilistic relations between memory items. at each step of a discovery process at which a question is selected by an agent, she can also select a source: her memory, the environment or other agents. obtaining answers from any of those sources allows her to draw further conclusions. obtaining answers from other agents can have such effects as broadening her range of attention."
"thereby, suitable evaluation methods should be needed to identify and confirm those wireless sensor networks with more contribution. in this paper, an evaluation index is proposed on the basis of considering different wireless sensor networks performance. the paper uses the gray theory method in the process of evaluation by experts, while the evaluation process is effective. this paper is organized as follows. in the next section, we describe the influence factors and considerations of performance evaluation forwireless sensor networks. in section iii, based on the gray theory, we present the framework and methods for evaluating performance. section iv base on the ahp and gray theory, an illustrative example is presented. finally, conclusion is demonstrated in section v."
"hypercube sampling (lhs) from within the prescribed bounds reported in the table 6 . the first group of coefficient sets 302 (group a) was selected to ensure a certain degree of correlation to ap d, as exhibited by many derived metrics. more categories. in other words, the probability that a randomly generated metric would perform as well as qn et, which also 310 achieves perfect accuracy, is quite low. figure 6a shows frequency of the drugs that are correctly classified by metrics 311 generated from the coefficient sets of group a. we observe that a classifier built on a metric with random coefficients the low probability of having randomly generated metrics that would perform at least as well as ap d90 and qn et 315 highlights the strong link between perturbed ap d in an isolated cell and tdp risk."
"in yellow for the cipaord model [cit] . for comparision the estimated surface separating the ead+ and ead− for the 560 original ord model is shown in blue. [cit] . the cipa training compounds were plotted as red (high risk drugs), yellow 561 (intermediate risk drugs) and green (low risk drugs) dots."
"the hintikkas' interrogative approach to inquiry is limited to the single-agent case: a 'game against nature' is in fact a single-agent decision problem under uncertainty, where no strategic interaction takes place between players [cit] . there are however contexts in which strategic interaction occurs if inquirer addresses her questions not only to nature but also to informants with their own beliefs and preferences. in some contexts, inquirer and her informants may have conflicting preferences and informants could prefer not to answer truthfully. [cit] . in such cases, inquirer chooses her question so as to prevent associations from her informants' part. however, when no conflict between the preferences of inquirer and those of her informants exists, associations are not only welcome but may be sought after. crucially, when informants are reasoners, they can form inferential anticipations of their own. rather than letting inquirer come up with new questions, they can suggest questions to inquirer, based on anticipations of what inquirer could infer from answers to those questions, together with her background knowledge. [cit] ."
"take approximated lfe contour, for example; each value (section 3.2) associates with a set of pca bases derived from the training approximated lfe contours. value being 1, 2, and 3, respectively. it is noted that only significant pca bases are shown in these figures."
"binary risk stratification of virtual drug population 149 logistic regression classifiers were constructed with both the direct, table 2, and derived features, table 3, individually or 150 in combinations as inputs for binary classification of the virtual population of drugs into ead+ and ead− categories. 151"
"tal' refer to the broadening of inquirer's range of attention in response to unexpected external circumstances, and: (2) letting 'sagacity' refer to her ability to form strategic anticipations based on the inferential role of answers. second, the interrogative approach readily captures at least one relation between the two, namely circumstances under which chance external events broaden an agent's range of attention, and through associations, result in inferential anticipations that would not otherwise have been possible. in fact, the approach can capture other relations, and in particular strategies that increase the odds that such chance events will occur in contexts where interaction with other agents (and their brain-attic) takes place."
"component analysis. in this paper, principal component analysis is adopted to represent the approximated speech feature contours in an efficient way. principal component analysis is a well-known technique in multivariate analysis and pattern recognition [cit] . in this study, pca is used to reduce the high feature dimension of an approximated speech feature contour."
"the crucial part of the task of the holmesian \"logician\", we are suggesting, is not so much to carry out logical deductions as to elicit or to make explicit tacit information. [cit], pp. 156-157) deductive inferences are a rather common interpretation of eliciting tacit information, in particular in cognitive psychology and theoretical computer science (cf. appendix). after m. b. hintikka's death, the substitution of proof theory for game theory as a theoretical basis for the imi emphasized this interpretation. but when m. b. and j. hintikka suggested that eliciting tacit information by questioning could be viewed as a recall procedure, they hinted at another interpretation. in fact, they anticipated on recent findings in cognitive psychology, articulated in the following passage by cognitive psychologist liane gabora:"
"given the architecture of human memory, recall is best described as a probabilistic process. since items in memory are distributed as bundles of microfeatures, there is always a certain probability that recalling item i 1 would trigger recall of some other item i 2 sharing some microfeature with i 1 . the probability that i 1 and i 2 are associated during a particular recall episode depends in part on individual history. hence holmes' recommendation to keep one's brain-attic in order, lest \"knowledge which might be useful […] is jumbled up with a lot of other things\". methods of memorization, such as the method of loci, reinforce associations. they foster potentially useful associations by introducing particular biases in the probabilistic process of recall. more precisely, they alter the odds that a state of nature at some time t i is a state of nature where i 1 and i 2 are recalled together, as the consequence of steps undertaken at some time t i−n . with the notion of behavioral strategy in place, this can be expressed as selecting a particular lottery for a behavioral strategy for recall. anticipating on that conclusion, we offered it as a first interpretation of the notion of strategic use of memory."
"a harder non-deductive problem. a harder non-deductive problem is solved by sherlock holmes in silver blaze, in the process of identifying the thief of a race horse (silver blaze) and the killer of the horse's trainer (john straker). the police hold a suspect (fitzroy simpson) that circumstantial evidence places at the crime scene. the police believe that straker expected a nightly visit from simpson, acted upon the suspicion, and met his end at simpson's hand. the night silver blaze disappeared, the horse was kept in a stable together with a watchdog. holmes' reasoning is represented by the following interrogative-deductive strategy: the evening prior to the crime (a stable boy had let the dog at simpson, and the dog had indeed barked at him). inspector gregory, who does not recall that fact, fails to include it in the premises he considers, and subsequently to ask q2-4. the difficulty of the problem comes from the fact that q2-4 cannot be obtained mechanically from the premises, which do not feature the appropriate vocabulary. q2-4 can only be asked based on associations between the premises and some other content stored in memory."
"the 2-class svm is extended to multiclass emotion identification by one versus one technique. totally, there are 2 2-class svms built, where is the emotion class number."
"analysis. the framebased acoustical features extracted in this paper include silence/active, voiced/unvoiced, pitch, lfe, subband power, spectral centroid, spectral bandwidth, spectral flatness, mfccs [cit], and rss [cit] . among these frame-based acoustical features, several simpler ones are briefly explained below. silence/active denotes if a frame is a silence frame or not, while voiced/unvoiced represents if a frame is a voiced frame. pitch expresses the repeating duration in a voiced speech frame. lfe makes a logarithm operation on a frame energy. in the following, the more complicated frame-based acoustical features are explained."
"human ventricular myocyte model 12 [cit] c drug,ead -concentration of the drug that produces ead. c drug,arrhythmia -concentration of the drug that produces arrhythmia in the model, t dr -transmural dispersion, cqinward -metric that that quantifies the change in the amount of charge carried by inal and ical, ap d90 -action potential duration at 90% amplitude, ap d50 -action potential duration at 50% amplitude, diastolicca 2+ -diastolic calcium concentration and t dp population,score -the fraction of models developing repolarization abnormalities(ra) multiplied by a factor inversely related to the drug concentration at which those ra occur methods 39 to perform gsa, we generated a large set of virtual drugs. a virtual drug comprises a random vector of changes to 40 parameters of ion channels of the model. responses to the virtual drugs were examined through evaluation of several 41 model-derived features such as ap d 90, qn et, and peak calcium concentration (peakca). the cipaord model and 42 input parameters section describes the in silico model used in the paper. the details of the input parameters considered 43 for generating the virtual drug population are presented in sampling virtual drug population. the section in silico 44 simulations and derived features presents details on the derived features extracted from the in silico models. the virtual 45 drugs were also tested for their ability to induce ead in the model. in the section ead protocols we describe the 46 protocols used to test for ead generation in the model. the methods used for gsa are described in the global 47 sensitivity analysis. the methods used for classifying virtual drugs based on direct and derived features with respect to 48 ead are listed in binary risk stratification of virtual drug population. finally, we describe the methods for classifying 49 the 28 drugs selected under the cipa initiative, which we refer in the manuscript as \"cipa drugs\", with respect to their 50 defined torsadogenic risk in the section tertiary risk stratification of \"cipa drugs\". 51 cipaord model and input parameters 52 in this study, we perform gsa on the cipaord model [cit] . the cipaord model was developed at fda by introducing 53 several modifications to the original o'hara-rudy ventricular myocyte model [cit] to improve proarrhythmic risk 54 assessment. 55 several input parameters have been used for simulation of drug effects. for the herg channels, we used the 56 concentration response of the drug, e max, the unbinding reaction rate, k u, and the membrane voltage at which half of 57"
"cipa drugs based on eads, we observe that prediction improves by correctly classifying 3 more drugs when accounting 437 for drug-induced effects of other parameters in addition to the sbikr and bical parameters (table 7) . however, our 438 results also point towards the important consideration that errors in measuring the most influential parameters regulating 439 a particular metric have a bigger impact on the predicted classification compared to neglecting some of the less influential 440 parameters. gsa allows us to determine and rank most of the critcal model components. uncertainties in in vitro measurements of drug-induced effects on ionic currents present an important concern in 443 evaluating the torsadogenic risk of compounds by interrogating in silico biophysical models. discrepancies in estimates 444 for model parameters based on available in vitro assay data have been recently highlighted in uncertainty quantification 445 studies [cit] . high uncertainty in model parameters leads to low confidence in model predicted risk, and thus, not 446 surprisingly, risk stratification of the cipa training drugs proved to be unreliable especially at high drug 447"
"rules allowing inquirer to ask questions about said state of nature. in this reconstruction, answers (when obtained) strengthen the background information available to inquirer from the premises and warrant further conclusions about the state of nature. in a deductive problem, one can reason to a solution through deductive inference from the premises alone. in a non-deductive problem, deductive inferences provide no solutions unless additional information is collected, 'transforming' in all effect the problem into a deductive one. the reconstruction uncovers the relations between strategies for solving deductive and non-deductive problems."
"a strategy conducive to serendipitous discovery, such as the one followed by penzias and wilson (and to a lesser extent by dicke and peebles), is a behavioral strategy with lotteries over informants at given positions of the inquiry. inferential moves are still deterministic, but informants can provide with unexpected information, and may open unforeseen possibilities for further inferences. hence, the inquirers' strategies cannot be represented by complete strategies, as they can become aware of new possibilities for inferential moves when new questions enter their range of attention. representing the inquirers' awareness requires resources beyond those of classical game theory. one needs to associate to each position it is one of the players' turn to play, a representation of the game for that player at that position. and one needs to define the players' strategy spaces relative to these indexed representations. extensive games with (un) [cit], and we discussed their application to inquiry games (in particular multi-agent cases with uncooperative sources) [cit] ."
"in this section, based on evaluation index of performance in wsn, a gray theory-based approach to evaluate performance of wsn is developed, in which a gray judge matrix is used too. moreover, the judge matrix is given by the whitening function. and the ahp method is applied to determine the weight of evaluation index. and this index in the wsn performance evaluation, such as energy validity, life cycle, time delay and so on, should be calculated to get the judge subjection matrix, using gray stat method. in the following section the step of this evaluation approach is presented."
"[a]nalytic thought requires a state of focused attention, and associative thought requires a state of defocused attention. creativity involves not just the capac-ity for both but the capacity to spontaneously shift between them according to the situation, referred to as contextual focus. […] [c]ontextual focus is explicable at the level of cell assemblies […] composed of neural cliques, some of which respond to situation-specific aspects of an experience, and others of which respond to general or abstract aspects."
(2) life cycle the life cycle of the wireless sensor networks is the time interval which from the network running to unproviding the information for receiver's needs. and the influence factor of life cycle in wireless sensor networks includes hardware and software.
"(3) time delay the time delay of wireless sensor networks is the time interval which from the request sending to the needs information receiving. and the time delay is related with using closely, which affects the usability and using field of wireless sensor networks."
"the scientific world journal where represent the resampled feature contour and its duration has been normalized to length. moreover, refers to the th feature in the feature contour of the th speech utterance, and is the th fourier coefficient of the th speech utterance."
"finally, cooperative search with combined fairness models produced the fairest nurse rosters. supposedly, this phenomenon can be ascribed to the fact that cooperation of the different fairness models introduced an element of efficiency. this could be in line with the strength of human cooperation, where the cooperation of nurses sharing different impressions of fair nurse rosters can lead to better rosters in terms of fairness. the results have confirmed that the cooperation of multiple decision makers who do not necessarily share the same models and the same heuristics produced fairer rosters than optimisation from a single planner's point of view. future work will investigate the fairness issues in other problem domains."
the algorithm stops the optimization process if one of two conditions is met. the first condition is a set number of iterations. the second condition is the distance between the current iteration and the previous one is less than a certain value . figure 2 shows the flow chart of the l 1/2 -nmf algorithm. the l 1/2 -nmf algorithm can be summarized as follows [cit]
"in the second test scenario, all the fairness models are used and tested at once. this is motivated by the idea that people planning a roster will have different concepts/models of fairness. if these models are allowed to cooperate, will the rosters be fair and obtained fast? furthermore will this approach enable us to identify the best performing fairness-based objective function and guide search process? to this end the experiments are conducted in accordance with the first scenario in that 12 metaheuristic agents and 1 launcher employed under the cooperative search framework and 20 runs are performed for each instance. however, in this scenario three agents are grouped to form four different groups. each group optimises with respect to one of the m inm ax, m indev, m inerror and m inss fairness-based objective functions, separately. table 6 provides the results for scenario 2 and compares its performance to the best stand-alone metaheuristic, vns using m inm ax and cooperating agents under scenario 1 using m indev based on the average and best jains fairness index over 20 runs. the scenario 2 algorithm outperforms all standalone algorithms (table 4) regardless of the objective function generating the most fair rosters for any given instance and this performance variation is statistically significant for all instances. moreover, cooperative search under scenario 2 consistently outperforms scenario 1 using m indev or any other fairness based objective function on average and this performance difference is statistically significant for each instance. scenario 1 and 2 produce the best rosters for three and four instances, respectively, with a tie in one instance. figure 4 provides the box plots of jains fairness index for the geriatric i instance over 20 runs considering cooperative and stand-alone metaheuristics using different objective functions as an example. vns, tabu and sa performs best using m indev on this instance. the results confirm the success of scenario 2 over an approach with no cooperation even if it is used with the best performing objective function and this performance variation is statistically significant, except for emergency i. still, scenario 2 performs slightly better than scenario 1 using m indev. table 6 : performance comparison of stand-alone vns metaheuristic and scenario one using m indev, and scenario 2 based on the average and best-of-runs jains fairness index values over 20 trials for each instance. bold entries mark the best algorithm."
"to compare the performance of two different fairness objective functions, a and b over a given set of instances, we use %gap indicating the percentage change that an algorithm using the objective function b generates over a (i.e., gap from a to b):"
each agent ran on a hp compaq 6000 pro with intel core 2 duo e8400 processor with 4 gb ram in a 2x2 gb configuration. the agents were configured to use only 1 gb of memory.
this protocol involves diversification and intensification phases. diversification involves pattern matching and the application of the greedy heuristic to produce a new roster. this roster is then intensified using a metaheuristic. one application of these two phases is called a conversation. it should be noted that the pattern matching results in a controlled diversification of the search. good patterns are retained and used to build new potential solutions which will diversify the search just enough to enable new areas of the solution space to be explored. the metaheuristics then intensify the search.
"the term µ(.) defines an additive non linear term that depends on the end-member matrix a, the abundance coefficients in s and additional non linearity coefficients b which adjusts the amount of non linearity in the pixel."
"in addition to the overall lack of correspondence between the model and the data, there is an important limitation in the interpretation of these results: the manipulations of the ''jaw'' component necessarily entail tongue shape changes. [cit] modeled cineradiographic images and labiographic data of speech and began by factoring out jaw motion with principal component analysis (pca). then, the pca was performed on the residual to find the contribution of the tongue and lips. thus the contribution of jaw movement to tongue shapes through linear regression might not correspond to the purely anatomical/biomechanical contribution of the jaw to the tongue shape, which is what is required to test the f/c account [cit] . specifically, french (on which the model was based) has many items with the vowels /i/ and / / in its vocabulary. we might expect jaw and the overall tongue body to be raised together for /i/ and lowered for /a/ or / / (cf. [cit] ) . there should be a high correlation between jaw and tongue's vertical position. but because /i/ also involves a substantial amount of tongue root advancement in french and english, the advancement will also be highly correlated with jaw movement even though such advancement is not in fact caused anatomically or biomechanically by the jaw movement, as can be seen, for example, in advanced tongue root vowels in akan [cit] ."
"the vowel dataset generated in section 3.1 was used in the ap simulations. hence, the target parameters of tbcl and tbcd gestures for vowels corresponding to each tbc position do not represent a vowel space for a specific language but rather a systematic sampling of typical vowel spaces. the vowel gestures, tbcl and tbcd, are realized by jaw (ja) and tongue body (cl, ca) articulators. we set all the articulator weights to the same value to exclude language-specific articulator synergy settings. the vowel dataset was compared to the consonant datasets which were modulated by constriction location and articulator weight, as detailed in sections 3.2.1 and 3.2.2, respectively. because the taskdynamical control model in tada currently functions only for an adult vocal tract, the simulations were only carried out on the adult vocal tract."
"optimisation problems are usually solved by one or more human planners who developed a mental model of the objective to be optimised and a heuristic procedure for generating solutions. one of the hardest issues to address when developing decision support systems is modelling of the problem in a way that the human planners experience as natural, while producing solutions that reveal part of the optimisation process. when multiple decision makers are involved, not necessarily sharing the same models and the same heuristics, the problem turns out to be even more challenging. an obvious example can be found in hospitals, where individual nurses cooperate and have a say in establishing the monthly roster for the entire ward, at least when the process is not fully automatised. an interesting question is whether situations with multiple decision makers can be mimicked into an optimisation approach and whether the obtained results are better than after optimisation from a single planner's point of view."
"according to the f/c account, a cycle of oscillation starting from an arbitrary initial vocal shape results in a cv-like sound. random configurations of the initial vocal tract shape were obtained by varying five articulator variables in casy: ja, cl, ca, tl, and ta (see fig. 1 ). reasonable ranges of the variables were chosen to cover physiologically plausible configurations, without making an initial constriction along the vocal tract. once a random vocal tract shape was selected, a cycle of up-and-down jaw (j) movement was modeled by varying the jaw angle (ja). as the jaw moved up and down, the other variables, ca, cl, ta, and tl were kept constant. the jaw was raised until either the tongue or the lower lip made contact with a fixed structure, creating a constriction. the constriction site was identified by finding the section where the area function was zero along the vocal tract, with the possible regions being labial, coronal or velar. due to the nature of the articulators and their movement, there were clear clusters at the coronal and velar places, with no ambiguous locations. once an obstruction in the vocal tract occurred, the jaw was lowered by modulating ja. how far the jaw was lowered, that is, the ja value at the landing site of jaw lowering, was randomly selected within a physiologically plausible range. during a complete cycle for producing a cv (starting with a random vocal tract shape and then adding jaw raising and jaw lowering), cross-sectional area functions were estimated at each movement frame and resonance characteristics were then computed to generate acoustic signals. any physiologically implausible configuration before or after jaw oscillation was excluded. the simulation was continued until 1000 trials were obtained. we employed tongue body position (more precisely, tongue-body circle center position (c in fig. 1) ) in casy to identify vowel type as front, center, or back."
"next we implement the element-by-element division of the numerator and the denominator terms. for this module we use a floating point divider as the fixed point divider outputs are quotient and remainder while the floating point divider output is a rational number. as shown in figure 13 denominator terms as shown in figure 14 . the resulting values of the numerator and denominator terms are passed to the element-by-element divider. the divider result is multiplied by the abundances matrix s and the result is saved into a ram block with the same dimensions as the abundances matrix s. two control blocks are used for generation of required reading addresses, read enable, division and multiplication enables. the resulting value are saved into a ram block with the same dimensions of the abundances matrix s."
"the modulation of articulator weight simulations were undertaken to ensure that obtained pattern of results was not specific to a particular assumption about the pattern of articulator cooperation in the production of a given constriction. the synergy values for all from weight values table 6 weight values (in model internal units) at step 1, 10 and 19 for three consonant types: labials (top), velars (mid) and coronals (bottom). proximal articulators for each consonant are put in bold."
"1. a metaheuristic agent taking on the role of conversation initiator starts a conversation. it takes a new roster either generated from a previous conversation or supplied by the launcher agent. the new roster is then improved by the initiator agent. when an improved roster is generated locally, it is sent to the other metaheuristic agents. 2. the metaheuristic agents have also generated their best-so-far rosters using their fairness objective functions. they break up the rosters sent from the initiator and their own into pairs. the pairs are then compared and only those that are common to both rosters are kept. heuristicdata objects are created from these pairs storing the first and second assignments of the pair. these are then sent by the metaheuris-tic agents to the initiator. the metaheuristic agents also send the value of their fairest roster found so far. these rosters will be used by the initiator to determine which metaheuristic agent will be the new initiator in the next conversation. 3. upon receiving the heuristicdata objects from the other metaheuristic agents, the initiator pools them locally. each heuristicdata object is scored by counting how frequently it occurs in the pool. the initiator then tries to build a linked list from these high scoring heuristicdata objects. for example, if the pool contains the following heuristicdata objects with first and second assignments expressed here as pairs (4,7) (6,1) (7,2) (2,6) (5,9) (3,8), the linked list generated from the heuristicdata objects will have the following order (4,7) (7,2) (2,6) (6,1). any heuristicdata objects not linked in this way are stored in an unlinked list (5,9) (3,8). 4. the initiator then determines which metaheuristic agent is going to be the initiator in the next conversation. this is done by pooling all the fairness objective function values of the best rosters found so far by the metaheuristic agents and then identifying which metaheuristic agent has the best fairness objective function value. the metaheuristic agent with the best objective value will be the new initiator in the next conversation. the initiator then sends these lists of best-so-far heuristicdata objects to the metaheuristic agents. in the same message it also indicates which metaheuristic agent will be the new initiator in the next conversation. 5. the metaheuristic agents receive the lists of heuristicdata objects. both initiator and metaheuristic agents then create a new roster from these objects, as well as their current best roster. the new roster is created by trying to build first a list of heuristicdata objects, while the metaheuristic agent's best-so-far roster provides any missing numbers. in this way a new unique roster for each agent is generated and the objective function value is calculated. 6. the conversations are repeatedly exchanged between the metaheuristic agents for a maximum number of conversations set in the configuration file of the launcher agent."
the variable neighbourhood search agents implement a perturbation heuristic which uses local search heuristics to improve an initial solution. the algorithm proceeds by randomly choosing a heuristic from the list of available neighbourhood operators. each possible move is validated against the hard constraints ensuring that at every stage of the search each new solution remains feasible.
"a brief overview of the nurse rostering problem and a set of alternative fairness objective functions along with measures of fairness for nurse rostering are provided in section 2. in section 3, we present an agent-based framework for cooperative metaheuristic search for fair nurse rostering. the experimental design is discussed in section 4, including the implementation details of different metaheuristic agents. the results of the computational experiments are reported in section 5. section 6 concludes with a discussion on the contribution and some directions for future research."
"mixing models are classified to linear and non linear models [cit] . the linear mixing model (lmm) is valid when end-members follow a discrete distribution where each endmember does not interfere with other end-members [cit] . under the linear mixing model, it is assumed that the endmembers are mixed linearly as stated in equation 1"
the metaheuristic agents cooperate asynchronously by iterating a communication protocol which is a distributed algorithm. at each iteration a conversation initiator (identified in the previous iteration) sends the result of its latest metaheuristic search using its own fairness objective function to the other agents. the agents compare this result with their own and then generate patterns (section 3.3) which they send back to the initiator. the initiator then identifies good patterns and shares them amongst the metaheuristic agents. each agent then generates a new potential solution. the metaheuristic agent with the lowest objective function value in the present iteration is chosen to be the initiator of the next. in the next iteration the new potential solutions are used to further the search in the manner just described. figure 1 shows how the agents perform a search by cooperating with each other running asynchronously in parallel and executing different metaheuristic and heuristic combinations with different parameter settings. they use ontologies (section 3.2) to enable the agent-based system to be adapted easily to new problem domains.
this paper describes a flexible generic agent-based cooperative search framework to build fair nurse rosters. the cooperative search framework combines the strength of multiple metaheuristics and several fairness objective functions to generate high quality rosters. the agents improve the quality of local rosters using the same or different metaheuristic solution methods with the same or different fairness objective functions and parameter settings. the agents cooperate asynchronously using pattern matching and reinforcement learning in order to generate fair nurse rosters.
"3.2.1.1. constriction location modulation. for initial consonant targets, we generated a range of gestures for each consonant type: lip (la, pro), tongue tip (ttcl, ttcd), and tongue body (tbcl, tbcd) gestures for labial, coronal, and velar stops, respectively. their constriction locations were modulated as follows. location for labials was not varied because physiologically plausible differences in the location of a labial constriction are small. five locations in the coronal region, using the tongue tip, and five in the velar region, using the tongue body, were used to simulate variability in consonant production. note that constriction degree was fixed to 0 for all the consonant types to ensure complete constriction. constriction location values were specified using the angle of a polar grid, in which 0 degrees represents the location of the upper lip, 90 degrees represents the center of the hard palate, and 180 degrees represents the mid-pharynx. the tongue tip closure location varied from 46 to 86 degrees (alveolar to palatal) and the tongue body location from 105 to 145 degrees (palatal to uvular) by increments of 10 degrees."
"all experiments were conducted on real-world benchmark problems with 20 repeated runs for each problem instance. in the first scenario, these experiments were conducted five times for each of the five objective functions. the resulting nurse rosters were then each recalculated using the other objective functions not used in the optimisation of the roster so that they could be compared with the original. in the second scenario each experiment was also repeated 20 times for each instance. the agents conducted only 200 conversations to complete each search taking no longer than 6 minutes to produce a new roster for each problem instance."
"this data set consists of three end-members representing water, soil, and tree. figure 22 shows the spectra of the three end-members. the image consists of 156 channels covering the wavelengths from 401 nm to 889 nm."
"in order to classify the resulting tongue body positions as front, center or back, we created a reference set of 300 physiological plausible tongue body center positions for the adult vocal tract, and 65 positions for the child (see fig. 3 ), and then divided the resulting two-dimensional space of points into regions corresponding to front, central, and back vowels. the region boundaries of this map were then used to classify final tongue body center position of each f/c simulation. the partitioning of the space of the vowels into regions was done using three separate criteria: articulatory, acoustic, and perceptual. in computing the acoustics (and therefore subsequent perception) of the vocal tract shapes resulting from the tongue body positions, all other articulators, including the jaw, were fixed. even though jaw height will affect vowel identity, the fixed jaw assumption is reasonable, as we are only attempting to classify the front-back classification of the tongue body positions, not the degree of openness (which is what will vary as a function of jaw height)."
"simulations of the f/c account were carried out for both adult and 7-month-old infant vocal tract configurations. the default configuration of casy, which is based on sagittal x-ray images of adult vocal tracts, was used for the adult vocal tract (fig. 3b) . the casy vocal tract configuration for babblers (fig. 3a) was estimated by visually fitting the casy vocal tract outline to that of a 7-month-old infant's midsagittal mri image [cit] ) using a visual fitting tool. although this gives good midsagittal fits, the effect on the distance-to-area functions necessary for generating three-dimensional tubes is unknown. here, we make the assumption that the cross-sectional area for a constriction of a given distance will be similar to that of adults, but it could well be that three-dimensional imaging of infant vocal tracts would reveal differences."
"in the second scenario, the same instances were used but all four fairness objective functions are used at the same time to generate the nurse rosters. the idea behind this test is to identify which fairness measures are best for each problem instance. of the twelve metaheuristic agents, a group of three different metaheuristic agents generated nurse rosters using m inm ax, three ran m indev, three ran m inerror and finally three ran m inss. when the search is finished, each agent sends their best roster to the launcher which then selects the roster with the largest jains fairness measure."
the second term of the cost function equation s 1/2 is implemented as shown in figure 7 . the l 1/2 term is implemented as in equation 4. the square root module is implemented in three blocks as shown in figure 8 . altera floating point square root ip is used along with two converters from fixed point to floating point and vice versa. the square root module needs 40 clocks to perform the operation.
the jains fairness index defined in (equation 8 ) is used to evaluate the relative fairness of the solutions produced by each configuration. the average percentage increase (equation 9) from m inw s for each instance was used to evaluate the efficiency of each fairness objective function in terms of number of constraint violations.
the parameter settings of the individual agents remain unchanged throughout all testing. each agent evaluates its given metaheuristic for 500 iterations or until no further improving solutions are found. experimentation shows that it provides a good balance between diversification using pattern matching while the 500 iterations of the metaheuristic intensifies the new roster just enough that the agents tend to converge to good rosters. ts agents have a tabu tenure set to seven. sa agents use a geometric temperature cooling schedule set at 0.9 for a more diverse search. all agents randomly select the next local search heuristic to run.
"the output of stage 1 is saved into a ram block and multiplied by the abundances matrix s. as shown in figure 17, the matrix multiplication is performed on two stages as a multiplier and an accumulator. control blocks are needed to generate control signals for the multiplier and accumulator enables, read and write enables and generating reading addresses."
"a word can be described as a constellation of gestures, and the lexical contrast between words can be related to the presence or absence of gestures and the relative timing among them, all of which are encoded for each word in a coupling graph [cit] . each gesture is specified with its constriction location and degree information. for example, /s/ involves a tongue tip constriction gesture, which is defined with a 'critical' (for the production of turbulence) constriction degree at 'alveolar' constriction location. /p/ and /n/ involve complete constriction ('closed' constriction degree), one employing the lip constrictor and the other the tongue tip constrictor at the 'alveolar' constriction location. the velum and glottis gestures are defined only by their constriction degree ('closed' or 'wide') because their constrictions occur at fixed locations."
"next, we multiply the result of stage 1 by the matrix a as done is stage 1. the result of stage 1 is saved in a memory element and accessed through control signals. both matrix multiplication processes are broken down to a multiplier and an accumulator as shown in figure 11 . as shown in figure 12, the numerator term is implemented in the same manner as the denominator using the off-chip memory, a randomly initialized memory element, a control block with control signals for multiplier, accumulator, read and write processes and another control block for reading addresses."
"step 1 step 10 step 19 for a given c were averaged and then cv synergy ratios were calculated as they were for the constriction location simulations. the results presented in table 8 are quite similar to the results of the constriction location simulations, which employed a fixed set of weights. thus, the obtained patterns of cv synergy ratios appear not to be due to particular weight settings."
"the proposed hardware implementation of the l 1/2 -nmf algorithm for the synthetic data set is synthesized using intel quartus ii 15.0 web edition. for the real data, the proposed design is synthesized using quartus prime 15.1 standard pro edition which supports newer fpga families. table 2 summarizes the resources used for the proposed hardware implementation of the l 1/2 -nmf algorithm for both synthetic and real data sets."
the goal of automated nurse rostering is to assign shifts to a set of nurses so that 1) the minimum staff requirements are fulfilled and 2) the nurses' contracts are respected [cit] .
"1. an agent-based framework for cooperative metaheuristic search cooperative search provides a class of strategies to design more effective search methodologies by combining metaheuristics for solving combinatorial optimisation problems. this area has been little explored in operational research. we introduce a generic agent-based distributed framework where each agent implements a metaheuristic and a fairness objective function. an agent continuously adapts itself during the search process using a cooperation protocol based on reinforcement learning and pattern matching. good patterns which make up improving solutions, are identified and shared by the agents."
"[f], for example). thus while assuming some level of control of articulators other than the jaw strikes some researchers as unparsimonious, it does do a better job of accounting for a wide range of findings. it is not, strictly speaking, an evolutionary account, but it makes the assumption that there was a chain of adaptations of facial imitation (common to hominids) to vocal imitation (apparently unique to humans among primates) [cit] )."
"3.2.1.2. articulator weight modulation. a consonant gesture involves synergistic movements of its associated articulators to achieve its constriction target (see section 2.2). the weighting parameter for each articulator specifies how much it is engaged in achieving the gesture's target, relative to the other associated articulators. we can introduce some amount of randomness by modulating the articulator weights. note that the weight parameters are relative values for a given gesture and their relations of any pair to one another should be understood in terms of their ratio. to systematically vary the articulator weights for a given c, we first need to determine the reasonable range of weight ratio sets. the weight ratio between proximal and distal articulators was varied in 19 steps where the proximal articulators were the lightest at step 1, the heaviest at step 19, and the mean value of the weight ratios coincided with step 10. the range for the weight modulation was chosen so that an appropriate constriction for a given consonant was ensured, and so that the mean weight ratios (for step 10) were as close to one as possible. table 6 shows the values of articulator weights used for each consonant. for labials, the mean weight of the upper lip is set slightly heavier than lower lip because the upper lip cannot move down as much as lower lip can move up. for coronals, we shifted the whole range of weight variation to avoid occurrences of non-coronal constriction and hence, the mean ratios of articulator weights are not as close to one as the other consonants. for velars, the mean ratios of cl, ca and ja are all equal at step 10. since we want to see the effect of constriction location and articulator weight variations separately, we used a default set of constriction locations: 56 degrees and 135 degrees for the constrictions of locations of tongue tip and body, respectively, for the weight modulation."
"we also introduce the objective function m inss which is similar to m inm ax as it tries to reduce the worst roster of a given nurse. however, it does this by emphasising on the worst individual rosters by squaring. the aim is therefore to reduce these poor individual rosters. w 1 and w 2 are weights used to scale the two parts of equation."
"the synergies for the front vowels show a peak at the tongue tip constriction location at 66 degrees (a post-alveolar position), with fairly continuous changes on either side of that. the central vowels have high synergy with the labials, low with the coronals and high again with the velars. there is both gradience and stability within this pattern. the back vowels have high synergy with the labials, consistently low synergy with the coronals, and increasing synergy with the velars as place of articulation becomes more posterior."
"where (.) t represents matrix transpose and.* and./ represent element-wise multiplication and division, respectively. s − 1 2 is the element-wise inverse square root of s. in order to ensure the full additivity constraint for the fractional abundances is achieved, both matrices x and a are augmented with a row of constants as follows:"
"the hypothesis that cv preferences have a physiological basis receives support from these simulations. the articulatory phonology account matches the observed data more closely than the frame/content account. while some caution is necessary in interpreting these findings because the synergies are based on an adult vocal tract (and this should addressed in the future), the fact that the model is so successful in reproducing the offdiagonals gives us some confidence. after all, the fact that it is an adult model did not a priori guarantee that it would generate the off-diagonals correctly. the synergy model can also help explain why it is that even in the overall lexicons of adult language, these preferences tend to appear, under the hypothesis that c and v gestures in a cv syllable are triggered synchronously. if the preferences only occurred when control was absent, they should not exist in the adult data. thus a gesture-based account of speech production can help us understand some of the properties of (seemingly universal) language acquisition."
"two simulations have shown that there are plausible articulatory sources for the observed preferences in consonant-vowel combinations, both in adult speech and in babbling. both the frame/content (f/c) account and the articulatory phonology (ap) were tested with articulatory synthesis. the ap account generated ratios greater than 1 for the preferred combinations that have been found in transcription data. the f/c account did not consistently generate such ratios, and often had velar/back ratios that were much larger than any reported from babbling data. the diagonals only account for approximately half of the observed data, though this cannot be seen from the ratios themselves. thus the ratios for the off-diagonals should be examined for systematicity. here, we found that the ap account produced ratios that were much closer to the observed data than the f/c account. even though the f/c account makes no predictions for the off-diagonals, our f/c simulation resulted in a better match to the off-diagonals than it did to the diagonals for the infant simulation. the error between observed ratios and the ap values, though, were 1/3 the size of the f/c errors."
"(2, 4), (4, 7), (7, 6), (6, 5), (5, 8), (8, 9), (9, 0), (0, 1), (1, 3), (3, 2) . the permutation is broken into patterns of the same length while retaining the basic order of the permutation. the agents can then each compare pairs of assignments generated by their own metaheuristic with pairs shared by the other agents. all the pairs from each agent are scored based on how frequently they appear. only those pairs that have the highest frequency score are shared out amongst the agents. the idea behind this step is that the lists of assignments have already been optimised by each agent's respective metaheuristic so each list must have elements that make up a good roster. it is argued that these pairs will occur more frequently as the search progresses."
"we start by implementing the matrix multiplication (ss t ) as shown in figure 10 . we use two ram blocks representing the abundances matrix and its transpose. in addition, a control block is used for read and write control signals, write addresses and read enable. another control block is used to generate the read addresses, multiplication and accumulation enables."
"the ontology is represented in xml, corresponding to the representation of most of the nurse rostering benchmark problems. this makes the interface between problem definition and ontology seamless in practice. as a consequence the elements defined in a specific problem instance will be used directly by the system. figure 2 shows the structure of the ontology and how solutionelements is the interface between the framework and the nurse rostering problem instances."
the l 1/2 -nmf algorithm follows the lmm. the only known term in equation 1 is the hyperspectral image x. in order to find a and s we perform l 1/2 -nmf to minimize the distance between x and as. many methods can be used to measure such difference. the algorithm uses the euclidean distance method and it is given as follows:
"the preference for certain cv combinations has been found repeatedly, but there is currently no study directly measuring the articulators during babbling in order to judge which biomechanical model might be supported. such data are difficult to obtain, although ultrasound measures are showing promise. instead, we can make use of articulatory synthesis, in the form of a software synthesizer that allows independent control of tongue, jaw and lips, to model the systematic variation associated with the f/c and the ap accounts."
"the global patterns found for the ap simulation can be seen at all four places of articulation within both the coronal and the velar locations. although there are some gradual changes within each category, there are clear differences between categories. further, construction of cv probability ratios using any one of the five coronals and with any one of the five velars exhibit the same pattern: all three diagonals are predicted to be greater than 1."
"the metaheuristic agents are configured from a file which they execute at start-up. this file specifies to the agent which metaheuristic to run; how many iterations to perform; which fairness objective function to use; also, parameters such as the size of tabu tenure or which cooling schedule function to use if configured to run sa. all agents run the same local search heuristics described above."
"despite outperforming the ppi, nfindr and vca algorithms [cit], l 1/2 nmf algorithm is not yet implemented on any target platform. this paper presents, to the best of our knowledge, the first fpga based architecture l 1/2 nmf algorithm. the paper provides real-time synthesis results, comparison with matlab simulation results and ground truth spectra."
"in this section, we illustrate the hardware implementation of the module given by equation 6. as shown in figure 9, equation 6 can be broken down to numerator and denominator terms. each of the terms is handled separately then the element-by-element division and multiplication operations are performed at the end. the numerator term is divided by the denominator term and the output is multiplied using and element-by-element multiplier by the end-members matrix a. two control blocks are used for generation of required reading addresses, read enable, division and multiplication enables. the numerator term values are divided by the denominator term values in an element-by-element operation. next, the division result is multiplied by the corresponding value in the end-members matrix a. the resulting values are saved into a ram block with the same dimensions of the end-members matrix a."
where δ is a positive factor that controls the achievement of full additivity constraint. δ value is in the range between 10 and 20 to balance between convergence rate and accuracy of the estimation. another important problem is dividing by zero in the term s − 1 2 . any zero value is replaced by a small value to avoid trivial results [cit] .
"we start by implementing the matrix multiplication between the transpose of the augmented end-members matrix a t f and the augmented matrix x f as shown in figure 15 . as previously mentioned, the matrix multiplication is broken down to a multiplier and an accumulator."
the next section describes the cooperation protocol and how this is implemented with the use of ontologies. the interface between the framework and problem instances 3.3. asynchronous cooperative search by pattern matching and reinforcement learning for nurse rostering the agents cooperate by exchanging patterns defined as a permutation of assignments. this is a simple distributed metaheuristic.
"an alternative account based on articulatory phonology (ap) relates the cv biases observed in infant utterances are due to inherent compatibilities or synergies in the speech production system. in ap, the vocal tract is decomposed into distinct constricting devices (lips, tongue tip, tongue body, velum and glottis), which are used to form linguistic gestures, i.e., constriction tasks at vocal tract locations appropriate to those constrictors. gestures for oral constrictors (lips, tongue tip, tongue body) are defined by constriction location and degree task variables: lip protrusion (lp)/lip aperture (la), tongue tip constriction location (ttcl)/tongue tip constriction degree (ttcd), and tongue body constriction location (tbcl)/tongue body constriction degree (tbcd). since the location of non-oral constrictors (velum and glottis) does not vary, they only use constriction degree variable: vel (velum) and glo (glottis). thus, these eight ''vocal tract variables'' constitute the dimensions along which constricting tasks are defined. importantly, control of each tract variable harnesses a set of articulator degrees of freedom that can contribute to the ongoing constricting action into a coordinative structure [cit] . individual articulator degrees of freedom can participate in more than one tract variable coordinative structure. table 3 shows the set of tract variables and their associated articulator model degrees of freedom. for example, tongue body articulators (cl, ca) are shared by tongue body tract variables (tbcl, tbcd) and tongue tip tract variables (ttcl, ttcd). the jaw articulator (ja) is shared by lip tract variables (pro, la), tongue body tract variables (tbcl, tbcd) and tongue tip tract variables (ttcl, ttcd). see section 3.1 for more detailed geometric description of the articulators."
the rest of the paper is organized as follows: in section ii we discuss the mathematical approaches of the linear mixing model (lmm) and the l 1/2 -nmf algorithm. in section iii we introduce the proposed fpga implementation of the l 1/2 -nmf algorithm. in section v we discuss the implementation and simulation results. finally we conclude our paper in section vi.
"in this section, we discuss the computational complexity of the l 1/2 -nmf algorithm relative to some other algorithms present in the literature. as we mentioned in section i, the l 1/2 -nmf algorithm outperforms nfindr, ppi and vca algorithms [cit] . however, the l 1/2 -nmf algorithm has higher complexity than nfindr, ppi and vca algorithms. this is due to the fact that the l 1/2 -nmf algorithm computes both the end-members and the abundances matrices while nfindr, ppi and vca algorithms compute only the end-members matrix. in general, statistical based approaches have higher complexity than geometrical based approaches [cit] . table 1 shows the computational complexity of the nfindr,ppi, vca and l 1/2 -nmf algorithms."
"in this paper, we presented the first fpga hardware implementation of the l 1/2 -nmf algorithm. although l 1/2 -nmf algorithm outperforms many implemented algorithms such as nfindr, ppi and vca, it has not been yet implemented in the literature. the proposed design is tested using both synthetic and real hyperspectral data sets. in the proposed design, the mathematical operations are broken down to element-byelement operations. matrix multiplication is implemented as a multiplier and an accumulator. each stage of the design has its own control blocks. two types of control blocks are used in the design. the first type is used to generate read enable, write enable, write addresses, multiplier, adders and subtracter enables. the second control block type is focused on generating read addresses synchronized with blocks outputs. the design is implemented using mentor graphics fpgadv 8.1 and synthesized using altera quartus ii 15.0 web edition for the synthetic data set and altera quartus prime 15.1 standard pro edition for the real data set. the experimental results were performed on altera cyclone v 5cgxfc9e7f35c8 fpga for the synthetic data set and altera arria 10 10as048e1f29e1sg fpga for the real data set. the hardware results are compared to the matlab simulation results using the same data set and ground truth signatures via sad calculation. the experimental results show the proposed hardware design successfully unmixes the data set with sad results comparable to the simulation results. the maximum matlab sad value was 0.1239 while the maximum fpga sad value was 0.2663 for the synthetic data set. on the other hand, the maximum matlab sad value was 0.3327 while the maximum fpga sad value was 0.3308 for the real data set. the proposed design of the l 1/2 -nmf algorithm works with a maximum speed of 52.6 mhz for the synthetic data set and 104.32 for the real data set. when compared to the matlab simulation, the fpga hardware design has higher speed which resulted in speed up factors of 3.9 and 1.14 for the synthetic and real data sets respectively. in the future, we will exploit pipelined and parallel implementation of matrix multiplication to decrease the run time of the implemented algorithm as well as to better utilize the available resources."
"as shown in figure 5, we start the calculation of the cost function by implementing the multiplication of the augmented matrix a f by the abundances matrix s. both matrices are initialized and implemented as ram blocks. two control blocks are used to control the read,write processes, the addresses generation and the multiplication process. control block 1 controls the writing addresses generation and the read and write enables. control block 2 is used to generate the required reading addresses, the multiplication enable and the accumulation enable. the multiplication process here is a matrix multiplication process which is divided into a multiplier and an accumulator in our proposed design. we perform the element-by-element multiplication and pass the result to the accumulator until the vector-by-vector multiplication is completed. figure 6 shows the next stage of calculating the cost function. the output of stage 1 is subtracted from the corresponding value in the augmented matrix x f . the resulting subtraction value is then squared and passed to an accumulator. this process continues until all values are covered to calculate the first term of the cost function equation x f −a f s 2 2 . in addition to the previously mentioned modules, we have two control blocks. the first block controls and synchronizes the subtraction, squaring, and accumulation processes with each output of the first stage. it also has the read enable sent to the off-chip memory. the second block generates the required addresses to read from the off-chip memory."
section 4.1 discusses the configuration of the agent-based system. section 4.2 real-world examples from two belgian hospitals are described that are used in the experiments. we also describe two different scenarios and the measures used to verify the fairness of a roster.
"a second pair of computational simulations was designed to examine ap predictions that articulatory synergy in different cv combinations would model the babbling data. for this, we used the task-dynamic application model (tada; [cit] ) . tada is a computational speech production model in which ap is mathematically implemented. an utterance is represented as an ensemble of constricting actions, or gestures, of five distinct constrictors (lips, tongue tip, tongue body, velum, and glottis) in the form of a gestural score. each gesture is associated with a relevant set of model articulators (see table 3 ), which cooperate to achieve the gestural target. for example, lip closing for /p/ is defined as a closure target for lip aperture and engages the following articulator degrees of freedom: uh (upper lip vertical displacement (''height'')), lh (lower lip vertical displacement), and ja (jaw angle) articulators. the gestural target of closing lips will always be achieved, but the articulators' cooperative movements will be determined in contextually distinct ways. in the model, a dynamical control regime defined in the task-(constriction-) coordinate space, whose parameters are time-invariant (throughout the interval during which a gesture is active), is transformed to time-varying, posturally-dependent dynamical parameters in the model articulator coordinate space. the transformation is accomplished through a weighted pseudo-inverse of the jacobian matrix that relates change in articulator state to change in constriction variable state [cit] . the weighting parameters (or articulator weights) modulate how much each articulator will be engaged by that constriction variable, everything else being equal. the task-dynamic procedure computes each tract variable's trajectory and coordinated movement trajectories of its associated articulators. casy, which is incorporated into tada, then estimates corresponding area functions from the computed articulator states at a given time point and generates acoustic signals."
"the ap simulation makes the correct prediction that the diagonals values are greater than one, as does the f/c simulation with the adult vocal tract. for the f/c simulation with the infant vocal tract, only two of the three diagonals are greater than 1 (though the third is very close to 1). this suggests that the infant model may be inadequate in some way, which could also account for the poor performance of the f/ [cit], which also employed an infant vocal tract. knowledge of the articulatory-acoustic relations may not be yet adequate for this kind of simulation."
each value in the dataset is converted in matlab to binary representation. the binary value is of length 64 bits with 15 bits representing the integer part and 48 bits representing the fraction part. the choice of 64 bits representation is to provide more accuracy during calculations especially while truncating the outputs of the multipliers and dividers. inputs and outputs data type is a standard logic vector 64 bits. intermediate signals and ram addresses are of an unsigned type.
"once the good pairs are shared out, the agents use these pairs to generate a new roster by using a greedy heuristic. this is a simple heuristic that makes the new roster by taking the good patterns first and then filling the rest of the roster with the agent's previous best-so-far permutation. the new permutation of assignments is now ready to be optimised by an agent's metaheuristic and fairness objective function."
"mainly, two types of spectral mixture models are used to describe the mixture in a pixel: linear and non linear mixing models [cit] . in the literature, most of the existing approaches that solve the unmixing problem are based on the linear mixing model [cit] . the linear mixing model based approaches are classified into geometrical and statistical approaches [cit] . the geometrical approaches are based on the fact that the pixel observations of a hyperspectral image are confined within volume 8, 2020 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/ figure 1. explanation of spectral unmixing concept [cit] ."
"the launch of earth observation satellites has enhanced the field of remote sensing considerably in the past few years. remotely sensed images cover many applications such as mineral exploration, military surveillance and environmental monitoring [cit] . a common problem that surfaces is the existence of mixed pixels in the captured image. this problem occurs due to the low spatial resolution of the sensor and the captured surface variability when capturing a hyperspectral image [cit] . this problem is handled via spectral unmixing the associate editor coordinating the review of this manuscript and approving it for publication was xian sun . algorithms. the spectral unmixing algorithms aim to decompose a mixed pixel into a collection of material spectra named end-members and each end-member's contribution in the pixel which is named the fractional abundance [cit] . the idea of spectral unmixing is explained more in figure 1 [cit] ."
"we implement our fpga design for the synthetic data set on altera cyclone v 5cgxfc9e7f35c8 fpga. the proposed fpga design of the l 1/2 -nmf algorithm occupies 9127 logic elements, 17133 registers, around 1.26 mb of memory and 192 dsp blocks. the proposed design operates at 52.6 mhz maximum frequency. our tests show that the simulation run time for the algorithm on matlab is 52.11 seconds while the implementation run time on fpga is 13.33 seconds which results in a speedup factor of 3.9. in table 2, we show the run time of each module per iteration since the algorithm can stop optimizing at different iterations according to the stopping conditions. for the real data set, we implement our fpga design on altera arria 10 10as048e1f29e1sgx fpga. the proposed fpga design of the l 1/2 -nmf algorithm for the real data set occupies 124474 logic elements, 258479 registers, around 17 mb of memory and 190 dsp blocks. our tests show that the simulation run time for the algorithm on matlab is 152.89 seconds while the implementation run time on fpga is 135.46 seconds wich results in a speedup factor of 1.14."
"in the current simulations, tada was used to model c and v gestures separately and to compare the output tongue body positions (tongue body center, or ''tbc'') for each, in order to determine the degree of synergy of c-v gesture pairs. if a c and v are synergistic with respect to the tongue body (fig. 2a), they will have final positions that are closer to each other than for less synergistic pairs (fig. 2b) . specifically, the euclidean distance between the output tbc positions can be assessed for any pair of c and v. the tbc output difference between c and v can be used to calculate the amount of synergy that would be exhibited in adult speech as scaled down to an infant's vocal tract size. this does not imply that these adult targets were also the targets of the babbling; rather, they are simply a way of evaluating similarity between the tongue body constriction produced passively during a consonanttype oscillation of the lips, tongue front (tip), or tongue dorsum and the tongue body constrictions for front, central and back vowels. tada is not being used here to produce simulated babbling output that would be perceived as cv syllables. indeed, that is not how the output would be perceived. the temporal control hypothesized for babbling would be oscillatory, not the temporal control of adult gestures. the point of the tada simulation is simply to reveal the consequences of making different consonant-type constrictions (in terms of the articulatory coordination and magnitude, not timing) on the tongue body, and to see how similar these consequences are to particular vowels."
"the ontology currently used by the framework generalises the notions of: assignments, pairs of assignments, constraints and roster or schedule. in the ontology these are called solutionelements, heuristicdata, constraints and solutiondata objects respectively."
"for our simulation of the ap account, we employed the tada parameterization of consonants and vowels in order to determine the relative synergy of pairs of consonant and vowel gestures. for adults we hypothesized that the maximally synergistic pairs should be easiest to consonants at a single labial location and a range of coronal and velar locations were modeled as lip, tongue tip, and tongue body gestures, respectively. vowels were modeled as tongue body gestures, whose constriction location and degree targets were chosen to correspond to the 300 systematically varying locations of tongue body center within the vocal tract (fig. 3) . articulatory trajectories for the formation of bare consonants (that is, movement from a neutral configuration to consonant closure) and bare vowels (movement from a neutral configuration to vowel target) were computed using the task-dynamic model. one possible way to measure the synergy is to compare movement vectors (with direction and distance) between two configurations. indeed, subtracting one vector from another provides a measure of how different two vectors are. however, vector subtraction is problematic in our case because it depends on the starting position of the vocal tract. fixing the starting position (e.g. the position for schwa) could constrain the variation but the simulation result would then be biased because articulatory movements in real speech would not always start from that position. instead, the euclidean distance between the end positions of the tongue for the two positions (c and v) is the most direct measure of articulatory compatibility for these syllables. we thus had our model produce bare consonant and vowel trajectories and used their final configuration for measurement. tongue body center (tbc) position is the major end effector for vowel constrictions, but it necessarily takes on a value for each consonant. how close the tbcs for a given pair of vowel and consonant are when they reach their targets was our measure of how synergistic the two gestures would be if they overlap in time (fig. 2) . this can be related to the notion of tongue-body synergy which iskarous, fowler, and whalen (2010a) used to show that the cv locus equations arise due to the synergy between the c and v."
"for the f/c simulation, we manipulated casy only (not the tada model) because the articulator variables in casy can be explicitly controlled according to the factors specified in the f/c account. on the other hand, we employed tada for the ap simulation because tada (j) is given with respect to the mandibular condyle (f). since the distance of the jaw from the condyle is not variable, the jaw position can be given by just the angle (ja) from a horizontal line passing through the joint. the tongue has a tongue body component and a tongue blade/tip component. the tongue body is represented as a circle of a fixed radius with the movable center (c). the tongue tip (t) and blade are attached onto it. the position of tongue-body circle center (c) is given as the angle (ca) with reference to condyle-tojaw line and the length (cl) of c from the condyle bone (f). the tongue blade is attached to the tongue body circle at 0.55p-ja (b) with respect to the horizontal line passing through c. the position of the tongue tip (t) is defined as b-to-t angle (ta) with respect to a horizontal line passing through b and the length (tl) of t from b. in sum, jaw (j) is represented as ja, tongue body (c) as cl and ca with respect to jaw (j), and tongue tip (t) as tl and ta with respect to tongue body. allows us to model the inter-articulator coordination patterns that might plausibly underlie oscillating lip, tongue tip and tongue body constrictions, and then to evaluate the degree of tongue body synergy between those c-type constrictions and particular vs, as necessary for testing that account."
"two simulation studies have attempted to evaluate the f/c account [cit] using articulatory synthesis. the studies used either maeda's model [cit] or a variant of it. [cit] claimed general support for the f/c account, but their report does not include a detailed exploration of the babbling articulatory space. [cit] modeled babbling by inferring initial articulatory configurations from 7-month-old vocalizations. then, they varied the ''jaw'' parameter to create an upward oscillation from these inferred vocal tract shapes until contact was made somewhere along the vocal tract. the combination of consonant-like sounds at the closure and the vocalization was considered a cv sequence. the majority of their front vowel syllables contained an alveolar consonant, as expected from the babbling data. however, the majority of syllables with central vowels also contained alveolar consonants rather than the expected labial (which accounted for only 22% of the syllables). labials dominated with the back vowels, rather than the expected velars. thus only one of the three vowel places of articulation resulted in the expected pattern."
"the generated dataset consists of articulatory trajectories of 300 vowels and either 11 (labial location plus 5 alveolar plus 5 velar) or 57 (19 weights for three locations) consonants from the constriction location and the weight modulations, respectively. all the utterances (bare consonants and vowels) began from a neutral position of articulators and ended when their constriction targets were achieved, which is when the positions of the tongue body constrictions were measured. the euclidean distance of the constriction positions between c and every v were taken as the index of cv synergy. the smaller the difference they exhibit, the greater the articulator synergy is. the values were normalized so that they range from 0 to 1; then they were subtracted from 1 so that greater synergy would result in a larger number. each consonant type has synergy values for 300 bare vowels. thus, a total of 3300 cv synergy values (11 cs â 300 vs) from the constriction location modulations and 17,100 values (57 cs â 300 vs) from the weight modulation were obtained. we used the same vowel maps (fig. 4 ) used in the f/c simulations (section 3.1) to identify each vowel type. fig. 6 shows the averaged synergy values for each of the nine places of articulation for consonants (x-axis), when combined with the front, central and back vowels (plotted as three functions)."
"tract hypothesized to result from the degree of synergy between a given oscillating organ (corresponding to the consonant) and the tongue body constriction associated with that type of vowel. [cit] ), but not all languages show the same preference in spoken corpora [cit] . this suggests that adults, despite having greater control over separate gestures for consonants and vowels, nonetheless exhibit an influence of the same synergies when it comes to selecting new lexical items. in order to understand this preference, we need to consider how consonants and vowels are combined to form cv syllables. phoneticians have long noted differences in how consonants pattern at the beginning (onset) and end (coda) of syllables. recently, gestural studies of syllable structure [cit] have found evidence that such positional differences (onset vs. coda) in syllable structure can be understood as a difference in mode of coordination between c and v gestures: timed so that their activations begin at the same time (''in-phase'') for cv or timed so that one begins half-way through the other (''anti-phase'') for vc [cit] . simulation and experimental studies [cit] have further shown that the distinct modes can predict such observed patterns as: (1) greater inter-gestural timing variability in coda clusters than in onset clusters; (2) faster reaction time to initiate production of cv rather than vc words; and (3) earlier emergence of cv syllables than vc syllables in infants, but also (somewhat paradoxically) earlier emergence of consonant clusters in coda than in onset. [cit] hypothesized that if cv combinations are produced with synchronous (in-phase) triggering of c and v gestures, then those combinations of c of v that exhibit greater synergy (in the sense defined above) would be preferred over others with less synergy, because all the articulatory motions for both c and v can readily be produced synchronously, at least in adults. this follows from the fact that the articulatory motions for the c and the v in synergistic c-v pairs are largely shared: the tongue body movements required for the v would be produced passively by the movements required for the c. this account predicts synergy-related cv preferences but not vc ones in adults because, according to this hypothesis, the latter are coordinated in anti-phase mode, for which synergy is not relevant. consistent with this view, systematic combination preferences, like those found with cvs, have not been reported for vcs . [cit] did find such preferences. in the account we are pursuing here, these preferences, like the cv ones, result from continuous oscillations of the organ (corresponding to the consonant) and the tongue body constriction associated with that type of vowel, which would be assumed to be in-phase oscillations. the factors that cause individual babbled syllables in other than final position to perceived (transcribed) as vc, as opposed to cv, are unknown, but could include how phonation (or lack of it) reveals particular chunks (phases) of the oscillatory cycle."
the agent-based framework for cooperative search is implemented using the open source fipa compliant development platform jade [cit] . we implemented three different metaheuristic agents in the framework:
"the denominator has two terms. the matrix multiplication a t f a f s, is implemented through two stages. figure 16 shows the first stage implementing the matrix multiplication a t f a f . the end-members are saved in two ram blocks with dimensions l by k and k by l. two control blocks are used for required addresses generation and control signals generation. the matrix multiplication is broken down to a multiplier and an accumulator as in the previous modules."
"simulations of the f/c account were conducted for both the adult and 7-month-old infant vocal tracts. we used a default vocal tract configuration of casy for the adult simulation. for the infant simulation, we adjusted the vocal tract shape to a 7 [cit] . for both simulations, we generated random initial vocal tract configurations by varying relevant articulator parameters and created mandibular raising and lowering by controlling the jaw parameter."
"3.1. simulation 1: the f/c account 3.1.1. method casy was used to model syllables articulated solely by movement of the mandible. in casy, the positions of all the articulators (jaw, tongue body, tongue tip, lips) are represented as polar coordinates (distance and angle) with respect to reference sites or other articulators (see fig. 1 )."
"from the perspective of ap, the child is thought to inherit some control of the lips, tongue tip, and tongue dorsum constriction devices, as well as the jaw from ancient mammalian capabilities [cit] . each of these systems is considered an ''organ'' capable of being controlled to a certain extent. they suggest that this proposal accounts for the early emergence of between-organ distinctions in the child's production of early words ([b] vs. [d], for example) compared to the late emergence of within-organ distinctions ([b] vs."
"no the fairness objective functions guiding the search process towards fair rosters and the metrics measuring fairness are separated. this is due to the reason that the objective functions take into account different components, particularly, contractual constraints and coverage constraints during the search process, while the fairness metrics ignore the coverage constraints to better assess how fair the full roster is from the point of view of nurses. in this study, we use the jains fairness index [cit], denoted by jains, to measure the fairness of a roster. equation 8 shows how this measure is calculated for a given solution with n, the set of nurses."
"the second term of the denominator is implemented through two stages. in the first stage, we calculate the element-wise inverse square root of the abundances matrix s. before calculating the inverse square root, the input value is tested. if the value is zero then it is replaced by a very small value. the inverse square root module is implemented in 3 blocks. altera floating point inverse square root ip is used along with two converters from fixed point to floating point and from floating point to fixed point. the inverse square root module needs 49 clocks to perform the operation as shown in figure 18 ."
"to ensure that both the vowel and consonant datasets were sufficiently random such that simulation results are not attributed to articulatory presets from a particular language, randomness was provided in two ways. in the first simulation, the gestures were modulated by the constriction location targets so that they were not those of a specific language. in the second simulation, a single location for place of articulation for each of the three consonant places was taken as the starting point but the articulator weights varied randomly. as described in table 3, a gesture is associated with a set of articulators that jointly contribute to achieving the gesture's target. for example, tongue tip gestures reach their targets by the cooperative engagement of the associated articulators, jaw, tongue body, and tongue tip. such articulators' contributions can be asymmetrically implemented by assigning different weights to them such that if an articulator is ''lighter'' than the others, it is more influential to the gestural realization. heavier articulators are harder to move, and therefore influence the vocal tract shape less than lighter ones. since the articulator weights can differ from speaker to speaker and even from language to language, and since infants cannot be presumed to have any specific pattern of weights, it was necessary to vary the weights, as these would be expected to influence the measured c-v synergy."
"the results for all three classifications show that all the diagonals show coronal-front, labial-central, and velar-back preferences are very similar to those in transcribed data [cit], 1995 [cit], 2002 [cit] ) . unlike the f/c account, the ap account is also similar to recorded data in ratios for the off-diagonals. for example, the large labial-front ratios obtained with f/c simulation are not obtained here. a detailed analysis with quantitative comparisons to observed ratios will be conducted in section 4."
"to test how well the four models accounted for the pattern of reported ratios from babblers across the nine cells (as opposed the absolute error of prediction shown above), we performed a correlation between the ratios obtained by each of the four models separately for acoustic, articulatory and perceptual classifications (tables 4, 5, 7 and 8), with the ratios reported for macneilage and davis's six babblers (table 1) ."
field-programmable gate arrays (fpgas) are the most common and most suitable resource for on-board processing of hyperspectral unmixing algorithms [cit] . fpgas have smaller size and weight while having also lower power consumption compared to other platforms like graphical processing units (gpus) and multicore processors [cit] . fpgas are also suitable for being used in satellite payload as we currently have fpgas with increased resistance to space ionizing radiations [cit] . fpgas also provide the ability of changing their functionality through reconfiguration [cit] .
"based on the results of model comparison, model merge synthesizes a combined model which reconciles the identified differences. this is not always possible due to conflicts between model changes carried out by different collaborators. a merged model is called syntactically correct if it corresponds to its metamodel, and consistent when additional constraints of the domain are satisfied."
"generic merge operations are change-driven transformations [cit], which consume or produce change models as additional input or output. the precondition selects an applicable change c from the deltas ∆l ∪ ∆r and may require the existence of certain model elements in the origin model o. the action part of a generic merge operation (1) modifies the original model o to apply a change, (2) moves the change c from the difference set ∆l ∪ ∆r into a completed set comp to prevent the application of the change multiple times. thus such change-driven rules transform state-based merging into operation-based merging [cit] ."
"the set of variables used for the present study is described in table s1 . the original bsm sample consisted of 105 cut marks made with retouched flakes, 246 cut marks made with simple flakes, 224 trampling marks and 58 tooth marks (scores) made by crocodiles. the original experimental bsm samples are described in refs 12, 24 . in order to provide the modelling with large training and testing/validation sets, the sample was bootstrapped 10,000 times, yielding a sample that is substantially bigger than bsm samples that one may encounter in archaeofaunal assemblages. a total of 70% of this sample was used for the training models. testing/validation was carried out on the remaining 30% of the sample. this is a standard procedure in predictive models in order to deal with the bias/ variance tradeoff. in the present work, the sample was initially bootstrapped with a function from the \"caret\" r library that considers bootstrapping the sample in proportion to the variable representation to each of the factors of the outcome variable. categorical variables were transformed into numerical (dummy) variables. after enlarging the sample, data were pre-processed. to minimize variance biases, data were centred and scaled. the ml algorithms used did not require data transformation to deal with normality, skewness or collinearity."
"an alternative taphonomic approach, developed in the twenty-first century, is systemic (as opposed to individualistic) and emphasizes that association of registered taphonomic entities, via assemblage formation, entails emergent properties beyond the mere addition of the properties of single taphonomic objects. taphonomic attributes are thus not just the ones pertaining to each single taphonomic entity, but also those emerging from associations of entities. this systemic approach has strong parallels with evolutionary biology 16 . taphonomic entities are considered dynamic entities that occur in specific taphotopes, organized in the form of taphons (i.e., taphonomic groups) and they even occasionally display the property of being reproductive by creating new taphonomic entities [cit] . regarding bsm, emergent properties can only be addressed through multivariate approaches and this entails using not only all variables simultaneously, but also complex statistics. each bsm contains its own structural information, in addition to that resulting from its association with other bsm on the same bone specimen as well as other features of the supporting bone specimen. this is what was defined as a configurational (i.e., relational) approach 19 . a configurational approach requires intrinsic (i.e. pertaining to the morphological and internal characteristics of a mark) and extrinsic (i.e., association of a specific mark with other marks or features on the same bone specimen) variables."
"a possible execution of the dse merge is depicted in fig. 5 which displays the completed changes for two solutions. in each iteration, one change is processed. where the fan2activator output is not referenced by any control unit. thus our domain-specific (composite) operation (unreferencedpart) has an activation that is executed on the model. after this iteration, there are no more activations and all goals are satisfied, so solution #1 is found. -itr. 8: after the solution, the strategy backtracks until it finds an activation for a must operation that should lead the model into a partially traversed state and forks the trajectory. only the setattribute operation related to coolingfan2 can be executed. after the execution, deleteobject of coolingfan2 could have activation, but it is disabled by the generic constraint. -itr. 9-11: the same activations are available as for the 4th iteration except the domain-specific operation. the algorithm randomly executes these operations and finds solution #2."
"-setattribute(ac,o): the precondition prescribes that an attribute change ac is available in change set ∆l ∪ ∆r and its object o exists in the current model. its action sets (i) attribute ac.attribute of object o to the given value ac.value, and (ii) moves the change ac to the completed set comp."
"in this paper, we propose a novel automated search-based model merge technique [cit] which builds on off-the-shelf tools for the model comparison step, but uses guided rule-based design space exploration (dse) [cit] for merging models. in general, rule-based dse aims to search and identify various design candidates to fulfill certain structural and numeric constraints. the exploration starts from an initial model and systematically traverses paths by applying operators. in our context, the results of model comparison will be the initial model, while target design candidates will represent the conflict-free merged models. while many existing model merge approaches detect conflicts statically in a preprocessing phase, our dse technique carries out conflict detection dynamically during exploration time as conflicting rule activations and constraint violations. then multiple consistent resolutions of conflicts are presented to the domain experts. our technique allows to incorporate domain-specific knowledge into the merge process by additional constraints, goals and operations to provide better solutions. finally, we propose to adapt a generic scalability benchmark for assessing model merge performance for large models and large change sets, which is also an innovative aspect of the paper."
"1) the initial model contains the original model o and two difference models (∆l and ∆r) 2) the main goal is that there are no executable changes left in ∆l and ∆r along a specific exploration path. 3) the operations are defined by change driven transformation rules to process generic change objects (create, delete, set, add, remove) of the difference models, and potentially composite (domain-specific) operators; 4) constraints may identify inconsistencies and conflicts to eliminate certain trajectories; 5) as main exploration strategy, any changes annotated as must are tried to be merged before resolving may changes."
"-if a change c 1 with must priority is in conflict with another change c 2 of may priority, then the merge will always select the former (c 1 ). -if two conflicting changes c 1 and c 2 are both annotated with may than the merge will randomly select one. -however, if two changes c 1 and c 2 of must priority are in conflict, then the merge process will enumerate both of them separately (in different solutions)."
"there are two traditions in taphonomy. one is anchored in the twentieth century and its focus is on registered taphonomic entities (i.e., objects) as individual entities, and on assemblages as concentrations of entities 16 . this additive concept of taphocenoses and ichnoenoses is individualistic. regarding bsm, such an approach places special emphasis on quantification of taphonomic attributes of each mark separately. methodologically, this approach to bsm uses taphonomic variables independently."
"the precondition states that a delete change dc is available in the current change set ∆l ∪ ∆r and its referred object o exists in the current state of the model where o is a leaf in the containment hierarchy. the action part (i) deletes the object o from current state, (ii) puts it into cemetary and (iii) moves the change dc to the completed set comp."
"domain-specific goals and operations. our example introduced in sec. 2 requires to extend model merge with domain-specific knowledge to guarantee the consistency of solutions. in the wind turbine control system (wtcs) domain, it is mandatory that all systeminput and systemparam instances should be referenced by at least one control unit and each systemoutput has to be referenced by a unique control unit. model merge needs to respect such domain specific knowledge, which can be captured by additional goals specified as constraints and depicted in a graphical representation in fig. 3c ."
"while model comparison is computationally more challenging, resolving conflicting model changes is still a cumbersome task in practice, which is frequently performed manually by the engineers. emf compare and diff/merge enable automated conflict resolution in a programmatic way -but writing code for an automated merge is hardly a task for a domain expert. furthermore, domainspecific conflict resolution strategies are rarely taken into consideration in industrial frameworks, hence the well-formedness of merge results is questionable."
"resolved conflicts. in iteration 3 and 8, two conflicting operations marked with must are executed which forks the exploration into two separate solutions to resolve the conflicts. at iterations of 4 and 9, two operations with may mark are in conflict. in each trajectory, only one of them is selected. similar happens in iteration 5 and 10, but this time the same operation is selected in each branch. solution. there are two solutions in the output of the merge process. we discuss solution #1 in details where the merged model is depicted in fig. 6 . it also displays in dashed line the deleted objects stored in cemetery, namely, coolingtemplimit2, coolingfan2 and fan2activator. there are four non-executed changes as shown in the bottom left corner of fig. 6 ."
"input. our model merge approach takes three models as input: the original model o and the difference models between local and original models ∆l as well as the remote and original models ∆r. these together constitute the initial model for dse. the calculation of the difference models ∆l and ∆r is carried out by an external comparison tool such as emf compare or diff/merge. furthermore, in order to derive efficient state encoding for the exploration process, we assume that each element in the original model has some unique identifier."
"by default, domain-specific composite operations only manipulate the model o without consuming the deltas. therefore, they need to be complemented with generic change-driven rules which identify the model-level changes carried out by them and record them as difference models in the completed set. in most cases, domain experts are responsible for capturing complex (domain-specific) operations only at the preparation of the merge tool for the specific domain. collaborating engineers only use them as part of the merge process."
"unfortunately, the direct use of vcs in mde is hindered by numerous factors implied by the differences between graph-based documents (e.g. models) and textual documents (e.g. source code). a major challenge is related to model comparison, which is also computationally more expensive over graphs, and it gave birth to advanced industrial strength frameworks like emf compare [cit] or diff/merge [cit] built into model-level version control systems (like in papyrus uml or amor [cit] ). in order to achieve scalability for large models, these frameworks frequently assume that unique identifiers are available for model elements. that assumption results in more efficient model comparison algorithms."
"based upon these components, we summarize how synthetic models are generated that contain conflicts serving as input for model comparison and model merge: (1) first, we generate a well-formed model. (2) next, we inject several faults into the generated model. the result of this phase acts as original (o) model. (3) then, local and remote changes are simulated by repairing these violations either in the local model (l) or remote model (r) or in both of them with different random seeds. in the latter case, the framework repairs the same problems in both cases by using different values, which leads to a conflict between two models. (4) we calculate the differences between the two with an (5) finally, these two model have to be merged with may annotations for changes using our merge tool. we evaluate our dse-based automated merge approach to assess its scalability using our benchmark where we investigate the scalability of the approach by measuring execution time for model comparison (carried out by emf compare) and model merge with respect to (i) the size of models, (ii) the size of change set, and (iii) the number of changes in conflict. for the evaluation, we generated models where the number of model elements is from 10, 000 to 350, 000, the number of faults injected into the models (i.e. size of the change set) is from 10 [cit] while the number of conflicts are set to 0%, 50% and 100% of the total number of changes. measurement results are summarized in table 2 taking the average of 5 separate runs."
"a domain-specific operation called unreferencedpart can be defined to eliminate unreferenced systeminput, systemoutput and systemparam instances (see fig. 3d ). here the precondition selects the unreferenced object o while the action part (i) initiates a new delete change independently from the current change set and (ii) executes the action part of the generic delete operation."
"the current paper presented an automated technique for three-way model merge exploiting design space exploration in the background. the original model and two difference models (original model↔remote version, and original model↔local version) calculated with existing model comparison tools (e.g. emf compare or diff/merge) serve as an input of our technique. our technique automatically derives consistent and semantically correct merged models in all possible ways and also highlights the remaining (unresolved thus conflicting) model differences. our approach incorporates the use of change-driven model transformations [cit] to capture and execute merge operations, and relies on an incremental reactive model transformation engine [cit] to detect and resolve merge conflicts. we proposed scalability benchmark for scalability aspect of merge components that demonstrates that dse-based model merge can be executed for models around 350,000 elements and conflicting change sets with 1000 elements."
"goals. in generic, we aim to apply as many changes in ∆l and ∆r as possible to derive the merged model m . when extending a trajectory by any of the remaining changes in ∆l or ∆r would cause a conflict with some already applied change, a solution state of the dse is reached. technically, it is detected by the termination of the rule system, i.e. no operations are activated. additionally, domain experts can provide domain-specific goals that act as heuristics for the exploration and provide consistent solutions."
"we use a simplified difference model derived from the emf compare tool [cit] to store the changes in emf models. this allows us to accept different types of comparison model (e.g. emf compare or diff/merge [cit] ) as an input of model merge. it contains the following default change types: (1) create or delete an object; (2) set, add or remove a value or an object to/from an attribute or a reference, respectively. furthermore, we annotate the priority of changes as may or must which will be decided by users. changes with must priority are mandatory to be involved in the solutions while the others with may priority can be omitted."
"we propose to exploit guided rule-based design space exploration (dse) [cit] for automated model merge with an architecture depicted in fig. 2 . rule-based dse aims at finding optimal solutions from the several design candidates which satisfy several structural and numeric constraints, and they are reachable from an initial model along a trajectory by applying a sequence of exploration rules."
"change-driven rules for generic operations. we defined the following generic operations in the merge process for creating/deleting object, setting/adding/removing attribute and setting/adding/removing reference. for space considerations, we only discuss operations for setting an attribute (setattribute) and deleting an object (deleteobject) in details (depicted in fig. 3 )."
the rest of the paper is structured as follows: a motivating case study of modeling wind turbine control systems is presented in sec. 2 together with the basics of model comparison and merge. a high-level overview of our approach is provided in sec. 3. a detailed explanation of executing a merge process is discussed in sec. 4. the case study will also serve as an initial assessment of the usefulness of a domain-specific merge technique while scalability evaluation will be carried out by adapting the train benchmark [cit] in sec. 5. related work is summarized in sec. 6 while sec. 7 concludes our paper.
"each solution is derived along a trajectory from the initial state to a solution state by applying generic and domain-specific operations. along this trajectory, we transform the original model o into the merged model m, and the change models ∆l and ∆r are gradually reduced to ∆l and ∆r . in each exploration step, conflicts are detected and resolved by incrementally tracking the matches (activations) of operations and constraints. finally, a solution state is identified if all goals are satisfied without violating a constraint along the trajectory."
"obviously, the first type of constraint could also be detected by using similar constraints as for the second type. however, lost activations reduce the number of states to be traversed, thus they are preferred. furthermore, note that when two operations are applicable in both order with a confluent result, the state encoding of dse identifies that the same model is reached as a state."
"operations. we incorporate two kinds of operations in the exploration based model merge: generic merge operations [cit] and (domain-specific) composite operations [cit] (such as refactorings, or repair rules). each operation is captured by (graph) transformation rules [cit], which consist of a precondition described as a graph pattern (using the emf-incquery language [cit] in our case) and an action part which captures model manipulations."
"the domain of our motivating example describes wind turbine control systems (wtcs) developed by ik4-ikerlan where different artefacts and algorithms for controlling a wind turbine are specified and connected to sensors and actuators. models are specified by several collaborators, and consequently modifications could result in merge conflicts."
"in case of many conflicts, the result set can too large to be presented to experts. therefore, in order to reduce the number of solutions retrieved by dse and guide the exploration in case of conflicts, model changes can be prioritized by the collaborators as may and must (see table 1 ) prior to executing merge."
"here we have shown that multivariate configurational ml methods can successfully classify all experimental bsm, with an accuracy rate of 100%. this method uses a combination of intrinsic and extrinsic properties of each bsm, which shows that only configurational approaches capture the additional and emergent associative properties of marks on any given bone specimen. the important feature of this method is not only its high accuracy in mark identification, but also that it provides a computer-based way to classify bsm, avoiding the subjective classification made directly by the analyst. this does not remove subjectivity completely, but it reduces it significantly 15, 20 . this work reinforces previous studies, which showed high success in bsm classification when using multiple variables 12, 14 . it also shows that independent variables (such as cross-section shape and internal striae), may produce equifinality if considered separately, but may provide higher resolution if considered in conjunction with other variables. for example, v-shaped marks that also display intense flaking and abundant shoulder effects are much more frequent in cut marks made with retouched tools than in other bsm."
"scalable collaborative model-driven engineering (mde) for complex projects with multiple stakeholders and development groups working in a distributed way (both geographically and in time) is a major research challenge [cit] . in traditional software engineering, version control systems (vcs) such as svn or git assist to work with textual documents in off-line collaboration scenarios having long transactions and complex modifications between commits. since multiple collaborators may try to commit changes to the same document, a comparison or difference is calculated prior to local commit, which may cause conflicts between remote changes (already published to the server) and local changes (aimed to be committed now). such conflicts need to be resolved by merging the remote and local changes in a consistent way before a commit succeeds."
"in the paper, we focus on three-way merge, which also uses the common ancestor o of local copy l and remote copy r to derive the merged model m . to determine the changes executed on o, a comparison is conducted between o ↔ l and o ↔ r. the solution of merge m is obtained by applying a combination of changes performed either on l or r to the original model o."
"once all models are completed, model selection takes place. his is usually done combining indicators of error (i.e., rsme or root mean square error) or accuracy. cost values (of bias-variance) were evaluated vis-a-vis"
"as future work, we plan to improve our model merge technique by further search strategies to better exploit the dependencies between rules and constraints and compare it with other search-based merge techniques [cit] . currently, we are conducting an experimental user evaluation to compare the usability of the presented dse merge tool with emf-compare and diff/merge."
"activations of rules and constraints are continuously and efficiently maintained when firing an operation (either generic or composite), thus disabled rules and violated constraints are immediately identified. for that purpose, we rely upon the reactive viatra framework [cit] and incremental model queries. the technicalities of conflict detection will be illustrated in sec. 4 ."
"several approaches address the model merge as depicted in table 3 . to position them against our approach, we use several characteristics proposed in a survey on model versioning [cit], which also guides the structure of this section."
"-calculate merged models automatically as a maximal subset of non-conflicting changes from the local and remote change set. when there is a large number of possible combination of changes where some of them are selected from the local and the others from the remote branch, a merged model may be restricted to solutions compliant with must and may change annotations. -use domain-specific goals and constraints to restrict merged models to consistent ones (to ensure that all inputs and parameters are referenced by at least one control unit and each output is referenced by different control unit). -specify domain-specific composite operations to guide the merge process into a consistent solution (e.g. to remove inputs, parameters and outputs not referenced by any control unit)."
"naive bayes (nb). bayes' rule, as used in the nb algorithm, estimates probabilities of classes on observed predictors (i.e., probabilities of previous outcomes), resulting in dynamic estimates of posterior probabilities of classes. the conditional probability (i.e., the probability of observing specific predictor values in relation to data associated with specific classes) is used to model classification. nb assumes that all predictors are independent. prior probabilities allow the decision of which class any case must be assigned to. if no prior estimates are provided, these are derived from the documented occurrence of classes within the training set and their relation to predictors' properties. predicted classes are created based on the largest class probabilities for each class as derived from the training set. nb uses a nonparametric density modelling process. here, the \"e1071\" qne \"klar\" r libraries were used. the tuning parameter was held constant at a value of 0 and the kappa parameter was used to select the optimal model."
"conflict resolution by exploration strategy. in case of a conflict between two operations, dse will investigate both trajectories as possible resolutions and derive two separate solutions correspondingly. thus a merged model m derived automatically as a solution contains no conflicts by definition."
"conflict detection and resolution is carried out during exploration by incrementally tracking rule activations and special constraints. we illustrate this step in the context of our running example (see fig. 4, which is an extract of iteration fig. 4 ) in the context of object coolingfan2. initially, all changes are located in ∆l or ∆r, cemetery and completed changes are empty. in this state, all constraints are satisfied, but goals are violated which means this state is not a solution. 2. our merge process first selects and executes the deleteobject operation (top branch of fig. 4 ) which removes coolingfan2 from the model, moves coolingfan2 to the cemetery, and the corresponding change is moved from ∆l to the completed set comp. as a side effect, operation setattribute loses its activation in the context of coolingfan2 since its precondition is no longer be satisfied in the new state. this fact is immediately identified by the underlying reactive transformation engine [cit] . in the new state, the exploration incrementally checks that all constraints are satisfied and goals are violated, and then selects another enabled (activated) operation for execution. 3. later, after backtracking to the first state, operation setattribute is scheduled for execution on object coolingfan2 (bottom branch of fig. 4) . as a result, cemetery remains empty, the change is moved to the completed set, all goals are violated, and all constraints are satisfied. as a main conceptual difference, the activation of deleteobject is not disabled on coolingfan2 as the corresponding object still exists, hence its precondition is satisfied. 4. next, the process selects and executes deleteobject operation. as a result, coolingfan2 is moved to the cemetery and the change is moved from ∆r to the completed set comp. we detect this conflict by (incrementally) checking a generic merge constraint: there are two changes in the completed-set comp which modifies the same object. in this case, exploration has to backtrack and finds another executable operation."
"by identifying the predictor combinations that optimally separate classes. it is commonly used in situations where predictor reduction is necessary (such as in lda based on pca scores), but it is more efficient than these two-step data reduction methods. plsda finds latent variables (components) that maximize classification accuracy. therefore, when data reduction is required for classification, plsda is preferred over pca-lda. in this test, the tuning parameter is the number of latent components to be retained in the final model. when the number of predictors is short compared to the number of cases, plsda can execute classification better than lda: predictor importance can be also identified. here, the \"plsr\" function within the \"pls\" r libary was used. model tuning was carried out with the \"caret\" r library. the number of components retained in the final model was ten."
"change annotation. after the comparison, the local collaborator annotates local changes l2, l3 and l4 and remote change r2 as must which prescribes that all such changes have to be present in the merged model unless some of them are in a conflict. in such a case, the merged model should contain as many (non-conflicting) must changes as possible, while some (conflicting) must changes might be omitted from the merged model. all other changes are marked as may to denote that the corresponding change may be included in the merged model."
"-inputs: wind turbine wt1 gets data from a temperature sensor specified by the systeminput identified as temperature. -outputs: wt1 acts on two fans for cooling the wind turbine generator specified by the systemoutputs: fan1activator and fan2activator. remote changes. another expert also remotely modified and already committed the model (before the first expert working on the local version managed to commit the model) to introduce the following remote changes: (r1) the cycle model comparison. table 1 shows the result of model comparison between the different versions of the model calculated by using existing tools (using e.g. emf compare or diff/merge [cit] ). the differences between the local and the original model is denoted with ∆(l, o) (or shortly ∆l), while ∆(r, o) (or ∆r) represents the differences between the remote and the original model."
machine learning techniques are powerful predictive and classification methods 33 . a selection of those identified as the most powerful classificatory methods available 34 were used and compared. these comprised the following (some of them initially described in ref.
the input of a rule-based dse includes (1) the initial model used as the start of the exploration; (2) goals which need to be satisfied by solutions; (3) the set of exploration rules; (4) constraints that need to be respected in each exploration state and (5) further guidance for the exploration process. we applied three-way model merge to a dse problem as follows:
"we introduce a simplified example of a wind turbine (wt1) in fig. 1 . real models are obviously larger, sample models of this paper contain only artifacts related to the cooling of the generator subsystem:"
"a series of eight ml algorithms were used in the bsm sample, using a total of 17 variables (complete set) ( table s1 ). all of the algorithms correctly classify more than 96% of all marks (table 1) . three algorithms are less efficient than the others: naive bayes (nb), partial-least square discriminant analysis (plsda) and mixture discriminant analysis (mda). nb shows the lowest sensitivity of these three. in contrast, neural networks (nn), support vector machines (svm), k-nearest neighbour (knn), random forests (rf) and decision trees using the c5.0 algorithm (dtc5.0) show maximum accuracy in the correct classification of all bsm, with perfect sensitivity (1.0) and specificity (1.0) in all bsm types. this shows that several ml techniques are able to correctly classify and discern bsm and crocodile bite marks from other similarly-shaped bsm at a rate of 100% accuracy (table 1) ."
"during the application of ml algorithms, the models were tuned with self-correcting methods. this is one of the great advantages of ml tests. during model elaboration, several techniques allow estimating the performance of the model. some statistics (e.g., rsme) enabled estimating the performance potential on new data. several and very diverse ml algorithms were compared for efficiency and accuracy. model evaluation took place through resampling techniques that estimate performance by selecting subsamples of the original data and fitting them in multiple submodels. the results of these submodels were aggregated and averaged. several techniques can be used for this subsampling and submodelling: generalized cross-validation, k-fold cross-validation, leave-one-out-cross validation or bootstrapping. here, we selected 10-fold cross-validation, which consists of the original sample being partitioned into 10 similarly-sized sets. a first model is subsequently generated using all subsamples, except the first fold. then the first subset is reintroduced to the training set and the procedure is repeated with the second fold and so on until the tenth one is reached. the estimates of performance of each of the ten processes are summarized and, thus, used to understand the model utility."
"conflict detection. finding the conflicting changes in the merge process is crucial task for a correct resolution. most approaches use an initial [cit] where inconsistency constraints are handled incrementally while conflict detection happens as preprocessing. merge automation. most approaches [cit] are semi-automated as they use a two-phase process: (i) they apply the non-conflicting operations and then (ii) let the user prioritize and select the operation to apply in case of two conflicting changes. this always results in a single solution due to the manual resolution by the user. in comparison, [cit] and dse merge resolve the conflicts automatically in different ways and offer several solutions."
"our approach is fully implemented in a tool developed as part of a european project, which operates on well-known open source components of the eclipse framework, such as emf compare [cit] or diff/merge for [cit] for model comparison and using the viatra dse [cit] as underlying design space exploration framework built on reactive transformations [cit] ."
"in order to compensate for the eye motions inflicted between the frames during image acquisition, a transformation model was calculated for each pair of retinal frames using image registration. the translation vector was found using phase-only correlation [cit] which served as an initial estimate for the transformation model. then, tracking of brightest cones was utilized in order to calculate the rotation parameter and correct for the residual displacement. the procrustes algorithm [cit] was employed to refine the transformation model. the final image was calculated by averaging the frames in order to compensate for the photon noise."
"the cci-health database [cit] contains 37,742 patients with cvd from 737 clinical sites. processing through the pipeline, each patient is finally characterized by 11 raw features including demographics, treatment duration, and co-existing conditions, and 1,757 standardized features in snomed-ct terminology including laboratory tests, diagnosed problems, and medications. for each patient, treatment duration is determined by calculating the elapsed time between diagnosis (indicated by the first prescription of a medication) and the last recorded activity (i.e. procedure, lab, etc.). measurements of lipids and lipoproteins are processed into time series, since these are closely related to cardiovascular conditions and can potentially be used to characterize the severity of cvd. lack of high-density lipoproteins (hdl) is significantly associated with the development of coronary heart disease [cit] . in contrast, low-density lipoprotein increases the risk of heart disease and is considered a \"bad\" cholesterol [cit] . triglyceride is also associated with incidence of heart disease but has a less significant effect [cit] ."
"in this section we present the preliminary results of our mpi implementations of algorithm1 and 2 for jtr. table 1 shows one of the results obtained from an experiment that evaluated the performance of the distributed wordlist algorithm. this experiment involved 7 passwords, and a wordlist file that contained 3 million words listed in decreasing probability of likelihood. the program was run on 32 processors on the whale sharcnet supercomputer 1 [cit] . table 1 shows the time to crack each password for the original john-mpi and our distributed wordlist algorithm. table 1 . 7 password entries in the password file, large wordlist (3 millions words), on 32 processors."
"where (, ) is the set of all possible alignments between two series of length n and m, and any alignment pair ( 1, 2 ) satisfies the warping"
"in many data extraction tasks, the targeted patient cohort contains millions of patient records amounting to terabytes of data. in such cases, table partitions are created to retrieve data by chunks and reduce local storage loads. temporary views are used to reduce server loads."
"kidney is an important organ of human bodyfiltering blood, removing waste, balancing fluid, and controlling the level of electrolytes. chronic kidney disease (ckd) is becoming more prevalent at a rapid speed around the world. ckd can be divided into 5 stages based on estimated glomerular filtration rate (egfr) measurement. early diagnosis of ckd prevents patients from regressing into late-stage ckd which causes serious complications. late-stage ckd can lead to end-stage renal disease (esrd) and cardiovascular disease (cvd), which steeply increase patient pain and economic burden. however, the gradual loss of kidney function is difficult to diagnose due to the absence of direct evidence from clinical trials [cit] . hence frequent and regular measure of serum creatinine-used to calculate egfr-is essential for evaluating changes in renal functions. identifying trends in egfr is more important than one-off readings, as suggested by the renal association, \"a progressive fall in egfr across serial measurements is more concerning than stable readings which don't change over time\" (2019) ."
"however, jtr has some limitations. jtr's primary functional limitation is its restriction to operation on password files with known formats. it is also non-adaptive and cannot operate on file formats or hashing algorithms that aren't explicitly defined in its source code. therefore jtr cannot be used directly on an arbitrary password authentication system. jtr's primary performance limitation is its lack of support for parallelization. it can only make use of a single thread and does not support multi-thread operation natively."
"in this study, we focus on tackling the first three challenges by 1) developing a framework for identifying and extracting key clinical features from structured and unstructured data, 2) developing a concept standardization procedure among the multitude of available clinical terminologies, and 3) implementing unsupervised learning algorithms for characterizing patient treatment outcomes based on longitudinal data. these critical data pre-processing steps allow us to better understand patient characteristics and treatment patterns. we can subsequently build outcome predictive models to identify critical features that contribute to variance in treatment outcomes. best practices can be developed based on these factors and can help hospitals to redesign and implement evidence-based treatment plans to achieve better outcome [cit] ."
"the wordlist mode uses a user-definable text file as the dictionary while single mode generates its own dictionary using data found in the password file, such as the user name and account information. the external mode allows user defined modes of operation to be run using program code created with a subset of the c language."
"the rapid progress in adaptive optics (ao) imaging, in the last decades, changed the entire approach underpinning the investigations of retinal tissues. capable of imaging the retina in-vivo at cellular level, ao systems revealed new insights into retinal structures, function, and the origins of various retinal pathologies. advances in image processing techniques contribute to a better observation of retinal microstructures and therefore more accurate detection of pathological conditions. quantitative analysis of highresolution retinal images assists eye professionals in the diagnosis and follow-up of patients with degenerative retinal diseases. based on the measurements of photoreceptor loss, it is possible to interpret retinal disorders and the associated visual deterioration [cit] . therefore, it is of vital importance to develop automated diagnostic tools capable of detecting photoreceptor cells and thereby assisting the diagnosis of eye pathologies during the early stages of their development."
"using longitudinal laboratory records measured during care delivery, we have also uncovered patient subgroups with different outcomes. we introduced an approach to cluster irregular mts by aggregating distances between univariate time series. this allows us to utilize multiple types of laboratory records for each patient to characterize treatment outcome. among the distance metrics used, gak produced the best clusters."
"(dated: february 25, 2014) unidirectional nonreciprocal transport is at the heart of many fundamental problems and applications in both science and technology. here we study the novel design of wave diode devices by engineering asymmetric shapes of nonlinear materials to realize the function of non-reciprocal wave propagations. we first show analytical results revealing that both nonlinearity and asymmetry are necessary to induce such nonreciprocal (asymmetric) wave propagations. detailed numerical simulations are further performed for a more realistic geometric wave diode model with typical asymmetric shape, where good non-reciprocal wave diode effect is demonstrated. finally, we discuss the scalability of geometric wave diodes. the results open a flexible way for designing wave diodes efficiently simply through shape engineering of nonlinear materials, which may find broad implications in controlling energy, mass and information transports."
the factor 2 is due to the fact that there are two transmitted channels to the left. note we use prime to denote the quantities in the backward direction.
"in order to analytically solve the transmission problem for the first model, we find the traveling wave solution of ψ l ∼ e −iωt for eq. (1) as:"
"passwords on computer systems are commonly stored together in files, along with other data like usernames and account information. to secure the password files their contents are encrypted, usually with a 1-way hashing algorithm. these algorithms produce an effectively unique summary, or message digest, of the original password. when the password hashes are used for authentication at a later time, the user inputted password is hashed and compared with the original hash stored in the password file. since hashing algorithms produce effectively unique message digests, if the input was correct, the hashes will match. jtr takes advantage of this system of storage by using the same algorithms used to create the password files to encrypt a series of guesses, then the resulting hashes are compared with the originals."
"due to various photoelectric noise sources in ao imaging system, high-resolution retinal images undergo a degradation process, which results in low signal-to-noise ratio and poor contrast [cit] . moreover, the visibility of retinal features is altered by inhomogeneous illumination, caused by imperfections in the imaging optics. in order to improve visualization of retinal images and facilitate next steps of the image processing framework, wavelet-based illumination correction was performed on each frame of the retinal datasets [cit] . a wavelet-fourier filter [cit] was used as modification to this model. it was applied to the reconstructed retinal images to eliminate block artefacts, and thereby to preserve the integrity of the retinal features."
"one processor is chosen to be the master which assigns password hashes from the password file to the rest of the processors. each slave processor then runs the entire dictionary against its assigned hash. upon completion, the slave processor then asks the master for a new hash. the primary advantages to this method are its simplicity and consistent load balancing."
"therefore, it is worthwhile to emphasize here that the asymmetry alone in linear (quadric) systems cannot induce nonreciprocal wave propagation. asymmetry must come to play together with nonlinearity to give rise to non-reciprocal wave diodes. in all our studied systems, we have either analytically or numerically checked that the effect of non-reciprocal wave propagation will totally disappear if the nonlinearity is tuned to be zero while keeping all other asymmetric settings. these would explain why the electric and thermal rectifications measured in the asymmetric graphene and its oxide systems are too low [cit] [both are below 0.13 according to our eq. (5)], because in these graphene-related systems electrons and phonons are weakly interacting so that the nonlinearity (high order many-body interaction) is nearly absent."
"a previous mpi implementation of jtr's wordlist mode [cit] similarly divides the workload along the password file, but does so with the addition of inter-process communication; the processors synchronize with a master processor which assigns passwords to each worker processor. our approach divides the file at the start of runtime which bypasses the overhead of inter-process communication, since the passwords are distributed among the processors at the start of runtime and synchronization between processors is not necessary."
"the interplay between some kind of symmetry breaking mechanism and nonlinearity is essential to realizing nonreciprocal wave propagation. ref. [cit] breaks the symmetry by setting spatially varying inhomogenous coefficients to induce asymmetric propagation. here we proposed a novel design of the so called geometric wave diode by tailoring the asymmetric shape of the central functional part of the device. the functional nonlinear part of asymmetric shape can be conveniently fabricated in practice and easily be replaced by other shapes with different or even reversed rectifications. therefore, the most manipulability of the non-reciprocal wave propagation can be solely achieved by geometry engineering of the central functional part."
"the first phase of this research explored existing mpi implementations of jtr. several of these implementations were tested and benchmarked for performance. the three primary modes of operation for jtr were analyzed with respect to parallelization. strategies were also devised for parallelizing each of the three modes, with focus on innovative approaches. as a result, two algorithms were designed to address how to improve the current mpi implementation of jtr."
"these are preliminary results, and although promising, more tests are needed to determine how effective our algorithms scale, for instance, by using 128, 256 or more processors. furthermore, experiments involving larger and more varied entries in the password file are needed to draw conclusive findings."
"john the ripper (jtr) is an open source software package commonly used by system administrators to enforce password policy. jtr is designed to attack (i.e., crack) passwords encrypted in a wide variety of commonly used formats. it is most commonly used to recover passwords in a computer system and is a useful tool for detecting weak passwords. jtr also has limited application in approximating the abilities of would be attackers whose tools may operate in a similar way."
"since the dnls has found widespread implications in optics, cold atoms and spin electronics, the proposed design in this article might find its applications in controlling the light propagation in nonlinear optical wave guides, cold atom transport in atomtronics [cit], or spin wave transport in spin caloritronics [cit] . in the future, it would be interesting to explore how the asymmetric geometry optimizes the rectification, which may have a deep connection to the quantum chaos [cit] with nonlinearity."
"jtr is a serial program that does not possess native support for parallelization. while cracking short or simple passwords is feasible, stronger passwords are very computationally expensive to crack. each crack attempt is a reasonably discrete operation that does not directly benefit from previous or future attempts. this makes jtr a prime candidate for parallelization."
"through the design and implementation of this pipeline, we have tackled some major big data challenges including volume, variety, veracity, and especially value. this results in a highly robust, efficient, and customizable pipeline that can be easily applied to current ehr databases to fulfil their potential in both academic and clinical research."
"prostate cancer is the most frequently diagnosed cancer in 105 countries and the fifth leading cause of cancer death in men [cit] . it is estimated that there will be 174,650 new cases of prostate cancer in the u.s. [cit] with an associated 31,620 deaths [cit] . early prostate cancer detection has been achieved through prostatespecific antigen (psa) test and biopsy of tissue removed during prostatectomy or at autopsy [cit] . through mathematical modelling, [cit] concluded that under the assumption that stage shift implies survival shift-which motivates early detection of cancer, psa screening likely explains half or more of the mortality reduction observed in the u.s. [cit] s. ehr provides long-term tracking of patient psa test results. these longitudinal data can be extracted using the lab component ids or names of the test procedure. the rate of increase in psa level, often represented using psa doubling time or psa velocity, has been widely used in the management of prostate cancer [cit] )."
"to circumvent the difficulty, in this article we report another novel design by seeking the symmetry breaking from pure geometric point of view, i.e, engineering asymmetric shapes of homogenous materials. the advantage of utilizing geometric asymmetry is guaranteed by the easy and convenient fabrication of functional devices merely through shape engineering. different functionality can thus be tunable by simply tailored adjusting of the geometric shape of the target device. therefore, the design of a wave diode device purely induced by geometric effect is very important in the practical point of view and would have found potential applications in the future."
producing guesses for use in password cracking can be accomplished in a variety of ways. jtr uses versions of the brute-force and dictionary methods. the brute-force method is the most thorough and is theoretically infallible although extremely time consuming and computationally expensive. it simply exhaustively tries every possible character combination. jtr's implementation of the brute-force method makes use of character frequency tables to try combinations containing more frequently used characters first.
"electronic health record (ehr) plays an important role in advancing clinical and operational processes. although early clinical medical records first appeared in 1600 [cit] that it was put into regular use [cit] . the launch of the 10year [cit] helped fuel its rapid adoption and medical advance [cit], 80 percent of u.s. hospitals had adopted a basic electronic health record keeping system [cit] . the value of ehr data is increasingly recognized by health care organizations and government. its utilizations significantly changed the patient-clinic interaction process [cit] ."
"the research described in this paper focused on enhancements to the wordlist mode of jtr, which uses the dictionary method for cracking passwords. based on our preliminary findings we have found significant benefits to parallelizing the dictionary cracking method."
"distributed worldlist time to crack abc123 6 sec 6 sec guinness 9 min 43 sec 9 min 43 sec password 6 sec 6 sec erschlangener 7 days 12 hours zhora 9 days 3 hours a second experiment was conducted to evaluate the performance of the original jtr and our distributed password algorithm and to facilitate a comparison between the two. the test was to determine how many passwords could be cracked within a 4 day period using the same supercomputer with 32 processors. a large password file that contained 132 random passwords of variable length up to 14 characters drawn from the full extended ascii table was used. in this experiment it was found that: 1. the original jtr cracked 29 of the passwords (21% of total) 2. algorithm1 (distributed wordlist), cracked 64 passwords (49% of total) 3. algorithm 2 (distributed password) cracked 74 passwords (56% of total) table 2 shows these findings."
"in this paper, we designed a comprehensive information extraction and pre-processing pipeline for epic-based ehr system. this pipeline consists of information extraction, de-identification and encryption, standardization, and time series processing and clustering. we applied this pipeline to three cohorts of patientsthose with prostate cancer, chronic kidney diseases, and cardiovascular diseases, and prepared tabularized data files with standardized terminologies and reduced feature dimensions. these data files can be input into machine learning algorithms for further knowledge discovery."
"we develop an end-to-end \"pipeline\" (software from epic systems coded to process data into a more usable form) for extracting key clinical features from narrative documents. these features are then filtered by negation detection and remaining features are mapped to standardized snomed-ct terminology. figure 1 shows the feature extraction pipeline from clinical text. we implement the content summarization module based on the textrank algorithm [cit] . we apply the cliner concept recognition model [cit] to extract key clinical features including problems, procedures, and tests. an improved negex [cit] algorithm is then used to filter features within a negated context. we then proceed in one of two directions: 1) utilize metamap to map the consolidated features to the snomed-ct terminology system and filter out features that are not mapped. the hierarchical structure of snomed-ct and metamap are utilized to remove general concepts (e.g. \"body structure\", \"clinical finding\", \"biological agent\") that are situated at the top two levels in the snomed-ct concept tree; 2) utilize the terminology mapping system developed in section 3.2 to directly map these concepts to snomed-ct. these standardized concepts can be consolidated into input features that could be directly input into machine learning algorithms for knowledge discovery."
"initial attempts in automatic photoreceptor detection algorithms were made by li and roorda [cit] and xue and choi [cit] . in both methods, the photoreceptors cell location is identified as the regional maxima of the image. a manual input is required for such parameters as the intensity threshold and inter-cell spacing. this can adversely affect the inter-rater reliability of cone density calculations and thus objective judgment on the patient's diagnosis. [cit] implemented an interactive tool that allows physicians to adjust the parameters of the algorithm in order to achieve optimal cone detection. this approach as well as manual cone counting procedures [cit] are generally impractical solutions in regular clinical examination where a large number of images needs to be processed."
"we can analytically solve the transmission problem of the wave propagation by the scattering approach similar as in ref. [cit] . for the incident wave coming from left to right, the solutions of the wave amplitudes in the central four layers [see fig. 1 (a)] can be expressed with the following boundary conditions:"
"the extracted dataset covers 33,303 patients with the icd-9 code starting with \"585\" or icd-10 code starting with \"n18\", both referring to \"chronic kidney disease\". [cit] and is composed of patient-level data (24mb), problem lists (288mb), medications (6.74gb), billing (1.90gb), laboratory orders (8.66gb), and clinical notes (18.55 gb), totalling 36.16 gigabytes. patient ids were successfully encrypted using sha-256 encryption. phi including patient names, addresses, institutions, age, phone numbers, and email addresses were detected and encrypted into dummy tokens. patient egfr laboratory test results were used as indications of disease progression. egfr records were retrieved by the following method: 1) component ids for lab records matching the query string \"%egfr%\" or \"%glomerular filtration rate%\" were retrieved; 2) irrelevant lab components were discarded, leaving 16 unique component ids. we then examined egfr records matching these component ids and found that only records corresponding to two component ids \"12122727\" and \"12122728\" were well-maintained."
"to extract patient data with certain treatment features (i.e. procedures, prescriptions, laboratory measurements), we must first identify all the possible vocabularies that represent the treatment features. these vocabularies are compiled into a list and are used to index the billing / laboratory / medication tables to select the target patient ids. alternatively, regular expressions can be used to represent groups of vocabularies to create more succinct queries."
"using snomed-ct ontology as the mapping standard, we successfully mapped 22,842 out of the 39,570 unique clinical concepts. these 22,842 concepts were mapped to 4,673 unique snomed-ct concepts. table 2 shows the number of unique concepts before mapping, with available mapping, and the number of snomed-ct concepts mapped to. through this process, we significantly reduced the feature dimension, removed data redundancy and inconsistency, and lowered the likelihood of data collinearity."
"several attempts have been made to parallelize jtr's brute-force attack mode [cit], and an attempt to parallelize the wordlist mode has also been made [cit] . furthermore, several script-based attempts to parallelize jtr by running multiple serial jtr processes in parallel have been made but with limited success."
"after applying the dot enhancement filter (fig. 1b), the contrast of photoreceptor cells was further improved by convolving the image with the a laplacian of gaussian (log) operator [cit] . the filter has the shape of a circular center region with positive weights, surrounded by another circular region with negative weights. therefore, applying this kernel over an image containing circular objects increases the contrast between the regions of interest (fig.1c) . after this process, photoreceptor cells can be easily segmented by thresholding the image (fig. 1e) . the cone coordinates are calculated as the maximum intensity of the original image within the region of interest. the photoreceptor cells located at the boundaries of the image are excluded from the counting procedure (fig. 1f) ."
"a wide variety of options can be specified to customize jtr's operation. by default, if jtr is run without additional command line parameters, the single mode will be used first followed by the wordlist mode and finally the incremental mode."
"in this algorithm multiple processors are used to crack one password. in this innovative approach the candidate password is distributed to all participating processors and the wordlist is divided based on probability so that each processor has the same likelihood in cracking the password. the overarching goal of this algorithm is to crack a single password as quickly as possible. figure 1 illustrates the distribution of potential passwords from an original wordlist onto processor 1 to n. the potential benefits of this approach are threefold. one, the most probable passwords are listed at the top of each processor's wordlist thereby increasing the likelihood of finding a match. two, the wordlist is distributed amongst n processors, resulting in gains due to the workload being evenly divided. three, there is no overlapping of efforts amongst participating processors. in previous versions of jtr, the same passwords are repeatedly cracked by different processors resulting in wasted cpu cycles. our algorithm ensures that all processors are utilized efficiently collectively attempting to crack the password."
"to extract patient data with certain disease or symptoms, we first utilize the icd-9 / icd-10 diagnosis codes. a patient id is selected from the problem list table if its corresponding record contains the target diagnosis code(s). in many cases, however, diagnosis codes are not well-maintained, so it is necessary to utilize billing information, laboratory data or narratives in clinical notes for more accurate case detection. this can be done using semantic matching of key terms describing medical conditions. the extracted patient ids are then used to link to the other data tables to extract the relevant information. table 1 lists the types and coverages of information extracted. although most demographics, medications, billing, procedures, and co-existing conditions can be found directly from structured data tables, encounter-level data containing physician visits and referrals, dietary management, and suspected problems must be extracted from the clinical notes table."
"jtr has been extended over the years to work with a wide variety of password file formats. the basic setup, without extensions, supports the following formats:"
"it is challenging to establish an efficient data extraction schema for ehr due to the complexity of data and lack of data standards. a common task in ehr is case detectionidentifying a cohort of patients with a certain condition or symptom. coded data such as international classification of disease (icd) codes are often not sufficient or accurate [cit] . informatics approaches combining structured ehr data with narrative text data achieve better performance [cit] . key clinical items can be extracted from narrative texts with simple methods such as pattern matching using regular expression [cit], full or partial parsing based on morpho-semantems [cit], and syntactic and semantic analysis [cit] . recently, more complex statistical and rule-based machine learning approaches [cit] have been developed to tackle this challenge. biomedical named entity recognition (ner) -the \"task of identifying words and phrases in free text that belong to certain classes of interest\" [cit], allows users to identify key clinical concepts such as physician visits, referrals, dietary management, and suspected problems normally not present in structured data tables."
"the ratio sin k r / sin k l appears in the definition as a renormalization factor because the group velocities of incident waves of the left and right leads might not be the same (if the upper energy branch of the left side is chosen, in our case). the factor 2 in the denominator is the consequence that there are two incident channels from the left due to the ladder structure. one can readily verify the conservation con"
"in order to further increase the quality of high-resolution retinal images, thus allowing for better distinction of photoreceptor cells, noise suppression was performed on the retinal datasets. since the scale of the retinal features is known, a gaussian band-pass filter was chosen as the method to suppress the high frequency noise, above photoreceptor cone stop frequency, and enhance the image contrast, within the start-stop frequency [cit] . based on the scale of the retinal features, corresponding frequencies were calculated and used as a stop and the start-stop frequency of the band-pass filter."
"3) egfr lab records are then retrieved by patient ids and these two component ids. 4) missing, erroneous, and duplicated records were removed, and the remaining records were sorted by date and transformed into time series format for each patient."
"the password is the most common method of authentication in computing. used in both low and high security applications, alone or in combination with other authentication methods, the character password remains essential to modern computer security. while the level of security provided by password based authentication depends on a wide variety of factors involving how the password is handled, stored and used, the strength of the password itself depends only on its length and formation."
"kaiser permanente (kp) uses the clarity module to transform data from epic's operational database into a relational form for reporting. clarity database from the kp's healthconnect ehr system stores patient data in over 7,000 tables with over 60,000 columns and update daily [cit] . the epic clarity database has recently been imported to oracle exadata for performance improvement. structured query language (sql) written in oracle sql developer is the primary programming language used to access the database."
"we demonstrate the use of our ehr information extraction and pre-processing pipeline for three different types of disease cases: prostate cancer, chronic kidney diseases, and cardiovascular diseases."
"tackling the problem of data heterogeneity is essential for conducting predictive analytics using artificial intelligence. many clinical records in the ehr adhere to different terminology systems and can cause problems such as data redundancy and inconsistency, hindering the performance of automated machine learning models. to establish interoperability among various naming systems, standardization of data is necessary. in our previous work, [cit], clinical concepts were standardized by a concept mapping system which links concepts describing diagnosis, laboratory, and medications to the standardized snomed-ct terminologies."
"in order to characterize patient conditions using multiple laboratory measurements in the form of mts, we develop a novel clustering approach for irregular mts based on existing distance metrics for variable-length time series. dtw, soft-dtw, and gak are used to calculate the pairwise distances between variable-length univariate time series. an aggregation function is then applied to the distance between all pairs of corresponding univariate time series composing the mts. this produces a pairwise distance matrix representing the similarity between each pair of patients. clustering based on a pairwise matrix can be done using hierarchical or medoid-based clustering algorithms, because it is difficult to determine the length of the cluster centers when using partitionbased clustering algorithms such as k-means [cit] . in this study, we apply the k-medoids [cit] ) clustering algorithm to the distance matrices."
"the results presented here provide a basis of a much broader research work for characterizing the density and spacing distribution of photoreceptor cells in-vivo. quantitative analysis of the final images obtained with the proposed image processing framework can be used for comparison with data related to pathological retinas, as well as for understanding the effect of age and retinal pathology on cone packing density and other microstructures. therefore, future work will include a thorough examination of the proposed model for different pathological cases. as a result, a refined model can be used to derive statistical correlations between spatial arrangement of cone mosaic and visual function of the eye."
the computational pipeline can be adapted to similar large ehr systems and datasets and for other patient cohorts. these modifications include: 1) redesigning sql queries by modifying diagnosis codes when extracting patient id list to accommodate different target cohorts; 2) modifying sql queries to extract additional data from target disease-specific tables; 3) reidentifying new motifs through expert recommendation and/or manual exploration of freetext data and redesigning new regular expressions for pattern-based feature extraction.
"there are many approaches to cracking encrypted passwords. one possible approach might be to reverse engineer the encryption algorithm and recover the password algorithmically, which unfortunately is both exceedingly difficult and highly impractical. jtr cracks passwords by attempting to produce a text string that matches the original password by a process of guessing."
future research may focus on combining the two approaches of workload distribution. one possible approach is to use both methods and implement an additional evaluation logic which will determine which method to apply given a specific scenario.
"in this paper, an automated framework is presented for processing ao high-resolution retinal images. the proposed image processing framework consists of several stages: illumination compensation, noise suppression, image registration, image enhancement and cone detection. details regarding the formulation of the implemented model and the performed validation test-cases are discussed in section ii."
"password strength is a measure of a password's resistance to guessing. assuming all other factors of a password based authentication system are immune to compromise, a password may still be guessed. therefore ensuring that all passwords in a system are of a reasonable strength to resist most guessing attempts is essential, and thus is the purpose of password policy."
"the dictionary method makes use of a user definable list of possible passwords, a dictionary, as a guide for creating guesses. jtr provides a variety of options for applying the text in the dictionary. in addition to trying the dictionary text as is, salts can be added to pad the text and the dictionary text can also be mangled [cit] ."
"weights are assigned to each laboratory time series depending on their importance in characterizing patient conditions. alternatively, mean, median, or the sum function could be used as the aggregation function. the aggregated distance represents an alignment score over each pair of univariate time series and provides a holistic similarity measure for the pair of mts. in this study, we compare the performance of gak metrics to that of dtw and soft-dtw when used in mts clustering."
"once patient information is extracted, data security and confidentiality must be ensured through de-identification steps. according to health insurance portability and accountability act (hipaa), patients' protected health information (phi) must be de-identified or anonymized for commercial and research interest. phi exists in both structured and unstructured clinical records [cit] . this includes patient names, addresses, phone numbers, etc. manual and rule-based or lexicon-based methods have been used to achieve phi de-identification [cit], but they are extremely time-consuming and can be inaccurate. machine learning approaches have also been developed [cit] . however, due to the complexity of data schemas and the heterogeneity of data structures, it is very challenging to detect phi with high sensitivity."
"while some disease severity can be characterized by a single type of laboratory measurementfor exampleserum cholesterol levels can be used to characterize conditions of patients with hyperlipidemia [cit], others can be better defined by multiple laboratory measurement time series. for instance, systolic blood pressure and diastolic blood pressure should both be considered for patients with hypertension. clustering approaches for such multivariate time series (mts) [cit] are limited. existing pca-based [cit], hidden markov model (hmm)based, partition-based [cit], and model-based approaches (košmelj [cit] have been applied to a variety of fields including chemistry and manufacturing, but have not been utilized in clinical settings. this is likely due to the irregularity of clinical time series. as far as we are concerned, clustering approaches have not been developed for mts with irregular intervals and unequal lengths. we will refer to these mts as \"irregular mts\" throughout this paper."
") [cit] . here, is a positive definite kernel function, and the gaussian kernel is used. distance between each pair of mts is calculated by applying an aggregation function on the gak distance between each pair of corresponding univariate time series. here, we aggregate the distances using the weighted average function. specifically, given two patients p x and p y, each characterized by m laboratory time series 1"
"the results of the aforementioned validation test-cases are presented in the next section. table i summarizes the averaged results for the tested sets of synthetic images, in terms of the packing density, obtained by the proposed cone counting method and the li and roorda algorithm. the density of human photoreceptor mosaic peaks in the fovea and declines rapidly as moving away from the fovea [cit] . therefore, in the images representing the section of retina further away from the fovea (set b), the cone contrast is generally better and thus the accuracy of cone identification is higher. an average accuracy of 99.4% was shown by the proposed method and 97.9% respectively by the li and roorda algorithm. in the images imitating retina closer to the fovea (set a), the photoreceptor cells are densely populated and the image sharpness is significantly poorer. this results in a lower detection rate in both cone counting methods: 98.1% for the proposed method and 89.8% for the li and roorda algorithm. fig. 2 demonstrates the results of two cone counting methods in set a and b. for better visibility, the detected cone locations are overlaid on the image with disks, which is created as the initial step in the process of artificial retinal image simulation [cit] . as it can be noticed from the images and table i, the li and roorda algorithm gives a higher number of false positives and false negatives than the proposed method. this happens due to the use of the dilation operator which tends to merge closely located cones."
"the current state of parallel implementation for jtr using message passing interface (mpi) consists of a basic modification of the incremental mode. this implementation was initially created by ryan lim and later refined by multiple authors, notably john anderson [cit] . it uses mpi's facilities to divide the workload of the incremental mode by dividing the key space between the available computation processors."
"analyzing longitudinal clinical data recorded during care delivery is challenging due to their incompleteness and non-uniformness. identification of subgroups of patients who experience symptoms with greater or lesser severity [cit] or respond to treatment procedures differently may reveal critical risk or treatment factors that impact patient outcome. laboratory and vitals measurements before, during, and after treatment may act as markers of disease severity [cit] and characterize recovery process. uncovering patient clusters also have prognostic significanceby constructing cluster-based clinical event predictive models, one can achieve superior performance when compared to treating all patient episodes as a single group [cit] . however, laboratory and vitals in the form of time series often exhibit different length and frequency due to different syndromes and schedules for different patients. thus, conventional clustering algorithms aiming to identify patient subgroups cannot be applied directly. pre-processing methods such as interpolation [cit] and resampling [cit] can normalize time series data. alternatively, clustering algorithms have been customized for variable-length time series. they utilize a variety of similarity (distance) measures such as dynamic time warping (dtw) [cit], soft-dtw [cit], global alignment kernel (gak) [cit], and time-warp edit distance (twed) [cit] ."
"in conclusion, we have shown the non-reciprocal geometric wave diode by engineering asymmetric shapes of nonlinear materials. for a simple model, analytical results have been obtained indicating that the non-reciprocal wave propagation can be induced by the asymmetric shape in nonlinear materials. numerical simulations have been performed to study a more realistic diode model with typical geometric asymmetry. profound non-reciprocal propagations of wave packets have been verified with tailored design of interplay between nonlinearity and geometric asymmetry. most importantly, the sign and magnitude of the non-reciprocity of the wave propagation can be manipulated by shaping the central functional part into different geometric structures."
"jtr has four modes of operation; incremental, single, wordlist, and external. the incremental mode simply applies the brute-force method while the wordlist and single modes use the dictionary method."
"data-driven healthcare has the potential to revolutionize care delivery while reducing costs. however, for policymakers, practitioners, and researchers to take full advantage, several challenges must be addressed: 1) extraction and coding methods for ehr data must be strategically designed considering issues related to data quantity, quality, interoperability, and patient confidentiality; 2) standardization of clinical terminologies is essential in facilitating interoperability among ehr systems and allows for multi-site comparative effectiveness studies; 3) effective methods for mining longitudinal health data common in ehr are critical for revealing disease progression, treatment patterns, and patient similarities, all of which play important roles in clinical decision support and treatment improvement; 4) advanced machine learning techniques are necessary for early detection and prognosis of disease and identifying critical factors that impact patient outcome; 5) practical intervention strategies must be developed to address healthcare disparities in rural and remote areas with lack of resources and access."
"our second algorithm, using a password file distribution method, has also produced promising preliminary results. this algorithm distributes the workload by dividing the password file. in our implementation, processes are working on different password sets, so workload duplication is avoided."
"our first algorithm, using a distributed wordlist method, significantly improves the efficiency of password cracking when a large number of processors are available to crack one or two passwords. the distributed wordlist algorithm can crack over twice as many passwords than the original jtr given the same duration of time in which to work. for several tests, this algorithm was shown to be 72 times faster in cracking passwords than the original jtr. since this method relies on distributing the wordlist file among processors and wordlist contain thousands of entries, its performance should scale well with the addition of more processors."
"unfortunately this method is inefficient with real-world workloads. each processor must keep a private copy of the entire dictionary which can become very memory expensive, especially with large dictionaries (~0.5 gbyte). since each processor must run the entire dictionary against each hash assigned to it, this approach may also become computationally expensive with very large dictionaries or when the dictionary is significantly larger than the password file, as is commonly the case."
"to unraveling the underlying mechanism of the nonreciprocal geometric wave diode, we first analytically solve the transmission problem for a simple model where the functional nonlinear part consisting of a triangle structure coupled to a linear ladder lattice in the left and a linear 1d chain in the right. the derived transmission coefficients exhibit clear asymmetry depending on the direction of the incident wave, thanks to the nonlinear triangle structure. the strength and sign of the non-reciprocity can also be tuned by the intensity of the incident wave. we then investigate a more realistic system with typical asymmetric geometry. the system is complicated and analytical result is no longer available so that we resort to intensive numerical simulations of the propagation of wave packets. the results clearly demonstrate non-reciprocal wave propagations induced by the asymmetric geometry of nonlinear materials. we further verify that our proposed wave diode devices will not change the frequency of the incident wave packet which would be important in practical applications. finally, we discuss the scalability of the geometric wave diode."
"patient psa laboratory test results were used as indicators of disease severity. psa records were retrieved by the following method: 1) component ids for lab records matching the query string \"%psa%\" were retrieved; 2) psa-irrelevant lab components were discarded, leaving 10 unique component ids corresponding to \"psa-screening\", \"psamonitoring\", \"psa\", \"psa free\", \"psa % free\", \"psa, external result\", \"psa, mhs\", \"psa with reflex fpsa, external result\", \"psa, screening\", and \"psa, cancer monitoring\"; 3) \"psa free\" and \"psa % free\" were removed from the list of candidate components since free psa is reported as a percentage of the total that is not protein bound, i.e., free. the higher the free psa, the lower the likelihood of cancer; 4) psa lab records were then retrieved by patient ids and the filtered component ids; 5) missing, erroneous, and duplicated records were removed, and the remaining records were sorted by date and transformed into time series format for each patient."
"for a scenario where the password list is found to be considerably longer than the number of available processors, the program could use the password distribution technique of algorithm 2. for scenarios where the password list is found to be short, relative to the number of available processors, the second method of workload distribution along the wordlist file implemented in algorithm 1 could be used."
"one of the primary sources of performance improvement over previous implementations is the greater workload distribution. in previous mpi implementations of jtr's wordlist mode, all processors will attempt all entries in the wordlist on the passwords they are assigned. this results in considerable workload duplication in scenarios where the wordlist file is large while the password file is small. the only workaround was to manually abort and resume individual jobs from time to time. our approach eliminates this workload duplication, making the first algorithm ideal for these scenarios."
"future works remains in the search of more robust and systematic methods for evaluating the quality of time series clusters. given the complexity of irregular mts and the difficulty involved in labelling clusters, it is necessary to combine effective visualization techniques with quantitative measures to achieve this task. machine learning analysis can help to quantify the separation performance of the clustering results. beyond the ehr data, there is also an opportunity to combine the ehr data with other types of omics data obtained from outside laboratory tests which are currently not recorded within the ehr systems."
"because ehr data include various types of records for patients, it is extremely difficult to analyze all these data without data standardization. in addition, since these data are recorded by different hospital staff members at various provider sites, data heterogeneity becomes a major issue due to the significant practice variation in style of reporting, use of terminologies, and descriptive content."
"standardization of terminologies not only facilitates the analysis of ehr data but can also increase the efficiency of operations and information sharing, thereby facilitating knowledge transfer and reducing practice variance among health care organizations."
"it has been well established that linear structures are not able to break the reciprocity in reflection-transmission once the time-reversal symmetry is preserved [cit] . therefore, both nonlinearity and some kinds of symmetry breaking mechanism are essential for the real non-reciprocal propagation. recently, an interesting design of wave diode is proposed [cit], based on the nonreciprocal transmission in a one-dimensional (1d) nonlinear layer structure [cit] . the model system is described by the discrete nonlinear schrödinger (dnls) equation, which is a reasonable approximation for the wave evolution in layered photonic or phononic crystals [cit] . the spatially varying coefficients breaks the mirror reflection symmetry and this non-homogeneity generates the asymmetric wave propagation together with nonlinearity. however, the inhomogeneity of coefficients is not necessary for nonreciprocal wave propagation and also it is usually very hard to design the inhomogeneous coefficient in real materials."
"in this research we are interested in the number of passwords that can be cracked in the finite amount of time because this is ultimately what the password-collector ultimately is interested in. we excluded passwords that are impossible to crack in the finite amount of time. in pursuing this goal, two distinct algorithms were designed, implemented and evaluated as enhancements to jtr using the message passing interface. the first algorithm uses multiple processors to crack one password by using an innovative approach to workload distribution. in this algorithm the candidate password is distributed to all participating processors and the wordlist is divided based on probability so that each processor has the same likelihood in cracking the password. the overarching goal of this algorithm is to crack a single password as quickly as possible. the second algorithm is intended to be used by system administrators for systems in which thousands of accounts may be in use. this algorithm divides a potentially large password file equally amongst participating processors. this approach offers efficient load-balancing and fault-tolerant performance. this paper discusses the single processor version of jtr in section 2 and related work in section 3. section 4 presents the design of our two jtr algorithms. section 5 presents the preliminary results followed by a discussion and future work."
"the incremental mode's brute-force attack attempts every possible character combination in search of a hash that matches those listed in the password file. the mpi implementation essentially runs multiple serial incremental mode attacks, but each instance only works with a subset of the overall key space. the primary advantage to this approach is its simplicity and minimal alteration to the original while this approach effectively parallelizes jtr's incremental mode, it has several limitations. the following list the current areas that are unsolved: 1. due to minimal communication between processors, if a processor completes its allocated workload it will sit idle until all processors complete. 2. there is no rebalancing of the workload after the initial division. 3. this implementation also doesn't address issues which may arise from one or more processors going offline before execution completes. 4. finally, this implementation only parallelized the incremental mode."
"for incremental mode parallelization, a method of consistent load balancing and processor failure management has yet to be attempted and should be explored. when a processor completes its allocated task, it should be allocated more work taken from the remaining workloads of the other processors. the possible scenario of a processor going offline during execution should also be addressed since execution of incremental mode attacks are very time consuming and graceful processor failure resolution should be explored. this might be implemented by dividing the workload of the failed processor among the remaining processors."
"the sha-256 cryptographic hash algorithm is used to encrypt patient ids contained in every data record. for unstructured free-text data, we apply the transition-based parsing model implemented in the python spacy package [cit] to detect and de-identify phi in clinical notes. we identify and replace the following types of entities: person, norp, org, and gpe. these entities cover patient names, nationalities, organizations, and addresses. in addition, we include a regular expression-based filter to replace telephone numbers as well."
"in order to enhance objects of all scales and at the same time compensate for the residual noise, the original image is firstly filtered with the second-order derivatives of the gaussian function."
"x … x, 1 y 2 y … y, and non-negative weights w1,w2,…,wm associated with each laboratory time series, the aggregated distance is expressed as"
"here, we describe the entire clustering process using the global alignment kernel (gak) metrics [cit] as an example. gak can be used to quantify the similarity between two time series of varying lengths. it is positive definite, rapidly computed, and operates on the whole spectrum of costs of alignments and thus contains a richer statistics than dtw, which considers only the minimum of the set of costs [cit] . gak distance is equal to the sum of the exponentiated and sign changed similarities of every alignment pairs:"
"the extracted dataset covers 98,806 patients with the icd-9 code 790.93 or icd-10 code 97.20, \"elevated prostate specific antigen (psa)\". [cit] and is composed of patient-level data (70mb), problem lists (384mb), medications (7.3gb), billing (167mb), laboratory orders (10gb), and clinical notes (46.1gb), totalling 64.02 gigabytes. patient ids were successfully encrypted using sha-256 encryption. phi including patient names, addresses, institutions, age, phone numbers, and email addresses were detected and encrypted into dummy tokens. we applied the clinical concept extraction system on a subset of patients treated with radioactive seed implants. an additional 2,194 standardized clinical features were extracted from their clinical notes, including \"chronic pain syndrome\", \"placement of stent\", \"nerve conduction testing\", \"vascular calcification\", \"overweight\", \"obstructive sleep apnea syndrome\", \"neoplasm, metastatic\", and \"lithotripsy\", etc."
"geometric phonon and electron diodes have been studied both theoretically and experimentally [cit] . however, to our best knowledge so far there is no existing exact results to clearly expose the underlying physics, especially for the more general geometric \"wave\" diode, before this work. in particular, the role of nonlinearity (many-body interaction beyond quadric order) to the geometric diode has not been appreciated."
"the transmission coefficient of eq. (3) can thus be derived analytically. for the reversed direction, the transmission coefficient of eq. (4) can also be derived with the same consideration."
"the pursuit of novel devices that are able to manipulate energy, mass and information transmission has stimulated huge interests in widespread physical branches such as phononics [cit], photonics [cit] and biophysics [cit] . as one of the most fundamental and applicable devices, the wave diode can rectify non-reciprocal wave propagation, in analogy to electronic p-n junctions, where the transmission coefficient is significantly direction-dependent. as such, the device conducts in the forward direction but insulates in the backward one. there are many interesting theoretical proposals for thermal diodes [cit], optical diodes [cit], spin seebeck diodes [cit] and acoustic diodes [cit], and many of them have been successfully verified by experiments [cit] ."
"beyond jtr's wordlist mode, future work will include creating an mpi implementation of jtr's incremental mode with a focus on exploring the limitations of current mpi implementations and addressing them. there are no current plans to attempt to parallelize jtr's single crack mode, due to its very short run-time even when run serially. while the single mode can conceivably be parallelized, the benefits of such an endeavour are in doubt."
"in this paper, we adopted two algorithms for comparison: fanmod [cit], which is widely used in motif identification, and comofinder [cit], which is publicized in our previous paper. additional algorithms exist, such as warswap [cit] which is designed to identify co-regulatory network motif, but this algorithm directly adopts the enumeration and classification process from fanmod. for consistency, the duration of experiments is shown in seconds by default."
"to verify the efficiency and accuracy of our method, we adopted three co-regulatory networks from previous research. these co-regulatory networks are listed in table 1 and include two small-scale, published co-regulatory networks: glioblastoma multiform (gbm) and alzheimer disease (ad). the largest co-regulatory network is derived from encode project. table 1. the information of three co-regulatory networks adopted in this study."
"in recent years, biological regulatory networks, including protein-protein interaction networks (ppin) [cit], signal transduction networks (stns) [cit], gene regulatory networks (grn) [cit], and metabolic networks (mn) [cit], have become a hot area of research in computational biology. with the development of high-throughput technologies, the study of micro ribose nucleic acids (mirnas), tfs, genes, and the regulatory relationships between these entities has produced a large amount of data [cit] . specifically, the co-regulatory network that combines mirnas, tfs, and genes has become a popular research focus [cit] . in contrast to a regulatory network that involves only one type of regulator, co-regulatory networks with multiple types of regulators are enriched with intricate biological regulatory relationships."
"subgraphs with g-tries are queried by a recursive backtracking procedure. algorithm 2 provides the details of the subgraph census process. initially, we follow all g-trie root children and start with an empty partial match (line 1 and 2). we then find all candidate vertices to fill the position of that g-trie node (line 6). in this line, a set of candidate vertexes are generated, and the vertexes that meet the symmetry condition are designated with a node type. when located at a g-trie leaf, we can find a complete match to a subgraph and increment its frequency (line 9). if not, we continue as before, recursively following all possible g-trie paths from that point."
"one approach to study biological regulatory networks is through network motif analysis. network motifs are subgraphs that are statistically more significant within a given network than expected for a random network [cit] . in general, if a subgraph g appears much more frequently in a given network g than in random graphs with similar degrees of distribution to g, the subgraph is considered a network motif. in colored networks involving multiple interacting node types, the topological structure of graph is ignored, but the node type and edge type are taken into consideration. the network motif containing node type and edge type information is called a colored network motif [cit] . in this paper, we mainly focus on the discovery of co-regulatory motifs (colored network motifs in co-regulatory network) in large human co-regulatory networks of tfs and mirnas to reveal their co-regulatory mechanisms. co-regulatory network motifs discussed here are motif patterns that involve at least one tf, one mirna and one target gene."
"network motif identification in a large-scale co-regulatory network is time consuming, as the process must enumerate a large number of co-regulatory subgraphs and determine graph isomorphism. furthermore, the addition of the color attribute indicates that there are more classes of subgraphs and, therefore, the progress of co-regulatory motif identification will be more time-consuming. for this reason, previous exhaustive searching methods only focus their attention on co-regulatory motifs consisting of 3 or 4 nodes [cit] . this limitation prevents further investigation of the regulatory mechanism within the cell, especially for the intricate interplays between multiple types of regulators in gene regulation."
"we discovered larger co-regulatory network motifs by sampling connected subgraphs from the co-regulatory networks. to obtain the candidate motif type, we sampled 100 subgraphs from the original network. the number of motifs that we identified is indicated in table 10 and table 11 . the large network motif examples are illustrated in figure 3 . because of space constraints, we only show 3 types of motifs on each scale. graphs from #1.1 to #4.3 in figure 2 are generated from the gbm network. graphs from #5.1 to #6.3 in figure 3 are generated from the ad network. these motif patterns clearly show regulations between mirnas, tfs and genes. for instance, in motif pattern #4.1, there are three mirnas regulate three genes and one tf together, another mirna regulate one of the three genes. we also proposed a quick sampling method to generate candidate graph patterns, which we applied with the esu sampling algorithm for 10 randomized graphs and chose the 100 graphs with the highest z score . the number of motifs that we identified is indicated in table 12 and table 13 . though the sampling time may be longer than random walk, it significantly increased the number of co-regulated motifs identified by lcnm. there is a phenomenon in which the motif is not always the highest frequency graph pattern. therefore, choosing the graph patterns that have higher probability of being a motif will save much time in motif identification."
"the compression ratio and cpu time of lcnm with ordering of node types to identify 4-node graph patterns in 3 co-regulatory networks graph and the 1000 randomized graphs. compare results are shown in table 2, table 3 and table 4 . lcnm shows the best performance in comparison with fanmod and comofinder."
"motifs were inserted into the g-trie structure one by one. because we want to construct a co-regulatory g-trie with fewer nodes, we give preference to the vertices with small vertex type label when calculating the canonical label. in other words, the algorithm ordered the vertex by mirna, tf, and gene."
"this study aims to increase the speed of identifying network motifs in co-regulatory networks and to identify largerscale network motifs. to verify the efficiency of the algorithm proposed in this paper, we implemented our algorithm in c++. experiments were performed on a general computer, which contains an intel xeon e3-1230 cpu with 4-cores and 8-threads and 8 gb memory."
"most existing motif discovery algorithms enumerate all subgraphs of a given size from a given network g and an ensemble of randomized graphs with the same degree distribution as g, which is feasible in small-scale networks. nevertheless, these algorithms are not applicable to large-scale networks due to their high computational complexity, which limits discovery of larger motifs. as a matter of fact, in our previous study [cit], we mainly searched for co-regulatory network motifs containing only three or four nodes. in addition, in a large co-regulatory network, the subgraph patterns grow exponentially as the subgraph scale increases. however there are only a minority of subgraph patterns that are motifs. moreover, a frequent subgraph is not always a motif. in other words, previous motif identification algorithms usually waste much time on non-motif subgraph patterns."
"to evaluate the efficiency of our algorithm, we compared the running time of lcnn with comofinder and fanmod in three co-regulatory networks. due to the limitation of computational time, previous research studied co-regulatory network motifs that only contain 3 or 4 nodes. in addition, identification of all 3-or 4-node motif types by our algorithm is highly efficient. therefore, our comparison focuses on 3 and 4 node motifs. for a fair comparison, the parallel mode is not turned on for comofinder and lcnm. here, we compare the cpu time of the entire process of network motif identification, including time elapsed in the original"
"network motif identification is a computationally hard problem. the execution time of a sequential algorithm grows exponentially with increased motif size, especially in co-regulatory networks. though computational complexity in subgraph enumeration improves with the use of g-tries, the entire process can be accelerated by implementing the algorithm in parallel. therefore, in this paper, we present a parallel version of our algorithm based on the openmp library [cit] . the procedure-code is described in algorithm 3. we created threads in line 1 and output the number of each thread in line 2. line 10 indicates parallel processing of the for-loop structure by dynamic scheduling. to remove the conflict of each thread and alter the variable frequency, we used a vector to replace the single frequency (line 13). finally, the sum of the frequency vector is calculated in line 6."
"the general framework of the network motif identification algorithm consists of three main steps, that involve several graph theory methods, such as subgraph enumeration, graph isomorphism and random network shuffling algorithm [cit] . the identification of network motifs is a computationally intensive task. even if the networks only contain a few thousand nodes, it may require several days to identify network motifs. specifically, in coregulatory networks, it would require significantly more time due to the increased information of node types and edge types."
"parallel version of our algorithm and compare the efficiency of this algorithm. we identified 4-node network motifs in encode co-regulated networks. the parallel version of the lcnm algorithm is designed to take full advantage of computing resources. considering that a high-performance computer is difficult to obtain for most researchers, we only tested lcnm on a typical computer, containing an intel xeon e3-1230 4-cores and 8-threads and 8 gb memory. the wall clock time of comofinder and lcnm is indicated in table 9 ."
"in this paper, we adopt a g-trie structure [cit] to efficiently identify co-regulatory network motifs. a g-trie is a prefix tree data structure that is able to store a set of graphs; its efficiency benefits from reusing the information of subgraphs with common prefixes. we extend the g-trie structure to identify network motifs with sizes larger than 4 nodes in large co-regulatory networks. specifically, we propose two sampling methods to generate candidate subgraph patterns: random walking and quick sampling. moreover, we design a parallel version to further improve the computational efficiency of the large co-regulatory network motif (lcnm) algorithm. to determine the potential biological significance contained in co-regulatory motifs, we also analyze the cluster characteristic of the identified co-regulatory motifs."
and any pair of vertices u and a vertex set of the subgraph have all the edges that the same vertex set has in the complete graph g.
"although g-trie is an efficient data structure to identify a set of graphs, its efficiency depends on the compression ratio [cit] of the subgraphs stored within it. the compression ratio can be calculated with eq. (3). by ordering the node types, we achieved better performance. the performance of g-trie without ordering of the node types is shown in table 5 and table 6 . the performance of g-trie with ordering of the node types is shown in table 7 and table 8 . the experimental results show that the compression ratio of lcnm is higher and the cpu time is shorter with node type ordering, which suggests that node type ordering improves the efficiency of lcnm."
"the rest of the paper is organized as follows: section 2 presents a co-regulatory network motif identification algorithm based on g-trie structure, section 3 shows the experimental result, and section 4 presents conclusions and future directions."
"in this paper, we proposed a novel algorithm to identify large network motifs based on the g-trie structure. experimental results indicated that our method is efficient for the identification of large co-regulatory motifs. moreover, larger network motifs provide new insights into co-regulatory networks. the advantage of the lcnm algorithm proposed in this paper consists of three components: (i) the identification of large network motifs in a short time period, (ii) lower computational complexity compared with previous methods, and (iii) an efficient parallel version of lcnm. however, generating candidate graph patterns by sampling is not applicable to identify all motif types in a co-regulatory network. furthermore, the calculation time remains excessive in the context of a network with tens of thousands of nodes. future studies will focus on parallel processing of this algorithm via supercomputer."
"for sentinel-5 and carbonsat phases a-b1, a slit homogeniser (sh) device was considered to mitigate this error. in its standard form, it consists in a thick slit made of two highly reflective parallel mirrors separated by spacers (see fig.4 ). the telescope (or spectrometer) must have some astigmatism: in the act direction, the telescope and spectrometer focal planes coincide to keep a perfect spatial image, while in the alt direction, the telescope is focused at the sh entrance and the spectrometer at its exit."
"similar strongly variable shifts have been found in swir-2. for carbonsat, smile values in the order of 10% (of the spectral pixel for one spatial sample) in nir, 5% in swir-2 would create a rms isrf distortion of 0.1%. the situation is dramatically different in swir-1 thanks to the shallow absorption bands and the resulting weak contrast variation. we found that the spectral shift in swir-1 is correctable to a large extent by spectral calibration, leaving no particularly demanding smile constraint."
"the remainder of the paper is organized as follows. section 2 describes the proposed methodology. section 3 describes the experimental results, conducted in this work using both synthetic and real hyperspectral data. section 4 concludes the paper with some remarks and hints at plausible future research lines."
"if w(x,y) remains within the interval [cit] the interpolation may be also interpreted as a weighted superposition. this parametrisation is practical as the spatial and spectral dependencies are well separated. the instrument psfs will only impact the weights w(x,y) in (1) when the signal propagates through the instrument. for the purpose of the analyses required in phases a/b1 of carbonsat, two types of heterogeneous scenes have been considered as shown on fig.1 : (a) uni-dimensional \"contrast scenes\", where the weight varies only along the x (alt) or y (act) direction and takes only two values, 0 or 1 (b) realistic scenes, where the weights have the full 2d variability and can take any value. these scenes aim at representing real cases and the weights are computed from modis reflectance (resolution 500m) or aviris radiance (typ. resolution 5m-20m) data."
"both radiometric and isrf errors have been calculated for one of the two carbonsat instrument concepts at end of phase a, assuming contrast scene #1 in alt (fig.1) . the results are displayed on fig.2 and fig.3 . we see that the radiometric error fluctuates around zero, consistent with a zero spectral average and a description in terms of isrf distortion. the two contributors to the spectrally variable isrf are plotted, as well as their relative weight. at first sight, the radiometric and spectral error descriptions seem to carry contradicting information: at the bottom of the absorption lines, the atmosphere becomes opaque and the weight of isrf hetero is zero, giving no isrf distortion while the radiometric error becomes maximum in this area, due to the presence of strong variations in the measurement. in the continuum where the isrf distortion is the largest, the radiometric error is actually very small as the signal lacks spectral structure. in practice, the peak amplitude of the radiometric error is quite sensitive to the exact shape of the lines in the measured spectrum and the assumed spectral grid, whereas the isrf distortion remains stable and more suitable for performance assessment."
"hyperspectral imaging spectrometers collect hundreds or thousands of bands (at different wavelength channels) for the same area on the surface of the earth [cit] . for instance, the nasa jet propulsion laboratory's airborne visible infrared imaging spectrometer (aviris) covers the wavelength region from 0.4 to 2.5 microns using 224 spectral channels, at nominal spectral resolution of 10 nanometers [cit] . the resulting multidimensional data cube typically comprises several gigabytes per flight."
"spatial heterogeneities may be induced by variations in the atmospheric layers (clouds, aerosol layers, ground altitude) and/or variations of ground albedo. as the foreseen co 2 and ch 4 retrievals are only possible in cloudfree conditions, and atmospheric scattering plays a lesser role in the carbonsat swir bands it is natural to focus on ground albedo variations. a convenient parametrisation that has been traditionally used at esa consists in a linear interpolation between two spectra, corresponding to the same atmospheric state but obtained with dark and bright albedos, and an interpolation weight w(x,y) carrying all spatial dependencies:"
"the most obvious effect of scene heterogeneities comes from the non-uniform slit illumination, and consists in a deformation of the isrf. its impact on measurements was discovered in-flight during the omi mission, and could be corrected thanks to the availability of temporally oversampled data [cit] . the errors resulting from nonuniform illumination of the slit in the alt direction have been studied for many missions, including sentinel-4, sentinel-5 and carbonsat [cit] . besides software correction [cit], hardware mitigation is possible with slit homogeniser devices, and has been considered for sentinel-5 and carbonsat [cit] ."
"if the slit image on the detector fpa is not exactly aligned with the spatial axis, the isrf may be distorted due to act scene heterogeneities. this occurs in presence of spectrometer smile, but also if the detector has been slightly rotated. the error mechanism is described on fig. 6 : the slope of the slit image induces a shift in the isrf barycenter if a spatial sample is partially illuminated [cit] . as it directly impacts the isrf, the error is a spectral one and has a similar radiometric signature as for non-uniform slit illumination in alt (see fig.2 )."
"we acknowledge useful interactions with the contractors involved in the early phases of sentinel-5 (kayserthrede gmbh, airbus defence and space) and carbonsat (airbus defense and space, ohb/thalès), as well as identification of the smile effect and communication to esa by iup bremen [cit] ."
"diffraction model: the input stimulus is a diffraction psf originating from the telescope. mirror images of this diffraction psf are computed to build a complex intensity profile over the complete sh input plane (not just the slit opening). propagation to sh output is done with a diffraction integral. this model is a bit more complex but more accurate, it fulfills energy conservation and also describes the progressive decrease of transmittance when the diffraction psf is moved towards the edge of the sh opening."
"the problem (2) is np-hard [cit] and therefore there is little hope in solving it in a straightforward way. a possible strategy is to use convex relaxation via analysis based regularization, which replaces the ℓ 0 norm with the ℓ 1 norm obtaining"
"alternatively, the error can be evaluated in the spectral domain. we rewrite the measured signal by defining two isrf. the distorted isrf can then be calculated:"
"to simulate this error mechanism with the traditional psf formalism, one has to separate artificially the imaging properties of the spectrometer (diffraction and aberrations) from the geometrical distortion (smile). this is in reality not possible as all these effects come from the same origin: optical wavefront distortion through the spectrometer. nevertheless, one reasonnable possibility to describe spectrometer smile is to use a convolution with the dirac function (x-sy) where s is the local slope. for simplicity we also assume that the spectrometer-pixel psf can be expressed as a product psf 1 (x)psf 2 (y). the measured signal in presence of smile and act heterogeneities is given by an equation similar to (3):"
"unfortunately, even if accurately known, this barycenter shift cannot be corrected by spectral calibration in the nir band, due to its too strong and fast spectral variability. to illustrate why, it is useful to introduce new concepts and clarify what is meant by isrf. if  is a incident monochromatic wavelength at the instrument's input, and  k the centroid of spectral channel k, the most general spectral response function srf(, k ) is mapping input wavelengths  into measured wavelengths  k . this 2-dimensional function is shown, in presence of smile, on the left part of fig.8 . on this figure the impact of smile was purposely amplified by an order of magnitude to make the distortions easily visible. the isrf, interpreted as the instrument response to a monochromatic stimulus, appears as a vertical slice of the srf. its integral is normalised to 1 due to energy conservation. the ismf (instrument spectral measured response), which depicts the spectral origin of the measured photons in one spectral channel, is an horizontal slice of this response. it is worth noting that on-ground calibration performed with a tunable laser actually provides the ismf, not the isrf. in general, if spectral properties have only slow variations, the isrf and ismf are mirror image of each other and the ismf integral is also normalised to 1. over act heterogeneous scenes and in presence of smile, this is no longer the case. we see on fig.8 that the isrfs are shifted vertically due to the impact of smile. this shift is highly variable and must be computed from the unconvolved spectra for each different monochromatic wavelength. the ismf, that describes the spectral origin of each measured point and averages the scene contrast within a certain spectral interval, is unfortunately distorted so that the effect cannot be corrected by simple wavelength re-assignment."
"in this paper we discuss a compressive sensing algorithm called hyperspectral coded aperture (hyca) and its constrained version (c-hyca). hyca framework takes advantage of two main properties of hyperspectral data, namely the high spatial correlation of abundance fractions and the low number of endmembers to explain the observed data. the . former property is exploited by minimizing the total variation (tv) of the reconstructed images of abundance fractions and the latter property is exploited by formulating the reconstruction problem with respect to abundance fractions, which have much lower dimension than the original data. while hyca depends on the tuning of a regularization parameter controlling the relative weight between the tv regularizer and the data term, c-hyca does not depend on any regularization parameter. as demonstrated by our experiments with synthetic and real hyperspectral data, both approaches provide good results in the task of compressing remotely sensed hyperspectral data sets and are strongly related with the concept of spectral unmixing that has been widely used to interpret hyperspectral data."
"to improve the accuracy, the coating properties, with phase and amplitude may be easily inserted in the two above models. in most cases, a 1d model is sufficient as the optical path difference does not vary with the act pupil coordinate (this results from the perfect focusing required in the act to preserve the spatial image). the only difficulty comes for the case of a diffraction model with a circular pupil, where a 2d model is required. more elaborate models may be established to consider the impact of the scrambler pattern (the scrambler spots are mutually coherent, and may interfere if they have similar act coordinates and polarisation), the impact of the spectrometer pupil (it cuts the diffracted light at large angles, e.g. if the input psf is close to the slit edges and partly clipped), and the impact of the spectrometer aberrations (the slit homogeniser strongly modifies pupil illumination in the spectrometer, altering the aberrations and then isrf in a way that depends on sh input illumination)."
"compressibility, or sparsity 1, is a necessary condition for success of compressive sensing. in our approach, and having in mind the characteristics i) and ii), we represent the spectral vectors in a basis of the signal subspace and model the spatial correlation by promoting small local differences on the images of coefficients by minimizing their total variation [cit] . under the linear mixing model [cit], if the spectral signatures of the endmembers are used to represent the spectral vectors, then the representation coefficients are the abundance fractions of the pure materials. in this way, the proposed approach is strongly connected with unmixing. to be more precise, and assuming we use the spectral signatures of the endmembers to represent the spectral vectors, our methodology implements hyperspectral unmixing in addition to hyperspectral compressive sensing."
"although most requirements for the carbonsat phase a are defined over spatially homogeneous scenes, it is known from previous missions and studies that the observation of real, spatially heterogeneous scenes create specific measurement errors. one obvious mechanism is a distortion of the instrument spectral response function (isrf) induced by a non-uniform slit illumination in the along-track (alt) direction. this error has been analysed for several missions (omi, sentinel-4, sentinel-5). the combination of spectrometer smile with across-track (act) scene non-uniformities induces similar errors. in this paper, we report about the analysis efforts carried out during carbonsat preliminary phases to evaluate and mitigate these effects. in a first section, we introduce common concepts and notations for heterogeneous scenes analysis. an exhaustive list of known error mechanisms is presented. in section 2 we discuss the effect of inhomogeneous slit illumination, and describe hardware mitigation with a slit homogeniser. the combination of spectrometer smile and act heterogeneities is studied in section 3."
"k is a linear dispersion coefficient (spatial units per spectral units); it is also needed in front of the dirac function for normalisation. two spectral coordinates are used:  0, wavelength of the monochromatic incident light, and , spectral coordinate of the measurement. to evaluate the radiometric error, this measured radiance can be compared to its error-free radiometric equivalent obtained by replacing the blurred weight with its average across the slit width:"
"this paper describes the role of spatially heterogeneous scenes on the radiometric and spectral performances of carbonsat. the cases of non-uniform slit illumination in alt, and smile combined with act heterogeneities are studied in details with emphasis on the methodology description. the former is mitigated in carbonsat with a slit homogeniser, whose principle and performance are described. the latter is tackled with dedicated requirements on spectrometer smile."
"an exhaustive list of error mechanisms occuring over heterogeneous scenes is presented in table 1. column 5 describes the behavior of each error wrt temporal averaging. while the satellite is flying, spatial heterogeneities create fast temporal fluctuations in the measured signal. as the overlap between consecutive spatial samples is limited, and becomes negligible between samples n and n+2, the associated correlation length is very short. for most errors, the temporal average reduces quickly to zero but in a few case a systematic error component is created. column 6 indicates the possibility to describe an error spectrally. \"spectral\" errors have a spectral average equal to zero : they consist in a spectral re-distribution of the detected photons and can be described with isrf distortion. for \"radiometric\" errors photons are added or subtracted to the useful signal. to evaluate the impact of these mechanisms, the most natural way would be to compute a dot product between a gain vector and the reflectance error spectrum, mapping directly the error on level-2 products. then, part of the mission requirements for carbonsat on rms (precision) and systematic (accuracy) level-2 errors for co 2 and ch 4 may be allocated to each error in table 1. unfortunately, the error-free measured signal and therefore the gain vector depend on the part of the scene that is actually measured (instrument position and footprint). in these conditions, a parametrisation or interpolation of the gain vector between l dark and l bright seems to be the only practical way to assess level-2 errors without a full retrieval algorithm. preliminary attempts to derive such parametrisation indicate that a high sampling between l dark and l bright is required. further efforts are on-going in this direction. for the spectral errors, an alternative possibility is to link the resulting isrf distortion to scientific requirements on isrf shape and centroid position knowledges. such isrf requirements applicable over contrast scenes in alt and act have been used during the phases a-b1 of sentinel-5 and carbonsat, and are investigated in the next sections."
"due to the narrow spectrometer bandwidth, a source point at the sh input must be considered as coherent so fig. 4 . slit homogeniser device (left). the instrument must have some astigmatism: the telescope is focused at sh input in alt (center) while in act the telescope and spectrometer focal planes coincide (right). that the superposition of all reflected beams at the sh exit creates an interference pattern. the slit homogeniser behavior is completely described by a so-called \"transfer function\", a 2d map giving the observed output interference patterns for each position of an input source point. an example of a transfer function is depicted on fig.5 . summing the transfer function along one direction gives either the sh transmittance as a function of input stimulus position, or the output intensity profile for uniform illumination. two possible approaches can be used for calculating transfer functions: geometrical optics model: the input stimulus is assumed to be a point emitting in a cone defined by the f number of the telescope optics. propagation inside the slit homogeniser occurs along straight lines (rays). the length of each optical path is measured and the corresponding phase information is included when the complex intensities are summed up in the sh output plane. this model is simple but has the drawback to slightly violate energy conservation."
"the isrf distortion was calculated over a few realistic scenes derived from aviris data. the obtained figures vary proportionally with the assumed smile amplitude, so a reference local smile value of 10% of the spectral pixel over one spatial sample was used. the max and rms isrf distortions obtained for one of the two carbonsat concepts, over the city of washington (aviris dataset f090706t01p00r13, see fig.1 ) are illustrated on fig.7 . the left plot, showing the isrf distortion vs wavelength, is similar to what was obtained for alt non-uniformities (fig.3), with a maximum distortion occuring in the continuum. the right plot shows that the isrf distortion occurs mostly at the isrf edges, suggesting a strong contribution from a barycenter shift. the barycenter shift is zero at the bottom of the absorption lines due to the opaque atmosphere, leading to no distortion, and is maximum in the continuum."
"in this paper, we develop a new compressive sensing (cs) framework [cit] for hyperspectral images, termed hyperspectral coded aperture (hyca), which exploits two characteristics of the hyperspectral images: i) the hyperspectral vectors belong to a low dimensional subspace, and ii) the imaged scene components present a high spatial correlation. these two characteristics mean that the hyperspectral data cubes are compressible, i.e., they admit a representation in given base or frame in which most of the coefficients are small, and, thus, the data is well approximated with just a small number of large coefficients."
"for carbonsat, the depth optimization was performed in all bands, and the final performance computed including all instrument contributions, in particular scrambler, spectrometer aberrations and detector crosstalk. the final isrf distortion, over contrast scenes #1 and #2, fits in the budget of 1% accuracy for in-flight isrf shape knowledge."
", where e denotes mean value, to simulate contributions from ambient and instrumental noise sources. fig. 1 displays the ground-truth abundance maps used for generating the simulated imagery."
"due to the extremely large volumes of data collected by imaging spectrometers, hyperspectral data compression has received considerable interest in recent years [cit] . these data are usually collected by a satellite or an airbone instrument and sent to a ground station on earth for subsequent processing. usually the bandwidth connection between the satellite/airborne platform and the ground station is reduced, which limits the amount of data that can be transmitted. as a result, there is a clear need for (either lossless or lossy) hyperspectral data compression techniques that can be applied onboard the imaging instrument."
"to size the slit homogeniser, it is convenient to make simplifying assumptions on the instrument. with no polarisation scrambler and a diffraction limited telescope, the contrast scene #1 in alt (fig.1) is transformed with the satellite smear into a linear illumination at sh input, ranging from 0.5*(1-slit/smear_distance) at one edge to 0.5*(1+slit/smear_distance) at the other. in that case the final isrf distortion is given by a simple formula:"
"we propagate this radiance through the instrument: blurring with the smear and telescope optics, cutting by the slit edges, blurring again by the spectrometer and detector psf, and conversion of x into a spectral coordinate:"
"in order to evaluate the performance of the hyca and c-hyca, we use as performance indicator the normalized mean squared error (nmse) of the reconstruction given by"
"as we can see in table 1, by disabling the non-negativity constraint the algorithm is more robust to noise. however, the implementation with the non-negativity constraint provides better results when there is no noise. this is expected, as noise introduces outliers in the model which lead to errors in the reconstruction when we use the non-negativity constraint. if we compare the hyca and c-hyca criteria on table 1, we can see that both versions provide very good results with low reconstruction errors. we can also observe that hyca outperforms c-hyca, which is probably due to the parameter optimization conducted in the case of hyca."
"we consider the policy improvement problem where we want to find a policy with the highest worst-case reward. we give an efficient algorithm to compute an optimal robust policy, assuming that the set p is r-rectangular."
"for a classical mdp, the value vector of the optimal policy is component-wise higher than the value vector of any other policy. this is known as the maximum principle (feinberg and shwartz [cit], section 2). using equation (4.7) and the strong duality property of theorem 4.2, we extend the maximum principle for mdps to a robust maximum principle for robust mdps: the worst-case value vector of the optimal robust policy is component-wise higher than the worst-case value vector of any other policy. we will write v π,w the value vector of the decision-maker when he chooses policy π and the adversary chooses factor matrix w . proposition 6.1 let p be an r-rectangular uncertainty set."
"the maximum deviation from each component p nom sas is τ . for the same reason as for the r-rectangular uncertainty set of the previous section, the total deviation from a given matrix p nom s is √ s · a · τ ."
"we prove in lemma c.1 in appendix c that φ(π, ·) is a component-wise non-decreasing contraction. therefore, we can apply lemma 3.2 to the reformulation (3.17) and we can solve the policy evaluation problem by computing the fixed point β * of φ(π, ·), i.e, by computing β * such that"
"we use (w nom 1, ..., w nom r ) as the nominal factor vectors and we find the coefficients (u 1, ..., u s ) as the blocks of the matrix"
"short hidden message is embodied as an audio signal. the pca performed on the final matrix contains the original data expressed in terms of retained eigen vectors that are orthogonal to each other, thus compressing the original data."
"where \"w\" indicates the weight vector required to minimize correlation between the two dimensions. this sets up the basis for the pca technique which not only provides a good representation of the signal but also reduces the redundancy in the data. the feature similarity is obtained by decorrelating the covariance matrix as (4)"
theorem 5.1 let p be an r-rectangular uncertainty set. algorithm 2 gives an -optimal solution to the policy improvement problem in time polynomial in the input size and log( −1 ).
"since nature might not be adversarial, we compare the performances of π rob,r and π rob,s on the same sample of kernels around the nominal transitions p nom . the robust policy π rob,r is maximizing the worst-case reward over (some) rank r deviations from the nominal transition kernels p nom . therefore, we simulate a random perturbation of rank r from the kernel p nom by uniformly generating a random factor matrix and some random coefficients matrices, such that the maximum deviation on each component of the transition kernel is smaller than τ . more precisely, let"
"therefore the two sides of (4.17) are attained at the same pair (π *, w * ) and we obtain the strong duality result: max"
"neurostream (nst) is a streaming coprocessor designed based on two observations: (i) modern convnets tend to have very small convolution filters, making coefficient reuse less practical (previously discussed in subsection ii-b). (ii) the most demanding operation in convnets is mac [cit] . therefore, unlike conventional simd coprocessors (e.g. arm neon), nst works directly on the shared multi-bank spm without having many internal registers (just one accumulator). this feature along with its dedicated hardware address generators allows it to perform arbitrary computations efficiently and directly on the spm. this removes the register-file bottleneck which is present in simd architectures and allows it to achieve a performance close to 1 mac/cycle. moreover, each nst can be treated as a scalar coprocessor working independently. yet, it is possible to instantiate several nsts inside a cluster to achieve a scalable parallelism without the need for fine-grained synchronization among them. this way, nsts are easier to program compared to simd units, and they offer more flexibility in terms of the size/shape/stride of the computations. in total, 128 instances of nst, clocked at a moderate speed of 1 ghz, sum up to 256 gflops of raw performance in the neurocluster. figure 2 illustrates the block diagram of nst, composed of the main controller, three hardware-loops (hwl), two address generation units (agus), and an fp32 datapath (fpu) compatible with the ieee-754 standard. the maincontroller is responsible for receiving the commands from the processor and issuing them to the datapath. a parametricdepth first-in-first-out (fifo) command-queue is implemented to hide the programming latencies. also, the control interface is memory-mapped, making it possible for the nsts to easily communicate with other processor micro-architectures (e.g. arm). nsts follow a nonblocking data-flow computation paradigm, and information flows in them as tokens. the main controller is, therefore, responsible for issuing enough transactions (2 in each cycle in case of mac) towards the spm and filling up the operand fifos to keep the fpu busy almost every cycle. the hardware-loops are programmable fsms capable of modeling up to three nested-loops in hardware. the agus can be programmed to generate complex strided spm access patterns (see subsection v-a). by having two direct ports to the cluster-interconnect, each nst can fetch two operands (typically one coefficient and one data) in a single-cycle and perform an operation on them."
"in step (2) . this operation is not currently supported by nsts. but since only comparisons with the zero are involved, the integer datapath of risc-v cores is used. pool layer, similarly, propagates back the gradients with a matrix scatter operation [cit], populating a sparse matrix without performing any actual computation. again, this operation is implemented on the risc-v cores in the current version. softmax (class) is calculated similarly to the forward-pass on the risc-v cores. finally, in step (3), the weights are updated either with fixed or adaptive step sizes (α or α i, respectively):"
"this procedure is repeated in an iterative manner for all variations of gd algorithms (e.g. stochastic gd, batch gd) [cit] . a fixed step implementation of this formula is currently supported by the nsts, while adaptive steps need to be calculated by the pes, once per each backward pass. an estimation for the performance of training on smc is presented in subsection vi-a."
"we present the proof in appendix e. in view of this, in the rest of the paper we focus on policies in the set π of stationary markovian polices (possibly randomized)."
"the algorithm for hiding n audio samples by is structured by selecting the prime principal components. the eigen vector corresponding to the highest eigen value is the first principal component. by thresholding the eigen value, the least important eigen vectors can be eliminated and a matrix of retained eigen vectors in the order of significance is formed whose transpose is multiplied with the transpose of audio sample matrix to obtain final matrix. the least significant bit of each pixel of the cover image is replaced by the bits of binary value of each element of the final matrix. this technique reduces the number of bits to be hidden as compared to the actual number of bits to be hidden."
"therefore cramer's rule implies that the function f is a continuous rational function on (0, 1), ie, it is the ratio of two polynomial of finite degrees and the denominator does not have any zeros in (0, 1)."
"silicon area and power consumption are also extracted from these models using topographical logic synthesis (see subsection vi-b). in addition, an epoch-based in-house simulator is developed (modeling the smc network shown on page 1) to estimate the performance and power consumption of executing full convnets on large images, based on the data obtained from the ca simulations. this strategy allows us to obtain both reasonable accuracy and very high simulation speed. our simulation platform supports caffe [cit] representation of the soa convnets. for every layer of the convnets under study, the optimum tile dimensions are found based on performance, energy efficiency, available spm size, and required dram bandwidth. this procedure requires multiple simulations with different combinations of parameters and is only done once per each convnet (at the beginning). optimally sized tiles can then be used in later simulations with different images. four serial link controllers in lob are modeled to consume up to 10 w of power for highest traffic pressure [cit] . we can share this 10 w power budget between the serial link controllers and neurocluster, and for example by turning off one of them we give a 2.5 w power budget to neurocluster allowing it to operate in the \"shadow\" of a powered-down serial link. performance is studied in subsection vi-a. detailed energy consumption and silicon area results are presented in subsection vi-b. finally, the overall results of the multi-smc network are presented in subsection vi-c."
"we have shown in lemma 4.1 that there exists a deterministic optimal robust policy. this motivates us to consider the following iterative algorithm that computes a deterministic policy in each iteration. in particular, in each iteration, we first consider a bellman update for the value vector of the adversary following (3.18), and then we compute a bellman update for the value vector of the decision-maker following (4.10)."
"using the model of r-rectangular factor matrix uncertainty set, we extend important structural results from the classical mdp literature to robust mdps. in this section we extend the notions of maximum principle and blackwell optimality."
"pca the number of principal components exhibits an inverse relation with the eigen threshold as observed in table 1, causing the compression ratio to decrease. for threshold value 10 _ −4, sufficient number of principal components is retained to obtain the retrieved audio message with no distortion. decrease in the number of principal components cause significant eigen vectors to be ignored, resulting in misinterpretation of the derived audio. for threshold value greater than 10 _ −4, the additional principal components do not provide any information of the message and are of lesser significance. also, the compression ratio will be decreased due to the additional data of lesser significance to be hidden in cover image. hence the optimum value for thresholding the eigen values is 10 _ −4 with ideal compression ratio of 0.8183."
"proof the function φ(π, ·) is component-wise non-decreasing because of the non-negativity of the sets w 1, ..., w r and of the fixed matrix t π ."
stegananalysis is art of reconnoitering the hidden information in a medium. the retrieval of speech signal from its redundant components is an essential criterion to discuss the robustness of the system. the generated principal components are used for deciphering the audio data using
"this result highlights the sharp contrast between r-rectangular and s-rectangular uncertainty sets. indeed, [cit] provide an example of an s-rectangular uncertainty set where all optimal robust policies are randomized."
"another point is that the straightforward topology of the traditional convnets such as lenet-5 has recently evolved to more complex topologies such as deep residual learning in resnet [cit] and the inception model (network in network) in googlenet [cit] . this makes application specific implementations less practical and highlights the need for flexible and programmable platforms. also, unlike traditional convnets with very large and efficient convolution filters (a.k.a. feature maps) of over 10x10 inputs, modern convnets tend to have very small filters (e.g. 3x3 in vgg and 1x1 in googlenet and resnet). it can be easily verified that the operational intensity (oi) 2 decreases as the convolution filters shrink. this can negatively impact computation, energy, and bandwidth efficiency (see section vi). in this paper, we design a scalable pim platform capable of running very deep networks with large input volumes and arbitrary filter sizes."
"row-major data layout: with the conventional tileoblivious layout, data is fragmented in dram, so several dma transfers are required to fetch one tile. even a dma engine with striding capabilities does not help with the inefficiency caused by opening a dram row with closed policy [cit] and partially reading from it in strides. to address this problem, we modify the underlying storage of the intermediate layers in dram to a row-major form (illustrated in figure 3c,e) . this way with a single large dma transfer request, the whole tile can be fetched by the processing cluster. this improves dram's read performance which can be exploited as described below. the implications of this mechanism on dma write and its overheads will be explained later in this section."
"where \"h\" represents the encoded bit generated from the x−or operation of the audio bit \"m\" with the bits of color information obtained from the image. the encoded bit then replaces the 680 s.h. karamchandani, k.j. gandhi, s.r. gosalia, v.k. madan, s.n. merchant, u.b. desai lsb of the respective color pixels in the cover image. since the exclusive disjunction of the three logical variables is associative and reversible the audio bit is recovered from (2)"
"backpropagation is the prevalent method for training nns including convnets [cit] . given a set of training sample inputs, first a forward propagation is executed layer-by-layer, then using an optimization algorithm, such as gradient descent (gd) [cit], the coefficients (weights) are updated backwards so that the network learns that sample. a modern training algorithm based on gd has three phases [cit] : (1) forward pass, (2) gradient calculation and routing, and (3) weight update. in step (1), the selected input (e.g. an image) is fed to the network and the outputs of all layers including the value of the loss function are calculated. this is similar to a normal inference pass, except that additional information about the current operating point (e.g., max-pool decisions) in all layers has to be stored, such that it can be retrieved later on for gradient calculation. this can be easily handled by our platform because plenty of dram is available to the neuroclusters through a high-bandwidth and low-latency 3d interface. for example, resnet-152 requires 211 mb for its coefficient and a total of 161 mb for all its layers. this aggregates to a total of 372 mb of dram storage. another difference with inference is that the pool layer should keep track of the inputs which were maximal in the pooling operation. this is called argmax and since just comparison with zero is involved, in this implementation we use the risc-v cores for it."
"the host system-on-chip (soc) is only responsible for coordination and receiving the results. it does not send or receive data at a high bandwidth, yet we keep its serial link (link0) always active, to make sure it can manage the other devices through that link. the other serial links, however, are turned on only when there is a data to send over them, and then turned off again. considering a typical bandwidth of 16 gb/sec for each serial link, and the power-state transition times obtained from the hmc specifications v2.1 [cit] [active to sleep: 600ns (t sm e ), sleep to power-down: 150µs (t sd ), and power-down to active: 50µs], the total power consumed in the smc network can be estimated as 42.8 w. the camera sends images to the cubes in a ping-pong fashion: while an smc is working on one image, the camera sends another image to its dram. this is easily achievable because there is plenty of space available inside each smc. our smc network can achieve 955 gflops @42.8 w. moreover, this architecture is scalable to a larger number of nodes because the average bandwidth demand over the serial links is not large (in the order of 10 mb/sec per image). therefore it is possible to turn on a link, transfer the image at a higher bandwidth, and turn it off, periodically. this asynchronous and on-demand mechanism allows us to achieve a scalable performance with high energy efficiency."
"nst supports strided convolution, max-pooling, reluactivation, along with some basic utilities for backpropagation and training. apart from these tasks, it can also be used for generic computations such as dot product, matrix multiplication, linear transformations, and weighted sum/average. even single fp32 operations (e.g. add, multiply) are supported for generality. more than 14 commands in three categories are implemented: streaming (e.g. stream mac, stream sum, stream max), single (e.g. single add, single mul), and memory commands (for configuration and memory transfers to/from the accumulator). subsection v-a describes how nsts can be programmed to do various computations."
"we highlight the generality of factor matrix uncertainty sets and we give an efficient algorithm to compute an optimal robust policy. to do so, we model the policy evaluation problem as an adversarial mdp and we give an alternate algorithm for evaluating the worst-case performance of a given policy, assuming that the columns of the factor matrix belong to a cartesian product set. we then prove that there always exists a deterministic optimal robust policy, which contrasts with s-rectangular uncertainty sets. we prove our strong min-max duality result and we provide an efficient algorithm to compute an optimal robust policy. we also present two examples where the optimal robust policy for factor matrix uncertainty sets address the problem of parameter uncertainty while remaining efficient on the nominal parameters. we present a computational study suggesting that when modeling uncertainty, one should care about the rank of the deviations from the nominal parameters, since empirically low-rank deviations are less conservative than independent perturbations on each component of the nominal transition kernel. it remains to investigate the case of coupled columns of the factor matrix when the parameter r is fixed."
"to summarize, three main assumptions motivate our proposed computation paradigm and tiling mechanism: a) focusing on synchronization-free parallelism rather than coefficient reuse; b) limiting the on-chip storage available to the pim cluster; c) supporting very large input images (up to 32mega-pixels). we will demonstrate that our scalable and flexible convnet acceleration platform provides higher energy efficiency compared to the best fpga and gpu implementations in similar technologies at a fraction of their system cost."
"in theorem 4.2, the fact that the optimal robust policy π * is the optimal nominal policy of the mdp where the adversary plays w * can be seen as an equilibrium for the game between the decision-maker and the adversary. this result is in sharp contrast with the case of s-rectangular uncertainty sets. while strong min-max duality also holds for the case of an s-rectangular uncertainty set p, the right-and side and the left-hand side of the following equality: max"
theorem 3.1 let p an r-rectangular uncertainty set and π a stationary markovian policy. algorithm 1 gives an -optimal solution to the policy evaluation problem in time polynomial in the input size and log( −1 ).
"neurocluster (illustrated in figure 1b ) is a flexible general purpose clustered many-core platform, designed based on energy-efficient risc-v processing-elements (pes) [cit] and neurostream (nst) coprocessors (described in subsection iii-b), all grouped in tightly-coupled clusters. each cluster consists of four pes and eight nsts, with each pe being responsible for programming and coordinating two of the nsts. this configuration is found to be optimal in the explorations presented in section vi. the pes are augmented with a light-weight memory management unit (mmu) along with a small sized translation look-aside buffer (tlb) providing zero-copy virtual pointer sharing from the host to neurocluster (more information in section v). instead of caches and prefetchers which provide a higher level of abstraction without much control, and they are more suitable for hostside accelerators [cit], scratchpad memories (spms) and dma engines are used with a simple and efficient computation paradigm to boost energy efficiency [cit] . also, caches introduce several coherence and consistency concerns and are less area and energy-efficient in comparison with spms [cit] . each cluster features a dma engine capable of performing bulk data transfers between the dram vaults and the spm inside that cluster. it supports up to 32 outstanding transactions and accepts virtual address ranges without any alignment or size restrictions. the nst coprocessors, on the other hand, have limited visibility only to the cluster's spm with no concerns about address translations and dma transfers. this mechanism allows for simple and efficient computation while maintaining the benefits of virtual memory support [cit] ."
"we give a detailed proof in appendix b. finally, we need the following lemma, which introduces the value vector β of the adversary in the adversarial mdp."
"we propose to camouflage short audio signals encrypted by their immanent principal components within the digital color images.the wave or \"wav\", the most commonly used audio file format accommodates the audio information exclusively in the later 43 bytes. the header files, followed by four bytes representing the length of the file precede the audio data. the proposed model is sub divided in two modules: steganography and steganalysis for encryption and deciphering the data as illustrated in figure 1"
"the r-rectangularity assumption enables us to develop an iterative algorithm for the policy evaluation problem. in particular, following the interpretation of the policy evaluation problem as an alternate mdp, we present a value iteration algorithm for the adversarial mdp in algorithm 1."
"numerical experiments. we present two numerical examples, where we detail the computation of the factor matrix from the estimated nominal kernel. we show that the performances of the optimal nominal policy can significantly deteriorate for small variations of the parameters and we compare the performances of robust policies related to factor matrix and to s-rectangular uncertainty sets. we show that our optimal robust policy improves the worst-case reward and has better nominal reward than the robust policy related to the s-rectangular set. our robust policy also has better empirical performances than the robust policy of the s-rectangular uncertainty set. our results suggest that the factor matrix uncertainty set is a less conservative model to handle uncertainty in parameters than the s-rectangular uncertainty set."
"we would like to note that this ball contains the uncertainty sets p (r) and p (s) . in the same table 3 we also report the means and 95% confidence levels of the rewards r(π rob,r, p ) and r(π rob,s, p ) when we draw 10000 kernels p uniformly within b ∞ . table 3 : empirical performances of the policies π rob,r and π rob,s . we draw 10000 kernels p in b r and b ∞ and we report the means of the ratio r(π, p ) r(π nom, p nom ) and the 95% confidence levels, defined as 1.96 · std/ √ 10000 where 'std' stands for the standard deviations of the observed rewards."
"therefore, we want to compare the performances of the robust policies π rob,r and π rob,s on the same sample of transition kernels. similarly as in the previous example, we first simulate 10000 kernels p uniformly in"
"might not be attained by the same pairs of policies and transition kernels. indeed, if it was the case, it would also imply that there exists an optimal robust policy that is deterministic. [cit] show that this is not the case and give an example where all optimal robust policies are randomized."
"since there is no data overlap among augmented-tiles (except possibly for some filter coefficients), each cluster can execute one tile at a time. this minimizes communication among the clusters. also, tiling information is prepared offline (only once) and is stored in a list accessible by all clusters in dram. the master pe (the first pe in each cluster) consults this list to obtain the required information (e.g. address in dram, size, and filter coefficients) for the next tile. then it issues a dma read to fetch the new tile. each cluster works based on ping-pong buffering to hide the setup and dma transfer latencies. while one tile is being computed by the nsts in the cluster, another tile is fetched by the master pe and tiling information is prepared for it. this procedure continues until all tiles in a layer are finished. at this point, all clusters are synchronized before proceeding with the next layer."
"they refer to this as a (s, a)-rectangular uncertainty set and show that for such uncertainty sets, one can efficiently compute the optimal robust policy using a robust value iteration. moreover, there is an optimal robust policy that is stationary, markovian and deterministic."
"equivalently, we could consider an algorithm based on the contraction f 2 . we present this alternate algorithm (algorithm 3) in appendix i. we would like to note that the running times of algorithm 2 and algorithm 3 essentially differ by a multiplicative factor related to the logarithm of the initial errors v 0 − v * ∞ and β 0 − β * ∞ . the vector v * is of size s and the vector β * is of size r. therefore, depending on the orders of magnitude of r and s, it can be faster to implement algorithm 3 or algorithm 2."
"where cov (x) denotes the covariance of the input data and i represents the identity matrix. the solution of the resulting covariance matrix is a function of the eigenvectors and eigenvalues of covariance of x. the decorrelating matrix w contains vectors that normalize the input's variance also called as the principal components while the audio signal gets scaled to a well developed gaussian curve with unit variance in all dimensions. the decorrelating matrix is used to find the directions of maximal variance in the audio data. the variance of each principal component is reflected in the eigen values obtained from the as a solution of diagonalization algorithm.the audiosigna lx e,is obtained in (6) as a scrambled waveform with its redundancies eliminated."
"the average performance efficiency (actual/peak performance) of a single cluster measured in ca simulation is illustrated in figure 6, where the cluster is executing tiled convolution on its nsts with 1x1, 2x2, and 3x3 filters over tiles with average dimensions of the studied convnets, listed in figure 7 . we define performance efficiency (p ef ) as follows:"
the framework of the paper is as follows. literature review of steganography related to audio embedding is detailed in section 2. the subsequent section 3 discusses the proposed algorithm elaborating the pca encryption of audio data followed by the implementation of steganography and stegananalysis modules in section 4. simulation results are tabled in section 5 with the conclusions drawn in section 6.
as in the previous example we construct two uncertainty sets p (r) and p (s) and we compute the worst-case of the optimal nominal policy π nom . we then compare the performances of the robust policies associated with these two models with the performances of π nom .
"finally, looking at the silicon area results in figure 12c,d we can see that the neurocluster (8.3 mm 2 ) occupies around 8% of the area in the lob die with the spm (55% of neurocluster), the risc-v cores (16.5%), and the nsts (16%) being its main contributors. the total area for the lob die was estimated as 99mm 2 . it is worth mentioning that the lob die of hmc is already larger than its dram dies [cit], and it is occupied by four serial link controllers (∼ 55mm"
"while mdps provide a tractable approach for modeling many practical applications, it is important to note that in many applications the transition kernel p is a statistical estimate from noisy observations where statistical errors are unavoidable. therefore it is an approximation of the true transition probabilities of the problem. the optimal policy for the nominal parameters could potentially be highly sensitive to even small perturbations in the problem parameters and lead to highly suboptimal outcomes. citemannor show that the expected reward (1.1) can significantly deteriorate even with a small variation in the parameters. therefore, it is important to address the uncertainty in parameter estimates while computing the \"optimal\" policy."
"it is important to explain how the raw output tile of one layer (l) is converted to an augmented tile for the next layer (l + 1), given that data cannot be \"magically\" reorganized in the dram. looking at t 0 in figure 3e, we can see that it has 4 regions (raw, a, b, c). the raw region of t l+1 0 is written to dram using multiple fragmented dma writes when t l 0 is computed in spm. this is shown in figure 3f are computed, respectively, using small dma chunks shown in figure 3f . zero-padding is also properly handled at this stage for the corner tiles. since dram writes are off the critical path, we can afford to perform these conversions, without incurring significant overheads. another key point is that the raw-tile width and height of the consecutive layers must be equal (for consistent row-major data layout) unless there has been a strided convolution [cit] or pooling stage between them, for which the tile dimensions will shrink. this way, as we move forward through the convnet layers, tile width and height (t fig. 4. (a) a convolution kernel to be performed on a 4d-tile, (b) a typical memory access pattern generated by this kernel."
"the functions f 1 and f 2 are component-wise non-decreasing contractions, see lemma f.1 in appendix f. therefore, their fixed-points are the unique solutions to the optimality equation (5.1). in order to solve the policy improvement problem, it is sufficient to compute the fixed point of f 1 (or f 2 ). following puterman [cit], we know that the"
"are not yet lower than its previous versions. instead in this paper, resnet-152 is extended to larger networks (accepting 250k/1m/2m/4m-pixel images shown in table i ) to further investigate the scalability of our approach and its applicability to beyond high-definition (hd) image resolutions. resnet is chosen for this purpose because it is more challenging to accelerate than the other networks (see subsection vi-a)."
"when a convnet such as googlenet is selected for execution over our pim system, first it is tiled using the 4d-tiling mechanism described in subsection iv-a. this procedure prepares it for parallel execution over the clusters, and optimally partitions it to achieve the highest efficiency under given constraints such as on-die spm and dram bandwidth usage. next, all coefficients are loaded in smc's dram and an additional space is reserved there for the intermediate results of the largest layer (shown previously in table i ). the input volume (e.g. the image or video frame) is loaded into this area before each run. the actual execution takes place layer-by-layer, each layer being parallelized over 16 clusters. each cluster executes one 4d-tile at a time with all its nsts working cooperatively to compute its final result inside the cluster's spm. only at the end of each layer, the clusters are synchronized. a more detailed description follows in subsection iv-a and subsection iv-b. co are the numbers of input and output channels to the tile. the output dimensions of each tile are calculated directly from input width and height, filter dimensions, striding, and zero-padding parameters. 4d-tiles have three main features essential for near-memory acceleration of deep convnets:"
"the pareto analysis is used to determine the total number of eigen values corresponding to the principal eigen vectors to be considered for audio retrieval. this process we terms as eigen thresholding. a pareto chart in figure 2 thresholding the eigen value, the principal components with least contribution have been ignored due to redundancy in theinformation. table 1 illustrates the quantum of principal components and compression ratio obtained for different eigen thresholds."
"in this paper, we proposed a scalable and energy-efficient pim system based on a network of multiple smc devices. each smc is augmented with a flexible clustered many-core called neurocluster, capable of executing deep convnets with growing memory footprints and computation requirements. to this end, neurostream (a streaming fp32 coprocessor) is proposed, along with an efficient tiling mechanism and a scalable computation paradigm. our proposal increases the lob die area of the standard hmc only by 8%, and achieves an average performance of 240 gflops for complete execution of fullfeatured modern convnets within a power-budget of 2.5 w. 22.5 gflops/w energy efficiency is achieved in a single smc (consuming 11 w in total) which is 3.5x better than the best gpu implementations in similar technologies. the performance was shown scalable with a network of smcs. it is worth stressing that our platform allows for offloading the convnet tasks completely into the memory cubes at a minor system power cost. this implies that the compute logic in the host soc is free to deal with other workloads. also, the cost increase with respect to a baseline hmc system would be negligible. therefore, essential convnet acceleration is provided at a very small system cost. ongoing research efforts include silicon implementation of the neurocluster with 5 clusters, parallel implementation of training on this architecture, and pushing further to achieve higher performance and efficiency inside the cubes (e.g. more advanced refresh and power management schemes to reduce the power in unused dram pages)."
"iii. system architecture convnets, by nature, are computation demanding algorithms. one forward pass of vgg19, for example, requires around 20 billion mac operations with over 100k operations per pixel. maintaining even a frame-rate of 10 frames per second will require over 200 gflops. in theory, convnets can reach extremely high oi ratios (discussed in subsection ii-b), as they reuse data efficiently. however, due to the very large memory footprints of deep convnets, their performance and energy efficiency is ultimately constrained by the main dram storage and off-chip communication."
"inside each cluster the master pe partitions the tile among the nsts in the order of t co dimensions first. this is to ensure that each output is written exactly by one nst, and to remove synchronization requirements among the nsts. if still more nsts are remaining (e.g. for small corner tiles), t"
"similarly, each nst is able to perform relu activation on arbitrary tiles using stream max command devised for this purpose on the same set of state machines and hardware blocks. for the sake of generality, stream sum, stream scale, stream shift, and stream min are implemented, as well. another widely used operation in convnets is pooling [cit] . nst supports max-pooling [cit] through the stream maxpl command. thanks to the flexibility of the agus and hwls, arbitrary tiles with different strides are supported. finally, fc layers can also be implemented using a set of stream mac commands, similar to the conv layers. the class layer, however, is executed on the pes in the current implementation using the softfloat library [cit] ."
"we introduce here a linear programming reformulation of z(π), which is useful to analyze the structure of the set of optimal robust policies in the next section."
"where (e.1) follows from π ⊂ π g, (e.2) follows from weak duality. the equality (e.3) follows from the fact that for a non-robust markov decision process, an optimal policy can always be found in the set of stationary markovian policies, and (e.4) follows from theorem 4.2. we conclude that all these inequalities are equalities, and it follows that f proof of lemma f.1."
remember that π g denotes the set of all policies (possibly history-dependent and non-stationary) and π ⊂ π g is the set of stationary markovian policies. we want to prove:
"budget of deviation τ 0.05 0.07 0.09 nominal reward of π rob,r 100.00 100.00 100.00 worst-case of π rob,r for p (r) 94.40 92.21 90.04 nominal reward of π rob,s 99.28 98.53 97.81 worst-case of π rob,s for p (s) 91.90 89.09 86.62 table 2 : worst-case and nominal performances of the robust policies π rob,r and π rob,s ."
a brief introduction to convnets is presented in subsection ii-a. the evolution of modern convnets and their uprising implementation challenges are explained in subsection ii-b. the existing implementations for them are compared with this work in subsection ii-c.
"having found the optimum tile dimensions, figure 8a depicts the overall performance (gflops) and total execution time for the studied convnets on a single cube. among the studied convnets from the soa, vgg networks have the highest execution time due to their higher requirement for mac operations, while googlenet and alexnet are the fastest ones executing in less than 12 ms. googlenet has a very low computation requirement (less than 2 gmac for each forward pass) compared to the other modern networks (resnet and vgg) mainly due to the use of strided convolution in the beginning layers. it can be further seen that the resnet group achieves the lowest performance efficiency. this can be associated with higher percentage of spm conflicts illustrated in figure 8b . in this figure, four main sources of performance loss are identified as: t l : loop overheads, t s : cluster synchronization, t b : bandwidth limit, t c : spm conflicts. in the neurocluster architecture, the risc-v cores are responsible for tile preparation, loop initialization, dma setup, and synchronization with other clusters. while nsts are responsible for the actual computation on the spm (t u : useful computation). for this reason, the risc-v cores account for (t l + t s ) overhead cycles, while the nsts account for (t c + t b + t u ) from the total execution time. overall, for all studied convnets, less than 6% of the total execution time was spent on the risc-v cores, and the rest was spent on nsts, either for useful computation, or waiting for the memory system. tile preparation and dma setup phases are also handled by the risc-v pes, nevertheless, they are overlapped with the execution on nst so they are hidden and do not contribute to the total execution time. it is also worthy to note that among the overheads shown in figure 8b, only loop overheads are caused by the proposed tiling mechanism, which account for less than 5% of the performance loss."
"the proof relies on lemma 4.3 and the strong duality of theorem 4.2. it is detailed in appendix g. we would like to note that the proof can not be adapted to an s-rectangular uncertainty set, since it relies on the fact that an optimal robust policy can be chosen deterministic when the uncertainty set is r-rectangular."
"( 3.18) this can be done by iterating the function φ(π, ·), and algorithm 1 is a value iteration algorithm that returns the fixed point of φ(π, ·). from puterman [cit], the condition"
"using the rectangularity of the policy set, one derives a fixed point equation for the value vector of the mdp from the classical bellman equation:"
budget of deviation τ 0.05 0.07 0.09 worst-case of π nom for p (r) 50.26 41.74 35.63 worst-case of π nom for p (s) 45.75 37.37 31.51 table 4 : comparison of nominal and worst-cases over p (r) and p (s) for the optimal nominal policy π nom .
"the pca technique incorporated in our proposed algorithm is a method of compression which is camouflaged as an encryption technique for the audio signal. the proposed technique performed in the spatial domain and suggests a robust encryption algorithm, thus overcoming the drawbacks of the existing methods."
"in proposition 2.4 of section 3, we have shown that there is a robust optimal policy that is stationary and markovian. in the following lemma, we show that a robust optimal policy can be chosen stationary, markovian and deterministic."
"additional computation do each of the training stages need compared to their corresponding stage in inference? (ii) how much efficiency is lost due to the extra usage of the risc-v cores for computation? for the forward pass, the only extra operation is the argmax function in the pool layer. for gradient routing, fc and conv layers require additional computations (on the nsts), while act and class need the same amount as inference. pool, also, implements a different operation (on the risc-v cores) as described in subsection v-b. finally, the weight-update phase works solely on the fc and conv layers, and its overhead is found to be less than 5% of the total execution time. we have chosen googlenet as a representative of the future convnets with strided convolutions, shrinking kernel coefficients, and complex topologies [cit] . for googlenet we have estimated the execution-time of each kernel relative to its corresponding kernel in inference. we have, then, scaled the execution times of inference with these results. table ii summarizes these estimates, where \"training (best)\" indicates the estimated execution-time provided that the nsts implement the additional required functions such as argmax and vector multiply. this is not achievable in the current version, and it is planned to be done as a future work. \"training (current)\" is the estimated execution time with the current platform. it can be seen that one training pass takes 3.6x longer than one inference pass (almost 3x more in the best case). this is reasonable and consistent with our previous measurements on gpus [cit] . also, the amount of efficiency loss due to using risc-vs for part of the computation is 17%. overall, the training performance can be estimated as 197 gflops for googlenet."
"a complete processing cluster was synthesized using synopsys design compiler [cit] .09-sp4) in the topographical mode in 28nm fdsoi technology of stmicroelectronics (1.0v, ss, 125"
"cryptosystem, the goal of steganography is to hide messages inside other harmless messages in a way that does not allow any enemy to even detect that there is a second message present. a modern steganographic system should defeat detection even by a machine. it replaces bits of useless or unused data in computer files such as graphics, sound, text, html with bits of different invisible information. this hidden information can be plain text, cipher text, sound, and even images. ideally steganography can be used for any communication channel. however in real life the cover media generally used are multimedia objects such as image, video, and audio files. the reasons include that cover media should be large compared to the size of the secret message, and the methods so far developed have less than one percent of the cover size, and the indeterminacy in the cover is necessary to achieve the necessary security. large objects without inderminacy like the value π at a very high precision are not suitable. the transmitting data should be plausible as in the modern digital society dependence on audio and image files are so prevalent. it is desirable that a steganography system should have a good embedding capacity, be secure and robust. modern steganography uses a number of techniques such as masking and filtering, algorithms and transformations, and least significant bit insertion [cit] . in masking and filtering, the information is hidden inside of an image using digital watermarks that include information such as copyright, ownership, or licenses. it adds an attribute to the cover the image thus extends the amount of information presented. in algorithms and transformations technique data is hidden in mathematical functions that are often used in compression algorithms and facilitates to hide the secret message in the data bits in the least significant coefficients. the least significant bit insertion is the most common and popular method of the modern day steganography, and it uses lsb of a picture's pixel information keeping the overall image distortion to a minimum while the message is spaced out over the pixels in the images. this technique works best when the image file is larger than the message file. we propose to inculcate audio information in color images following the encryption of the short audio messages using the principal component analysis (pca). the age old adage of pca incepts as an encrypting tool diversifying from its myth as a compression standard."
"convnets are typically built by repeated concatenation of five classes of layers: convolutional (conv), activation (act), pooling (pool), fully-connected (fc), and classification (class) [cit] . conv is the core building block of the convnets doing most of the computational heavy-lifting for feature extraction. it essentially consists of multiply-andaccumulate (mac) operations as shown below [cit] :"
"the intractability of the policy evaluation problem indicates that the policy improvement problem is also intractable. in view of the intractability for general factor matrix uncertainty set, we make the following additional assumption on the set w."
"in fact, we prove a stronger result: the left-hand side and the right-hand side attain their optima at the same pair (π *, p * ). this implies that the optimal robust policy π * is an optimal policy for p * . therefore, an optimal robust policy can be chosen stationary and markovian. this also implies that there always exists a deterministic optimal robust policy. note this is not always the case for any uncertainty set: for s-rectangular uncertainty set in particular, the min-max duality holds, but as we mentioned earlier, there might not exist a deterministic optimal robust policy."
"we now show that for r-rectangular uncertainty sets, there exists an optimal robust policy that is stationary and markovian. in particular, we have the following proposition."
"to gain further insights from this plot, a breakdown of total execution time is depicted in figure 9a versus the size of the convolution filters. as can be seen, a significant portion of the execution time (over 45%) of the resnet group is spent in 1x1 filters, and as we saw previously in figure 6, 1x1 filters cause more spm conflicts than larger filters. plus, for 1x1 filters the 3 convolution loops illustrated in figure 4a change into a single loop iterating over t c i . this increases the relative overhead of nst initialization (shown in figure 8b) ."
"this section presents the estimated performance and energy efficiency for the smc network previously shown on page 1. four smc devices are connected to each other using mesh topology with an hd camera recording raw images of 8 m-pixels. resnet has been chosen as the most difficult to accelerate convnet among the studied ones due to having a significant percentage of 1x1 and 3x3 kernels (more than 95%), with a very large number of layers (more than 100). the camera sends the images to the memory cubes over the highlighted links in figure 1a, and each smc executes resnet on one complete frame, independently from the other cubes. this ensures minimum communication among the cubes and allows for turning off the serial-links for a large portion of the time. each smc has a copy of the convnet coefficients inside its dram dies, and the coefficients have been preloaded once at the beginning."
"maximum principle and blackwell optimality. we show that certain important structural properties that holds for classical mdp also hold for the optimal robust policy for factor matrix uncertainty sets. in particular, we present the robust maximum principle, which states that the worst-case value vector of an optimal robust policy is component-wise higher than the worst-case value vector of any other policy. moreover, we prove the robust counterpart of blackwell optimality, which states that there exists a pair (π *, p * ) that remains optimal for the policy improvement for all discount factor sufficiently close to 1, and π * can be chosen deterministic."
"performance parameters such as mean square error (mse), peak signal to noise ratio (psnr), normalized absolute error (nae) [cit], maximum difference (md) and structural content (sc) are used to determine the quality of stego image and are calculated as (9)− (13). table 2 charts the performance parameters of stego image for various thresholds of eigen value."
"on scalable network-on-chips [cit] . these networks have a great potential for solving time-dependent pattern recognition problems because of their inherent dynamic representations. all these emerging dl models can be future targets for our pim proposal, yet, in this paper, we focus on convnets for image and video."
"optimal robust policy. we give an efficient algorithm to compute an optimal robust policy. to do so, we first show that the evaluation of the worst-case of a policy can be reformulated as an alternate mdp. we then show that the problem of maximizing the worst-case reward can be reformulated as a coupled mdp, where the decision-maker is playing a \"zero-sum\" game against an adversary. then computing an optimal robust policy reduces to finding the fixed point of a contraction. this yields an efficient algorithm for finding an optimal robust policy, using robust value iteration. to the best of our knowledge, this is the first example of an uncertainty set where transition probabilities across different states are related and still one can compute the optimal robust policy."
the encryption is based on the gaussian assumption that an n-dimension data evolves n directions of variance. the one dimension audio signal is trapped in a 2-d array and is represented as (3)
"we show that the policy evaluation problem can be written as an alternate mdp with r states and s actions, and a set of policy w. this alternate mdp is played by the adversary. let us introduce some notations to formalize our intuition. we fix a policy π and let"
"role of r-rectangularity. in the classical mdp framework, one assumes that the decision-maker can independently choose the distributions π s across different states. this is because the set of stationary markovian policies π is itself a cartesian product:"
it is worth noting that the transition probability only depends of the chosen action s and the arriving state j but not of the current state i. the reward only depends of the chosen action s and not of the current state i.
"outline. in section 2, we present the factor matrix uncertainty model and discuss its generality. we present an efficient algorithm for computing the worst-case of a given policy under our model in section 3. in section 4, we present the strong min-max duality result and the structure of the optimal policy. we present an efficient algorithm to compute the optimal robust policy in section 5, and discuss structural properties of our robust mdp in section 6. finally, we present numerical experiments to compare the empirical performance of our model in section 7."
"as we will show throughout this paper, in a near-memory context some of these constraints can be relaxed, providing the possibility to improve energy efficiency and programmability. subsection iii-a describes the design of our many-core pim platform."
"2 ), with almost no free space available in it. any major addition to this die requires modification of the 3d stack structure and power delivery network. this is why we have tried to keep the area increase to a maximum of 3% in each dimension. the optimal parameters found in this section were listed in section i and used in all experiments."
for the sake of completeness we give a proof of lemma 3.2 in appendix a. we also need the following reformulation of the policy evaluation problem.
"lastly, an estimation of the training performance on smc can be obtained by answering two questions: (i) how much figure 1, where the number of pes has been changed from 2 to 8."
the same analysis as for algorithm 2 shows that the running time of this algorithm is in o r · s 3 · log 2 1 + s · a · log 1 .
"in this study, we develop a pipeline that includes two machine learning algorithms, inspired by simple linear models, coupled with follow-up approaches for systematic data analysis. our systematic analysis includes data quality checks, identification of important features, as well as combinatorial and stability analyses. we applied and validated our pipeline with three different previously published -omics datasets. our approach successfully identified the markers reported in the literature as well as potential novel markers."
", represented by five-dimensional feature vectors, is used to train and test k-nn classifier. several classifiers, such as naïve bayes (nb), decision tree (j48), random tree (rt), and svm, are utilized to take performance comparison with k-nn. figure 5 shows the process of training and testing a classifier. in this stage, 10-fold cross validation is used to avoid the variability of the samples that affect the performance of model training and testing. in 10-fold cross validation, the whole dataset is divided into 10 unduplicated subsets. nine of the 10 subsets are used as training subset and the remainder is used for testing. then 10 classification results are obtained by 10-fold cross validation. finally, classification accuracy is the average value of the 10 classification results."
"with the development of network and storage capacity, multi-dimensional data representation and analysis tasks are becoming increasingly difficult and challenging. inspired by the applications of graphical representation approaches above, the radar chart, originally applied to analyze the financial situation of enterprise, is used to general new graphical features for intrusion detection in this paper. as a result, a hybrid approach is proposed to transform the original multi-dimensional feature space into low-dimensional graphical feature space. then, the graphical features are regarded as the new feature space and used for the final classification decision."
"one advantage of using feature selection algorithms is that the final model is built automatically, including only those biomarkers which are useful in predicting patient condition. thus, we do not have to rely on the cut off for selection of genes and metabolites upfront. all the estimates are decided based on either biomarkers' effects. however, one of the drawbacks of this method lies with the selection of the appropriate penalty parameters. failure to decide on appropriate penalty factor will result in underfitting or over fitting of the results. to address this, we split the data into two subsets, training and testing. within the training subset, we estimated the penalty factor by using ten-fold cross validation. the optimized model was then fit to the unseen testing subset."
"all analyses were performed in the r statistical computing (r version 3.4.3) environment [cit] . all r packages can be found in our project's github repository stated below. the necessary software dependencies are described in the readme file located in the repository. all analyses can be performed on a standard pc environment with the run time increasing with larger datasets. for example, an analysed rna microarray acute myeloid leukaemia (aml) dataset described below took 8 h to complete, but at no time did the r environment use more than 1 gb"
"it is worth mentioning that certain areas of precision medicine benefit greatly from incorporating single-cell sequencing data, especially cancer. while multiple -omics approaches can be used with single-cell sequencing [cit], rna-sequencing applied to single-cell data has been used extensively, and we will focus our discussion on this area [cit] . single-cell transcriptomics (scrna-seq) have potential for monitoring patient response to treatment and characterizing lineage-specific mutations which may respond to variable treatment protocols. before incorporating scrna-seq data into the pipelines described above, experimentalists and analysists must be aware of several differences in protocol which affect normalization of scrna-seq data. [cit] produced a fundamental review of challenges which are currently being addressed by the community. in essence, data must be carefully curated, select data must be normalized after additional quality controls not applicable to bulk rna-seq. this may necessitate the inclusion of synthetic or alternate species controls (spike-ins) in the sequencing experiments not always used in bulk data analysis. after normalization, populations of cells may be identified by several unsupervised learning methods, from clustering to tsne [cit] . our pipeline may add value to single-cell analyses by picking up at this stage and integrating count matrices of cell-types separated by clustering approaches. with the separation of cell populations, dominated by driver genes characterizing cell types or disease states as labels, our pipeline can be applied to select gene transcripts which act as biomarkers. with these biomarkers identified, subsequent patient monitoring may be applied to surveil tissues for tumour progression and guide the application of treatment or help reveal mechanisms in cells which survive treatment after resequencing [cit] . finally, it is worth mentioning too, that useful clinical translation will only follow from a better understanding of the underlying biological mechanisms for the biomarker´s discovered."
"the genes identified by our pipeline are often discussed in pancreatic cancer literature [cit] . not only did we identify gene sets in relevant tissues which are, in combination, highly discriminative between pancreatic cancer and control, but the in-built multivariate analysis revealed interacting networks which model differences between cancer and control patient data better than single genes alone. our analysis also highlighted the crosstalk between autophagy and certain cancer types. given the prevalence of autophagy pathways perturbed in pancreatic cancers, this result confirms recent novel studies demonstrating autophagic control of pancreatic cancer metabolism [cit] ."
"besides the quality of the classification results obtained by gfnn, it is important to consider the efficiency of the method. we evaluate the cpu time of gfnn by comparing against the baseline k-nn, tann, and dssvm. we divide cpu time into feature generating, training and testing. the feature generating for tann, dssvm and gfnn contain three parts: (i) dataset preprocessing; (ii) cluster centers extraction with k-means; (iii) dataset transformation. table 4 shows a comparison of the cpu time taken by each approach. looking at table 4, although k-nn, with 41 original features, doesn't need to take cpu time to do feature generating, it requires greatest training and testing time. meanwhile, the training and testing time and the total cpu time taken by gfnn is the least among these approaches. overall, gfnn's new low-dimensional feature vectors composed of the sub-barycenter can provide relatively high efficiency for intrusion detection."
"we present a data-driven, generalizable, robust, low-bias machine learning workflow that generates easily interpretable outputs and focus on simple visualizations aiming at actionable biomarker discovery. we believe that our workflow will help researchers to identify significant explanatory features of experimental -omics data, reducing the search space for good candidates for experimental"
"in this paper, we propose a novel hybrid method based on a graphical features-based k-nearest neighbors approach, namely gfnn, for intrusion detection. the proposed approach tests intrusion detection over the kddcup99 dataset. firstly, the k-means clustering algorithm and radar chart are used as feature generation tools. then, this new and 5-dimensional sub-barycenter based feature is used to represent each data sample for intrusion detection by a k-nn classifier. from simulation results, the performance of the proposed algorithm outperforms other existing approaches for class dos, u2r and r2l. it also provides high computational efficiency for the time of classifier training and testing. in future work, some issues will be considered. firstly, we plan to extract other graphical features from radar chart to construct low-dimensional feature space. secondly, different clustering algorithms and classification techniques can be applied during the cluster center extraction and classifier training stages, respectively. finally, our proposed feature generation methods will be used as a preprocessing step over other 02041-p. 6 datasets which contain different numbers of classes and also do a thorough comparison of our proposed feature generation methods with other ones."
"in -omics literature, there has been a recent trend towards the identification of data pre-and post-processing steps. for example, [cit] developed an infrastructure comprised by a combination of web services, transmart, galaxy, and minerva platforms. [cit] developed an integrated reference-independent analysis of metagenomic and metatranscriptomic data for the analysis of microbiome derived datasets. feng [cit] developed a proteomics pipeline called firmiana. firmiana is a cloud platform that allows scientists to deposit mass spectrometry (ms) raw files online and performs automated bioinformatic analyses on the uploaded data. such existing robust pipelines for analysing -omics data are often either focused on specific -omics data or can be used only for either classification or regression purposes. for example, [cit] developed a workflow for quantitative metabolomics datasets, [cit] developed an -omics fusion tool but focused on metabolomics data in regression mode only. [cit] developed a pipeline based on transcriptomics data called confero that extracts gene lists from research papers and performs automatic extraction and storage of gene sets. while this is useful for downstream analysis, there is a need to combine these approaches and deal with multiple types of outcome data as well as consider their categorical or continuous nature. in some cases, the complexity of machine learning models associated visualizations used hinder the interpretability of the results and therefore impair their translation into clinical science."
a graphical depiction of our workflow can be seen in fig. 1 . our pipeline can be divided in the three modules that we describe below with the purple data quality module offering different options depending on the type of data introduced e.g. microarray and generic/other data.
"in order to assess the wider applicability of our approach for identifying target molecules in different types of -omics data, we also applied it in two transcriptomic datasets, one for acute myeloid leukemia and one for pancreatic ductal adenocarcinoma (pdac). whereas the lipidomic analysis could be validated by expert curation, our transcriptomic analyses were validated via external pathway and ontology gene set enrichment tools. aml driver gene analysis revealed a set of genes known to be enriched for targets of the myb transcription factor. myb is known to play a crucial role in hematopoietic stem cell cycles, including proliferation and survival, and recent research has shown that aml-specific micror-nas target c-myb [cit] . additionally, a potentially drugable compound targeting myb was recently discovered [cit], highlighting the clinical role of myb targets. by highlighting the genes which are both predictors of aml and enriched as a set for myb targeting, we have identified a set of novel gene targets of the myb transcription factor."
"our pipeline iterates model creation 100 times and selects the features that appear more than 90 times in the analysis, as these we deem to be the more significant for the classification model. moreover, in order to better understand the relationship between the features selected and the outcome variable analysed, a display of the weight ( β coefficients) distribution per model (see additional file 1) and a box plot of the class differences per feature is generated (fig. 3b) . these selected features are then considered potential candidate biomarkers. in order to ascertain their validity as biomarkers, their performance is evaluated both alone and in combination."
"the proposed gfnn consists of three stages, which are cluster centers extraction, new data formation by graphical features, and training and testing k-nn based on the new data. from methods [cit] we learn that the distance between the data sample and each clustering center has high discriminative power, and may lead to classification algorithm (e.g., k-nn and svm) achieving similar or better classification accuracy than the original high-dimensional feature vectors. moreover, the graphical feature of radar chart generalization approach is a visualization strategy that enables people to identify the class to which the data belongs [cit] . the experiment"
"for evaluating the performance of gfnn, two recently proposed approaches are reproduced to compare with gfnn. these approaches are trained and tested over the 52-dimentional preprocessing dataset. one approach is distance sum-based support vector machine (dssvm) [cit] . we examine several kernel function (such as polynomial kernel, rbf kernel) and different degree in order to obtain the best performance for comparison. another is triangle area-based nearest neighbors (tann) [cit], in which different k values are examined for getting the best k-nn classifier for comparison. table 3 shows the performances of these approaches over the 52-dimentional preprocessing dataset. as illustrated in table 3, the total classification accuracy of gfnn is 98.536%, which is slightly lower than that of tann (98.542%) and higher than that of dssvm (98.037%). in addition, the gfnn also works the best for class dos, u2r and r2l among the three classifiers with an accuracy of 99.38%, 57.69% and 95.80%. although gfnn does not perform the best for class normal and probe, accuracy and detection rates of gfnn is very similar to the ones of tann. this indicates that there is no significant difference in their performances."
"in this paper, we try to develop a novel effective and efficient intrusion detection system. the main goal of this paper is aiming to achieve feature reduction, data presentation, feature generation, and to enhance ids with hybrid method combing k-means clustering algorithm, graphical features generation approach and k-nn classifier. firstly, k-means clustering algorithm is used to extract cluster center of each pre-defined category in the given dataset, which contains n categories. secondly, the distance between a specific data sample and each cluster center is calculated, and then a radar chart is applied to represent each new data sample composed of distance based features. thirdly, new graphical features (e.g., barycenter) for each sample are extracted from the radar chart. finally, for classification, some classifiers are used to detect attacks based on the new dataset described by graphical features to obtain the best performance."
the experiments are carried on an intel core 2.90 ghz computer with 4 gb ram running windows 7. we use a data mining software weka to build classifiers in the experiment. weka is a collection of machine learning programs for data mining tasks written in java. the version used is weka 3.7.
we developed a systematic way of analysing -omics datasets to identify potential biomarkers from largescale -omics datasets. we used three different datasets (two transcriptomics and one lipidomics) to validate our approach by identifying potential markers or signatures b a fig. 2 a the frequency of the lipids (y axis) ranked by their selection out of 100 randomized sampling splits using elastic net. b a similar analysis is shown using lasso. features identified in both analyses are used for further investigation and comparing with existing markers found in the literature.
"in this section, we discuss the experimental dataset: kddcup99 dataset. kddcup99 [cit] . kddcup99 dataset has become a benchmark dataset for intrusion detection. the complete dataset has almost 5 million input patterns and each record represents a tcp/ip connection. considering the sake of simplicity, the dataset used in our study is the 10% subset, namely kddcup.data_10_percent, which contains 494,021 instances.except normal class, the recordings in kddcup99 dataset can be divided into four attack classes as follows:"
"in order to enhance detection precision and detection stability, machine learning has been widely used to improve idss, such as fuzzy theory [cit], k-nearest neighbor (k-nn) [cit], support vector machine (svm) [cit], artificial neural networks (ann) [cit], naïve bayes networks [cit], decision tree [cit], genetic algorithm (ga) [cit], self-organizing maps (som) [cit], and markov chains [cit], etc. based on the number of learning techniques used, those idss can be categorized as two types: idss using single learning techniques and idss with hybrid learning techniques. prior researches have demonstrated that an ensemble of several different techniques performs better than each technique individually [cit] . however, those hybrid techniques should upgrade the computational complexity and make it hard to deal with large datasets. for classification problem with high dimensional data, feature reduction techniques are always an effective method to delete the redundancy and irrelevancy features, decrease the computation time, and finally improve the ids system performance. in this line of research, some feature reduction techniques have been applied to improve the performance of idss, such as ga [cit], entropy, principal component analysis (pca) [cit], etc."
"in the pancreatic cancer dataset gse15471, top features selected included the following 20 genes: sulf1, col10a1, mir34ahg, inhba, col8a1, fn1, thbs2, nox4, ntm, rasal2, adamts12, capg, cthrc1, fap, vcan, slpi, wisp1, ltbp1, gprc5a, timp1. our biological pathway analysis revealed a class of biomarkers enriched in several known cancer pathways. after stringent multiple testing correction, the following pathways were identified as being enriched: the gene set was further enriched in the human proteome map in adult esophagus, lung, and pancreas tissues, indicating a potential cross-talk among tissuespecific cancer pathways. the full results of our pipeline, including feature ranking graphs and roc auc, sensitivity, specificity, and accuracy scores of each variable, as well as all the executed code are included in additional file 1."
"in high-dimensional -omics data analysis, we are interested in finding a relevant smaller subset of variables that are associated with the response (a clinical phenotype). procedures to identify such smaller subsets are called variable or feature selection procedures. by employing such procedures, it is possible to reduce the dimensionality of the data [cit] . moreover, feature selection can assist in removing noise variables (variables which have no predictive power for the response variable) in the dataset. more specifically, typical reasons to employ feature selection procedure include: large number parameters, features or variables (p) compared to the number of the samples or individuals (n) and correlated features."
"as a result, the n-dimensional original feature vectors are transformed into new five-dimensional sub-barycenter feature vectors. finally, the new dataset l d is used for training and testing the k-nn classifier."
"we choose the confusion matrix as the performance measures to estimate the efficiency of classifier. the following measures are derived from the confusion matrix, and will be used for evaluating the proposed scheme: moreover, we use the area under the receiver operating characteristic curve (auc) to measure the performance of the proposed method for ids. usually, the auc value ranges from 0.5 to 1.0, with larger auc values representing better performance of the classifier."
"in the kddcup99 dataset, there are several nominal features. before extracting cluster centers based on k-means, the nominal features are mapped into binary numeric features. note that \"service type\" has 67 different values, which would heavily increase the dimensionality if mapped into binary features. specially, for comparing with dssvm [cit], the \"service type\" feature is not used in this paper. after mapping nominal features into binary values, 52 numeric features are constructed. because the scales of some numerical features in the kddcup99 dataset are different, for instance, \"logged in\" has binary value, whereas \"source bytes\" has a range from 0 to 693,375,640, normalization is required that it can avoid attributes with greater values dominating these attributes with smaller values. in this paper, the numerical attributes are scaled into the interval [cit] by dividing every attribute value by its own maximum value."
"here, the outcome of model fitting produces the vector of estimated regression coefficients through ordinary least squares (ols), with the objective function as the minimum of the residual sum of the squares (rss) equation (eq. 2). the values minimizing the function are the estimated regression coefficients (β)."
"the sub-barycenter features of radar chart are extracted as the following. each triangle (see figure 4) in the pentagon has a barycenter (, ) for simplification and dimensionality reduction, we only choose ij l as the graphical features that can be obtained by"
"after the deletion, the dataset still have 87,832 records of class normal and 54,572 records of class dos. in order to reduce the experimental time, we randomly select 10% of class normal and dos as the final experimental normal and dos records, while the other three classes remain unchanged. as a result, the size of the final experimental dataset is reduced from 145,585 to 17,421.the final size of each class is shown in table 1 ."
"the rest of this paper is organized as follow. some related work on idss, including the use of feature reduction and representation methods, is reviewed in section 2. section 3 presents a detailed description of the proposed framework, including each step of the proposed approach. the dataset, evaluation strategies, and results of a performance comparison are presented in section 4. finally, some conclusions are provided in section 5."
"the data quality module consists of different checks on both features and samples to exclude or data based on the amount of missingness, and to standardize data to make measurements of features in different experiments comparable. missing value imputation and normalisation steps can be implemented as needed by end users. normalization methods can be bypassed if rna microarray datasets are downloaded pre-processed directly from repositories. for these and other microarray datasets, optional filtering steps to reduce dimensionality include many-to-one probe-to-gene mapping. features may be further reduced by using external tools or expert biological insight to exclude features, but such reduction is optional. in all cases, the input for the 'feature selection' segment of the pipeline is a matrix (file or data frame) of features (potential biomarkers) as well as a set of samples along with a target or outcome variable."
"in addition to the validation of our method against pancreatic cancer, a validation was performed against an acute myeloid leukaemia (aml) cohort. this analysis revealed sixty genes contributing significantly to aml (see additional file 3). this gene set interacts with several aml-associated transcription factors, including nkx2-3, hoxa7, and myb. the analysis of genes active in cell lines available in the cancer cell line encyclopedia [cit] revealed that our derived gene set is significantly enriched in multiple haematopoietic and lymphoid tissue lines. additionally, investigation of the presence of our depicted genes in biological pathways, annotated in the kegg database [cit], confirmed their known amlgene associations mediated by the 'hematopoietic stem cell lineage' and 'transcriptional misregulation in cancer' pathways. further results from these analyses are presented in additional file 3."
"while either lasso or en can be used for both classification and regression tasks, our method focused on validation tasks based on binary outcomes (classification). in a planned update of the software accompanying our method, we will enable users to switch between classification and regression tasks. users will also be able to choose between different feature selection algorithms and machine learning models including random forests, artificial neural networks, and deep leaning which can capture alternative patterns of interactions in the data that we might miss out with regularized linear models [cit] . we also plan to implement our code in a portable docker environment to eliminate the need for end users from dealing with version control and software dependencies. lastly, it should be noted that our model currently accepts numerical variables whilst categorical variables should be dummy (one-hot) encoded."
"our pipeline is composed of statistical machine learning modules whose methods are described below. additionally, we applied and validated our pipeline against three independent published datasets; two rna microarray datasets and one lipidomics experiment."
"the left part of the equation is the normal least squares criterion, whereas the right part is the penalized sum of the absolute values of the regression coefficients. in ridge regression [cit], the precursor of lasso, the penalization p is incurred in the l2 norm of the coefficients (sum of the squares). in this case, selection is not sparse since coefficients are never zero but close and so, a rank of features based on the penalised regression coefficients, is produced. elastic net [cit], on the other hand, is a mixed version of both lasso and ridge (eq. 5). it allows for the sparse representation, similarly to lasso, and theoretically improves its performance in p ≫ n cases with high collinear groups of features by allowing grouped selection or de selection of correlated variables. lasso instead tends to select only one \"random\" variable from the group of pairwise correlations. en is created through the merging of both ridge and lasso penalizations (eq. 5). a different representation of the same equation can be seen below (eq. 6), with a single parameter α regulating the relationship between ridge and lasso. when α is equal or closer to 0 we have a stronger penalization and so a solution closer or equal to lasso whereas, when α is equal or closer to 1, the behaviour resembles ridge."
"to completely understand the underlying biological mechanisms driving diverse phenotypes, a multiomics approach is often necessary. however, this is a challenging step due to the data size, measurements, and data analysis involved. [cit], and the linking of somatic mutations, rna expression, dna methylation and ex vivo drug responses [cit] . in addition to -omics datasets, there are other unstructured clinical phenotypic datasets such as medical images, electronic health records, and medical questionnaires. these pose new challenges for data integration and reproducibility that needs standardization and to put into clinical practices [cit] . proposed strategies for integrating these data into our current pipeline include deriving numerical features from these unstructured data, for example by creating vectors of word representations with word2vec models [cit] ."
"a unique strength of our approach lies with the provision of automated pre-processing and feature selection. based on our approach, we were able to reduce the number of potential causative genes in each experiment to under 100 (from an input of over 22,000 genes) whilst the high confidence selections were reduced to less than 15. this robust selection creates a useful feature for end users, eliminating the need to pre-filter data based on perceived biological knowledge thus eliminating bias."
"the 10% kddcup99 dataset has a huge number of redundant records, that almost 70.5% of the records are duplicated, which cause the learning algorithm to be biased towards the most frequent records. for these duplicates, we just delete them. then the size of the dataset changes to 145,585."
the application of single-cell omics to our pipeline may be useful in model organism and basic research to guide future translational projects or prioritize experiments for biomarker validation.
"we applied our pipeline in the published lipidomics data available from three cohorts: the cambridge baby growth study (cbgs1and cbgs2) and the pregnancy outcome prediction study (pops). our objective was to identify potentially nutritional lipid biomarkers for the classification of babies fed with formula, human or a mix of human and formula milk. in fig. 2a, we display the frequency of appearance of the lipids in 100 different elastic net models of classification between formula and human milk nutrition from cbgs2 data. figure 2b shows the same results but for lasso. it can be seen that en allows for a less stringent solution with more features appearing. additional file 2: table 2 reveals the highranking lipids identified by our approach as well as their associated nutritional outcomes. for those selected features, performance evaluation was then performed. results can be seen in fig. 3a where, a combination of the three selected lipids sm(39:1), sm(32.1) and sm(36.2) shows a significant improvement in the models ability to classify between human milk and formula and human mixed milk (from a 0.5 in permuted data to a 0.83 roc auc value) in the cbgs1 data. moreover, as seen in fig. 3b direct visualization of the selected lipids in a box plot, allows for a clear display of the differing prevalence of this feature in babies fed with these different milk nutrition and so explaining its selection and inclusion in the classification model. these plots are easy to interpret and hence reach out to a non-expert domain. moreover, our analysis revealed a consistent biomarker robustness, between hm and fm diets, across three different cohorts, summarised in the additional file 2. for example, sm(39:1) is identified as a robust biomarker for segregating infants on hm vs. fm diets (additional file 2: table 2 )."
"of ram. the workflow is embedded in a r markdown file which, when altered with a user's working and output directories and the name of the input data file, runs the analysis in real time. after running, it compiles a pdf report of containing both all code generated and figures produced. figures generated include analogues to figs. 2 and 3. additionally, roc auc curves are generated via stability analyses for individual selected features as well as combinations shown to be significant in predicting binary outcome. importantly, a list of significant features (genes, metabolites, etc.) is printed in the pdf report. these lists can easily be copied so as to be used as input for pathway and ontology enrichment analyses. for the purposes of our validation studies, pathway analysis and ontology enrichment wereperformed with the enrichr tool with default settings (analysed on sept 7, 2018) [cit] ."
"in order to investigate the performance of the selected markers, our pipeline performs stability analysis through a permutation test. this consists of the randomization of the label features, resulting in incorrect sample labels for predictions and generating models with roc auc values showing a performance subject to the random distribution. both the real model and permutation tests are produced by sampling 1000 random training and test sets, then using simple machine learning models to consider the fit of data, with roc auc performance results plotted as density plots alongside their means and standard deviation. the roc auc offers a graphical overview of the diagnostic ability of binary classifiers with varying thresholds. in addition to this, more information on the predictive ability of the model is obtained through the calculation of the sensitivity, specificity, precision and accuracy values."
"with the popularity of computer network, network security problem becomes more and more important. although many security techniques, such as user authentication, data encryption, and firewalls, have been used to improve the security of networks, there are still many unsolved problems, which has made many researchers focus on building systems called intrusion detection systems (idss) [cit] . generally, idss can be classified into two categories: misuse detection and anomaly detection. in misuse detection, the system identifies intrusions based on known intrusion techniques and triggers alarms by detecting known exploits or attacks based on their attack signatures. misuse detection can discover attacks with a low false positive rate, but it cannot discover novel attacks without known signatures. on the other hand, in anomaly detection, the system discovers attacks by identifying deviations from normal network activities, it can discover novel attacks but with a high false positive rate."
"(2) the measures proposed to increase the energy efficiency of the motor system, together with the possible % improvement, according to the different scenarios. (3) the annual electricity used by the motor system."
"the methodology allows for presenting the costs and benefits of the technical and operational measures that could be implemented in energy intensive systems for the energy efficiency. with respect to literature models, the proposed one makes explicit the influence of maintenance operations and operational optimization."
"manufacturing companies, and process plants within them, can define their strategies and competitive priorities on different key performance indicators for their production systems: flexibility, productivity, quality, but also safety, environmental protection, and energy efficiency."
"a. identification of the most critical system. depends on the energy consumed by the single equipment or system. b. data collection and observation. lifetime observation and data collection about energy consumption, corrective and preventive maintenance activities, and failures occurrence. it includes the maintenance costs estimation, the economic evaluation of maintenance policies, and the estimation of the operating costs of the system. c. energy efficiency model. analysis of the energy efficiency benefits, using an energy efficiency model, as further discussed in section 2. d. efficiency measures proposal. definition of scenarios and efficiency measures. e. determination of the impact of the proposed measures on the performance of the plant, when considering both the equipment under analysis and other pieces of equipment or systems that could be affected by the measures implemented. the scheme in figure 1 can be summarized as follows:"
"to gain the above discussed benefits, most of all for plants where the energy efficiency policy is still under development, it is important to define a prioritization criterion for the multiple energy efficiency measures that could be identified, leading to a maximization of the implemented measures efficacy."
"the analysis is then reiterated for all the energy critical systems of the plant. points d. and e. will be detailed in section 3, while section 4 will demonstrate the whole process through the application to a bituminous material process plant."
"therefore, in this study, measure 6, entailing the use of an inverter, is technically possible, but it is not economic or convenient [cit] . the motor consumption can be instead managed minimizing the peaks during the startup or the stop of the equipment (measure 6a), e.g., starting the motor one time a day, and turning off the motor only at the end of the daily work. this means that the motor could have a single peak of consumption (400 a) per day, for 5 s."
"bitumen, together with polymer and other chemical additives, is fed to the primary mixer and mixed up until the expected quality of the product is obtained."
"cce is the cost of conserved energy for the energy efficiency measure in €/kwh, i is the capital cost in €, q is the capital recovery factor, y −1, m&o is the annual change in maintenance and operation costs in €/y, s is the annual energy saving in kwh/year, d is the discount rate, and n is the lifetime of the conservation measure, in y."
"on the motor efficiency was about 89%, as estimated by plant technicians. figure 5 shows the trend of energy absorption of the motor when the failure occurred:"
"recent advances in information, communication, and computer technologies, as internet of things and rfid (radio-frequency identifications), support prognostic and health management philosophy and make predictive maintenance applications more efficient, applicable, affordable, and consequently more common and available for different industrial domains [cit] ."
"maintenance and operational costs model, as shortly discussed in section 2, was used to estimate total m&o parameter for the motor system. following the failure, a new motor was installed, which was characterized by an efficiency of the 96%, as estimated by plant technicians. energy absorption trend of the new motor is shown in figure 6 . the working consumption was around 150 a."
"the implementation of energy saving measures for the case study was based on both technical and economic feasibilities. on the other hand, for industrial application, some limitations have to be acknowledged: some difficulties in obtaining the historical data required for the evaluation and the presence of multiple factors that could affect the analysis-like productivity-that should be at least qualitatively considered."
"furthermore, based on analysis of historical data, expert inputs and analysis of the economic impacts (balanced cost), the efficiency measures and solutions to achieve higher efficiency of the motor systems were determined."
"when considering the importance of energy saving in industries, this work aimed at developing a model for energy efficiency analysis that is able to support decisions making in industrial process plants."
"the first step should be a literature review to gain an initial set of information, to be enriched with expert knowledge. the procedure could be summarized as follows:"
"cscs were first introduced by meier at lawrence berkeley national laboratory [cit] . later, they were used in various studies to capture energy efficiency potentials in different economic and industrial sectors [cit] . csc was chosen for the purpose of the present analysis framework."
"the maintenance activities carried on in the plant are both preventive and corrective, as summarized in table 2, while table 3 shows the estimated costs of maintenance activities according to equation (3) figure 6 . trend of energy absorption of the new motor."
"a new motor was available in the stock, so the loss of production following the failure was limited to the 10 h that was needed for the substitution. figure 4 shows the electric energy absorption trend of the motor, before failure. it shows regular cycles, each of 1 h, with a maximum consumption of 230 a. the company was already monitoring the energy absorption of the plant, but these data were not employed for decision making in terms of process optimization."
"expert opinion and data obtained from literature were used to choose efficiency measures and to construct a preliminary efficiency recovery for motor system based on maintenance activities, operating procedures, and conditions of the system (technical and economic feasibility)."
"in the end, the mixture passes in the secondary mixer, which operates the final homogenization: the final product is modified bitumen with high quality and performances."
"(1) development of a baseline information from literature and technical data; (2) energy audit and data collection; (3) expert knowledge input; and, (4) definition of base efficiency scenarios and proposal of operational and technical measures of improvement."
"out. in this approach, as illustrated in detail by mckane and hasanbeigi [cit], the measures were initially treated in isolation, and then, with a refined calculation, their mutual relation was considered. the annual electricity saving obtainable by each individual efficiency measure was estimated by using equation (1) . 2010 was used as reference year for the estimation, so in the economic analysis a constant cost of energy of 0.15 (€/kwh) was considered. figure 12 shows the resulting csc as a function of the marginal cce, taking into account the costs that are associated with the implementation of the measures in table 6, that explicitly accounts for the maintenance and operation costs. the technical electricity saving potential is 43,189 kwh/year. figure 12 shows the resulting csc as a function of the marginal cce, taking into account the costs that are associated with the implementation of the measures in table 6, that explicitly accounts for the maintenance and operation costs. the technical electricity saving potential is 43,189 kwh/year."
"in general terms, table 1 should be built for each piece of equipment or system to be considered for optimization, combining observable data with expert opinions to allow for developing energy efficiency supply curves."
"the discount rate can be a relevant parameter for a sensitivity analysis. figure 10 shows e.g., the effect in case the real discount rate is used: a value of d of 1.25 [cit] brings to a q of 1.37, [cit], the d value of 0.75 is maintained (with a consequent value of q of 0.922). [cit] appears higher than for the following years [cit] . discount rate."
"when considering the importance of energy saving in industries, this work aimed at developing a model for energy efficiency analysis that is able to support decisions making in industrial process plants."
"in this paper, a framework is proposed ( figure 1 ) that is able to guide plant technicians in initiating and following up this course, through the exploitation of the full set of information available in the plant."
"data collection and observation. lifetime observation and data collection about energy consumption, corrective and preventive maintenance activities, and failures occurrence. it includes the maintenance costs estimation, the economic evaluation of maintenance policies, and the estimation of the operating costs of the system. c."
"maintenance and operational costs model, as shortly discussed in section 2, was used to estimate total m&o parameter for the motor system. following the failure, a new motor was installed, which was characterized by an efficiency of the 96%, as estimated by plant technicians. energy absorption trend of the new motor is shown in figure 6 . the working consumption was around 150 a."
"fixed costs for maintenance and operation include plant operator's wages, general and equipment maintenance costs, insurance and taxes. the variable part of m&o costs includes expenses that are dependent on the amount of production or on the equipment operation time. [cit], 6, 25 4 of 15 strictly depend on the maintenance policy of the process plant. unavailability costs are related to the failure of the equipment and on the lack of production following the plant stop."
"however, thanks to the cooperation with the company, the analysis and the results were validated by the experts of the system. these results were compared with the results obtained using traditional method of energy balance sheet, and similar outcomes were confirmed [cit] ."
"bitumen, together with polymer and other chemical additives, is fed to the primary mixer and mixed up until the expected quality of the product is obtained."
"the plant object of the study is a process to produce modified bitumen, as in figure 2 . it produces 20-21 t/h of modified bitumen. the plant is made of two mixers, the primary and the secondary one. the operation of the mixers is switched every year."
"furthermore, based on analysis of historical data, expert inputs and analysis of the economic impacts (balanced cost), the efficiency measures and solutions to achieve higher efficiency of the motor systems were determined."
"according to equation (1), [cit], as shown in figure 9, assuming: an average annual energy consumption of 300,000 kwh over the three years, the same average energy price, 0.15 €/kwh, and the same capital recovery"
"in the end, the mixture passes in the secondary mixer, which operates the final homogenization: the final product is modified bitumen with high quality and performances."
"as shown in figure 13, results demonstrated that even only through maintenance optimization-as upgrading the maintenance of the equipment and initiating a predictive maintenance program-it is possible to increase the performance of the system by up to 10%, for a medium base case scenario."
"the procedure for the identification of the efficiency measures and their optimization on the basis of cce is here described. with respect to literature methodologies, our research focused the attention not only on the technical measures to increase energy efficiency, but also on the effect of maintenance and operational management of the plant itself. thus, it has to be considered that, depending on the initial level of maintenance policy and operational management, the energy recovery capabilities will change, decreasing at the increasing of the maintenance and operational management capacity of the company considered."
"determination of the impact of the proposed measures on the performance of the plant, when considering both the equipment under analysis and other pieces of equipment or systems that could be affected by the measures implemented."
"however, thanks to the cooperation with the company, the analysis and the results were validated by the experts of the system. these results were compared with the results obtained using traditional method of energy balance sheet, and similar outcomes were confirmed [cit] ."
"the methodology allows for presenting the costs and benefits of the technical and operational measures that could be implemented in energy intensive systems for the energy efficiency. with respect to literature models, the proposed one makes explicit the influence of maintenance operations and operational optimization."
"three classes of costs compose the m&o factor: (i) fixed operational costs; (ii) variable operational costs; and, (iii) unavailability costs [cit], as shown in equation (3):"
"three different base case scenarios-low, medium, and high-were hypothesized for energy efficiency, each of them with a decreasing potential energy recovery related to the optimization of maintenance activities and operating procedures, as in table 1 . the levels of efficiency base case scenarios, with their relative potential of electricity power recovery based on maintenance activities, are defined through expert opinion and data obtained from the literature."
"with reference to the framework detailed in section 1, the most critical system was identified: a three-phases electric motor was the main energy consumer in the analyzed process. the motor drags the mill for the polymer homogenization to a rotation speed of about 1800 rpm. typical motor energy absorption is shown in figure 3 . a peak of energy absorption of about 400 a is present at the startup of the motor, for around 5 s. in case the motor works unloaded, it absorbs little electricity power, while when it works under load it shows an almost constant absorption, which depends on the motor efficiency, the viscosity of the product, and other parameters."
"(2) the measures proposed to increase the energy efficiency of the motor system, together with the possible % improvement, according to the different scenarios. (3) the annual electricity used by the motor system. according to equation (1), [cit], as shown in figure 9, assuming: an average annual energy consumption of 300,000 kwh over the three years, the same average energy price, 0.15 €/kwh, and the same capital recovery factor, 0.922, in each year. the interest rate assumed in our simulations was 3% and the discount rate d was 0.75% per year, in order to reflect the barriers to energy efficiency investment-as perceived risk, lack of information, management concerns about production, and capital constraints, within others."
"the horizontal line in figure 12 represents the energy cost line. the efficiency measures that fall below the energy cost line are both technically feasible and economically feasible, while those falling above the line are technically feasible, but not cost effective."
"in the end, it has to be notices that the proposed methodology constitutes a good screening tool figure 13 . maintenance optimization impact (case study)."
"the implementation of energy saving measures for the case study was based on both technical and economic feasibilities. on the other hand, for industrial application, some limitations have to be acknowledged: some difficulties in obtaining the historical data required for the evaluation and the presence of multiple factors that could affect the analysis-like productivity-that should be at least qualitatively considered."
"expert opinion and data obtained from literature were used to choose efficiency measures and to construct a preliminary efficiency recovery for motor system based on maintenance activities, operating procedures, and conditions of the system (technical and economic feasibility)."
"after calculating the cce for all of the energy efficiency measures proposed, the measures are ranked in ascending order of cce, against an energy price line. all measures that fall below the energy price line are cost-effective. on the curves, the width of each measure (plotted on the x-axis) represents the annual energy saved by that measure. the height (plotted on the y-axis) shows the measure's cost of conserved energy."
"in the end, it has to be notices that the proposed methodology constitutes a good screening tool to present energy efficiency measures and to capture the potentials for improvement and a base to develop the tools for prognostic and health management systems in process industries."
"the maintenance activities carried on in the plant are both preventive and corrective, as summarized in table 2, while table 3 shows the estimated costs of maintenance activities according to equation (3) and company data. preventive maintenance costs showed a slight increase during the years, due to the cost of work increase. when the new motor was activated, its preventive maintenance for the first year was very limited (in actions and costs), and then it increased in the second year of activity. from the second year on, the progress of the costs for preventive maintenance actions became only dependent on the cost of the work."
"therefore, in this study, measure 6, entailing the use of an inverter, is technically possible, but it is not economic or convenient [cit] . the motor consumption can be instead managed minimizing the peaks during the startup or the stop of the equipment (measure 6a), e.g., starting the motor one time a day, and turning off the motor only at the end of the daily work. this means that the motor could have a single peak of consumption (400 a) per day, for 5 s."
"a simpler approach is the csc (conservation supply curve), which assesses the cost effectiveness and the technical chances for energy efficiency. the energy conservation potential is shown as a function of the marginal cce (cost of conserved energy), and provides a clear visualization of energy efficiency."
"current analysis of energy consumption in process plants is mostly performed through process simulations, allowing for considering the effect of some factors affecting energy consumption, as operating conditions, raw materials, fuel, and yield [cit] . these methodologies require a huge amount of data and are recognized to be inefficient in speculating factors that affect energy consumption."
"the motor efficiency was about 89%, as estimated by plant technicians. figure 5 shows the trend of energy absorption of the motor when the failure occurred: following the failure, a new motor was installed, which was characterized by an efficiency of the 96%, as estimated by plant technicians. energy absorption trend of the new motor is shown in figure 6 . the working consumption was around 150 a."
"typical motor energy absorption is shown in figure 3 . a peak of energy absorption of about 400 it produces 20-21 t/h of modified bitumen. the plant is made of two mixers, the primary and the secondary one. the operation of the mixers is switched every year."
"the same authors thus discussed different index analysis methods to analyze energy efficiency-as data envelopment analysis (dea), which gained great popularity in energy literature because it is a non-parametric approach dealing with multiple inputs and outputs that allow assessing relative energy efficiency [cit] . however, they were judge unable to produce a complete picture of the energy efficiency measures impact in terms of energy consumption, energy saving potential, and economical gain."
"thus, in this research, the recorded data were used to analyze the energy footprint and to define possible efficiency measures to increase the performances of the system. this knowledge was integrated with the analysis of the maintenance activities and failures occurrence related to the observed motor system."
"in this paper, a framework is proposed ( figure 1 ) that is able to guide plant technicians in initiating and following up this course, through the exploitation of the full set of information available in the plant. the scheme in figure 1 can be summarized as follows:"
"since the system under analysis fell within the medium potential for energy recovery, because the plant already implemented preventive maintenance measures, the cost of conserved energy cce for the medium base case scenario was calculated accordingly (10%). table 5 . efficiency improvement % over each base case scenario for the electric motor system (case study). figure 11 shows the procedure according to which the calculation and data analysis was carried out. in this approach, as illustrated in detail by mckane and hasanbeigi [cit], the measures were initially treated in isolation, and then, with a refined calculation, their mutual relation was considered. the annual electricity saving obtainable by each individual efficiency measure was estimated by using equation (1) . 2010 was used as reference year for the estimation, so in the economic analysis a constant cost of energy of 0.15 (€/kwh) was considered."
"thus, in this research, the recorded data were used to analyze the energy footprint and to define possible efficiency measures to increase the performances of the system. this knowledge was integrated with the analysis of the maintenance activities and failures occurrence related to the observed motor system."
"with reference to the framework detailed in section 1, the most critical system was identified: a three-phases electric motor was the main energy consumer in the analyzed process. the motor drags the mill for the polymer homogenization to a rotation speed of about 1800 rpm."
"the results demonstrated, that even only through maintenance optimization, the recovery could account for the 10% of the total for a medium base case scenarios electric motor system."
"a new motor was available in the stock, so the loss of production following the failure was limited to the 10 h that was needed for the substitution. figure 4 shows the electric energy absorption trend of the motor, before failure. it shows regular cycles, each of 1 h, with a maximum consumption of 230 a."
"the results demonstrated, that even only through maintenance optimization, the recovery could account for the 10% of the total for a medium base case scenarios electric motor system."
