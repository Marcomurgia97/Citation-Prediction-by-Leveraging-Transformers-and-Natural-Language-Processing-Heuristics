text
"in contrast, the bilateral module of lower leg muscles revealed strong coupling at multiple frequency bands, consistent with previous analyses on functional muscle networks (20), and showed the strongest long-range connections observed in the present study (fig. 3c) . bilateral connectivity between arm and leg muscles during balancing could be generated by the vestibulospinal tract, which is known to be involved in postural stability and to innervate the spinal gray matter bilaterally (21) . bilateral connectivity has been observed at all levels of the corticospinal axis (38) and is paramount for functional brain networks, particularly between homologous left-right cortical regions (39) . the present findings suggest that bilateral coupling is also a defining feature of functional muscle networks. the differences in functional connectivity between bilateral arm and bilateral leg muscles indicate that the functional muscle network, like the anatomical muscle network (25), does not show serial homology."
"these functional networks were converted to binary networks to facilitate comparison with the anatomical network. weights were thresholded to obtain a minimally connected network across conditions and frequency components. this thresholding procedure yields a single, unique threshold value, which corresponds to the percolation threshold (50) . this resulted in sparse networks in which each node was connected to at least one other node by an edge at one of the layers of the multiplex network."
"functional muscle networks were defined by mapping correlated inputs to different muscles. to map functional networks, we measured surface emg from the same 36 muscles while healthy participants performed different postural tasks. a full-factorial design was used in which we varied postural control (normal standing and instability in the anteriorposterior or medial-lateral direction) and pointing behavior (no pointing and pointing with the dominant hand or with both hands; see materials and methods for details). we used these tasks to experimentally manipulate the required coordination between muscles and to induce changes in the functional muscle network. we assessed functional connectivity by means of intermuscular coherence between all muscle combinations and used nonnegative matrix factorization (nnmf) to decompose these coherence spectra into frequency components and corresponding edge weights. this yielded a set of weighted networks with their corresponding spectral fingerprints (frequency components)."
"bipolar surface emg was recorded from 36 muscles distributed across the body (18 bilateral muscles; table 1 ). we selected a representative group of antagonistic muscle pairs involved in postural control that can be properly measured with surface emg due to their location and size. emg was acquired using three 16-channel porti systems (tmsi), online high-pass-filtered at 5 hz, and sampled at 2 khz."
"from a systems biology perspective, the brain and spinal cord are interwoven with the body-they are \"embodied\" (7)-and brain network analysis can thus be extended to investigate the intrinsic organization of functional networks in the human spinal cord (47) . functional interactions between supraspinal, spinal, and peripheral regions can be integrated using network analysis as a common framework. such an integrated framework is well placed to provide new insights and interventions for neurological disorders (48) ."
"this approach fits within the broader framework of network physiology, investigating brain-body interactions (45) . similar to the current results, research on network physiology has shown that dynamic interactions among organ systems are mediated through specific frequency bands (46) . we extended this approach by investigating the network topology of functional interactions between muscles, which are mediated through neural pathways within the spinal cord. future studies may extend the number of muscles that are investigated, include electroencephalography to map brain-body networks and investigate the cortical control of muscle networks, and consider individual differences in anatomy."
"the traditional idea that the cortex controls muscles in a one-to-one fashion has been challenged by several lines of evidence (4) . for example, it is widely recognized that the many degrees of freedom (dofs) of the musculoskeletal system prohibit a simple one-to-one correspondence between a motor task and a particular motor solution; rather, muscles are coupled and controlled in conjunction (5) . a coupling between muscles-whether mechanical or neural-reduces the number of effective dofs and hence the number of potential movement patterns. this coupling thereby reduces the complexity of motor control (6) ."
"we used a network approach to study the structure-function relationship of the human musculoskeletal system. several principles governing the functional relationship between muscles were revealed: (i) functional connectivity patterns between muscles are strongly shaped by the anatomical constraints of the musculoskeletal system, with functional connectivity strongest within anatomical modules and decreased as a function of anatomical distance; (ii) bilateral connectivity between the homologous upper and between the homologous lower extremities is a key characteristic of the functional muscle networks; (iii) the functional relationships are task-dependent, with postural tasks differentially affecting functional connectivity at different frequency ranges. the use of a multiplex approach allows the integration of functional muscle networks across frequencies and provides a unifying window into the distributed circuitry of the human central nervous system that controls movements by innervating the spinal motor neurons."
"the human body is a complex system consisting of many subsystems and regulatory pathways. the musculoskeletal system gives the body structure and creates the ability to move. it is made up of more than 200 skeletal bones, connective tissue, and over 300 skeletal muscles. muscles are attached to bones through tendinous tissue and can generate movement around a joint when they contract. the central nervous system controls these movements through the spinal motor neurons, which serve as the final common pathway to the muscles (1) . while the anatomical and physiological components of the musculoskeletal system are well characterized (2, 3), the organizational principles of neural control remain poorly understood. here, we elucidate the interplay between the anatomical structure of the musculoskeletal system and the functional organization of distributed neural circuitry from which motor behaviors emerge."
"comparison of anatomical distance (path length) and functional connectivity revealed that anatomically nearby nodes are more likely to receive common input (fig. 3) . we first examined the percentage of all possible edges, that is, the number of edges above threshold, which decreased as a function of anatomical distance: 11.3, 0.9, 0.6, and 0.0% for anatomical distances 1, 2, 3, and 4, respectively. this decline with distance was even more pronounced for the higher frequency components (fig. 3b) . next, we examined the distribution of functional weights as a function of anatomical distance. the highest weights were observed for edges connecting muscles within the same module. the edges within most modules had an anatomical distance of 1. only a few edges had an anatomical distance of 2 or 3, and all these edges were contained in the forearm and lower leg modules. in particular, edges connecting bilateral lower leg muscles showed relatively large weights at an anatomical distance of 3 ( fig. 3c )."
"anatomical and functional connectivity between muscles may also be influenced by external factors. for example, connectivity patterns of descending pathways are, in part, genetically determined (31) . a somatotopic organization is observed across the neural motor system, and the community structure of the anatomical muscle network mirrors the organization of primary motor cortex control modules (19) . likewise, the spatial organization of motor neurons of the spinal cord is also related to the anatomical organization of muscles (32), and muscles that are anatomically closely located to each other are also innervated by the same spinal nerves ( fig. s2) (2) . the topographic organization of spinal motor neurons is similar across species (33) and may hence be a result of evolutionary conservation (34) . musculoskeletal anatomy and neuronal pathways are hence both subject to some sort of genetic control."
supplementary material for this article is available at http://advances.sciencemag.org/cgi/ content/full/4/6/eaat0497/dc1 section s1. alternative anatomical muscle network section s2. spinal nerve network section s3. weighted functional networks fig. s1 . the adjacency matrix of anatomical muscle networks. fig. s2 . community structure of the spinal nerve network. fig. s3 . community structure of weighted functional network. table s1. spinal nerve innervation.
"identifying relationships between anatomical and functional muscle networks is crucial for understanding how movement is coordinated. previous studies investigated either how biomechanical properties of the musculoskeletal system constrain the movement patterns that can be generated (8, 9) or how muscle activation patterns can be explained by a combination of only a few coherent muscle activation patterns (11) . our combined analyses of anatomical and functional muscle networks reveal a strong relationship between anatomical connections in the musculoskeletal systems and correlated inputs to spinal motor neurons. this finding builds on previous research showing that common input is strongest to spinal motor neurons that innervate muscle pairs that are anatomically and functionally closely related (10, 21) . the similarity between structural and functional networks has been a signature of the study of brain networks (26), and the topology of brain networks depends on the brain's spatial embedding (27) . the present findings suggest that the principles governing embodied structural and functional networks also apply to the neural circuitry that controls movements and may hence reflect a general principle of the central nervous system."
"we next sought to study the influence of task on this structure-function relationship. this was achieved by using clustered graphs to compare functional muscle networks across task conditions. the functional modules identified using the preceding multiplex modularity analysis form the nodes of these clustered graphs. figure 4a shows the clustered graphs in the nine experimental conditions and for the four frequency components. the clustered graphs were very sparse, as modules have dense within-module connections but sparse connections between nodes in other modules. most edges were observed between leg muscles modules (lower leg, right upper leg, and left upper leg) at the lowest frequency components (0 to 3 and 3 to 11 hz), consistent with the lower modularity scores, in particular when postural stability was challenged by instability in the anterior-posterior or medial-lateral direction. edges between the arm muscle modules (right upper arm and forearm) and the torso were mainly observed at the higher frequency components (11 to 21 and 21 to 60 hz) during pointing (unimanual and bimanual)."
"to facilitate the comparison of functional networks across task conditions, we coarse-grained the networks (52) . we used the set of functional modules estimated across conditions and frequency components as a frame of reference to coarse-grain the 36 binary networks and then compared the strength of the inter-and intramodule connections across networks using these module boundaries. in the clustered networks, the nodes represent the modules (groups of muscles, identified above) and the edges represent the connections between modules. the nondiagonal elements of the resulting weighted adjacency matrix represent the average edge weights between two modules, and the diagonal elements represent the average edge weights within a module."
we assessed the relationship between anatomical and functional connectivity of key muscles involved in postural control tasks (36 muscles distributed throughout the body). we investigated a muscle-centric network in which the nodes represent the muscles and the edges of the network are anatomical connections or functional relations between muscles.
"anatomical muscle networks were defined by mapping the physical connections between muscles (19, 25), based on gross human anatomy (2) . the anatomical network constituted a densely connected, symmetrical network (network density, 0.27; fig. 1 ). modularity analysis revealed five modules that divided the anatomical muscle network into the main body parts (right arm, left arm, torso, right leg, and left leg) with a modularity of 0.39."
"there is continuing debate about the nature of the coupling between muscles. the mechanical coupling in the musculoskeletal system constrains the movement patterns that can be generated (7, 8) . for example, the biomechanics of the limb constrain relative changes in musculotendon length to a low dimensional subspace, resulting in correlated afferent inputs to spinal motor neurons (9) . the coupling between muscles could also result from redundancies in the neural circuitry that drives spinal motor neurons (10) . electrophysiological studies reveal that a combination of only a few coherent muscle activation patterns-or muscle synergies-can generate a wide variety of natural movements (11) . some of these patterns are already present from birth and do not change during development, whereas other patterns are learned (12) . this arrangement supports the notion that the neuromuscular system has a modular organization that simplifies the control problem (13) . spinal circuitry consists of a network of premotor interneurons and motor neurons that may generate basic movement patterns by mediating the synergistic drive to multiple muscles (14) . these spinal networks may encode coordinated motor output programs (15), which can be used to translate descending commands for multijoint movements into the appropriate coordinated muscle synergies that underpin those movements (3) ."
"network theory can provide an alternative perspective on the modular organization of the musculoskeletal system. community or modular structures, which refer to densely connected groups of nodes with only sparse connections between these groups, are one of the most relevant features of complex networks (16) . the investigation of community structures has been widely used in different domains such as brain networks (17) . this approach has recently been applied to investigation of the structure and function of the musculoskeletal system: the anatomical network can be constructed by mapping the origin and insertion of muscles (18, 19) . we have previously shown how functional muscle networks can be constructed by assessing intermuscular coherence from surface electromyography (emg) recorded from different muscles (20) . these functional networks reveal functional connectivity between groups of muscles at multiple frequency bands. coherence between emgs indicates correlated or common inputs to spinal motor neurons that are generated by shared structural connections or synchronization within the motor system (10, 21, 22) ."
"task-dependent changes occurred at different frequencies, which indicate the functioning of a multiplex network organization, whereby the four frequency components reflect different types of interactions between muscles. four distinct frequency components (0 to 3, 3 to 11, 11 to 21, and 21 to 60 hz) were extracted using nnmf. these frequency bands closely match those found previously (20), demonstrating the robustness of this finding. an interesting possibility is that these frequency components reflect the spectral fingerprints of different pathways that project onto the spinal motor neurons. it has been suggested that these different frequencies may have specific roles in coding motor signals (43) . functional connectivity at the lowest frequency components may result from afferent pathways, while functional connectivity (10, 36) . the highest frequency components observed in this study (21 to 60 hz) showed the most local connectivity patterns. these local connectivity patterns may reflect propriospinal pathways (3, 15) . these functional connectivity patterns may be used to uncover the contribution of structural pathways in the formation of coordinated activity patterns in the motor system (23). these findings mirror observations in cortical networks where frequency-specific networks reveal different topologies and are differentially expressed across brain states (44) . the differences in the frequency content of functional connectivity observed between the upper limb and lower limb muscles suggest distinct neural circuitry controlling these body parts. in summary, our network analysis revealed widespread functional connectivity between muscles, indicative of correlated inputs to spinal motor neurons at multiple frequencies. correlated inputs indicate divergent projections or lateral connections in the neural pathways that innervate spinal motor neurons and can hence be used to assess spinal networks (23) . these findings are consistent with a many-tomany rather than a one-to-one mapping between brain and muscle (4), in which complex movements arise through relatively subtle changes in the coactivation of different distributed functional modes. we present a novel approach that aligns movement neuroscience with current research on brain networks by showing how the central nervous system interacts with the musculoskeletal system of the human body."
"here, we investigate the organizational principles governing human motor control by comparing the community structure of anatomical and functional networks. we use multiplex modularity analysis (24) to assess the community structure of functional muscle networks across frequencies and postural tasks. as biomechanical properties of the musculoskeletal system constrain the movement patterns that can be generated, we expect a similar community structure for anatomical and functional muscle networks. deviations in community structure indicate additional constraints imposed by the central nervous system. we also compare functional connectivity between modules during different tasks to investigate changes in functional organization during behavior. while the average functional connectivity is constrained by anatomical constraints, we expect that functional muscle networks reconfigure to enable task-dependent coordination patterns between muscles. these task modulations would indicate that functional interactions between muscles are not hard-wired but are instead governed by dynamic connectivity in the central nervous system that is shaped by the anatomical topology of the musculoskeletal system."
"at the lower frequency components, as muscles act as a low-pass filter of neuronal inputs and kinematics of the musculoskeletal system unfold on a slow time scale. this generates correlated activity at low frequencies that are fed back to spinal motor neurons via sensory afferents. the spatial distribution of common input would arguably mirror the topology of the musculoskeletal system. anatomical constraints may also be imposed during neural development. during early development, changes in the topographical distribution of axon terminals of descending projects are dependent on patterns of motor activity and anatomical connectivity between muscles (29). likewise, large changes in functional coupling are observed in infants between 9 and 25 weeks, which reflects a sensitive period where functional connections between corticospinal tract fibers and spinal motor neurons undergo activity-dependent reorganization (30) . the anatomy of the musculoskeletal system will limit the motor activity patterns that can be performed."
"functional connectivity was not entirely determined by anatomy; we observed several key differences between anatomical and functional muscle networks. bilateral modules consisting of muscles in the upper or lower extremities were a key characteristic of the functional muscle network that were absent in the anatomical network. the two bilateral forearm muscles (flexor digitorum superficialis and extensor digitorum) showed coherent activity at 3 to 11 hz, consistent with previous studies showing bimanual coupling at ~10 hz between homologous hand and forearm muscles (35, 36) . the observed bimanual coupling at 3 to 11 hz may be generated by the olivocerebellar system, which is known to produce oscillations in this frequency range and for its involvement in the formation of functional muscle collectives (35) . the bilateral forearm muscles were only weakly coupled to other muscles (fig. 2), which may reflect the relatively high proportion of direct corticospinal projections-and thus a relative low proportion of diverging projections-to motor neurons innervating hand and forearm muscles (37) ."
"functional connectivity displayed distinct task-dependent modulations that were linked to the task the subjects performed: functional connectivity was increased within and between the leg modules during postural instability and increased within and between arm and upper body modules in the pointing conditions. functional connectivity between muscles is thus task-dependent (21, 36), which may suggest the presence of multifunctional circuits in which a given anatomical connectivity pattern can generate different functional activity patterns under various conditions (40) . such a distributed circuitry creates the substrate to support many behaviors that are driven by the concerted actions of a large distributed network as opposed to simple, dedicated pathways. the underlying network connectivity hence constrains the possible patterns of population activity to a low-dimensional manifold spanned by a few independent patterns-neural modes-that provide the basic building blocks of neural dynamics and motor control (41) . again, this finds similarities with recent investigations of the functional principles of cognitive networks in the brain (42) ."
"a fixed power contribution is also considered when traffic is absent. hence we assume the following expression of the power consumption p: (11) wherein p max is the maximum server power consumption, a is the ratio of the baseline power to the maximum power and a p is the average number of packets handled by the switch. the energy consumption e h con is composed by two components: i) the static one e h con,f represented by the fixed energy consumption related of only server turned on in the states"
"finally the outputs of the algorithm are: the list l of the mappings and the minimized total energy u n,1 of the migration policy."
"when only one mapping is admissible for the traffic state t 0, a simplified solution can be provided for the meavr heuristic. it is based on the application of the viterbi algorithm [cit] . a fictitious stage is introduced in the multistage graph of"
"when traffic reduction occurs, the vnf instance can be dimensioned with a number of cores lower than the one evaluated during the peak hour interval and consolidation techniques can be applied by migrating vnfis so as to occupy fewer servers and to save power consumption. the migration can be performed when the vnfis are supported by virtual machines (vm) but at the price of increasing the energy consumption when the memory associated to the vms are moved. this migration energy may impact the total energy consumption, thus we need vnfi migration policies that allows to minimize it and, consequently, to find a right compromise between energy saving due to consolidation and the energy consumed for the migration."
"we show in fig. 1 an example of sfc routing and resource dimensioning. a three levels datacenter network [cit] composed by eight servers, eight switches and two routers is considered. two sfcs are considered in fig. 1 .a. the first one shows that packets need to be processed through two functions: firewall and vpn encryption, in that order. the second one involves the execution of a firewall function only."
are not admissible in general because they are evaluated in lower traffic conditions and involve the switching off of some servers; this switching off does not allow the support of the peak traffic state without overcoming the server processing capacities and link bandwidth.
"finally we conclude by discussing about the complexity of the meavr heuristic. because the determination of a least cost cyclic path can be reduced to the one of a shortest path, we can affirm that the complexity is the one of the dijkstra algorithm that in the worst case is applied in a graph with n 2 nodes and n 3 links. it follows that if the candidate list in the dijkstra algorithm is implemented with a binary heap [cit], the computational complexity of the meavr heuristic is o (n 3 logn ) . this complexity is much lower than the one involved when the migration policy is achieved by applying the markov decision processes [cit] ."
"to support the two sfcs, two vnfi are instantiated by activating the vms vm fw 1 and vm ev 8 in the servers s 1 and s 8 respectively. we illustrate in fig. 1 .b the routing of the two sfcs considered. the first one uses the memory and processing resources of vm fw 1 and vm ev 8 for the running of the fw and ev vnfs respectively. the second one uses the resources of vm fw 1 only. the memory amount and the number of cores allocated to the vms are also shown in fig. 1 .b. in particular two and three cores have been allocated for vm fw 1 and vm ev 8 while we assume that the number of cores available in each server is four."
the aim of this paper is to propose migration policies that establish when and where cold migrations of vnfis have to be accomplished so as to minimize the total energy characterized by the sum of the consolidation and migration energies occurring when the vnfis are moved from the initial location. we have given the ilp formulation of the optimal problem and proposed the simple meavr heuristic based on the viterbi algorithm. we have shown how the meavr heuristic is characterized by total energy values near to the optimization results. finally by applying the meavr heuristic we have verified how the proposed migration policy allows for percentage energy saving that can reach the 40% with respect to the solution in which migrations are not performed.
"this paper deals with migration policies for the case of cold migration [cit] in which the virtual machines are suspended before performing migrations. this type of migration can be easier to perform, even if it can be quite disruptive for on going traffic. its application is implemented in cloud environments with redundancy support in which each virtual machine is replicated. the main concern is, therefore, not qos degradation during migration but rather the energy consumed during the migration for transferring the memory associated with migrated virtual machines. the cold migration mechanism is already widely implemented in systems for the management of virtual machines. for instance vmware [cit] allows for the cold migration of virtual machines."
"this work is an extension of the paper [cit], in which we defined and investigated two policies whose aim is the minimization of the total energy. the first one, referred to as global policy and based on the application of markov decision process (mdp), allows for the minimization of the total energy in cycle-stationary traffic scenarios and is applicable to the case in which the entire traffic profile is known. the second one, referred to as local policy, is applicable in traffic scenarios in which the knowledge of the actual traffic only is assumed. the main differences between the two papers are twofold:"
"in the formulation of the optimal evimp, the objective function to be minimized is the total energy consumption e tot in the cycle-stationary interval given by the following expression:"
"in this paper we investigate policies in the case of cold migrations where the virtual machine is suspended before performing the migration and high migration times are involved; in this scenario qos degradation is not the main concern because this type of migration is only applied in case in which the virtual machines are redundant and the switching off of one of them does not impact on the quality of service. conversely the main concern in this scenario is the energy consumption due to transferring the bit of the memory allocated to the migrating virtual machines. this energy consumption characterizes the reconfiguration costs to be taken into account when migration policies, minimizing the total energy cost, have to be identified."
"in order to save energy, server consolidation procedures can be activated in low traffic periods when the needed bandwidths and processing resources decreases. as shown in fig. 1 .c, this decrease may lead to the need to allocate only one and two vcores to vm fw 1 and vm ev 8 respectively. we illustrate in fig. 1 .d how the migration of the vm fw 1 in server s 8 leads to the advantage of switching off the server s 1 ."
"compared with the other three methods, another advantage of the method is that our method can distinguish between surgical instruments with the same appearance in an image frame. the instruments in two datasets are of the same appearance and the compared method takes them as one class, so they cannot differentiate these instruments. as for our method, the output of the heatmap network is heatmaps which are actually confidence maps, one for each surgical instrument. fig. 5 shows the rgb input images of which red channel is replaced by the predicted heatmap of each instrument. based on this, our method can track each instrument although they are of the same appearance in an image frame."
"the goal of this regression network is to predict a precise region box for each instrument. in contrast to bounding-box regression from the faster rcnn, we train our network using a multiple l 1 loss function defined as follows:"
"where p m ij andp m ij represent the ground-truth value and corresponding sigmoid output at a pixel location x (i, j) in the mth heatmap of size s."
"to evaluate the performance of the proposed method, we apply the method to two multi-instrument datasets, namely the atlas dione dataset and the endovis challenge dataset. we compare the method with three other mainstream detection methods in terms of detection accuracy and speed."
"1. introduction: robot-assisted minimally invasive surgery (rmis) systems, like the davinci surgical system (dvss), have gained more and more attention in recent years. rather than cutting patients open, rmis allows surgeons to operate by telemanipulation of dexterous robotic tools through small incisions, which results in less pain and fast recovery time. with rmis systems, surgeons sit at a console near the operating table and utilise joysticks to perform complex procedures. such systems will translate surgeons' hand movements into small movements of the surgical instruments in real time. the location of surgical instruments is a common requirement to provide surgeons with important information for observing tool trajectory and can lighten their burden of finding the instruments during an operation. on the other hand, having real-time knowledge of the motions of the surgical tools can help in the modelling of gestures and skills for the real-time automated surgical video analysis [cit], which is good for training the novice surgeons [cit] . hence, in this study, we focus on real-time instrument detection."
"in this letter, we presented a novel frame-by-frame detection method for real-time multi-instrument detection and location using a cascading cnn which consists of an hourglass network and a modified vgg-16 network. the hourglass network is applied to detect a heatmap of each instrument, and the modified vgg is responsible for bounding-box regression. to train our model, we use a two-step training strategy: firstly, we train the hourglass network using a pixel-wise sigmoid cross-entropy loss function, and then keep this fixed and train the whole framework using a multiple l 1 loss function. the proposed detection model is validated on two datasets: the atlas dione dataset and the endovis challenge dataset. the experimental results for these two datasets show that our method achieves a better tradeoff between detection accuracy and speed than the other considered state-of-the-art methods. moreover, our method can distinguish between instruments of the same appearance while other methods cannot. we think that we can further improve the detection accuracy by replacing vgg-16 with a deeper cnn, but this will reduce the speed correspondingly."
"2. methodology 2.1. heatmap network: the overall design of our cnn-based surgical instrument detection model is shown in fig. 1 . this section will describe the architecture of each sub-network of our detection-regression network in more detail. in the proposed framework, the heatmap-regression network (section 2.1) takes the rgb image frame as input and outputs heatmaps which are confidence maps of the tool tip areas. these heatmaps guide the bounding-box regression network (section 2.2) to focus on the location of instruments in the input image. finally, it outputs four real-valued numbers for each tool which encodes the bounding-box position in the image coordinate system."
"we approach a rough instrument location as a binaryclassification problem in each heatmap. the ground truth for our regression network is encoded as a set of m binary maps, one for each surgical instrument. as shown in fig. 2, in each ground-truth heatmap, we set the values within a certain radius around the centre of an object bounding box to 1 as the foreground, and the remaining values are set to 0 as the background. the bigger the object bounding box is, the larger the radius is in the ground-truth heatmap. as we treat the heatmap regression as a multiple binary-classification problem, we train the hourglass network using a pixel-wise sigmoid cross-entropy loss function which is defined as follows:"
"in this letter, we propose a novel frame-by-frame real-time detection and location method for multi-instruments, which consists of an hourglass network [cit] and a modified vgg-16 [cit] network. the former heatmap network is used as a fully convolutional regression network to output the heatmaps which represent the location of the instruments tip area. the latter performs bounding-box regression on these heatmaps stacked with input rgb image frames. in this way, we can simultaneously predict the tools' location and perform recognition. our method is more like human behaviour. humans glance at an image and instantly know what objects are in the image and their approximate location (heatmap network), and then locate them precisely in the image (boundingbox network). to evaluate the performance of the proposed method, we evaluated our method on the publicly available multiinstrument atlas dione [cit] and endovis challenge [cit] datasets. our approach obtains better performance than three state-of-art detection methods in terms of detection accuracy and speed."
"where t m (t ) represent the corresponding predicted object bounding box and ground-truth bounding box, respectively, in the image coordinate system, and t (t x, t y, t w, t h, ) is defined as"
"a multi-scale representation is completely specified by the transformation from a finer scale to a coarser scale. in linear scale-spaces the operator for changing scale is a convolution by a gaussian kernel. after the convolution with gaussian kernel the images are uniformly blurred, also the regions of particular interest like the edges [cit] . this is a drawback as the edges often correspond to the physical boundaries of objects. the edge and contour information may be the most important of an image's structure for human to capture the scene. to overcome this issue, non-linear multi-resolution signal decomposition schemes based on morphological operators have been proposed to maintain edges through scales [cit] ."
"the momentum rate-of-change equals the summation of all the external wrenches acting on the robot. in a multi-contact scenario, the external wrenches reduce to the contact forces plus the mass force:"
after adding this two dataset types the resulting 3d force space and 3d torque space with all types of datasets are depicted in figure 3 . studying the results of these new types of datasets allows to understand how the calibration results are affected by the excitation of the sensor represented in the 3d force space and 3d torque space.
"on the other hand, music information retrieval exchange 2 (mirex ) community proposed a categorical mood taxonomy for audio based mood classification task [cit], which is quite popular among the mir researchers."
"semi-amorphous data sets, by contrast, are often not strongly structured in terms of the respondent's pattern of interaction. this is not to say that the observables are not well aligned to the construct. on the contrary, because of their very complexity, teas for formal measurement are usually robustly designed based on evidence-centered practices, domain modeling, cognitive task analysis and other often intensive means of construct alignment. but, however, semi-amorphous data are specifically designed such that each examinee has substantial self-selected response differentiation choice [cit] for any particular indicator. thus, saliency to the construct resides in interpretation over a pattern. for instance in the example used here, which will be introduced fully later in this paper, the semi-amorphous data involves whether respondents did or did not take the opportunity to observe a variety of frogs and water samples in a science simulation educational assessment."
"disadvantages from a measurement perspective are, in a sense, the same. with bayes nets, content developers often quickly try to implement quite complex structures with frequently hundreds or thousands of nodes and equally many associated probability tables. together, these represent statistically the equivalent of very complex path diagrams, with numerous mediating variables hypothesized. strong but often relatively untested assumptions describe how the variables all relate. often it is quite easy for learning scientists to propose other alternate hypotheses for these network structures, that would organize the nodes somewhat or very differently. when many of the mediating variables in such large networks can be moved to new locations in the network equally acceptably to the learning scientists developing them, or nearly equally, this is a problem both for interpretation and for stability and reliability of the measurement results. alternate node and arc positions represent radically different networks. this potentially leads to very different proficiency estimates for respondents simply by moving some of the nodes around. this is especially the case when network structures grow beyond a small size to validate in comparison to the various alternatives. large size network structures generate solutions that are difficult to validate due to the size of the outcome space when comparing the many alternative structures and hypotheses of other structures with so many nodes and arcs employed. this is the equivalent in other statistical techniques of validating extremely large and complex path diagrams. learning scientists also find it hard to fully interpret the meaning of large networks, which can readily be created but not as easily understood. especially their statistical implications often are difficult to grasp in the context of latent variables."
"it was observed that the n-gram features yield f-measure of 46.5% alone for the mood classification system based on the lyric features. the main reason may be that the hindi is free word order language and the hindi lyrics are also more free in word order than the hindi language itself. whereas, the text stylistic features do not help much in our experiments as it reduces the f-measure of the system by around 2.7%."
"vision-based quality measures for 3d dibr-based video, both full-reference fr-3vqm [cit], and noreference nr-3vqm [cit] are proposed to evaluate the quality of stereoscopic 3d video generated by dibr. both measures are a combination of three measures: temporal outliers, temporal inconsistencies, and spatial outliers, using ideal depth. ideal depth is derived for both no-reference and for full-reference metric for distortion-free rendered video. 3vqm metrics show better performances than psnr and ssim using a database of dibr-generated video sequences."
"where r is the resistance value in ω, is the strain, s is the gauge factor of the conductor, r o is the resistance with no stress applied in ω."
"in this section, experimental setup for the validation of proposed morphological multi-scale measures is described. the performances of two versions of the proposed morphological multi-scale metric, the morphological pyramid peak signal-to-noise ratio measure, mp-psnr, and the morphological wavelet peak signalto-noise ratio measure, mw-psnr, are presented and discussed. moreover, the psnr performances by multiscale decomposition subbands are analyzed. it is shown experimentally that psnr has very good agreement with human judgment when it is calculated for the images at higher morphological decomposition scales. therefore, we propose the reduced versions of the morphological multi-scale measures, reduced mp-psnr, and reduced mw-psnr, using only detail images from higher decomposition scales. the performances of the reduced morphological multi-scale measures are presented also. since the morphological operators used in morphological multi-resolution decomposition schemes involve only integers and only max, min, and addition in their computation the calculation of morphological multiresolution decompositions have low computational complexity. the calculation of mse is of low computational complexity also. therefore, the calculation of both measures, mp-psnr and mw-psnr, is not computationaly demanding."
"the mse error of each estimation type is shown in table 3 . this value is linked to the calibration dataset in which the calibration matrix was estimated and is affected by the number of calibration points. although the value can not be compared among datasets, the tendencies are similar. one of them is how the mse drops by taking temperature into account. for sphere types (sntdto, swtdto and swtrto), removing the temperature offset further reduces the error while for the centralized (cntdto, cwtdto and cwtrto) and oneshot (ontdto, owtdto and owtrto) types there seems to be no benefit. it can be observed that the fitting from the centralized and oneshot types are identical. for the calibration datasets considered, the centralized/oneshot types give better results in general. the only exceptions appear in the notz calibration dataset. in this dataset the sphere types have an slight advantage in three axis: f x, τ x and τ z . table 4, shows a comparison between the different estimation types including the workbench matrix. in general, the highest values are obtained when comparing with the workbench matrix. from these, the most different matrices are the ones obtained not considering temperature. sphere type with no temperature (sntdto) is the most different of all with respect to the workbench matrix."
the experimental platform is the floating base robot icub. it is a child-sized humanoid robot originally developed during the robotcub european project for research in embodied cognition [cit] by the icub facility at the italian institute of technology.
the performances of mw-psnr for different wavelet transformations are presented in the upper part of table 3 . the performances of mw-psnr using morphological wavelet transforms are better than the performances of mw-psnr using linear wavelet transforms. the best mw-psnr performances have been obtained
"the data imported using the rtl-sdr tool sampled at 2.4mhz is fed into the decimator which reduces the sampling rate to 240 khz by a factor of 5, producing a complex baseband fm signal which is fed into the complex frequency discriminator, see figure 5 . the discriminator as indicated in the demodulation model is made up of delay, math function, product and complex to magnitude-angle which performs fm demodulation. the result as shown in figure 6 is proportional to the fm multiplex sent to finite impulse response (fir) filter to bandpass filter to isolate channel 16 from other channels. the demodulated frequencies of the 16 channels are shown in figure 7 ."
"in case of indian music mood classification, koduri and indurkhya worked on the mood classification of south indian classical music using categorical mood representation and they considered the mood taxonomy consisting of ten rasas (e.g., srungaram (romance), hasyam (laughter), karunam (compassion) etc.) [cit] . similarly, velankar and sahasrabuddhe prepared data for mood classification of hindustani classical music consisting of 13 different mood classes (e.g., happy, exciting, satisfaction etc.) [cit] ."
"new frog takes place in a virtual version of a village with four farms. in new frog, students explore the problem of a frog with six legs. there are five causal factors to investigate in new frog for the anatomical anomaly: pollution, pesticides, parasites, genetic mutation, or aliens."
"when the morphological wavelet decomposition is used in the first stage of morphological multi-scale iqa framework, multi-scale wavelet mean squared error (mw-mse) is calculated as weighted sum of mse ji values for all subbands at all scales of the two wavelet representations as final pooling (11) ."
"for auv real-time navigation, different filter methods have been developed, including the extended kalman filter (ekf), unscented kalman filter (ukf), and particle filter (pf) [cit] . the ekf uses the first-order taylor expansion to calculate the jacobian matrix, which not only increases the complexity of the mathematical computation, but also introduces linearization errors. the pf has good performance under nonlinear conditions, but requires a large number of particles to achieve an optimal filtering effect. at the same time, the degradation of the pf will seriously affect the performance of the algorithm. the ukf utilizes unscented transform (ut) to estimate the system state vector and its covariance matrix, which reduces the impact of linearization errors [cit] . considering both the efficiency and accuracy of these algorithms, the ukf is more suitable for auv acoustic navigation."
"for any particular frog or water sample, there could be many reasons why the simulation artifact was observed or not from a given respondent. it could be that the simulation player virtually wandering about in the scenario didn't happen to encounter the location of that particular frog. or for a specific water sample, the respondent might have spoken to a farmer \"simulated agent\" about it instead of making his or her own observation, feeling that he or she has gained sufficient information to move on in the limited timeframe."
one of the aims of the paper is to provide useful tips to use the calibration method more effectively. the effectiveness of this tips is shown using real robot experiments.
"this section describes how the study was conducted, including definitions of the variables, data set, and tasks used in the study, as well as summarizing the analytic methods employed."
"for accumulating such data into well-supported inferences, results of using a hybrid mirt-bayes measurement modeling approach are reported here. the mirt-bayes two-stage measurement process introduced here nests information from small bayes net structures within a multidimensional random coefficients multinomial logit model (mrcml) model [cit] b; [cit] although other models could be used. nested within are small acyclic directed graph structures, shown here in examples of two layers and three layers; again other bayes net structures could be used."
"the reasoning behind this arrangement of datasets is to see what combination of datasets provides the best results. since it was proven before that grid and balancing together improve the calibration, the variables to test are the inclusion of z-torque and non-support leg datasets."
three simultaneous estimation applications are launched. one for each type of offset used. the ground truth is calculated offline in the sections of the experiments in which there was only one contact with the environment. multiple experiments were performed on the robot in the span of three hours to allow the ft sensors to heat up.
the sentiment classification system achieved f-measure of 68.30% using the same lyric features for hindi songs. [cit] performed the sentiment analysis on the lyrics of telugu songs using word2vec features.
"one of the simplest example of non-linear morphological wavelets is the morphological haar wavelet (minhaar) [cit] . it is very similar structure to the linear haar but it uses non-linear morphological operator erosion (by taking the minimum over two samples) in the update step of the lifting scheme [cit] . an illustration of one step of the wavelet transform with minhaar wavelet using the lifting scheme is shown on fig. 6 . initially, the signal x (the first row in fig. 6 ) is splitted to the even samples array (white nodes) and odd samples array (black nodes). the detail signal d (middle row in fig. 6 ) is calculated as the difference of the odd array and the even array (3). the lower resolution signal s (bottom row in fig. 6 ) is calculated from the even array and detail signal (4)."
"in actual marine navigation and positioning, if abnormal system noise and measurement noise occur simultaneously, the proposed adaptive robust ukf may provide unsatisfactory results or even lead to filter divergence in this rare situation. therefore, future research should focus on how to adjust the system noise and the measurement noise at the same time."
"however, incorporating more material is likely to extended the performance event beyond an acceptable assessment window for schools. so other designs such as repeated interim assessments might need to be considered, due to the length of time to measurement for performance assessments."
"the relationship between the regularization parameter and performance is clearly shown. this may allow to further refine and guide the selection of regularization parameters. the comparison with previous results suggests that the relevance and impact of each offset estimation strategy may be linked to the amount of data available. for small amounts of data, the physical assumption gives the highest improvements. with bigger amounts of data having no assumptions to estimate more accurately the calibration. the loss of effectiveness when using the physical assumption could be related to the fact that it does not consider temperature at all for offset estimation."
"the key stage of the multi-scale image quality assessment may be how to represent images effectively and efficiently, so it is necessary to investigate various kinds of transforms. most of the current multi-scale iqa metrics use linear filters in the multi-resolution decomposition. in this paper, we propose to use non-linear morphological operators in the multi-scale decompositions in the first stage of multi-scale iqa framework, fig. 2, in order to better deal with specific geometric distortions in dibrsynthesized images. introduced non-linear morphological filters used in the multi-scale image decomposition maintain important geometric information such as edges on their true positions, across different resolution levels [cit] . more precisely, we investigate two types of morphological multi-scale decompositions in the first stage of multi-scale iqa framework: morphological bandpass pyramid decomposition in mp-psnr and morphological wavelet decomposition in mw-psnr. in the second stage of the multiscale iqa framework, fig. 2, we propose to calculate squared error maps between the appropriate images of the multi-scale representations of the two images, the reference image and the dibr-synthesized image, in order to measure precisely, pixel-by-pixel, the edge distortion. in this way, the areas around edges that are prone to synthesis artifacts are emphasized in the metric score. in the third stage of iqa multi-scale framework, mse is calculated from each squared error map. mse of all multi-scale representation images are combined into multi-scale mean squared error, which is transformed into morphological multiscale peak signal-to-noise ratio measure."
"n-gram features work well for mood classification using lyrics [cit] as compared to the stylistic or sentiment features. we considered the scores of term frequency and document frequencies (tf-idf ) up to trigram levels, because including the higher order n-grams reduce the accuracy. we considered only those n-grams having document frequency more than one and removed the stopwords while considering the n-grams."
"the harvard virtual performance assessment (vpa) and new frog data set was used in this study. the data set consists of 1,986 cases with scores on 23 items and less than 1 percent missing data. respondents were middle-school students working with harvard university on the validation of the virtual performance assessments. students on average spent about 30 to 40 minutes taking the assessment, which contains in addition to the 23 response items also about 55 actions collected by the interface that the students may perform. some of the actions are considered salient to the construct by the assessment developers, and others are less so or not at all. more information on the assessment is provided below."
"it was observed that the hevner's adjectives [cit] are less consistent in case of the intra-cluster similarity, whereas mirex mood taxonomy [cit] suffers with inter-cluster dissimilarity and confusion between the categories were observed [cit] . from the above, [cit] concluded that the psychological models have some similarity with the social tags though it may not be suitable for today's music listening reality [cit] . in case of the indian songs, no such social tags were collected or reported till date."
the estimate offset varies more on the centralized and oneshot types. is noteworthy that the fitting of the data is equal even if the offsets are different as seen from the mse error in table 3 . the temperature coefficients and the calibration matrix are the same. what changes is the contribution from temperature. it seems that the offset estimation of the centralized/oneshot types collects both the force-torque offset and the temperature offset into the force-torque offset if no temperature offset is explicitly removed. therefore using the temperature offset might facilitate to gauge the offset purely in terms of forces with not temperature influence. table 5 . offsets for a calibration dataset for each estimation type.
"with the development of underwater acoustic communication, energy, control, and navigation technologies, autonomous underwater vehicles (auvs) have been widely used in submarine detection, ocean environmental monitoring, deep-sea facility laying and monitoring, marine mining, and other applications [cit] . the autonomous navigation systems for auvs include acoustic navigation, gravity-magnetic matching navigation, and integrated navigation systems (ins) [cit] . compared with gravity-magnetic matching navigation and ins, acoustic navigation does not need to store accurate underwater gravity-magnetic maps in advance, and has a smaller size and lighter weight. although acoustic navigation technology is challenging due to high costs, difficulty in construction of seabed beacon, and complexity of calibration for obtaining optimal positioning accuracy, it is widely used in auv navigation as an independent navigation system or as a key part of integrated navigation [cit] ."
"in the even step, the signal on the quincunx lattice is separated on two subsets, both on cartesian lattice, one subset with white pixels x and the other subset with gray pixels y, fig. 10 upper row. the pixel of the error signal d is calculated from the four nearest pixels on diagonal fig. 6 one step of the morphological wavelet transform using minhaar wavelet. the calculation of the detail signal d and the lower resolution signal s from the higher resolution signal x using the lifting scheme fig. 7 one step of the morphological wavelet decomposition using minlift wavelet. the detail signal d and the lower resolution signal s are calculated from the higher resolution signal x using the lifting scheme directions (7), fig. 10 bottom row left, and the lower resolution signal s is updated from four nearest detail signal pixels on diagonal directions (8), fig. 10 bottom row right."
"grouping the results by dataset, the calibration datasets can be ordered from best to worst in the following order: allgeneral, onlysupportlegs and notz. the behavior of a group of results by dataset is clear using the pallets of colors in figures 6 and 7 . there is a big improvement when adding the z-torque dataset and just a small improvement from adding the balancing non-support leg on top of that. showing that the z-torque gives relevant new information to the calibration dataset, while the balancing non-support leg adds few more information. therefore, for the considered dataset types the optimal combination is composed of grid, z-torque, balancing support leg. the balancing non-support leg can be considered optional and not strictly required. the fact that the best results not using the estimated offset is without using the balancing non-support leg dataset reinforces the previous statement. it can be seen that adding the temperature creates a clear difference in the results of a dataset. it almost divides each different calibration set results in two."
"where x 0,x 0 denote the initial and mean state information, including the x coordinate, y coordinate, speed, turning radius, and turning angle of auv, while p 0 stands for the initial covariance matrix."
"we used three lexicons to classify the moods present in the lyrics and these lexicons are hindi subjective lexicon (hsl) [ to the best of our knowledge, the performances of the available pos taggers and lemmatizers for hindi language are not up to the mark. the crf based shallow parser 9 is available for pos tagging and lemmatization, but it also did not perform well on the lyrics data because of the free word order nature of hindi lyrics. thus, the number words matched with these sentiment or emotion lexicons are considerably less. the statistics of the sentiment words found in the whole corpus using three sentiment lexicons are shown in table 3 . we also extracted the positive and negative words that were annotated by our annotators. we found 641 and 523 positive and negative unique words from the total corpus."
"complex information streams and rich assessment designs have been emerging recently in technology-enhanced assessments. increasingly these include semi-amorphous data, which can be content or process, such as click streams, patterns of resource use such as podcasts or vodcasts, alternating waves of dialog from live chats, and selected or constructed content responses that represent a range of reasoning facets."
"figure 14: sdr-rtl signals (location dependent) [cit] 6. conclusion in this paper, we have explored and demonstrated the use of rtl-sdr in matlab and simulink to receive and process fm, am, and speech signal. the signal processing schemes includes modulation and demodulation, multiplexing and de-multiplexing, filtering, sampling, channel coding, decimation, interpolation, signal conversion and spectrum analysis. we have shown how fft and waterfall analysers were used to effectively determine signal characteristics such as bandwidth, frequencies, power, noise level, and types. we also showed the mathematical and configurations issues in am and fm modulation and demodulation, and speech signals. the future benefits of sdr technology as shown in the last section has shown that sdr is an important part of new generation communication systems like 5g systems and internet of things based applications. there is a high demand for sdr based solution which is now driving a further investigation into the security concerns with sdr and rtl based communication systems."
"conceptually, it is possible to generalize this hybrid to a wide variety of bayes network structures and to a range of unidimensional, sequential unidimensional, or multidimensional irt models, depending on the example."
"for the original nodes in the bayes net structures, see figure 2, frogs and tadpoles located at the \"yellow\" farm, the \"blue\" farm and the \"red\" farm were employed, along with water evidence including observations of laboratory and red farm water samples. each vpa participant could achieve a 0 (not observed) or 1 (observed) for inspecting each frog, tadpole and water sample, as well as a partial credit score on a water evidence question, which for the purpose of the network was dichotomized into low and high. the original network representation for frog observables shows in figure 2 and the joint distribution tables in figure 3 . blue nodes show the frogs and water samples possible to observe, green nodes show inferences to be made in the original network. the frog evidence node was queried for posterior probabilities and expectations."
"in a one-stage process, nested models such as mirt-bayes could also potentially be estimated. one approach could draw on bayesian network models with continuous proficiency variables as equivalent to multidimensional irt models [cit] ). where such configurations apply, it could be possible to institute design matrices or other approaches through the estimation software that sufficiently constrain portions of the calibration, such that the nested directed acyclic graph portion of the model and the overarching irt structure are calibrated simultaneously, meaning within one estimation."
"given, as before, that up to 5 percent lesser fitting is expected in any calibration at the 95 percent confidence level, the model fitted here at 4 percent within expected tolerance remains good (1/26)."
"most of the taxonomies in the literature were used for evaluating the western music. ancient indian actors, dancers and musicians divided their performance into nine categories based on emotions and called the different emotions together as navrasa, where rasa means emotions and nav means nine. unfortunately in the modern context of music making, all the nine types of emotions are not frequently observed. for example, the emotions like surprise and horrific belonging to the navrasa are rarely observed in current hindi music. the emotion word hasya (happiness) need a further subdivision, for instance, happy and excited. hence, this model cannot be used for analyzing the mood aspects of indian popular songs [cit] ."
"these results show the mirt-bayes hybrid model may be promising to pursue further, based at least on performance with this data set. operationally it appears to be more capable of distinguishing among student performances, although limitations are noted in the above section. the approach improved measurement results. in terms of evidence quality, this helps support that mirt-bayes may be useful in making finer grain inferences while also reporting reliable scale scores aggregated at a larger grain size through the mirt model."
"the contribution of the temperature can be added separately. in case no temperature calibration is available, these coefficients are loaded as zeros by default. this is helpful in cases where temperature measurements are available for some but not all sensors."
"in mirt-bayes, the intention is that bayes-related inferences from semi-amorphous data patterns might be effectively entered into overarching irt models. [cit] described, that over six decades of research and application, measurement technology has matured to establish well-accepted procedures for important issues, which include calibration and estimation of a student overall score, reliability and precision information, test form creation, linking and equating, adaptive administrations, evaluating assumptions, checking datamodel fit, differential functioning, and invariance. all of these remain in place as mature approaches within mirt-bayes, due to the use of the overarching mirt model hybridized into the approach."
"bayes nets are also well understood models previously presented extensively in the research literature as stand-alone models and used for instance in classroom-based assessments [cit] bayes nets are acyclic directed graph structures, represented with nodes and arcs, which are shown in the results section in an example. to generate the joint probabilities in the network, a strong assumption is made in bayesian network approaches that p, the local probability distribution of variable x i, is made conditional only on the value of the parent nodes connecting to a given node from above. this is a defining assumption of using bayesian networks. in this case, each bayes net b embedded within the mirt model generates a single item score,which is added to the mirt model as a score."
"by improving the measurements of the six-axis ft sensor through in situ calibration, it was possible to see improvements in the behavior of the floating base robot icub."
"results are shown in tables 7 and 8 . it can be observed that using the in situ calibrated sensors, reduces the error in the contact forces and is more coherent when switching from a contact to another. this behavior can also be observed in the attached video. the red lines are the estimated values for the external force. there is a smoother transition in the length of the lines during the switching of double support and single support when using the new calibration matrix. also the contact forces remain close to the expected values during the execution of the demo. this observed by how small the red lines are on the parts of the robot not in contact. table 7 . offset estimated while hanging using imu. the balancing demo [cit] was observed to have oscillations of the robot when reaching the different pre-defined position tasks. after the six-axis ft measurements were improved, it was feasible to increase the gains of the low-level controller between 10 and 15 units depending on the joint. in some joints, this signified a 50% increase. it was observed that the movements of the robot seemed more defined and there were clearly fewer oscillations and less time required to switch to the next position task. moreover, it was clear that the same gains without the in situ estimated matrix make the robot fail immediately. thanks to the reliability of the measurements, the feedback of the low-level controller is more useful to control the robot creating a faster convergence to the desired value. this can be observed in the attached video s1."
"the morphological haar wavelet decomposition scheme may do a better job in preserving edges as compared to linear case [cit] . the morphological haar wavelet has some specific invariance properties. besides of being translation invariant in the spatial domain, it is also grayshift invariant and gray-multiplication invariant [cit] ."
"owing to the symmetry in the quincunx grid, the nonseparable transform is insensitive to edge directions and image orientation. non-oriented wavelet subbands from the first level of non-separable wavelet decomposition with quincunx sampling using morphological minliftq wavelet of the synthesized frame from the video sequence newspaper are shown on fig. 11 . the detail image from the odd step is rotated 45 ∘ before display. the detail images are almost zero at areas of smooth gray level variation. sharp gray level variations are mapped to positive (white) detail image values."
"the farthest right column is a scale of the performance, showing the difficulty of achieving the various generalised item thurstonian thresholds estimated from the data. briefly stated, thurstonian thresholds are plotted at the point on the display where a student falling adjacent has a 50 percent chance of achieving at least the indicated level of performance on an item, for that dimension [cit] ."
"the survey work on music mood classification can be divided into two parts, one outlining the mood taxonomies proposed for the western and indian songs and second describing the mood classification systems developed for the western and indian songs till date."
"treating the observations as if they were a set of sparsely populated but systematically missing indicators, with any one indicator able to stand on its own, is not a good representation of the conceptual meaning of the tea data. any observation is not intended to be a sole indicator. since it is only the pattern over them that is meaningful, then the pattern is the intended unit of analysis, not the observable, for semi-amorphous data (there may be many other observables in the tea that are intended to be individual indicators though)."
the results of the interpolations carried out in the modulation process are presented in figure 10 . the frequency domain of the signal is also shown in figure 12 .
"using the linear inverted pendulum model constraining the height of the com to be constant (p cog z ), the com dynamics can be estimated from the zmp with the following equation:"
"using these important techniques of construct mapping [cit] with mirt models can reveal a portrait of student readiness to learn, or area of the standards where students are actively constructing knowledge."
"while a two-stage process is used here, extension to one-stage is also possible [cit] ). however, there are advantages in flexibility of assessment development and scoring to keep the analytic process as two-stage."
"(3) the measurement prediction vectorẑ − k, the variance matrix p z,k, and the covariance matrix p xz,k are calculated by equations (11)-(13)."
"in this section, the adaptive robust ukf is validated by the simulation experiment of auv positioning, because the measured data from the ultrashort baseline positioning has only one observation at each observation time, and the validation of the robust algorithm cannot be completed without redundant observations. the experimental trajectory and error simulation of the simulation experiment are the same as in section 4.1.1. the only difference is that gross errors are simulated based on normal distribution with zero mean and standard deviations of 0.005 s, and added in the acoustic observations every 20 s. at the same time, rms is used to evaluate the performance of the ukf, the robust ukf, and the adaptive robust ukf."
"when the morphological pyramid decomposition is used in the first stage of morphological multi-scale iqa framework, fig. 12, multi-scale pyramid mean squared error mp_mse is calculated as weighted product of mse j values at all pyramid levels (9) ."
"the research design therefore is based on a comparative study of model fit statistics and proficiency estimate results, with and without the hybridization of the mirt model with the bayes nets. first, in exploration 1, model fit and results from applying the mirt model alone for the data set used are investigated (see methods section). exploration 1 employs only the item response data appropriate to the mirt model alone and not the semi-amorphous process data. then, for exploration 2, the same data set and procedures are used, but expanded with the semi-amorphous data patterns and with application of the mirt-bayes model. results are then compared to see if the hybridization has offered any advantages, such as described in the hypotheses above."
"in parallel with exploration 1, first a unidimensional and then a multidimensional random coefficients multinomial logit model (mrcml; were fit to the now 25-item data set, following the bayes net application to provide the scoring, using acer conquest generalised item response modelling software."
"morphological wavelet decomposition using minlift wavelet is both gray-shift invariant and gray-multiplication invariant [cit] . min-lifting scheme has the nice property that it preserves local minima of a signal, respectively, over several scales. it does not generate any new local minima. the detail signal is almost zero at areas of smooth gray level variation and sharp gray level variations are mapped to positive detail signal values (white). as an illustration of the wavelet decomposition using morphological minlift wavelet, the oriented wavelet subbands from the first decomposition level which contain vertical, horizontal, and corner details are shown on fig. 8 for the synthesized frame from the video sequence newspaper."
this validation was performed twice. one using only the calibration matrices estimating the offset in a few of the samples and the other using also the estimated offsets. the results of the contact force validation are shown in tables a1 and a2. from the evaluation of the estimation types behavior is clear that the centralized types and the oneshot types give the same result. because of this only the sphere types and the oneshot were considered.
"here, a measurement model is considered to be defined as a mathematical model that aggregates or accumulates data from individual indicators or observables to make an inference [cit] . the mirt-bayes two-stage measurement process examined here nests information from small bayes net structures within an irt model. using bayes nets within irt, or conversely irt within bayes nets, has been proposed as a potential solution in educational measurement for complex assessments to capture advantages and mitigate disadvantages of each approach for some time [cit] . but little research is available exemplifying the possibilities. the advent of complex tea is stimulating new interest, so the new mirt-bayes modeling approach is proposed here."
"reliability estimates under mirt-bayes are good for both dimensions, ranging from .83 to .89 with a cronbach's alpha of .87, as shown in table 2 . as before, the missing data percentage remained less than 1 percent for the analysis reported here."
"semi-amorphous data include a wide range of possibilities such as click streams of mouse movements in a digital setting, patterns of resource use such as podcasts or vodcasts, or alternating waves of dialog in a chat stream. semi-amorphous data could also include selected or constructed content responses (sr or cr), such as a choice made by the respondent (sr) or an explanation provided (cr), if the result represents a range of reasoning facets rather than a clearly scorable answer with a correct response, as in a traditional assessment question. such a range of ways to elicit evidence is advancing what is possible for learning analytics [cit], especially in hard-to-measure constructs [cit] ."
"regarding goals and objectives of measurement for the harvard new frog vpa, the assessments were aligned by harvard with the then newly released framework for k-12 science education as well as with the college board standards for college success. [cit] public draft of the \"next generation science standards for today's students and tomorrow's workforce\" was used for the alignment. [cit] ms.ls-gdro growth, development, and reproduction of organisms [cit] ."
"for this paper, the statistical innovation is to embed bayesian networks, or bayes nets, within the mirt model during the mrcml estimation. the bayes nets results are embedded here as additional item scores in the mirt model. so the bayes nets are essentially used here to provide a \"score,\" or item-level valuing for the respondent, over a pattern of semi-amorphous data developed to be salient together as a pattern informing on the construct of interest, and added to the prior more explicit items already in the model. hence, each semi-amorphous pattern becomes an item."
"the icub has various sensors including inertial measurement units (imu), force-torque (ft) sensors, cameras, microphones, joint encoders and tactile sensor arrays, that cover the surface of the robot. six custom-made six-axis ft sensors [cit] ) are placed as shown in figure 1 . only the force-torque sensors mounted on the legs and feet have temperature sensors. these sensors use silicon strain gauge technology. the interface to interact with the icub is through yet another robot platform (yarp). more specifically, yarp supports building a robot control system as a collection of programs communicating in a peer-to-peer way, with an extensible family of connection types (tcp, udp, multicast, local, mpi, mjpg-over-http, xml/rpc, tcpros, ...) that can be swapped in and out."
"(2) the state is predicted by the ut. then, the state vectorx − k and the covariance matrix p − x,k are estimated by equations (9) and (10)."
"where f i is the force in the i-th axis in n or n m depending on the axis, r m is the digital response of the m-th strain gauge in bit counts, c m is the slope of the linear model of the m-th strain gauge in n/bit and o m is the bias of the m-th strain gauge. the orientation of f i with regards to the m-th strain gauge will change the value of k required. it depends on the strain being normal, shear or a combination of both. as a result, the array of c m coefficients c m and o m will be different for each i-th axis. taking this in consideration, the approximation function for these sensors for all axes has the following form:"
"based on the psnr performances by subbands for the irccyn/ivc dibr database given in fig. 23, it can be concluded that the psnr performances of wavelet subbands at decomposition levels 4-7 are much better than the subband psnr performances on levels 1-3. therefore, we propose reduced version of mw-psnr using only these higher level subbands. reduced versions of mw_mse is calculated as weighted sum of the used subbands' mse. for the separable wavelet decomposition, the reduced version of mw-psnr is calculated using only 11 subbands from levels 4-7 with indices 41-72. for the nonseparable wavelet decomposition with quincunx sampling, reduced version of mw-psnr is calculated using 6 subbands from decomposition levels 4-7 with indices 42-71. matlab implementation of the reduced version of mw-psnr is available online [cit] . the performances of the reduced mw-psnr are presented in the bottom left part of table 3 . it is shown fig. 25 ."
"the sample was based on an extant data set and employed de-identified secondary data analysis of the assessment data supplied by the harvard project. future work might look at samples intentionally designed for representative characteristics. furthermore, information on incentives, compliance rates, and other specific settings that might have been involved in the study were not available, except for missing data rates directly in the data set."
"construct mapping such as here, when models fit, estimates are reliable and a valid case has been made for the uses and purpose of the instruments, gives a \"data\" picture to teachers, subject matter experts, and learning scientists. it provides empirical evidence on both the construct/standards and where students stand. the next step in the process is for teachers and other experts to apply their knowledge of how to intervene with students to address growth and improvement. readiness to learn is depicted on the wright map; steadiness in making learning gains, or growth, is then addressed by teachers, scientists, science educators, schools, districts, states and others who are working with students. the end goal here is for all students to have the opportunity to learn and to achieve success in their efforts."
"thus, our final mood classes are angry (alarmed, tensed), calm (satisfied, relax), excited (aroused, astonished), happy (pleased, glad) and sad (gloomy, depressed). one of the main reasons for collecting songs and grouping the similar songs into a single mood class is to consider the significant invariability of the audio features at subclass level with respect to their main class. for example, a \"happy\" and a \"delighted\" song have high valence, whereas an \"aroused\" and an \"excited\" song have high arousal."
"for the comparision of dibr algorithms, virtual views synthesized from the uncompressed data which contain only synthesis artifact need to be evaluated. when encoding either depth data or color sequences before performing the synthesis, compression-related artifacts are combined with synthesis artifact. in this paper, the distortions introduced only by view synthesis algorithms are evaluated using the irccyn/ivc dibr image dataset [cit] and part of the mcl-3d image dataset [cit] ."
"in both cases, we end up with a modified version of the raw data in which the effect of the offset is removed. with a little abuse of notation we have:"
"in this paper, we propose to use mse for distortion measurement between pyramid images in mp-psnr and between wavelet subbands in mw-psnr. in the second stage of the multi-scale iqa framework we use squared error maps between the morphological multi-scale representations of the two images: the reference image and the dibr-synthesized image. squared error maps calculated pixel-by-pixel show wrong displacement of the object edges induced by dibr process through different scales of multi-scale representations. from the squared error maps, mean squared errors are calculated and combined into the multi-scale mean squared error which is transformed into multi-scale peak signal-to-noise ratio in the third stage of the multi-scale iqa framework."
"each of the lyrics was annotated by at least three annotators aged 20±4 years, who were undergraduate students and research scholars and worked as volunteers for annotating the lyrics corpus. the lyrics were asked to annotate after reading it with either of the aforementioned five mood classes."
the items in the new frog data set and the two dimensions on which they load in in the explored models show in table 1 .
"in the previous model based formulation without temperature [cit], less amount of data was used to calibrate the sensor. when considering only one type of dataset for calibration, the sphere types outperform the centralized ones. if more than one type of dataset was to be combined, it needed to be collected immediately one after the other for having the assumption of same offset still valid for both datasets. it was observed that even after the sensors were calibrated the offset estimated during calibration not suitable to use on the robot. the reason was that the measurements would drift and the estimated offset would not be valid after a short time. this was resolved by introducing the temperature as linear variable [cit] . in the previous paper of model-based in situ calibrations with temperature compensation, results showed that the sphere types still had a certain advantage over centralized types. in those experiments, the amount of data used in calibration had less types of datasets equivalent to the notz dataset here. by including the new types of datasets better results were obtained. also, the one shot gave better performance than the sphere type. this shows a correlation between estimation type and types of calibration data used. the experiments described here show that datasets for calibration can be taken in different days in different conditions. this makes the data collection less restrictive despite the need for more data types. in the previous validation dataset, the offset was estimated at the beginning of the datasets. this is similar to the performance of the online estimated offset. this masked the difference in performance of the centralized offset with temperature and without. in here, the benefits of the method are further highlighted by showing the impact in the performance of floating base robots due to better ft measurements and the advantages of using the temperature are extended."
"for music mood classification using audio features, the linear kernel of libsvm was selected since it provides the higher f-measure in our case as compared to the polynomial kernels. we performed the classification by adding the features one by one. initially, the timbre features were used to classify the moods, then added intensity features and then rhythm features, incrementally. after adding all the features together, the audio based mood classification system achieved the maximum f-measure of 58.2%. the contribution of each feature in f-measure is given in table 5 ."
"to verify the behavior of the estimation types only the results from a single calibration dataset is showed. the one selected is the onlysupportlegs dataset. nonetheless, the results extend to the other two calibration datasets."
"using notation from the mrcml sources described above and from developers of the software used in the calibration [cit], proficiency estimates are generated according to the mrcml as:"
"instead of estimating the offset before the experiment, the offset estimated by the calibration method can be used by summing the offset after the new calibration matrix and temperature compensation is applied."
where e is the modulus of elasticity of the material and depends on the kind of stress and strain applied to it. the other principle depends on the type of sensing technology. for semiconductor strain gauges it is the piesoresistive effect. the model is the linear function [cit] :
"the evaluation of dibr system depends on the application. the main difference between free viewpoint video (fvv) and 3dtv is the stereopsis phenomenon (fusion of left and right views in human visual system) existing in 3dtv. fvv does not have to be used in 3d context. it can be applied in 2d context. in this paper, the quality assessment of still images from mvd video sequences in both 2d and 3d contexts as a first step of 3d quality assessment is concerned. the evaluation of still images is important scenario in the case when the user switches the video in pause mode [cit] ."
"most of the depth-image-based rendering (dibr) techniques produce images which contain non-uniform geometric distortions affecting the edge coherency. this type of distortions are challenging for common image quality assessment (iqa) metrics. we propose fullreference metric based on multi-scale decomposition using morphological filters in order to better deal with specific geometric distortions in the dibr-synthesized images. introduced non-linear morphological filters in multi-resolution image decomposition maintain important geometric information such as edges across different resolution scales. the proposed metric is dedicated to artifact detection in dibr-synthesized images by measuring the edge distortion between the multi-scale representations of the reference image and the dibr-synthesized image using mse. we have explored two versions of morphological multi-scale metric, morphological pyramid peak signal-to-noise ratio measure, mp-psnr, based on morphological pyramid decomposition and morphological wavelet peak signal-to-noise ratio measure, mw-psnr, based on morphological wavelet decomposition. the proposed metrics are evaluated using two databases which contain images synthesized by dibr algorithms: irccyn/ ivc dibr image database and mcl-3d stereoscopic image database. both metric versions demonstrate high improvement of performances over standard iqa metrics and over tested metric dedicated to synthesis-related artifacts. also, they have much better performances than their linear counterparts for the evaluation of dibr-synthesized"
"before using the values obtained through force-torque sensing, they can be corrected by pre-multiplying with the secondary calibration matrix. this way the measurements used by the robot are the same as if the sensor was calibrated using the in situ calibration matrix."
"a wide range of textual features such as sentiment lexicons, stylistic and n-gram features were adopted in order to develop the music mood classification system."
"the result from the model-based in situ calibration is a new calibration matrix. it is when using this calibration matrix that the improved measurements are obtained. therefore, the new calibration matrix should be used somehow by the robot to obtain the improved measurements and better dynamic performance as a consequence."
"india is considered to have one of the oldest musical traditions in the world. hindi is one of the official languages of india and stands fourth with respect to the most widely spoken language in the world 1 . hindi music or bollywood music, also known as popular music [cit] are mostly present in hindi cinemas or bollywood movies [cit] . hindi or bollywood songs make up 72% of the total music sales in india [cit] . it is observed that hindi or bollywood songs include varieties of hindustani classical music, folk music, pop and rock music. indian film music is not only popular in the indian society, but has also been on the forefront of the indian's culture around the world [cit] . mood related experiments on western music based on audio [cit], lyrics [cit], and multimodal approaches [cit], achieved promising milestones in this arena. in contrast, experiments on indian music moods were limited, for example, mood classifications of hindi songs were performed using only audio features [cit] and lyric features [cit] . to the best of the author's knowledge, no multimodal mood classification system was developed for hindi songs."
"(2) when there are gross errors in the observation, the adaptive ukf has no ability to resist the effect of gross errors. to solve the problem, this study presents an adaptive robust ukf based on the sage-husa filter and the equivalent gain matrix. the adaptive robust ukf can adaptively adjust the system noise to solve the divergence problem and control the effects of gross measurement errors on dynamic state estimates. compared with the classical ukf and adaptive ukf, the accuracy of the adaptive robust ukf is significantly improved. therefore, the proposed adaptive robust ukf can provide an effective method for auv acoustic navigation and positioning."
"so it is not the individual observable that matters. rather saliency comes in the pattern of observation over the set of frogs and water samples, relative to the simulation goal. while this could be handled in the measurement design with approaches used in the past such as giving partial credit over the set of observables, using decision rules to award a score, or having substantial systematically missing data, there are drawbacks to these approaches. for instance, as the semi-amorphous opportunities become more complex, it becomes very hard for learning scientists designing the teas to provide decision rules to score all the possible permutations of patterns, in the complex tea tasks. there becomes a tendency to reduce the relevant information or discard it altogether for ease of use."
"for the tea field, whenever observables are statistically aggregated to make an inference, a measurement model is considered as being applied [cit] . since both stages of the mirt-bayes process are accumulating data from sets of observables, the full approach is considered here to be a two-stage, or multi-stage, measurement model process."
"this fact enhances the importance of developing an automated process for music organization, management, search as well as the generation of playlists and various other music related applications."
"in this section, the adaptive robust ukf is validated by the simulation experiment of auv positioning, because the measured data from the ultrashort baseline positioning has only one observation at each observation time, and the validation of the robust algorithm cannot be completed without redundant observations. the experimental trajectory and error simulation of the simulation experiment are the same as in section 4.1.1. the only difference is that gross errors are simulated based on normal distribution with zero mean and standard deviations of 0.005 s, and added in the acoustic observations every 20 s. at the same time, rms is used to evaluate the performance of the ukf, the robust ukf, and the adaptive robust ukf."
"bandwidth and modulation identifier sdr is a wireless communication system having significant part of it defined in software instead of conventional hardware radio system. this technology came with several advantages ranging from ease of development to flexibility in reconfigurations. matlab is one of the tools with many support packages for sdr. to further, understand how sdr works in simulink, recorded signals were at one stage or the other used to carry out this study. the identification of the type of radio frequency signal, the signal bandwidth, modulation and demodulation techniques in the radio frequency spectrum has always been a difficult task to achieve using sdr toolbox in simulink. to identify the radio spectrum of the given signal, figure 4 consisting of two spectrum analyzers configured as fast fourier transform (fft) and waterfall of the same bandwidth were employed. the results in figure 4 fft and waterfall above shows group of burst independent signal having separation (wide band) of 200 khz as specified by the gsm standard for both uplink and downlink bandwidth. it is evident that the signal under study is a global system for mobile communication (gsm) whose frequency of operation ranges from 900 to 960mhz. proper examination of the spectra shows that channels a as indicated in waterfall analyser contains much powerful signals than others like channel b and cannot be used to determine the busty nature of the packet because it has almost a continuous transmission rate (broadcast control channel). also, a critical examination of figure 4 shows that time division multiple access (tdma) is the channel access method for the shared medium. it allows many users to share the same frequency channel by dividing the signal into different timeslot [cit] . the period of a single gsm burst is approximately 577 and the period of a time division multiple access (tdma) frame serving 8 users is 4.614 gsm used gaussian minimum phase keying (gmsk) as a modulation format. it is mainly applied in all the cellular system networks. gsm signal is mostly used in location tracking and estimating a mobile user position through cell allocation and mobile equipment coding. the point from where the signal is being transmitted can also be determined though difficult with limited packet information. however, the identified signal in figure 4 is a gsm signal by observation. spectrum analyzer is an efficient tool for signal identification in the frequency domain. it aided in identifying the signal's bandwidth, frequency and other signals characteristics such as power and noise level."
"it has 53 degrees of freedom (dof), weighs around 33 kg and is 104 cm tall. the dofs are distributed as in the following way: six for each leg, three for the torso, six for the head and eyes, seven for each arm and nine for each hand. one additional servo motor is used to open and close the eyelids. for the calibration, only a subset of 32 dofs (legs, torso, arms, and neck) are used. the version of icub used is known as 2.5. it has brushless direct current electric motor (bldc) with an harmonic drive transmission, making them suitable for joint torque control."
"(1) the initial position information, speed, turning radius, and turning anglex 0 of the auv are initialized and the sigma points χ k are generated."
"data accumulation, or in other words measurement modeling, challenges for complex teas are myriad [cit] . this paper addresses one concern: often some important observables, whether process-oriented or content-related, are directly scorable by assessment developers, while other salient data present as \"semi-amorphous,\" or less immediately interpretable in teas. in the tea semi-amorphous situation, any particular observable may not be especially meaningful alone but is key with other data in observing a pattern, or trend over a set of observables. this paper explores approaches to effectively accumulating semi-amorphous tea data with new models."
for this experiment the offsets used were estimated with a month difference with respect to the experiment date. the comparison between the expected ft value and the actual value obtained after applying both the offset and the secondary calibration matrix correction. three different offsets where used:
"the hybrid model process employs two major modeling approaches in the measurement field, irt and bayes nets. therefore usual measurement standards of exploring data fit, validity, reliability and other high quality evidence concerns should be addressed for tea applications, as briefly exemplified here."
limitations of this research study are of course extensive. data from only one vpa task were examined in this study. other data sets may indicate differently regarding use of the mirt-bayes model for accumulating semi-amorphous process data.
"the rest of the paper is organized in the following manner. section 2 briefly discusses the state-of-the-art mood taxonomies and music mood classification systems developed for western and indian songs. section 3 provides an overview of our proposed mood taxonomy and data annotation process for hindi songs. section 4 describes the features collected from audio and lyrics of the hindi songs, while section 5 presents the mood classification systems and our findings. finally, the conclusions and future directions are listed in section 6."
"we have considered the key features like intensity, rhythm, and timbre for mood classification task. these features have been used for music mood classification for indian languages in state-of-the-art systems [cit] . we have listed the audio features used in our experiments in table 2 and these features were extracted using the jaudio toolkit 7 [cit] ."
"from the resulting generic formulation to add linear variables, described in section 2.2.3, it is possible to formulate the problem in a way that the offset does not have to be removed from the calibration data before computing the least squares solution. this offset estimation type is named one shot estimation. it estimates the offset as another set of coefficients of the calibration matrix by adding a linear variable in which the reference values are all 1. contrary to the other two offset estimation types it makes no assumptions and allows the least squares to simultaneously solve the offset with the calibration matrix."
"the simplest use is as a threshold. when the load on the sensors reaches a chosen value, the controllers change state assuming a stable contact has been achieved. the chosen value can be a percentage of the total body weight of the robot. example of applications are balancing [cit] and standing up [cit] ."
"where x k, x k−1 are the state vectors of the auv at times k, k − 1, respectively; ϕ(x k−1 ) is the nonlinear state transition matrix; ω k−1 is the system noise; γ(ω k−1 ) is the system noise distribution matrix; z k is the acoustic observation vector at the time of k; h(x k ) is the nonlinear observation matrix; and ∆ k is the measurement noise. the system noise and the measurement noise with covariance matrices q k, r k are usually assumed to be uncorrelated gaussian white noise:"
therefore the most used model for predicting the force-torque from the raw strain gauges measurements of the sensor is a linear model. the inverse function is:
"since the robot is standing on flat ground it is expected that the only force acting on the robot feet is mass on the z axis. with no other force acting on the robot the forces in x and y should cancel each other in double support or be 0 when on single support. with this as a ground truth, is possible to evaluate the estimated contact forces at the feet when switching."
"to verify the adaptive ukf, the simulated long baseline positioning experiment of the auv and the real marine experimental data of the ultrashort baseline positioning of an underwater towed body are carried out. then, the adaptive robust ukf is analyzed and compared in the different situations through the simulation experiment."
"the earliest categorical music mood taxonomy was proposed by hevner [cit] and is known for its systematic coverage on music psychology [cit] . another traditional categorical approach uses adjectives like gloomy, pathetic and hopeful etc. to describe different moods [cit] ."
"item fit to the model was good, with only 4 percent (1 of 23 items) with estimated item difficulty parameters outside 3/4-4/3 mean square weighted fit [cit], for parameters in which the weighted fit t was greater than 2. even randomly, at the 95 percent confidence level, about 5 percent of item parameters could be expected to be less fitting. note that this item parameter was also at an extreme high of the difficulty distribution, where fewer were located to provide data to estimate the parameter."
"when the bayes net structures are kept small and discrete, however, these issues are more ameliorated while the advantages of flexibility and representation are retained. even though there potentially may be several small networks nested into a mirt-bayes, each one can be handled on its own merit. small network by small network, learning scientists are more able to understand and interpret them, and statisticians to validate the small network structures each separately. furthermore, within a hybrid approach, the overarching mirt model supplies a great deal of information about data fit and item analysis of the nested structures."
"there was also not a strong correlation between the two dimensions; rather the relationship between the two dimensions was only moderate, as estimated by the model. the correlation between the inquiry and explanation latent variables was 0.42. note that this correlation is effectively already corrected in the conquest software for attenuation caused by measurement error. this is a much lower correlation between dimensions than found in many analyses of other science and engineering constructs and instruments using multidimensional models, where dimensions are often found to be very highly correlated, at the .8 level or higher. in stem education, mirt may be advocated even at correlations between dimensions as high as .7-.8 [cit] ."
"the performances of the proposed metrics are much better than the performances of the commonly used 2d metrics and better than the performances of the metric dedicated to synthesis-related artifacts, 3dswim. the pearson's correlation coefficients of the selected commonly used 2d metrics, the metric dedicated to synthesisrelated artifacts, 3dswim, and the reduced versions of mp-psnr and of mw-psnr are shown on fig. 26 ."
"where f r i is the 6d force reference vector andf c i is the 6d force vector obtained using the estimated calibration matrix of each estimation type. a second way is to compute the mean of the absolute value of the difference between matrices. this is to get a general idea of how much the calibration matrices differ one from the other. the third way is looking at the offset values. this is to see how the different estimation types affect the estimation of the offset. although there is no ground truth for the offset to serve as a comparison, similarity in the offset values might indicate a general idea of what the true offset might be."
"regarding fit statistics, the original 23 items in a2 and a3 are slightly better fitting under the exploration 2 mirt-bayes model than previously under exploration mirt only. for instance, the parameter at the extreme high difficulty reported previously for a2 and a3 as falling slightly outside 3/4-4/3 mean square weighted fit no longer falls outside under the new model. this is perhaps a consequence of reducing the ceiling and floor effects of the test by lowering the standard errors at the extremes. in addition the new frog observable network and the 6-legged frog item showed good fit, within the tolerances described previously, with the tadpole observable slightly less well-fitting."
"5. functioning with much more data (e.g, big data), but also noisier data, while still being able to generate high quality evidence, often across large-scale and classroom-based practices if needed."
"in the case of mcl-3d database, the operation sum is used in the calculation of mp-mse (9) as better performances of mp-psnr are achieved. for the fig. 18 are based on the mixed statistics of the dibr algorithms a2-a7 for the irccyn/ivc dibr database and a1, a2, a7, a8 for the mcl-3d database. mp-psnr using pyramid decomposition with morphological filters has much better performances than mp-psnr using pyramid decomposition with linear filters."
"on the high end, much larger aggregations encompassing more than about 10 nodes become subject to the concerns of bayes net alternate hypotheses discussed previously. since large aggregations can readily be handled by the overarching mirt model, there is less need to construct large nested networks within an embedded model. also, key portions of larger structures that seem meaningful to learning scientists can be \"nodalized\" or isolated from the large structure and form a new nesting within mirt-bayes. at this time, there is no suggestion for how many total nested structures might be included; this is a subject for further investigation."
"in this section, the performances of the morphological pyramid peak signal-to-noise ratio measure, mp-psnr, are analyzed. morphological bandpass pyramid decomposition using morphological operator erosion for lowpass filtering in analysis step and morphological operator dilation for interpolation filtering in synthesis step (mbp ed) is applied on the reference image and the dibrsynthesized image. the influence of different size and shape of structuring element used in morphological operations and different number of decomposition levels in mbp ed pyramid decompositions on mp-psnr performances are explored. for comparison with linear case, mp-psnr performances are calculated using laplacian pyramid decomposition with linear filters. in addition, psnr performances calculated between two pyramids' images on different pyramid scales are investigated. the reduced version of mp-psnr using only lower resolution images from higher pyramid scales is proposed and its performances are analyzed. the shape and the size of the structuring element (se) used in morphological filtering determine which geometrical features are preserved in the filtered image especially the direction of object's enlargement or shrinking. using square structuring element the objects are enlarged or shrinked equally in all directions. squaredshaped structuring element is suitable to detect straight lines while round se is suitable to detect circular features. the mp-psnr performances using different shapes of structuring element (square, round, rhomb and cross type structuring element, fig. 16 ) for morphological filtering in analysis step are evaluated. better performances of mp-psnr are achieved with square or round type se than by rhomb or cross type se. the results are similar with square and round type structuring element, but the computational complexity is significantly lower when the square structuring element is used. namely, in that case separable pyramid decomposition by rows and columns with downsampling after each step can be easily implemented."
"moreover, the best psnr performances by wavelet subbands for each wavelet decomposition are shown in table 4 . for instance, for the irccyn/ivc dibr database for the separable wavelet decomposition using morphological minlift wavelet, the best psnr performances are obtained for subband on the scale 6 with vertical details (d61), pcc 0.887 and scc 0.828. also, for all tested wavelets, the psnr of the wavelet subband with the highest pcc show much better performances than psnr calculated between the reference image and the dibr-synthesized image without decomposition in the base band."
feature extraction plays an important role in any classification framework and depends upon the data set used for the experiments. we have considered different audio related features and textual features of lyrics for mood classification.
"the developed algorithm has been proven to improve the measurements of the sensor and the dynamic behavior of a floating base robot. it successfully accounts for temperature drift and can be extended to account for other lineal phenomena, allowing controllers to reliably use the sensor measurements as feedback, be it directly or indirectly."
"in normal conditions with known initial system noise of the dynamic model and white noise of the measurement model, the conventional ukf can provide good estimation results for auv navigation [cit] . however, if the initial system noise deviates too much from the real noise, or if gross error exists in the observation, the ukf can be greatly influenced, causing inaccurate results or even filter divergence. the conventional ukf cannot adaptively adjust the system noise and control the influence of gross errors. in order to solve the aforementioned problems, this paper proposes an adaptive robust ukf based on the sage-husa filter and the robust estimation to adaptively estimate the system noise of the dynamics model and resist the effect of gross errors in the acoustic observations. the paper is organized as follows. we begin by presenting the theory of the ukf in section 2. section 3 introduces the theory of the adaptive ukf, as well as the theoretical derivation and algorithm implementation of the adaptive robust ukf. the adaptive ukf and the adaptive robust ukf are verified and analyzed by experimental data in section 4. finally, we summarize the significant conclusions in section 5."
"to verify the adaptive ukf, the simulated long baseline positioning experiment of the auv and the real marine experimental data of the ultrashort baseline positioning of an underwater towed body are carried out. then, the adaptive robust ukf is analyzed and compared in the different situations through the simulation experiment."
"in this section, the performances of the morphological wavelet peak signal-to-noise ratio measure, mw-psnr, are analyzed. mw-psnr uses morphological wavelet decomposition of the reference and the dibr-synthesized images. both separable morphological wavelet decompositions using morphological haar min wavelet (minhaar) and min-lifting wavelet (minlift) and non-separable morphological wavelet decomposition with quincunx sampling using minlifting wavelet (minliftq) are investigated. separable morphological wavelet decompositions are computationally less expensive than non-separable wavelet decompositions. also, they are less expensive than morphological pyramid decompositions for the same filter length. the influence of different number of wavelet decomposition levels on mw-psnr performances are explored. for the comparison with linear wavelet decompositions, mw-psnr performances are calculated using separable linear wavelet decompositions using haar wavelet (haar) and cohen-daubechies-feauveau wavelet cdf(2,2) and non-separable linear wavelet decomposition with quincunx sampling using cdf(2,2)q. psnr performances calculated by wavelet subbands through decomposition scales are investigated. the reduced version of mw-psnr using only wavelet subbands with better psnr performances is analyzed."
"6 is annotated as \"sad\" and \"happy\" while reading the lyrics and listening to the audio, separately. this song portrays negative emotions by using sad or negative words like \"tere liye hi mar jaunga (i would die for you)\", but, the song contains high valence. the above observations emphasize that the combined effect of lyrics and audio is an important factor in indicating the final mood inducing characteristics of a music piece."
"robots are expected to perform highly dynamical motions. being able to perform these motions repeatably and reliably is an active research topic. a crucial part of these motions is the interaction of the robot with other objects or the environment. whenever an interaction happens there exist an exchange of forces. therefore, knowledge of the forces exchanged at contacts is a fundamental part of endowing robots with the ability to perform dynamical motions. the six-axis force-torque (ft) sensors convey a complete information of a contact force by providing measurements of the three axes of forces and three axes of torques. robots with their base fixed to the ground have been using ft sensors to measure contact forces since a long time [cit] . despite being common sensors in many floating base platforms [cit], their potential use in floating base robots has not been full documented. one of the reasons is that the reliability of this sensors may decrease after mounting them [cit] . the model-based in situ calibration method has shown promising results in the improvement of the the paper is structured as follows: in section 2, the in situ calibration method is detailed. it also contains a description of the robotic platform and the ways the sensor information is used in such a platform. section 3 contains the useful tips to better exploit the model based in situ calibration method with temperature compensation in a simple way and shows a way to exploit the generic formulation to simultaneously estimate offset and calibration matrix. we also describe two new types of datasets with the aim to discuss how to understand sensor excitation easily. a simple non-intrusive method to exploit the resulting calibration is described. a description of the constant offset hypothesis can be found as well. section 4 details how the experiments were setup, the datasets used and how the results were validated. in section 5, the results are used to verify the usefulness of the tips, the constant offset hypothesis and improvements in floating-based robot performance when using the model-based in situ calibration method with temperature compensation. conclusions can be found in section 6."
"both measures, mp-psnr and mw-psnr, are highly correlated with the judgment of human observers, much better than standard iqa metrics and much better than their linear counterparts. they have better performances than tested metric dedicated to synthesis-related artifacts also. since the morphological operators involve only integers and only max, min, and addition in their computation, as well as simple calculation of mse, the proposed morphological multi-scale metrics are of low computational complexity."
"however, the dramatic innovations in tea constructs, observations and scoring come along with interpretation challenges of statistically aggregating scores from the new tea assessment instruments [cit] . new statistical models that can provide high quality proficiency estimates for the psychometrics of complex tea contexts are needed [cit] . here, a hybrid mirt-bayes two-stage modeling approach is introduced and explored through statistical application in a simulation-based harvard university tea context [cit] )."
"considering that most of the drift is assumed to be caused by temperature, it follows that by properly compensating the temperature drift, the offset of the sensor itself should be time invariant. this can be proven by using the offset estimated at the time of the calibration and applying it on some other time while the sensor is subjected to different temperatures."
this formulation allows to easily expand the solution to m number of extra linear variables. the extra linear variable can have its offset removed or not. assumptions can be made by taking the first value and consider it as the offset of that variable.
secondary data analysis of the de-identified data set was the focus of this study. subjects had previously been recruited by harvard university according to harvard's irb protocols.
"in those tests, no temperature offset was considered. the validation datasets were collected the same day in between the calibration datasets. the results shown were about the external force estimation, but how this affects robot performance was not presented."
"the improvement in the measurements among the different estimation types is compared among the different kinds of possible calibration datasets to select the best way to improve the ft sensor performance. for comparison, results using the workbench matrix are included as an estimation type in its own. at a first stage, the different dataset types were compared on their own. to further test the robustness of the in situ estimation, a different set of calibration and validation datasets were collected."
"thus, p, the local probability distribution of variable x i is made conditional only on the value of the parent nodes, pa(x i ), as is always the case in bayes nets, since this is a defining assumption of using bayesian networks."
"there are advantages to both a one-stage and a two-stage process. as in typical in one-stage processes for a number of other statistical models and might be the case here, standard error estimations are often improved in a single stage. however, first, conceptually for learning scientists and assessment developers, it may be easier to think of the nested models separately, better preserving the flexibility desired of the representations. not fully integrating the calibrations also may help preserve moving nested structures in and out of the overarching model. if structures can be preserved, modified or removed in a more modular fashion, this reflects the role of understanding those cases when semi-amorphous patterns are found to effectively contribute information and when they do not. finally, many current software packages may have difficulty estimating models for a one-stage process at this time, while the two-stage process can readily be completed using a variety of familiar applications. this is called a \"low threshold\" advantage in the technology community. open access or widely available products for the learning science and developer communities mean there is a lower implementation threshold for a two-stage process at this time."
"note too that for the very small case of three nodes or less, probability estimates and \"beliefs\" from prior data sets may be helpful to learning scientists in establishing their decision rules even if a network structure is not needed. so there remains a role for an empirical basis based on \"prior beliefs\" at the smaller size as well."
"in this section, the developed in situ calibration method is described in detail. starting from the general mathematical model of the ft sensor to the problem statement that is solved through the model-based in situ calibration method."
the channel demodulation of a 16 channels fm modulated signal which contains an am-dsb-tc modulated signal was achieved using figure 5 . great care was taking in ensuring that the appropriate sampling rates was used at each decimation stages of the decimation process to avoid aliasing which will result to a wrong information being decoded. the different colours in figure 5 shows the different frequencies used at each stage. the 2.4mhz information signal (red) which was down converted to 480khz (green) was finally reduced to 96khz (blue) at the output (speakers).
"besides the quantitative evaluations of performance, an important qualitative behavior was observed when the measurements were used in high level controllers. during the experiments for the jerk control of floating base systems with contact-stable parametrised force feedback, it was observed that without the secondary calibration matrices the error in the estimated external forces was so big that the controller was unable to perform the experiments. using the secondary calibration matrices reduced the error to ±2.5n [cit] ) in the worst case. the error was low enough to successfully perform the experiments by using the regularization term."
"some results for the mirt-bayes analysis show in table 2 and figure 4 . a major feature to notice under mirt-bayes with the added item information from the network structures is that standard errors for the student estimates are improved over the prior mirt-only run. as described previously, neither the unit nor origin are necessarily the same between calibrations unless the model is anchored, otherwise sufficiently constrained, or rescaled to match, which was not done here. however, the standard errors and grain size of inference relative to the overall estimate performance range can be compared. as an example, consider the inquiry scale. previously under mirt alone for this data set, a range of four average standard errors (4x bins) was 2.85 logits for the inquiry scale, yielding approximately 3 unique proficiency levels of the 4x bins. under mirt-bayes, a range of four average standard errors is only 1.66 logits for inquiry. student estimates on inquiry ranged from a minimum of -3.33 logits to 3.35 logits, giving a range of 6.68 logits, so 4x bins (6.68 logits/1.66 logits) exceeded 4 unique proficiency levels."
"in the next section, the distortion of the dibrsynthesized view is shortly described. previous work on the quality assessment of the dibr-synthesized views and multi-scale image quality assessment is also shortly reviewed in section 2. in section 3, we describe two versions of the proposed multi-scale metric, based on two types of multi-resolution decomposition schemes, morphological pyramid, and morphological wavelets. description of the distortion computation stage and pooling stage of the proposed multi-scale measures is given also in section 3. the performances of mp-psnr and mw-psnr and discussion of results are presented in section 4, while the conclusion is given in section 5."
"these results are congruent with the space each dataset covers in the forces and torques 3d space. in figure 3, is possible to see that balancing non-support leg is more or less contained between the grid and balancing support leg. the other three dataset types are clearly different among them. therefore is possible to use that graphical representation to gauge the expected usefulness of a dataset. table 6 shows the best results by axis and the performance of the workbench matrix. from the difference in the results with respect to the workbench is possible to see that the most affected axis by the mounting are f x and f y . is possible to see that f z, τ x and τ y, actually perform better using the estimated offset. the fact that only the f y and f z get better results in both cases taking into account the temperature might imply these are the axis mainly affected by the temperature drift. figures 8 and 9 show the axes that improved the most and the least respectively by the calibration procedure. although the variation in lambda value is big, looking at figures 8 and 9 it can be seen that the difference among the best solutions is small."
"on the lower bound, the aggregation permutations of three nodes or less can usually be readily specified by learning scientists or subject matter experts in teas using traditional decision rules or rubrics, so a more extensive model is not needed."
"each of the lyrics was also annotated with positive, negative, and neutral polarities. in several cases, we observed that the mood class that was assigned to an audio is different from the mood class assigned to its corresponding lyric for some of the hindi songs. the statistics of annotation during listening to audio (l audio ) and reading of the lyrics (r lyrics ) are provided in table 1 ."
"the hypothesis investigated in this paper is whether mirt models can be extended to better handle complex tea data by embedding bayes nets results into the mirt data aggregation and model calibration. it is further hypothesized that if the hybrid model fits acceptably, the semi-amorphous data previously not possible to include in the mirt model alone might be successfully included. this could improve the measurement characteristics of the tea instrument, specifically to amplify the information available from the complex technology-enhanced assessment, and reduce the standard errors in the proficiency estimates. if these hypotheses are supported, the mechanism of improvement is expected to be through allowing more information that is already captured by the tea to be made available to include in the proficiency estimation model, which would have been eliminated previously."
"the gross error in the observation affects the state vectorx k through the gain matrix k k . therefore, the influence of the gross error on the state estimate is significantly reduced or even eliminated by using the prediction residual to constrain the gain matrix. based on huber's constructed equivalent weight function [cit], the equivalent gain matrix is constructed as:"
"many artifacts in the item analysis potentially can result from ignoring the intended structure of the data, including misfit for observables treated as items, sparseness for well-estimating item parameters, and lack of stability in parameters."
"a range of automated scoring techniques have begun to be explored for tea, for instance for collaborative problem tasks [cit] . mirt-bayes is a new approach. a two-stage estimation is employed here although one-stage could be possible (see discussion section 4.2 on choices for one-stage and two-stage statistical approaches for estimations)."
it has been shown that the local contrast in different resolutions can be easily represented in terms of haar wavelet transform coefficients and computational models of visual mechanisms were incorporated into a quality measurement system [cit] . experiments have shown that haar filters have good ability to simulate the human visual system (hvs) and the proposed metric is successful in measuring compressed image artifacts.
"to compare the performances of the image quality measures the following evaluation metrics are used: root mean squared error between the subjective and objective scores (rmse), pearson's correlation coefficient with non-linear mapping between the subjective scores and objective measures (pcc) and spearman's rank order correlation coefficient (scc). the calculation of dmos from given mos and non-linear mapping between the subjective scores and objective measures are done according to test plan for evaluation of video quality models for use with high definition tv content by vqeg hdtv group [cit] ."
"for both databases,it is shown that psnr shows very good agreement with human quality judgments when it is calculated at higher scales of mbp ed pyramid, much better than for the full resolution images in the base band. matlab implementation of psnr by morphological pyramid images is available online [cit] ."
"the assessments are used without direct prior instruction on the tasks, and are meant to show a model of how to supplement for instance state examinations. assessment data are captured by the environment as the student proceeds, including both logs of actions and activity completion associated with scores aligned to the measurement context. data generated are exported and analyzed for research studies."
the ft sensor measurements have a direct impact on the estimation of the zmp and as a consequence in estimation of the com. this information is used in a walking controller [cit] .
the scheme of this controller can be seen in figure 2 . description of the variables in the scheme can be found in table 1 . it is a pid controller with friction compensation. the feedback values are the estimated joint torques using the measurements from the ft sensor [cit] .
"for the irccyn/ivc dibr database, fig. 23, pearson's correlation coefficients calculated for wavelet subbands on decomposition levels 4-7 are higher than pearson's correlation coefficients calculated for wavelet subbands on decomposition levels 1-3. for the mcl-3d database, smaller differences by wavelet subbands between pearson's correlation coefficients can be noticed, fig. 24 ."
"dibr algorithms introduce new types of artifacts mostly located around disoccluded regions [cit] . they are not scattered in the entire image such as 2d video compression distortions. as dibr algorithms involve geometric transformations, most of them introduce mainly geometric distortions affecting edges coherency in the synthesized images. these artifacts are consequently challenging for standard quality metrics, usually tuned for other types of distortions. in order to better deal with specific geometric distortions in dibr-synthesized images, we propose multi-scale image quality assessment metric based on morphological filters in multi-resolution image decomposition. due to multi-scale character of primate visual system [cit], the introduction of multi-resolution image decomposition in the image quality assessment contributes to the improvement of metric performances relative to single-resolution method. introduced non-linear morphological filters in multi-resolution image decomposition maintain important geometric information such as edges on their true positions, neither drifted nor blurred, across different resolution levels [cit] . edge distortion between appropriate subbands of the multi-scale representations of the reference image and the dibr-synthesized image is precisely measured pixel-by-pixel using mean squared error (mse). in this way, areas around edges that are prone to synthesis artifacts are emphasized in the metric score. mean squared errors of subbands are combined into multi-scale mean squared error, which is transformed into multi-scale peak signal-to-noise ratio measure. more precisely, two types of morphological multi-scale decompositions for the multi-scale image quality assessment (iqa) have been explored: morphological bandpass pyramid decomposition in the morphological pyramid peak signal-to-noise ratio measure (mp-psnr) and morphological wavelet decomposition in the morphological wavelet peak signal-to-noise ratio measure (mw-psnr). morphological bandpass pyramid decomposition can be interpreted as a structural image decomposition tending to enhance image features such as edges which are segregated by scale at the various pyramid levels [cit] . using non-linear morphological wavelet decomposition, geometric structures such as edges are better preserved in the lower resolution images compared to the case when the linear wavelets are used in the decomposition [cit] . both separable and true non-separable morphological wavelet decompositions using the lifting scheme have been investigated."
"one major conclusion from the vpa results here is that students in the sample data set are distributed over a wide range of readiness to learn. also, due to the only moderate correlation between dimensions, students often show different readiness that teachers will need to address in inquiry skills and knowledge mapped here as compared to explaining."
"the new offset formulation is shown to give identical results as the centralized offset with the added benefit that the offset and calibration matrix are estimated simultaneously. the graphic representation of the sensor excitation in the 3d force and torque space has proven useful to provide intuitive insight into the comprehensive excitation of the sensor. in the figure 3, it is evident to see that z-torque type of dataset provides new information. it is also visible that the balancing non-support leg gives redundant information. this was confirmed when looking at the results grouping by a calibration dataset. considering it is possible to generate random movements of the robot and then evaluate if the dataset provides new useful data, this avoids the need to carefully design the joint trajectories for the calibration data. since the approach is model-based, robot simulations can easily provide this graphic sensor excitation representation. the use of the secondary matrix allows a simple non-intrusive way to provide the improved measurements to the robot. it was shown that by adding the temperature we are able to use the offset of the sensors as a constant. this proves that the drift is mainly generated by the temperature. furthermore, using offsets that were estimated a month in advance proves the robustness of the estimation of both calibration matrix and offset. it also demonstrates a higher reliability of the sensor measurements. the possibility to use a constant offset eliminates the need to estimate the offset before every experiment. this is especially useful for floating base robots that have a harder time anticipating the exact time of contact. it minimizes the preparation steps for using the robot and allows to do longer experiments without the need to stop. the improvement in robot performance is clear from the contact force coherence when switching or the fact that low and high-level controllers are able to perform better when using the ft measurements after in situ calibration."
"where n is the number of data samples in the dataset. the offset is usually estimated separately from the calibration matrix. because the offset can vary across different experiments due to temperature drift. the offset is removed from the raw measurements separately, and the calibration problem is reduced to:"
"each strategy of offset estimation is considered an estimation type. including temperature as a linear variable (wt) or not (nt) in the estimation are also considered different estimation types. if the temperature is considered, it is possible to take the first value as an offset (rto) or not (dto). considering the three offset removal possibilities, adding the temperature as a linear variable to each of them and the temperature offset option results in the following nine estimation types: the logic behind the estimation type names can be seen in figure 4 ."
"as future work, ways to account for nonlinearities will be explored. studying the dynamical response of the sensor is also interesting and might provide further improvements in the performance of the sensor."
"for exploration 1, which is the mirt alone analysis, first a unidimensional and then a multidimensional random coefficients multinomial logit model (mrcml; were fit to the 23-item data set using acer conquest generalised item response modelling software. a description of the scored items and two dimensions on which they load in the mirt model show in table 1 in the methods section."
"the first decade of 21st century witnessed the growth and popularity of music distribution in cds, dvds or other portable formats. another important change was also witnessed recently when the internet connectivity led to the rapid growth in downloading and purchasing of music online. the number of music compositions created worldwide already exceeds a few millions and continues to grow."
"in the generation of the mcl-3d database, four dibr algorithms are used: dibr with filtering, a1 [cit], dibr with inpainting, a2 [cit], dibr without hole-filling, a7 and dibr with hierarchical hole-filling (hhf), a8 [cit] . hhf uses pyramid-like approach to estimate the hole pixels from lower resolution estimates of the 3d wrapped image yielding to the virtual images that are free of any geometric distortions. adding the depth adaptive preprocessing step before applying the hierarchical hole-filling, the edges and texture around the disoccluded areas can be sharpened and enhanced. the results presented in sections 4.2 -4.4 for the mcl-3d database are based on the mixed statistics of four dibr algorithms a1, a2, a7, and a8. the original image shark and the left images from the stereopairs synthesized using four dibr algorithms (a1, a2, a7, a8) are shown on fig. 15 from top to bottom and from left to right."
"compared to other in situ calibration methods [cit], the model based in situ calibration method avoids the need to install other sensors or specialized structures on the robot to perform the calibration. this avoids effort and possible human errors introduced in the estimation of the position of the other sensors with respect to the ft sensor."
"the evaluation could be roughly divided in three parts: one to observe the results of each estimation type, another to check the expected improvement on the robot of the generated calibration matrices and a third to verify the impact of using the contributions in a real robot. the sensor to calibrate is located near the hip of the left leg of an icub robot."
"the preparation of an annotated dataset requires the selection of proper mood taxonomies. identifying an appropriate mood taxonomy is one of the primary and challenging tasks for mood classification. mood taxonomies are generally categorized into three main classes namely, categorical, dimensional, and social tags [cit] ."
"the synthesis process changes the pixels position in the synthesized image and induces new types of distortion in dibr-synthesized views. view synthesis noise mainly appears along object edges. typical dibr artifacts include object shifting, geometric distortions, edge displacements or misalignments, boundary blur, and flickering. incorrect depth map induces object shifting in the synthesized image. object shifting artifact or ghost artifact manifests as slight translation or resize of an image regions due to depth map errors. a large number of tyny geometric distortions are caused by the depth inaccuracy and the numerical rounding operation of pixel positions. geometric distortions appear in the synthesized images because the pixels are projected to wrong positions. blurry regions appear due to inpainting method used to fill the disoccluded areas. incorrect rendering of textured areas appears when inpainting method fails in filling complex textured areas. when the objects move, the distortion around edges is more noticeable. the view synthesis distortion flickering locates on the edge of the foreground object which has a movement. flickering can be observed as significant and high-frequency alternated variation between different luminance levels [cit] . the temporal flicker distortion is the most significant difference between the traditional 2d video and the synthesized video. some of the typical artifacts due to dibr synthesis are shown on fig. 1 ."
the importance of analyzing images at many scales arises from the nature of images themselves [cit] . scenes contain objects of many sizes and these objects contain features of many sizes. objects can be at various distances from the viewer. any analysis procedure that is applied only at a single-scale may miss information at other scales. the solution is to carry out analysis at all scales simultaneously. psychophysics and physiological experiments have shown that multi-scale transforms seem to appear in the visual cortex of mammals [cit] .
"a typical sequence before using the robot, involves removing the offset when only one contact with the environment exists. this offset estimation requires the information from the mass vector. this can be imposed in a known robot configuration or measured using an imu. this offset is then subtracted from the measurements. there are three possibilities to estimate the offset:"
"it was observed that the feature selection improved the performances of the mood classification systems [cit] . thus, the important features were identified from the audio and lyrics using the feature selection technique. the state-of-the-art mood classification systems achieved better results using the support vector machines (svms) [cit] . thus, the libsvm implemented in weka tool 10 was used for the classification purpose. we 10 http://www.cs.waikato.ac.nz/ml/weka/ performed 10-fold cross validation in order to get reliable accuracy."
"posterior distributions show how the given evidence gathering pattern is associated with probability of success on frog observable, see figure 3 . the result is called the \"belief\" given the \"evidence.\" obtaining the belief given the evidence is called \"belief updating\" and it can occur over a variety of semi-amorphous variables depending on the network structure."
"at the same time, additional data collection or hand-scoring was not required in any of the bayes items to achieve the improved performance characteristics. all the observation evidence in both frog bayes and tadpole bayes had been previously captured but not used, and scoring was based on automated scoring."
"over the centuries, music has shared a very special relationship with human moods and the impact of music on moods has been well documented [cit] . we often listen to a song or music which best fits to our mood at that instant of time. naturally, such phenomenon motivates the music composers and singers and/or performers to express their emotions through piece of songs [cit] . it has been observed that people are interested in creating music library that allows them to access songs in accordance with their moods compared to the title, artists and/or genres [cit] . further, people are also interested in creating music libraries based on several other factors e.g., what songs they like/dislike (and in what circumstances), time of the day and their state of mind [cit] etc. thus, organizing music with respect to such metadata is one of the major research areas in the field of playlist generation. recently, music information retrieval (mir) based on emotions or moods has attracted the researchers from all over the world because of its implications in human computer interactions."
"in tea, bayes nets alone are perceived by many content developers as desirable to use in technology-enhanced context because, conceptually, the networks are easy to implement and flexible. content developers like the way bayes net software visually represents complex tea assessment designs. they also like that teas can be made almost arbitrarily more complex to meet the perceived creative and content needs of the delivery innovators, simply by dropping in more nodes and arcs to the bayes net visualizations."
"the cop is defined as the point where the resultant force can be exerted with a zero resultant moment. when the contact is with a flat ground the cop and zmp, can be calculated as :"
"first, for the new frog network, posterior probabilities for the frog evidence node were calculated. for instance for frogs, a student could observe the yellow and red frogs but not the other evidence, for a given posterior probability."
"it is possible that is not easy to change the current calibration matrix of the sensor. in these cases, the proposed solution takes the form of a secondary calibration matrix. the secondary calibration matrix is the required transformation of the current calibration matrix to the new calibration matrix. it requires the knowledge of the current calibration matrix used by the sensor. it is calculated as follows:"
"in this case, the offset is calculated taking some samples of the test experiments in which is known the robot is on one foot and not moving or moving slowly. the offset calculation includes not only the forces but also the temperature if coefficients are available."
"the harvard vpa project assesses middle school students' science inquiry and process skills. the vpas are delivered using technology that \"has the look and feel of a videogame\" [cit] . the new frog vpa from which the data set was drawn for this study is a serious role-playing game in which students take on the identity of an avatar who is confronted with the dilemma of a six-legged frog appearing in a farming community. acting as virtual scientists, students walk around the environment, explore, \"speak\" to other characters such as virtual farmers in the region who respond with text via decision rules, gather data, and attempt to solve the scientific puzzles."
f x (n m) f y (n m) f z (n m) e x (n m) e y (n m) e z (n m) double
to understand better the behavior of the estimation types three comparisons are done. the first uses the mean square error (mse) calculated between the force-torque data using the new calibration matrix and the model-based estimated data. a lower value would indicate a better fitting of the data. mean square error (mse) of each axis is calculated as follows:
the test performed consist in switching from single support to double support. if the offset is calculated with the robot standing on one foot the sequence is:
"technology today offers many new opportunities for innovation in educational assessment through rich new tasks [cit], 2008, potentially powerful automated scoring [cit], innovative reporting [cit], and real-time feedback mechanisms [cit] ."
"the end result is that relevant information is lost in many teas if potentially meaningful patterns are reduced more than necessary, or when eliminated entirely because the salient semi-amorphous data is deemed too difficult to include in the statistical estimation of the measurement model."
"(4) the gain of the kalman filter k k is computed by equation (14). based on the kalman filter, the equivalent gain matrix k k is calculated to achieve robust estimation by equation (28), while the state vectorx k is calculated by equation (31)."
"thus the mirt-bayes approach effectively aggregates more of the available evidence in the technology-enhanced assessment, for this data set, and allows a finer grain size of inference, without requiring additional data to be collected or using more student or teacher time. the impact of improved measurement is most evident at the extremes, as can be seen in figure 4, since this is where the least measurement information was available previously."
"under mirt-bayes model with the added item information from the network structures, standard errors for the student estimates are improved over the prior mirt-only run. thus the mirt-bayes approach based on this data set effectively aggregates more of the available evidence in the technology-enhanced assessment, as predicted in the study hypotheses. lowering the standard error allows a finer grain size of inference without requiring additional data to be collected or using more student or teacher time for the assessment."
"the estimated offsets can be seen in table 5 . it shows that taking into account the temperature offset changes the results of the offset estimation. the estimated offsets without temperatures are not very different between them. something similar can be seen for the offset obtained considering the temperature offset. in contrast, the offset including temperature, but neglecting the temperature offset, has considerably different behavior between the sphere and the other types."
"exploration 2: mirt-bayes hybrid model. for exploration 2, the same new frog data set was used but with information added though two bayes net structures, frog observation and tadpole observation, incorporated to amplify the assess- ment information. this added two items to the original data set, since each bayes net explored a pattern and delivered a score for pattern of semi-amorphous data. a third item on six-legged frog observation was also added, see information on this below. to summarize, semi-amorphous information was added about whether respondents observed various frogs, tadpoles and water samples available in the simulation."
"where δ k j is kronecker delta function. the basic principle of the ukf is to take sigma points according to the ut in the original state distribution. therefore, the mean and covariance of these points are equal to those of the original state distribution. then, these points are substituted into the nonlinear function to get the transformed mean and covariance [cit] . the standard ukf implementation for state estimation is given as follows."
"the wright map is empirically generated by the analysis, and is based on the actual student data in the new frog vpa data set. the two sets of x's in the left columns of the figure show vertical histograms (histogram turned on its side) of student performance for the sample group. lower performing students appear with x's at the bottom of the map and higher performing with x's at the top of the map. there is one histogram per each dimension, with inquiry on the left and explanation on the right."
"the multidimensional model will be used for the rest of exploration 1. the reliability was high for the new frog vpa on each dimension, with various indicators ranging from reporting a reliability of .82 to .89, and an overall cronbach's alpha of .88. high reliability on both dimensions with the mirt model helps give empirical justification for reporting separate student proficiency scores in each area."
"the multimodal mood annotated dataset (lyrics and audio) was developed for research in music mood classification of hindi songs. the automatic music mood classification system was developed from the above multimodal dataset and achieved the maximum f-measure of 68.6%. the different moods were perceived while listening to a song and reading the corresponding lyric of song. the main reason for this difference may be that the audio and lyrics were annotated by different annotators. another reason may be that the moods are transparent in audio as compared to lyrics of hindi songs. later on, we intend to perform deeper analysis of the listener's and reader's perspectives of mood aroused from the song."
"we have investigated psnr performances by wavelet subbands at different wavelet decomposition scales. the reference image and the dibr-synthesized image are decomposed into a sets of lower resolution subbands using morphological wavelet decomposition. at each decomposition scale, for each wavelet subband, psnr is calculated between the subbands of the two wavelet representations, the reference image wavelet representation and the dibr-synthesized image wavelet representation. pearson's correlation coefficient (pcc) of psnr to subjective scores is calculated for each subband for three types of morphological wavelets: minhaar, minlift and minliftq. matlab implementation of psnr by morphological wavelet subbands is available online [cit] ."
"when assumptions well hold, however, the bayes nets give typically a sparser and more parsimonious structure than use of the multiplication rule alone for full nodal structures. justification for the parent node arrangement of the network structure here in this paper is made in part by employing only very small bayes nets. these have been identified for a salient pattern relative to the construct by learning scientists employing techniques such as content analysis of student pattern results on prior data to generate the bayes networks, and to provide the relationships between parent and child nodes."
subsections below include descriptions of the data set and information on the procedures used in the study. this includes description of the assessments used and how they were delivered.
"the research design is as described above: a comparative study of model fit statistics and proficiency estimate results, with and without the hybridization of the mirt model with the bayes nets. subjects were observed through data collection in a technology platform used in school settings. use of the mirt model alone is exploration 1. use of the hybridized mirt-bayes model is exploration 2."
"in most cases the dynamic behaviors of icub are obtained through high-level controllers. they contain many tuning parameters that affect the behavior of the robot. the measurements of the sensor are used mainly in an indirect way by the high-level controllers. therefore, finding quantitative measures of the improvement in the dynamic motions of the icub caused by the in situ calibrated sensor is a challenging. nonetheless, from the uses of the ft sensor on icub, three quantitative methods were used:"
the performances of the metrics mp-psnr and mw-psnr are evaluated using two publicly available databases which contain dibr-synthesized images: the irccyn/ivc dibr image database [cit] and part of the mcl-3d stereoscopic image database [cit] .
"to address the problem of unknown noise statistics in the dynamic and measurement models, different adaptive kalman filters have been studied, such as maximal post-filter statistics based on sage-husa, an adaptive filter based on variance component estimation [cit], multiple-model-based adaptive estimation (mmae) [cit], innovation-based adaptive estimation (iae) [cit], and residual-based adaptive estimation (rae) [cit] . in addition, an adaptive kalman filter based on the attenuation memory method and the limited memory method was proposed to solve the divergence problem of the classical kalman filter [cit] . for the sage-husa method, to achieve unbiased estimation, measurement and system noise cannot be estimated simultaneously, and the positive or semipositive definite values of the noise matrices cannot be guaranteed, which can cause filter divergence [cit] . for the iae and rae methods, the adaptation is applied directly to the covariance matrices of the measurement and system noises in accordance with the difference of the observation residual or innovation sequence. to realize these methods, the innovation or residual vectors of a certain history window must be known, causing an increment in the storage burden, and the width of the moving window must also be known [cit] . in the attenuation memory and the limited memory method, it only relies on the adaptive attenuation factor for adjustment and can easily make the filter diverge [cit] . additionally, if the observations contain gross errors, the filter cannot work effectively. the gross error control strategies include outlier detection and robust estimation [cit] . in terms of robust estimation, the equivalent weight scheme based on the m-estimation [cit] has been widely used. the commonly used robust estimation methods include the huber method [cit], minimal one-norm estimation (l 1 estimation), minimal p-norm estimation (l p estimation) [cit], the danish method, the hampel method [cit], the institute of geodesy and geophysics (igg) method [cit], and other. in recent years, many studies have focused on the robust kalman filter based on the m-estimation. the robust kalman filter is widely used in global navigation satellite system (gnss) positioning and navigation. yang proposed an adaptive robust kalman filter that combines the adaptive factor and the institute of geodesy and geophysics iii (iggiii) equivalent weight [cit] . chang proposed an adaptive method with fading memory and a robust method with enhancing memory in the kalman filter [cit] . yang proposed an adaptive robust kalman filter based on a global positioning system (gps)-dead reckoning (dr) integrated navigation system to give more actual and reliable parameter estimation of the maneuvering vehicles [cit] ."
"the robot was tuned to the maximum gain value in which the robot is able to perform the balancing demo in a satisfactory way. beyond a certain value, the robot is observed to vibrate and fall. for this test, the high level gains of the controller are kept the same and only the low-level controller gains are changed. a higher gain value indicates that the robot is able to rely more on the measurements of the sensor as feedback. thus, being able to use higher gains is better if it does not introduce unstable behaviors."
"the multimodal mood classification system achieved the maximum f-measure of 68.6% after adding all the audio and lyrics (excluding the ts features) features using libsvm. to the best of our knowledge, there is no state-of-the-art multimodal mood classification system available for hindi songs. [cit] performed multimodal mood classification of western songs using both audio and lyric features and achieved 98.3%, 86.8%, 92.8%, and 91.7% for angry, happy, sad and relaxed mood classes, respectively. they made the classification much easier by classifying one mood class at a time, i.e. for the first time they classified \"angry\" or \"not angry\" and so on. hu and downie [cit] achieved 67.5% for multimodal mood classification using late fusion on a dataset of 5,296 unique songs comprising of 18 mood classes. our multimodal mood classification system outperforms the above system by 1%."
"formally, letting the latent traits define a two-dimensional latent space with the dimensions allowed to be non-orthogonal, the respondent's position in the space is represented by the vector,"
"furthermore, only one mirt model was employed in this study, since it was well-fitting to the data here. it was extended with the bayes nets. however, other mirt models might be employed and should be investigated, including more parameterized models. future work might explore anchoring well-fitting items according to the mrcml model here and treating any less well-fitting networks, if they arise, with a more parameterized irt model. the generalizability, or external validity, of the findings is also in question when an assessment is based on a single performance task. student performance in this one context may not be representative of measuring a fuller science inquiry/exploration construct. additional measures would need to be incorporated to consider such concerns as content validity and other assessment material sampling concerns over the educational framework. this was not the purpose of this modeling study, but would have important relevance in actual operational assessment settings."
"the confusion matrix for the multimodal mood classification system is also given in table 6 . from the confusion matrix, it is observed that there is biassness in the classification system towards the nearest classes. there were confusions in between the mood class pairs such as \""
"separable 2d discrete wavelet transform (dwt) is implemented by cascading two 1d dwt along the vertical and horizontal directions [cit] producing three detail subbands and approximation signal. separable wavelet decompositions using 1d morphological haar wavelet (minhaar) and 1d morphological wavelet using minlifting scheme (minlift) [cit] are explored. their linear counterparts, haar wavelet and biorthogonal wavelet of cohen-daubechies-feauveau (cdf (2,2)) [cit], are also tested for comparision."
"the standard kalman filter requires the system and measurement noise to be zero-mean white noise. owing to the complexity of the marine environment, gross errors inevitably exist in the underwater acoustic observation [cit] . therefore, robust estimation is usually used to reduce the influence of gross errors on positioning results."
"for each source, three views are used for the calculation of the metric score, fig. 14 . original textures (t1, t2, t3) and their associated depth maps (d1, d2, d3) are obtained by selecting key frames from each of nine multi-view test sequences associated with depth maps. from the middle view (t2, d2), using one of the four dibr algorithms, the stereoscopic image pair (sl, sr) is generated. the textures from the outer views, (t1, t3) are used as the reference stereo pair. we have calculated iqa metric score between the dibr-synthesized stereopair (sl, sr) and the reference stereopair (t1,t3). the score for the stereo pair is calculated as the average of the left and right image scores."
"the comparative analysis of different mood taxonomies revealed that the clustering of similar mood adjectives has a positive impact on the classification accuracy. based on this observation, we opted to use russell's circumplex model of affect [cit] by clustering the similar affect words located close to each other on the arousal-valence plane into a single class as shown in figure 1 . we considered the mood classes angry, calm, excited, happy and sad for our experiments. each of the classes contains another two nearby key affect words of the circumplex model of affect."
"text stylistic features have been used effectively in text stylometric analysis such as authorship identification, author identification [cit] . these features have also been used for mood classification from lyrics of western music [cit] . the ts features such as the number of unique words, number of repeated words, and number of lines etc. were considered in our experiments. the detailed list of ts features along with their descriptions is given in"
"there are two physical laws at play in strain gauge force sensors. one is common to all kinds. it is the relationship between the deformation of a spring and forces, it is the hooke's law of elasticity."
"a similar process generated posteriors for the tadpole observable network. after all patterns were generated and results were inspected, the observational water evidence (lab and red) was determined to add little useful information while the water evidence node did through the water evidence question directly, so the network was adjusted accordingly."
"the validation datasets were taken on two different days, both different from the day the calibration datasets were collected. this was done to test the robustness to possible different ambient conditions. the datasets and their temperatures are showen in table 2 . the calibration datasets were grouped into:"
"this increases test taking time for respondents as more data then has to be collected to provide a reliable score given that so much useful but semi-amorphous information is discarded as hard to model. furthermore, certain types of information, e.g. the \"semi-amorphous\" data, are systematically discarded and replaced with more direct responses. this may not as well represent the construct, potentially skewing the estimates, and certainly does not take full advantage of the tea medium. the same is true for partial credit scores, whereby learning scientist find that after accumulating more than a few pieces of salient but semi-amorphous tea data the myriad permutations of patterns become overwhelming."
"more broadly, hybrid modeling approaches of nesting bayes nets within irt, or conversely irt with bayes nets, have been suggested as potentially complementary of each other for next-generation measurement models in complex technology-enhanced contexts. this is because strengths and limitations of the two approaches can tend to offset each other in demanding contexts. together, or blended, they may offer potential, as shown here."
"a bandpass filter which completes the complex envelop detection is then used to recover the channel. resampled and filtered to help remove the direct current components in the form of noise and produce a high fidelity audio signal which is at approximate sampling rate to the original signal. the three spectrum analysers were used to monitor the received signal, the demodulated multiplexed signal and the output of the filter. the decimations carried out in the simulation was done in two stages. the first decimation configuration reduced the frequency from 2.4mhz to 480khz while the second decimation at a factor of 5 reduce the sample rate further to audio level of 48khz. both were configured as a frame based channel allowing multirate processing. the decoded signal information is affected by induced noise. the information in the audio signal could not be decoded initially because it was a mix of high frequency components of 16 different audio frequencies. the peaks at various points in figure 6 shows the fm sidebands at their carrier frequencies having maximum value of carrier power. frequency analyzer shown in figure 7 indicates that channel 16 has been correctly received. others channels can be decoded by adjusting the parameters of the model correctly. it is also very important to say that the observed audio signals were not crystal clear as a result of band limit of 5 khz between channels and noise due to spectrum analyzers. the modulation of a given audio signal onto the decoded channel above is realized at 80khz, representing the centre frequency of the channel. figure 8 was used for this analysis. the input to the am modulation model is the dsp multimedia block used to import the given audio signal. the signal interpolated using fir filter at a factor of 4 is presented in figure 9 . the mono 48khz, 16-bit audio signal, having a 1024 samples per audio channel and of sample based data type was interpolated using fir filter. the fir resamples the signal and performs lowpass filtering thereby preventing aliasing of higher frequency components. the cosine function introduced generated the carrier signal. it is configured to have the same sample time and frequency with the modulating signal. this is to ensure that there is no phase shift between the two signals. the mathematical implication of this is shown in figure 10 and eq. 1. to amplitude modulate the given signal (t), it is superimposed on a carrier signal (t), to produce the am signal (t). (t) is the frequency of the local oscillator usually made of crystal."
"feature level correlation is used to identify the most important features as well as to reduce the feature dimension [cit] . thus, the correlation based supervised feature selection technique implemented in weka toolkit was used to find out the important contributory features for audio and lyrics. a total of 431 audio features were extracted from the audio files using jaudio. a total of 12 sentiment features, 12 textual stylistic features and 5832 n-gram features were also collected from the lyrics. the feature selection technique implemented using weka yields 148 important audio features and 12 sentiment, 8 stylistic, and 1601 n-gram features from lyrics. we subsequently use these features for our classification purpose."
"considering possible errors during the calibration procedure, linear regression is the most suitable approximation function. multi-axis force-torque sensors usually contain multiple strain gauges, each of them can be seen as a separate sensor. because of this, multiple regression is a valid option for this kind of sensor. therefore, each force axis will be calibrated using the information from all strain gauges,"
"posteriors and expectancies for the final network were calculated, and then merged into the original data set as two partial credit items to represent the frog observable and tadpole observable networks. as well one new dichotomous item was added to the data set tracking whether or the not the six-legged frog itself was observed. (this was not considered semiamorphous data but possible to treat alone as a response item because the overall goal of the simulation was understanding the six-legged frog appearance in the farming community.) consequently from the first stage of the mirt-bayes process, three new items were added to the original 23 in the new frog data set."
"full-reference objective image quality assessment metrics, vsqa [cit], and 3dswim [cit], have been proposed to improve the performances obtained by standard quality metrics in the evaluation of the dibr-synthesized images. both metrics are dedicated to synthesis-related artifacts without compression-related artifacts and both metrics are tested using irccyn/ivc dibr images dataset. vsqa [cit] metric dedicated to view synthesis quality assessment is aimed to handle areas where disparity estimation may fail. it uses three visibility maps which characterize complexity in terms of textures, diversity of gradient orientations, and presence of high contrast. ssim-based vsqa metric achieves the gain of 17.8 % over ssim in correlation with subjective measurements. 3dswim [cit], relies on a comparision of statistical features of wavelet subbands of the original and dibr-synthesized images. only horizontal detail subbands from the first level of haar wavelet decomposition are used for the degradation measurement. a registration step is included before the comparison to ensure shifting- fig. 1 typical artifacts due to dibr synthesis. original images are in the left column and synthesized images are in the right column resilience property. a skin detection step weights the final quality score in order to penalize distorted blocks containing skin-pixels based on the assumption that a human observer is most sensitive to impairments affecting human subjects. it was reported that 3dswim metric outperforms the conventional 2d metrics and tested dibr-synthesized views dedicated metrics."
"moreover, it is experimentaly shown that psnr has very good agreement with human judgment when it is calculated for the subbands at higher morphological decomposition scales. we propose the reduced versions of morphological multi-scale measures, reduced mp-psnr, and reduced mw-psnr, using only detail images from higher decomposition scales. the performances of the reduced versions of the morphological multi-scale measures are improved comparing to their full versions."
"because of the complex marine environment, the it is difficult to accurately obtain the system noise, and the acoustic observations usually have gross errors, which influence the positioning accuracy of the classical ukf. based on the sage-husa filter, an adaptive robust ukf algorithm is proposed in this paper. the following conclusions can be drawn from our analysis, computation, and comparison."
"note that this strong assumption employed by bayes nets requires that the network structure be well validated as compared to alternative structures that might be proposed, which could potentially have quite different sets of direct parent nodes. even if much of the network includes the same or similar nodes and is broadly but not specifically similar, then substantial difference in estimation of score for the pattern can result."
"multi-scale image quality assessment (iqa) framework can be described as three-stage process. in the first stage, both the reference and the distorted images are decomposed into a set of lower resolution images using multi-resolution decomposition. in the second stage, image quality/distortion maps are evaluated for all subbands at all scales. in the third stage, a pooling is employed to convert each map into a quality score, and these scores are combined into the final multi-scale image quality measure score."
"automatic music mood classification systems were developed based on some popular audio features like spectral, rhythm and intensity. such features have been used for developing several audio based music mood classification systems in the last decades [cit] . among the various audio based approaches tested at mirex, spectral features were widely used and found quite effective for the mood classification of western songs [cit] . the emotion in music task 3 [cit] at mediaeval benchmark [cit] . in the above task, the arousal and valence scores were estimated continuously for every music clip in a time frame of 0.5 seconds with the help of several regression models [cit] ."
"for the mcl-3d database, pearson's correlation coefficients of psnr versus mos for pyramid images at all pyramid scales using structuring elements of different sizes are shown on fig. 20 . for this database, smaller differences between pcc for pyramid images at different scales exist. the smallest pcc is at the first scale (detail images d 0 ) and the highest pcc is for the aproximation images at the highest scale. the best pyramid image psnr performances for different sizes of se used in morphological pyramid decomposition are shown in table 2 ."
"disadvantages of the more standard operational irt models, when used stand-alone and not in a hybridized model, are also evident for teas. the irt models, which are latent variable models, often do not offer the flexibility and representation that many tea designers seek for their innovative content. additionally and critically for the topic of this paper, irt often encounters difficulty with the inclusion of semi-amorphous tea data. used as item level data, indicators are likely to show inadequate model fit along with well-documented dependency issues (li, y., bolt, d. m., & fu, j. 2006; [cit] . however, aggregating with traditional irt testlet or item bundle models encounters the issues discussed previously in the introduction section, such as misrepresenting the intended data structure."
"(1) he unknown or imprecise system noise can cause the divergence of the classical ukf, so this paper proposes an adaptive ukf based on the sage-husa filter. no matter how much the initial system noise is given, the adaptive ukf can accurately estimate the system noise through the time-varying noise estimator. at the same time, the adaptive ukf can avoid the problem of a negative definite value of the system noise variance matrix based on the eigenvalues judgement."
"as in most other areas of image processing and analysis, multi-resolution methods have improved performances relative to single-resolution methods also for the image quality assessment. pyramids and wavelets are among the most common tools for constructing multi-resolution signal decomposition schemes used in image processing and computer vision. both redundant image pyramid representation and non-redundant image wavelet representations have been explored for multi-scale image quality assessment metrics."
"according to equations (18) and (20), the sage-husa filter based on the time-varying noise estimation can theoretically obtain both the system and measurement noise matrices. however, the deviation of residual vector v k affects the calculation ofq k,r k simultaneously, easily leading to filter divergence. hence, equations (18) and (20) cannot be used simultaneously. [cit], 20, 60 5 of 16 ukf algorithm studied in this paper only adjusts the system noise using equations (17) and (18) . the covariance matrix in equation (10) is rewritten as"
"in order to further verify the adaptive ukf, the ultrashort baseline data of the towed body is tested and analyzed for underwater positioning. the towed body refers to the marine integrated survey instrument, including the multibeam sounder, the sub-bottom profiler, and others. the towed body is regarded as the auv for the real data experiment. [cit] . the experimental ship is equipped with a global acoustic positioning system (gaps) ultrashort baseline unit, a differential global positioning system (dgps), a gyrocompass, a sound velocity profiler (svp), and an ins. the towed body equipped with the transponder is mounted behind the experimental ship. the position information of the ship can be obtained by the shipboard dgps and the position of the ship-bottom transducer array can be obtained through coordinate transformation [cit] . data such as the attitude, heading, and speed of the ship are obtained through the gyrocompass and ins. the position of the underwater towed body can be gained in real time through the gaps ultrashort baseline acoustic positioning system. the mean sound speed measured by the svp is 1530 m/s. the sampling interval of the ins is 0.01 s and the sampling interval of the gaps is 1 s. the complete motion trajectory of the experiment is shown in figure 5, with the red line indicating the position of the transducer and the blue one indicating the position of the transponder. the positioning accuracy is evaluated by the position difference of the abovementioned algorithms and the output value of the instrument, which can be used as references."
"however, while semi-amorphous data in teas do represent a set of observables, they have some characteristics differing from items in traditional \"item bundles\" or testlets in the prior measurement literature [cit] . while sharing issues of dependency well explored in the literature [cit], traditional bundle structures typically involve strongly organized and predesigned item groups, with well-defined respondent interactions taking place. for instance, a typical traditional example of a testlet or item bundle might involve a reading passage or math problemsolving situation, with stimulus material shared across a set of items, and all respondents answering a sequential set of subsequent questions."
"according to levy, other new approaches are in their infancy when it comes to their application as measurement models in larger assessment enterprises. yet current irt-based approaches used operationally may not always cover the full need of teas and new construct challenges. so hybridization routes seem important to investigate."
"for mood classification using lyrics, the linear kernel was selected and the classification was performed by adding features one by one. initially, the experiment was performed using only sentiment features, and then added other features subsequently. the maximum f-measure of 55.1% was achieved using the sentiment and n-gram features for five class mood classification of hindi songs as shown in table 5 . we also annotated each of the lyrics with positive, negative, and neutral polarity in addition to five mood classes. the maximum f-measure of 69.8% was achieved for polarity classification system using the sentiment and n-gram features of hindi song lyrics."
"where v k, v k are the prediction residual vectors without gross error and with gross error, respectively. the gross error is fully reflected in the prediction residual, so the state vector iŝ"
"finally, the experiments were performed using both audio and lyric features. again, we used the linear kernel of libsvm for the classification purpose. the ts features reduced the performance of the systems, thus these were excluded while developing the final multimodal system for mood classification of hindi songs. the multimodal mood classification system achieved the maximum f-measure of 68.6% after adding all features for hindi songs and the system performance is given in table 5 ."
"the overall inter-annotator agreement scores with five mood classes were found to be 0.80 for hindi lyrics. however, the inter-annotator agreement was around 0.96 for the lyrics data while annotating with positive, negative, and neutral polarity."
"the forces acting on a moving robot can be separated into two categories: forces exerted by contact and forces transmitted without contact (mass and, by extension, inertia forces). the cop is linked to the former, and the zmp to the latter. nonetheless, it has been shown that both points coincide [cit] . therefore, is possible to use the contact force information to calculate dynamic quantities such as the zmp and by extension affect the estimation of the center of mass (com)."
"the advanced 3d video (3dv) systems are mostly based on multi-view video plus depth (mvd) format [cit] as the recommended 3d video format adopted by the moving picture experts group (mpeg). in the 3dv system, smaller number of captured views is transmitted and greater number of views is generated at the receiver side from the transmitted texture views and their associated depth maps using depthimage-based rendering (dibr) technology. dibr techniques can be used to generate views for different 3d video applications: free viewpoint television, 3dtv, 3d technology based entertainment products, and 3d medical applications. the perceptual quality of the synthesized view is considered as the most significant evaluation criterion for the whole 3d video processing system. reliable quality assessment metric for synthesized views is of a great importance for the 3d video technology development. the use of subjective tests is expensive, time consuming, cumbersome, and practically no feasable in systems where real-time quality score of an image or video sequence is needed. objective metrics are intended to predict human judgment. the reliability of objective metrics is based on their correlation to subjective assessment results."
"1. introduction software defined radio (sdr) is a computer based software transceiver scanner that makes use of terrestrial standards in the modified digital video broadcasting television (dvb-t) principles [cit] . this is achieved through compression and conversion of signal's in-phase/quadrature phase (i/q) components into a wideband software to enable high frequency reception and processing [cit] . it is a paradigm shift from a traditional hardware analogue or digital based system of radio transceiver to a software based characterized by signal processing tool boxes and commands. it main function is to perform all waveform processing like modulation, channel coding, filtering on a computer system with high speed processing like interpolation, decimation, digital up and down conversion in a processor. the implication of this is that the border between the analogue and digital transceivers is being pushed closer to the radio frequency by adopting analogue to digital (adc) and digital to analogue conversions (dac) close to the antenna. matlab is one of the engineering software that have many sdr toolboxes. the major difference between sdr and other radios is that sdr is dynamic in frequency spectrum usage (see figure 1 ). it has an ungraded crystal, capacitors and inductors when compared to other radio devices."
"such differences were analyzed from the perspectives of both listeners and readers. we studied various problems of annotation and developed two mood classification frameworks for hindi songs based on the audio and lyric features, separately. further, a multimodal mood classification framework was developed based on both audio and lyric features of hindi songs. the results demonstrate the superiority of a multimodal approach over a uni-modal approach for mood classification of hindi songs."
a recent method for exploiting force-torque sensing is to use the estimated contact force as feedback to a high-level jerk controller of floating base systems with contact-stable parametrised force feedback [cit] .
"additionally, the frog observable network, composed of semi-amorphous data previously discarded, exceeded the maximum item discrimination of .72 seen by any other item in the prior mirt only run. as well, tadpole bayes at .71 nearly exceeded all others, with the exception of only one item in the prior mirt run. thus the bayes network additions to the mirt model appear to be highly informative, and adding the semi-amorphous data is shown here to have good utility."
"applied to a simulation-based data set from harvard's virtual performance assessments, the first investigation using the mirt model alone showed the viability of using this approach for aggregating data in the harvard new frog vir-tual performance assessment. the second investigation, using mirt-bayes, showed additional improvement including reduction of the standard error of measurement compared to use of mirt alone, for the simulation data set example. especially when considering extremes of the proficiency scale, salient information difficult to include in the mirt model directly was recouped through the nested bayes network structures. at the same time, the two-stage approach preserved interpretability by keeping nested structures small and bounded within the irt framework."
the evaluation of dibr views synthesized from uncompressed data using standard image quality metrics has been discussed in literature for still images from fvv in 2d context [cit] using irccyn/ivc dibr image database. it has been demonstrated that 2d quality metrics originally designed to address image compression distortions are very far to be effective to assess the visual quality of synthesized views.
"this multidimensional extension assumes two or more latent traits are to be estimated from the data [cit] . in this case there will be two traits estimated, scientific inquiry and scientific explanation, from the mrcml statistical modeling of the individuals' responses. theta estimates form the mrcml are provided as the student proficiency estimates on each dimension, which are generated for each dimension for each student, according to the mrcml model applied here through the conquest software package [cit] ."
"the objective function and gradient calculation of equation 1 correspond to simple matrix vector products yet on large did sets d ij . hence, fo is a memory bound problem [cit] . the arithmetic load of the cpu does not limit the runtime of the algorithm but the transportation of the dose influence data d ij from the main memory to the cpu, which typically amounts to 100 mb -1 gb for a single beam ensemble. on state-of-the-art multiprocessor workstations the speed of data transportation from the main memory to the cpus is non-uniform across the ram; every cpu has a dedicated memory location (node) with minimum access time. we apply a thread node binding strategy to ensure that cpus only access did from their preferred node. the did of every candidate beam is distributed equally to all nodes. hence, we guarantee an ideal load balancing for every potential beam ensemble that might be evaluated during bas. furthermore we sort the did according to the access pattern during the objective function and gradient calculation in order to minimize the overall data transportation of base data between ram and cpu [cit] . efficient cache memory filling is ensured by always processing chunks of 64 bytes, which corresponds to the standard cache line size, at the same time. we use the openmp standard for parallelization in c++. the bas framework is implemented on an amd opteron workstation with 4 cpus with 12 cores each and 128 gb main memory. besides a small overhead required for the optimization, the entire main memory is available to store precomputed did."
we have presented an ultra-fast fo implementation for bas that requires about 15-70 min to compare 1 000 treatment plans. this enables an efficient evaluation of different bas strategies for clinical application [cit] .
"an efficient application of gpus in the context of bas is currently not possible. due to the limited memory available on gpus, the did of all candidate beams cannot be loaded with a single gpu. an efficient application of gpu clusters is infeasible due to synchronization and load balancing issues. futhermore gpu clusters are very expensive; the amd workstation we applied for this study is available at us$ 4500."
"even though our implementation relies on the standard quadratic objective function [cit], the acceleration concepts are generally applicable to treatment planning approaches that use a matrix vector product for the dose calculation. this includes alternative objective functions, multicriteria optimization, and direct aperture optimization, among others."
"automated beam angle selection (bas) incorporating fluence optimization (fo) requires repeated fos in order to compare different beam ensembles. relying on commercially available treatment planning software, which may require a couple of minutes per fo, however, would result in prohibitive computation times. in order to accelerate fo, bas approaches including fo often compromise regarding the resolution in the dose deposition space [cit] or regarding the number of evaluated beam ensembles [cit] . furthermore, the number of considered candidate beam orientations is often restricted [cit] because precomputed dose influence data (did) of all candidate beams has to be handled in higher memory."
"we have recently investigated the structure of the combinatorial bas problem [cit] and the performance of different bas algorithms for intracranial imrt [cit] . therefore, we have developed an ultra-fast bas framework operating on the same resolution like clinical planning systems, considering up to 1 400 candidate beams, and evaluating up to 10 000 beam ensembles per patient. here, we present a detailed description of the underlying fo algorithm and the hardand software implementation of our bas framework."
"in this study, we have used an amd workstation with 128 gb main memory in order to cope with the did of all candidate beams. however, our implementation can also be used to accelerate fo on conventional personal computers [cit] ."
"bas strategies including fo usually comprise two independent modules, as depicted in figure 1 . a bas module controls which beam ensembles b i will be evaluated by the fo module in iteration i. in subsequent iterations i + 1, the bas module suggests new beam ensembles based on the preceding objective function values f (b i ). choices for bas modules are manifold and include simulated annealing [cit], genetic algorithms [cit], and iterative search strategies [cit], among others. usually, the runtime of the bas module is orders of magnitude shorter than the runtime of the fo module. hence, accelerating the entire bas algorithm is only possible by accelerating the fo."
"is the transmitted modulated symbol in the k -th sub-carrier as described in equation (1). to recover the transmitted symbol, we can apply single-point frequency-domain equalization to each sub-carrier. for example, using zf equalization, the desired signal can be denoted as"
"with the \"or\" connector (i) one or more paths may be followed as a result of the decision; (ii) each event, or combination of events, will give rise to the function."
"in the shipping process, a single hr needs to manage all physical activities (e.g., drop battery into the truck) and informational (e.g., take the s-manif) activities in the process. in both cases, the information used is mostly paper-based. on the other hand, there is no means to verify that the type of batteries that is being picked corresponds to the one being ordered. the smooth running of these processes heavily depends on the judgment of the hr involved. for example, there is no mechanism to know for sure that that the batteries that are being shipped are those featuring in the manifesto. this situation often leads to a shipment mismatch, and thus increasing transportation costs as the truck driver needs to bring back the shipment and reschedule a new one, which also affects customer service delivery. resolving this situation is one of the goals of rfid technology adoption by the tpl management board when it comes to batteries management."
"the \"and\" connector allows (i) the flow of the process branches into two or more parallel paths; and (ii) all the events must occur in order to trigger the following function."
"as can be seen in figure 1, the 2d model edge is projected to the image plane using the prior homography of the planar object. instead of tackling the line segment itself, we sample the projected line segment (black solid line in figure 1 ) with a series of points (brown points in figure 1 ). then the visibility test for each of the sample points is performed, since some of these sample points may be out of the camera's view field. for each of the visible sample points, 1d search along the normal direction of the projected model line is employed to find the edge point with the strongest gradient or closest location as its correspondence. finally, the sum of the errors between the sample points and their corresponding image points is minimized to solve for the homography between frames subsequently."
"in the literature [cit], it is proved that the conditional density of the projection of the 3d model line l given its 6 journal of sensors observed noisy image line segmentl is only dependent on the noise perpendicular to line l:"
"from (25), it can be seen that the error model allows us to encode the measurement error for image edge point ( ) explicitly and obtain the intuitive impact of image line length. moreover, long line segments produce more accurate location than shorter ones and small produces higher confidence about the line location."
"in this scenario, which is considered as the best rfid implementation scenario by the tpl firm, the rfid baseline infrastructure is build in the suppliers facilities (e.g., for the product tagging, rfid information exchange, etc.), which is then gradually extended to the rest of the supply chain stakeholders. with this scenario, all batteries are now rfid-enabled prior to their shipment to the tpl focal firm warehouse and the rest of the supply chain, and thus increasing the level of supply chain electronic integration and the rfid network externalities. new activities when using rfid figure 6 . the shipping process with incremental change when using rfid by contrasting this scenario with the current picking process ( figure 3 ) and shipping process (figure 5 ), the following observations allowed us to analyze the impact and understand the resulting opportunities. now the picking and shipping processes are performed jointly as a single process and involve only one hr (in opposition to 3 hrs before), all paper-based activities being now cancelled, redesigned or automated. this single process is triggered by an electronic picking order (e-pick) sent by the customer is to the tpl wms and erp (this is the case of inter-organization process and is integration). more precisely, the following steps can summarize this transformation scenario:"
"symbol timing is one of the key factors in ofdm synchronization, which decides the accurate selection of fft window starting position for ofdm demodulation. as shown in fig. 3, the ideal synchronization position of symbol timing is the first sample of the transmitted signal removing the cp. if the fft window is opened at the permitted zone, the received signal will produce no inter-symbol interference (isi), only inducing the common phase rotation on demodulated symbols. while the initial location is selected outside the permitted area, it inevitably will produce the isi, thus resulting in the inter-sub-carrier interference (ici). assuming over the awgn channel, if the symbol timing position is captured in the permitted zone, then the output demodulation signal on the ' k -th sub-carrier of the m -th ofdm symbol is expressed as"
"where γ(⋅) is the projection function which takes the camera parameters and the 3d line segment and returns the corresponding edge in the image., are the intrinsic and extrinsic parameters of the camera in the image, respectively. (⋅) denotes the conditional density."
"camera calibration has always been an important issue in the field of computer vision, since it is a necessary step to extract metric information from 2d images. the goal of the camera calibration is to recover the mapping between the 3d space and the image plane, which can be separated into two sets of transformations. the first transformation is mapping of the 3d points in the scene to the 3d coordinates in the camera frame, which is described by the extrinsic parameters of the camera model. the second one involves mapping of the 3d points in the camera frame to the 2d coordinates in the image plane. this mapping is described by the intrinsic parameters which models the geometry and optical features of the camera. in general case, these two transformations can be expressed by the ideal pin-hole camera model."
"with regards to rfid technology, its characteristics such as multiple tags items reading, more data storage capability and data read/write capabilities and no line of sight may act as facilitators of its widespread adoption and usage, while its perceived complexity and cost may constituted the inhibitors factors."
"in order to meet the emerged new requirements, many powerful and implementationefficient transmission schemes have been proposed for the standardization of the latest and future communication systems. for the downlink transmission, the ofdm scheme has been widely accepted due to its high spectral efficiency and flexible resource allocation. for the uplink transmission, however, the power efficiency is particularly critical for mobile terminals with the restriction on the transmission power and power consumption. therefore, the papr performance becomes one of the most important criterions in selecting the transmission scheme for the uplink. from this point of view, the single-carrier based frequency division multiple access (sc-fdma) scheme is favoured by future wideband wireless communications. in fact, one kind of sc-fdma schemes, i.e., dft-s-ofdm, is accepted as the uplink basic transmission scheme by the 3gpp-lte standard and incoming 3gpp-lte-advanced standard [cit] . in order to reduce the papr, the dft-s-ofdm scheme utilizes the dft spreading processing on the transmitted constellation symbols before ofdm modulation. by this way, the modulation method can be view as a dft-based interpolation processing, and the modulated signals can be regarded as single carrier signals."
it is assumed that the two random vectors s and h are statistically independent. and then we can approximate the conditional density ofl given m as
"our approach exploits the line/edge features of the handy objects to calibrate both the internal and external parameters of the camera, since they provide a large degree of stability to illumination and viewpoint changes and offer some resilience to hash imaging conditions such as noise and blur. a first challenge of the solution proposed in this paper is to automatically estimate the homography and establish the correspondences between model and image features. in this sense, we redesigned the model based tracking method [cit] to robustly estimate homography for the common planar object in the clutter scene. an advantage of such methods is handling the occlusion, large illumination, and viewpoint change. with a series of homography from the planar object to the image plane, the initial camera parameters can be solved linearly. a second challenge is to optimize the camera parameters by developing effective object function and by making full use of the finite nature of the observation extracted in the images. in this paper, the error function for the model and image line, which encodes the length of the image line segment and the information of the midpoint, is derived from the noisy image edge points in the least square approach."
"the rest of this paper is structured as follows. section 2 presents a review of the literature on the value chain model for value creation through an integrated flow of materials and information, the importance of information technology in the value chain, outbound logistics in the tpl industry and the role of rfid technology as enabler of warehouse process optimization. section 3 presents the research methodology and a canadian tpl supply chain studied. section 4 presents the results obtained and discusses the lessons learned. finally, section 5 provides a conclusion including our research limitations and future research directions."
"(1) when the e-pick is received by the tpl wms, based on business rules in the system, an automatic message with the e-pick is sent to the dedicated forklift terminal for picking. (2) when the message is received, the picking clerk goes to the dedicated rack display on the forklift terminal to pick the requested number of batteries. (3) as soon as the picking clerk drives through the shipping dock equipped with an rfid reader (rfid portal), an automatic reading of all rfid tags is performed, followed by a linking of the data collected from the tags to the shipping order for automatic match validation. (4) if there is a mismatch, owing to the business rules that are configured in the rfid middleware, an automatic message is sent to the clerk to stop the process for further verification. otherwise, a set of operations are performed in parallel (e.g., automatically send an e-asn or update inventory into the wms, etc.) while placing the forklift into the truck to drop the batteries. (5) once the batteries are dropped into the truck, the truck driver leaves the tpl facility to bring the batteries to the shipping destination while the location-based system (lbs) is automatically initiated for the \"in-transit\" visibility."
"in order to further investigate the stability of the proposed method, we vary the number of lines from 4 to 23. the results are shown in figure 12 . and recovered by the proposed method are around the values estimated by the corners based method only with a small deviation. the reprojection errors for the projected method decrease significantly from 4 to 17. when the number is above 17, the reprojection error is very close to that of the corners based method."
"as the main objective of this study is to improve our theoretical and practical understanding of the impact of rfid technology in real-life contexts, the research design clearly fall into the realm of exploratory research. [cit] in a tpl supply chain following two main phases. [cit] defined the case study as \"a research strategy which focuses on understanding the dynamics present within single settings\" (p. 534). this research strategy allows to focus on emerging phenomena and eventually induce theories [cit], and is recognized by many researchers as an appropriate approach to answer research questions such as \"why\" and \"how\" things are done [cit], and is therefore suitable to study the impact of rfid technology on the warehouse processes of a tpl industry, where research and theory are at their early and formative stages [cit] ."
"in the last two image sequences, the covers of two books are chosen as the model planes, respectively. to validate the performance of the proposed homography tracking method, the books are put in the clutter environment with the smartphone undergoing large rotation and translation. [cit] images. figure 10 exhibits some sampled results."
"when noise is present in the measuring data, we denotep as the noisy observation of the projection of the 3d points p andl as the noisy observation of the projection of 3d model line l."
the motion in the image is related to the twist in model space by computing the partial derivative of the normal distance with respect to th generating motion at current homography:
assume that we have a current estimation of the homography h . the posterior homography h +1 can be computed from the prior homography h given the incremental motion δh:
"in an economic context where the growing development of information technologies has been generating unprecedented repercussions on the management of activities within warehouses, it goes without saying that the use of a centralized inventory management, for example, could lead to an increased productivity and short response times of the warehousing systems, and that \"shorter product life cycles will imposed a financial risk on high inventories and, therefore, on the purchase of capital intensive high-performance warehousing systems\" [cit] (p. 519)."
the derivation of the interaction matrix for the proposed approach is based on the distance between the projection of sample point p and its projected image pointp . the motion velocity of the object is then related to the velocity of these distances in the image.
"as can be seen from the figure, the theoretical result using the exact expression agrees well with the simulation results. this clearly shows that the exact expression for calculating the effective sinr in equation (29) can be used in order to assess the effect of the carrier frequency offset accurately. c. turbo coded ber performance of cfo effect in this section, we will focus on the ber performance in the presence of cfo for the coded dft-s-ofdm system. due to the mathematical complexity of the iterative turbo decoding algorithm, the analytical derivation of the ber of turbo codes is not available. to simplify the analysis, the ber of coded system in awgn channels can be approximated by an expression of the form [cit] coded exp( )"
"an \"event\" describes the situation before and/or after a function is executed, and may correspond to the post-condition of one function and act as a pre-condition of another function."
"in practice, the corners detection often suffers from a failure, when the angle between the model and image plane is large or when some of the corners are invisible or corrupted by the image noise and blur. however, the edge detection is more stable in such case. moreover, in the simulated experiments, since the line segment is fitted by the corners lying on it, the proposed method provides almost the same performance with the corners based method. in our homography tracking framework, much more image edge points corresponding to the sample model points are utilized, and therefore the line segment can be fitted with higher accuracy. in addition, the proposed method is more flexible and suitable for the general user of the smartphone who wants to take the vision task, since it only uses the common and handy planar object rather than the prepared planar pattern."
"where ξ is the timing offset. however, if the symbol timing position is outside the permitted zone, for example delay ξ samples, then the output demodulation signal on the ' k -th sub-carrier of the m -th ofdm symbol can be denoted as ( ) where the second item on the right side is for ici, and the third part is for isi. as shown in fig. 4, the constellations of the demodulated signals are divergent besides the phase rotation. effect of carrier frequency offset in ofdm system, the existed cfo will lead to frequency shift of the received signal. if the offset of the sub-carrier frequency is integral multiple of sub-carrier spacing, the orthogonality among sub-carriers is still maintained, just with a shift relative to the subcarrier for the data symbols. however, if the frequency offset is a fractional sub-carrier spacing, the ici will be introduced. for ofdm systems composed of a large number of subcarriers, sub-carrier bandwidth is relatively much smaller compared with the channel bandwidth. therefore, the small amount of frequency offset will result in substantial ber performance degradation."
"representation of activities in a network of multiple organizations through the alternative use of events, functions and connectors to specify the routing logic based on the required decision rule [cit] ( table 1 ). the use of the \"aris toolset\" in the project enables all stakeholders involved to understand the impacts of integrating rfid technology on business processes both at the focal firm level and at the network level. moreover, the epc formalism is considered as a viable means to study a collaborative e-business process [cit], and more recently, the impacts of rfid technology on sc-enabled business process transformation [cit] . and finally in the second phase, two most plausible scenarios of rfid-enabled supply chain were being chosen, assessed and discussed in the rfid solution provider laboratory by key project stakeholders through the \"living laboratory\" approach. then, one scenario is retained to be implemented and monitored during a pilot project in the focal firm warehouse. in fact, \"living laboratory\" approach was planned to support various research settings, including the simulation of business experiments and the use of the laboratory over an extended time by all key rfid project stakeholders for \"self-trial\" learning, joint problem solving, interaction, knowledge generation and exchange among all key project stakeholders [cit] ."
"ofdm plays a significant role in modern broadband communication systems. the wireline high-speed access technology, i.e. asymmetric digital subscriber line (adsl), was the first widely used application for the ft-based ofdm system. several other wireless standards, such as the ieee 802.11a wireless local area network (wlan) and ieee 802. 16 (wimax) series, have adopted ofdm as a key transmission technology. ieee 802.20 working group on mobile broadband wireless access uses ofdm as the wireless high speed transmission technology. in the area of cellular mobile communications, ofdm was also adopted as a basic downlink transmission scheme of 3gpp-long-term evolution (3gpp-lte) standard and the incoming 3gpp-lte-advanced standard [cit] . ofdm is also widely applied in the areas of audio and video broadcasting. digital audio broadcasting (dab), [cit] s, adopts coded ofdm as the transmission technology. dvb-t based on ofdm in an 8 mhz channel is now a popular technology for terrestrial video broadcast in the world. an additional new application area of ofdm is in ultra-wideband (uwb) personal area networks [cit] ."
"where m is the total number of sub-bands, 0 k is the user-specific sub-band offset, r is the repetition factor. for dft-s-gmc system, both distributed and localized mapping policy could be supported, which is corresponding to r greater than or equal to one respectively. after sub-bands mapping, an ifbt is performed on the data sequence, yields the output signal, i.e., the ifbt symbol, as"
the remainder of the paper is organized as follows. section 2 gives the procedure of the proposed camera calibration algorithm. section 3 presents an overview of the redesigned homography tracking method based on edge model. section 4 derives the error model between image and model lines and expresses the problem of the camera calibration in the probabilistic formulation by the maximum likelihood approach. section 5 details how to solve the problem of camera calibration by the nonlinear technique. some experiment results are given in section 6.
where is the number of 3d mode points. it is clear that proposed approach can obtain the maximum likelihood estimation of the homography by minimizing the sum of the square of normal distances.
"the measurement noise for the localization of the 2d line segments can be decomposed into two components: noise perpendicular to the line and noise along the length of the line. the first noise is modelled as a gaussian random variable related to orientation error and the noise model has been derived in the last section, whilst the second one is assumed to conform to any distribution (not necessarily gaussian) related to line fragmentation."
"in the \"as-is\" shipping process is triggered by the shipping manifesto, and followed by the physical picking of the batteries from the shipping staging to the truck by the shipping clerk. in the case of the shipping process with incremental change when using rfid, the following steps can summarize the execution of the process ( figure 6 ): (1) when the p-pick (paper-based picking list) and the n-tags (number of rfid tags required in the picking list) are received by the second picking clerk (picking-clerk-2), he drives the forklift to the dedicated picking rack so as to pick the requested number of batteries and move the loaded forklift to the shipping staging area where he manually attaches the rfid tags to the batteries (figure 7 (a) ). once the tagging process is finished, he drives the forklift through the shipping dock equipped with an rfid reader (rfid portal) (figure 7(b) ). (2) as soon as the picking clerk drives through the shipping dock equipped with an rfid reader (rfid portal), an automatic reading of all rfid tags is performed, followed by a linking of the data collected from the tags to the shipping order for automatic match new activity when using rfid validation (figure 7 (c) ). (3) if there is a mismatch, owing to the business rules that are configured in the rfid middleware, an automatic message is sent to the clerk to stop the process for further verification. otherwise, a set of operations are performed in parallel (e.g., automatically send an e-asn, automatically update inventory into the wms, automatically generate and send the manifesto, etc.) while placing the forklift into the truck to drop the batteries."
"model plane. this subsection investigates the influence of the orientation of the model plane with respect to the image plane. in the experiment, three images are used with two of them similar to the last two planes in section 6.1.1. the initial rotation axis of the third plane is parallel to the image plane, and the orientation of the planes is randomly chosen from a uniform sphere with the rotation varying from 5 ∘ to 75 ∘ . the noise level is fixed to 0.5 pixels. the results are displayed in figure 8 . best performance seems to be achieved with the angle around 40 ∘ ."
"as shown in figure 2, the line segment m and its projection m in the image plane are represented by their endpoints (p 1, p 2 ) and (p 1, p 2 ). the line segments m and m lie on the journal of sensors 5 infinite lines l and l, respectively. the perspective projection of 3d line segment can be given by the projection of its two endpoints:"
"the first scenario of the rfid-enabled warehouse process optimization (section 4.1.1) required the rfid tagging of products in the tpl warehouse. as for the next phase, it assumes that this tagging activity is conducted in each supplier's facilities, and represents, from the tpl firm perspective, the best scenario of rfid adoption in the tpl supply chain (section 4.1.2). however, the suppliers were unwilling to make the initial rfid investment required. in consequence, the best scenario for the warehouse process optimization could not be chosen for implementation. instead, the first scenario was implemented and monitored in the pilot project at the tpl warehouse. figure 3 presents the current picking process (\"as-is\"), while figure 4 presents the incremental redesign process of the same process when using rfid technology (\"to-be\"). figure 5 presents the current (\"as-is\") shipping process and figure 6 the incremental redesign of the shipping process integrating rfid technology (\"to-be\")."
"mapping figure 8 describes the time-frequency property of dft-s-ofdm signal. for dft-s-ofdm signals, the time-domain waveform can be viewed as a dft-based interpolation of transmitted constellation symbols. therefore, the energy distribution within one dft-s-ofdm symbol keeps the same transmission order of the constellation symbols in the timedomain. in the frequency-domain, due to the dft based spread spectrum processing, the spectrum of each transmitted constellation symbol is distributed on all occupied subcarriers, i.e., each sub-carrier contains only a part of spectrum component of the transmitted constellation symbol, which is substantially different form ofdm signals."
assume that we have a current estimation of the rotation r at the time of . the posterior rotation r +1 can be computed from the prior rotation r given the incremental rotation exp(̂):
"as presented in the table 2, the cm performance of frequency-domain implemented dft-s-gmc transmitter is very close to that of time-domain implemented dft-s-gmc transmitter. moreover, the cm of dft-s-gmc is smaller 1.7 and 1.1db than that of ofdm for qpsk and 16qam modulation respectively. as shown in the table 3, with frequency-domain implementation structure, the computation complexity of dft-s-gmc transceiver with equalization can be reduced significantly, compared with that with time-domain implementation structure. for 28 and 1 sub-band(s) transmission, the computational complexity can be reduced about 47% to 66%."
"in the first phase or the rfid technology opportunities exploration, both qualitative and quantitative data were collected through multiple sources such as several interviews, three focus groups, multiple on-site observations, rfid workshops, organizational documents and rfid technical papers in order to understand the dynamics within the supply chain. for example, on-site observations allowed us to map existing processes, interviews with operational staff and managers on \"how and why things are done?\" provided more exhaustive information that can help to solve possible inaccuracies in our mapping. furthermore, a business process analysis tool called \"aris tools\", which is based on the event-driven process chains (epc) formalism [cit], was used to understand, represent, and \"map\" the existing intraand inter-organizational processes into a current model (as-is), and therefore identify bottlenecks and areas of opportunities when using rfid technology. using this current model as a guide, we performed an analysis of business and technological information requirements together with key stakeholders involved in the warehouse project. in addition, the tool enables managers involved in the project to make decisions about the management of rfid-enabled business processes by looking for example, the impact of the technology on resource utilization, is-integration and information flows. this analysis enabled us to generate various plausible scenarios of business process optimization, integrating rfid into the warehouse processes. these optimization scenarios were discussed with the stakeholders during rfid workshops. table 1 . elements of epc model [cit] in fact, \"aris toolset\" allows a global definition, mapping, analysis, optimization and implementation of business processes. this formalism offers a logic"
"based on the analysis of the current processes (\"asis\" picking process in the figure 3 and \"as-is\" shipping process in figure 5 ) and on the incremental change when using rfid (\"to-be\" picking process in figure 4 and \"to-be\" shipping process figure 6 ), the following observations are made: (i) the picking process involves two human resources (hr) and the shipping process one human resource; (ii) the main activities of the hr in the picking process (picking-clerk-1) include the processing of information (e.g., manually print the picking order (e-pick) that is used in this process, while the second hr deals with physical activities in the process (e.g., picking the quantity of battery on the p-pick), and generates and prints the manifesto (smanif) marking the end of the process."
"from equation (44), the dft-s-gmc symbol is formed by cyclically accumulating several ifbt symbols in time domain, and each ifbt symbol has a srrc waveform. therefore, as shown in fig. 15, the spectrum of each sub-band has a raised cosine function shape in frequency-domain. moreover, the sub-band spacing is specially designed and differs from any conventional filter-bank systems in that a certain guard band is inserted between neighbouring sub-bands. by adding some guard band between the sub-bands, the nearorthogonality between neighbouring sub-bands is guaranteed, which further simplifies the detection algorithm greatly. meanwhile, interferences among successive symbols for each sub-band could be easily mitigated in the receiver because of the shift orthogonality of prototype filter and the narrow-band single carrier transmission. similar to dft-s-ofdm systems, dft-s-gmc systems also exploit dft based spreading among sub-bands. therefore, each sub-band contains only a part of spectrum component of transmitted constellation symbols, and transmitted signal over all occupied sub-bands can be viewed as single-carrier signal as a whole. with proper designed prototype filter, the frequency response of the prototype filter for each sub-band can be simplified such that the number of total selected values for all subbands is equal to the number of total tones q. by this way, the signals for each sub-band can be even mapped to the tones directly and exclusively, and the simplified implementation structure is shown in fig.17 . as shown in fig. 17, by tone mapping, rather than summation processing, the implementation complexity of dft-s-gmc transmitter can be significantly reduced, and the performance loss is very limited as illustrated by the following simulation results. fig.19 for the dft-s-gmc receiver with mmse and zf sc-fde respectively. over the same channel condition, with mmse equalization, dft-s-gmc receiver achieves higher sinr than that with zf equalization in the low snr range and with wider band transmission, due to the noise enhancement effects of zf sc-fde. the simulation specification is shown in table 1 . as shown in the fig.20, the ber performance of frequency-domain implemented dft-s-gmc transceiver is almost the same as that of time-domain implemented dft-s-gmc transceiver."
the proposed algorithm has been tested on simulated data generated by the computer and real image data captured from our smartphone. the closed-form solution is yielded by the approach [cit] except that the homography matrices are estimated by the proposed method. the nonlinear refinement within the irls algorithm takes 5 to 8 iterations to converge.
step 4. estimate the camera parameters by minimizing the sum of the distance between the finite image line segments and the model lines in the maximum likelihood approach.
"throughout this paper, the perspective projection model is utilized. the relationship between a 3d world p and 2d image point p can be given as"
where is the objective function that measures the disparity between the actual image observations and their corresponding predicted ones by the current camera parameters. s corresponds to the distances from the endpoints of the image line segment to the projected model line.
"the picking process with the incremental change from rfid technology is similar to the current (\"asis\") picking process with a new no-value activity, which is \"manually print the number of rfid tags (ntags)\" -that the first picking clerk (picking-clerk 1) needs to pick the order using a rfid printer. in fact, because the suppliers of the tpl firm are not yet involved in the project, the tpl needs to conduct the tagging process. now, not only is the second picking clerk responsible for picking the quantity of battery on the p-pick to bring them in the shipping staging area, but he is also in charge of all shipping activities. indeed, because the integration of the rfid infrastructure with the firm's erp and wms allows the automatic validation and generation of the manifesto, all picking activities related the manifesto are now transferred to the shipping process and are automated all together, and thus increasing the level of electronic integration between the picking and the shipping process. one implication of this new level of electronic integration is that the picking and the shipping processes should henceforth be carried out when the truck is ready to carry the shipment to the remote sites, which therefore increases the use of the warehouse staging area."
"with the \"xor\" connector (i) one, and only one, of the possible paths will be followed; (ii) one, and only one, of the possible events will give rise to the function."
"this experiment examines the performance of the proposed method with respect to the number of the lines utilized to recover the camera parameters. for our method, more than 4 lines should be employed. we vary the number of lines from 4 to 25. three images of the model plane are also used with the same orientation and position as last subsection. 100 independent trials are conducted with the noise level fixed to 0.5 pixels for each number of the lines. the results are shown in figure 7 . when more lines are used, the errors decrease. in particular, from 4 to 15, the errors decrease significantly."
"then according to the localized allocation pattern, the signal is converted into a timedomain signal by n -point idft processing. n is greater than k . after the cp padding, the time-domain signal, i.e., the sc-fdma symbol, can be written as ( ) ( )"
"it is clear that these two noises are negatively correlative. since the observation noises conform to gaussian random variables, the joint density for the random variables 1 and 2 is a gaussian pdf, which can be given by"
"defined as a wireless automatic identification and data capture (aidc) technology [cit], radio frequency identification (rfid) is emerging as a new interorganizational system (ios) that will transform supply chain business processes and practices [cit] . despite this high potential of rfid as an enabler of supply chain transformation, the related literature shows a lack of empirical studies that provide support for the enabling impact of rfid technology in improving outbound logistics activities such as warehousing, order fulfillment, transportation, and distribution, which are performed by third-party logistics (tpl) firms in supply chain management. this study intends to bridge this knowledge gap, by drawing on an earlier study on the rfid research agenda [cit], as well as on a longitudinal real-world case study in a tpl supply chain. in this regard, this study examines the following three research questions: 1. how are business processes and work systems changed due to rfid at all points in the value chain?"
"a tpl service provision is defined as \"a relationship between a shipper and a third party which, compared with the basic services, has more customized offerings, encompasses a broad number of service functions and is characterized by a long-term, more mutually beneficial relationship\" [cit] (p. 35). today the tpl industry appears as a viable solution to the market globalization, increased competition, cost pressures and an increasing use of outsourcing [cit] . the growing importance of the tpl industry is highlighted by the results of multiple survey studies. [cit] found that, among more than 1,500 logistics and supply chain executives, almost 82% of the respondents were using tpl services. similarly, in a more recent study, [cit] found that 70% of firms in north america, 73% of firms in europe, and 75% of firms in asia pacific were using tpl services for their warehousing activities. in addition, tpl is more and more considered as a strategic tool for the it-enabled supply chain to reduce environmental uncertainty and improve logistics management efficiency. indeed, it capabilities play a vital role in achieving the integration of logistics services provided by tpls. for example, [cit] found that 91% of tpl users will outsourced their web-enabled communications and visibility tools while 83% of them will do the same as regards warehouse/distribution center management. finally, the tpl industry is viewed as a lead user of rfid technology."
"where m is the modulation order, and snr is the average received signal to noise ratio. fig. 6 shows simulation results of the impacts of the cfo on the ber performance of ofdm system with qpsk modulation over the awgn channel."
"in this experiment, we investigate the performance of the proposed method with respect to the number of the images of the model planes. in the first three images, we use the same orientation and position of the model plane as those used in the last subsection. for the following images, the rotation axes are randomly chosen in a uniform sphere with the rotation angle fixed to 30 ∘ and the positions are randomly selected around [−105, −145, 2000] . the number of the model plane images ranges from 3 to 17. at each number of the images, 100 independent trials of independent plane orientations are generated with the noise level for the image points fixed to 0.5 pixels. the errors including the relative errors in camera intrinsic parameters and the reprojection errors for the two methods are shown in figure 6 . the errors decrease when more images are used. from 3 to 7, the errors decrease significantly. moreover, the reprojection errors of the proposed method are around 0.7, when the number of the images is varying."
where the components of s are the distances between the endpoints ofl and m along the direction perpendicular to m. the components of h are the distances between the endpoints of the two line segments along the direction of m.
"in this chapter, the principle, implementation structure, time-frequency property of three fourier transform-based transmission systems, namely ofdm, dft-s-ofdm and dft-s-gmc, are presented for broadband wireless communications. for ofdm systems, the spectrum of each sub-carrier has a sinc-function shape, spectrums of all sub-carriers are independent each other which cause high papr of transmitted signal; for dft-s-ofdm systems, each sub-carrier contains only a part of spectrum component of transmitted constellation symbols, and the time-domain waveform can be viewed as a dft-based interpolation of transmitted constellation symbols, which bring in lower papr of transmitted signal; for dft-s-gmc systems, each dft-s-gmc symbol is formed by cyclically accumulating ifbt symbols with srrc waveform in the time domain, hence, the spectrum of each sub-band has a raised cosine function shape, and due to dft based spreading among sub-bands, the transmitted signal over all occupied sub-bands can be viewed as single-carrier signal as a whole. moreover, the effects of time and frequency offset on ofdm and dft-s-ofdm systems are analyzed quantitatively. theoretical analysis and simulation results show that except the first symbol, all other demodulated symbols of dft-s-ofdm system have a better sir than that of ofdm system under the same cfo condition. furthermore, the post-processing sinr of dft-s-ofdm and dft-s-gmc are addressed for different equalizer. the closed-from expressions of sinr are presented and verified by the simulation results."
step 2. fit the image line segment from the image edge points obtained by 1d search along the normal direction of the corresponding model line.
"the focal firm is a canadian-owned medium-size tpl service provider, with annual revenue of nearly us$23 million and 52 full-time employees and owns a large distribution centre in canada and warehouse facilities in the united states of america (usa). the company provides a variety of services such as storage, transport and customs clearance fees. its canada-based distribution centre, where the study was conducted, is used to store telecommunications batteries from various suppliers. the company is in charge of (i) shipping new batteries to different customers' remote sites based on their needs, (ii) collecting used batteries, and (iii) bringing them to the recycling plant. it relies on set of it systems to conduct its intra-and interorganizational business processes -which range from the very basic practices (e-mail, fax, and paper-based system) to the sophisticated ones (bar code systems, inhouse warehouse management system, rfid-enabled transport management system, b2b web portal). in addition, the firm often uses the canadian postal services to communicate with its supply chain stakeholders. however, the firm primarily uses its paper based system to track and trace batteries analyzed in this study. nevertheless, the tlp firm is already using rfid technology. [cit], the firm had its first experience with the rfid technology when it started providing a \"slap & ship\" rfid solution service to its customers in order to meet rfid mandates from their trading partners. later in the same year, the firm started deploying a rfid-enabled truck tracking solution so as to have better visibility on its \"truck in transit\". according to the management board of the tpl company, the use of rfid technology as an enabler of telecommunications stationary batteries management is a \"logical next step\" in their it innovation process for value creation."
"we now present and discuss the results of the rfid implementation in relation to the tpl warehouse process optimization and the future rfid-enabled warehouse optimization that is plausible, but has not been implemented largely because of the lack of stakeholder investment."
"where 1, 2, and 0 correspond to the distances from the two endpoints and midpoint of the image line segmentl to the projected model line l . it is clear that the error function between 3d model line and 2d image line is weighted by the length of the image line segment."
"as can be seen in figure 3, both the projection of the 3d line segment m and its noisy observationl are represented by their endpoints, (p 1, p 2 ) and (p 1,p 2 ), receptively. the noise vector s perpendicular to the line and the noise vector h along the line are expressed as follows:"
"in this paper, the camera calibration problem can be formulated in terms of a conditional density function that measures the probability of the image observations predicted from the camera parameters given the actual image observations. this section describes how to construct this conditional density function."
"besides, an analysis of time required to perform activities related to the tracking and tracing of batteries in the picking and shipping processes shows that in the case of incremental scenario (s1), the total time of the picking and shipping processes is decreased by 17% compared to current situation (\"as-is\"). but with an increase of around 133% for the picking process mainly due to the introduction of no value added activities such as manually print the number of rfid tags and manually apply them to the batteries. in the second scenario (s2), we have an improvement of 83% compare to the \"as-is\" situation (table 2 ). however, this high level of savings can only been achieved if the rfid tagging process is done by the supplier."
"in this paper, we have investigated the possibility of camera calibration using common and handy planar objects undertaking general motion for the smartphone vision tasks. a linear algorithm supported by the edge model based homography tracking is proposed, followed by a nonlinear technique to refine the results. both the computer simulated and real images have been utilized to validate the proposed algorithm. the experimental results exhibited that the proposed algorithm is valid and robust and provides more flexible performance than the widely used planar pattern based method. in addition, for the general user who will do vision task, the prepared planar calibration may be not always in hand. however, the common items in our daily life almost have the standard size and planar structure. by exploiting the edge information, we proposed an easier and practical camera calibration method. moreover, in the proposed method, the uncertainty of the image line segment was encoded in the error model, which takes the finite nature of the observations into account. the problem of camera calibration using lines was formalized in the probabilistic framework and solved by the maximum likelihood approach."
"by the properties of the fourier transform, the spectrum of each sub-carrier has a sincfunction shape in frequency-domain. namely, the fourier transform of () . this means although the spectrum of sub-carriers are overlapped each other, the orthogonality among each sub-carriers can be maintained when the sub-carrier spacing is set to be f δ ."
rfid to merge the existing picking and shipping processes (s2). figure 8 presents the case of a merge of the picking and shipping processes when using rfid technology.
2. to what extent should the initiators encourage the process redesign in their trading partner facilities? 3. how does rfid change the job descriptions and work roles? (p. 99)
"from the above expression, we can see that although the timing point is allocated inside the permitted zone, it not only induce the common phase rotation, but also brings the ici on the demodulated symbols, which is shown in fig. 9 . this character is different from ofdm system. furthermore, if the timing point is outside the permitted zone, it will cause both the isi and ici, which is similar with ofdm. because the energy of ici inducing by cfo is distributed in all the sub-carriers, as shown in fig.10, the cfo brings about not only the ici and the linear phase rotation, but also the isi and the attenuation on the demodulated symbols. fig. 11 . sir comparison between dft-s-ofdm and ofdm system with cfo as shown in the figure, except the first symbol, all other demodulated symbols of dft-s-ofdm system have a much higher sir than that of ofdm system under the same cfo condition. b. uncoded ber performance of cfo effect [cit] we first derive an exact closed-form ber expression for the dft-s-ofdm system without channel coding. as we know, an arbitrary rectangular qam can be viewed as two independent pulse amplitude modulation (pam), i.e., i -ary and j -ary pam's, through two www.intechopen.com quadrature branches. as a result, the average bit probability of the detected symbol ', m j a in the presence of cfo can be obtained by averaging the bit error probabilities from ["
"if the image line segment is fitted by lst and the intervals of the edge points are fixed for all of the image line segments, then we have"
"(4) once the batteries are dropped into the truck, the truck driver leaves the tpl facility to bring the batteries to the shipping destination while the locationbased system (lbs) is automatically initiated for \"intransit\" visibility."
"the fuel cell stack is composed of numerous single fuel cells in a series. thus, the total voltage of the stack is approximately equal to the sum of every single cell voltage. the pemfc model is developed using matlab/simulink as shown in figure 2 . the electrochemical model is validated by comparing it with the experimental results. the experiment was performed with 5 kw water-cooled pemfc system as shown in figure 3 . in figure 4, the experimental data and simulation result matches closely. thus, our electrochemical model for designing the controller is feasible."
"variational and pde-based image restoration methods play an important role in image processing. their goal is to recover an image u from the noisy version f . this is a typical example of inverse problem. the classical way to overcome inverse problem is to use regularization techniques. that is to say, images can be reconstructed by means of minimizing a variational energy functional. many techniques for image restoration based on energy minimization have been presented [cit] . the total variation (tv) regularization has become a well-known model in inverse problems because it enables sharp edges and fine details to be recovered."
"as most of the waste heat is taken away from the fuel cell stack by coolant passing across the stack, the heat removal rate can be denoted as follows:"
"numerous high-performance control strategies are available in the field of control research, and several of these stratigies have achieved remarkable results in certain previously published papers [cit] . in the current study, three other main control strategies (on/off control, state feedback control, and pid control) are also designed and applied to the established fuel cell model in the simulation to compare their control performances. by comparing the fuzzy control of the simulation result, the superiorities and effectiveness of the fuzzy control can be demonstrated convincingly."
"to analyze the dynamic system model of a 5 kw water-cooled pemfc thoroughly and clearly, a thorough understanding of its principle, mechanism, and property is necessary [cit] . the schematic of the water-cooled pemfc system is demonstrated in figure 1 . fuel cell stack, made up of numerous single cells in a series, is the core module and the only electricity source of the entire system. a specific electrochemical reaction producing water, heat, and electricity occurs in the cathode and anode of every single cell. the corresponding reaction equations can be described as follows:"
"to solve the variational problem (4), we first employ an alternating minimization technique which alternatively minimizes one variable while fixing the other ones. so, we should consider the following two coupled minimization subproblems:"
"to obtain the optimal value of u and d, the subproblem (8) and (9) are supposed to be solved to full convergence, however, it is found unnecessary in practice. for many applications, we have found that optimal efficiency is obtained when only one iteration of the inner loop is performed."
"in which, a 1, a 2, a 3 and c 1, c 2, c 3 are calibrating parameters, and all parameters values except c 2 are list in table 2, and c 2 can be determined using the following equation:"
"the development of prognostics algorithms face similar constrains as reliability engineering in that both need information about failure events of critical electronics systems. these data are is rarely ever available. in addition, prognostics requires information about the degradation process leading to an irreversible failure; therefore, it is necessary to record in-situ measurements of key output variables and observable parameters in the accelerated aging process in order to develop and learn failure progression models."
"with the occurrence of an electrochemical reaction in each fuel cell, a large amount of heat will be produced. the fuel cell stack can operate efficiently and continuously only if the waste heat is removed from the stack instantly. excellent operating performance requires the stack temperature to vary within the desired narrow range. for the small electrical power fuel cell stack, air cooling is a simple and economic method. however, if the stack power excesses 5 kw, then water cooling or evaporative cooling is recommended to remove the extra heat efficiently."
"other components of this system can be called a balance of plants (bops) [cit] . an air blower and a humidifier coupled with a water separator make up an oxygen intake module, that provides compressed and humidified air prior to the stack cathode. a hydrogen supply module consists of pressurized hydrogen cylinder and corresponding valves. the cooling module is divided into two circulation loops-one for the coolant and the other is for cooling water composed of two pumps, two reservoirs, a radiator coupled with a fan and a heat exchanger along with the circulation pipeline. the novel structural design of the cooling module has two main advantages. first, this module avoids the chemical incompatibility between coolant and radiator. second, it can be conveniently used for proposing waste heat recovery at the water reservoir."
"in-situ measurements of the drain current (i d ) and the drain to source voltage (v ds ) are recorded as the device is under aging regime. the on-state resistance r ds(on) in this application was computed as the ratio of v ds and i d on the on-state of the square waveform. in the accelerated aging system, it is not possible to measure junction temperature directly, as a result, the increase in junction temperature is observed by monitoring the increase in r ds (on) . furthermore, junction temperature is also a function of the case temperature, which is measured and recorded in-situ. therefore, the measured r ds(on) was normalized to eliminate the case temperature effects and reflect only changes due to degradation. due to manufacturing variability, the pristine condition r ds(on) varies from device to device. in order to take this into account, the normalized r ds(on) time series is shifted by applying a bias factor representing the pristine condition value. the resulting trajectory (δr ds(on) ) from pristine condition to failure, represents the degradation process due to die-attach failure and represents the increase in r ds(on) through the aging process."
"a prognostics algorithm in this application predicts the remaining useful life of a particular power mosfet device at different points in time through the accelerated life of the device. as indicated earlier, δr ds(on) is used in this study as a health indicator feature and as a precursor of failure. the prognostics problem is posed in the following way."
", where n i and n j are called the neighbourhood centered at i and j. the discrete gradient and laplace operators are given by"
"as both hydrogen and air are humidified before entering to the anode and cathode, the flow rate of humidifying vapor in cathode and anode is expressed as follows:"
"the model works very well for image denoising, deblurring, and decomposition. however, it cannot completely separate the cartoon part from the textural part and also produces staircase effects. there have existed plenty of variants and numerical attempts to overcome the problem."
"in this study, as the research object is the water-cooled 5kw hydrogen pemfc stack, the excess heat generated during the electrochemical reaction will be dissipated via conduction and convection by regulating the speed of the coolant pump, thereby inducing the change in the coolant flow rate correspondingly [cit] ."
extended kalman filter allows for the implementation of the kalman filter algorithm for on-line estimation on nonlinear dynamic systems [cit] . this algorithm has been used in other applications for health state estimation and prognostics. the general form of extended kalman filter is given as;
"the degradation model presented in equation (1) is converted into a dynamic model in order to obtain the statespace representation needed for bayesian tracking. defining the parameters and be time dependent parameters, then the derivative of (1) is given by, ."
"in accordance with the results shown in figure 10, the control effect of the state feedback controller is not as capable as the performance of the other three controllers in the field of temperature management. from the partial enlargement of the periods of 3-14 s and 148-160 s, the temperature disturbance of the proposed stack is minimal under the management of the fuzzy control strategy. the proposed fuzzy controller shows better performance in response time and temperature varying range compared with that in the references [cit] the mean square error (mse) of temperature is introduced as the criteria to indicate the performance of each control strategies. the temperature mean square error is expressed as follows:"
"for simplicity, it is assumed that the amounts of reactants which involved in the electrochemical reaction are proportional to the corresponding input gases. the coefficients of proportionality are expressed by γ h 2 and γ o 2 respectively. the reactant flow rates are calculated as follows:"
"where is time and and are model parameters that could be static or estimated on-line as part of the bayesian tracking framework. this model structure is capable of representing the exponential behavior of the degradation process for the different devices. table 1 presents parameter estimation results for model (1) based on non-linear least-squares estimation. the estimate for both parameters is presented along with their corresponding sample variance. it is clearly observed that the parameters of the model will be different for different devices. therefore, the parameters and need to be estimated online in order to ensure accuracy. figure 2 presents the estimation results for device #36. figure 2 . non-linear least squares for device #36."
"to analyze the response characteristics of the four controllers more clearly, several partial enlargements of figure 8 are shown in figure 9 . all four controllers respond instantly when the current load changes (at 50s, 100s, 200s, and 250 s as shown in figure 9 (a)∼(d) ). however, the on/off controller and the pid controller respond more violently than other controllers."
"in this paper, we shall focus on a new model which uses nonlocal tv and nonlocal laplace operator to regular an image, thus, it can effectively exploit the available information of the input image. then, we formulate a nonlocal variational functional for image restoration which performs adaptive smoothing but also preserves edges. the rest of the paper is organized as follows. section 2 recalls some results on nonlocal operators and split bregman method. the proposed variational bi-regularized model for image restoration and its split bregman algorithm are presented in section 3. in section 4, we demonstrate the experimental results on natural images and textured images which show the validity of the new model, and the conclusion is given in section 5."
"where ξ is the empirical parameter and its value can be obtained by experiments.ṁ coolant is the coolant flow rate, and c p,coolant is the special heat of the coolant. t coolant,out and t coolant,in indicate the temperature at which the coolant enters and exits the stack, respectively. the desired output of a pemfc stack is the electric power, which is expressed as follows:"
"these measurements do not have a fixed sampling rate. on average, there is a transient response measurement every 400 ns. this consists of a snapshot of the transient response which includes one full square waveform cycle. therefore a resampling of the curve was carried out to have uniform sampling and a reduced sampling frequency on the failure precursor trajectory. the signals were filtered by computing the mean of every one minute long window. there are six available aged mosfets under thermal overstress. figure 1 presents the δr ds(on) trajectories for the six cases."
"where, i represent current density [a/cm 2 ]; i max is the limiting current density [a/cm 2 ]; r ohm is the internal electrical resistance [ · cm 2 ]; e nernst is the nernst open circuit voltage [v] and its value is determined by the function of fuel cell temperature t fc [k], ambient temperature t amb [k], hydrogen partial pressure p h 2 [bar] and oxygen partial pressure p o 2 [bar]. following the thermodynamic conditions in the standard state entropy change, e ocv and other parameters can be formulated as follows [cit] :"
"where c p,st and c p,coolant nominal value of two specific heat capacities of the stackc p,st and coolantc p,coolant, δ st and δ c are the corresponding value perturbation within the extent from −1 to 1. in this study, simulates the robustness of the proposed fuzzy controller and the on/off controller is simulated and analyzed in seven different conditions as shown in table 5 . figure 12 demonstrates the stack temperature profiles under the management of fuzzy controller in different parameter variation conditions, and figure 13 figure 10 and figure 11 reveal that the fuzzy controller and the on/off controller can manage stack temperature fluctuating within a small narrow range. therefore, the on/off controller also is considered as the robust controller. figure 14 and the results demonstrated in figure 10, figure 12, and figure 14 prove that, the proposed fuzzy control and the on/off control are effective control strategies in a certain condition. however, when the system suffers from some fluctuations, the fuzzy control shows superior robustness in the performance of temperature management."
"in accordance with equation (13) and equation (31), the relationship between the nominal value and the actual value of these two parameters is given as follows: 1"
"in this section, we shall give a description of the new nonlocal image restoration model and present the corresponding algorithm via alternating directional minimization and split bregman method."
"therefore, f is a vector valued function given by equation (8) . the on-resistance is the only measured value; therefore, the measurement equation is given by equation (9) ."
"the 5 kw water-cooled pemfc system model is built and validated by conducting experiments on the real water-cooled fuel cell system with the same power magnitude. the cooling control method coupled with a selective heat recovery subsystem is proposed, and the four widely used control strategies (on/off controller, state feedback controller, pid controller, and fuzzy controller) are introduced to achieve rapid and effective regulation of stack temperature. these four controllers are designed and applied to the cooling module of this proposed fuel cell system model. the coolant pump speed is subject to the controller and the coolant flow rate changes consequently. thus, the excess heat generated by electrochemical reaction is taken out of the stack by the coolant. in this manner, the stack can operate efficiently and healthily. after comparing the control efficacy on the stack temperature of each controller on the same model, the fuzzy control is proven the best control strategy for the stack temperature control of the proposed 5 kw water-cooled fuel cell system. the robustness of this proposed fuzzy controller is also validated. in addition, the effectiveness of the fuzzy controller is verified by the real experimental. in summary, given the excellent performance of the fuzzy control, it is the first choice for temperature control for a water-cooled pemfc systems that possess highly nonlinear dynamic and coupling characteristics."
"in this paper, we present a new nonlocal variational biregularized model for image restoration, which is the nonlocal version of the cep-l 2 model. we apply the alternating minimization technique and the split bregman algorithm to solve the variational bi-regularized minimization problem. by applying the new model to image denoising problems, we show that it is an effective technique that can produce satisfactory denoised image. the experimental results have verified that it can obtain better results compared to some previous methods."
"to understand clearly how the stack temperature varies as the current load profile changes, an exact electrochemical model should be developed. the fuel cell voltage is affected by various factors, such as partial pressure of oxygen and hydrogen, but the voltage of every unitary fuel cell in a stack varies. for simplicity, every cell voltage in a stack is assumed to be equal at the same specific moment. the function of fuel cell voltage for a single cell is nonlinear because of the existence of voltage loss caused by ohmic, concentration and activation losses. an electrochemical voltage model was developed in references [cit] . therefore, the operating voltage of a unitary fuel cell v fc can be denoted as follows [cit] :"
"here, we outline the discrete nonlocal operators which will be used in the numerical computation. the weight w(x, y) is denoted by w i,j in the discrete setting."
"prognostics is an engineering discipline focused on predicting the time at which an in-service component will fail. the science of prognostics is based on the analysis of failure modes, detection of early signs of wear and aging, and fault conditions. these signs are then correlated with a damage propagation model and suitable prediction algorithms to arrive at a \"remaining useful life\" (rul) estimate. the discipline that links studies of failure mechanisms to system lifecycle management is often referred to as prognostics and health management (phm). power semiconductor devices such as mosfets (metal oxide field effect transistors) are essential components of electronic and electrical subsystems in onboard autonomous functions for vehicle controls, communications, navigation, and radar systems. in current practices, maintenance schedules are usually based on reliability data available from the manufacturer. however, while this approach works well in aggregate on a large number of components, failures on individual components are not necessarily averted. for mission critical systems it is extremely important to avoid such failures. this calls for condition based prognostic health management methods."
"in this model, and are also state variables that change through time. therefore, the model is a non-linear dynamic system and bayesian tracking algorithms like the extended kalman or particle filters are needed for on-line state estimation. the forward difference method is used to approximate the time derivatives in order to discretize the model in equation (3) . the first step in the process is 1 ∆ . 4"
the alternating minimization method and split bregman method are combined to obtain the algorithm for our new model. we summarize the algorithm for the biregularized model (4) as follows:
"tv regularization technique is used in the aforementioned methods for image restoration to preserve edge. however, the use of the classical tv norm in the functional causes staircase effects in the smooth regions. one way of reducing staircasing in image restoration from tv regularization is to combine higher-order derivatives. chambolle and lions (cl) incorporated higher-order derivatives into the image restoration model [cit] . thus, the minimization problem is:"
"the last term of entire heat balance, q conv, is the rate of convection heat dissipation, and the following expression is used: table 3 shows the operating specification of the pem fuel cell. figure 5 shows the block diagram of the cooling module in matlab/simulink platform and figure 6 demonstrates the simulation results of current, voltage, power, and power distribution of the proposed model, respectively."
"in many remote area or islands, there are the small quantity and scattered distribution of residences. it is difficult and costly to supply power by conventional electricity grids. the application of the stationary pemfcs is an acclaimed scheme to solve the abovementioned energy supply problem in these cases. the household pemfc has kilowatt-level power but adverse passive heat dissipation condition, therefore, temperature management is still a major challenge to its widespread commercial application."
"where n denotes the quantity of single fuel cells, i st is the stack current [a], f represents faraday constant. the total electrochemical energy can be shown as follows:"
"it is assumed that the atmosphere air consists of oxygen and nitrogen, accounting for 21% and 79% by volume respectively. also, oxygen is considered completely consumed during the electrochemical reaction."
"where t st is the stack temperature, t ref represents the reference temperature. the smaller the mean square error (mse) is, the better the performance of the corresponding control method is. the mse values of temperature for each control strategy are listed in figure 11 . based on the mse value, the fuzzy control is the best temperature management method among the four control strategies. the fuzzy controller shows excellent performance and the smallest deviation. this result is consistent with the conclusions drawn by observing the temperature profiles in figure 10 ."
"to display the capability of denoising of the new model, we illustrate the noise image f − u − v in figs. 5, 6, 7 and 8."
"in actual use, operating and maintenance can affect the cooling performance of the cooling module. long-term use without unscheduled maintenance will have a negative effect on the thermal conductivity of water-cooled fuel cell stack. the different types of coolant and its used time also present various cooling characteristics. to understand the impact of fluctuations in certain system parameters on stack cooling and analyze the robustness of the proposed system, two specific heat capacities of the stack c p,st and coolant c p,coolant are selected as the system fluctuation parameters."
"an empirical degradation model is suggested based on the degradation process observed on δr ds(on) for the six aged devices. it can be seen that this process grows exponentially as a function of time and that the exponential behavior starts at different points in time for different devices. an empirical degradation model can be used to model the degradation process when a physics-based degradation model is not available. this methodology has been used for prognostics of electrolytic capacitors using a kalman filter [cit] . there, the exponential degradation model was posed as a linear firstorder discrete dynamic system in the form of a state-space model representing the dynamics of the degradation process. the proposed degradation model for the power mosfet application is defined as follows. let ∆ be the increase in on-resistance due to aging."
"participants' written feedback on the text-and-image tutorials was generally more positive than for the video tutorials. commenting on the text and image tutorial, one participant remarked that it was a \"great resource\" while another said that it was \"very easy to use. will become really helpful when put into full effect.\" another observed that the tutorial \"was pretty precise.\" not all the comments on the text and image tutorials were positive, however. more than one participant noted that the images used in the tutorials were blurry. one even suggested that \"more animations to the text would make it much more open to people with different learning styles.\""
"the provision of library instruction online has become increasingly important, given that more than one third of higher education students now take at least some of their courses online and that the number of students enrolling in online courses continues to increase more rapidly than the number of students in higher education as a whole. 1 academic library websites reflect the growth of online education. [cit], online versions of journals had become ubiquitous. 2 in contrast, electronic books have been slower to be adopted in academic libraries, but there has been a steady and significant growth of their use in recent years. [cit], for example, the average number of electronic books available at academic libraries in the united states increased by 93 percent. 3"
"in order to ease coexistence of these addresses with legacy equipment, hlmacs are defined as a subset of the existing mac address space. hlmacs are defined as 48-bit standard format mac addresses in which the bit 1 of byte 0 is set to 1 to indicate that the address is local. the rest of addresses with the bit 1 of byte 0 set to 0 are the usual mac addresses, flat and globally unique. bit 0 of byte 1 indicates that address is individual or (multicast) group address. the remaining 46 bits are structured into 6 hierarchical levels in which the first level has 6 bits, and the other 5 have 8 bits. these levels correspond to the position of the bridge in the spanning tree. hlmac addresses are interpreted as variable-length address conveyed in the standard 6 byte length field. the first level (from left to right) containing all zeroes indicates the end of address."
"furthermore, this study shows the broader value of usability testing of library instructional material. although participants who received the text-and-image tutorials performed better than either of the other two groups, the tests helped researchers identify two problems with the tutorials: users found the images blurry and often misinterpreted how to download citations in mla format. such information gleaned from the user's perspective would be valuable in creating future library online point-of-need instructional tutorials."
"it is to be noted that the csma/ca mechanism in wsn normally uses the mac mechanism of ieee 802.15.4, where one backoff unit/slot in the ieee 802.15.4 standard is of 20 symbols. cca requires eight symbols time to know the status of the channel, and the turnaround time (tx-rx or rx-tx) is 12 symbols. if the remaining symbol of the data frame at the end of backoff slot is less than eight, the receiver transmits the acknowledgement in the very next backoff slot. otherwise, it has to transmit into the next two backoff slots. our intention is to avoid this situation by sending a known signal to make the data channel busy so that other nodes can know about it and go for the appropriate sleep period based on its average packet duration time to avoid the energy consumption due to unnecessary cca. accordingly, we propose to insert three known signal sequences at the start of the data, end of the data and start of the acknowledgement frames if the remaining symbol of the data frame at the end of the backoff slot is greater than eight. otherwise, insert only two known signal sequences at the start of the data and acknowledgement frames. while accessing the data channel, the nodes that receive these known signals during the cca calculate the appropriate sleep duration and random backoff time to avoid further unnecessary ccas based on the correlation value of the known signal and received signal."
"the complete and detailed hurp forwarding algorithm is shown at fig. 5 below and forwarding examples at fig. 3 above. it operates as follows: if the hlmac of the bridge being traversed is included into the destination hlmac (i.e. the destination hlmac is longer), the frame is forwarded downwards the tree using the output port identified by the first octet exceeding this bridge hlmac. if the destination hlmac is included into the bridge hlmac, the frame is forwarded through the root port. the root port is also the default route if there are no alternative routes. this is shown in fig. 3 by dotted line."
"we simulated 30 power law (scale free) topologies of 32, 64, 128 and 256 nodes. topologies were generated with brite selecting barabasi-albert node distribution model. the results for absolute maximum, minimum and average values for up/down and hurp are shown in fig. 10 . results are grouped by topology size and ordered by degree inside each topology. turn prohibition decays with increasing network sizes. the graphs show that the maximum and minimum values of hurp are significantly below those of up/down. the values increase with average node degree, and decrease when the number of nodes is increased. up/ down performance stays bounded regardless of the bridge selected as root."
"we now combine, for comparison purposes, the results for spanning tree, up/down, hurp above with those for turn prohibition (tp) and tree based turn prohibition (tbtp) obtained with similar conditions of fixed node degree topologies. results for spanning tree, and up/down obtained as described above in 2.b coincide with those obtained at [cit] for tp and at [cit] for tbtp, and suggest a fair comparison. we have included in table 1 the results from tp and tbtp, respectively to obtain a global view of the five protocols for the fixed node degree topologies. hurp nearly coincides in performance with tbtp (although it is decentralized and much simpler than both tbtp and tp) and performs better than up/down. as said above, fixed node degree topologies perform worse with any turn prohibition algorithm than any other topologies evaluated."
"other authors have explored the need to accommodate different learning styles in library tutorials rather than relying too heavily on text to convey information. 26 at the university of leeds in the united kingdom, an information literacy tutorial was planned and created to support online distance learners in the geography postgraduate program. using an articulate presenter, the authors created a tutorial that covered the same material that would be taught in a face-to-face session, and which incorporated visual, auditory, and textual elements. these researchers concluded that the online tutorial is supplemental and did not alleviate the need for face-toface instruction. 27 to reach different types of learners, many librarians have begun to use adobe flash (formerly macromedia flash) to create multimodal online information literacy tutorials. authors who use flash note that learning how to use the software correctly represents a significant investment in time and effort. 28 another study, conducted via a suny albany web design class, focused on the effect/outcome of teaching with web-based tutorials in addition to or instead of face-to-face interaction. the authors of this study pointed out that self-paced instruction, lab time, office, hours, and email exchange were all factors that are affecting web-based multimedia (wbmm) flash that were incorporated into instruction. 29 rather than focusing purely on the content of online library instruction tutorials, some studies considered and evaluated the various tutorial-creating software tools. blevins and elton conducted a case study at the william e. laupus health sciences library at east caroline university, which set out \"to determine the best practices for creating and delivering online database instruction tutorials for optimal information accessibility.\" 30 they produced \"identical\" tutorials using microsoft's powerpoint, sonic foundry's mediasite, and techsmith's camtasia software. they chose to include powerpoint because \"previous research has shown that online students prefer powerpoint presentations to video lectures.\" 31 their testing results indicated that participants found specific tutorial features to be most effective: video (33.3 percent), mouse movements (57.1 percent), instructor presence (28.6 percent), audio instruction only (28.6 percent), and interaction (28.6 percent). they concluded that camtasia tutorials provided optimal results for short sessions such as database instruction and that for instruction where video and audio of instructor + screen shots, mediasite was more appropriate. however, they also determined that powerpoint tutorials were an acceptable solution if cost were an important factor. 32 in a separate study at florida atlantic university, researchers described the process of designing and creating library tutorials using the screencasting software camtasia. in addition to the creation of the tutorials themselves, the authors described how the project entailed the development of policies and guidelines for the creation of library tutorials, as well as training for of librarians in using camtasia software. 33 this study provides another good example of the time investment involved in the creation of multimedia tutorials."
we can get all states' probabilities by solving these equations. then the probability φ that a node attempts carrier sensing for the first time can be calculated as follows.
"to stress the hierarchical nature of the hlmac addresses we use for their notation a different separator, '-', between each 8 bit block of the address, coded in hexadecimal. hlmac addresses are assigned hierarchically following the path defined by a spanning tree, from the root to the edges. the root bridge is assigned with address 40-00-00-00-00-00 (all zeroes except for the local bit). in the rest of the paper we represent only the 46 bit of hlmac address and omit bits 0 and 1 for the sake of clarity. as explained above, at each stage, the address of a b bridge connected to a bridge a that precedes b in the spanning tree is obtained by inserting the number of the port at a that connects to b in the first hierarchical level with null value. in other words, an address a-b-c, expresses the chain of designated port ids a; b; c, traversed in the descending path from the root bridge till the root port of that bridge. please remember that an address represents both the address of bridge and the subtree rooted at that bridge. fig. 3 illustrates the hlmac address assignment: the root bridge assigns to d1 the hlmac address 32 by appending 32 (port id of designate port) to the null value, d1 appends 7 and assigns 32-7 to d2 and so on. port numbering starts at value 1, not 0, to distinguish the end of the address."
"the researchers created four tutorials for this study. two were flash-based dynamic audio/video tutorials created using techsmith's jing software. the static text-and-image tutorials were created using microsoft word, which was then converted into a pdf document. the dynamic and static tutorials mirrored each other in terms of content, and were designed with the specific goal of helping participants complete the tasks successfully, though in both cases there was some variation between the tutorials and the tasks. the tutorials received by group 1, for instance, showed participants how to find articles about the occupy wall street movement, limiting the search to \"published in the new york times,\" and how to download the citation in mla format. the tutorials for group 2 [cit] ."
"where, p s is the probability of the successful transmission, e s is the energy consumption of successful transmission, and e c is energy consumption due to collision. e s and e c can be calculated as the following equation."
"the second task was given to all participants in the second round of tests and was more complex, comprising five components. [cit] that included color photographs. as with the first task, these participants were also required to download a copy of the citation for the article in mla format from the database. participants who attempted the second task were labeled \"group 2\" (see appendix ii)."
"the feedback on the video tutorials was generally positive, with comments such as \"very straightforward,\" \"helpful,\" \"easy to follow,\" and \"i would use this for school assignments.\" however, a common complaint about the dynamic tutorials was that the audio was not very clear. (this may be because the quality of the microphone used for the recordings.) other participants seemed to criticize the layout of the database itself, saying that bigger size of words would have made it easier to follow. another complained that the dynamic tutorial was too simple, and that it should cover more advanced and in-depth topics."
"consider an example in the scenario depicted in fig. 7 : h1 and h2 are global macs, and b1 and b2 hlmacs. host h1 sends a unicast frame to b1 with origin global mac of h1 and destination global mac h2. b1 receives the frame and looks for a hlmac associated to the destination global mac but it does not find it. then, it encapsulates the frame on a header with the broadcast address as destination, and with the hlmac of b1 as source address. the frame is forwarded through the spanning tree built by cstp. the frame arrives at all edge nodes, including b2, and b2 learns the b1-h1 association, it decapsulates frame and forwards the original frame to h2. the destination node h2 responds to h1 with a unicast frame and destination address h1 and source address h2. b2 encapsulates the frame with destination address b1, and origin b2. b2 reads the route table for finding the next hop to b1 destination and selects shortest path through a cross link. the same process is performed at each hurp bridge in the core till b1 is reached. finally, b1 receives the frame, removes hurp header and delivers frame to h1."
"a transmission is successful if exactly one node transmits data without any collision and channel error. a node can be successfully associated to a coordinator if the coordinator receives the association request successfully and the node receives the acknowledgement successfully. hence, the probability that the node is associated successfully with the coordinator is"
this proposed lpp model evaluates the optimal allocation for each node so that the energy consumption can be balanced with a lower packet drop rate.
"again, it is worth noting that the control group, without the aid of point-of-need instructional materials, achieved some success in completing the tasks. it is possible that the members of the control group gained important knowledge simply by being told about asp ebsco and that there was enough implied information in the tasks themselves to provide basic information about the content and functionalities of the database. this suggests that databases like asp ebsco are intuitive enough that people can learn how to use them independently. the higher number of serious errors, and the greater length of time members of the control group spent on tasks, however, shows that efforts to raise student awareness of databases and library resources should be coupled with point-of-need instruction."
"note that steps 1 and 2 are triggered by a change in the spanning tree changes, i.e. the addition or deletion of a node, or the addition or deletion of a link belonging to the spanning tree. turn prohibition computation is performed on each node when its own address changes, a new neighbour appears, or any neighbour changes its address due to rstp reconfiguration. finally, shortest path route computation is performed each time the distance vector information exchanged periodically changes."
"after completing the task, participants were asked to provide anonymous, written feedback on the instruction they received. (members of the control groups were not asked to provide feedback because the purpose of the study was to compare different types of library tutorials.) participants were asked ten questions, eight of which were on a likert scale and two of which were openended. although the feedback for both the static and dynamic tutorials was generally positive, the text-and-image tutorials also received higher combined scores than the audio/visual tutorials on the likert scale questions (see figures 5 and 6)."
"hurp exchanges routes between bridges using an enhanced distance vector protocol. messages and operation are similar to those of the rip protocol. the metric can be the hop count as in rip or, as in 802.1d, inversely proportional to the link speed. at each node, the hurp protocol selects routes through cross links when their cost is equal or better than cost via tree links. every bridge transmits to its neighbours the known shortest distance routes that do not use a prohibited turn (down-up), i.e. routes that would contain a down-up turn at the announcing node are filtered. each bridge builds a distance vector with the best available routes obtained from neighbors using the bellman-ford algorithm. the routes learnt from each neighbor are also excluded from the announcement to that neighbor (split horizon). note that the usage of a loop-free underlying virtual topology below the distance vector information exchange precludes the occurrence of the count-to-infinity problem."
"forwarding with hurp can operate in two modes: -with shortest paths via both tree and non-tree (cross) links using routing tables built by the vector distance protocol, or -by using exclusively the information encoded in the hierarchical destination address."
let us consider a wsn in which n number of nodes are associated with a coordinator. the csma/ca mechanism reduces collisions by using binary exponential backoff. let
"the performance of hurp is similar or superior to other turn prohibition algorithms like tp and tbtp, but with lower computation complexity (similar to the one of up/down). hurp performs consistently better than up/ down using the topology information carried on the hierarchical mac addresses. performance in all aspects is much closer to shortest path routing than to up/down. performance does not vary significantly with root bridge election. the impact of the number of prohibited turns in system performance decreases with the node degree, because the number of possible turns per node grows with the square of node degree (possible turns per node is equal to d * (d à 1)/2). therefore, as node degree grows, a large number of alternative shortest paths are created."
"a node enters into the active state when it has to transmit the unsuccessfully transmitted packet or a new packet. hence, the corresponding steady state probability can be derived as"
"the default operation of the protocol requires the encapsulation of the ethernet frame entering the hurp bridges core. in this case the ingress frame with global macs from hosts as source and destination addresses gets encapsulated with an outer frame header containing the hlmac addresses of source and destination edge hurp bridges close to the destination, to allow hurp frame forwarding in the core mesh. when the source address is a global mac, the external header includes as source address the hlmac of the hurba bridge through which the frame has accessed to the hurba core, to allow the destination to relate for response frames the remote source mac address with the remote source hlmac address of the closest bridge. in any other case, the hlmac appears in both the internal and external header. frames arriving to the hur-ba core that are addressed to global macs for which the bridge does not have a mapping to the corresponding hlmac are encapsulated with its original global mac and broadcasted through the spanning tree of the core. in the core bridges, forwarding of frames containing global addresses at hurp bridges is performed according to standard 802.1d rules."
"we evaluated a series of 120 node waxman model random topologies with different values of average node degree with random degree distribution. for each average node degree, 40 topologies were generated with the brite tool [cit] (intra as level, waxman model and default parameter values) and evaluated. fig. 8 shows the absolute maximum and minimum and average fraction of prohibited turns for spanning tree, up/down and hurp. hurp performs between 5% and 10% better than up/down in all cases."
"where, l d is the data packet length. the data packet is transmitted successfully if the node is associated and gets the gts slots successfully. hence, the reliability is"
"the library literature on online library tutorials might be divided into subcategories: early development of online instructional tutorials, library website usability testing, evaluation of online information-literacy instruction tutorials, best practices for the creation of library tutorials, and the best mediums for the creation of library tutorials."
a node can get the gts slot successfully if the coordinator receives the gts request successfully and the node gets the acknowledgement along with the gts reply packet successfully. the equation of probability that the node gets the gts slot successfully from the coordinator is given as follows.
"number thirty freshmen at st. john's university participated in this study. while usabilitytesting experts do not place a great deal of importance on recruiting participants from a specific target audience, the researchers wanted to choose users who were less likely to have had significant experience with university library database searching, since prior knowledge could make it harder to determine the effectiveness of the tutorials. they therefore chose freshmen as the participants in the study. they did not seek any other variables such as age, gender, ethnicity/culture, or any other demographic information. participants were recruited through the st. john's central portal, which is the main channel of internal communication at st. john's university, and through which mass emails can be sent to a targeted population of students. the email to students provided a registration link to a google form, which asked students to provide their name, year of study, time availability preference, and contact information. freshmen were selected from the response list."
"in this section, a synchronous mac protocol is designed for the wsns. to reduce the power consumption for communications in the control channel, a new mac protocol is designed and the performance analysis of the proposed mac is developed in the next section. the system model of our proposed protocol can be designed as follows."
"other researchers developed guidelines and best practices for future planning and implementation. bowles-terry, hensley, and hinchliffe at the university of illinois conducted interviews to investigate the usability, findability, and instruction effectiveness of online video tutorials. although shorter than three minutes, students found the tutorials to be too lengthy, and would have preferred the option to skip ahead to pertinent sections. other participants found the tutorials too slow, while some preferred to read rather than watch and listen. on the basis of their study, the authors recommended a set of best practices for creating library video tutorials, including pace, length, content, look and feel, video versus text, findability, and interest in using video tutorials. 24 at regis university library, librarians created online interactive animated tutorials and incorporated google analytics for use statistics and tutorial assessment, from which they developed a list of tips and suggestions for tutorial development. these included suggestions regarding the technical aspects such as screen resolution and accessibility. of some significance is that the data from the analytics suggest that the tutorials are being used both within and without the university. most useful here is the \"best practices for creating and managing animated tutorials\" found in the article's appendix. 25"
"in this section we describe the basis of up/down routing algorithm based on prohibiting the down-up turns using the node identifiers, which is seminal for the definition of hurp. fig. 1a and b show, respectively a complete network and its spanning tree active topology. fig. 1c shows the up/down principle."
"as an incentive for participation, the student participants became eligible for a kindle fire tablet for each of the two rounds of the study. prior to beginning the study, the authors consulted st. john's university's office of institutional research, which oversees all research at the university, and provides approval for the study of human subjects. since this study focused on tutorials rather than the participants themselves, the authors were granted a waiver for the study."
"random topologies of 120 nodes with fixed node degree (all nodes equal degree), were generated and simulated, with 40 topologies per degree, with the results shown in fig. 9 . the ratio of prohibited turns is worse than waxman random topologies increasing significantly for all protocols: up/down, hurp and spanning tree. the ratio of prohibited turns is worse also than regular network topologies like three dimensional meshes. we consider random fixed node degree topologies difficult to find in practice and less representative."
"up/down relative performance in random fixed node degree topologies is low, likely due to the relatively higher number of nodes with high degree (more possible turns) located in a low tree position. table 5 shows the throughput for the random topologies with barabasi-albert degree distribution. hurp performs close and even exceeds shortest path throughput, exceeding shortest path is a matter of path diversity and has no special meaning in this context. due to the high connectivity of ba topologies, path lengths (see section 7) are practically identical for hurp and shortest path. the small relative differences in throughput obtained for hurp and shortest path are more related with statistical variations for each topology derived from the addressing system dependent of root bridge for hurp and path selection and tie cost resolution mechanisms in shortest path routing."
"the worst-case computational complexity of hurp is polynomial in the number of nodes n, more precisely oðnd 2 þ, where d represents the maximal degree of any node in the network, as it is reasoned next:"
"a wireless sensor network (wsn) consists of a large number of autonomous sensor nodes, which are mostly used for environmental monitoring, military surveillance, smart buildings, health care and industrial applications. deployment of wsns is suitable for harsh environments, where human interventions are limited. wsns are constructed through the interconnection of wireless sensors, where each node not only sends its own data but also relays data from other neighboring nodes. the main advantages of wsns are that data can be transmitted from different sensor nodes simultaneously by spatial reuse of a channel and can always have alternative options to send the data, even if some sensor nodes have failed. thus, the network spectrum can be reused with high reliability. in many applications of wireless sensor networks, wsns are considered as an emerging technology."
"the nodes of a wsn are normally powered by batteries that cannot be replaced and replenished. in the case of power failure, data collected and routed through those nodes are lost. it is desired that the nodes should optimize the power consumption to extend the network lifetime. moreover, selection of relay nodes is another important factor that needs to be considered. in this section, we discuss how to choose the next hop relay nodes to extend the network lifetime. let s 1, s 2, ..., s n be the n number of nodes present within the carrier sensing range of a node s 0 . hence, the node s 0 transmits the data to the sink through these nodes. by choosing the suitable nodes with a good signal to noise ratio (snr), we can reduce the number of retransmissions, which will ultimately reduce the energy consumption and increase the network lifetime. hence, it is proposed to choose those neighbor nodes as the relay nodes, which are one hop away from the sink/base station with higher remaining energy and value of the snr greater than a certain threshold."
"in this work we aim to improve the performance of up/ down without resigning to self-configuration, full distributed operation and sufficient compatibility with standard bridges. the improvement in performance results from the ability to permit some extra turns that would be prohibited in up/down, by considering the topological information embodied into the hierarchical address. additionally, we want to preserve the standard ethernet frame format and transparency to hosts and routers. keeping the functionality simple would permit legacy bridges to be upgraded with just software updates, the architecture defined to achieve these goals is named hurba (hierarchical up/down routing and bridging architecture), in which we propose three novel components."
"while the professional literature thus shows that flash-based tutorial software is popular among librarians, and the desire to accommodate students with different learning styles is a laudable goal, at least one study suggests that the time and money involved in the creation of multimedia tutorials could be better spent in other ways. a university of illinois urbana-champaign study found that students from different learning styles performed better after using tutorials made with a combination of text and screenshots than from tutorials created with camtasia software. 34"
"two mechanisms for interoperability with standard 802.1d bridges and auto configuration have been devised for hurba: the automatic construction of the hurp core by all directly connected hurp bridges surrounded by separate and standard spanning trees rooted at the edge hurp bridges, and the encapsulation of frames entering the hurp core from 802.1d sub networks. fig. 6 illustrates the process of core construction by cstp protocol in a mixed network environment with hurp and standard bridges. in a) the initial topology, consisting of hurp capable bridges interconnected with 802.1d bridges is shown. cstp operates as rstp building the hurp spanning tree, but only the ports connected to other hurp bridges connect to the hurp core tree. the ports connected to standard bridges execute the standard rstp protocol and behave as edge bridges. these hurp bridges participate in the standard spanning tree and not in the hurp core spanning tree. they announce at their 802.1d ports a high priority value to be best candidates to become elected as root. if more than one edge hurp capable bridge (like r and b3 in fig. 6 ) the one with lower bridge id is elected as root of spanning tree of the standard bridges connected to them. additional hurp capable bridges not being elected root will block their links to that tree. isolated hurp capable bridges, disconnected from the hurp root bridge, like b4, default to standard 802.1d operation."
"on completing the task, participants who received either the text-and-image or dynamic audio/video tutorial were asked to complete a short questionnaire giving feedback on the instruction received (see appendix vii). participants who received no instruction were not asked to provide feedback."
"using adobe connect software, the testing activities, tutorials, participants' attempt(s) at task(s), participants' computer screen, and any conversation between the participants and the facilitators were simultaneously recorded and broadcast to a separate room, where the two other researchers observed, listened and took notes. the participants were asked to verbally describe the steps they were taking, as per the \"think aloud\" protocol that is essential to usability testing. recorded sessions were then available for later review by the research team."
"the advantages of text-and-image instruction were more pronounced in the second round of tests, which involved a more complex task (see figure 3) . as in the first round of tests, the participants in the control group had the lowest number of satisfactory task completions, and spent the greatest amount of time on task. although most of the participants in control group 2 had at least partial success in completing the task, most did so through trial and error, and showed a general lack of understanding of database terminology and functions. one participant, for example, attempted to use \"peer-review\" and \"color photographs\" as search terms. another attempted to search for \"deepwater horizon\" as a journal title. only two of the participants completed all components of the task successfully. two others partially completed the task -one found a suitable article with color photographs, but published in the nation, which is not peer-reviewed. one user failed to complete any part of the task and gave up in frustration (see table 5 ). as in group 1, however, all but one of the participants who received the text-andimage tutorial first attempted to download the mla citation by clicking on the \"mla\" link, rather than simply copying the text. two of the participants referred back to the tutorials after they had begun the task, which was permissible according to the facilitator's instructions. this suggests that the text-and image-tutorials are suitable for quick reference and allow users to access needed information at a glance. among the five participants who received the dynamic audio/visual tutorial, only one completed all five components of the test successfully. one was unable to locate the citation feature, while another failed to limit to peer-reviewed articles. [cit] to the present. all participants correctly used the publication limiter. although given the option, none chose to return to the dynamic tutorial after starting the task. this might be because of the length of the tutorial (more than three minutes) and the difficulty in navigating to specific sections."
"two basic approaches are being proposed in the literature to overcome these limitations: routing bridges [cit] and vlan-based multiple spanning tree approaches [cit] . on one hand, routing bridges, that are bridges that perform routing at layer-two, suffer from the limitations of the use of a flat address space for hosts and bridges. furthermore, rbridges [cit], one particular instance of the routing bridge proposals, uses a non-ethernet additional encapsulation with a ttl field to prevent loops. on the other hand, the main problem with the use of vlan-based multiple spanning trees is the high configuration complexity and limited number of spanning tree instances."
"the need to evaluate and assess the usefulness of online instructional tutorials is not new. although not explicitly related to today's environment, tobin and kesselman's work contains an early history detailing the design of internet-based information pages and their use in the library information environment. 6 they also included the early guidelines of the association of college and research libraries (acrl), the international federation of library associations (ifla), and the american library association (ala). a study by dewald conducted around the same time evaluated twenty library tutorials according to the current best practices in library instruction, and concluded \"online tutorials cannot completely substitute for the human connection in learning\" 7 and should be designed specifically to support students' academic work. further, it was noted that tutorials should teach concepts, rather than mechanics, and incorporate active learning where possible. 8 in a separate article, dewald argued that the web made possible new, creative ways of teaching library skills, through features such as linked tables of contents, and the provision of immediate feedback through cgi scripts. users also were able to open secondary windows to practice the skills they learned as they moved through tutorials. she further concluded that effective instructional content should not be text heavy, but rather include images and interactive features. 9 another early study of online tutorials discussed the development of a self-paced web tutorial at seneca college in toronto, called \"library research success,\" which was designed to teach subject-specific and general research skills to first-year business majors. the creation of the tutorial was first requested by seneca college's school of business management, which collaborated with seneca college library, the school's centre for new technology, and centre for professional development in completing the project. the tutorial was a success, with overwhelmingly positive feedback from students and faculty members. 10 despite such successful examples, a common concern expressed in early studies was that online tutorials would not be as effective as face-to-face instruction. one article compared and evaluated library skills instruction methods for first-year students at deakin university. 11 another tracked the difference between cai (computer assisted instruction) without a personal librarian interaction and a more traditional library instruction incorporated into an english classroom setting, and which concluded that while useful, cai was not a good substitute for face-to-face instruction. 12"
"after successfully associating with the coordinator, the nodes go for the data transmission. the transmission of aperiodic data by a node is successful if exactly one node is transmitting without any collision or channel error. the equation of the probability that the node transmits aperiodic data successfully is"
"ethernet is expanding in backbone and campus networks due to its excellent price/performance ratio, configuration convenience and backward compatibility. the use of transparent learning bridges with spanning tree protocols allows loop-free operation with zero configuration, without requiring complex routing information, ip address and segment planning and configuration or a hop-by-hop modification of the header of the frame (as occurs with the time-to-live field of the ip packet)."
"besides the above described topologies, a typical metro topology (fig. 11 ) was also evaluated for throughput, path length and fraction of prohibited turns. traffic distribution is uniform, as in previous topologies, assuming one flow between each client connected to a node every other node. table 9 shows the results."
"usability testing typically involves having participants complete a task or tasks in front of an observer. for this study, the authors designed two tasks that required participants to find articles in academic search premier ebsco database (asp ebsco). the first task, given to all participants in the first round of tests, was relatively simple, and consisted of three components: finding an article about climate change published in the journal lancet and downloading a copy of the citation for that article in mla format from the database. participants who attempted the first task were labeled \"group 1\" (see appendix i)."
"a node enters the idle state after successfully transmitting the data or it has no packet to transmit. hence, the corresponding steady state probability is"
"a link or bridge failure may cause reconfiguration in the spanning tree and result in changes in active topology. reconfiguration in hurp protocol follows similar rules than rstp [cit] and produces the same effects regarding port states. the main difference is that in rstp the learnt mac addresses are flushed while in hurp the assigned hlmacs are deleted as bridge addresses and routing table at every hurp bridges is erased after receiving the topology change notification. forwarding of frames with hlmacs is immediately stopped at ports that lose their valid hlmac address until new hlmac addresses are assigned (i.e. until spanning tree branches are reconfigured). hlmac addresses might appear volatile due to their dependency of the spanning tree. however, it must be taken into account that, unless a root bridge failure occurs, only a fraction of addresses will be affected in case of reconfiguration. besides this, the fast reconfiguration capabilities of rstp [cit] minimize unavailability of hlmac routing."
"where n is the number of nodes and d the degree of node with max degree in the graph. this complexity leads to a limited scalability when network size increases. tbtp includes a version that is backward compatible with 802.1d. a distributed version of tbtp (dtbtp) is proposed at [cit] to improve scalability and compatibility with standard bridges, although performance results are slightly inferior to that of tbtp and complexity remains oðn"
"besides prohibiting turns, routing information (excluding routes forbidden by the turn prohibition mechanism), can be exchanged over the links to minimize path length between nodes, obtaining the shortest cycle-free routes."
"the important parts of communication by a node in wsns are network discovery, association and successful transmission of requests. before any communication in the network, a node performs a random backoff and clear channel assessment (cca). during the backoff process, nodes deployed over the network cannot identify whether the channel is currently idle or not. the tagged node found the channel busy during its cca when the remaining time of an ongoing transmission is greater than the chosen random backoff. hence, the tagged node goes for the next cca after additional random backoff. based on the existing mac mechanism of ieee 802.15.4 [cit], two times consecutive cca operation is required before any data transmission. however, performing ccas twice is inefficient as it requires more delay and energy consumption than performing it once. energy is the main constraint for the battery powered sensor devices and therefore a suitable channel access mechanism should be designed to optimize the energy usage in wsns."
"this study suggests that library users benefit from online instruction library instruction at pointof-need, and that text-and-image tutorials are more effective than dynamic audio/visual tutorials for its provision. librarians should not assume that instructional tutorials must use flash or other video technology, especially given the learning curve, time, and financial commitments involved in creating video tutorial software. although the researchers in this study used the free software jing, learning to use it effectively was still a significant investment in time. more importantly, it is evident that the participants learned more and were more satisfied with text-and-image tutorials, which were more easily navigated than dynamic audio/video tutorials and which allowed users to more easily review tutorial content than did dynamic audio/video tutorials. this study corroborates the findings of mestre, who found that text-and-image tutorials were more effective than audio/video tutorials in teaching library skills. 37 it also lends credence to the work of bowles-terry, hincliffe, and hutchinson, who found that users preferred tutorials that allowed them to read quickly and navigate to pertinent sections rather than watch and listen. 38 as lim suggests, it is important to create instructional material that is clearly written. 39 it further suggests that regardless of the technology used, librarians should focus on creating content that is relevant and helpful to our user population."
"in wsns, discovery of neighboring nodes is a fundamental task, where a new node initiates the channel scan over a given list of channels before joining the network. a coordinator in a network initiates and manages the whole network. nodes in beacon-enabled networks rely on passive scans in different channels to find a coordinator for association. in passive scans, a to-be-associated node only listens to the medium for beacons and the latency of discovery depends on the coordinator's beacon interval (bi) and the available channels of the wsn. the node is not aware about the particular channel number as well as the beacon broadcast interval used by the coordinator. hence, the node has to scan all the channels with all possible beacon orders, which is a time consuming process. an active scan, as suggested in ieee 802.15.4e standard [cit], uses the beacon request command to extract the beacon from a coordinator. the to-be-associated node transmits the beacon request command in each channel one by one and waits on that channel for the most certain threshold time to receive the information."
"the hurp maximums are close to the average of up/down and stay below in all cases. the advantage of hurp over up/down diminishes as the network size increases, as occurred in other topologies."
the limitations of the channel access mechanism of ieee 802.15.4e deterministic and synchronous multichannel extension (dsme) mac used for the wsn in multi-channel synchronous environments are analyzed.
"for example, as shown in figure 3, let us consider three data channels to be accessed by the nodes a, b, c, d, e and f. the channel access in the data channel d ch1 is successful for the device a and it starts transmitting the packet and waits for the acknowledgement. the reception of the corresponding ack is interpreted as the successful packet transmission. as shown in figure 3, the device b finds the channel busy during the cca in the d ch1, just as device a starts the data transmission. device b receives one signal during its cca and knows if the cause of the channel is busy during the cca. hence, it consequently adopts the sleep state for the average duration of the data and acknowledgement period. similarly, the device d finds the channel busy due to the transmission of data by the device c in the data channel d ch2 . here, device d does not know the exact average remaining time for the ongoing communication and therefore goes to the sleep state for a random duration of the average packet communication period. as shown in figure 3, the device f performs its cca at the last backoff slot of the data packet duration of the device e in the data channel d ch3 and finds the channel busy. hence, it switches to the sleep state for the duration of the acknowledgement period to save energy. similarly, a device may find the data channel is busy due to the acknowledgement and switches to the sleep state for the remaining duration of the acknowledgement. it is to be noted that the nodes can easily detect a signal by comparing the amplitude of a correlation value against the present threshold without demodulating an exact symbol in random noise. let x[n] be the complex number representing the nth transmitted symbol and y[n] be the complex number representing the nth received symbol. then"
"undergraduate students receive a laptop computer at no cost, and the entire campus is wireless accessible. full-time faculty members receive laptop computers as well. the university libraries has 24/7 access to electronic resources, both on and off campus. the libraries' portal is located at http://www.stjohns.edu/libraries. an online catalog can be found at http://stjohns.waldo.kohalibrary.com. wireless computing and printing are available at the four campus library sites as well as in other areas across campus."
"however, spanning tree protocols also limit severely the scalability and performance of ethernet networks because they block all links exceeding the number of network nodes minus one. the scalability limitation of the spanning tree protocol derives from two factors: low link utilization and vulnerability of the bridged domain to network failures and configuration errors. network infrastructure utilization is low because the loop prevention mechanism of the spanning tree protocol relies on the activation of just a subset of the available links. as a consequence, the resulting routes along the spanning tree between two arbitrary hosts are not pair-wise shortest paths due to the low connectivity. vulnerability exists because of hardware failures or configuration errors, that may produce broadcast storms and even network meltdown of the switched domain. to prevent this, ip routers are deployed to segment the network and limit the size of bridged domains. the drawbacks of the use of ip are the requirement of proper ip address and segment configuration, the restrictions imposed to host mobility inside the network, and the need for ip routing configuration."
"consider a network modelled as a directed graph g, composed of nodes and bidirectional links [cit] . a ðn1; n2þ pair describes a link from node n1 to node n2. a path is a sequence of nodes successively connected by links so that each two subsequent nodes are connected by a link. in opposition to graph theory, a cycle in a path occurs when the first link and the last link of the path are the same, instead of requiring the first and the last nodes to be the same. therefore, a node may be visited repeatedly without creating a cycle. the degree of a node is the number of links connecting the node to neighbour nodes."
a node starts channel assessment if the remaining cap duration after a random backoff period is enough for data transmission. hence the corresponding steady state probability can be derived as follows.
"therefore, the effective arrival rate of these n nodes should be selected in such a way that it should not exceed the buffer size. hence, the effective arrival rate of these nodes should be modeled as follows so that all data packets can be transmitted."
"for the evaluation of tutorials in dynamic audio/video tutorials compared with text and image tutorials, the researchers employed usability testing, which is \"watching people use something that you have created, with the intention of making it easier to use, or proving that it is easy to use.\" 35 usability testing requires relatively small numbers of participants to provide meaningful results, and it does not require the selection from a representative sample population. 36"
"the node starts transmitting data after accessing the channel successfully in the ith backoff state. hence, the corresponding steady state probability can be deduced as"
"library reference and research assistance services are delivered in-person or electronically. library reserve services are accessible in either print or electronic formats. interlibrary loan has both domestic and international borrowing and lending via the illiad software platform. when the main queens campus library is not open for service, a 24/7 quiet study area is available for current students within the library space."
"the reception of the corresponding ack is interpreted as the successful packet transmission. the node finds the channel busy during the cca if any other node is transmitting during the tagged node's cca. as shown in figure 3, there are four cases where a node can find the channel busy. upon knowing the cause of the channel being busy during the cca, the tagged node goes for an appropriate sleep period based on its average packet duration time and then performs the random backoff before trying for the next cca in that data channel. the maximum value of nb is maxmacbackoff (max nb ) and the value of nb is incremented for each channel access failure. if the value of nb exceeds the value of max nb, it is treated as a channel access failure and the packet is discarded. the maximum number of retransmissions due to collision or channel error is limited to a value a maxframeretries. each transmission failure increases the value of retransmission (rt) by one. if the value of rt is more than a maxframeretries (max rt ), the packet is also discarded. the procedure continues until the tagged node attains the maximum number of channel access failures."
"in this latter case, the frame is sent in the ascending direction trough the spanning tree via root ports until the frame reaches the common node to destination and source tree sub-branches and descending by destination address decoding, without routing tables. if origin and destination addresses are in separate branches, the frame ascends until the root bridge and descends afterwards through destination branch. this forwarding mode does not use routing tables, only needs stable hlmac addresses, and seems suitable also for very high speeds switches by avoiding table look up delays. the preferred forwarding mode is one based in distance vector shortest paths, and the alternative through the spanning tree should only be configured when performance requirements or other reasons preclude the use of routing tables."
"as shown in fig. 4 above, hurp provides a significant enhancement in performance over the up/down mechanism for prohibiting turns because it can take advantage from the topological information contained in hlmac addresses in the following way: a turn that should be prohibited, can be permitted if the turn ends up at a bridge belonging to the destination branch. a bridge belongs to the destination branch of a frame traversing it if the bridge hlmac address is a prefix of the frame's destination hlmac address, or vice versa (the hlmac is a prefix of the bridge hlmac). in other words, the destination bridge hlmac and the next hop bridge belong to the same branch (i.e. have a common prefix). this mechanism is illustrated with an example: suppose node 5.2 belongs to branch of node 5. in fig. 4 frames with destination node 1.3 may execute turn at node 5 toward node 1 because node 1 belongs to destination branch. the same occurs for the turn 5-5.1-1.3. both of them are down-up turns and would be prohibited by up/down without topological identifiers."
"segment-based routing (sbr) [cit], another variation on turn prohibition, divides the network in subnets and subnets into disjoint segments, enforcing one turn prohibition per segment. segmentation is performed by the successive visitation of nodes from a visited node to a not visited node using a not visited link. the segments found are labeled and classified in three basic types: initial, regular and unitary. each segment is a set of links that completes a potential cycle in the network cycles are prevented by selecting at every segment a restriction (bidirectional local turn prohibition) that depends on the type of segment. performance of sbr is better than up/down and tbtp. however, it is not compatible with standard ethernet switches, since all bridges must execute sbr. sbr can operate in a zero-configuration mode, although an alternative is the use of centralized computation of tables and static configuration of switches with snmp. fully distributed dynamic reconfiguration is thus not possible."
"it is worth to note that an additional advantage of using hierarchical identifiers is that routes may be aggregated by nodes and a single identifier announced when routing information is exchanged. therefore, the number of advertisements can be greatly reduced (at the cost of loss of routing accuracy), so that a higher number of nodes could be served."
"comparisons of earlier works are shown in table 1 . however, from the survey of the related literature, it is observed that most of the works on performance analysis are based on the ieee 802.15.4 mac with star topology or single channel. to the best of our knowledge, though few works analyze the performance of the wireless mesh sensor network, none of the work evaluates the reliability and latency of the deterministic and synchronous multichannel extension (dsme) mac of ieee 802.15.4e used for the wsn in a multi-channel synchronous environment. we design here the performance analysis models as follows."
"as noted above, participants in all groups tended to make errors related to publication date, which may have stemmed from the wording of the task itself rather than misunderstanding the functionality of the database. [cit] onward. clearer wording of the task probably would have alleviated this problem."
"other algorithms like turn prohibition (tp) and tree based turn prohibition (tbtp) process the complete network topology by selecting iteratively the candidate nodes to prohibit turns around them. the criteria to assign the turn prohibitions can vary, being one example the minimum node degree (tp). to show the effectiveness of selective prohibition of turns to prevent cycles, just prohibiting the turn 2-4-3, prevents cycles in 1-2-3-4. to prevent all cycles in fig. 1, many combinations exist. one possibility is to prohibit all turns around node 1, namely turns (3,1,4), (4,1,2), turn (3,1,2), and around node 5, the turn (4, 5, 6) . note that each prohibited turn prevents one or several cycles. prohibited turn around node 5 breaks two cycles: 1-2-5-6-4-3 and 2-5-6-4. care must be taken, however [cit], to avoid disconnection of the graph. tp and tbtp are complex because they require the knowledge and visitation of the complete topology to minimize the set of prohibited turns. therefore, we choose up/down as a starting point for hurba by its low complexity and because it only requires the construction of a spanning tree. once the identifiers are assigned, each node may enforce at once down up turn prohibition just by comparing its identifier with the identifiers of its neighbors."
"we have seen that up/down routing assigns identifiers to nodes according to distance to the root bridge to assign direction to links. hurp assigns hierarchical local mac addresses to bridges and uses them as node identifiers for up/ down routing. we will discuss below that the use of hierarchical addresses enables improvements in performance and operation compared to the up/down proposal. unless otherwise stated, we will consider that these addresses are only assigned to bridges, although the addressing method is applicable to the hosts and routers connected to bridges as well. we now describe a mechanism to automatically assign hierarchical local macs (hlmac) to bridges by means of a variation of rstp that assign identifiers to bridges according to distance to the root bridge. we will show later that it is not required to involve the hosts in the hierarchical addresses assignment to assure the prevention of cycles. hlmacs are also used by the distance vector routing protocol for cross link routing and for direct forwarding through the spanning tree links through destination address decoding when routing tables are not available. fig. 2 shows the address assignment principle. root port is the origin, has no hierarchical address. r assigns address 1 to bridge 1 because designated port 1 is connected to that bridge (point to point are links assumed, as with rstp). bridge 1 assigns address 1.3 to that bridge because it is connected at designated port of bridge 1, and so on."
we can see that the worst topology for path length of all the ones considered in this paper is fixed node degree topologies. hurp path lengths are always shorter than up/down and closer to the optimum of shortest paths than to up/down.
"in wsns, data transmissions in the network are mainly sink oriented. when a node fails due to lack of energy, a new link is established for the data transmissions. a node that wants to send data to the sink simply sends to its one hop neighbor close to the sink. the coordinator/neighbor, upon receiving data from its children, sends to the next hop along with its own data. hence, selection of the relay node is a crucial factor for the data routing to the sink from a node in the wsn, which can improve the throughput. in order to mitigate the problems related to the latency, reliability, throughput and energy consumption in a synchronous wsn, novel channel access mechanisms and performance analysis models are proposed. accordingly, the main goals of our work can be summarized as follows."
"the remainder of the paper is organized as follows: section 2 introduces the fundamentals of the turn prohibition paradigm, with special attention to up/down routing. section 3 describes the hurba architecture. first, the structure of the hierarchical address and its assignment is discussed, then the combined spanning tree protocol (cstp) used to actually assign the address is discussed. next, the hurp protocol is presented from a control plane/user plane perspective and the computational complexity of the combined operation of the hurba components is analyzed. we finish the description of the hurba architecture discussing the mechanisms that enable compatible operation with legacy 802.1d devices. section 6 contains the simulation results that characterize the performance of the routing configuration resulting from hurba operation, in terms of number of turns prohibited, throughput, and path length. the next section presents related work, and section 8 the conclusions."
"we have evaluated the number of prohibited turns for different types of random topologies. parameters like number of nodes, average node degree, and node degree distribution, have a relevant impact in the performance of turn prohibition algorithms. to evaluate these effects we first simulate a variety of random 120 node topologies with a random (waxman) distribution of node degrees and another set of 120 node topologies with fixed node degree. then we simulated topologies of varying sizes (16, 32, 64, 128 nodes) with waxman, power law (barabasi-albert) and fixed node degree. due to the low performance of the spanning tree protocol, it is not included in all comparisons to enhance the precision of graphic comparisons of hurp with up/down."
"1. building the spanning tree with cstp: cstp (and in general any spanning tree protocol) is basically a distance vector protocol in which the distance is computed to one node, the root bridge. distance vectors to the root are interchanged between neighbour nodes. the complexity is oðndþ. 2. address assignment: addresses are assigned by cstp based on the spanning tree ports. each node assigns d addresses in the worst case, so the overall complexity is oðndþ. that would result in a prohibited turn around the announcing node. a maximum of d vectors are received at each node. complexity is the same as with bellmanford, i.e. oðndþ."
"in our analysis, energy consumption is defined as the average amount of energy consumed by a node for successful transmission of data. the energy consumption for receiving data, turning around from receiving to transmitting mode or vice versa and for transmitting a packet is taken to be p rx, p avx and p tx, respectively. let t l, t a, t cca, t ta and δ max be the time duration for transmitting a packet, for receiving an acknowledgement, for successful channel assessment, for turnaround and maximum time to wait for acknowledgement, respectively. the total energy consumption per node can be analyzed as follows."
"librarians at wayne state university conducted an assessment of their revamped information literacy tutorial, known as \"re:search.\" 19 [cit] wayne state federal trio student support service summer residential program, which was based on donald kirkpatrick's evaluating training programs: the four levels. 20 they concluded that their study highlighted some flaws in their tutorials, including navigational problems. as a result, they would consider partnering with wsu faculty in the future to develop better modules. one curious comment by the authors in their introduction warrants further discussion about assumptions made by librarians regarding student research skills: \"the internet has bolstered student confidence levels in their research abilities, increasing the demand for point-of-need instruction. students are accustomed to online learning, not only because of the shift in higher education to online coursework, but also because they have been leaning online through youtube, social networking, and other websites.\" 21 at purdue university, librarians evaluated the success of their seven-module online tutorial through the distribution of a post-test survey. these researchers found that the feedback received was essential for planning future versions of online instruction at their institution. 22 a report from zayed university (united arab emirates) outlined an evaluation of infoasis, the university's online information literacy tutorial, testing 4,000 female students with limited library proficiency and remedial english aptitudes. 23"
"a third approach is the prohibition of certain turns in the topology, instead of the link prohibition that occurs for typical spanning trees. therefore, the decision of which links forward frames that must been broadcasted depends on the label associated to the turns defined among two links connected to a node. a turnða; b; cþ around a node b is defined as the pair of links that join b with other two nodes like a and c. if the turn ða; b; cþ is prohibited, packets arriving at node b from link a à b cannot be forwarded to link b à c. when some turns are prohibited in a network topology, it can be assured that loops will never occur. compared to link prohibition strategies, turn prohibition provides increased link utilization. it has been proven for some algorithms that the restriction of less than a fraction of 0.33 of total turns guarantees loop-free topologies without blocked network links [cit] . note that the spanning tree protocol prohibits typically in the range of 0.7-0.9 of turns. the prohibition of turns was first proposed in autonet [cit], through the approach known as up/down routing, and since then other proposals have appeared, such as turn prohibition [cit] and tree based turn prohibition (tbtp) [cit] . the main drawbacks of the up/down routing proposal are the requirement for additional hardware and software in hosts and the non miscibility with standard bridges. the main drawback of tp and tbtp is its limited scalability due to the computational complexity of the conceptual approach, based on a complete knowledge of topology to select a near optimum set of prohibited turns that prevent loops. up/down is selected as a base due to its inherent simplicity that permits local election of prohibited turns once the spaning tree has been built."
"the first component is a mechanism to assign fixedsize hierarchical addresses, usable as local mac addresses, to hurba bridges. the design presented in this paper modifies the proposal previously made by the authors [cit] to define fixed length addresses that fit in the standard ethernet frame format. these ''private\" mac addresses, hierarchical local mac addresses (hlmac), are distinguishable from global mac addresses by their local/global bit set to the local value. this split of the addressing space allows the coexistence of standard, globally unique mac addresses with hlmac addresses. the combined spanning tree protocol (cstp) is a variation of rstp used to assign hierarchically the hlmacs to the bridges. hlmac addresses are used as ordered node identifiers by the improved up/down mechanisms to break cycles [cit] . they are also used by the distance vector routing protocol for cross link routing and optionally for direct (i.e. neither broadcast nor mac address learningbased) forwarding through the spanning tree links through destination address decoding."
"in wsns, sensor nodes communicate with other sensors through a wireless medium, where channel conditions oscillate between good and poor. however, when heterogeneous nodes in wsns share the same ism band, the network performance is degraded in single channel operations due to interference. besides, hidden and exposed terminal problems [cit] are most important issues in single channel communication environments. hence, single channel communication cannot provide reliable transmissions. multichannel communications can enhance reliable transmissions, alleviate hidden/exposed terminal problems, minimize the network interference and support parallel data communication, as a result of which network throughput can be increased substantially. in order to access a channel in a wsn, synchronous and asynchronous medium access control (mac) protocols are used. in the synchronous protocol, all nodes wake up at the same time, while in the asynchronous protocol nodes have different wake up times. thus, the synchronous protocol is most suitable for real time data transmission as it does not require any extra negotiation for data exchange."
"(asp ebsco uses boolean connectors by default, and natural language is usually ineffective.) another participant reached several dead-ends in the search before finally succeeding. while most of the control group participants were at least partially successful in completing the task, it is reasonable to suspect that they would have given up in frustration in a non-test situation, and would have benefited from point-of-need instruction. the participants who received the static text-and-image tutorial performed the best, completing the task with the highest speed and with the greatest accuracy (see table 3 ). all five of the participants in this group managed to find appropriate articles and to download the citation in mla format, though several had difficulty with the final task. all were able to navigate to the \"cite\" feature effectively, but all participants chose to click on the \"mla\" link rather than simply copy the citation. clearer directions in the tutorial might alleviate this problem. participants who received the dynamic video tutorial were more successful than those in the control group, but spent significantly longer on task than did those who received the static tutorial (see table 4 ). interestingly, two of the participants searched for \"climate change\" as the \"subject term\" in asp ebsco, even though the tutorial did not instruct them to do so. (su -subject term is one of the options in the drop-down menu in asp ebsco, which otherwise searches citation and abstract by default.) while \"climate change\" is a commonly accepted scientific term, and the searches produced relevant search results, it is not generally advisable to begin a search with controlled vocabulary terms."
"library instructional services take place in formal classes that are requested by faculty, as well as library faculty-initiated workshops held in either the libraries' computerized classrooms or at other on-campus locations. there is no mandated information literacy session. [cit], 333 instruction classes were offered to 4,435 students."
"for the first round of tests, members of the control group spent longer on the task and made more mistakes than those who received either the dynamic or the static tutorial (see table 2 ). for example, one participant in the control group was unable to download the mla citation, and another in the control group ventured outside the asp ebsco database platform to find the correct citation format. when members of the control group did succeed, they did so without a clear search strategy, evidenced by their use of natural language instead of boolean connectors."
"therefore, the probability of finding the channel busy during the cca is length of the aperiodic data packet; l 3 length of the gts request packet; l 4 length of the information request packet; l 5 length of the association reply packet; l 6 length of acklodgement to aperiodic data packet; l 7 length of the gts request reply packet; l 8 length of the information request reply packet; l 9 length of the gts request notify packet; l 10 length of the beacon packet; q 1 generation probability of the association packet; q 2 generation probability of the aperiodic data packet; q 3 generation probability of the gts request packet; q 4 generation probability of the information request packet. p s 1 probability of association success; p s 2 probability of aperiodic data transmission successful; p s 3 probability of getting gts slot successfully; p s 4 probability of information retrieval successful; p gts probability of gts transmission successful."
"each participant was scheduled for a specific fifteen-minute time slot. tests were conducted in a small meeting room in the library, with one participant at a time working with the facilitator. as the participants entered the meeting room, the facilitator greeted them and confirmed their identities. participants were provided with an information sheet (see appendix vi), which told participants that the session would be recorded, that the researchers were concerned with testing the libraryinstruction tutorials, not the participants themselves, and that the tests were confidential and anonymous. participants were also told that they could end the test at any time for any reason. additionally, the facilitator read aloud the points-ofinformation sheet. participants were invited to ask questions or voice concerns."
"for both rounds of tests, participants had use of a laptop computer with a browser window open to the asp ebsco home page. for those who received instruction, a second browser window was open to either the dynamic or the static tutorial. for members of the control group, no tutorial was available. those who received instruction were allowed to return to the tutorial at any point they wished."
"we have presented hurba, a novel layer-two architecture that combines distance vector routing and up/down, enhanced with the use of hierarchical bridge identifiers. hurba is designed as a zero-configuration architecture that extends and uses rstp as the underlying protocol for core tree building, bridge identifier assignment and core reconfiguration. the low computational complexity and the preservation of the ethernet frame format in the hur-ba core allow upgrading legacy bridges to hurba operation with just software updates, hurba is consistent with current 802.1d architecture and self configures building a core mesh to which legacy bridges attach building peripheral standard spanning trees."
"the maximum depth (number of levels) of the address is 6 for the default (implicit) format with 8 bits (up to 255 active ports per switch with the exception of up to 63 ports for the first level). when a switch is assigned an address of maximum depth it will not assign further addresses through its designated ports and will act as a hurp edge switch, thus setting the limit of the hurp core mesh."
"consider a wireless sensor network deployed over a certain region, which forms a two tier mesh topology as shown in figure 1 . the wireless sensor nodes in the second layer form a mesh topology to collect the data and to forward them to the next higher layer, i.e., to the network coordinator or the sink through two hops. it is to be noted that all nodes/coordinators in the network are refereed to as nodes throughout the paper and they use the carrier sense multiple access with collision avoidance (csma/ca) channel access mechanism to send or receive data. let, n be the number of nodes present within the carrier sensing range of one node in the wsn. let us assume that each node uses a fixed number of multiple channels and a multi-channel superframe (mcs) structure to compete with other nodes for accessing those channels to transmit the data. out of the multiple channels, one channel is used as the control channel and the rest are used as the data channel. the control channel is used only for beaconing by the devices in the network. for simplicity, the superframe structure can be considered as the dsme mode of the ieee 802.15.4e standard, as shown in figure 2 . the bi comprises several multi-superframes without inactive parts. each multi-superframe has a beacon slot at its beginning and consists of several superframes with contention access periods (caps) and contention free periods (cfps). these caps and cfps are used for the control and data packet transmission, respectively. it is to be noted that each superframe consists of 16 time slots of equal duration and the first slot of the first superframe present in the mcs is reserved for the pan coordinator to broadcast the beacon. the number of beacon slots is equal to the number of superframes present in an mcs. for example, as shown in figure 2, there are four beacon slots as the mcs consists of four superframes. only one beacon slot is used by one node to broadcast the beacon and the rest of the beacon slots are used to receive the beacon frames from other nodes. it is to be noted that the beacon interval comprises several multi-superframes without inactive parts and each multi-superframe has a beacon slot at its beginning. one beacon slot is used by the pan coordinator to broadcast the beacon and the rest of the beacon slots are used to receive the beacon frames from other nodes. the beacon broadcast by the pan coordinator is used for the purpose of synchronization with the superframe structure. it is also used for the re-synchronization of the nodes which are in the power saving or sleep mode. it is to be noted that a node is usually equipped with a half-duplex transceiver, i.e., it cannot listen when it transmits. hence, the channel coordination is a crucial issue for solving the deafness problem. to handle these problems, the nodes use the cap period to be associated with the other nodes or to send the aperiodic data or reserve the contention free time slots using the slotted csma/ca. a node during cap defers transmission if the remaining slot time is not big enough to transmit the request. the cfp period supports the multichannel communication and is used for the periodic data transmission by the allotted nodes without contention. besides, a group acknowledgement (gack) option is considered to provide a retransmission opportunity of a data frame within the same superframe if it failed in its gts transmission. the group acknowledgement also improves the efficiency of the communication as the acknowledgement for multiple data frames is aggregated in a single ack frame."
"in this paper, we propose a new channel access mechanism, which can significantly reduce the power consumption during the communications in control channel. an lpp model is designed to select the relay nodes, which can reduce the delay and packet drop rate among the nodes in a mesh topology as compared to the current ieee 802.15.4e standard. analytical models are designed to study the performance metrics of wsns such as data transmission reliability, throughput, energy consumption and delay. the results obtained from the simulation indicate that our protocol can improve the energy, reliability, throughput and packet success rate significantly. hence, our protocols can be implemented in synchronous mac based wsns for industrial, commercial and healthcare applications, where energy, latency, reliability, scalability and drop rate are the major requirements."
"the probability of the bit error rate p b (ζ) for the transceiver of ieee 802.15.4 radios is calculated [cit] as given in the following equation, where ζ is the signal-to-interference-plus-noise ratio (sinr)."
"the combined spanning tree protocol is an extension of the rstp protocol to benefit from its advantages and preserve compatibility with stp and rstp. it builds a spanning tree with all the interconnected hurp capable bridges and assigns to each one a hlmac addresses. the assignment of hlmac addresses to the bridges is piggybacked on the exchange of information required to build the spanning tree by the rstp protocol from root node to designated nodes. therefore, the cstp bpdu is a rstp bpdu extended with a field containing the hlmac (6 byte) address assigned to the bridge connected to the designated port that is emitting that bpdu. these bpdus, containing rstp information plus the hlmac address assigned to the receiving bridge, are exchanged periodically, every hello_time by every bridge with its neighbors. a bridge hlmac address emitted in a bpdu by a designated port is considered stable and assigned when the root port connected to the designated port is enabled. as with rstp, this occurs when the port transits to forwarding state. this situation is known at once by the root port bridge of the hurp bridge getting its hlmac assigned."
"the results of the usability study revealed two things: participants benefited from library instruction, through which they evidently acquired new skills; and participants benefited more from static text-and-image tutorials than from the dynamic audio/video tutorials. in both rounds of tests, the participants who received the text-and-image tutorials performed the tasks more effectively than did members of the control group or those who viewed the dynamic tutorials."
"the second component is hurp (hierarchical up/down routing protocol), an enhanced up/down routing algorithm and protocol that establishes shortest path routes that respect turn restrictions based on distance vectors with topologically significant node identifiers. compared to up/down, hurp enables turns that would be prohibited by up/down, but are known to end up in the branch of the spanning tree to which the destination belongs."
equation (7) derives that the nodes decrement the backoff counter with probability 1. equation (8) deduces the channel busy probability during cca and selects the next backoff state. equation (9) derives the probability of going back to the idle state. based on these transition probabilities the steady state probabilities can be derived as follows. the steady state probability of entering into the next backoff state by a node is
"a tagged node starts transmission when the channel is found to be idle during cca. it enters into the initial backoff state if the transmission is successful and has a new data packet to send or transmission failure occurs without exceeding the retransmission limits. hence, the steady state probability can be derived as"
"in fig. 1, the path 4-3-1-2-4-6 does not contain a cycle: although node 4 is visited twice, no link is traversed twice. a turn is defined as a pair of input-output links around a node. the tuple ða; b; cþ represents the turn at node b from link a à b to link b à c. in fig. 1 the turn ð3; 4; 2þ is the turn from node 2 around node 3 to node. unless otherwise stated, the turns are symmetrical by default, so the turn ða; b; cþ is identical to the turn ðc; b; aþ. note that the total number of possible turns around a node of degree d is"
"the third component is an extension of the rapid spanning tree protocol (rstp) as a core building protocol that auto configures a core of hurp bridges, where the spanning trees of 802.1d bridges get attached. the result is a zero-configuration architecture, compatible (in island mode) with standard 802.1d bridges."
"at fig. 1c the prohibited turns by up/down protocol using the down-up criteria will be (3-4-2), (3-4-1), (2-4-1) and (4-6-5) because the identifier of the node in the middle of tuple is higher than the other two. with the identifier assignment criteria, the ''up\" part of a link is the end that is closer to the root bridge, so this assignment forbids in particular paths that descend in the spanning tree and use another branch of the spanning tree to ascend. note that the identifiers used by up/down routing are flat and ordered, but do not convey other tree related topological information."
"ieee 802.1aq shortest path bridging (spb) [cit] aims to improve network infrastructure utilization and reduce path length by replacing the mstp [cit] control plane with shortest path trees. spb operates in a shortest path tree (spt) bridging region in which it defines multiple tree instances rooted respectively at the edge bridges to obtain shortest paths among spb bridges. accordingly, a shortest path trees bridging region corresponds to a multiple spanning tree region of the 802.1q standard. as in mstp, multiple regions may exist and are differentiated by a per-region identifier. each tree is associated to a specific vlan. spb uses shared vlan learning (svl) of mac addresses among vlans for frames allocated in different spanning tree instances. different choices are available for the computation of the set of symmetric shortest path trees between each of the bridges of an spb region: a derivation of mstp protocol (distance vector based) with the addition of cut-bit vectors to ensure symmetry of tree instances; the use of an extension of the is-is protocol with additional information and procedures; and finally, the use of a new link state tree protocol (lstp). the proposal is compatible with rstp, and with carefully configured mstp (802.1q)."
"in this part, we analyze the throughput for the cfp period. as per our assumption, a node should get a gts slot to transmit the data in the cfp period. when the node looses synchronization, it retrieves the information through the csma/ca mechanism before the scheduled gts slot. hence, the data packet will be successfully transmitted if the node synchronizes and transmits the data successfully during the allocated gts slots. the throughput is expressed as the fraction of time spent in transmitting the data packet successfully. hence, the throughput during cfp can be derived as follows."
-the dependency of up/down and hurp with the bridge elected as root bridge is low and bounded in all simulated networks. it seems that this dependency appears just in very specific topologies in which only one or a few nodes have high degree and are positioned in the lowest levels of the spanning tree would exhibit this dependency.
"in the protocol version identifier field, a new value is used to distinguish whether the bridge is executing the standard stp, rstp, or the new cstp protocol. using a similar mechanism to the stp/rstp protocol migration dialog between 802.1d standard bridges [cit], hurp capable bridges listen to the protocol version identifier at the other end at each port and execute accordingly to the highest functionality protocol common to them: stp, rstp or cstp. bridges that have no connection to the cstp spanning tree default to standard bridge (stp or rstp) as described below."
"as concern grew at the onset of the twenty-first century for the need to evaluate online library tutorials, articles on library website usability testing began to appear more frequently. in one study, the authors noted that they would not have identified problems with their website had they not done usability testing: \"testers' observations and the comments of the students participating in the test were invaluable in revealing where and why the site failed and helped evaluators to identify and prioritize the gross usability problems to be addressed.\" 13 librarians aiming to examine their patrons' ability to independently navigate their library's webpage to fulfill key research needs, conducted similar studies. at western michigan university (wmu), librarians investigated how researchers navigated the wmu library website in order to find three things: the title of a magazine article on affirmative action, the title of a journal article on endangered species, and a recent newspaper about the senate race in new york state. they successfully used the data gathered to identify problems with their website and to establish goals and priorities in clarifying language and navigation on their site. 14 more recently, researchers conducted a usability study with the aim of showing how librarians could build websites to better compete with nonlibrary search sites such as google, which would allow greater personalization by the individual user and more seamless integration into learning management systems. 15 other researchers have studied the readability of content on academic library websites. in one such study, lim used a combination of readability formulas and focus groups to evaluate twenty-one academic library websites that serve significant numbers of academically underprepared students and/or students who spoke english as a second language. they concluded that the majority of information literacy content on library pages had poor readability, and that the lack of welldesigned and well-written information literacy content could undermine its effectiveness in serving users. 16 kruger, ray, and knight employed a usability study to evaluate student knowledge of their library's web resources. the study produced mixed results, with most students able to navigate to the library's website and the opac, but large numbers unable to perform basic research tasks such as finding a journal article. the authors noted that such information would allow them to modify library instruction accordingly. 17 another study focused on the use of language as it relates to awareness of relevant databases. at bowling green university library, staff members attempted to learn more about how users find and select databases through the library website's electronic resources management system (erm). because of their study, the authors recommended that librarians should focus on promoting brand awareness of relevant databases among students in their subject disciplines by providing better database descriptions on the library webpages and by collaborating with subject faculty members. 18"
"in this section we compare the maximum throughput obtained with the algorithms studied and the same topologies described in the evaluation of the number of turns prohibited and path lengths. to evaluate the dependency of compared algorithms with the root bridge elected, the calculation is iterated n times per topology, with a different bridge root bridge at each iteration."
a new channel access mechanism is designed to reduce the energy consumption during communications in any control channel. the rest of the paper is organized as follows. related works are given in section 2 and our proposed network model and methods are elaborated on in section 3. performance analyses of various network parameters are given in section 4. model validation and performance evaluation of our proposed protocols are presented in section 5. the overall concluding remarks are made in section 6.
"the hurp (hierarchical up/down routing protocol) is a hierarchical distance vector routing and forwarding protocol. hurp exchanges distance vector information over a virtual topology built using enhanced up/down for bridges that have been assigned hlmac identifiers. hurp allows routing through cross links, (as long as turns are permitted), otherwise blocked by the spanning tree protocol, while preventing frame loops just by turn prohibition of the down-up turns. hlmac addresses are also used to forward frames along the branch of the spanning tree in which the destination is located by direct decoding of the ports to select, that are coded in the destination address, without neither routing tables nor mac address learning, when no routes are available."
"for each graph generated the spanning tree is computed and hierarchical identifiers are assigned. prohibited turns are computed for spanning tree, up/down and hurp. routes are computed with the corresponding stp, up/down and hurp protocol restrictions, and without prohibited turns, to be able to compare with shortest path. the metrics evaluated are: fraction of prohibited turns, average path length and maximum throughput. the fraction of prohibited turns and the path length are obtained from the resulting graph, while for the throughput estimation we use a flow model that is described in section c."
"it would be desirable to develop a new ethernet architecture that could combine the features of standard bridges and routers, while avoiding the limitations of the spanning tree protocol. such architecture should fulfill a number of requirements: it should scale to large networks (up to 20,000 bridges or more) with adequate network segmentation to limit damages, should require low configuration (ideally, zero-configuration), and it should preserve transparent operation regarding to hosts and routers."
"if a node misses the beacon, it can get its gts information by sending the information request packet to the coordinator. the retrieval of gts information will be successful if the coordinator receives the information request successfully and the node gets the corresponding acknowledgement along with the information response successfully. hence, the probability of retrieving the information packet successfully from the coordinator is"
"in this section we perform simulations to compare the performance of hurba with other protocols such as shortest path routing, spanning tree and the de facto reference for turn prohibition algorithms, the up/down algorithm. however, conditions similar to papers presenting performance results for tp and tbtp are also simulated to allow a comparison with these protocols. the main performance parameters to evaluate are throughput and path length. the fraction of prohibited turns to total turns is also evaluated, in order to provide an insight on throughput and path length results."
group 1 and group 2 were divided into three subgroups each. the first subgroup was the control group and received no instruction. the second subgroup was given access to the dynamic audio/visual tutorial (see appendix iii). the third subgroup was given access to the static text-and-image tutorial instruction (see appendixes iv and v). each subgroup consisted of five unique participants.
"although the usability tests generally went smoothly, researchers did encounter occasional difficulties with audio between the testing room and the observation room when it became difficult to hear what the participant was saying as he or she was completing the task. fortunately, the researchers kept recordings of each test, which allowed them to review those where the audio quality was less than optimal. to save time and run the tests more efficiently, however, the researchers recommend purchasing a high-quality microphone like those used for teleconferences."
"the turns that reach the destination branch can be permitted because, once a frame arrives at any point of the destination branch, it is guaranteed that the frame will reach the destination bridge, without cycles, just descending or ascending through the destination branch. this is so because the remaining path to the destination is a shortest path because both transited and destination bridges are located in a branch of the spanning tree. since the spanning tree is built selecting shortest paths to root bridge, fractions of these paths are also shortest paths."
"tree based turn prohibition (tbtp) [cit], is an evolution of turn prohibition. tbtp relies partially on spanning tree information to prohibit less than half of the turns of any graph. the improvement in the fraction of prohibited turns increases with the node degree. tbtp requires a global knowledge of the network topology to determine the turns to prohibit, and the complexity of the algorithm is"
"after successfully getting the gts slots, the node transmits the data packets during its allocated gts slots. transmission in the gts slots fails due to channel error, failure of beacon tracking or if the node does not get the gts information before the allocated gts slots. thus, the probability of successful transmission in the gts slot is"
"forwarding in shortest path mode through cross links makes use of the routing tables constructed with the interchange of distance vectors. routing may be performed on an exact match of destination address or on a prefix matching basis. this allows the aggregation of routes and shortens routing tables. fig. 3 shows, with a discontinuous line, the route followed by a frame from an originating terminal with hlmac address 32.7.6.5.1 until destination host with address 32.7.1.5.0. the first leaf bridge 32.7.6.5 has a shortest path route through intermediate bridge 32.7.6.1 and forwards the frame through cross links a and then b until destination f 32.1.5"
"in this section we compare the results for path lengths of the spanning tree, shortest path, up/down and hurp protocols in the above mentioned topologies. hurp consistently improves path length compared to up/down. table 6 shows the average path lengths for the 120 node waxman topologies with random node degree distribution as a function of average node degree of topology. path lengths for hurp are very close (less than 3%) to shortest path and always shorter than up/down. table 7 shows the average path lengths for fixed node degree topologies. note that hurp results are much closer to shortest path than to up/down. table 8 shows the path length results for the random topologies with barabasi-albert topologies, scale free degree distribution. hurp path lengths are very close or coincide with shortest paths. it is worth noting that path length is nearly protocol independent, i.e., the resulting values are very close for up/down, hurp and shortest path. this path shortening is probably the result of the ''hub effect\" of nodes with high connectivity."
"here, we call it leave one subject out cross validation which is hasher than the (a) action \"throw\" (b) action \"push\" experiment setup in paper [cit] which used leave one sequence out cross validation (loocv). the recognition accuracies for each action are listed in table 3 . from table 3, we observe that most of the actions are correctly classified and the average accuracy is 95.5%. the action throw obtains the lowest accu-310 racy while most of the wrong classified samples are confused with action push due to the similarity of the two actions (fig.5) . table 4 shows the performance of our method compared to the state-of-the-art approaches. here, we adopt cross-subject experimental setting in which subjects 1,3,5,7,9 are used for training and subjects 2,4,6,8,10 are used for testing. our results are obtained 315"
"the first step in attempting new assignments involved recalling the rule that horizontal corners could only be labelled in pairs of 1-2 or 3-4, applying the rule to a given diamond, and adding the appropriate numeral to the corners. the second step was to mentally add one of the numerals to the bottom corner and assess whether the resulting diamond would be identical to any of the fully labelled diamonds in the chain. if the diamonds were distinct, then the numeral was added to the corner (if they were identical, the process was repeated with a different numeral). given the psychological benefits of employing external diagrams, it is not difficult to conjecture why the diagrammatic method was useful. in the last step, for instance, a derived diamond had to be compared with the previously labelled diamonds in the chain. the fact that previous diamonds had been recorded on paper made this comparison quicker and more reliable than if one relied on memory. for similar reasons it was beneficial to write down the numerals for the horizontal corners. for then one did not need to remember them when comparing the diamond to others. in conclusion, the diagrammatic method allowed task decomposition, including the conversion of mental images to external marks for later reference. the diagrams thus served as an external information store and freed up working memory. as noted in the introduction, what matters here is that the diagrams were physical as opposed to merely imagined. not at issue here is the specific type of physical medium used, i.e. ink and paper instead of, say, chalk and blackboard. 9 the phenomenon of task decomposition sheds light on the relative significance of physical diagrams and their mental images. if diagrammatic reasoning involves both, then what is their relation? is one somehow more important than the other? or do they play distinct roles and, if so, which? these questions form the backdrop of a dispute about diagrammatic reasoning with chemical formulas. [cit] objected that chemists mostly manipulate mental images of formulas. the present case study suggests a possible resolution. in the case of diamond diagrams, researchers manipulated a mental image of a diagram, recorded the result on paper, and then iterated this process until all viable options were exhausted. thus, the final diagram materialised gradually as a result and record of mental manipulations. both physical diagrams and their mental images were important, but their roles were distinct: physical diagrams acted as an external information store, whereas their mental images were the substrate of mental manipulations. this may also be the pattern at work in the case of chemical formulas."
"i argued that scientists employed the diagrams (physical models) as external information stores to which they returned repeatedly in order to perform subsequent modelling steps. more specifically, external diagrams were instrumental for decomposing complex tasks into many smaller steps so that the work with diagrams alternated between mental manipulations, recording their results, and then feeding the results back into subsequent mental manipulations. in addition, physical models allowed replacing explicit reasoning about chemical facts and theoretical assumptions with the visuospatial manipulations of diagrams, which are cognitively less demanding. both crick's arrow (fig. 3 ) and gamow's remark about \"rotating the diamonds\" provide historical evidence that visuospatial reasoning in the form of mentally rotating 20 i thank a reviewer for prompting me to clarify the implications of this case study for distributed cognition. diamonds was part of modelling with diagrams. therefore, the models' physicality affected the very nature of scientific reasoning."
"in the first instance, diamond diagrams were static depictions of purported real-world entities (dna cavities). in addition, diamond diagrams were malleable devices for filling gaps in gamow's (1954a) original proposal. one scientist engaged in this activity was francis crick. his unpublished notes have survived and now allow us to understand the diagrammatic procedure [cit] . i will explain some of crick's work by reference to a handwritten note that remained unpublished (pp. 17-26 in pdf). 2 the note counts 9 numbered pages, but we only need the brief section reprinted in fig. 2 . this section contains three main steps. first, crick wrote down a known amino acid sequence. he then wrote down the diamonds he had assigned to specific amino acids in previous sections of his note. the third step was key: it was the attempt to derive new assignments from the information contained in the diagrams. let us look at these steps in more detail."
"another approach, however, was much simpler: mentally rotating one diamond (t 3 ) and then comparing the rotated diamond with the second diamond (t 4 ). if they were identical, then one could infer that the two diamonds depicted the same cavity. remarkably, a physical record of this procedure has survived in crick's notes: the curved arrow on the right side of fig. 3 [cit] . 184 of pdf). the arrow stood for the activity of mentally rotating the top diamond 180°sideways around its vertical axis. this manipulation moved the 3 in the left corner to the right and the 4 from right to left. crick recorded the outcome of the rotation as the second diamond (fig. 3) . notice that he counted both diamonds as variants of the h-diamond. applied to the diamonds above: one rotates t 3 180°sideways and compares the outcome to t 4 . since the rotated diamond was identical with t 4 (fig. 3), they represented the same functional cavity, i.e. the one specifying leucine."
"reasoning with external diagrams can have a second major benefit, which is to replace explicit reasoning by something less cognitively demanding, i.e. visuospatial processing [cit] . this benefit, too, is manifest in the diagrammatic method. at this point, another feature of gamow's proposal becomes relevant. gamow (1954a) assumed that some cavities were functionally equivalent while being structurally distinct: some cavities could specify the same kind of amino acid despite having different base configurations. in particular, cavities differing only in the position of their complementary base pair specified the same amino acid. the pair 3-4, for instance, could be positioned in two ways. in one cavity, base 3 might sit on the dna parent strand and 4 on the daughter strand (top diamond in fig. 3 ). the reverse might hold in another cavity (second diamond in fig. 3 ). gamow (1954b) argued that this structural difference was functionally irrelevant because most amino acids were bilaterally symmetric. their side chain could be turned around the cα-bond by 180° [cit] ), 10 which would allow them to bind to both cavities. so, 9 i thank a reviewer for emphasising this contrast. 10 gamow's explicit claim was restricted to the twelve cavities that had distinct top and bottom bases (marked * in fig. 1 ), but it equally applies to the other eight cavities. he may have emphasised the 12 cavities because swapping the complementary bases in these cavities resulted in changing the combination fig. 3 rotating the h-diamond sideways (circular arrow on the right) and bottom-up. extract of crick's notes on \"errors in gamow's paper\" [cit] -54, p. 184 of pdf) . published with the permission of the wellcome collection the first and second diamond in fig. 3 represented cavities distinct in structure but not function (hence denoted as the same diamond h). the second difference concerned the orientation of an entire cavity within a chain of cavities. on gamow's scheme, a cavity sitting in one direction could just as well face in the opposite direction, and without affecting its function. for example, the upper right side of the cavity depicted by the h diamond was formed by numerals 1 and 4, and its lower left side by 3 and 4 (figs. 1 and 3). but this cavity could be positioned in the opposite direction, so that the 1-4 side came to lie on the lower left and the 3-4 side on the upper right (fourth diamond in fig. 3 ). [cit] noted, gamow simply assumed that the orientation of a cavity within a chain made no functional difference. so, diamonds one and four in fig. 3 depicted cavities distinct in structure but not function, as did diamonds two and three. the four diamonds depicted four structurally distinct cavities that all specified the same amino acid. indeed, h was one of 12 diamonds with four functionally equivalent variants each, marked * in gamow's (1954a) gamow's additional assumption (that some structurally distinct cavities were functionally equivalent) complicated the translation of known amino acid sequences into corresponding diamond chains. it raised peculiar challenges, such as deciding whether or not two distinct diamonds represented functionally equivalent cavities and demonstrating that two structurally distinct diamonds did in fact represent functionally equivalent cavities. the latter challenge arose with respect to the hypothetical leucinespecifying cavities for insulin b. crick represented the leucine-specifying cavity that lay between the alanine-and tyrosine-specifying cavities as the t -diamond from footnote 10 continued of bases lying on the same strand. note also that the 12 cavities were initially referred to as \"bilaterally symmetric\" [cit] b ) and later as \"bilaterally antisymmetric\" [cit] ). gamow's (1954b) table. that t -diamond had a 3 in its left corner (call this diamond t 3 ). by contrast, the diamond representing the leucine-specifying cavity above the tyrosine-specifying cavity had to have a 4 in its left corner (t 4 ). the difference in numerals between t 3 and t 4 implied a structural difference between the corresponding cavities, a difference that had to be functionally irrelevant since both cavities were supposed to specify leucine. so, the task was to demonstrate that, indeed, the cavities' structural difference did not affect their function."
"since then, deep learning has been widely employed in many research areas such 30 as image classification, speech recognition, object recognition, etc. [cit] . there are also many studies on action recognition using deep learning based on either 2d color image sequences [cit] or 3d depth video sequences [cit] ."
"according to the difference of extracting features from video sequence, the 15 action recognition methods can be grouped into two categories: hand-crafted feature-based methods and automatic learning feature-based methods. handcrafted feature-based methods usually employ a three-stage procedure consisting of feature extracting, feature representation, and classification. firstly, hand-crafted features such as space-time interest points (stip) [cit], bag-of- 20 visual-words [cit], histograms of oriented gradient/histograms of optical flow (hog/hof) [cit], and motion history image (mhi) [cit] are extracted from video sequences. then more discriminative descriptors are constructed from the extracted features using transformations or clustering techniques, such as fourier temporal transformation [cit] and k-means clustering [cit] . finally, a 25"
"however, the inputs of networks in most of these deep learning methods are pre-extracted hand-crafted features instead of raw depth sequences. unlike the 35 existing methods, in this paper, we apply deep learning to automatically learn discriminative features from raw depth sequences for human action recognition."
"from the conclusion of section 4.4, the superiority of 3d 2 cnn is to learn the 350 structural information of the video, not the fine-grained one. while hand-crafted methods extract features from every pixel, which means structural knowledge and fine-grained features are treated in the same way."
"reasoning with external diagrams is known to have several benefits, such as freeing up working memory by storing information externally [cit] . external diagrams are especially useful to the extent they allow decomposing a complex task into a series of smaller, more manageable activities. in task decomposition, one engages mental imagery in a stepwise fashion by recording intermediate results externally before proceeding to the next mental task [cit] . crick employed the diagrammatic method in exactly this manner. the task of assigning amino acid-cavities proceeded in three major steps, each of which involved mental processing followed by recording intermediate results externally: (1) identify a known amino acid sequence and write it down, (2) identify the dna cavities that are already assigned to diamonds and then writing out the series of diamond, and (3) attempting new assignments. the latter task was, in turn, decomposed into steps in which cognitive processing alternated with recording intermediate results."
"the diagrammatic method allowed inferring from facts about the post-rotational identities of diagrams to the functional equivalence of real-world entities. in order to see what justified these inferences, let us return to crick's figure and consider the words \"symmetry concepts\" written next to the first diamond (fig. 3, top left) . the words invoked gamow's assumption that cavities differing only in the position of their complementary base pair specify the same amino acid (due to the bilateral symmetry of the latter). now, two cavities differing only in the position of their complementary base pair were depicted by diamonds differing only in the position of their left-right numerals (e.g. t 3 and t 4 ). when one such diamond was rotated 180°sideways, it became identical with the other. the post-rotational identity of diamonds therefore showed that the diamonds represented two cavities whose structural difference gamow deemed functionally irrelevant."
"while translating the insulin b sequence one-by-one into the corresponding dna cavities, crick eventually reached the valine in the sequence valine-glutamic acid-alanine-leucine-tyrosine-leucine. the only diamond left to represent the valinespecifying cavity was s. yet drawing an s-diamond in that position proved impossible. an s-diamond would need a 3 in its top corner ( fig. 1 ), but the top corner of the diamond in that position of the chain could only be 1 or 2. 7 this result allowed a conclusion about the corresponding cavity. since one could not draw an s in the right position of the diamond chain, there could not be a valine-specifying cavity where there needed to be one. however, the empirical data (i.e. the presence of a valine) clearly implied that on gamow's proposal there had to be such a cavity. crick highlighted this result with an arrow, adding \"contradiction since val s\" (fig. 2) . the impossibility of drawing an s-diamond was therefore an important result. it meant that insulin b could not be synthesized in the way gamow had proposed. his proposal was unworkable."
"scope the diamond diagrams did not increase the scope of correct what-if inferences because gamow's hypothesis was false. nevertheless, we can ask: (1) would the diagrams have increased the scope if the hypothesis had been correct? (2) would construing the diagrams as cognitive components have affected the scope? in order to answer these questions, recall gamow's attempt to determine whether his proposed mechanism allowed exchanging exactly one amino acid in a protein (sect. 2.2.2, fig. 4 overall, it can be seen that the diagrammatic method improved scientific understanding by increasing the reliability of reasoning and by making theoretical assumptions explicit. furthermore, these achievements can be explained by the fact that the method relied on external, physical marks. there is no evidence that construing the marks as parts of cognition makes a difference. these findings strongly suggest that embodied cognition is sufficient for explaining how the diagrammatic method improved scientific understanding. that said, the findings do not exclude the possibility that construing the marks as cognitive might affect other kinds of theoretical roles or that it might deliver benefits of an altogether different sort. however, at present these are theoretical possibilities without positive support. so, on balance, the current evidence favours the conclusion that the diagrammatic method is best understood as an instance of embodied, rather than extended, cognition [cit] sense) ."
"at the feature learning step, the 3d 2 cnn model is implemented in torch7 model to be properly normalized log-probabilities and can be achieved using a soft max function. we employ a stochastic gradient algorithm to train the 260 neural networks. the active function is the hyperbolic tangent function (tanh) since our datasets are relatively small and the learning rate is set to 5e-4 empirically. on classification phase, the linear classifier (liblinear) [cit] with default parameters is used for action recognition."
"here, 3d 2 cnn means we take convolution both from spatial and time dimen-40 sion over the input video by using deep convolutional neural network. to our knowledge, there are little research which use raw depth video sequences as an input in deep learning-based action recognition. our proposed framework is evaluated on two well-known datasets: utkinect-action3d [cit] and msraction3d [cit] . our method obtains comparable performance to state-of-the-art 45 methods on the utkinect-action3d dataset (table 4) and achieves superior performance in comparison to baseline methods on the msr-action3d dataset (table 5 )."
"however, translating the insulin sequence into a diamond chain was far from trivial. gamow had proposed that adjacent cavities share bases such that a given cavity shares two bases with its upper and two with its lower neighbour. since bases were represented by numerals, only diamonds that had the same two numerals on a side could be written next to one another (fig. 2, circled numerals). crick approached the task by first focusing on the diamonds he had already allocated to amino acids. in earlier sections table of 20 diamonds. each diamond depicted a dna cavity that (purportedly) specified one of the 20 amino acids. numerals denote dna bases; the horizontal lines within diamonds connect complementary bases; capital letters denote diamonds [cit] b) . reproduced with the permission of the royal danish academy of sciences and letters"
this paper investigated how the physical features of scientific models affect their use and whether modelling is extended cognition. these topics were approached by means of a historical case study in which diagrams served as models for elaborating a mechanistic hypothesis.
"we first evaluate the efficiency of the proposed 3d 2 cnn method and compare the recognition result with the state-of-the-art results on the utkinectaction3d and the msr-action3d datasets. for the utkinect-action3d dataset, we train a network for each subject. for each network, sequences of one subject are used for testing and sequences of other subjects are used for training."
"( the remainder of this paper is organized as follows: section 2 reviews the related 65 work. section 3 presents the overall structure of the proposed framework and the detail of 3d 2 cnn model. the experimental results and discussions are described in section 4. finally, section 5 concludes the paper."
"the third step aimed at establishing new diamond-amino acid assignments. this step can be subdivided further. the start was made with an implicit rule according to which the numerals in the left-right corners of a given diamond could only be 3-4, 4-3, 1-2, or 2-1. the rule reflected the convention that the left-right numerals depicted the complementary bases of the corresponding dna cavity. consider the diamond marked 'cys' in fig. 2 and note the 3 in its left corner. for a 3 on the left, the rule prescribed a 4 on the right. hence a 4 could be added to the right corner of that diamond. similarly, the diamond marked 'ala' required a 3 in its left corner since it had a 4 on the right (fig. 2) . after labelling the horizontal corners, attention shifted to the bottom corner of the diamond marked 'ala'. 5 its bottom corner could only be 1 or 2 (\"1, 2\" fig. 2) . 6 given the numerals in its other corners, that diamond could be either d or n. since the diamond needed to depict an alanine-specifying cavity (due its position within the chain), that type of cavity could only be represented by either d or n."
"the diagrammatic method was an instance of modelling because of the way diagrams were modified and then enabled inferences to purported real-world entities. consider the diamond that depicted the alanine-specifying cavity (fig. 2) . initially the diamond lacked numerals in its left and bottom corners. it was then subjected to a series of operations, such as considering the already labelled corners and the other diamonds in the chain. on this basis, the missing numerals were added and it was concluded that the diamond in this position was either d or n. since the diamond in this position had to depict the alanine-specifying cavity, it was possible to draw the following inference about a real-world target: the alanine-specifying cavity has one of two possible base configurations, either the one depicted by the d-or the one depicted by the n-diamond. in short, the diagrammatic method proceeded by modifying certain initial features of one system (diagrams) and then taking the resulting features as the basis for justifying conclusions about a real-world system (template units). and this procedure is the strategy of working with scientific models [cit] . the more general point here is that, while gamow's article presented his mechanistic hypothesis in both linguistic and visual formats (text and diagrams, respectively), his modelling practice employed the visual format."
"in this section, we present a comparative performance evaluation of our proposed method on two datasets: the utkinect-action3d dataset [cit] and the msr-action3d dataset [cit] . both two datasets were captured using a station- in total, there are 567 action sequences. according to the test setting of the 280 baseline method [cit], the 20 actions was divided into action subsets as1, as2 and as3 (table 2), each consisting of 8 actions. the as1 and as2 were intended to group actions with similar movement, while as3 was intended to group complex actions together. the main challenge in the msr-action3d dataset is that some of the actions are very similar to each other, especially in as1 and as2."
"in order to evaluate the performance on different spatio-temporal size to per- cross-subject method on all actions, in which subjects (1,3,5,7,9) are used for training and subjects (2, 4, 6, 8, 10) are used for testing for both datasets. table 6 and 7 report the performance comparison on the utkinect-action3d and the msr-action 3d as3 dataset using different spatial size respectively. from these two tables, we observe that the classification accuracy is slightly improved and 365 tends to be stable with larger spatial sizes since 3d 2 cnn can learn the whole structure information of the video. the video reserves majority of information as long as the cuboids retain basic structure of each frame and keep smooth in time dimension after being resized."
"for the first step, crick took the sequence of insulin b [cit] and represented its middle section as a series of three-letter words (e.g. leu for leucine, fig. 2 ). on gamow's hypothesis, the existence of this particular amino acid sequence implied the existence of a corresponding series of dna cavities, which could synthesize it. if gamow was right, then it should be possible to represent the insulin sequence as a sequence of the corresponding dna cavities and, hence, as a series of diamonds."
"let us look more closely at the cavities' structure and their diagrammatic representations. cavities were thought to be composed of four bases arranged in a cross pattern, thereby creating diamond-shaped holes on the surface of the double helix. the cavities were depicted as diamonds with numerals (bases) in their corners (fig. 1) . a single cavity was depicted as a single diamond, and a sequence of cavities as a sequence of diamonds (fig. 2) . note that fig. 2 lacks the straight lines of the diamonds depicted in fig. 1 . however, the four numerals belonging to one diamond are still arranged in the shape of a diamond, just as in fig. 1 ."
"let us review the key findings of sect. 2: modelling with diamond diagrams included a lot of work with physical marks on paper, and such work was clearly beneficial. physical diagrams freed up working memory by acting as external memory stores (sect. 2.1) and helped to replace explicit reasoning in chemical terms with visuospatial reasoning (sect. 2.2). it is hard to see how the results, the elaboration and refutation of gamow's hypothesis, could have been achieved without diagrams functioning as external, physical entities. these findings do not undermine the importance of imagination for scientific modelling [cit] . indeed, the mental rotations described in sect. 2.2.2 underscore the centrality of imagination as well as the intricate to-ing and fro-ing between imagining and inscribing."
"compared to hand-crafted feature-based methods, deep learning-based methods are deemed to be more generic in transferring of learned model among differ- can extract high-level features (fig.2) for each video in the utkinect-action3d dataset. then, the extracted high-level features and jointvector are fused to recognize the actions. in this feature transferring experiment, we achieve an excellent performance at an accuracy of 95%. the performance is comparable to the state-of-the-art results of paper [cit] whose recognition accuracy is 97.08%."
"let us turn to the mechanistic character of the diagrammatic method. employing the method amounted to mechanistic research because it was supposed to help elucidate certain components of a mechanism as well as elaborate the corresponding mechanistic hypothesis. more generally, the aim was to determine the mechanism responsible for arranging amino acids into a linear order (the explanandum phenomenon). gamow (1954a) hypothesized that the linear order of amino acids was the result of incoming amino acids being attached to a pre-existing template, i.e. to the linear series of cavities on the surface of the dna double helix. thus, gamow postulated a number of entities and activities, e.g. cavities and non-covalent bonding, that could give rise to the explanandum phenomenon. since gamow's hypothesis concerned a possible mechanism for arranging amino acids, it was a mechanistic \"how-possibly model\" [cit] sense."
"the 3d deep network for learning features in fig.2 is responsible for highlevel feature learning tasks. we construct the 3d deep network according to previous studies [cit] and the size of our training data set. it consists of two 3d convolution layers, each of which followed by a 3d max pooling layer. the input to the network would be cuboids which originate from raw depth sequences table 230 1 list the size of convolutional filter and convolution stride corresponding to cuboids of different input size. unless stated specifically, all other parameters in following experiments are same as that used on the msr-action3d dataset."
"overall, we see that mentally rotating physical marks on paper generated the same conclusion as explicit reasoning with chemical facts and theoretical assumptions, albeit with less effort. there was no need to interpret the numerals semantically, no need to remember gamow's assumption about complementary base pairs, and no reason to verify the bilateral symmetry of particular amino acids. the diagrammatic procedure bore a striking similarity to the mental rotation tasks used in contemporary psychology in order to study visuospatial reasoning [cit] . visuospatial reasoning is, broadly speaking, a form of reasoning about the visual and spatial properties of objects. in many rotation tasks, subjects are given two images of geometrical objects and asked to decide whether or not the objects are the same. solving this task requires mentally rotating one of the objects until it can be compared to the other, making the comparison, and then judging whether or not the objects are the same. these are the key activities of visuospatial reasoning that we also find in the diagrammatic procedure."
"with the ever-increasing growth in the popularity of digital videos, there are many research topics on automatic video analysis. among these topics, human action recognition (har) has been widely applied to a number of real-world applications, e.g., surveillance event detection, human-computer interaction, video 5 retrieval, etc. however, it is of great challenge to recognize human actions in unconstrained videos due to some real conditions such as occlusions, different viewpoints, different action speeds, light variances, etc. fortunately, the emergence of depth cameras with acceptable price provides a prospect future for action recognition. compared with traditional rgb cameras, depth cameras 10 can obtain the conventional two-dimensional (2d) color video sequences as well as the depth sequences which are more insensitive to lighting changes [cit] and more discriminative than color and texture features in many computer vision problems such as segmentation, object detection, and activity recognition [cit] ."
"the task could be approached by explicit reasoning in physico-chemical terms. with respect to the example above, the first step was noticing that the 3 and 4 in the corners stood for complementary bases and that therefore the two diamonds represented cavities in which the complementary bases sat on different dna strands. the second step was to recall gamow's assumption that this difference did not matter functionally, as long as the specified amino acid was bilaterally symmetric. leucine happened to be bilaterally symmetric and it could therefore bind to both cavities, simply by turning its residue 180°around its cα-bond. such reasoning, i.e. applying general chemical facts and theoretical assumptions to a particular case, allowed concluding that the two cavities were indeed functionally equivalent."
"giere maintains with respect to diagrams that \" [w] hat is interesting about diagrammatic reasoning is the interaction between the diagram and a human with a fundamentally pattern-matching brain. rather than locating all the cognition in the human brain, one locates it in the system consisting of a human together with a diagram\" [cit], italics in original) . here, the cognitive aspects of diagrammatic reasoning include not only brain states but also diagrams, i.e. things outside one's head. similarly, nersessian (2006, pp. 701-702) defines 'cognition' in the context of model-based reasoning as comprising \"a complex system, \"stretched over\" what have been thought of as \"internal\" and \"external\" representations and processes."
"this study also explored the link between distributed cognition and modelling with physical aids. i adopted the distinction between embodied and extended cognition (both forms of cognition rely on physical marks, but extended cognition demands in addition that the marks are cognitive components) [cit] . it can be shown that the method's reliance on external marks was critical, whereas construing the marks as cognitive was not. consequently, the weaker and less controversial form of distributed cognition is sufficient for understanding modelling with diamond diagrams."
"the fact that modelling with diamond diagrams was very likely an instance of embodied cognition raises the prospect that other kinds of physical models also rely on embodied cognition (instead of extended cognition). of course, diamond diagrams may be the exception among physical models; here we reach the limitations of a case study. nevertheless, the present findings provide good reasons for focusing attention on embodied cognition and investigating the scope of this less controversial form of distributed cognition. 20"
"[…] cognition involves minds but extends beyond human biological capacities to encompass material artifacts and social interactions.\" [cit] appear to endorse the following claim 12 : a cognitive system engaged in scientific modelling comprises not only the things usually associated with cognition, i.e. brain processes, but it also includes (or can profitably be construed as including) the external, physical aspects of a scientific model. once the reliance of scientific modelling on external resources is apparent, it is tempting to conclude that these resources are integral parts of the cognitive processes involved in modelling. we might reason as follows. the modelling activities of scientists rely not only on their internal brain processes but also, and essentially, on physical features (external models). since the reasoning depends essentially on the physical features, these features qualify as components of cognition as much as any other essential features, such as internal brain processes."
"so far, i focused on rotating diamonds sideways. a closely related, second type of mental transformation was rotating a diamond upside down around its horizontal axis by 180°. if this operation is applied to the top diamond in fig. 3, then the 1 in its top corner moves to the bottom, and the 4 in its bottom corner ends up on top. this is the outcome crick recorded as the third diamond (fig. 3) . although i found no explicit record of the upside-down rotation in crick's notes, there is no reason why crick would have switched to explicit chemical reasoning at this point. crick wrote \"lack of direction\" next to the third diamond, referring to gamow's assumption that changing a cavity's direction within a chain was functionally irrelevant. cavities differing only in their direction were depicted by diamonds that differed in the position of both their left-right numerals and their top and bottom numerals. as we saw, an example of such a pair of diamonds was the first and fourth diamond in fig. 3, whose left-right and top-bottom numerals are reversed. another such pair is the second and third diamond. turning one diamond (of a pair) sideways and upside down generates an outcome that is identical to the second diamond of the pair. again, post-rotational identity of diamonds demonstrated that distinct diamonds represented cavities with a structural difference that had no functional impact. from these considerations it is possible to derive a simple diagrammatic procedure for determining which structurally distinct diamonds represented functionally identical cavities and which did not: all and only the diamonds that can be transformed into one another by sideways and/or upside-down rotation represented the same cavities. so, if two distinct diamonds could be transformed into one another by sideways and/or upside-down rotation, then they depicted the same cavities. on the other hand, if they could not be transformed, then they represented functionally distinct cavities. although neither crick nor gamow articulated such a rule explicitly, it guided their work with diagrams. it underpinned, for instance, crick's claim that the four diamonds in fig. 3 are all variants of h. it can also be seen in the way gamow approached a more complex task, to which i turn now. [cit] noticed that some of the known proteins had identical sequences except for one amino acid. generating this difference with dna cavities required changing the function of one cavity in the sequence while retaining all others. it was not obvious whether gamow's scheme would allow for this. after all, neighbouring cavities shared bases, and so any functional change to the bases of one cavity necessitated at least structural changes in both its neighbours, which in turn had effects on their neighbours, and so on. in order to determine whether the function fig. 4 rotating the t -and s-diamonds [cit], fig. 10) . reprinted with the permission of elsevier of a single cavity could be altered, gamow began by rotating the bottom diamond in a chain (fig. 4), noting the consequences for its upper neighbour, and then repeating this process for the remaining diamonds in the chain, gradually working upwards. rotating the t -diamond upside-down generated a second variant of the t -diamond. it also changed the left corner of its upper neighbour to a 4, and so the question arose whether that neighbour could remain an s-diamond. it could, by rotating the s-diamond sideways. this rotation changed the bottom corner of s's upper neighbour to a 3. again, the question was whether the upper neighbour could remain, in this case, a g-diamond. it could not, since rotating the g-diamond upside-down would place a 2 in its bottom corner, not a 3. so, the upper neighbour was no longer a g but an n instead, and therefore represented a cavity for a different amino acid. with the n not rotated, the side that n shared with its neighbour a remained unaltered. therefore, the whole a-diamond was not changed either. all in all, gamow's scheme did allow changing the function of only one cavity in a chain. or, as gamow put it, \"by rotating the diamonds, one can change one single amino acid … without changing the rest of the sequence.\" gamow's remark of \"rotating the diamonds\" is important here, because it shows that the authors used mental rotations to derive their conclusion."
"for instance, action hammer is likely to be confused with forward punch in as1 and action draw x is a little different from action draw circle only in the part"
"explicitness in the case of diamond diagrams, the assumptions of gamow's hypothesis were expressed physically by shaping external marks in certain ways and giving them a meaning. for instance, the assumption that neighbouring dna cavities share bases was expressed by drawing adjacent diamonds so that they shared numerals and by stipulating that numerals represent bases (and diamonds depict cavities). could this assumption have been expressed more clearly by construing the physical marks as components of a cognitive process? it is hard to see how it could have. there simply is no discernible link between the factors determining explicitness and the cognitive nature (or otherwise) of the physical marks. consider a second example. gamow's assumption that each cavity has one complementary base pair was expressed by limiting the numerals of left-right corners to four combinations and stipulating that the latter represent the four possible base pairs. once these conventions were in place, gamow's assumption was made fully explicit. there was neither a need for, nor a benefit from, construing the numerals as cognitive components. 18 reliability identifying new diamond-amino acid assignments involved a decision about whether adding a numeral to an as-yet-unassigned diamond would render that diamond identical to a diamond that was already fully labelled. this decision required, in turn, a comparison between the diamond in question and the already labelled diamonds in the chain. recording the labelled diamonds on paper generated a lasting, physical resource for such comparisons. by contrast, there is no indication that the comparisons could have been made more reliable by counting the physical records as parts of cognition. construing the records as parts of cognition, or failing to do so, has no detectable effect on the reliability of modelling with diamond diagrams."
"as evidenced from the results obtained it can be concluded that clustering and fuzzy logic together provide a simple yet powerful method, which can be applied to many other petroleum industries."
"the parameter a r strongly affects the number of clusters that will be generated. a large value of a r generally results in fewer clusters that lead to a coarse model, while, a small value of a r can produce an excessive number of rules that may result in an over defined system. in this work, particle swarm optimization (pso) is used to suggest the best a r for tsk model based on sct."
"suppose that the dimension for a searching space is d, the total number of particles is n. the position and the velocity of the it is found that usually the particles velocities build up too fast and they may converge to a suboptimal solution. shi and eberhart [cit] introduced the concept of inertia weight to the original version of pso, in order to reduce the velocity. the velocity of the particle, with the inertia term expressed as follows:"
"one of the important tasks to design a fuzzy system is how to determine the number of rules (structure identification). there are two approaches to generate initial fuzzy rules: manually and automatically. the manual approach forces designers to spend troubled time on tuning fuzzy rules. in many cases the expert's knowledge is not easily available and in some of them, this knowledge is faulty, contains uncertainty, so in this situation, the manual approach becomes more difficult to generate suitable rules. the basic idea behind of the automatically approaches is to estimate fuzzy rules through learning process from inputoutput sample data. an automatic data-driven based method for generating the initial fuzzy rules is chiu's subtractive clustering technique (sct) [cit], which is an extension of the grid-based mountain clustering method [cit] . the main idea of the sct is to obtain useful information by grouping data from a large dataset that represent a system behavior. each cluster center obtained by this technique represents a rule."
"permeability is one of the most important rock parameters in reservoir engineering that affects fluids flow in reservoir. in most reservoirs, permeability measurements are rare and permeability is determined from rock sample or well testing data. core analysis and well test data are expensive and time consuming."
"although the sct is fast, robust and accurate, the user-specified parameter a r (the radius of influence of cluster center) in this method, strongly affects the number of rules generated. a large a r generally results in fewer rules, while a small a r can produce immoderate number of clusters. determination of a r to obtain optimum number of fuzzy rule with minimum error in output of the model is a very important problem. search-based intelligent algorithms such as genetic algorithm (ga) [cit], simulated annealing [cit], ant colony optimization (aco) [cit], can be used for this determination. recently, a new evolutionary algorithm has been proposed by eberhart and kennedy [cit], which has inspired by social behavior in the nature, called particle swarm optimization (pso). pso has a simple structure and easy implementation in practice. in this work, we propose for choosing the best radius of influence of cluster center."
"where  is a positive constant greater than 1 and is called the squash factor. when the potentials of all data points have been revised by (2), the data point with the highest remaining potential is selected as the second cluster center. in general, after the kth cluster center has been obtained, the potential of each data point is revised as follows: the process of acquiring new cluster center and revising potential repeats in relation to squash factor together with the accept ratio, reject ratio and influence range. the accept ratio sets the potential, as a fraction of the potential of the first cluster center, above which another data point will be accepted as a cluster center. but reject ratio sets the potential, as a fraction of the potential of the first cluster center, below which a data point will be rejected as a cluster center."
"the under study field is located at 40 km away from south of ahwaz city. this field dimension at woc is 30 km in length and 3.5 km in width. mansuri field has two reservoirs: asmari and bangestan. more than 46 wells have been drilled in bangestan reservoir and all the wells have logging data. only six wells in this reservoir have core data, wells: 1, 4, 14, 25, 44 and 54. the core data and logging data of all wells that core analysis data were available were used for this study."
"the rest of the paper is organized as follows. in section 2, we briefly describe the subtractive clustering based tsk fuzzy modeling method. the pso method is explained in section 3."
"the particle swarm optimization algorithm inspired by the behavior of the social organisms such as flock of birds. similar to other population-based algorithms, such as genetic algorithms, the pso algorithm is initialized with a population of random solutions, called particles. these particles moves over the search space with an adaptable velocity, and record the best position it has discovered in the search space. each particle can adjust its velocity vector, based on its own flying experience and the flying experience of the other particles in the search space."
"by the end of clustering, a sufficient number of cluster centers and cluster sigma is generated. the initial number of rules and antecedent membership functions are determined by this information and then fuzzy inference system of tsk model is identified."
"in recent years, fuzzy systems, which is based on fuzzy logic [cit], have attracted the growing attention and interest in different subjects because of the following two useful properties and capabilities: capability of approximating any complex nonlinear system and model determination through the input-output data (learning process). fuzzy system is adaptive and relies on inputoutput data rather than on a classical method, so the resulting scheme is valuable, efficient and capable of reflecting changes in the reservoir permeability behavior. takagi-sugeno-kang (tsk) ( [cit] fuzzy system is a more general class of fuzzy systems which is used in this paper. in this system, the consequent part is a crisp function."
"to extract rules from data, we chose the subtractive clustering method by chiu [cit] . the subtractive clustering is one-pass algorithm for estimating the number of clusters and initial location of cluster centers, and extracts the tsk fuzzy rules through the training data. this method operates by finding the point with the highest number of neighbors as center for a cluster based on the density of surrounding data points [cit] . the subtractive clustering method is described as follows:"
"in this study, first-order tsk fuzzy approach based on subtractive clustering was used to predict permeability of the reservoir. the process of model building using subtractive clustering was carried out by making of clusters in the data space and translation of these clusters into tsk rules. the firstorder tsk fuzzy model is defined as follows: the rule base of the models built by using subtractive clustering method and particle swarm optimization with minimum modeling error is shown in table 1 . each row in the table represents a rule. the results of these experiments are in good agreement with those predicted using the proposed model as shown in figure 1 to 4. figures 1 and 2 show the comparison between proposed model output with the actual measurements at training and validation phase, respectively. figures 3 and 4 show the scatter plot of the model at training and validation phase, respectively. table 2"
", n is the number of inputs) is jth input, k y is the consequent of the kth rule, jk a and k j p is the mf and regression parameter in the kth rule, respectively. construction of the tsk model includes two steps: structure identification and parameters estimation [cit] . structure identification involves an initial rule generation, which is usually done by fuzzy clustering. parameters estimation of each cluster that includes consequent parameter estimation is usually done with a leastsquares method [cit] ."
"this paper has presented a framework for the construction of a tsk fuzzy approach based on subtractive clustering technique with particle swarm optimization to predict permeability of the reservoir. this approach has been tested with the dataset of mansuri bangestan reservoir in ahwaz, iran. in this paper, a method proposed to use particle swarm optimization to choose the value of radius of influence ( a r ). the experimental results"
"the intuition behind the non-interference property is that high-security input to the program must never affect low-security output. in other words, public outputs are not dependent on secret inputs. in the following secrecy analysis of the agents' interaction models, we consider received messages, role arguments, and sometimes constraint arguments as input and the sent messages as output. there are formulations of noninterference. [cit] ."
"performance in the presence of experimental imperfections. the magnitude images for fronsac in the presence of a timing delay in the linear gradients, though the expected slope is generated in the phase. note there is never a timing delay in the nlg gradient because it is empirically mapped and applied identically to each new scan. in addition, the off resonance artifact in fig. 3b for the fat/water shift is identical in the low-bandwidth cartesian and fronsac images; both images show a simple spatial shift with no further image distortion. this too is in contrast to many non-cartesian trajectories that improve parallel imaging, which often give dramatic and complicated artifacts in the presence of off-resonance spins."
"the security type system and a prototype of dynamic security checking application have been implemented to demonstrate that the proposed framework is feasible and can be automated. the original version of lcc which is implemented in prolog has been extended to support security type checking. the security type system is implemented in sicstus prolog and a user interface for security analysis of lcc codes is designed in visual c#.net. this tool is designed for annotation of lcc interaction models with security labels and performing the security type checking. 6 non-numeric constant 7 contravariant denotes the possibility of converting from a narrower type to a wider type, e.g. from h to l. 8 covariant means the possibility of converting from a wider type to a narrower type, e.g. from l to h."
"artifact mitigation with addition of non-optimized fronsac waveform sets. figure 2a shows experimental images where folding artifacts due to undersampling improve as a function of increasing number of nlg waveforms added to a cartesian trajectory. artifacts are mitigated from the addition of fronsac waveforms, despite the fact that these waveforms have not been optimized for frequency, phase, or amplitude and use only 10% of the available gradient amplitude. the rows compare images of a water bottle phantom reconstructed from undersampled cartesian data (no nlg encoding) versus additional fronsac encoding with 1, 2, or 3 nlg waveforms. scan times are reduced by acquiring fewer measurements to produce an image as shown in the columns with undersampling factors of 4 and 8. the applied field shapes are shown in the third column, each labelled with their common name. defining equations for these shapes are available in supplementary note s1."
"some example techniques are: proof carrying code [cit], sandboxing [cit] and code signing [cit] . however, the importance of these security issues originating from the mobility of agents should not diminish the importance of many other security threats in open mass and a subset of these will be our concern henceforth in this paper."
"in another example below, the public agent p can guess the range of secretid, by receiving a public message containing a public variable x, although the message passing part does not explicitly disclose any information. either the public agent receives publicmsg(x) or publicmsg(1), knowing the value of x, some information about secretid is leaked."
"4) exhaustive checking of every possible path in the execution tree of the lcc code is time-consuming, while dynamic checking is faster, because it concerns only one execution path of the program."
"this can be proved by induction on derivation of  ⊢ t : τ and  ⊢ t : con τ; i.e. induction on the smaller derivations that are used to derive  ⊢ t : τ and  ⊢ t : con τ, then proceeding by case analysis on the typing rule that was applied last in the proof of  ⊢ t."
"common security techniques such as conventional access control, encryption, digital signatures, virus signature detection and information filtering are necessary but they do not address the fundamental problem of tracking information flow in information systems, therefore, they cannot prevent all information leaks. access control mechanisms only prevent illegal access to information resources and cannot be a substitute for information flow control [cit] . encryption-based techniques guarantee the origin and integrity of information, but not its behaviour."
"this paper is laid out as follows. first, different types of insecure information flows in open mas governed by interaction models are introduced. second, a security type system is proposed by defining security types and the type inference rules. then, the security type system is evaluated by proving some of its properties. next, the dynamic and the static approaches in the interaction type checking are reviewed and non-interference and declassification are discussed."
adaptation of the proposed security type system for similar first-class agent protocol languages such as map 11 and rasa is straightforward. similar idea can be applied on other agent languages with further edition.
"this breach of security can occur in an lcc clause, when a public agent p sends the secretmessage to any (public or secret) receiver agent r: a(publicagent, p)::"
"in which  is empty (with no binding) or an updated environment that contains a mapping of the term t to the type . if there exists a  that  ⊢ t : , then t is called a well-typed lcc expression under the security context of ."
"applicability on different hardware configurations. finally, to illustrate the flexibility of this approach, data is presented in fig. 7 from an entirely different set of hardware sited at university of freiburg. this laboratory has built an 84-channel matrix gradient coil which is capable of generating a number of nearly arbitrary gradient shapes 32, 33 . for this study, the 84 channels were run in a combination that approximates a c3 shape, and amplitude was then modulated sinusoidally in time. due to the limited amplitude and single field shape used in that experiment, artifact reduction is somewhat lower than that shown in preceding figures. however, the reduction in undersampling artifacts compared to the cartesian image is unmistakable, and more importantly it is similar to comparable c3 fronsac1 images acquired at yale. this demonstrates, though improvements may be reduced by weaker field amplitude and fewer nlg channels, field purity does not significantly affect the fronsac experiment."
"insecure implicit flows disclose some information through the program control flow. [cit], we can define an implicit information flow from term t1 to term t2, when a performed operation causes a flow from some arbitrary t3 to t2, based on the value of t1. thus, conditional lcc expressions are the sources of insecure flows."
"this definition of non-interference is termination-insensitive, by which we mean that it disregards information leaks due to the termination of the program (e.g. the last example in section 4.2). thus, our type system cannot detect this type of insecure flow."
"information leakage denotes disclosure of secret information to unauthorised parties via insecure information flows. information leaks in agent interactions occur when secret data are revealed through message transfers, constraints or assigning roles to agents."
"if  ⊢ l : , i.e. l is a well-typed lcc expression, then either l is a final step or else there exists some l' that l⇝l'."
"when a role is assigned to an agent in the definition of an lcc clause, the security level of the role and the agent identifier need to be compatible. the following role definition is not a permissible flow, because it assigns a secret role secretagent to a low security agent publicagent. a(secretagent, publicagent)::"
"the algorithm for simple type checking (typechk) is defined in fig. 7-3 . in fig. 7-4, an updated typechk algorithm is defined, in which based on the result of the type checking and the reaction policy π, true or false is returned."
"in a similar example, the following lcc clause binds r to the precise value of secretid if the role completes successfully. so, it discloses the value of secretid to the public agent p by sending publicmsg(r) message. in this example, even if r is not sent as a message parameter (i.e. publicmsg instead of publicmsg(r)), the public agent p can discover the value of secretid by counting the number of received messages."
"an lcc expression is in its final step when either it can be marked as a closed expression by an lcc rewrite rule or it is a constraint that is evaluated by a satisfy or satisfied rule. this is an example of a compound constraint expansion: assume l is null ←c and the compound constraint c is c1 ∧ c2, when c is unfolded into c1 ∧ c2 then l' is null ← c1 ∧ c2 and we can write l ⇝l'."
"compatibility with other contrast and acceleration strategies. compatibility with advanced reconstruction strategies. recently, tremendous improvements in undersampled images have been found with advanced reconstruction methods such as compressed sensing, which promotes sparsity in an appropriate transformation domain 25, 26 . it has previously been shown that nlg encoding, and fronsac in particular, can result in data that is better conditioned for reconstruction by compressed sensing 30, 31 . figure 6 shows that images reconstructed from accelerated fronsac2 data benefit greatly from a compressed sensing approach, while virtually no benefit is seen for images reconstructed with evenly undersampled cartesian data without nlg encoding. these reconstructions further suggest that significant improvements can be made even with lower amplitude fronsac encoding with judicious choice of reconstruction algorithm. details of the image reconstruction strategy is available in supplementary note s2."
"an electronic institution [cit] or an interaction model is an organisation model for mass that provides a framework to describe, specify and deploy agent interaction environments [cit] ."
"subtyping rules ( fig. 5-2 ). each rule is read from bottom left and is applied recursively, e.g. rule agnt states that in order to assign a role to an agent in form of a(r, id) that has security type of agent τ, we must first check whether the security type of the role r is τ and then whether the security type of the agent identifier is utrm τ. it guarantees that a high level role will not be assigned to a low agent. the environment γ is a confidentiality policy, which is an input of our secure interpreter ( fig. 7 -1-a). security labels are assigned to lcc terms as annotation of lcc interaction models ( fig. 5-3 )."
"where  ranges over elements of security levels, agent identifiers have only type \"utrm \", agents have only type \"agent \", constraint expressions have only type \"con \", operational commands 5 have only type \"op \" and messages, constraint arguments have type \"utrm \" or . role names and other terms (variables, constants and structures) have only type ."
"in this paper, we have addressed information leakage problems in open mas governed by interaction models and, consequently, developed secrecy analysis frameworks for an agent language called lcc. explicit and implicit insecure information flows have been explained using a number of lcc examples."
"having defined a type system for a class of security properties, our purpose in this section is to prove key security properties of the system. other work (s. [cit] ) gives empirical examples of the consequences of these properties in specific interactions but, to save space, we focus here on generic properties across all appropriate lcc interactions."
"where cn is an expansion of the original lcc clause ci in terms of the interaction model p and in response to the set of received messages mi, on is an output message set, mn is a remaining unprocessed set of messages."
"type soundness (or type safety) is the most basic feature of a type system [cit] . two properties that show the type soundness in a type system are progress and preservation. in our security type system, preservation means that expansion of a well-typed term by the lcc rewrite rules is a well-typed term (clause expansion preserves well-typedness). progress guarantees that a well-typed lcc expression does not get stuck in the execution of lcc clauses, assuming that agents can evaluate (satisfy/dissatisfy) the constraints and the necessary input/output messages are generated."
an example of an explicit flow that discloses the value of a secret variable to a publicly observed variable is assigning secretid to a publicvariable in an lcc constraint:
"b. ⊢ t : means that an identifier, a role name, or a message t (with every identifier inside it) has a security level lower than or equal to in context . d. ⊢ c : con means that the constraint name and every argument within c has a security level or lower in context ."
"the main problems in mobile agent systems, which are not in the scope of our review, are threats from agents to hosts and vice versa."
"in our security analysis lightweight coordination calculus (lcc) is used to implement agents' interaction models and formulate attacks. lcc [cit], is a declarative language used to specify and execute social norms in a peer to peer style. lcc is a compact executable specification based on logic programming."
nyquist sampling rate determined the number of lines of data acquired to avoid image aliasing. each line of data was oversampled. note that oversampling each line does not increase the scan time.
"based on the reaction policy, type checking could result in termination of the execution or breach detection and continuation of the clause expansion ((a) (b) fig. 7-1) . in order to integrate dynamic information flow analysis into the lcc interpreter and to detect or prevent information leakage, the lcc clause expansion mechanism [cit] ) (explained in section 2) has been upgraded by amending the lcc rewrite rules."
adversaries can exploit explicit or implicit information flows to perform attacks. we need to prevent both explicit and implicit insecure information flows in order to ensure no information leaks to unauthorised parties.
"although the notion of non-interference is a popular and natural way of describing confidentiality and integrity, it may be too restrictive for many applications [cit] . the next section addresses this issue."
"two approaches to address information flow problems in mass governed by interaction models are conceptual modelling by analysing the abstract models of the code and language-based information flow analysis. in the first approach, an lcc interaction model is translated into an abstract model, in which information leakage is investigated using an existing reasoning tool [cit] . in language-based analysis of the agents' code, we employ security types for the lcc terms and enforce a security policy by type checking."
"the static approach is a promising method that prevents insecure implicit and explicit flows, but it suffers mainly from non-permissiveness, so it may also reject genuine flows. another drawback of the static analysis problem is that due to the dynamic behaviour of open mas, there is a lack of information about the security classification of agents, constraints, etc. before run-time."
"like other non-cartesian trajectories with a dynamic readout gradient, the fronsac waveform does require empirical mapping. however, unlike most non-cartesian trajectories, once this single mapping is performed, the same waveform can be used for a huge range of scan prescriptions, as shown in fig. 4, and still yield significant image improvement. some image quality improvement is expected when extending this work to any imaging orientation, and has been shown for oblique orientations, but the extent may vary."
"the rule seq say that if two lcc expressions have the same security level, their composition has also that security level. the choice rule functions in the same way, only it also considers the security level of the constraint of the first part a1 to prevent implicit information flow from the constraint in a2. the rule role checks whether the role definition a(r,id) agrees with the body of the lcc clause. the remaining rules of the security type system are subtyping rules in fig. 5-2 . the subtyping rules agentrule, utrmrule and oprule are contravariant 7 and the conrule is covariant 8 ."
"we have focused on one aspect of security, i.e. confidentiality. the other important aspect of security is integrity. we would suggest defining other security properties for security typing that guarantee integrity through analysis of agents' interactions in this regard. we also leave automatic security annotation of agent interaction models (with secrecy labels) as another topic for future research. table 0 -3 summarise the acceptable and unacceptable explicit and implicit information flows in message passing, role assignment and conditional statements in lcc codes. it is assumed that a secret lcc term and a public lcc term are shown by h and l, respectively."
"2) lack of information in static checking; we may not know the security level of all peers and components of the program, especially in an open mas we may not know who will join the system during the interactions. in practice, security policies cannot be determined at the time of program analysis and may vary dynamically."
"in cartesian brain images reconstructed with conjugate gradients (cg) or grappa, ghosting in the anteriorposterior direction, a remnant of the undersampling in that direction, is greatly reduced by the additional nonlinear encoding (fig. 2b,c) . since fig. 2b shows all cartesian images with cg reconstruction, there may be concerns that a grappa reconstruction would reduce artifacts without the addition of nonlinear gradients. to address this, fig. 2c shows that grappa does not improve artifacts in this data, and the cg reconstruction does not give fronsac an unfair advantage over cartesian acquisitions."
"several non-cartesian trajectories that deserve special mention are bunched phase encoding, zigzag sampling and wave-caipi techniques, which bear some resemblance to the fronsac approach [cit] . these techniques use standard linear gradients, typically with far fewer oscillations per readout, and thus are a modified k-space trajectories which inherently collect points in k-space. in contrast, fronsac uses nlg fields, which induce spatially-varying encoding and cause the k-space sampling distribution to change during the acquisition. additionally, the above mentioned techniques are often limited by peripheral nerve stimulation from the rapidly oscillating linear gradients, whereas the nlgs used in fronsac switch multipolar fields, which mitigate peripheral nerve stimulation 8 . and like other non-cartesian trajectories, the gradient waveforms used in these techniques must be adapted to scan geometry, introducing potential challenges in field mapping."
"to minimise this kind of information leakage and to have more flexible secrecy policies, new options forming the type checking strategy can be defined in π. in table 7 -1, the following three reaction policy modes and their priorities are shown: prevention (prevmode), detection (detectmode) and no-detection (nochkmode)."
"in which label is a keyword, term is any lcc term and level is the security levels high (h) or low (l). the security types are then assigned based on the term definitions. all security types can be inferred from the term structure automatically, except constraints' arguments, which need to be defined explicitly (by the user). by default, a constraint's arguments are assumed to be non-updatable and to have a security type, τ, assigned to them."
"the particular tradeoffs and bandwidth limitations are highly hardware dependent. for example, experiments with the freiburg 84 channel matrix coil, which was not optimized for fronsac encoding, used 70% of the maximum available gradient strength and could not be safely increased further for higher frequency acquisitions due to heating, whereas those on the yale spherical harmonic coil used only 10% and showed no detectable heating. the slew rate limits of the yale hardware, which was not designed for fronsac encoding, necessitate lower bandwidths for fronsac imaging, but even the current parameters yield significant improvements. as first generation custom hardware, it is difficult to specify what parameters can be expected in a clinical implementation. however, the presented results do demonstrate that widely achievable fronsac gradients, which do not violate dg(t)/dt limits of hardware or db(t)/dt limits of peripheral nerve stimulation, can dramatically improve image quality without particular optimization, demonstrating the versatility of the method."
"informally, if two clauses look the same to an adversary, they also behave the same. in other words, low output (the sent messages to an adversary) depends on low inputs (the immutable visible parts of the contexts)."
"many non-cartesian trajectories can also improve parallel imaging without nlgs or additional hardware, yet the presented work is highly significant for several reasons. notably, though high performance non-cartesian trajectories have been well understood for many decades (spiral, rosette, radial), they are still in limited use for clinical applications [cit] . one reason is that these methods can be highly sensitive to errors in the gradient trajectory, off-resonance effects, or subject motion, and another is that they can yield complicated contrast 34, 37 . cartesian fronsac, by contrast, is a small modification to the workhorse cartesian sequence, so it shares many of the desirable characteristics of cartesian imaging."
"confidentiality is one of the main features of a secure system that is challenging to be assured in open mas. open mass are convenient platforms to share knowledge and information, however usually there exists some sensitive information that we want to protect. the openness of these systems increases the potential for unintentional leaking of sensitive information. thus, it is crucial to have mechanisms that guarantee confidentiality and to assure that the publicly accessible information during the interactions is what we deliberately want to share."
"information may leak because of the termination behaviour of the interaction model 4 . recursion is the key to this type of leaks. in the following sample lcc clause, the adversary learns that the value of the secretid is 0 if the interaction model terminates."
"it is a formalism which defines agents' interaction rules and their permitted and prohibited actions. while interaction models can be used to implement security requirements of a multi-agent system, they also can be turned against agents to breach their security in a variety of ways, as we will show in this paper."
"in the next section, we show how the security type system can be used in the agent interaction to verify whether an interaction model is secure."
this can be proved by case analysis on the rule that was applied last in the proof of  ⊢ t :  and by induction on the type rules that are used to derive  ⊢ t : .
the type rules are judgments of the form:  ⊢ t : where  is a type environment that maps term t to type .
"declassification is intentional release of secret information by lowering security levels of information [cit] . sometimes, we need a way of information declassification in our security system."
"while fronsac gradients are extremely beneficial even when they are not optimally tailored to the scan prescription (fig. 4) with regard to amplitude and frequency 22, number of waveforms (fig. 2), or field shape (fig. 7), the optimal fronsac gradient for a given scan prescription can be qualitatively described by considering the distributions in k-space. the area of the distribution is proportional to the cumulative area under a fronsac waveform or its moment and should be large enough to spread the sampling distribution into the gaps of k-space created by undersampling. therefore, higher amplitude gradients are better for larger undersampling factors 33 . meanwhile the number of overlapping orientations of each sampling distribution, related to the frequency of the out of phase fronsac waveforms, should be comparable to the number of unknowns being sampled by each distribution, making higher frequencies more favorable. in experiments, the fronsac waveform amplitude and oscillation frequency must be kept within hardware and physiological limitations. as frequency increases, amplitude and slew rate would need to increase quadratically to maintain a given nlg moment. with very large nlg moments, the resolution needs to be adjusted to account for intravoxel dephasing. while the choice of nlg fields for this work was dictated by existing nlg hardware, it is possible to systematically optimize the nlg field shapes for specific applications and receive coil arrays 33 ."
"the main disadvantage of purely run-time information flow analysis similar to the one discussed above, is that they can produce false negative results as they cannot detect implicit information flows. this is because in dynamic security analysis of lcc, not all execution paths of the program are checked. the following simple example show when the dynamic analysis can go wrong. all terms are low security and the only terms with high security levels are secret and this (i.e. the current clause environment)."
"3) the proposed type system which is based on denning's work ignores leaks via the termination behaviour of programs. therefore they satisfy only termination-insensitive non-interference, which is defined in the next section."
"if t is a well-typed agent definition or lcc operation; i.e.  ⊢t: agent τ or  ⊢t: op τ, then any agent identifier in the agent definition, any receiver of a message, or any updated term in an operation, has a security level equal or higher than τ."
"in general, any lcc code containing a low security expression within a high security constraint, which does not hold at run-time is rejected by static type checking, even though it is permissible. this is due to the fact that the security checker is not guaranteed to know whether or not a constraint holds at the time the interaction model is checked, so it conservatively rejects the interaction model."
"the following explicit flow, in which the instance of a variable secretmessage is sent to a low level agent p with the risk of secret information leakage:"
"a final important strength of cartesian-fronsac, compared to many non-cartesian trajectories, is that it also preserves simplified contrast behavior, for example in multiecho acquisitions 39 . cartesian-fronsac preserves the blockwise sampling of central k-space in sequences such as echo planar imaging or turbo spin echo, which can further compound image acceleration. our results in fig. 5 show that controlling contrast in fronsac tse is straightforward, and images are indistinguishable from the cartesian case, except for better undersampling behavior."
"another possible problem is late detection of the insecure flow in run-time security checking of lcc interaction models. this may result in the rewriting of some illegal lcc expressions, thus changing the state of the agent before finding the breach -for example,detection of the breach after a high security message is sent to a low security agent is too late."
"some role names, constraints, variables and the security level of the terms may not be available to our static analysis. the lcc programmer or the expert who annotates the code by security levels may not know about the behaviour of some constraints and other variables, which will be available at run-time. e.g. in the cloud configuration case study, some general patterns are used and some constraints and roles' arguments are defined at execution time by the counterpart agent."
"'no write down' means having an operational command with the security level of op τ (any operational command), any updatable identifier within it has a security level higher than or equal to τ. by updatable identifier, we mean an agent when a role is assigned to it or a message is sent to it. we also mean an argument in a constraint whose value is updated. e.g. this denotes that it is not possible to assign (send) a higher role (higher message) to a lower agent."
the human investigation committee granted institutional review board approval to image healthy human volunteers. after obtaining informed consent the brains of two volunteers were imaged. subjects reported no discomfort during the scans. the study was in accordance with the declaration of helsinki.
"performance at various scan prescriptions using a single non-optimized fronsac waveform set. figure 4 shows that once a fronsac nlg waveform has been well characterized by spatial and temporal mapping, it can be applied to a variety of desired cartesian scan prescriptions and still produce profound improvements in undersampling artifacts. the columns of fig. 4 show that adding the same fronsac waveform at different imaging slice orientations, or different sized fields of view (fov), or different resolution improves undersampling artifacts in all cases. at higher resolution, there is effectively less relative fronsac encoding (fewer modulations of the encoding distribution per distance in k-space), so the improvement is somewhat reduced, but still easily observed. importantly, the fronsac gradient never degrades image quality. similarly, switching the readout line direction and thereby the undersampling direction changes the orientation of the undersampling artifacts, but does not diminish the effectiveness of the fronsac waveform."
"in other words, when a message (or a constraint) has a security level τ, it assures us that it will not reveal any information with security level more than τ."
"fronsac is a very different approach to matching receiver array and gradient shape for better undersampled imaging, based on complementarity in k-space rather than the spatial domain, where fronsac minimally changes the real encoded resolution of each voxel (fig. s2 ) 22 . as previously mentioned, nlg encodings create a sampling distribution in k-space that can be varied dynamically as the linear gradients sweep the sampling distribution across k-space (supplementary videos s4 and s5) 23 . nlgs in conjunction with receiver arrays generate sets of dynamic sampling distributions that are simultaneously sampled at each timepoint. this additional degree of freedom can be used to design trajectories that more efficiently measure the gaps created by the \"skipped\" parts of k-space, which are sampled only by the wings of these distributions. in all cases, the linear gradients of a traditional trajectory translate these distributions across k-space until the entire space is sufficiently sampled."
"security a mas could be defined as a subcategory of a software system, a high level application on top of the osi 1 networking model; therefore the security of mass is not a completely different and new concept; it is a subcategory of computing security. however, some traditional security mechanisms resist use in mas directly, because of the social nature of mass and the consequent special security needs [cit] . open mass are 1 open systems interconnection particularly difficult to protect, because we can provide only minimum guarantees about the identity and behaviour of agents."
"the static checking explores all execution paths in lcc interaction models, hence it guarantees that detection of any insecure flow based on the defined type system. to perform a static type check, we can modify the lcc rewrite rules for the static type check, in a way that the whole expansion tree of an lcc clause is explored. in recursions, the clause is expanded if it has not already been expanded (fig. 7-5) ."
"this states that for any two contexts 1 and 2 which are alike up to level of l, a successful expansion of the lcc clause ci in one of the contexts with behaviour on1 and a successful expansion in the other context with behaviour on1 guarantee that the behaviours are observationally equivalent."
"a typical example is any system that asks the user credentials for authentication. consider the access request to a patient record by a specialist. rejection of an incorrect password violates non-interference, because of the dependency between high input (i.e. password) and low output (i.e. rejection message). that implies the system leaks partial information about the password (i.e. incorrectness of the password) to a potential attacker."
"we employed a language-based information flow analysis approach in the context of open mass. in static information flow analysis, agent interaction models are validated before being run. static analysis of programmes using security type systems conservatively detects implicit and explicit information flows and provides stronger security assurance [cit] . dynamic security checks may be accomplished via two similar approaches: monitors or dynamic security typing [cit] )."
"adding fronsac encoding to linear undersampled trajectories improves imaging because the nlgs spread the sampling distribution in k-space, providing more information about the nominally \"skipped\" regions in k-space (fig. s1 ). more precisely, the product of each nlg encoding with the receiver array encoding generates as many sampling distributions as coils 23, and these independent distributions sample the gaps in the nominal trajectory (fig. 1c,d ). since each stamp no longer unambiguously measures a point in k-space, it is necessary to measure these distributions for many overlapping positions and orientations, so the underlying k-space can be accurately deduced. this is the rationale for the fronsac waveform. the orientation of the distribution is modulated by the out of phase, rapidly oscillating timecourse of various nlg waveforms, while changes in position accumulate from evolution under the linear gradient. by extracting more spatial information from a smaller number of k-space lines, fronsac reduces the required scan time while still resulting in good image quality."
"in this approach, static analysis is performed on an interaction model and if it is rejected, the system informs the user. the user then can decide to continue with the interaction model and perform dynamic checking at run-time. there is also a hybrid approach, in which static and dynamic analysis are merged to take the best of both worlds. this is especially useful in flow sensitive analysis. in flow sensitivity, variables may store values of different sensitivity (low and high) over the course of the interaction. we leave flow-sensitivity analysis in lcc interaction models as a topic for future research."
"role definitions in lcc can be recursive and the language supports structured terms in addition to variables and constants so that, although its syntax is simple, it can represent sophisticated interactions. notice also that role definitions are \"stand alone\" in the sense that each role definition specifies all the information needed to complete that role. this means that definitions for roles can be distributed across a network of computers and (assuming the lcc definition is well engineered) will synchronise through message passing while otherwise operating independently. [cit] defined the following clause expansion mechanism for agents to unpack any lcc interaction model they receive and suggested applying rewrite rules to expand the interaction state:"
we propose a language-based information flow analysis technique for the lcc language to prevent information leaks problem by introducing a novel security type system. [cit] .
"hardware. all imaging experiments were performed on a 3t mri scanner (magnetom trio tim, siemens healthcare, erlangen, germany). parallel data acquisition was performed using an integrated 8 channel rf head coil (siemens). the majority of the phantom and in vivo experiments were performed at the yale magnetic resonance research center using a nlg insert (tesla engineering ltd, storrington, uk) rated at 321 a with an inner diameter of 380 mm which generates 3 spherical harmonic gradient fields: x 3 − 3xy 2, 3yx 2 − y 3 and x 2 + y 2 (commonly known as c3, s3, and z2) (fig. 1b) . the gradient coil is capable of achieving maximum c3, s3, and z2 fields of 3254.8 mt/m 3, 3155.4 mt/m 3 and 475.08 mt/m 2 . phantom experiments were also performed at the university medical center freiburg using an 84 channel matrix gradient coil driven by 12 gradient amplifiers rated at 150 amps with an inner diameter of 350 mm which allows flexible gradient field shapes generation 33 . a cluster of elements capable of achieving nlg fields approximating the c3 spherical harmonic field was used. the approximate maximum c3 strength of the matrix gradient is 452 mt/m 3 . the cluster was set up for general scanning and not optimized for generating the c3 shape."
"any constraint that updates the value of a public term using a secret term causes an unacceptable information flow. the constraints in lcc play an important role, although the implementation details of constraint solvers are invisible to lcc clauses and the constraint solver might even be a remote web service. however, it is the responsibility of the lcc programmer to prevent any illegal information flow caused by invoking a constraint."
"resilience to errors in gradient trajectory as demonstrated in fig. 3 is a particularly important feature of fronsac because it is directly linked with the ability to change image geometry on the fly, a vital requirement for clinical imaging. in other non-cartesian trajectories, the gradient waveforms must change for scans of different resolution, fov, or orientation, but without advanced equipment, it is infeasible to empirically field map the real output of each waveform 33, 38 . therefore, there is often a mismatch between the prescribed and executed waveforms, which can lead to serious degradations in image quality. in contrast, if such mismatches arise in cartesian or cartesian-fronsac imaging, the resulting artifacts are mild or even undetectable. similarly, off-resonance effects and susceptibility gradients can have complicated and profound impacts on non-cartesian images, but they result in simple localized artifacts in both cartesian and cartesian-fronsac imaging."
"generally, dynamic checking (in the best case), may assure that the current execution of an interaction model does not leak information, but does not tell us that the code is safe and will never reveal any confidential information in future, because it does not check all possible execution paths of the lcc program. in other words, if no breach occurs in dynamic checking, it means that there exists a secure execution path in the lcc interaction model. this is a liveness property, which specifies that eventually \"good things\" do happen versus a"
"to address the false alarm of static approach, static analysis is performed on an interaction model and if it is rejected, the system informs the user. the user then can decide to continue with the interaction model and perform dynamic checking at run-time. the proposed security type system supports non-interference. the intuition behind the non-interference property is that high-security input to the program must never affect lowsecurity output. this definition of non-interference is termination-insensitive, by which we mean that it disregards information leaks due to the termination of the program. as non-interference may be too restrictive for many applications, the proposed framework supports declassification."
"let us assume the constraint does not hold; i.e. satisfied (check(secret)) return false, so the first part of the conditional statement fails and the second part is processed by this rewrite rule:"
non-interference is a popular information flow property that guarantees secrecy of information flow and tells us whether there is any information leakage in the information system. [cit] .
"if t is a well-typed lcc constraint, message or identifier with security type τ; i.e.  ⊢t: τ or  ⊢t: con τ, then t contains only identifiers with security level not higher than τ."
"on the other hand, a message passing pattern can occur without a security breach. the following explicit flows that sends (receives) a publicmessage variable to (from) a secretagent s is permissible."
"in summary, the presented work not only proves the experimental feasibility of cartesian-fronsac, but also hypothesizes and verifies features that establish the experimental practicality of cartesian-fronsac. the method is robust to the most inescapable hardware errors, such as small timing errors or off-resonance spins. it requires minimal field mapping to improve a wide range of scan prescriptions, whether using a different contrast or a different scan geometry. the nonlinear fields themselves do not require a high degree of purity and can be realized from different hardware configurations. finally, the method is fully compatible with other acceleration strategies, such as multi-echo acquisition or compressed sensing reconstruction. this suggests that cartesian-fronsac is both an effective and highly practical approach to improving scan acceleration in a broad range of clinical applications."
"in each case, the fronsac encoding reduces the residual ghosting artifact due to undersampling, particularly in the presence of 2 or more waveforms, which provide more degrees of freedom to manipulate the sampling distribution. the 3 waveform case, chosen because it provides nlg encoding in all 3 dimensions, was also demonstrated in vivo (fig. 2b), with no reported peripheral nerve stimulation."
"the extended lcc rewrite rules augmented with dynamic type checking are shown in fig. 7-2 . the updated rewrite rules in fig. 7-2 as a result of rewrite rules in fig. 7-2, the clause of the interaction model appropriate to the given role is expanded. the first rule starts unpacking a clause by expanding its body (b) and the rules (2) to (12) expand different parts of the clause body. the closed rules in (13) to (18), determine whether an interaction rule has been completed through earlier interpretation (in which case we say that it is closed)."
a. ⊢ x: utrm means that an updated agent identifier in a role assignment or message passing operation or an updated argument in a constraint has a security level higher than or equal to in context .
e. ⊢d: op means that every receiver of a message or any updated identifier in an operational command (i.e. def) has a security level τ or higher in context .
"the security typing rules id and uid explain if an lcc identifier (a constant or a variable) is defined in the environment γ, security types τ or utrm τ may be assigned to it. selection of τ or utrm τ is based on the structure of the lcc expression. the security label of the current clause (this) is important while message passing and calling a constraint. this is created and added to the security environment γ by the init rule. ⊢ 1 :, ⊢ 2 : ⊢ 1 ∧ 2 :"
"an example annotation of an lcc interaction model that assigns security levels to lcc terms is shown in label(buy, l). label(afford, l). label(sold, l). label(p, l)."
"we have proposed and implemented a language-based information flow framework to analyse information leaks in lcc interaction models. the security-typed lcc has been introduced by inventing a security type system, which formally defines security levels, security types and the type inference rules. next, the proposed type system has been evaluated and proven to hold basic, important properties: type soundness, simple security and confinement."
"the interpreter tries to find a matching rewrite rule for each lcc expression, if no match is found, it means that there is a syntax error in the lcc code. if a match is found but the conditions of the rewrite rule are not fulfilled, it returns false and continues to search for another rewrite rule that matches the expression. it is the agents' responsibility to satisfy the constraints in the clause and it is assumed that agents have a mechanism to fulfil the constraints. satisfied(c) is true if c can be satisfied from the current knowledge state k of the agent and satisfy(c) is true when k can be made to fulfil the constraint c. clause (p, x) is true if clause x exists in the interaction model p."
"importantly, however, wave-caipi and other non-cartesian trajectories should not be regarded as competitive with fronsac encoding but rather synergistic. fronsac preserves the features of the underlying trajectory while allowing larger gaps in k-space. as the challenges of these non-cartesian trajectories are overcome, each method can be still further accelerated by allowing larger gaps in k-space and using fronsac to better encode these gaps. previously published simulations show that fronsac encoding can improve nearly any trajectory, whether cartesian or not 33 . finally, fronsac encoding has some advantages in reconstruction that are likely to enhance clinical applicability. because of the highly parallelizable nature of the reconstruction, scan quality can be verified in ~15 seconds with two conjugate gradient iterations using just one gpu, and it is reasonable to predict that a near instantaneous image could be generated from an inverse transformation found via machine learning 43 . furthermore, the diffuse fronsac encoding in k-space creates unstructured undersampling artifacts, which are suitable for suppression with compressed sensing (fig. 6) . compressed sensing improves other trajectories, including many non-cartesian or randomly sampled acquisitions, but the addition of fronsac gradients improves compressed sensing reconstructions without requiring alteration of the underlying linear trajectory."
"insecure explicit information flow denotes direct sending or assigning of secrets. explicit flows in lcc interaction models may occur in three situations: (a) message passing, (b) invoking a constraint and (c) assigning a role to an agent. in explicit information flows, the operations are performed independently of the value of their terms [cit], e.g. the content of an lcc message does not affect the sending operation. insecure explicit flow may cause secret information to be leaked to a publicly observable term. consider the following lcc codes as examples of explicit information flows:"
"the call rule states that when we call a constraint, the security level of its functor 6, the security level of the current clause (this) and the security level of either read-only arguments (ti: τ ) or write-only arguments (ti: utrm τ ) have to be the same. this ensures us that a public agent can not access secret constraints and a public constraint may not reveal secret information to a public agent. the struct rule denotes that in structured nonupdatable terms (such as messages, role names and read-only arguments) the security types and levels of the functor f and the arguments ti must be the same. the rules and, or and not regulate the composition of constraints in lcc. the rule if1 states that the security type of constraint c and the message sending operation (m  a) needs to be matched so that the conditional expression is allowed. security typing of other conditional expressions (if2 to if4) is performed in a similar way to if1."
"one metric to evaluate the encoding efficiency of a nlg trajectory is the width of the psf in k-space at each location when reconstructed from the acquired sampling distributions in k-space. numerical optimization with this metric constrained by realistic hardware limitations yielded a sequence with highly dynamic nlg encoding similar to fronsac 24 . figure 1d compares this metric evaluated for each point of k-space (the width of a sampling distribution reconstructed at each location in k-space) provided by an undersampled linear acquisition versus one enhanced by fronsac encoding. the addition of fronsac nlg encoding can improve image quality whether the nominal trajectory through k-space is rectilinear cartesian or some arbitrary non-cartesian path. it is a general approach to improve sampling of the gaps in any traditional linear trajectory through k-space. however, in this work we focus on improving undersampled cartesian encoding, since it is nearly ubiquitous in clinical imaging due to its insensitivity to various experimental imperfections."
"the right-hand side of a conditional statement is a constraint. constraints provide the interface between the interaction model and the internal state of the agent. these would typically be implemented as a java component which may be private to the peer, or a shared component registered with a discovery service."
"the proposed security type system supports non-interference; suppose ci is a message sending operation ⇒ . if the type of the agent a is agent h, the typing rule snd allows sending a message (with any security level) to the high security agent a, in either case, an adversary of level l cannot observe any output message. if the type of a is agent l, then the type system requires that m : l, then any the observable output of the lcc rewrite rule for an adversary of level l will be message m. the other cases of ci that can have an observable behaviour are similar."
"we have discussed two approaches for applying the security type system on the agent interaction models; dynamic (run-time) and static type checking. two disadvantages of dynamic information flow analysis are its inability to detect implicit information flows and late detection of insecure flow. all execution paths of the program are not checked in dynamic analysis and some paths are disregarded, which could lead to implicit information flows. to overcome this problem, we provide the following options: a) extending the dynamic approach with the control flow stack mechanism described in (s. [cit], 103) b) using the static approach instead of dynamic analysis:"
regression models were initially evaluated using jackknife validation procedure and the best performing models were selected based on the pearson's correlation coefficient and root mean square error (rmse;
"collation of published pharmacokinetic data. published work describing the intravenous pharmacokinetics of dendrimers were identified by undertaking pubmed searches of the terms 'dendrimer and pharmacokinetics' or 'dendrimer and biodistribution' . only papers that described -or provided enough information to extrapolate -composition of the dendrimer scaffold and surface, molecular weight, terminal plasma half-life (t 1/2 ) plus % dose excreted in urine and/or % dose recovered in liver at termination were included in the database. papers describing the pharmacokinetics of loaded drug rather than the dendrimer scaffold were excluded, since following the loaded drug is a poor predictor of the biopharmaceutical behavior of the dendrimer itself. supramolecular structures (such as micelles) that were comprised of dendritic polymers and hyperbranched polymers that were based on dendritic cores were also excluded. dendrimers conjugated with distinct targeting moieties (such as tumor targeting ligands) were also not included since this is expected to change the pharmacokinetics of the base construct."
"usability of the spl-adr-200db files for evaluating adr extraction system was validated using metamap 11 to extract adrs, and evaluating the results under several conditions known to maximise recall or precision. we obtained the expected variation in these metrics: precision of 69.90% with 59.25% recall, when precision was maximised, and 62.36% precision with maximum recall of 71.65%."
"in general, two steps are needed to complete a sensing design: (1) to design the test statistics; and (2) to derive the probability density function (pdf) of the test statistics. for spectrum sensing with multiple antennas, the test statistics can be designed based on the standard principles such as generalized likelihood ratio testing (glrt) [cit], or other considerations [cit] . surprisingly these studies all give the test statistics using the eigenvalues of the sample covariance matrix. it is thus important to derive the pdf of the eigenvalues so that the sensing performance can be quantified."
"users have the option to browse the database via the web interface ( fig. s5, supporting information), search/ filter specific information as well as show/hide construct properties, surface group properties and phamacokinetics details."
"in this paper, theoretic distributions of the test statistics for some eigenvalue based detections have been derived for any fixed k but large n, from which the probability of detection and probability of false alarm have been obtained. the results are useful in practice to determine the detection thresholds and predict the performances of the sensing algorithms. extensive simulations have shown that theoretic results have higher accuracy than existing stuties."
"we have implemented a user-friendly web server that will enable researchers to search, predict and visualize the pharmacokinetic properties for their molecules of interest (http://biosig.unimelb.edu.au/dendpoint). in addition, we have implemented a comparison feature that enables users to rapidly compare the pharmacokinetic profiles of two molecules, allowing systematic evaluation of pharmacokinetic profiles as various physicochemical properties of the dendrimer are modified. considering the sensitive nature of many projects, the web server does not retain any information submitted to it. this will hopefully facilitate the development and optimization of dendrimers for specific biological roles, and provide a foundation for the evaluation of nanoparticles more broadly. www.nature.com/scientificreports www.nature.com/scientificreports/"
"weka toolkit (version 3.8.2) 38 were used to train models for the different pharmacokinetic properties collected and store on the database. the best performing models, based on the evaluation metrics below, were obtained using random forest 39 (default settings, 100 trees) for both classification and regression tasks."
"a standardised, validated searchable dataset containing information about labelled adverse reactions, i.e., adverse reactions present in structured product labels (spls) for fda approved drugs is needed to support such important activities as determining if an adverse reaction reported in clinical trials is caused by the study drug or a concomitant drug; conducting post-market surveillance for previously unobserved reactions; determining whether a drug could be repurposed (i.e., for a new indication); or finding patterns to predict drug interactions or other toxicity by pharmacologic class or similar chemical moieties."
"for all molecules in the database are shown in fig. s1 . this highlights the relatively broad distribution of half-life, clearance and percentage dose recovered in the urine across the range of dendrimer constructs that have been characterized. there was little accumulation of dendrimer dose (median of 7%) in the liver for the majority of constructs in the database (with 80% of dendrimer having a %dose in liver below 20%)."
"in the process, we identified 114 adrs that could not be mapped to meddra pts, e.g., label text denoting 'decreased embryo viability'. these terms were submitted to editor representatives from meddra for review. meddra editors suggested alternate mappings, some underspecified, for 39 unmapped terms and 2 annotations were deleted on their suggestions. 73 terms remain unmapped."
"the susceptibility of the dendrimer scaffold to in vivo biodegradation may also impact upon the rate of dendrimer elimination from the body, but surface functionalization with non-biologic groups (such as non-natural amino acids and peg) slows scaffold breakdown 3 . to this end, with the exception of two unmodified amine-terminated polylysine dendrimers, none of the dendrimers in table s1 were reported in their respective publications to have shown significant in vivo biodegradation that was expected to have driven the reported intravenous pharmacokinetics. biodegradability of the scaffold was, therefore, not included as a parameter that defined the ultimate pharmacokinetics, although the composition of the scaffold was."
"the final spl-adr-200db database was generated in two formats: 1) a dataset of 200 pairs of text files and brat annotation files, and 2) a database of distinct asserted adrs for each adr section of each of the 200 drugs in comma-separated value (csv) format. although both forms of spl-adr-200db present the same coding information, the paired text files format is intended for development of nlp applications, whereas the csv format (shown in table 1 ) is intended to serve as the first publicly available set of labelled adrs, and as an example for similar datasets in the future. to facilitate development of nlp applications, and as a final quality assurance step, we augmented brat annotation files with meddra and umls codes as shown in fig. 3 ."
"alternatively, we were also able to build predictors to assess whether a dendrimer construct would be cleared by the liver and/or excreted in urine. constructs were defined as having limited liver uptake or urinary excretion using a 20% cutoff for dose in liver and urine, respectively (see details in materials and methods). figure s3 of www.nature.com/scientificreports www.nature.com/scientificreports/ supplementary materials shows the roc curves obtained for both predictors, which achieved aucs of 0.87 and 0.86 for urinary excretion and liver uptake, respectively. the predictors were successful in predicting the differences between constructs that had limited liver uptake/limited urinary excretion compared to those that were cleared by the liver or excreted in urine, achieving an accuracy of up to 80%."
"the attributes that contributed most to the performance of each predictor were evaluated by principal component analysis (pca). both construct molecular weight and total peg molecular weight were consistently well ranked for all four predictors. this was expected given these attributes correlated well with the pharmacokinetic parameters by themselves. pca showed that these attributes contributed largely to the variability of the half-life data set, together with generation and structure flexibility (fig. s4a) . these were consistent with the other data sets. drug conjugation, both in terms of the type and number of surface drugs, played an important role in prediction performance, despite a clear correlation not being noticed during analysis of individual features. a histogram of the percentage of explained variance per feature (fig. s4b) shows that the majority of the features are necessary to explain variability (a linear drop on explained variance, instead of a usual logarithmic drop), suggesting that the selected group of variables are diverse and complementary."
"to correct for the individual biases of the annotators, the annotation process was as follows: four annotators (two mds, a medical librarian and a biomedical informatics researcher, all trained in biomedical annotation) annotated independently. each label section of interest (boxed warning, warnings and precautions, and adverse reactions) was doubly annotated. the disagreements between the annotators were reconciled in pairs. the remaining disagreements were resolved by all four annotators. using f-score as measure of pairwise agreement between the annotators 10, the agreement was good and improved during the process: the f-score was 74.9 ± 0.9% on average for the first 100 spls and 85.5% ± 0.9% for the second half of the spls."
"using this criteria, 20 papers collectively describing the pharmacokinetics of 69 distinct dendrimer structures were identified that provided sufficient information to compile the database. plasma clearance (cl) was included in the database where possible and was either taken directly from published papers or was extrapolated from available data by dividing dose by area under the plasma concentration versus time curve. where pharmacokinetic parameters needed to be extrapolated from graphical data, the data recovery program getdata graph digitizer v2.26 (getdata pty ltd, nsw, australia) was employed. where area under the plasma concentration time curve needed to be calculated to determine cl, this was calculated manually using the trapezoid rule and extrapolated to infinity by dividing the last plasma concentration detected by the elimination rate constant (k)."
"we downloaded the dailymed spl database from https://dailymed.nlm.nih.gov/dailymed/spl-resourcesall-drug-labels.cfm on september 29, 2015. the umls files containing medical terminology used in the db column example1 example2"
"for ebd schemes, the pdf of the test statistics is usually derived using random matrix theory (rmt); see, e.g., [cit] . in fact, the maximum and minimum eigenvalues of the sample covariance matrix have simple explicit expressions when both antenna number (k) and ample size (n ) are large [cit] . it is reasonable to assume that n is large especially when the secondary user is required to sense a weak primary signal, in practice, however, the number of antennas equipped at a single secondary user is usually small, say 2 or 3. thus the results obtained under the assumption that both k and n are large may not be accurate for practical multi-antenna cognitive radios. in this paper, our objective is to derive the asymptotic distributions of the eigenvalues of the sample covariance matrices for arbitrary k but large n . the asymptotic results obtained form the basis for quantifying the pdf of the test statistics for ebd algorithms."
"in summary, here we describe dendpoint, the first relational database and predictive method that associates physicochemical properties of a complex hyperbranched polymeric structure (notably dendrimers) with experimentally measured intravenous pharmacokinetic data. this provides the first opportunity to begin to systematically analyze the relationship between dendrimer structures and their biological behaviors, in the attempt to guide construct design and development. it has been carefully curated from the literature and will be updated regularly. although, in practice, dendrimers may ultimately be delivered via non-intravenous routes (such as subcutaneously or via inhalation) which will require the need for additional base physicochemical properties for optimal pharmacokinetic behavior, the intravenous route is currently standard practice for the systemic delivery of nanomedicines."
"the rest of the paper is organized as follows. section ii presents the system model for spectrum sensing using multiple antennas. two ebd algorithms are reviewed in section iii, including maximum eigenvalue detection (med) and condition number detection (cnd) algorithms. in section iv, we derive the asymptotic distributions of the test statistics of the two ebd algorithms for the scenario when the primary users are inactive. in section v, the results are derived for the scenario when there are active primary users in the sensed band. performance evaluations are given in section vi, and finally, conclusions are drawn in section vii."
"here, we describe dendpoint, the first in silico and widely available model to predict the intravenous pharmacokinetics of complex polymeric nanomaterials based on scaffold structure and physicochemical properties. we have manually curated a detailed relational database describing dendrimer biopharmaceutical behavior with various structural and chemical characteristics. this was used to develop a model to predict key pharmacokinetic parameters for dendrimers. dendpoint is available via a user-friendly freely available web-based system, accessible at http://biosig.unimelb.edu.au/dendpoint. this computational platform encompasses a relational database of pharmacokinetic properties of different dendrimer scaffolds together with a web-service capable of predicting and comparing dendrimer properties, including half-life, volume of distribution, clearance and dose in liver and urine, allows users to rapidly and easily browse literature-derived properties as well as predict, compare and visualize dendrimer pharmacokinetic properties."
"for the scope and level of details for each adverse reaction, we had to decide whether to annotate specific drug doses that caused it; specific populations in which it occurred; the severity of the reaction; its comparison to the occurrences in the placebo arms of the trials; the prevalence, and many other factors. for this pilot annotation, we settled on annotating the adrs, their severity and whether they were observed in animals or reported for the class of the drug or for the drug itself. more details are provided in the annotation guidelines: https://bionlp.nlm.nih.gov/ [cit] dversereactions/ annotationguidelines_tac2017adr.pdf."
"suppose the su is equipped with k antennas, which are all used to sense a radio spectrum for collecting n samples each. one of the following two scenarios happens."
"pharmacokinetic data was therefore obtained for dendrimers based on non-biodegradable triazine and polyamidoamine (pamam) scaffolds, as well as biodegradable polyester and polylysine scaffolds. while the effective 'size' of polymeric nanomaterials may be reported in terms of hydrodynamic radius or molecular weight, few of the papers reported in table s1 provided this information on radius and therefore, this parameter could not be included in the database. regardless, it has previously been suggested that the terminal half-life of dendrimers correlates more significantly with molecular weight than with radius 3 . in addition, three quarters of the dendrimers included in dendpoint presented some degree of surface pegylation. this is likely a result of the fact that since the molecular weight of the dendrimer scaffold is limited by poor conjugation efficiency and greater polydispersity for generations higher than approximately 5-6, surface pegylation is commonly required to increase size and prolong plasma exposure 3 . pegylation is also often employed either alone or in combination with acetylation (ac) to block surface reactive sites and prevent binding to cells and tissues."
"in small molecule drug design, the development and application of generalized rules and these tools have been widely used to improve compound quality and success rates. in addition to providing the first curated database of dendrimers to facilitate the analysis of nanoparticles, we demonstrate that it can be used as the basis to train novel predictive pharmacokinetic models."
". from theorem 3, γ 1 and γ k are asymptotically independent, and the limiting distributions of γ 1 and γ k areḡ q1,1 (x) andḡ qr,qr (x), respectively, which are defined in section iv."
"we cross-validated the brat annotation files and the comma-separated value (csv) files that contain distinct adrs for each spl section by assuring that all mention-level annotations are accounted for in the csv files, and that all adrs in csv files correspond to at least one mention-level annotation. [cit] adr challenge in xml format. the xml files combine the text files extracted from spls with the mention level annotations from brat annotation files and meddra and umls terms and codes for each mention."
"job submission can be easily done by informing construct properties and surface group compositions via an intuitive submission form (fig. s6 ). generation and construct molecular weight are required fields. after prediction a results page is exhibited (fig. s7), showing the pharmacokinetics properties, dendrimer depiction and plasma concentration curve, giving the user the option to wither modify the job for resubmission or compare the predicted properties with another dendrimer construct (fig. s8) . see supporting information for supplementary figures."
"during reconciliation, we noticed that for a list of adrs, common modifiers are often mentioned only once with the first adr. for example, 'injection-site reactions (e.g., injection-site rash, erythema)'. initially, we sometimes annotated such listed adrs on their own, but in the qa process we extended annotations to the fully stated adrs, e.g., 'injection-site erythema'."
manual annotation of the text required several decisions on: 1) the scope of annotation; 2) the tool to use for annotation; and 3) the annotation process.
"several attempts have been made to generate such a resource automatically from spls, one of the most comprehensive and authoritative sources of drug information. the widely used sider (side effect resource) database combines natural language processing (nlp) and filtering techniques to automatically extract adverse drug reactions (adrs) from spls 1 . sider is somewhat noisy: 1.2% of the extracted adrs were indications 1 . nlp-based approaches might also miss out-of-vocabulary adrs. the adverse drug reaction classification system (adrecs) combines sider data with adrs extracted directly from spls 2 . the adrs in adrecs are partially manually mapped to the unified medical language system (umls) metathesaurus (https://www.nlm.nih.gov/research/umls/) and to the medical dictionary for regulatory activities (meddra) (https://www.meddra.org/.) meddra is a standardised international medical terminology for regulatory communications in the registration, documentation and safety monitoring of medicinal products through all phases of the development cycle: from clinical trials to post-marketing surveillance. the umls integrates and distributes key biomedical terminologies and coding standards, including meddra. mapping the adrs to the umls facilitates interoperability, e.g., if an electronic health record contains adrs encoded in another medical terminology, umls will provide a crosswalk from this terminology to meddra."
"it is noticed that there are studies on the exact distribution of the condition number of sample covariance matrices for arbitrary k and n [cit], the formulas derived however are complex and cannot be conveniently used to analyze the sensing performance. furthermore, there are no results published for the case when the primary signals exist."
"this database reveals some general rules of dendrimer design, where construct molecular weight, flexibility and pegylation can all be used to tunably adjust the plasma exposure of a dendrimer. to begin with, whilst half-life is used as the most common parameter to describe the plasma exposure of a nanomaterial, clearance is a more appropriate parameter since it takes into account urea under the whole plasma concentration-time profile, rather than simply the elimination kinetics alone. with this in mind, plasma exposure can be increased (or rather, clearance decreased) via restricting urinary excretion, extravasation and uptake into cells and tissues by employing the following basic rules: (1) increased degree of surface conjugation or molecular weight of hydrophilic, biocompatible and poorly-biodegradable polymers such as peg (polymerisation), (2) increase construct molecular weight (size), (3) reduce surface charge (charge), (4) reduce structural flexibility (flexibility). based on the available data and our model, drug conjugation to the surface has a negligible effect on intravenous pharmacokinetics."
database and web interface. dendpoint's database and predictive models have been implemented as a user-friendly web-server freely available at http://biosig.unimelb.edu.au/dendpoint. the dendrimer pharmacokinetics information collected from literature search was consolidated as a mysql relational database (version 5.5.35). front-end development was created using the bootstrap framework (version 3.3.7). the server back-end runs on a linux server and was implemented in python using the flask frame-work (version 0.12.3). dendrimer depiction was developed with the javascript library d3.js (version 4.2.6) and plasma concentration plot was created with the highcharts charting tool (version 5.0.0).
"using the xml structure of the spls and the loinc codes shown below, we have extracted the following sections, if present in a given spl (the variations in the section naming are due to the changes to xml schema over the years):"
"we also identified 102 additional underspecified terms, e.g., we could only map the label text 'flexor tendon ruptures' to the meddra pt 10043248 'tendon rupture'. we submitted these additional 102 terms to the meddra editors. after their review, 83 adrs remained underspecified, while 19 had alternate mappings suggested by the editors. for example, we coded the adr 'renal tubular injury' to the meddra pt 10061481 'renal injury', whereas meddra editors suggested using 'renal tubular dysfunction' instead, and further proposed to add 'renal tubular injury' as a preferred term to the next version of meddra dictionary."
"each of the papers used described dendrimer pharmacokinetics in rodent models (notably mice and rats), and as such, the body weight of the animal models used can vary by up to 20 fold. while this has no bearing on t½ or proportion of injected dose recovered in liver or urine, cl is a function of distribution volume (vd) and therefore body weight. cl was therefore normalized to ml/h per kg body weight by dividing cl (in ml/h) by the average reported body weight of animals used in the study. where mean body weight or body weight ranges were not reported, mean body weights for the animals described and at the reported ages were extrapolated from growth curves published online by several breeders (harlan, charles river laboratories, taconic and the animal resources centre)."
"in an attempt to address the lack of effective predictive models for the in vivo behavior of nanomaterials, riviere and colleagues 8 [cit] . the approach involved comparing the surface adsorption of a set of small molecule probes and generating a 'surface adsorption index' to predict the binding of biomolecules (the 'protein corona') which is known to play a significant role in dictating the biodistribution behavior of nanoparticles 9 . subsequent to this, a number of investigators have used physiologically based pharmacokinetic models (pbpk) to simulate the mass-time biodistribution profiles for a range of metal nanoparticles [cit] as well as some polymeric nanoparticles [cit] . in most cases, these models were developed based on limited experimental data sets to predict the biodistribution and elimination kinetics of nanoparticles with a fairly narrow set of physicochemical variants (such as size and charge). the intention behind these models was to aid researchers in their selection of optimal particle properties for further development or in risk assessment analysis. the pbpk approach however, is not appropriate for predicting the pharmacokinetic behavior of more complex nanostructures such as liposomes and polymers that may be comprised of a variety of different scaffold components (such as different lipids or monomers). these models are also not easily adaptable and available for use by researchers with limited or no knowledge of biometric analysis."
"an essential task to complete the sensing design is to determine the test threshold, which affects both probability of detection and probability of false alarm. to do so, it is important to derive the pdf of the test statistics."
"we chose the brat annotation tool that allows annotating text documents via a web browser. although brat allows assisted annotation, i.e., it will display automatically identified adrs that can be manually edited, we chose to annotate the adrs completely manually (i.e., from scratch) to avoid bias, e.g., missing the adrs unmarked in the pre-annotation process 7, 8, as well as potential slow-down due to too many false-positives generated by a pre-annotation tool 9 . we annotated adrs as events, which allows associating the details about the adr to it as shown in fig. 2 . brat stores annotations in stand-off format in the annotation files paired with the text files shown to the annotators. in the annotation files, the adrs are presented as follows:"
"database curation. in total, the pharmacokinetic parameters of 69 distinct dendrimers, from over 600 papers, were manually curated into the dendpoint database ( fig. 2, table s1 ). many structural and physicochemical properties can dictate the pharmacokinetic behavior of dendrimers, including scaffold composition, dendrimer size, degree of surface pegylation, peg chain length, surface functionality (including charge and the presence of hydrophobic drugs) and structural flexibility 3, 23, 24 . the impact of each of these parameters on intravenous pharmacokinetics has been summarized previously 3 and they were therefore included in the database as summarized in table s1 (supporting information)."
"dendrimers are well defined hyperbranched polymeric systems that can range in size from 1-20 nm in diameter 19 (fig. 1 ), which can provide several pharmacokinetic advantages over much larger colloids and nanoparticles [cit] . drugs can be loaded either peripherally via internally triggered chemical linkers, or can be non-covalently loaded into the hydrophobic scaffold. although the clinical advancement of nanomedicines has been a slow process, starpharma's topical microbicidal gel (vivagel ® ) has recently gained regulatory approval in australia and europe for the treatment of bacterial vaginosis and a dendrimer-based formulation of docetaxel (dep ™ -docetaxel) recently successfully completed phase i clinical trials for the treatment of advanced solid tumors. the establishment of an in silico model capable of accurately predicting dendrimer pharmacokinetics is therefore timely and of increasing relevance."
"drug id 4e338e89-3cf2-48eb-b6e2-a06c608c6513 e9481622-7cc6-418a-acb6-c5450daae9b0 normalisation steps can be found in https://download.nlm.nih.gov/umls/kss/2017aa/ [cit] aa-active.zip. we uploaded to open science framework spl-adr-200db training set files (data citation 1), the test files (data citation 2) in brat annotation format, and the comma-separated value files containing adverse reactions by sections (data citation 3)."
"looking closer at the distribution of half-life and clearance divided by scaffold and flexibility revealed some general trends ( fig. 3a-d) . notably, as structural flexibility increased, half-life decreased and clearance increased. when assessing the effects of surface charge on the pharmacokinetic properties ( fig. 3e,f) a similar behavior was also observed. while clearance increased as surface charge move further from neutral (increasing charges, either negative or positive), half-life decreased. the polylysine-based scaffold presented the largest variability in pharmacokinetic properties, in part due to the large number of diverse constructs that have been systematically analyzed to date. triazine-based dendrimers were associated with lower clearance and longer half-lives. construct properties, information on surface functional groups as well as pharmacokinetic behaviour of 69 different dendrimers were collected and included in a relational database. this was used as evidence to train and test predicted methods via supervised learning. a user-friendly web interface was created for both database and predictive method."
"theorem 3: the limiting distribution of √ n b k µ k − i is given by (9) and (12) for real-valued case and complex-valued case, respectively, with the parameter k being replaced by q k . furthermore,"
"we extracted 5,098 distinct adrs from the brat annotation files. [cit] ab rest api and the downloaded meddra 18.1 files to find perfect matches to the exact adr strings. the primary terminology for fda pre-marketing and post-marketing drug safety evaluation purposes is meddra. therefore, we initially restricted the umls searches to meddra, and only used the appropriate semantic types from other umls sources if the adr could not be mapped to meddra. the 1,383 perfectly matched meddra preferred terms (pts) were split into four parts and reviewed manually by one of the annotators. the unmatched terms were also split into four parts, mapped manually by one of the annotators, and then the mappings were verified by another annotator. the disagreements were reconciled by all annotators."
"an early study of the authors [cit] b] has investigated the effect of reverberation on the efficiency of cms in improving the performance of svr. the performance of an svr system was measured by calculating the eer in rooms with different rts and volumes. test speech segments were made reverberant with rirs that were simulated using the image method of allen and barkley [cit] . it was shown that for high rts, the efficiency of cms decreases. in this section we extend the research to reverberant speech generated by convolution with measured rirs. the environments in which the rirs were measured are tabulated in tab. [cit] . in order to compare the results with simulated rirs, the image method was used to simulate rirs of rooms with similar dimensions and rts to the rooms in tab. 1. the svr system was using 20 msec speech frames in which mfcc and δmfcc were calculated to form 24-dimensional feature vectors, for which cms was either applied or not. target models were trained using the agmm approach [cit] . a background gmm (bgm) of 1024 gaussians was generated from one-minute long non-reverberant speech segments of 50 [cit] sre database. this bgm was used to train target agmms for 198 [cit] sre [cit] database. [cit] sre, for 686 male speakers with half-minute long speech segment each. the test speech segments were made reverberant by convolving them with simulated and measured rirs. the eer results were calculated by introducing the reverberant test speech segments to the target agmms and bgm of non-reverberant speech. figure 5 shows a scatter plot of eer values as a function of rt. the cross, circle, and triangle marks on fig. 5 represent eer values when either feature normalization was not used, or cms was applied, or cms was applied along with variance normalization, respectively. linear fitting to the eer values is shown in fig. 5 feature normalization, dashed curves denote using cms, and with thin solid curves denote using cms along with variance normalization. figures 5(a) and 5(b) refer to simulated and measured rirs, respectively. in the case of simulated rirs as well as in the case of measured rirs, it can be seen that cms is improving the performance of svr in a reduced manner with the increase of rt. moreover, it can be seen that for some high values of rt, cms may increase the eer rather than decrease it. these results support previous results [cit] b] in which it was shown that cms is improving the performance of svr in a reduced manner with the increase of rt, and validate them with measured rirs."
"with these two measures, dbi appropriately calculates the closeness of the two clusters by eq. 11, as the sum of their standard deviations is divided by the distance of their centroids. so dbi can be defined in eq. 12 where small values of dbi show that clusters are well separated. thus, ipo algorithm tends to reach the minimum value of dbi for the best result. note that in eq. 11, the worst separation of the clusters in each time iterations is selected by maximizing the value of dbi from each two clusters (worst case) in order to guarantee the best results for ipo algorithm. in the next section our proposed method of unsupervised ipo clustering is further explained (davies and for better understanding, assume that a ball has 5 cluster centroids for the maximum number of cluster limitation and it is in 3 dimensional spaces as shown in table 1 . for a threshold of 0.5, only the second, third and fifth cluster centroids are active."
"we propose to choose the synchronizing pattern sequence with minimal global distance. in statistical sense such signal can be considered as a median value over all the performances that belong to the set of a k or can be referred to as \"median\" sequence."
figs. 4-9 illustrate three standard grayscale images with their histograms. the comparison results of three methods on standard images are shown in table 5 . the parameters of ipo remain the same as seen in previous section. the ipo and two other methods were tested 20 times on each image histogram.
"the a-caching algorithm [cit] optimizes a single mjoin operator by adding/removing temporary caches for certain intermediate results. our proposed algorithms differ significantly from a-caching in several aspects. first, a-caching only deals with equi-join predicates, relying on value-based hashing to detect a cache hit/miss. our two-layer plan generation framework is general and can work with any physical implementation of multijoin queries. second, a-caching restricts the problem space by only considering solutions with a set of non-overlapping caches. in other words, two caches cannot have common joins. our proposed algorithms do not have this restriction, thus broadening the range of intermediate results that can be simultaneously stored. lastly, a-caching is a novel operator-level join implementation strategy of a single mjoin operator. our solution instead looks at the query plan-level, exploring the entire jtree solution space."
we now describe our state-selection algorithm that iteratively selects intermediate results to be stored thereby saving valuable cpu resources otherwise spent on re-computation. the pseudo-code for the algorithm is presented in algorithm 3.
"selecting which intermediate state(s) to be stored can be viewed as selecting edges in a join graph. the choice of which intermediate states to store is determined by two factors: edge frequency and edge weight. the edge frequency is the number of times an edge appears in the join sequences. in figure 9 .b, for input stream a the first layer of optimization generates the best possible join order (in polynomial time) to process the input tuples from a, similarly we have a join ordering for the input stream b, and so on. in figure 9 .b, the edge between input stream a and c appears 6 times in the set of join orders, while the edge between d and c appears twice. therefore, storing the intermediate results of a ✶ c would be of more benefit than storing results of d ✶ e. thus, heuristically, the higher the edge frequency, the more likely it is that storing the corresponding intermediate results can save cpu costs."
"as we mentioned above, the original vector space is redundant. additionally, the human body motion is intrinsically constrained, and these natural constraints lead to highly correlated data in the original space. therefore, we aim to find a more compact representation of the original data to avoid redundancy. to do this, we consider a set of performances corresponding to a particular action a k and perform the principal component analysis (pca) to all the postures that belong to that action. eventually, the following eigenvector decomposition equation has to be solved:"
"the ipo algorithm is designed to find the optimum answer for engineering problems and inspired by the phenomena of \"losing potential energy\". for instance, each ball has three specifications of position, height and angle in relation to other balls. the positions these balls assume create feasible solutions for the problem using the objective function to calculate the height for each ball."
"the mrf energy minimization approach shows the perfect performance in stereo matching and segmentation. likewise, we present a dense matching algorithm based on dp, which is used to synchronize human motion sequences of the same action class in the presence of different speeds and accelerations. the algorithm finds an optimal solution in real-time."
"we note that for some stream characteristics and system resource threshold settings a qualified plan may not exist in the solution space. this is reflected in figure 13 .c where jtree-finder is unable to find a qualified plan-given that jtreefinder explores the full search space, this implies that no solution exists. no trends can be drawn from figure 13 .c as it merely reflects the state of the particular solution space we studied."
"the intuition is to have a join solution that now exploits the benefits of both mjoin and bjtree join methods. our experimental evaluation supports the claim that considering jtrees in the solution space increases the possibility of finding qualified plans (see section 4). lastly, scenarios (as depicted in figure 5 .e) where the available system resources are too restrictive, no qualified plan exists. in such cases, a deployed system would have to resort to applying more drastic approaches such as load-shedding [cit] and memoryspilling [cit] to reduce the load of the system. these techniques incur loss of result accuracy or delays due to the addition of i/o costs. since these strategies are orthogonal to our topic of generation of query plans that are cpu and memory resource adherent, we refer the reader to the literature to learn more about these methodologies [cit] ."
"numerous methods have been proposed for solving the clustering problem. k-means is one of the more famous and widely used clustering methods in science and engineering fields compared to other methods. kmeans algorithm begins by specifying the numbers of randomly selected cluster centroids from a search space. each particle in the problem space is later assigned to a cluster using the minimum distance between particle and all of the clusters centroids. in the next step, new cluster centroids are calculated by averaging each cluster items. this process continues untill cluster centroids become constant and reflect the result of k-means algorithm. k-means is a very useful clustering method except for solving a massive dataset. in that case estimating the numbers of centroids seem unfeasible and k-means offers less than optimum solutions [cit] ."
"we present an action-specific model of human motion suitable for many applications, that has been successfully used for full body tracking [cit] . in this paper, we explore and extend its capabilities for gait analysis and recognition tasks. our action-specific model is trained with 3d motion capture data for the walking action from the cmu graphics lab motion capture database. in our work, human postures are represented by means of a full body 3d model composed of 12 limbs. limbs' orientations are represented within the kinematic tree using their direction cosines [cit] . as a result, we avoid singularities and abrupt changes due to the representation. moreover, near configurations of the body limbs account for near positions in our representation at the expense of extra parameters to be included in the model. then, pca is applied to the training data to perform dimensionality reduction over the highly correlated input data. as a result, we obtain a lower-dimensional representation of human postures which is more suitable to describe human motion, since we found that each dimension on the pca space describes a natural mode of variation of human motion. additionally, the main modes of variation of human gait are naturally represented by means of the principal components found. this leads to a coarse-to-fine representation of human motion which relates the precision of the model with its complexity in a natural way and makes it suitable for different kinds of applications which demand more or less complexity in the model."
"to estimate an inclined plane, ipo method uses straight lines to cross from the centroid of one ball to the centroids of other balls. to minimize the problem, formed angles between the straight lines and the horizontal line are calculated to find the direction and acceleration of each ball. the position of i-th ball from n p balls system can be defined as shown in eq. 1 with a restriction. here x i is the decision variable, k is the coordinate number and n d is the space dimensions. the position of i-th ball in k-th dimension is presented by"
"the body model employed in our work is composed of twelve rigid body parts (hip, torso, shoulder, neck, two thighs, two legs, two arms, and two forearms) and fifteen joints; see figure 1 (a). these joints are structured in a hierarchical manner, constituting a kinematic tree, where the root is located at the hip. however, postures in the cmu database are represented using the xyz position of each marker that was placed to the subject in an absolute world coordinates system. therefore, we must select some principal markers in order to make the input motion capture data usable according to our human body representation. figure 1(b) relates the absolute position of each joint from our human body model with the markers' used in the cmu database. for instance, in order to compute the position of joint 5 (head) in our representation, we should compute the mean position between the rfhd and lfhd markers from the cmu database, which correspond to the markers placed on each side of the head. notice that our model considers the left and the right parts of the hip and the torso as a unique limb, and therefore we require a unique segment per each. hence, we compute the position of joints 1 and 4 (hip and neck joints) as the mean between the previously computed joints 2 and 3, and 6 and 9, respectively."
"where x n,m (t) is the best synchronized version of the action x m (t) to the action x n (t). in literature the function τ(t) is usually referred to as the distance-time function. it is not an apt turn of phrase indeed, and we suggest naming it as the rate-to-rate synchronization function instead."
"objectives. the goals of this second experimental evaluation include the comparison of: 1) the average time required by the proposed algorithms to generate a qualified plan, and 2) the effectiveness of the proposed algorithms to find a qualified query plan if one such plan exists in the solution space. 3) the memory and cpu resource utilization of the qualified plans generated by the state-selection versus by the state-removal algorithms."
3. wisconsin breast cancer: in this dataset there are 683 instances with 9 numeric features consisting of 444 objects in class 1 (malignant) and 239 objects in class 2 (benign) [cit] .
"in this paper, a novel dense matching algorithm for human motion sequences synchronization has been proposed. the technique utilizes dynamic programming and can be used in real-time applications. we also introduce the definition of the median sequence that is used to choose a time-scale pattern for all other sequences. the synchronized motion sequences are utilized to learn a model of human motion and to extract signal statistics. we have presented an actionspecific model suitable for gait analysis, gait identification and tracking applications. the model is tested for the walking action and is automatically learnt from the public cmu motion capture database. as a result, we learnt the parameters of our action model which characterize the pose variability observed within a set of walking performances used for training. the resulting action model consists of a representative manifold for the action, namely, the mean performance, the standard deviation from the mean performance. the action model can be used to classify which postures belong to the action or not. moreover, the tradeoff between accuracy and generality of the model can be tuned using more or less dimensions for building the pca space representation of human postures. hence, using this coarse-to-fine representation, the main modes of variation correspond to meaningful natural motion modes. thus, for example, we found that the main modes of variation for the walking action obtained from pca explain the combined motion of both the legs and the arms, while in the bending action they mainly correspond to the motion of the torso."
"experiment set 1: when both mjoin and bjtree are qualified plans they have similar accumulated throughput 2 as shown in figure 7 .a. sufficient cpu ensures that new tuples can be processed quickly without delay, which is true for both mjoin and bjtree, as in figure 7 .c. figure 7 .b clearly displays the much larger memory usage of bjtree for storing intermediate results in comparison to mjoin."
"our algorithm assumes that a particular sequence is chosen to be a time scale pattern for all other sequences. it is obvious that an arbitrary choice among the training set is not a reasonable solution, and now we aim to find a statistically proven rule that is able to make an optimal choice according to some appropriate criterion. note that each synchronized pair of sequences (n,m) has its own synchronization distance calculated by (12) . then the full synchronization of all the sequences relative to the pattern sequences n has its own global distance:"
"i k is the number of training performances for the action a k, ψ t i corresponds to the tth posture from the ith training performance, and finally, f denotes the total number of postures of each synchronized performance."
"here ɸ ij k is the angle between the i-th ball and jth ball at t (time interval). for updating each ball's position at every time interval, the law of motion with constant acceleration is used where rand 1 and rand 2 are two random weights with uniform distribution at interval [cit] to give a stochastic characteristic to ipo algorithm. it is important to notice that in heuristic algorithm adopting a natural phenomenon is followed by certain modifications on the relations. for example, gravitational constant in gsa (g 0 ) is changed by adaptation at each time iterations. thus, the term 1/2 seems negligible in the law of motion with constant acceleration."
"researchers used davies-bouldin index (dbi) as an objective and criterion for ipo algorithm clustering process [cit] . in another study, each of the ipo algorithm agents called \"tiny ball\" represented the number and position of cluster centroids in the problem space. algorithm was initialized step by step, where each ball length was randomly changed to find the best one by using minimum db index and a threshold for each time interval. this process was repeated until terminated criterion was reached and the best db index value occurred [cit] ."
"multi-join query the second layer of optimization piggybacks on the first layer by using the generated best possible mjoin or bjtree plans as the starting point as they provide lower bounds on memory or cpu utilization respectively. for the second layer we propose two polynomialtime hill-climbing search algorithms, named state-selection and state-removal. both exploit the negative correlation between cpu and memory usage to generate a qualified jtree. more precisely, state-selection starts with the previously generated mjoin-based plan (i.e., guaranteed to be minimal in memory usage) and reduces the excessive cpu resource utilization by sacrificing some memory resources ( figure 3) . in this process, we aim to save on cpu costs wasted on the re-computations of intermediate results. on the other hand, state-removal starts with a good bjtree-based plan (i.e., optimal or near-optimal in cpu usage), and aims to reduce the excessive memory usage by selecting intermediate states to be removed at the expense of increasing cpu resources needed for their re-computation."
"to summarize, generating a query plan that is optimal in one resource usage while out-of-bound in the other is not an acceptable solution. therefore, the aim is to generate a query plan with both resource consumptions within their respective system resource capacities, henceforth called a qualified plan [cit] . all qualified plans are guaranteed to produce results at the same output rate [cit] ."
"to illustrate this, consider the two commonly used methods for executing continuous joins: binary join trees (bjtree) [cit] and multi-way join operators (mjoin) [cit] . a bjtree is composed of binary join operators that store intermediate results, while an mjoin is a single operator that takes as input all participant streams. the new tuples from each stream in mjoin are joined with the remaining streams in a particular order. existing optimization techniques for both these join methods aim to minimize the total number of intermediate results [cit] . in bjtree, this reduces the memory required to store intermediate results as well as the cpu costs for future joins. on the other hand, in mjoin, this reduces the cpu costs needed to recompute intermediate tuples."
"x  initial population numofballs  number of balls numofdimensions  number of dimensions repeat heights  fitnesses of balls bestx  position of ball with best fitness till now a(1 to numofballs, 1 to numofdimensions"
"return p // a qualified plan 18: return -1 // no qualified plan found time complexity: a join graph with n vertices has at most n − 1 intermediate states. therefore, state selection and operator merging process may be repeated at most n − 1 times. if the chosen state-of-the-art mjoin ordering algorithm [cit] has the timecomplexity of o(n 2 log(n)), the total running time is bounded by o(n 3 log(n))."
"is the feature vector of the t'th frame (t here is a discrete time index), where n is the number of mfcc coefficients. transmission channels may add a convolutive effect to the speech signal prior to the process of feature extraction. this may result in feature vectors distortion. for that reason feature normalization may be used. in this chapter we discuss the cms technique, which is the operation of subtracting the sample mean [bimbot et [cit] ."
"many directions for future work are possible. we could consider the constraint problem for handling multiple queries. two alternatives for addressing this problem may be to: 1) first allocate a fixed amount of the cpu and memory resources to each query, and then to directly apply the strategies presented in this work to each of the queries, or 2) to simultaneously build the plans for the different queries such that their collective resource consumptions are within bounds. while the former alternative can be easily achieved, the latter may be more practical as the criteria for pre-allocation of resources is a challenging problem in itself. another avenue for future work is to add additional constraints such as latency and then to identify their correlations to cpu and memory resource usages."
"organization: our experiments are categorized into four sets by adjusting 1) availability of system resources cp u avail and mem avail and 2) input stream arrival rates and selectivities. our aim is to reproduce the four distinct scenarios presented in section 3.1 ( figures 5.a-d), where a qualified plan exists in the solution space. the experimental sets are: 1) memory and cpu resources are sufficient for executing both bjtree and mjoin (figure 5.a), 2) where cpu resources are sufficient for executing the bjtree but insufficient for mjoin ( figure 5 .b), 3) memory is configured such that it is sufficient for mjoin but not for bjtree ( figure 5 .c), 4) both cpu and memory resources are insufficient for either bjtree or mjoin, hence needing to include general jtrees ( figure 5 .d) in the solution space."
"in the iso 3382 standard [iso 3382:1997 [iso 3382:, 1997, rt is calculated from a least squares based linear fitting of schroeder's energy decay curve in order to compensate for the non-linearity and for the noise-floor effect. room response from a source to a receiver can be given in the frequency domain by the room transfer function (rtf). in rectangular rooms, the rtf is known to be a combination of natural or eigen modes. at frequencies where the density of the eigenmodes is more than three eigenmodes for a 3db bandwidth of a given eigenmode, the sound field is usually considered to sufficiently satisfy the assumptions of diffuse field theory. in diffuse fields, rt is related to the volume by sabine formula [cit]"
"where a is the average absorption coefficient along the room boundaries. an important room parameter that can be measured from the rir is rt, which is the time that takes the energy in a room to decay by 60 db once the source is turned off. by assuming that until the source was turned off it had been producing a stationary white noise, rt can be calculated from the rir by using schroeder's energy decay curve [cit] 22 10 10 0"
"once all the sequences share the same time pattern, we learn an action specific model which is accurate without loosing generality and suitable for many applications. in this section we consider the waking action and its model is useful for gait analysis, gait recognition, and tracking. thus, we want to learn where the postures lie in the space used for representation, how they change over time as the action goes by, and what characteristics the different performances have in common which can be exploited for enabling the aforementioned tasks. in other words, we aim to characterize the shape of the synchronized version of the training set for the walking action in the pca-like space."
"the memory cost of an mjoin, immaterial of the chosen join ordering, is the same and fixed to be the total state size for maintaining input stream tuples. this cost is relatively stable through out the life span of the query. the run-time memory costs may fluctuate as intermediate tuples temporarily exist and can be minimized by choosing the optimal join orderings. the memory costs for the mjoin (figure 4 (b)) thus is estimated as:"
"in section 7 we survey the related work. we analyze the cpu and memory cost models to reveal the conditions under which these resources have a positive versus a negative correlation in section 2. in section 3, we present various resource settings that would require the search space to be extended to include general jtrees. additionally, here we also present our dynamic programming based jtree-finder algorithm. section 4 experimentally highlights the high time complexity of jtree-finder. we propose our polynomialtime two-layer plan generation framework in section 5. the effectiveness of our approach is presented in section 6 through our experiments. section 8 concludes the paper."
qualified plan generation is an exponential time-complexity problem even with richer pruning techniques [cit] . we therefore now put forth an efficient hill-climbing based twolayer plan generation framework that generates qualified query plans. algorithm 2 presents the pseudo-code of our framework.
"image is a dataset used for experimental activities to evaluate clustering methods. in this section, researchers explain the proposed method and present results on a series of standard images. these grayscale images are lena, cameraman and peppers. using ipo clustering, we can find the best number of clusters on an image dataset, where each ball is reconstructed similar to the previous section and data clustering ipo structure. image is a matrix, with a reduce computational cost for using an image histogram instead. an image histogram is a chart that shows the distribution of intensities in a grayscale or color image. you can use the information in a histogram to choose an appropriate enhancement operation. for example, an image histogram shows the highest and the lowest levels of intensity in an image which can be used as a criterion for a better separation in clustering algorithms."
"for reverberant speech, if the rt is larger than the short time fourier transform (stft) frame size, there will be time-smearing of the feature vectors. an increase in rt increases this time-smearing. this effect may cause the gaussian means of the gmm to come closer together. in order to examine this, the weighted average distance between the gaussians in the gmm and the overall mean feature vector can be calculated in the following form:"
"clustering is a way of finding the hidden data structure and refers to a set of data with shared common properties as separate entities. a suitable clustering method helps classify a large group of n-data items with p-dimensional features, to be placed into smaller groups, where each group will share similar properties with its items, and dissimilarity with items in the other groups. clustering algorithms are used in various fields of science to solve engineering problems specific to bioinformatics [cit] ."
"the example illustrated in fig. 1 helps to understand these concepts. in fig. 1a, three types of 2-dimensional samples are shown along the clustering results with suitable clusters. in fig. 1b, a clustering result shows a big interclass distance (d) causing a smaller number of clusters and in fig. 1c, the number of clusters increases to more than expected due to a shorter interclass distance (d) or larger intra-class distance (d)."
"cpu costs for the mjoin in figure 4 (b) is the cumulative cost of processing tuples from streams a, b and c. based on the optimal join orderings in figure 4 (b), a new tuple from a is first inserted into state s a (at cost c i ). existing tuples that are now outside the window frame are purged 1 from s a (at cost c d ). this inserted tuple is then joined with tuples in state s b and the resulting tuples are used to join with tuples in state s c . a similar process applies to tuples from b and c. the cpu costs for input a in a unit time"
"both p miss and p fa are functions of the threshold θ, and they each come at the expense of the other. the threshold θ is used as a parameter to yield the detection error trade-off (det) curve, which plots p miss as a function of p fa . the point on the det curve where p miss equals p fa is the eer. the eer is usually used as a scalar measure of the performance of svr systems."
"to illustrate and discuss the results of our experiment, we have prepared the following two subsections. at first data clustering results are explained, and then clustering method results on image histogram are shown."
"as indicated by equations 3 and 5, mem bjtree is always larger than mem mjoin as the bjtree stores all intermediate states. so a negative correlation between cpu and memory may exist when the cpu costs of bjtree are smaller than the cpu costs of mjoin. at first glance, this seems to always hold, because without storing intermediate results, mjoin requires extra cpu resources for re-computation. however, bjtree also needs extra cpu resources to maintain intermediate states. the recomputation of the state bc requires cp u mjoin recomput while cp u bjtree maintain is the cost to maintain the state ab. from equations 2 and 4, we have:"
"the parameters ω i, σ i, and μ i are estimated using the expectation maximization (em) algorithm [cit] . the covariance matrix σ i can be selected as either diagonal or a full matrix. the interpretation of a diagonal covariance matrix is that the feature vector coordinates are independent of one another. the computation of the parametric pdfs is much simpler in this case. the advantage of the full covariance matrix, however, is the enhanced generalization of the parametric pdfs in modeling the conditional pdfs. in practice, gmm is used with diagonal covariance matrices to approximate the case of one gaussian with a full covariance matrix with less computational effort. speakers that are known to a certain hypothesis are referred to as target speakers of that hypothesis, and impostor speakers to other hypotheses. performance analysis of svr is measured with miss probability, p miss, which is the probability that a target model was rejected."
"the effect of gmm order on svr with reverberant speech was investigated. time-smearing of the feature vectors due to reverberation reduces the optimal gmm order in terms of minimum bic and kic. when tested on a gmm-based svr system, reducing model order improves system performance for highly reverberant speech. a future adjustment to agmm may be proposed in this direction the effect of room volume and rt on the performance of cms applied to mfcc feature vectors in svr was investigated. it was shown that the performance of cms may degrade with the increase of rt. in some cases of high rt, cms may increase the eer of svr rather than decrease it. hence, in these cases, cms should not automatically be used. as a future work, we purpose combining a cms decision block in svr."
"4. contraceptive method choice (cmc): this dataset consists of 1473 samples, including 3 classes where samples are characterized by 9 features. there are 629 instances in class 1; 334 instances in class 2 and 510 instances in class 3 [cit] ."
"jtree-finder differs from the classical algorithm [cit] in several aspects. first, rather than just looking into left-deep bjtrees [cit], the proposed algorithm explores the much richer jtree solution space. the complete search space contains all possible query plan shapes, including bushy bjtrees, mjoins and jtrees, as in figure 1 ."
"recognizing the recent decade of ever growing information and data base expansion, has made the need for a method capable of clustering massive amounts of information most urgent. hence researchers have tried to find new methods to address the issue in a timely and cost efficient manner [cit] . clustering methods consist of two concepts known as: \"intra-class\" and \"interclass\" distance. the \"intraclass\" is the distance between particles of the cluster to its centroid, and \"interclass\" is the distance between centroids of two different clusters [cit] . it is understandable that if a clustering algorithm attempt to minimize the first (intra-class) concept, the number of clusters would grow unintentionally, and, if the second (interclass) concept is maximized, only the number of clusters will decrease more than expected. thus, an optimum solution would have to include a tradeoff between these two concepts."
"in our study, we used davies-bouldin's index (dbi) to assess the ipo algorithm as an objective function by compromising the distance between inter-class and intra-class distances. a more detailed description of dbi is offered in the next section."
"as stated before, the training sequences are acquired under very different conditions, showing different durations, velocities, and accelerations during the performance of a particular action. as a result, it is difficult to perform useful statistical analysis to the raw training set, since we cannot put in correspondence postures from different cycles of the same action. therefore, a method for synchronizing the whole training set is required so that we can establish a mapping between postures from different cycles. let us assume that any two considered signals correspond to the identical action, but one runs faster than another (e.g., figure 2(a) ). under the assumption that the rates ratio of the compared actions is a constant, the two signals might be easily linearly synchronized in the following way:"
"after assigning this structure to the balls in ipo method, k-nearest neighbor (knn) method is applied to find clusters by using the number of clusters and their centroid positions as specified by balls. datasets table 1 . example for a ball in ipo method. 0.3 0.6 0.8 0.1 0.9 6.1 3.2 2.1 6 4.4 7 9.6 5.3 4.2 5 8 4.6 8 4 4"
", is join-order-dependent. choosing a better join ordering lowers the size of intermediate states, which decreases the memory cost as indicated by equation 5, and also lowers the cpu costs as indicated by equation 4 . hence in bjtree, cpu and memory costs are positively correlated."
"population-based methods are inspired by the social interactions dynamics between individuals. for instance, pso simulates group cooperation in flocks of birds where each particle tries to move toward the best position by using its own previous experience guided by the neighboring particles. sharing information in population-based algorithms is a common strategy when each individual shares its information with others in order to guide the swarm to its goal of \"optimum position\". this cooperation between particles is known as swarm intelligence, with a significant improving effect on the algorithms' results [cit] ."
"initially dtw method was developed for the onedimensional signal processing (in speech recognition, e.g.). so, for this kind of the signal the euclidean distance minimization with a weak constraint (the derivative of the synchronization path is constrained) works very well. in our case the dimensionality of the signal is up to 37d and weak constraint does not yield satisfactory robustness due to the noise and the signal complexity. we propose to minimize a composite distance that consists of two terms: a distance itself and a smoothness term. such kind of a distance has the same meaning of the energy in mrf optimization techniques."
"to prove the correctness of our approach, we manually synchronized the same training set by selecting a set of 5 key-frames in each sequence by hand following a maximum curvature subjective criterion. then, the training set was resampled; so each sequence had the same number of frames between each key-frame. in figure 5 (c), the first 4 dimensions within the aspace of the resulting manually synchronized sequences are shown. we might observe that the results are very similar to the ones obtained with the proposed automatic synchronization method. the synchronized training set from figure 5 (b) has been used to learn an action-specific model of human motion for the bending action. the model learns a mean-performance for the synchronized training set and its observed variance at each posture. in figure 5 synchronized posture. the input training sequence set is depicted as dashed blue lines. this motion model can be used in a particle filter framework as a priori knowledge on human motion. the learnt model would predict for the next time step only those postures which are feasible during the performance of a particular action. in other words, only those human postures which lie within the learnt variance boundaries from the mean performance are accepted by the motion model. in figure 6 we show two postures corresponding to frames 10 and 40 from the learnt mean performance and a random set of accepted postures by the action model. we might observe that for each selected mean posture, only similar and meaningful postures are generated."
"the dp technique is a core of the dynamic time warping (dtw) method. dynamic time warping is often used in speech recognition to determine if two waveforms 2 eurasip journal on advances in signal processing represent the same spoken phrase [cit] . in addition to speech recognition, dynamic time warping has also been found useful in many other disciplines, including data mining, gesture recognition, robotics, manufacturing, and medicine [cit] ."
"the discrete function δ(p) coincides with the optimal path through the dsi trellis as it is shown in figure 3 . here term \"optimal\" means that the sum of the cost values along this path plus the weighted length of the path is minimal among all other possible paths."
"(i) prepare a 2d dsi matrix, and set initial cost values e 0 using (14) . (ii) find the optimal path through the dsi using recurrence equations (16)- (17). (iii) synchronize x m (t) to the rate of x n (t) using (18) ."
"additionally, to prove the advantage of our approach with respect to dtw we applied our algorithm with the cut objective function (without smoothness term), which is coincide with the dtw algorithm. in this case the synchronization process was not satisfactory: some selected mean postures were completely outliers or nonsimilar to any meaningful posture. it means that the smoothness factor μ in (12) and (16) plays an important role. to find an optimal value of this parameter a visual criterion has been used (the manual synchronization that had been done before yields such a visual estimation technique). however, as a rule of thumb the parameter can be set equal to the mean value of the error term e(i,d):"
"to evaluate performance of the proposed method, we have used 4 well-known standard benchmarks and tested the ipo algorithm on 3 standard images. to compare data clustering results, three other algorithm results were used such as particle swarm optimization (pso), gravitational search algorithm (gsa) [cit] and central force optimization (cfo). also, image histogram clustering results were compared with those of two other methods such as genetic clustering with undefined k (gcuk) [cit] and variable length improved genetic algorithm (vliga) [cit] ."
"time complexity: traditional optimizers [cit] that use dynamic programming to generate optimal plans have an exponential time complexity [cit] . similarly, jtree-finder has an exponential time-complexity and therefore is not practical for stream query processing"
"now we formulate an optimization problem as follows: find the time-disparity function δ n,m (p), which minimizes the synchronization distance between the compared signals x n and x m, that is,"
"comparative study: in section 6, we present an extensive comparative study of the proposed polynomial-time techniques by varying the number of streams n in between 3 to 20 streams."
"state-of-the-art query optimization algorithms in static databases [cit] primarily focus on generating an optimal or near-optimal plan by minimizing a single cost function, typically the total processing costs comprised of i/o or cpu [cit] . continuous query processing [cit] differs from its static counterpart in several aspects. first, the incoming streaming data is unbounded and the query lifespan is potentially infinite. therefore, runtime output rate is a better metric than the total cpu time needed to handle all input data [cit] . when the per-unit-time cpu usage of a query plan is less than the available system cpu capacity, the query execution is able to keep up with incoming tuples and produce real-time results at an optimal output rate [cit] ."
the dense matching algorithm that synchronizes two arbitrary x n (t) and x m (t) prerecorded human motion sequences x n (t) and x m (t) is now summarized as follows.
"the algorithm takes as input arguments a user query represented by its join graph jg, system resource constraints cp u avail and mem avail and the mjoin plan p mjoin generated by the first layer of plan generation. mjoin plan only stores the tuples from each of the input streams in states, thus p mjoin has the least memory utilization. however, since the mjoin plan has to recompute all intermediate results, the cpu resource usage needed to recompute the intermediate tuples is high. in this technique, we apply the principle of negative correlation of sacrificing memory resources by storing some intermediate results, thereby saving on the corresponding re-computation cpu costs."
the optimal path problem can be easily solved by using the method of dynamic programming. the method consists of step-by-step control and optimization that is given by a recurrence relation:
"the time and speed of problem solving are also good criteria to compare these methods. in table 4, the speed of each method on iris data is illustrated in seconds and compared the number of particles used in calculation (neval)."
"where u is the normalized height of the pelvis, and θ x l, θ y l, θ z l are the relative directional cosines for limb l, that is, the cosine of the angle between a limb l and each axis x, y, and z, respectively. directional cosines constitute a good representation method for body modeling, since it does not lead to discontinuities, in contrast to other methods such as euler angles or spherical coordinates. additionally, unlike quaternion, they have a direct geometric interpretation. however, given that we are using 3 parameters to determine only 2 dofs for each limb, such representation generates a considerable redundancy of the vector space components. therefore, we aim to find a more compact representation of the original data to avoid redundancy. let us introduce a particular performance of an action. a performance ψ i consists of a time-ordered sequence of postures"
"to understand the search space and provide the baseline to compare our polynomialtime heuristics, we now extend the classical bottom-up dynamic programming approach [cit] to now search the extended jtree solution space and generate qualified plans. henceforth, it is called jtree-finder. for a given user query and system resource constraints, if a qualified jtree plan exists in the solution space, jtree-finder is guaranteed to find it. in this work, we primarily introduce jtree-finder and its results as a benchmark for evaluating the effectiveness our heuristic-based two-layer qualified plan generation framework presented in section 5. in principle, other dynamic programming techniques [cit] and pruning techniques [cit] could similarly be employed."
"here r i,qt is the maximum value of dbi for i-th cluster with respect to other clusters; t and q are the same previously mentioned constants."
"in this paper, inclined planes system optimization (ipo) algorithm is used to cluster a number of standard datasets. the ipo clustering process is evaluated in each time interval to determine the correct number of clusters with a validity criterion function. various validity functions are designed by researchers such as hubert and levin, likelihood, ssi, marriot and others [cit] ."
"as shown below, v i k (t) is the velocity of ball i in dimension k, at time t. the k 1 and k 2 are two changing constants with time as seen in eq. 7 and eq. 8. the v i k is defined as eq. 6, where x best k is the ball with the lowest height (i.e., fitness) among other balls in all time iterations till the current time iteration for k-th dimension."
"on the other hand, we are also interested in characterizing the temporal evolution of the action. therefore, we compute the main direction of the motion v t for each that is,"
"after clustering image histograms, images are clustered by using histogram threshold by values of cluster centroids. as we can see in table 5, ipo has a better result for value of objective function in all cases. ipo method can find the number of cluster centroids in near optimum range in comparison to vliga and gcuk methods. so, in terms of fitness, the ipo clustering has offered significantly better results than the two other methods. in figs. 10-12, the illustrated image results reveal a fine clustering by using the ipo method."
"in summary, the plan generation framework will have one of four possible results: (1) a qualified mjoin, (2) a qualified bjtree, (3) a qualified jtree, or (4) a negative result indicating that the system resources are not sufficient. the first-layer either outputs (1), (2) or (4), or triggers the second-layer. the second-layer either identifies a qualified jtree (3), or terminates without a result (4)."
"we thus design a polynomial-time qualified plan generation solution in the form of a two-layer framework. in the first layer, the 2-dimensional problem is transformed into a 1-dimensional minimization problem. this allows us to employ state-of-the-art solutions [cit] that exploit positive correlation among the two resources to generate a good mjoin or a good bjtree plan respectively. this layer either returns 1) a qualified mjoin or bjtree plan, if found, or 2) a negative result to denote that the available resources are too limiting and hence no qualified plan exists, or 3) triggers the second layer of plan generation into action."
"in summary, state-of-the-art techniques avoid including general jtree shaped plan as this makes the search space considerably bigger and therefore have longer execution time. however, generating an optimal plan for one resource usage while being out-of-bound in another resource is not a viable solution in our context. figure 5 clearly demonstrates that exploring the extended search is indeed useful and can potentially avoid the repeated triggering re-optimization or load shedding."
"in contrast, approaches focused on 3d tracking and reconstruction require to deal with a more detailed representation about the current posture that the human body exhibits [cit] . the aim of full body tracking is to recover the body motion parameters from image sequences dealing with 2d projection ambiguities, occlusion of body parts, and loose fitting clothes among others."
"this study investigated the application of inclined planes system optimization algorithm on data clustering and grayscale histogram images. hybrid of ipo algorithm, dbi and knn method were combined to find optimum number of clusters in available data. several famous data benchmarks and few standard image datasets were used to illustrate the proposed method results. histogram of images was used to reduce the amount of the data and increase the calculation speed. in terms of data clustering, the proposed ipo method was compared with other well-known methods. four optimization algorithms were used in terms of image clustering and the results of 2 similar image clustering methods were compared with the proposed ipo method. in conclusion, researchers found the results of ipo method compared with other similar methods were more powerful in most cases."
"the process described so far ends up generating binary join nodes for the merged vertices. we now add further steps to merge several binary joins into an mjoin. this is done only when both memory and cpu costs can be saved. let the edge between vertex c and de in figure 11 .a be chosen as the next edge to be merged. the new merged vertex can either be implemented as a bjtree or a single mjoin as shown in figures 11.b and c respectively. in the state-selection algorithm, we define a function called optimize (line: 9) that takes as inputs the merged vertex, the modified join graph and the system resource"
"are searched by knn and clusters are determined. afterwards, dbi is calculated for clusters, and its value is used as a fitness for ipo algorithm. by time iterations, ipo algorithm tends to reach a minimum value for dbi as an objective function. so, as time passes, the best cluster centroids are stored. this process continues until the termination criteria occur. the best ball obtained from the proposed method will hold the best cluster's number and center position."
"in this study, performance of the proposed method is based on 4 standard datasets and 3 histograms from standard reference images to reveal its effectiveness on reliability and power of the method on clustering problems in similar applications."
"numerous clustering methods have been designed including: hierarchical clustering, fuzzy clustering, k-nearest neighbor (knn) [cit] . traditional clustering methods perform their duty perfectly up to a certain point and until some difficulties arise with unknown number of clusters in a database showing numerous dimensions. this rapid growth of scientific information will inevitably pose problems with an expanded volume of scientific data [cit] ."
"balls in ipo tend to go toward lower heights on each plane. to assign an acceleration value to each, balls with lower heights (fitness) are used. these accelerations on various planes are added to obtain the total acceleration of each ball. in fact, the acceleration in each dimension is calculated separately for each ball without consideration for the movement of other balls. it means acceleration is calculated between two sequential time intervals and later, the acceleration amount and direction are calculated as shown in eq. 3 and eq. 4 below, where u(.) is the unit step function."
"the ipo algorithm design was built on the sliding motion dynamic along a frictionless inclined surface. agents or \"tiny balls\" in this algorithm, similar to the particles in pso or ants in aco have the capacity to search the problem space and find the nearest optimal solution. these tiny balls reach a certain height for fitness."
"a commonly used procedure of mfcc feature extraction is shown in fig. 1 [cit] . the pre-emphasis filter is applied to enhance the high frequencies of the spectrum, which are generally reduced by the speech production process. the stft block splits the signal in the time domain into overlapping frames where the signal is considered to be stationary, and calculates the fast fourier transform (fft) of each frame. then, filter banking is applied by integrating the magnitude fft of the signal frames with triangular windows in the mel-frequency domain. afterwards, the db level is calculated. this results in a series of energy scalars for every frame. discrete cosine transform (dct) is calculated, from which coefficients are selected to form mfcc feature vectors. applying a discrete-time derivative results in δmfcc feature vectors, such that"
"the focus of this work is to design efficient algorithms for finding qualified continuous query plans at runtime. we employ existing methods [cit] to collect statistics at runtime. when a better plan is found by any one of our proposed algorithms, we apply migration strategies [cit] to safely transfer the current query plan to the new plan. these migration strategies which focus on binary join plans can be easily extended to handle the movement of join states across our general jtree plans."
"we aim to find an optimal gmm order for reverberant speech. the bayesian, akaike, and kullback information criteria (bic, aic, and kic, respectively) [cit] were used to estimate the unknown order of the target models with the training observation. the criteria are defined as follows"
"new clustering methods are mainly aimed at the compilation of past methods and heuristic algorithms. various forms of heuristic algorithms were initially introduced decades ago. the most popular and famous algorithm was simulated annealing (sa) [cit], artificial immune system (ais) [cit], ant colony optimization (aco) [cit], the genetic algorithm (ga) [cit], and there were particle swarm optimization (pso) [cit] and harmony search (hs) [cit] ."
the synchronized version of the training set is utilized to learn an action-specific model of human motion. the observed variances from the synchronized postures of the training set are computed to determine which human postures can be feasible during the performance of a particular action. this knowledge is subsequently used in a particle filter tracking framework to prune those predictions which are not likely to be found in that action. this paper is organized as follows. section 2 explains the principles of human action modeling. in section 3 we introduce a new dense matching algorithm for human motion sequences synchronization. section 4 shows some examples of data base syncronisation. section 5 describes the action specific model and explains the procedure for learning its parameters from the synchronized training set. section 6 summarizes our conclusions.
"the proposed method of ipo tends to find minimum location of objective function f(x) defined by the problem space. for each ball, ipo parameters are calculated in separate dimensions. the angle between the i-th ball and j-th ball at the time interval of t is calculated in the following equation, where f i (t) and f j (t) are the objective function values (heights) for the i-th and j-th ball in time t respectively."
"the join graph (jg) (in figure 4 .a) represents a multi-join query along with statistical information such as input rates, selectivities, etc. a vertex in the join graph represents an input stream, marked by its stream name and arrival rate. an edge between two vertices indicates a join predicate between the two streams and is marked by the join selectivity. for simplicity, henceforth we assume independent join selectivities. in principle a richer selectivity estimation-model [cit] could also be utilized. however, this would further complicate the already np-hard problem considered in this work."
"second, real-time response requirements make continuous queries memory resident [cit] . stateful operators, such as joins, store input tuples in states with which future incoming tuples of other streams will join. in time-critical applications, such as fire-sensor monitoring, it is common to have multi-join queries with large numbers of participant streams with high input rates. in such scenarios, the size of the in-memory operator states could potentially grow to be very large, making memory a precious resource. memory overflow can result in unacceptable outcomes, such as temporary halt of query execution [cit], approximation of query results [cit] and in some cases thrashing."
"1. iris: this is perhaps the most famous dataset in literature and in the field of clustering. the iris dataset consists of 150 instances with four numeric features, which contains three classes of 50 instances, where each class refers to a type of iris plant [cit] ."
"objectives. the goals of the first experiment are: 1) verify the cost analysis used (section 2) by comparing the performance of best mjoin and bjtree plans against the general jtree-based plans generated by jtree-finder under different resource constraints, and 2) show that the inclusion of jtrees in the solution space increases the possibility of finding a qualified plan. environment. the proposed plan generation framework is implemented in the continuous query processing system, [cit] . algorithms proposed in this work were implemented in java. all experiments are conducted on an intel 1.5 ghz machine with a 512 mb."
"we first present our dynamic programming-based jtree-finder algorithm that explores the complete jtree search space and guarantees the generation of a qualified plan. this guarantee comes at a high complexity cost, making it not practical for runtime optimization needed for streaming applications."
"in table 2, ipo clustering method from each dataset shows better results compared to the three other methods with an exception of cancer data where none of the algorithms reaches the best number of clusters. in wine data, pso has a better fitness but reaches a wrong cluster number. in terms of the average results, pso, as seen in table 3, has better results compared to ipo method in iris and wine data. in table 3, the proposed method shows better results for wine and cmc data with respect to the number of times used to reach a true cluster number. as the final analysis, we can assert that, in table 3, the ipo method has a slight change around the best results, and diversity is smaller than other algorithms, especially in cmc data."
"the dp approach has been widely used in literature for stereo matching and image processing applications [cit] . such applications often demand fast calculations in real-time, robustness against image discontinuities, and unambiguous matching."
in this section we represent a brief description on svr with gmm approach [cit] . speaker verification is the task of accepting or rejecting a tested speaker as a hypothetical speaker. let
"similarly, the cpu costs of a bjtree are the cumulative cost of processing tuples from each input stream in one time unit. in figure 4"
"neither mjoin nor bjtree are qualified plans, while a general jtree is found to be qualified. figures 8.e and f compare the accumulated throughput and memory consumptions of the three plans. bjtree has the highest initial throughput. however, it quickly runs out of memory at around 80,000ms. although jtree has a lower initial throughput than bjtree, it has a higher throughput than mjoin. since jtree requires less memory than bjtree, it is able to continuously produce results. in summary, the above results confirm our cost analysis, and demonstrate the need to extend the search space to also include general jtrees."
"we use directional cosines to represent relative orientations of the limbs within the kinematic tree [cit] . as a result, we represent a human body posture ψ using 37 parameters, that is,"
"in the above equations c 1, c 2, shift 1, shift 2, scale 1 and scale 2 are experimentally determined constants for each function [cit] . the pseudo code for ipo algorithm is illustrated in algorithm 1."
"2. wine: there are 178 instances in the wine dataset, characterized by 13 numeric features. the features are explained in the chemical analysis of three types of wine. there are also, three categories of data: 59 objects in class 1, 71 objects in class 2, and 48 objects in class 3 [cit] ."
"to better understand the entire solution space, we investigate all possible scenarios depicted in figure 5 where we vary the system capacities cp u avail and mem avail, and the cpu and memory utilization of popular join methods bjtree and mjoin. in figures 5 .a, 5.b and 5.c the estimated cpu and memory usages of bjtree and/or mjoin lie below their respective system constraints. in such situations the existing optimization techniques can be successfully applied. that is, in the case of figure 5 .a we can pick either the mjoin or the bjtree, as both are qualified. however, in figure 5 .b only the bjtree and in figure 5 .c only the mjoin solution is a qualified solution and therefore we choose accordingly. now consider the scenario in figure 5 .d where the cpu utilization of mjoin and the memory utilization of the bjtree are above their respective available system resources. we observe that in such scenarios where neither mjoin nor bjtree query plans are qualified, a qualified plan may nonetheless exist. this is achieved by exploiting the negative correlation that arises between cpu and memory utilization (as discussed in section 2.4). to elaborate, in figure 5 .d the cpu utilization of mjoin (cp u mjoin ) is above the system cpu threshold. while the memory utilization of mjoin (mem mjoin ) is well below the available system memory resources. in such scenarios we can sacrifice memory resources by storing some intermediate results and therefore saving cpu resources that would have otherwise been wasted for their re-computation. the resulting query plan is a jtree whose memory and cpu resource consumptions meet their respective resource constraints (as in figure 5 .d). as neither the pure mjoin operator nor a pure bjtree are viable at all times, we now introduce a generalization of a join plan that enables us to profit from this negative resource correlation."
"we introduce a median sequences or the best pattern for time synchronization, which is another contribution of this work. the median sequence is automatically selected from the training data following a minimum global distance criterion among other candidates of the same class."
"the nature of the open problems and techniques used in human motion analysis approaches strongly depends on the goal of the final application. hence, most approaches oriented to surveillance demand performing activity recognition tasks in real-time dealing with illumination changes and low-resolution images. thus, they require robust techniques with a low computational cost, and mostly, they tend to use simple models and fast algorithms to achieve effective segmentation and recognition tasks in real-time."
"in this work, we recast that continuous query plan generation, no longer a minimization problem as typically approached by state-of-the-art techniques, but rather as a constraint satisfaction problem. for this, we expand the traditional search space of pure mjoin-or pure bjtree-based query plans to include general jtrees. we explore the trade-offs and correlations between cpu and memory usage functions, both positive and negative. in this effort, we first present our jtree-finder algorithm that is guaranteed to generate a qualified plan. second, to provide an efficient run-time solution we present a two-layered plan generation framework. in our framework, we propose two polynomial-time algorithms state-selection and state-removal exploit the correlations to generate a qualified plan. our experimental evaluation using an existing continuous query engine verifies our cost model and measures the cpu and memory resource usages of the generated plans. the experimental results clearly demonstrate: 1) the need to search the entire solution space including general jtrees so not to miss potentially qualified plans and 2) the effectiveness of our algorithms over the state-of-the-art techniques."
"step 3: second, jtree-finder makes use of the two-dimensional cost model described in section 2 which calculates both cpu and memory resource utilizations. while traditional optimizers [cit] instead only use the one-dimensional cpu or io processing costs. the dual-constraints cost model enables us to prune sub-plans whose memory or cpu usage is greater than their respective system thresholds at an early stage."
"in this section, we analyze the cpu and memory costs for the state-of-the-art methods of implementing multi-join continuous queries, namely mjoin and bjtree. this comprehensive analysis reveals the conditions under which cpu and memory usage have a positive or a negative correlation."
"we analyzed 148 protein datasets, which were chosen because they included very few gaps and structural data for at least one of the extant sequences were available. [cit] . for each dataset, the phylogenetic tree was inferred using phyml-structure with the ex2 model and the conf/ mix mode with no among site rate variation [cit] . notably, the ex2 model is identical to the be model described below. for each tree topology, the branch lengths were optimized under the maximum-likelihood criterion, under the specified model. the cþþ code for branch length optimization under each of these models was added to the fastml program [cit] ."
"for each discrete rate r, the algorithm is the same as the algorithm described above except that here, the 'up', 'down' and 'marginal' components are calculated n times (once for each rate category) separately. in each such a computation, instead of using the branch length t, the branch length used is t á r. the total marginal probabilities pðejaþp a for a specific node v are computed by"
"it is similar to the p(e) calculation shown above, but here we multiply each branch length by the rate r. the unconditional probability will therefore be:"
"the evolutionary rate at a specific site in a protein-coding gene dictates the number of substitutions that this site experiences along its evolution. while early modeling approaches to sequence evolution assumed, for simplicity, that all sites evolve at the same rate, this was shown not to be the case more than 50 years ago [cit] . rate variation among sites is affected by several factors, among them are functional and structural constraints [cit] . currently, the gamma distribution is most commonly used to model among site rate variation [cit] ."
"a variety of software for asr exist [cit] . the ever-growing sequence data and the interest in accurate asr algorithms pose new challenges for such asr tools. for example, indels were initially treated as unknown characters, which led to ancestral sequences that are longer than all extant sequences. in fastml, we thus reconstruct indel presence/absence in each node prior to sequence reconstruction [cit] . in this work, we aimed to further improve asr methodologies by allowing the sequences to evolve according to multiple replacement matrices. specifically we have shown that fitting a replacement matrix to each position based on structural information can be highly beneficial for asr."
"inferring ancestral sequences can be challenging. while it is possible to use molecular paleontology, i.e. the extraction and recovery of dna information from fossils, this method still has many obstacles to pass before it can be widely used [cit] . thus, a method called ancestral sequence reconstruction (asr) was developed and so far, it is the best way to deduce the origins of modern proteins [cit] . in asr, the ancestral sequences are inferred by using the extant sequences, a phylogenetic tree and a model of sequence evolution [cit] ."
"using felsenstein's dynamic programming algorithm [cit], the posterior probabilities at the tree root (and thus the most likely ancestor) can be computed in o(n) where n is the number of sequences. we can re-root the tree in each possible node and repeat the above computation, leading to an o(n 2 ) algorithm to find the ancestral sequences at all internal nodes. in this study, we describe a more efficient dynamic programming algorithm designed to find the ancestors in all the nodes simultaneously in o(n). similar dynamic algorithms were previously utilized by us for the task of maximum-likelihood tree inference using expectation maximization [cit] ."
"clearly, the scheduler either assigns the subsequent slot to node r with probability 0.5γ a or assigns it to node a with the complementary probability 1−0.5γ a . after some derivations and accounting for the above propositions, we establish:"
"even though the be model had higher log-likelihoods than lg, it is plausible that the differences in the log-likelihoods are negligible. we thus next compared the log-likelihood differences between all models and lg (fig. 4) . while the other models scored mostly lower than lg (negative difference) the be model scored substantially higher in most datasets. of note, log-likelihood differences of fig. 3 . comparison of all models to the lg model. the new be model with the structural data scored higher than lg in over 90% of the datasets tested. for each model the y axis shows the percentage of datasets which had a higher log-likelihood score compared to the lg model 10 points or higher are considered highly significant in such cases as the one considered here, in which there is no difference in the number of free parameters between the compared models."
"the relay improves the throughput of the originator by sacrificing its own energy efficiency. extra energy is spent by the relay on the eavesdropping, as well as on the simultaneous packet transmissions with the originator. to save some of its energy, the relay may act opportunistically. as such, in each time slot the relay may decide not to eavesdrop on the transmissions from the originator with probability 1 − p rx and/or not to relay a packet with probability 1 − p tx . the probabilities p rx and p tx correspond to a particular client relay policy and may be used to trade overall system throughput for total energy expenditure."
"we divide the calculations into three parts: a post-order tree traversal we call 'up', a pre-order traversal we call 'down' and another tree traversal (for which the order is not important) we call 'marginal'. [cit] to calculate tree likelihoods."
"in the above analysis, all tree topologies were estimated using the be model. it was previously shown that the tree topology may vary depending whether the be or lg model is used [cit] . to verify that the superiority of the be model over lg does not stem from the fact that the be model was used to reconstruct the tree topologies, we repeated the above analysis, this time when all tree topologies were reconstructed using the lg model as implemented in phyml 3.0 [cit] . the superiority of the be model remains even when lg is used to reconstruct tree topologies ( supplementary fig. s1) ."
"to test the be model in cases where the 3d structure was unavailable for any of the extant sequences, we analyzed the 148 datasets as above, but this time, their solvent accessibility was predicted from the consensus sequence using sable [cit] . we compared the log-likelihood results to those obtained using the single-matrix models (fig. 5) . similar to the case in which the structural data are known, the be model obtained substantially higher log-likelihood scores compared to the single-matrix model. interestingly, in 84 out of the 148 analyzed protein datasets, the be model with predicted solvent accessibility had slightly higher loglikelihood score than the be model for which the solvent accessibility was retrieved from the 3d structure."
"models that account for among sites rate variations and assume a single replacement matrix across all sites are still an oversimplification of the evolutionary dynamics. specifically, such models ignore the biological intuition and knowledge that sites in a protein are subjected to different evolutionary constraints affected by differences in biochemistry and structure. amino-acid replacement propensities substantially vary among different structural parts of the protein and mainly due to solvent accessibility [cit] ."
the above expression accounts for the fact that out of n slots spent to serve a packet from node a the last slot was assigned to node a and its transmission in this slot was successful. the previous n − 1 slots were either not assigned to node a or its transmissions in these slots were unsuccessful.
"the proliferation of wireless networks introduces novel important research directions, including client cooperation, energy efficient communication, multiradio co-existence, spectrum aggregation techniques, and others. these directions are insufficiently addressed by the conventional simulation methodology and existing analytical models, which only cover static or semi-static cellular environments [cit] . in this paper, we developed a tractable dynamic model that"
"of note, the matrices and their associated weights can be estimated directly from the data analyzed using maximum-likelihood. however, the matrices and weights can be also obtained based on data external to sequences being analyzed. here, we apply this latter case, where we use pre-computed matrices for buried and exposed protein regions and the weights are computed based on the protein solvent accessibility values (see below). note, that using precomputed empirical amino-acid matrices is the standard in the field of phylogenomics, e.g. when analyzing with the lg, wag or jtt matrices. as these matrices and weights are not directly estimated from the sequence data, they are not considered as free parameters when comparing different models in model-selection procedures. [cit] . thus, test data and the training data used to estimate the buried and exposed matrices are truly disjoint. this provides further justification for not considering these matrices as additional free parameters."
"we next tested the extent of differences in reconstructed sequences when comparing the lg and mix1 models. position differ between models, if the models do not agree for the position in at least one internal node. out of all protein datasets analyzed, differences were observed in 98% (145/148). the distribution of the number of positions that differ in at least one internal node between the lg and mix1 models out of the total length is shown in figure 7 . as can be seen, for some datasets, the fraction of affected positions was higher than 30%."
"the single matrix lg model [cit] was chosen as a baseline to compare the performance of the be model to the simpler models that assume a single amino-acid replacement matrix for all sites. performance was estimated using the log-likelihood score for the most likely marginal reconstruction at the root of each dataset. while all models which comprise of a single replacement matrix scored better than lg in less than 20% of the datasets, the be model scored higher than lg in more than 90% of the datasets tested (fig. 3) ."
"accounting for τ * a0, the resulting approximation for the mean packet delay δ * r of node r, as well as expressions for the throughput η * a and η * r of nodes a and r in the system with cooperation are similar to the respective metrics in the system without cooperation from the previous subsection."
"we predicted the ancestral sequences of all datasets using five protein models for nuclear encoded proteins and three structure-aware models, which account for the solvent accessibility of each protein site (listed in fig. 3) . briefly, the models assign different weights to two replacement matrices: 'buried' (b) and 'exposed' (e). the simplest of these models is the be model, which assigns a 'buried' replacement matrix to buried positions and an 'exposed' matrix to exposed positions (see materials and methods section). the be model was first applied using solvent accessibility data extracted from the three-dimensional (3d) structure of each analyzed protein."
"the performance metrics of node r may be calculated analogously, due to the symmetric nature of the respective direct links. additionally, we may obtain the exact value of the mean energy expenditure of e.g. node a as:"
"secondly, we study the behavior of node a within the framework of the queueing theory. due to the fact that the queues of nodes a and r are mutually dependent, the notorious pollazek-khinchine formula may not be used to obtain the exact mean queue length of node a. we, however, apply this formula to establish the approximate value of the mean queue length of node a as:"
"the above be model classifies each position as either exposed or buried based on a strict 10% solvent accessibility cutoff. however, positions that are 20% solvent-exposed are expected to experience different selective constraints compared to positions which are 50% solvent-exposed. it is also expected that some of the positions that have 20% solvent accessibility have accumulated amino-acid replacements similar to buried positions, while others, similar to exposed positions. we thus next tested whether accounting for such uncertainty in classifying positions to either 'buried' or 'exposed' can benefit asr. specifically, we tested two mixing variants, mix1 and mix2, for which positions with intermediate solvent accessibility are modeled according to equations (12)-(15), i.e. the probability of each ancestral character in these positions is a weighted average over the two matrices b and e (see materials and methods section) . when comparing the log-likelihood score to the be model, a clear significant improvement for most datasets was observed ( fig. 6 ). among these two models, mix1 had a higher average loglikelihood than mix2, but the differences were insignificant."
"in a recent study that benchmarked various asr methodologies, fastml achieved one of the best scores [cit] . unfortunately, the experiment resulted in relatively easy to reconstruct sequences, and differences among asr methodologies were minimal. nevertheless, that research motivates the development of improved experimental benchmarks for asr."
"in this paper, we extend our earlier client relay research [cit] to a more practical scenario enabling opportunistic cooperation and include the relevant control parameters into consideration. as such, the relay may balance its extra energy expenditure and cooperative benefits for the network by reasonably choosing a client relay policy. in what follows, we detail the system model and analytically establish the primary performance metrics of cooperative networking, such as throughput, mean packet delay, energy expenditure, and energy efficiency. finally, we conclude with some guidelines on choosing the client relay strategy."
"where a 1 and a 2 are the amino acids assigned to nodes a 1 and a 2, respectively. the first step in estimating ancestral sequences is to compute the posterior probability of each character in each of the tree nodes. these posterior probabilities are computed using bayes theorem. in the above example, the posterior probability of each character in the root node is computed according to following equation:"
"a and q (t) r respectively. as we observe the client relay system in stationary conditions, we omit the upper index t of variables q"
"in this section, we briefly refresh the basic client relay system model from our previous work [cit] and then extend it to the more realistic opportunistic scenario with non-mandatory reception and transmission of relay packets. for the sake of analytical tractability, we study the simplest but practical network topology (see figure 1 ) and summarize our main assumptions below."
"as we show here, using an array of replacement matrices can be beneficial when taking structural data into account. such an approach should also be advantageous for studying proteins that contain both trans-membranal and cytosolic domains or to analyze separately different secondary structures. protein engineering also uses asr to generate proteins that are more stable than extant proteins and to increase the substrate range of engineered proteins. it is expected that the approach suggested here in which structural information is integrated into the asr computations should lead to improved engineered proteins."
"finally, in figure 5 we address the maximum throughput gain of the cooperative system by varying p rx across its feasible range in saturation. we plot the overall throughput of both nodes a and r to conclude that client relay technique may considerably improve the performance of cellular wireless networks."
"our approach to integrate structural information implicitly assumes that both buried, and solvent-exposed positions remain so along the entire course of evolution. in cases where the phylogenetic tree is large and contains dramatic structural changes this assumption might be violated. in addition, the extent to which the protein structure is accounted for in this work is limited to buried and exposed information. more sophisticated models that integrate sitespecific structural attributes with amino acid replacement propensities are expected to provide even more accurate estimates of ancestral sequences [cit] and references therein). fig. 7 . percentage of positions that differ between the lg and mix1 models. for each of the 148 datasets, we computed the percentage of positions that differ between the two models. shown is the distribution of these percentages"
"solvent accessibility was extracted from structural data using dssp [cit] . absolute solvent accessibility values were normalized using maximum solvent accessibility values calculated empirically for each amino acid [cit] . when structural data were unavailable (or to simulate such cases), solvent accessibility was predicted using sable [cit] . predictions were performed on a consensus sequence that contained for each position, the most common character. positions were dichotomized to either 'buried' or 'exposed' using a 10% relative solvent accessibility as threshold [cit] . it is possible that even when the structural data were available, the solvent accessibility of some positions within the multiple sequence alignment was ambiguous, e.g. due to the introduction of short insertions. in such cases, these positions were assigned 50% weight buried and 50% weight exposed."
"understanding how genes and genomes evolve is a major goal in molecular evolution. ancestral sequences can help elucidate molecular pathways that evolved millions to billions of years ago [cit] . other than the valuable evolutionary knowledge gained from these sequences, ancestral proteins may contain desirable properties that modern proteins lack, such as broader substrate range and higher thermostability. therefore, they can be used as a good starting point for protein engineering [cit] ."
"studying the system with opportunistic cooperation, we firstly consider an important special case when the queue at node r is always empty. we establish the distribution of the number of slots required to serve a packet from node a. by using the obtained distribution, we then generalize the proposed approach for the case of non-empty queue at node r. all the respective performance metrics for the system with opportunistic cooperation are marked by symbol ' * ' in the rest of the text."
"it is similar to p(ejr) shown above, but here we use the replacement probabilities and stationary probabilities based on a specific m matrix. the unconditional probability will be:"
"for each replacement matrix m the algorithm is the one calculated for the single replacement matrix with discrete rate variation. it means that for every matrix in the model array, both 'up', 'down' and 'marginal' are calculated for all rates. the total marginal probabilities for a specific node v are computed by:"
"pharmmapper predicted molecule 3 as a large ribosomal subunit inhibitor, whereas it has been shown that molecule 3 works by activating transient receptor potential vanilloid (trpv) channels in insect chordotonal organs [cit] ."
"the target data of molecules 1, 2, 3, 4, and 6 and molecules with a structural similarity of 85% and more to molecules 1, 2, 3, 4, and 6 for the sterol 14α-demethylase inhibition were not found in chembl database, but based on the obtained information, it is unlikely that the target of these molecules is sterol 14α-demethylase inhibition. employing the minimum structure in imidazol-1-yl group for identifying a bioactive molecule with the target of sterol 14α-demethylase inhibition has been shown in figure s8 ."
"pass online and pharmmapper predicted molecule 6 as a large ribosomal subunit inhibitor, whereas molecule 6 (p-chlorophenyl-α-glycerol ether) has been recommended as an antibacterial and antifungal agent of pharmaceutical interest [cit] . the glycerol ethers (e.g., molecule 6) are nonirritant but possess the weakest antimicrobial action [cit] . the antimicrobial activity increased with the number of substituents on the benzene nucleus and, to a certain extent, was a function of the position of substitution [cit] ."
"considering that membership function of a given fuzzy set is possibly changeable dealing with time variable, [cit] introduced an extended concept of fuzzy sets, called dynamic fuzzy sets as the following definition."
"the primary target information on 15 bioactive molecules was collected from scientific literature ( table s2) . based on the proposed method, the primary target can be identified for all 15 bioactive molecules with a known primary target. in addition, the primary target can be predicted for two bioactive molecules with an unknown primary target by the use of the proposed method. generally, 3-glutarimidyl group contains 17 bioactive molecules with a predictable primary target on the basis of the proposed method (table 3) ."
"pass online predicted molecule 5 as a small ribosomal subunit inhibitor, whereas based on the studies carried out using blocked mutants, the first stable oxytetracycline intermediate is likely the fully aromatized tetracyclic compound called molecule 5, which then undergoes further processing and tailoring reactions of the fully formed tetracycline backbone [cit] . pass online predicted molecule 6 as a small ribosomal subunit inhibitor, whereas molecule 6 is a neuropeptide y receptor antagonist [cit] . due to a lack of key pharmacophores of tetracycline antibiotics, molecule 6 showed no antimicrobial activity against staphylococcus aureus, bacillus subtilis, micrococcus luteus, escherichia coli, and saccharomyces cerevisiae at a concentration of 100 µg/ml [cit] ."
"pass online, pharmmapper, targethunter, and chemprot applied to 64 bioactive molecules with a known primary target led to make accurate predictions on 64 (100%), 51 (79.7%), 41 (64.1%), and 16 (25%) bioactive molecules, respectively (table s5) . also, pass online, pharmmapper, targethunter, and chemprot applied to 74 bioactive molecules with an unknown primary target led to make accurate predictions on 74 (100%), 49 (66.2%), 46 (62.2%), and 10 (13.5%) bioactive molecules, respectively (table s5 ). since the primary target information on the 2,4(or 5)-diaminocyclohexanol group is not available in ppb, sea, hitpick, spider, and superpred, these tools are not able to predict the primary target of bioactive molecules of this group."
"pass online predicted molecule 3 and molecule 4 as small ribosomal subunit inhibitors, whereas the most prominent characteristic of chemically modified tetracycline analogs (e.g., molecule 3 and molecule 4) is their loss of antibacterial activity, accompanied by retention (or even enhancement) of their efficacy as inhibitors of mammal-derived matrix metalloproteinases [cit] ."
"based on the proposed method, the primary target can be identified for all 64 bioactive molecules with a known primary target. in addition, the primary target can be predicted for 74 bioactive molecules with an unknown primary target by the use of the proposed method. generally, 2,4(or 5)-diaminocyclohexanol group contains 138 bioactive molecules with a predictable primary target on the basis of the proposed method ( table 2) ."
"pharmmapper predicted molecule 3 as a large ribosomal subunit inhibitor, whereas molecule 3 is a tetralone-fused spiroglutarimide derivative mainly known for sedative and hypnotic activity [cit] ."
"pass online, pharmmapper, and chemprot applied to 25 bioactive molecules with a known primary target led to make accurate predictions on 25 (100%), 14 (56%), and 0 (0%) bioactive molecules, respectively (table s5) . also, pass online, pharmmapper, and chemprot applied to three bioactive molecules with an unknown primary target led to make accurate predictions on 3 (100%), 0 (0%), and 0 (0%) bioactive molecules, respectively (table s5) . since the primary target information on the cytosine group is not available in ppb, sea, targethunter, hitpick, spider, and superpred, these tools are not able to predict the primary target of bioactive molecules of this group."
"the target information is collected for all structurally related bioactive molecules. if the target of the query molecule is not known, information on the structure-activity relationship (the consistent correlation of structural features or groups with the biological activity of molecules in a given biological assay; [cit] ) and the pharmacophore (the spatial orientation of various functional groups or features necessary for activity at a biomolecular target; [cit] ) will be collected for all structurally related bioactive molecules. if the target of the query molecule is known, information on the structure-activity relationship and the pharmacophore will be collected only for structurally related bioactive molecules with the same target as the query molecule. information on the target, the structure-activity relationship, and the pharmacophore are obtained from databases with the annotated target (e.g., kegg, pubchem, drugbank, and chembl databases), scientific literature, and pharmacophoric descriptors (including hydrogen bonds as well as hydrophobic and electrostatic interaction sites; [cit] ) . the proposed method criteria for allocation of target-bioactive molecule interactions are not limited to cellbased and/or in vivo evidence, and binding data are not necessary to find out interactions."
"pass online predicted molecule 5 as a large ribosomal subunit inhibitor, whereas molecule 5 binds at a distinct binding site associated with a cl − ionopore at the gaba a receptor, increasing the duration of time for which the cl − ionopore is open [cit] ."
"pass online and sea predicted molecule 1 as a large ribosomal subunit inhibitor, whereas molecule 1 (an amino acid) is an important intermediate in many syntheses [cit] . amino acids are extensively used in the synthesis of several products used in chemical, pharmaceutical, food, and health industries [cit] ) ."
"pass online and sea predicted molecule 4 as a sterol 14α-demethylase inhibitor, whereas molecule 4 is a potent, highly selective 5-ht 1b/1d receptor agonist with rapid onset of action for acute treatment of migraine [cit] ."
"based on the proposed method, the primary target can be identified for all 10 bioactive molecules with a known primary target (table 3) . pass online, pharmmapper, and sea applied to 10 bioactive molecules with a known primary target led to make accurate predictions on 10 (100%), 9 (90%), and 0 (0%) bioactive molecules, respectively (table s5) . since the primary target information on the (1r)-propanol group is not available in ppb, chemprot, targethunter, hitpick, spider, and superpred, these tools are not able to predict the primary target of bioactive molecules of this group."
"the target data of molecules 1, 2, 3, 5, and 6 and molecules with a structural similarity of 85% and more to molecules 1, 2, 3, 5, and 6 for the large ribosomal subunit inhibition were not found in chembl database, but based on the obtained information, it is unlikely that the target of these molecules is large ribosomal subunit inhibition. employing the minimum structure in (1r)-propanol group for identifying a bioactive molecule with the target of large ribosomal subunit inhibition has been shown in figure s7 ."
"for example, six molecules of figure s7 were used to predict the large ribosomal subunit inhibition by the proposed method and the ctpts. only molecule 4 in figure s7 is a large ribosomal subunit inhibitor, based on the proposed method. among the ctpts, pass online and pharmmapper predicted molecule 4 as a large ribosomal subunit inhibitor. thiamphenicol and molecule 4 were shown to be as chloramphenicol in inhibiting peptidyl transferase activity specifically on 70s ribosomes [cit] ."
"here, the primary target information on 381 bioactive molecules was collected from scientific literature ( table s2 ) and databases ( table s3 ). the databases include kegg [cit], pubchem [cit], drugbank [cit], and chembl [cit] . of these 381 bioactive molecules, scientific literature and databases contain the primary target information on 372 (97.6%) and 160 (42%) bioactive molecules, respectively. here, a part of primary targets of bioactive molecules has been extracted from databases. however, the primary target information has not been collected from annotated targets in bioactivity assays and target predictions of databases, because in those parts, primary targets are not distinguished from other targets. three hundred eightyone bioactive molecules with known primary targets were used for the identification of eight minimum structures. then, these eight minimum structures were employed to predict primary targets of 169 bioactive molecules with unknown primary targets. finally, predictions made by the proposed method (tables 1-4) were compared with those 381 bioactive molecules with known primary targets and 169 bioactive molecules with unknown primary targets in nine ctpts ( table s5 ). the nine ctpts include pass online [cit], ppb [cit], sea [cit], targethunter [cit] ), pharmmapper [cit], chemprot [cit], hitpick [cit], superpred [cit], and spider [cit] ."
"based on the proposed method, the primary target can be identified for all 42 bioactive molecules with a known primary target. in addition, the primary target can be predicted for 12 bioactive molecules with an unknown primary target by the use of the proposed method. generally, imidazol-1-yl group contains 54 bioactive molecules with a predictable primary target on the basis of the proposed method (table 4) ."
"pass online, sea, ppb, targethunter, chemprot, superpred, and hitpick applied to 42 bioactive molecules with a known primary target led to make accurate predictions on 42 (100%), 37 (88.1%), 26 (62%), 22 (52.4%), 18 (42.9%), 17 (40.5%), and 16 (38.1%) bioactive molecules, respectively (table s5) . also, pass online, sea, ppb, targethunter, hitpick, superpred, and chemprot applied to 12 bioactive molecules with an unknown primary target led to make accurate predictions on 12 (100%), 11 (91.7%), 11 (91.7%), 8 (66.7%), 4 (33.3%), 3 (25%), and 1 (8.3%) bioactive molecules, respectively (table s5) . since the primary target information on the imidazol-1-yl group is not available in pharmmapper and spider, these tools are not able to predict the primary target of bioactive molecules of this group."
"the target data of molecules 1, 3, 4, 5, and 6 and molecules with a structural similarity of 85% and more to molecules 1, 3, 4, 5, and 6 for the sterol 14α-demethylase inhibition were not found in chembl database, but based on the obtained information, it is unlikely that the target of these molecules is sterol 14α-demethylase inhibition. employing the minimum structure in 1,2,4-triazol-1-yl group for identifying a bioactive molecule with the target of sterol 14α-demethylase inhibition has been shown in figure s9 ."
"it is well known that drugs and pesticides interact with multiple targets rather than with a single target (called the offtarget effect) [cit], and this fact can be beneficial [cit] or harmful (known as side effect or toxicity) [cit] . for instance, a recent study on a set of 802 drugs and drug interactions data assembled from seven different databases has shown that known drugs have on average six molecular targets on which they exhibit activity [cit] . also, because one target may have thousands of structurally diverse ligands, one unique model may not recover all features, and the prediction performance may not be satisfying [cit] ) . hence, a minimum structure is identified for each group of neighbor molecules in the proposed method. each group of neighbor molecules represents a distinct structural class of molecules with the same function in relation to the target. the proposed method is not applicable in cases where no neighbor bioactive molecules for a target exist, since in these situations no training on the minimum structure-based information is possible."
"pass online and pharmmapper applied to 27 bioactive molecules with a known primary target led to make accurate predictions on 27 (100%) and 20 (74.1%) bioactive molecules, respectively (table s5) . also, pass online and pharmmapper applied to seven bioactive molecules with an unknown primary target led to make accurate predictions on 7 (100%) and 3 (42.9%) bioactive molecules, respectively (table s5) . since the primary target information on the (4ars,5ars)-sancycline group is not available in ppb, sea, targethunter, chemprot, hitpick, spider, and superpred, these tools are not able to predict the primary target of bioactive molecules of this group."
"the primary target information on 27 bioactive molecules was collected from scientific literature ( table s2 ) and databases (table s2) . of these 27 bioactive molecules, scientific literature and databases contain the primary target information on 26 (96.3%) and 15 (55.6%) bioactive molecules, respectively."
"it has possibly happened, a membership function of a fuzzy set given by certain knowledge is changeable over time. thus, it is necessary to propose a new concept, called knowledge-based dynamic fuzzy sets (kdfs) as a hybrid concept of dfs and kfs. here the kdfs may also be considered as a concept of two-dimensional fuzzy multisets. three kinds of summary fuzzy sets are proposed and discussed. they are the knowledge-based summary fuzzy sets, the time-based summary fuzzy sets and the gen-eral summary fuzzy sets. some basic operations of kdfs such as equality, contentment, union, intersection and complement are defined. their properties are verified and examined."
"pass online predicted molecule 4 as a sterol 14α-demethylase inhibitor, whereas it is now clear that farnesyl pyrophosphate synthase is a major site of action of the nitrogen-containing bisphosphonates (e.g., molecule 4) [cit] ."
"bioactive molecules such as drugs and pesticides are produced in large numbers by many commercial and academic groups around the world [cit] . most bioactive molecules perform their actions by interacting with proteins or other macromolecules [cit] . however, for a significant fraction of bioactive molecules, targets remain unknown [cit] . moreover, even for well-studied molecules, our knowledge of their targets is far from complete [cit] ."
"here, depending on the context of applications, υ and φ may use any existed functions such as maximum, minimum, average, etc. by taking the maximum and the minimum functions as the maximum and the minimum values of both functions, respectively. eq. (5) is exactly similar to the knowledge-based summary fuzzy set as discussed by intan and mukaidono [cit] . practically, the knowledge-based summary fuzzy set of as formulated in (5) means an agreement given by a group of persons represented by a set of knowledge to describe at the time . similarly, eq. (4) may be regarded to provide the time-based summary fuzzy set. for it is sometimes happened in reality, subjective opinion of someone to a given fuzzy set may be changeable according to the changing of times, the objective of (4) is to summarize the multiple opinions of a certain knowledge to the fuzzy set . therefore, related to the reason behind calculating both summary fuzzy sets as given in (4) and (5), it may be more applicable to use the weighted average as the aggregation function as shown in the following equations."
"based on the proposed method, the primary target can be identified for all 25 bioactive molecules with a known primary target. in addition, the primary target can be predicted for three bioactive molecules with an unknown primary target by the use of the proposed method. generally, cytosine group contains 28 bioactive molecules with a predictable primary target on the basis of the proposed method (table 3) ."
"for example, six molecules of figure s6 were used to predict the large ribosomal subunit inhibition by the proposed method and the ctpts. only molecule 6 in figure s6 is a large ribosomal subunit inhibitor, based on the proposed method. among the ctpts, sea, pass online, ppb, targethunter, and chemprot predicted molecule 6 as a large ribosomal subunit inhibitor. [cit] showed molecule 6 as an inhibitor of 60s ribosomal subunit (large ribosomal subunit) in resistant and sensitive strains of saccharomyces."
"we evaluate our predictive performance by applying it to 550 drugs and pesticides with a known or an unknown target and comparing results to those from the ctpts. these drugs and pesticides include fungicides and bactericides that may be used for medical, veterinary, and agricultural applications. common name, cas registry number, inchikey, inchi, smiles, primary target, chemical structure, and pharmacophore for 550 bioactive molecules of the present study have been shown in table s1 ."
"the target data of molecules 1, 3, 4, 5, and 6 and molecules with a structural similarity of 85% and more to molecules 1, 3, 4, 5, and 6 for the large ribosomal subunit inhibition were not found in chembl database, but based on the obtained information, it is unlikely that the target of these molecules is large ribosomal subunit inhibition. employing the minimum structure in cytosine group for identifying a bioactive molecule with the target of large ribosomal subunit inhibition has been shown in figure s5 ."
"for example, six molecules of figure s9 were used to predict the sterol 14α-demethylase inhibition by the proposed method and the ctpts. only molecule 2 in figure s9 is a sterol 14α-demethylase inhibitor, based on the proposed method. among the ctpts, pass online predicted molecule 2 as a sterol 14α-demethylase inhibitor. molecule 2 is known to bind and inhibit fungal sterol 14α-demethylase, a cytochrome p450 enzyme found in plants, animals, fungi, and mycobacteria [cit] ."
"the target data of molecules 1, 2, 4, 5, and 6 and molecules with a structural similarity of 85% and more to molecules 1, 2, 4, 5, and 6 for the dna gyrase and topoisomerase iv inhibition were not found in chembl database, but based on the obtained information, it is unlikely that the target of these molecules is dna gyrase and topoisomerase iv inhibition. employing the minimum structure in 4-pyridone group for identifying a bioactive molecule with the target of dna gyrase and topoisomerase iv inhibition has been shown in figure s2 ."
"pass online and pharmmapper predicted molecule 4 and molecule 6 as small ribosomal subunit inhibitors, whereas molecule 4 and molecule 6 are trehalase inhibitors [cit] ."
"ppb predicted molecule 6 as a dna gyrase inhibitor, whereas molecule 6 is a highly selective hiv-1 integrase inhibitor with a potent antiviral activity against both b and non-b subtypes of hiv-1 [cit] . ppb, chemprot, and targethunter predicted molecule 2 as a dna gyrase inhibitor, whereas molecule 2 (particularly as its potassium salt) is a plant growth regulator that is used as a chemical hybridization agent for commercial hybrid seed production [cit] ."
"the target data of molecules 2, 3, 4, 5, and 6 and molecules with a structural similarity of 85% and more to molecules 2, 3, 4, 5, and 6 for the small ribosomal subunit inhibition were not found in chembl database, but based on the obtained information, it is unlikely that the target of these molecules is small ribosomal subunit inhibition. employing the minimum structure in (4ars,5ars)-sancycline group for identifying a bioactive molecule with the target of small ribosomal subunit inhibition has been shown in figure s4 ."
"pass online predicted molecule 1 as a small ribosomal subunit inhibitor, whereas molecule 1 binds to large ribosomal subunit and inhibits its peptidyl transferase activity [cit] ."
"based on the proposed method, the primary target can be identified for all 27 bioactive molecules with a known primary target. in addition, the primary target can be predicted for seven bioactive molecules with an unknown primary target by the use of the proposed method. generally, (4ars,5ars)-sancycline group contains 34 bioactive molecules with a predictable primary target on the basis of the proposed method ( table 2) ."
"the primary target information on 42 bioactive molecules was collected from scientific literature ( table s2 ) and databases (table s3) . of these 42 bioactive molecules, scientific literature and databases contain the primary target information on 38 (70.4%) and 30 (55.6%) bioactive molecules, respectively."
"the whole list of predicted targets with any statistical significance, including high confidence targets (e.g., low p-value, low target rank, high probability of being active, or low probability of being inactive) and low confidence targets (e.g., high p-value, high target rank, low probability of being active, or high probability of being inactive), was considered to obtain the maximum prediction potential in the ctpts. also, since the ctpts are not able to distinguish between a primary target and other targets, the primary target found in the target list predicted by the ctpt was considered as an accurate prediction. predictive results of the proposed method and the ctpts are presented in eight groups."
"ppb predicted molecule 5 as a dna gyrase inhibitor, whereas preliminary clinical studies suggest that molecule 5 has marked diuretic and natriuretic activities [cit] ."
"pass online predicted molecule 1 as a large ribosomal subunit inhibitor, whereas molecule 1 is a pyrimidine analogue that has an activity against fungal species by interfering with purine and pyrimidine uptake and deaminating to 5-fluorouracil and then converting to 5-fluorodeoxyuridylic acid monophosphate, a noncompetitive inhibitor of thymidylate synthetase that interferes with dna synthesis [cit] ."
"fifty-one molecules without target data and 111 molecules not found for predictions of the proposed method in chembl database were used to search for molecules with a structural similarity of 85% and more in chembl database. results of bioactivity assays and target predictions of structural similarity molecules from chembl database were applied to evaluate new target predictions of the proposed method in 51 molecules without target data and 111 molecules not found in chembl database. activity values of structural similarity molecules in chembl database found with predicted targets of the proposed method were reported 56.45% as moderate activity (613 activities), 22% as potent activity (239 activities), 11.33% as weak activity (123 activities), and 10.22% as inactive (111 activities) ( table s4) ."
"for example, six molecules of figure s3 were used to predict the small ribosomal subunit inhibition by the proposed method and the ctpts. only molecule 2 in figure s3 is a small ribosomal subunit inhibitor, based on the proposed method. among the ctpts, pass online and pharmmapper predicted molecule 2 as a small ribosomal subunit inhibitor. molecule 2 is a 30s ribosomal subunit (small ribosomal subunit) inhibitor, as shown by experiments on reconstituted 70s ribosomes containing subunits from sensitive and from resistant ribosomes [cit] ."
"pass online and sea predicted molecule 6 as a sterol 14α-demethylase inhibitor, whereas it has been suggested that molecule 6 is a selective thromboxane synthase inhibitor [cit] . oky (development code) is a group of selective thromboxane a 2 biosynthesis inhibitors that have been collaboratively developed by the two companies [cit] . as a result of screening of oky derivatives synthesized by the two companies, molecule 6 was selected [cit] ."
"if groups that do not have the primary target information or the neighbor molecule in the ctpts are removed from results of the ctpts, then we will have predictions of the ctpts as follows: a) prediction results of pass online, sea, ppb, chemprot, targethunter, pharmmapper, hitpick, superpred, and spider on bioactive molecules with known primary targets led to make accurate predictions on 99.2%, 91.7%, 85.9%, 53.8%, 53.6%, 50.7%, 27.6%, 17.5%, and 13.3% bioactive molecules, respectively. b) prediction results of pass online, ppb, sea, targethunter, pharmmapper, chemprot, hitpick, superpred, and spider on bioactive molecules with unknown primary targets led to make accurate predictions on 99.4%, 95.3%, 92.9%, 47.2%, 46.2%, 37.7%, 25%, 6%, and 0% bioactive molecules, respectively. among the ctpts, pass online, sea, and ppb had the most accurate predictions. unlike the other six ctpts, the accuracies of predictions on bioactive molecules with unknown primary targets in ppb, sea, and pass online were respectively 9.4%, 1.2%, and 0.2% higher than those on bioactive molecules with known primary targets. this means that ppb, sea, and pass online can predict unseen interactions between bioactive molecules and potential targets better than other six ctpts."
"pass online, sea, ppb, chemprot, targethunter, pharmmapper, and superpred applied to 135 bioactive molecules with a known (table s5) . since the primary target information on the 4-pyridone group is not available in hitpick and spider, these tools are not able to predict the primary target of bioactive molecules of this group. for example, six molecules of figure s2 were used to predict the dna gyrase and topoisomerase iv inhibition by the proposed method and the ctpts. only molecule 3 in figure s2 is a dna gyrase and topoisomerase iv inhibitor, based on the proposed method. among the ctpts, pass online, ppb, chemprot, and targethunter predicted molecule 3 as a dna gyrase and topoisomerase iv inhibitor. [cit] proved the target of molecule 3 as a dna gyrase inhibitor by applying molecule 3 in escherichia coli and staphylococcus aureus."
"the proposed method steps for target prediction of bioactive molecules from chemical structures include i) query molecule, ii) similarity searching, iii) data collection, iv) minimum structure identification, and v) target prediction. the proposed method process of target prediction from chemical structures can be found in a hypothetical example with a simple expression in figure s1 ."
"for example, six molecules of figure s5 were used to predict the large ribosomal subunit inhibition by the proposed method and the ctpts. only molecule 2 in figure s5 is a large ribosomal subunit inhibitor, based on the proposed method. among the ctpts, pass online and pharmmapper predicted molecule 2 as a large ribosomal subunit inhibitor. the target of molecule 2 was investigated and results suggest that the molecule blocks the peptidyl transferase center (large ribosomal subunit) [cit] ."
"target-identification and mechanism-of-action studies have important roles in bioactive molecule probe and drug and pesticide discovery [cit] . experimental and computational approaches are used to predict biological targets that interact with bioactive molecules. experimental approaches are usually more costly and slower than computational approaches. computational approaches make predictions based on models with several approximations [cit] . the common drawbacks of these models are that the real predictability beyond training space cannot always be guaranteed [cit] . computational approaches can facilitate the study of biological targets of bioactive molecules and assist the discovery of on-target and off-target effects and understand the mechanism of action of bioactive molecules, thereby playing a crucial role in many scientific projects [cit] ) . with the ever-increasing public availability of chemical structures, bioactivity data, and receptor structures [cit], it is possible to construct reliable target prediction models [cit] for ctpts in machine-readable materials using chemical structure similarity searching [cit], data mining/machine learning [cit], panel docking [cit], and bioactivity spectra-based algorithms [cit] . the target of these ctpts may be a protein, cell line, whole organism, or biological activity. however, bioactivity data have not been increased in all areas. for example, databases are rich in human targets and molecules that modulate these targets, but contain limited information when it comes to bacterial targets [cit] ."
"the primary target information on 135 bioactive molecules was collected from scientific literature ( table s2 ) and databases ( table s3) . of these 135 bioactive molecules, scientific literature and databases contain the primary target information on 134 (99.3%) and 48 (35.6%) bioactive molecules, respectively."
"based on the proposed method, the primary target can be identified for all 135 bioactive molecules with a known primary target. in addition, the primary target can be predicted for 59 bioactive molecules with an unknown primary target by the use of the proposed method. generally, 4-pyridone group contains 194 bioactive molecules with a predictable primary target on the basis of the proposed method ( table 1) ."
a bioactive molecule such as a drug or a pesticide is used as a query molecule. the query molecule may have a known or an unknown target. the query molecule with a known target can be used as a reference to predict the target of neighbor molecules with an unknown target.
"for example, six molecules of figure s4 were used to predict the small ribosomal subunit inhibition by the proposed method and the ctpts. only molecule 1 in figure s4 is a small ribosomal subunit inhibitor, based on the proposed method. among the ctpts, pass online and pharmmapper predicted molecule 1 as a small ribosomal subunit inhibitor. the target of molecule 1 was assessed using both cell-based and in vitro assays and confirmed target of 30s ribosomal subunit [cit] ."
"pass online and pharmmapper predicted molecule 5 as a small ribosomal subunit inhibitor, whereas molecule 5 is used as the substrate of 3-ketovalidoxylamine a c-n lyase [cit] . one of the three key enzymes in production of valienamine is 3-ketovalidoxylamine a c-n lyase, which is a potent glucosidase inhibitor from validamycin a [cit] ."
"an attempt to improve the target prediction success rate led to the creation of an innovative method based on chemical similarity. the proposed method is significantly different from the available methods based on chemical similarity. the proposed method not only refers to the application of chemical similarity without employing statistical methods to use in both formats of non-digital and machine-readable materials but also helps in improving the target prediction success rate. the proposed method has several distinctive features compared to the available computational target prediction methods. first, the prediction is performed without employing statistical methods. second, it is highly accurate. third, it can be used appropriately without similarity calculations in non-digital materials and with similarity calculations (perfect similarity) in machine-readable materials. fourth, it enables us to gain a deeper understanding (more informative) of the relationship between the chemical structure and the target. fifth, little knowledge regarding high-performance computing techniques or algorithms does not prevent its implementation."
"the primary target information on 10 bioactive molecules was collected from scientific literature ( table s2 ) and databases (table s3) . of these 10 bioactive molecules, scientific literature and databases contain the primary target information on 9 (90%) and 6 (60%) bioactive molecules, respectively."
"pass online and pharmmapper predicted molecule 3 as a large ribosomal subunit inhibitor, whereas molecule 3 is a commonly used intense artificial sweetener, being approximately 200 times sweeter than sucrose [cit] . the interaction of sugars (or any sweet tasting ligand) with the t1r2+t1r3 sweet receptor (taste receptor type 1 members 2 and 3) sets into motion a biochemical chain of events that impacts on the activity of the trpm5 cation channel (transient receptor potential cation channel subfamily m member 5), which is critical for further propagation of the taste signal [cit] ."
"the proposed method was applied to 550 target predictions, of which 169 are new predictions. results of bioactivity assays and target predictions from chembl database were available for seven predictions of the proposed method, which confirmed five targets predicted by the proposed method, two of which were validated in vitro to be potent with affinities less than 1 µm. recently, the proposed method has been applied to predict mechanisms of action and targets in herbicides, two of which were confirmed by in vivo and in vitro experiments with an ic 50 of less than 1 µm. if the reliable prediction of bioactive molecule targets from nondigital materials is not the most important achievement in this field, it is undoubtedly one of the most important achievements."
"generally, the available computational target prediction approaches fall into two major categories of target-based methods (also called structure-based or receptor-based) and ligand-based methods [cit] b) . ligand-based methods incorporate chemical structures to predict targets [cit] . hence, the chemical similarity criteria for bioactive molecules play key roles in ligand-based modeling [cit] . target-based methods rely on three-dimensional (3d) receptor structures to predict receptor-bioactive molecule interactions [cit] . while ligand-based methods are fast, target-based methods take considerably more computational resources for a docking run against hundreds, or even thousands, of targets while still not achieving reliable results [cit] ."
"the prediction of biological targets of bioactive molecules from machine-readable materials can be routinely performed by ctpts. however, the prediction of biological targets of bioactive molecules from non-digital materials (e.g., printed or handwritten documents) has not been possible due to the complex nature of bioactive molecules and impossibility of employing computations. despite many advances over the last decades, computational target prediction is still a very challenging task, as reflected by the low experimental target validation success rate [cit] b) . the removal of false positives reduces the risk of yielding predictions that could incorrectly affect the downstream experiments for drug and pesticide discovery [cit] ."
"pass online and ppb predicted molecule 6 as a sterol 14α-demethylase inhibitor, whereas molecule 6 is an organotin miticide whose target is to disrupt oxidative phosphorylation by inhibition of the mitochondrial atp synthase [cit] ."
"as discussed by intan and mukaidono [cit], the uncertainty might be categorized into two types, deterministic uncertainty and non-deterministic uncertainty. fuzziness may be regarded as a deterministic uncertainty. it means that even in uncertain (unclear) situation or definition of an object, a subject (person) through his/ her knowledge is able to determine the object subjectively. similarly, as what happened in fuzziness, someone may subjectively determine the membership function of a given fuzzy set using his/ her knowledge. different knowledge may have different membership function of a given fuzzy set. thus, n-knowledge may have n different membership function of a given fuzzy set. here, knowledge plays significant roles in determining membership function of a given fuzzy set. a concept of knowledge-based fuzzy set is defined as follows."
"the target data of molecules 1, 3, 4, 5, and 6 and molecules with a structural similarity of 85% and more to molecules 1, 3, 4, 5, and 6 for the small ribosomal subunit inhibition were not found in chembl database, but based on the obtained information, it is unlikely that the target of these molecules is small ribosomal subunit inhibition. employing the minimum structure in 2,4(or 5)-diaminocyclohexanol group for identifying a bioactive molecule with the target of small ribosomal subunit inhibition has been shown in figure s3 ."
"based on the proposed method, the primary target can be identified for all 63 bioactive molecules with a known primary target. in addition, the primary target can be predicted for 12 bioactive molecules with an unknown primary target by the use of the proposed method. generally, 1,2,4-triazol-1-yl group contains 75 bioactive molecules with a predictable primary target on the basis of the proposed method (table 4) ."
"pass online, sea, ppb, targethunter, chemprot, superpred, and hitpick applied to 63 bioactive molecules with a known primary target led to make accurate predictions on 63 (100%), 62 (98.4%), 57 (90.5%), 47 (74.6%), 32 (50.8%), 24 (38.1%), and 13 (20.6%) bioactive molecules, respectively (table s5) . also, pass online, sea, ppb, targethunter, chemprot, hitpick, and superpred applied to 12 bioactive molecules with an unknown primary target led to make accurate predictions on 12 (100%), 12 (100%), 11 (91.7%), 7 (58.3%), 3 (25%), 2 (16.7%), and 2 (16.7%) bioactive molecules, respectively (table s5) . since the primary target information on the 1,2,4-triazol-1-yl group is not available in pharmmapper and spider, these tools are not able to predict the primary target of bioactive molecules of this group."
"recently, authors applied the proposed method to predict 22 targets and 15 mechanisms of action against more than 100 herbicides, two of which were validated in vivo and in vitro to be potent. the utility of the proposed method was documented by predicting and confirming the mechanism of action and target of tiafenacil and ipfencarbazone. the proposed method is well suited to provide insights into mechanisms of action and targets of bioactive molecules. for example, tiafenacil herbicide mechanism of action was predicted as protoporphyrinogen oxidase (ppg oxidase or protox) inhibition based on the proposed method [cit] proved tiafenacil mechanism of action as an inhibitor of ppg oxidase or protox with an ic 50 of 22-28 nm through biochemical and physiological experiments. for example, the target and mechanism of action of ipfencarbazone herbicide were predicted as very long chain fatty acid (vlcfa) synthesis inhibition (k 3 group) and mitosis inhibition (15 group), respectively, on the basis of the proposed method [cit] . ipfencarbazone inhibited the incorporation of [2-14 c] malonyl-coa into stearoyl-coa (c18:0) and arachidoyl-coa (c20:0) in rice and late watergrass microsomes at a low concentration (ic 50 less than 1 µm), similar to cafenstrole, a known vlcfa synthesis inhibitor [cit] . therefore, the target of ipfencarbazone was considered to be vlcfa synthesis inhibition [cit] ."
"the primary target information on 25 bioactive molecules was collected from scientific literature ( table s2 ) and databases (table s3) . of these 25 bioactive molecules, scientific literature and databases contain the primary target information on 25 (100%) and 1 (4%) bioactive molecules, respectively."
"the primary target information on 64 bioactive molecules was collected from scientific literature (table s2 ) and databases (table s3) . of these 64 bioactive molecules, scientific literature and databases contain the primary target information on 63 (98.4%) and 26 (40.6%) bioactive molecules, respectively."
"sea, pass online, ppb, targethunter, pharmmapper, chemprot, and spider applied to 15 bioactive molecules with a known primary target led to make accurate predictions on 13 (86.7%), 12 (80%), 10 (66.7%), 10 (66.7%), 10 (66.7%), 7 (46.7%), and 2 (13.3%) bioactive molecules, respectively (table s5) . also, sea, pass online, ppb, targethunter, pharmmapper, chemprot, and spider applied to two bioactive molecules with an unknown primary target led to make accurate predictions on 2 (100%), 2 (100%), 2 (100%), 2 (100%), 0 (0%), 0 (0%), and 0 (0%) bioactive molecules, respectively (table s5) . since the primary target information on the 3-glutarimidyl group is not available in hitpick and superpred, these tools are not able to predict the primary target of bioactive molecules of this group."
"a minimum structure does not represent a real molecule or a real association of functional groups, but is a part of a molecular structure that is necessary to ensure the target prediction of bioactive molecules. the minimum structure describes the presence or absence of chemical features in the molecule. it can be employed for distinguishing bioactive molecules based on their targets without similarity calculations in non-digital materials and with similarity calculations (perfect similarity) in machinereadable materials. ligand-based approaches employ statistical methods to link structural features to biological activities [cit], whereas the minimum structure involves specific structural features of a ligand required for interacting with its target without employing statistical methods. in the proposed method, unlike ligand-based approaches, it is necessary to know which part(s) of the bioactive molecule determines the precise target or targets responsible for the observed phenotype. ligand-based approaches suffer from the problem of activity cliff, which is defined as pairs of structurally similar molecules with large differences in potency [cit] . the minimum structure is identified using data collection about structurally related bioactive molecules. the minimum structure consists of the core with or without the peripheral part. here, the peripheral part is shown as the comment. the core plays an essential role in a bioactive molecule. furthermore, modifying at some key position on the peripheral part can make a big change in the target or the activity of a bioactive molecule. thus, the peripheral part can be useful for distinguishing bioactive molecules based on their targets. since the minimum structure depends on structurally related bioactive molecules and information about them, when they become available, the minimum structure can be updated to further refine it."
"the target data of molecules 1, 2, 3, 4, and 5 and molecules with a structural similarity of 85% and more to molecules 1, 2, 3, 4, and 5 for the large ribosomal subunit inhibition were not found in chembl database, but based on the obtained information, it is unlikely that the target of these molecules is large ribosomal subunit inhibition. employing the minimum structure in 3-glutarimidyl group for identifying a bioactive molecule with the target of large ribosomal subunit inhibition has been shown in figure s6 ."
"as the primary objective of this paper, we introduce a new concept, called knowledgebased dynamic fuzzy sets as a hybrid concept between dynamic fuzzy sets and knowledge-based fuzzy sets. practically in the real world application, even a certain membership function of fuzzy set has already given by a certain knowledge, the membership function is possibly changeable over the time. the following definition starts the concept of knowledge-based dynamic fuzzy sets."
"databases that can be used for the ligand-based target prediction have grown tremendously in size in the past [cit] but are still far from perfect. the screening data in databases are less rigorous than those in peer-reviewed articles and contain many false positives [cit] . deposited data are not curated, and hence, mistakes in structures, units, and other characteristics can and do occur [cit] . worse, since structural similarity does not guarantee similar bioactivity, chemical structures without other data are not always useful [cit] . in addition, chemical structures in some journals are not provided as machine-readable descriptions, which can be deposited in databases."
"the query molecule is used to search for structurally related bioactive molecules with similar chemical scaffold (molecules are compared to each other as a whole) and similar substructures (the specified substructures in molecules are compared to each other). however, it must be born in mind that structurally related analogs may bind in a slightly or considerably different manner [cit] . the main shortcoming of most ligand-based methods is that it results in insufficient extrapolation in practice since only molecules are compared to each other as a whole [cit] . it should also be pointed out that our proposed method can predict unseen interactions between bioactive molecules and potential targets in other methods. kegg [cit], pubchem [cit], drugbank [cit], and chembl [cit] databases provide common names and chemical structures for large numbers of bioactive molecules and, in some cases, their targets. all four databases support structure similarity searches."
"phenotypic effects of bioactive molecules result from interactions of bioactive molecules with protein targets, i.e., primary targets for which they were designed for as well as off-targets [cit] . it is well known that the majority of bioactive molecules have more than one target [cit], and predictable targets may vary in the ctpts. therefore, in this study, only the prediction of primary targets of bioactive molecules was considered, because primary targets should be identified before other targets."
"molecule 3 in pass online, sea, ppb, and chemprot, as well as molecule 5 in pass online and sea are predicted as sterol 14α-demethylase inhibitors, whereas molecule 3 and molecule 5 are non-steroidal molecules that potently inhibit aromatase in vitro and in vivo [cit] b; [cit] ) . also, molecule 5, in contrast to the azole antifungal agents, is devoid of effects on the p450-dependent ergosterol and cholesterol synthesis [cit] ."
"altogether, pass online, sea, ppb, chemprot, targethunter, pharmmapper, superpred, hitpick, and spider applied to 381 bioactive molecules with known primary targets led to make accurate predictions on 378 (99.2%), 243 (63.8%), 219 (57.5%), 185 (48.6%), 171 (44.9%), 140 (36.7%), 42 (11%), 29 (7.6%), and 2 (0.5%) bioactive molecules, respectively (table s5) . also, pass online, ppb, sea, targethunter, pharmmapper, chemprot, hitpick, superpred, and spider applied to 169 bioactive molecules with unknown primary targets led to make accurate predictions on 168 (99.4%), 81 (47.9%), 79 (46.7%), 75 (44.4%), 67 (39.6%), 61 (36.1%), 6 (3.6%), 5 (3%), and 0 (0%) bioactive molecules, respectively (table s5) ."
pass online and pharmmapper predicted molecule 5 [cit] suggest that norepinephrine or the norepinephrine precursor molecule 5 acts through an adrenergic receptor in nearby microglia to stimulate their migration toward and phagocytic clearance of aβ aggregates.
"pass online, sea, and ppb predicted molecule 1 as a sterol 14α-demethylase inhibitor, whereas molecule 1 is an imidazole derivative that is devoid of antifungal activity [cit] . molecule 1 is known to inhibit several cytochrome p-450 enzymes, including retinoic acid 4-hydroxylase and aromatase [cit] ."
"for example, six molecules of figure s8 were used to predict the sterol 14α-demethylase inhibition by the proposed method and the ctpts. only molecule 5 in figure s8 is a sterol 14α-demethylase inhibitor, based on the proposed method."
"the high accuracy of the proposed method in target prediction can be attributed to several causes. first, information on the target and the structure-activity relationship is mainly collected from peer-reviewed articles. a wealth of information on the activity of bioactive molecules exists in the literature, and access to this information can enable many types of analysis and making the right decision [cit] . second, information obtained from various sources including scientific literature, databases, and pharmacophoric descriptors is checked for mistakes. third, while information on the target, the chemical structure, the structure-activity relationship, and the pharmacophore is valid in its own right, the confidence in the observed outcome is significantly increased by a multi-validation method. fourth, the nature of the proposed method is based on minimal mistakes because it is necessary to know which part(s) of the bioactive molecule determines the precise target or targets responsible for the observed phenotype in the proposed method."
"the minimum structure is employed as a query to search for molecules that perfectly satisfy the minimum structure of what is guessed crucial for the targeted activity. the proposed method is based on chemical similarity, but only molecules that perfectly satisfy the minimum structure are considered. structurally related bioactive molecules found with the same minimum structure were considered as neighbor molecules of the query molecule. the known target of the neighbor molecule is used as a reference for predicting the target of the neighbor molecule with an unknown target. a lot of information is needed to identify the minimum structure, because it is necessary to know which part(s) of the bioactive molecule determines the precise target or targets responsible for the observed phenotype. therefore, the predicted target based on the minimum structure without employing the statistical significance is considered as a reliable prediction. since only molecules which perfectly (and not partly) satisfy the minimum structure are considered, the minimum structure can be used without similarity calculations in non-digital materials and with similarity calculations (perfect similarity) in machinereadable materials. without doubt, the assessment of molecule similarity for activity cliff definition and analysis is the most difficult task, for several reasons: first, the quantification of molecule similarity is strongly dependent on chosen molecular representations (descriptors), and second, there are no generally accepted similarity measures [cit] ."
"pass online predicted molecule 2 as a large ribosomal subunit inhibitor, whereas molecule 2 is an acetamide derivative of safingol. safingol is a lysosphingolipid protein kinase c inhibitor that competitively interacts at the regulatory phorbol binding domain of protein kinase c [cit] ."
"the primary target information on 63 bioactive molecules was collected from scientific literature ( table s2 ) and databases (table s3) . of these 63 bioactive molecules, scientific literature and databases contain the primary target information on 62 (98.4%) and 34 (54%) bioactive molecules, respectively."
"a simple denoising technique based on zernike reconstruction that greatly improves the wavefront reconstruction accuracy was used in this paper [cit] . in this noise removal method the spot patterns corresponding to individual subapertures of a shs are reconstructed using zernike polynomials by calculating zernike moments. the reconstructed images are thresholded and the resultant images are used for centroid estimation. since the noise features arising due to background, readout are of low spatial extent, they will not be found on the images reconstructed using less number of zernike moments. the scintillation effects or rather prominent events comparable to the actual spot image will sustain even after following the zernike reconstruction and thresholding process. in this paper we propose a gaussian pattern matching (gpm) algorithm to improve the centroid detection accuracy. in this method we take the advantage of the shape of the spot pattern. as a first step, we identify the features. secondly, we selectively eliminate the features that do not have a circular shape. finally we recount the number of features, fn and if it is greater than one, we use the fact that the intensity distribution from the feature centroid drops like a gaussian. this algorithm can be further improved by including wavefront prediction algorithms [cit] . the methodology used for simulating the shs spot pattern in the presence of readout noise, photon noise and background noise is described in section 2. zernike reconstructor based denoising technique is presented in section 3 with illustrations and graphs. section 4 describes the basic idea behind the gaussian pattern matching algorithm and the design steps involved to minimize faulty recognition. the results obtained by applying monte carlo simulations on various centroiding algorithms are presented in section 5 and the derived conclusions are presented briefly in section 6."
"the management of hl and its consequences at a public health policy making level can benefit from the analysis of heterogeneous data, including ha usage, noise episodes causing threshold shifts, audiological, physiological, cognitive, clinical and medication, personal, behavioural, life style data, occupational and environmental data. the analysis of these types of data using big data analytic techniques can enable the investigation of whether hl relates to other comorbidities and contextual factors and patterns of such relations."
"the effective management of hl depends on and requires appropriate public health policies (php) [cit] . public health policy affects the affordability and hence access to has and ongoing treatment services (e.g., hl check-ups, ha adjustments, provision of related rehabilitation services). public health policy can also have a significant effect on:"
"the spot pattern hence obtained after addition of the above described noise is used for further analysis. they are used in the monte carlo simulations for testing the centroiding algorithms. in the simulations the user has control over the actual position of the spot. the centroid estimation error (cee) can be evaluated using the formula, (4) where, (xc *, yc * ) is the estimated centroid and (xc, yc) is the actual centroid. effectively, cee measures the distance of the estimated centroid from the actual position of the spot."
"in the previous step, the shape of the spot was used for selective elimination. there can be cases of large noise features that are circular in shape as shown in fig. 10 . in the profile identification step, the intensity profile is used to choose the actual spot feature. the number of features are recounted and re-identified after going through the shape identification step. the intensity fall off from the centroid position of individual features is measured and pattern matching was performed with a standard one dimensional gaussian function,"
"prune is the best choice for this case. the model constructed has 82.4% correlation and 0.322 relative error. afour layer network, with one input layer, two hidden layers, and one output layer, was formed for the present case. the first hidden layer has 3 neurons and second hidden layer has 2 neurons. table 3 gives the statistical evaluation of prediction and figure 4 shows the predicted values versus experimental values for training and testing."
"this ranking is higher than diabetes and conditions causing visual impairment [cit] . current trends in the spread of hl are expected to continue or increase further due to exposure to workplace and social noise. in the uk, for example, the percentage of people suffering from hl is expected to exceed 20% [cit] ."
"rbfn is the best choice for this case. the model constructed has % 76.3 correlations and 0.444 relative error. a three-layer network, with one input layer, two hidden layers, and one output layer, was formed for the present case. the first hidden layer has 20 neurons. table 5 gives the statistical evaluation of prediction and figure 6 shows the predicted values versus experimental values for training and testing."
". in this process, the features that do not follow a gaussian like structure are eliminated. a comparison of the actual spot line profile with the noise feature line profiles is shown in fig. 11 . the sum of squares of errors was used as a statistic to measure the total deviation of the actual values from the fitted values."
the denoising technique of spot images consists of two steps. firstly the images are represented in terms of a finite number of zernike polynomials. the process of conversion of the image in terms of zernike polynomials is performed by the zernike reconstructor. thresholding is performed on the reconstructed images to obtain a nearly noise free image.
"wavefront reconstruction accuracy is a critical factor in high resolution ground based astronomical imaging through turbulence. centroiding contributes to most of the wavefront reconstruction error in shack hartmann sensor based adaptive optics system with readout, background noise and strong scintillations. using an effective denoising technique based on thresholded zernike reconstructor, the small scale noise features in the spot pattern images are removed. at very low signal to noise ratio, it was shown that taking the advantage of the shape of the spot can improve the centroiding accuracy of cog, iwc,"
"evotion aims to do support this through, the development of an integrated platform incorporating a big data analytics (bda) platform, which will enable the collection and analysis of heterogeneous data related to hearing loss, including hearing aid usage, physiological, cognitive, medical, personal, occupational, behavioural, life style, environmental and open web data. these data will underpin a decision support system to the identification, simulation, selection and monitoring of the effectiveness of possible and implemented interventions related to the management of hearing loss and the overall inclusion and well-being of hl patients in processes aimed at the formulation of related public health policies, based on the outcomes of the bda."
"the aim of this paper is to investigate the effects of humidity, tool type, cutting speed, feed rate, diameter of cutting equipment, and depth of cut on average surface roughness using the neural networks. analyses of ann for different materials (dry and humid pa6g) having different cutting conditions are performed in average surface roughness. this paper introduces ann's technique for modeling the average surface roughness. 129 results were used as data sets to train the network, while 63 results were used as test data from the total experimental results of each case. different algorithms were studied and not a unique perfect technique is determined but the best results were obtained from exhaustive prune and rbfn algorithms. for testing data, correlation values have changed from %62.7 to %82.4. this difference could occur because of the change in importance of factors affecting the process. to train with more data could also improve the correlation. on the other hand, these ann predicted results can be considered within acceptable limits. the results show good agreement between predicted and measured values."
"the use of bda has significant potential in reducing the costs of health care in several areas, including: high-cost patients, re-admissions, decompensation, adverse events, and treatment optimisation for diseases affecting multiple organ systems [cit] . further benefits from the use of bda in healthcare include the potential for generating new knowledge, enabling personalised medicine, delivering information directly to patients and empowering them to play a more active role [cit] . however, a prerequisite for successful learning from big health care data is to gain actionable insights into evidence generated by the data [cit] ."
"artificial neural networks vary according to connections between neurons (single layer and multilayer) and function used in neurons (linear, sigmoid, gaussian, etc.). experiments were conducted and a data set was obtained containing 192 sets of input parameters for each humidity and tool condition and the corresponding output parameter. this data set was used for training and testing the neural network model. the set was then divided into two parts. one part contained 129 data points and was used for training the network. another part contained 63 data points and was used for testing the network. the data set used for testing the network was chosen randomly from the experimental data set. some statistical methods, r 2, rmse, and mape values have been used for comparison. some selected sample data sets used for training and testing the network are shown in table 2 ."
"various solutions are available for bda. these can be divided into (1) processing/compute (e.g., hadoop [cit], cuda [cit], and twitter storm [cit] ) (2) storage (e.g., hdfs [cit], hbase [cit] ), and (3) analytics: mlpack [cit] or mahout [cit] . although there are commercial products for data analysis, most of the studies on the traditional data analysis are focused on the design and development of efficient and/or effective \"ways\" to find the useful evidence in the data. nevertheless, most of the current systems will not be able to handle the whole dataset all at once. hence, designing a good data analytics framework becomes very important for the data analysis process. data mining algorithms for data analysis also play a vital role in the bda, in terms of the computation cost, memory requirement, and accuracy of the end results."
"zernike polynomials are a set of continuous orthogonal circular polynomials defined over the unit disk. since they form a complete set of orthogonal polynomials, any two dimensional function, i(x, y) can be represented as a proper linear combination of this basis set."
"prune is the best choice for this case. the model constructed has % 76.5 correlations and 0.421 relative error. a fourlayer network, with one input layer, two hidden layers, and one output layer, was formed for the present case. the first hidden layer has 5 neurons and second hidden layer has 4"
"the operation of the evotion platform will be driven by public health policy decision-making models (phpdm models). these models will specify: (i) the generic goal(s) underpinning the decision to be made (e.g. policies regarding the frequency of follow up care for patients who have been issued has) and the alternative decisions that may be made for this goal (e.g., having no, one or two follow ups within a specific time period) (ii) the criteria to be used for making such decisions (e.g., whether the difficulties faced by different types of ha users depend on the type of their hl, their cognitive capabilities, their life style and behaviour, other comorbidities that they may have and/or their overall compliance with ha usage guidelines given to them by clinicians and whether such difficulties are alleviated depending on the number of follow up treatments and the time that elapses between them) (iii) the bda evidence required for applying the criteria (e.g., whether any combination of the factors considered above is a good predictor of the difficulties faced by ha users as confirmed by specific types of statistical analysis or data mining based classification) and the bda process for producing it (iv) simulations that should be executed for exploring the consequences of alternative decisions (v) processes to be followed for making specific types of health policies (e.g., what is the threshold of evidence that should be considered sufficient for a particular decision, who are the stakeholders whose views should be considered and recorded prior to reaching a decision, who has responsibility for making the final decision, whether a decision should be continually or periodically reviewed upon the acquisition of new evidence etc.)"
"the key reason underpinning ineffective ha use is that has are fitted to suit the audiogram rather than the patient's needs and overall profile. ideally, ha fitting should take into account: (a) a range of personal and real life behavioural, physiological and other auditory related data; and (b) an analysis of the circumstances which are challenging for individual ha users on a continuous basis. it should also be appropriately supported by individualised rehabilitation treatments such as auditory training [cit], as ha users depend more on their cognitive resources than normal hearing listeners in order to understand speech [cit] . nevertheless, evidence on how to link such information with appropriate, individualised management strategies is still lacking."
"the development of the evotion platform is related to the several scientific and clinical research areas, the most prominent of which are (a) public health policy making and (b) big data analytics and decision making. in the following, we present an overview of research in (a) and (b)."
"beyond this, however, we believe that evotion has the potential to generate a generic platform for evidence based model driven public health policy making in other areas of healthcare, as well."
"most features do not have a circular shape as shown in fig. 8c . the circularity or the extent of the feature being circular is measured for each of the features. in the proposed algorithm, the centroid position was computed for a single feature locally and the distance from the centriod at which intensity becomes zero is measured. this distance is called the radius parameter. the radius parameter is estimated at different angles (0-360) from the feature centroid position. we define circularity, c of a single feature as the inverse of the variance of the radius parameter computed over different angles from the centroid position. for an ideal circular feature, the variance is zero and hence the circularity is infinity. a lower cutoff for this parameter is chosen to eliminate features that are not close to a circular shape. this shape identification process was applied to fig 8(c) and is shown in fig. 9 . it is possible to eliminate all the noise features in this step at better snr and applying a suitable feature size thresholding."
"wcog and iwcog perform better even without the application of gpm algorithm at low snr and very small shift in the spot as shown in fig. 13 . but for large shift in the spots, the performance of wcog is worst in the absence of gpm algorithm [cit] . the performance of wcog can be enhanced by applying gpm algorithm and use it as a centroid pre-calculator. the weighting functions for wcog are evaluated from the pre-calculated centroid position. the improvement in the centroiding accuracy for wcog when the spot shift is 5 pixels is shown in fig. 14."
"the consequences of hl in the overall health condition of the people suffering from it are significant. several studies have shown that hl increases the risk of cognitive decline/dementia by 20% [cit], mental illness [cit], depression [cit], and even the risk of mortality due to reduced physical and mental activity and social isolation [cit] . the latter factors also lead to a poorer quality of life overall both in physical and mental terms for hl patients [cit] . furthermore, hl increases the risk of accidental injury (e.g. hl is associated with incidents of falls [cit] )."
"at high noise conditions, the thresholded zernike reconstructed images lead to multiple features as shown in fig. 8 . this is due to the fact that at high noise level conditions there can be large scale features (scales comparable to the size of the spot) which might not be removed even after the denoising procedure is implemented."
"pa6g material in 46 mm plates that is used in the experiments is supplied from polimersan firm. pa6g obtained in plates are cut in dimensions of 112x82x46 mm. and they are kept in humid and dry place. humid and dry samples are processed in tmc500 cnc vertical machining center. cycle of bench can be adjusted between 60 and 6000 cycles/min. carbide and hss cutting tools are used in experiment. detailed cutting conditions are given in table 1 . schematic picture of experiment setting is depicted in figure 1 . for the measurement of roughness, marsurf ps1 portable surface roughness measurement equipment is used. measurement needle has the measurement diameter of 2 m and pressure force is averagely 0.7 mn. measurement scanning length is adjusted as 5.6 mm. [cit] s trademark measurement equipment and electronic assay balance are used. machining of workpiece and measuring machined surface is shown in figure 2 ."
"hearing loss (hl) is the most frequent sensory deficit and one of the most prevalent chronic diseases, affecting approximately one-third of people over the age of 65 and over 5% of the world's population [cit] more than 360 million people had disabling hl, i.e., a 3-fold increase from 120 [cit] . hl ranks as the fifth leading cause of years lived with disability (yld), a component of the disability-adjusted life year (daly), used to measure the global burden of disease [cit] ."
"in this step, the features on the spot pattern image are counted and identified. feature recognition can be performed by using many existing pattern recognition algorithms. in this paper, we used a simple hough peak identification method to detect the features and number them in the order of peak height. to eliminate small scale features which may arise due to unavoidable scintillations, we imposed threshold conditions on the size of the features. the number of features varied from 30 to 1 from very low snr to high snr."
all ann types are tested and the ones giving best correlation are used in the cases. this section is divided into four cases according to humidity and to condition and results for each case are given under appropriate part. number of input and output parameters is kept constant as 4 and one during modeling. it is seen that exhaustive prune and rbfn are the anns giving more accurate results.
"on-going reforms of php in this area (e.g., changes in the free provision of has for different types of hl in the uk) and the spark of social debate that they have caused demonstrate the importance of php in this area. examples of policy fields that influence big part of the population and mobilize resources encountered in billions of euros yearly are cut off points of hearing aid fitting covered by insurance, decision for unilateral or bilateral ha fitting or cochlear implantation, noise protection measures in working environments, and default maximum db levels of electronic devices."
computational intelligence and neuroscience neurons. table 4 gives the statistical evaluation of prediction and figure 5 shows the predicted values versus experimental values for training and testing.
"the envisaged physical architecture of the evotion solution is shown in figure 2 . as shown in the figure, evotion will be deployed on a cloud infrastructure and the and accept retrospective and prospective (dynamic) data from different components, including existing clinical repositories, the mobile application that will be part of the platform, enhanced has and wearable devices (biosensors) available offthe-shelf. evotion has will be communicating with the evotion application to send real time ha usage and environment data to it, and receive adaptation control signals from it. this will be through the mobile application or directly to the platform (for ha users who do not have mobile phones). wearable biosensors will also be sending data to the evotion platform, through either the mobile phone or directly to the platform. the evotion platform will also be capable of connecting and receiving existing and periodically updateable data from existing clinical repositories (e.g., outcomes of audiograms and other hearing aid fitting tests, clinical questionnaires etc.)."
"the main overall contribution of evotion to existing research will be the development of a novel model driven platform for establishing public health policies for the management of hl, based on evidence arising from the analysis of static and dynamic health data. our focus on hl management and the availability of related big data sets in this area, gives our research a clear driver and background framework for evaluation. to the best of our knowledge, the above contributions are clearly beyond the current state of the art and introduce a significant innovation potential to the management and treatment of hl, hearing deterioration and related cognitive decay."
"the evotion platform will provide a tool supporting the specification of phpdm models into some high-level language. their verification and transformation into executable bda and simulation tasks and decision making processes would be passed as inputs to the bda platform, simulator and decision support system of the platform to execute them and realize the policy making process specified by them."
"photon noise, background noise and detector readout noise are three major noise sources in ao. in astronomical adaptive optics system, a shack hartmann spot pattern is greatly influenced by photon noise which is caused due to the low photon count. photon noise can be minimized by using a lgs or a bright natural guide star (ngs) as a reference star. background noise occurs because of the finite sky brightness and other unwanted light sources. readout noise is an unavoidable noise caused by the detector. we included these effects in our simulations. the following steps were followed to simulate the spot pattern."
"the experimental results were used to train and test. 100 experimental results were used, from the total of 120, as data sets to train the network, while 20 results were used as test data [cit] . bozdemir and aykut used more data in another study and studied lm, scg, and cgp algorithms and found lm algorithm to be the best [cit] . in this study, different ann models were developed with the purpose of guessing unachievable gap values and studying carefully the creation of ra values in the experimental results. ann is used to determine the effects of cutting conditions (cutting speed, feed rate, diameter of cutting equipment, and depth of cut) besides tool type and humidity conditions on average surface roughness. for each tool type and humidity condition 192 experimental data sets are used in modeling. the results of the model indicate a relatively good agreement between the predicted values and the experimented ones."
"although different wavefront measuring devices exist like curvature sensor, shearing interferometer, common path interferometer, most generally used sensor is the shack hartmann sensor (shs) which is made up of tiny lenses arranged in a two dimensional array. a wavefront incident on the shs forms an array of spots near the focal plane of the lenses. unlike an ideal plane wavefront normal to the shs which forms well focused equidistant spots, a distorted wavefront forms spots that are not equidistant as shown in fig. 1 . from the measurement of the shift in the spot positions from ideal case, it is possible to calculate the local slopes (over each lenslet/subaperture) of the wavefront. using the slope values, the approximate shape of the wavefront is estimated. the two steps involved in wavefront reconstruction are hence (a) centroiding of individual spots to calculate local slopes, (b) formation of the wavefront shape from local slope measurements."
"the evotion platform will support its users to specify the types of analysis that they would like to apply upon the data collected by the platform in a declarative form, as well as the forms in which the outcomes of this analysis can be visualised, the certainty thresholds that these outcomes should exceed in order to be deemed usable in the decision-making process, and the criteria in which they can inform the selection of policy alternatives. the forms of bda that will be performed by the evotion platform and the usage of its outcomes will be specified as part of phpdm models (see sect. iv below)."
"anns offer a computational approach that is quite different from conventional digital computation. digital computers operate sequentially and can do arithmetic computation extremely fast. biological neurons in the human brain are extremely slow devices and are capable of performing a tremendous amount of computation tasks necessary to do computational intelligence and neuroscience everyday complex tasks, commonsense reasoning, and dealing with fuzzy situations. the underlining reason is that, unlike a conventional computer, the brain contains a huge number of neurons, information processing elements of the biological nervous system, acting in parallel. anns are thus a parallel, distributed information processing structure consisting of processing elements interconnected via unidirectional signal channels called connection weights. though modeled after biological neurons, anns are much simplified and bear only superficial resemblance [cit] ."
"there are various studies to the aim of detecting the relationship between surface roughness and cutting conditions [cit] . davim has studied the difference between processing conditions of turn bench and surface roughness formation of glass fiber reinforced and nonreinforced pa66 polyamide materials [cit] . bozdemir carried out experiments on pa6g materials having the characteristic of dehumidification up to 7%, resulting in that average surface roughness value changes although process conditions have not changed for dehumidified pa6g material [cit] ."
"to use this erroneous spot for accurate centroiding, we take the advantage of the fact that the final spot formed at the focal plane of a lens must assume an airy pattern which can be approximated to a gaussian like structure. the pattern matching algorithm is implemented in three steps:"
"these models will also provide a structure for organising possible alternative decisions in policy making, the arguments and rationale for making decisions, the stakeholders participating in the decision-making process, the views they express and the final decision making rules. hence, phpdm models will drive a collaborative stakeholder decision making process, and provide a structure for recording information that will make it traceable and accountable. furthermore, phpdm models will be specified parametrically to make their customisation easy in case that this would be required in different policy making settings. thus, phpdm models can be repeatedly executed in the same or different policy making settings (e.g., for making policy on the very same issues in different regions)."
"where, zi(x, y) represents zernike polynomials and the coefficients ai\"s standing before zernike polynomials are image weights called zernike moments. since zernike polynomials are mathematically complicated functions, the calculation of zernike moments of images is tedious. in this paper, we used a very fast and nearly accurate method suggested by hosny to compute the zernike moments [cit] . the denoising ability varies with the number of zernike orders used for representing the image. the reconstructed noisy spot pattern images are shown in fig. 5 . it is advised to use more number of zernike moments since the central gaussian feature can be captured better during reconstruction."
"the world health organization (who) defines \"health policy\" as the \"decisions, plans, and actions that are undertaken to achieve specific health care goals within a society\" [cit] . research on public health policy making has primarily focused on policy formation processes and guidelines. these may be related to four key stages in policy making, namely: (i) situational analysis (i.e., the assessment of the needs and gaps, the resources available, and eventually the strengths weaknesses, opportunities and threats (swot) arising in connection with a situation that needs to be addressed by health policy); (ii) development of action plan (i.e., setting the initial aim, objectives, activities and all priorities for implementing a health policy programme, and identifying the resources needed for this implementation); (iii) implementation and monitoring of programme; and (iv) programme evaluation (i.e., the assessment of the effects and other outcomes of implemented health policy programme in long/medium term)."
". rbfn is the best choice for this case. the model constructed has % 76.3 correlations and 0.444 relative error. a three-layer network, with one input layer, two hidden layers, and one output layer, was formed for the present case. the first hidden layer has 20 neurons. table 6 gives the statistical evaluation of prediction and figure 7 shows the predicted values versus experimental values for training and testing."
"the main components, overall information flow, big data analytics, decision support capabilities policy making platform envisaged by evotion are shown in figure 1 the data collected by the evotion solution will be correlated and analysed, using bda techniques, which will be employed in order to detect patterns of: affecting hl and the overall well-being of hl patients suffering from it in detecting (a)-(d), our key focus will be to identify factors and parameters that define subgroups with different treatment outcomes, and the risks arise for such subgroups and the outcomes."
"considering inputs as numerical values eases the modeling process because ann finds out relations between inputs and outputs by using formulas. on the other hand, some recent techniques (c&r tree, chaid, etc.) focus on the classification of the data and they can handle numerical and alpha-numerical characters as inputs more effectively. in this study, different ann models are tested using whole data (including humidity condition and tool type) and results showed weak correlation (under 60%) between predicted and experimental data. so, the results are given at different sections in the following paragraphs."
"another result that should be mentioned is the efficacy of multilayer anns. generally they outperformed the single layer ones as predicted. these models have the advantage of having much more flexibility in defining constants in relation formulas of neurons. as a consequence, they have better adaptability in defining the relation between inputs and outputs."
"the canadian foundation for healthcare improvement (cfhi) has developed a framework of 18 processes to support evidence-informed health policymaking [cit] . these processes are aimed at ensuring that relevant research is identified, appraised and used to inform health policy decisions. cfhi guidelines cover all stages (i)-(iv) above, albeit (ii) more comprehensively who has developed a manual for planning and monitoring national health strategies, aimed at raising awareness about ear and hearing problems among individuals and communities. these have been tailored and targeted separately to the general public, policy-makers, programme managers and funding providers [cit] . who's guidelines focus on use of swot in situational analysis, suggest aims for the development of action plans, identify subphases in implementation and monitoring (i.e., pilot, expansion and evaluation). the applicability and transferability of evidence (ate) tool [cit] has been developed to help health planners make decisions about local health planning priorities. ate focuses mainly on directing investigations of the literature as part of public health policy decision making to aid situational analysis."
"the outcomes of such analysis can also enable the stratification of related risks and effects to hl patients, andthrough correlation with other economic, social and physical constraints -help developing a holistic systemic perspective of over interventions regarding the management of hl. moreover, it can enable the broader support, social and occupational inclusion and the well-being of hl patients, exploration of missing, under or over-estimated value of specific interventions (e.g. noise protection, visualization of public announcements etc.) and analysis of their effectiveness."
"usage of engineering plastics has been constantly increasing due to its characteristics of lightness, cheapness, and strength. today, it has a wide range of usage nearly in all the fields of industry. there are different types and characteristics of engineering plastics. polyamides (especially cast polyamide or castamide as industrial name) are one of these most widespread plastic types [cit] . pa6g has superior properties over many metals by being cheaper, easily processed, light, high-resistant, and abrasion-resistant. many studies have been carried out for different characteristics of polyamides (friction condition [cit], wear properties [cit], thermal properties [cit], machinability [cit], etc.). in this study, machinability is concerned."
"number of neurons is also another effective parameter in finding best correlation. having more neurons can increase efficacy of the model up to certain values as can be seen in literature [cit] . even though bozdemir has selected one layer, he could manage to get correlation as much as ours by means of using 13"
"bda can be separated into three parts (input, data analytics, and output) and seven operators (gathering, selection, pre-processing, transformation, data mining, evaluation, and interpretation). nowadays, the data that need to be analysed are not just large, they are also composed of heterogeneous data types and real time streaming data. these characteristics make pre-processing an important task, and affect the applicability of statistical and data analysis approaches (as heterogeneous, incomplete and noisy and continually updated data affect the performance of the data analysis algorithms) [cit] ."
"in this paper, we have introduced evotion, a new european research project, aims to develop an integrated platform supporting: (a) the analysis of related datasets to enable the identification of causal and other effects amongst them using various forms of big data analytics, (b) policy decision making focusing on the selection of effective interventions related to the holistic management of hl, based on the outcomes of (a) and the formulation of related public health policies, and (c) the specification and monitoring of such policies in a sustainable manner. in this position paper, we describe the evotion approach."
"although public health policy has a profound effect on health status, there is a considerable gap between what research shows as effective and the policies that are enacted and enforced. research is most likely to influence policy development through an extended process of communication and interaction [cit] . systematic reviews of studies of decisionmaking by health care policy-makers show that researchers could better inform health care management and policy-making by making several changes to how they produce and update systematic reviews and by adapting existing reviews that are relevant to local health care issues [cit] . also, according to a study on systematically reviewing qualitative and quantitative evidence to inform management and policy-making, policymakers and managers increasingly require access to highquality evidence syntheses that include research and nonresearch evidence, and both qualitative and quantitative research findings [cit] ."
"adaptive optics (ao) is a well developed technology that helps in quality imaging of objects hidden behind turbulent media. it is widely used in astronomical imaging, retinal imaging and free space communication systems [cit] . a simplest ao system comprises of a wavefront sensor that detects the shape of the incoming optical wavefront and a wavefront corrector that compensates the effects of wavefront distortions by imposing a conjugate wavefront on the incident wavefront [cit] . the wavefront sensor and corrector are connected via a processor where the required wavefront reconstruction computations are performed."
"the performance of iwcog remains the same even for large shift in the spots. problem arises for iwcog only when large noise effects occur very close (closer by more than half-width of the spot) to the actual spot position. since large noise effects occur randomly in time, it is advantageous to have gpm algorithm which can nearly point out the centroid position prior to a more precise estimation using iwcog."
"even if one it not interested in potentials very close to the cell, complicating effects like anisotropic conductivity, frequency filtering or ephaptic potentials may shape the extracellular potential to such a degree that classical models are rendered inapplicable. further work is required to enhance the novel models and to meticulously compare them with experimental recordings in different extracellular constellations."
"in regard to using a similar music search system, to identify the music source, an audio fingerprinting technique is often used. the audio fingerprint is an inherent feature of the audio signal like a human fingerprint or biometric feature. as the audio feature vector, for example, one can utilize the short-time fourier transform (stft) coefficients, the chroma codes, and zero-crossing intervals and so on."
"finally, i compared decipher's performance to promals [cit], which is a program that relies on more accurate secondary structure predictions obtained from psipred [cit] . promals first performs psi-blast searches with representative sequences from the input set, and then uses accurate secondary structure predictions with a consistency-based approach to align the sequences. promals greatly out-scored all of the other alignment programs on the smallest sets of two sequences, but its advantage disappeared once other sequences were added to the input set (fig. 5) . furthermore, it was several orders of magnitude slower that the other aligners (fig. 6), and testing input sets larger than 125 sequences proved prohibitively time consuming. more recent approaches that make use of solved protein structures are available, such as promals3d [cit] . however, it is unclear how to test such approaches on structural benchmarks, because the reference sequences are likely present in the same structure databases used by these programs."
"different benchmarks often result in contrasting optimal parameters (e.g., gap opening and extension penalties) and an incompatible performance ranking of alignment programs [cit] . for these reasons, the choice of benchmark is of utmost importance when developing and comparing algorithms for sequence alignment. to choose alignment benchmarks for this study, i began by comparing secondary structure concordance across common benchmarks. this method of comparison requires that the secondary structure of reference sequences be available, which excludes the popular balibase benchmarks [cit] because the corresponding secondary structure of most balibase sequences is unknown [cit] . although secondary structure agreement alone is insufficient to ensure a high quality benchmark, a lack of agreement can be an indication of alignment inaccuracy."
"the focus on maximizing true homologies has been furthered by a reliance on q-score for performance comparisons with structural benchmarks. q-score is defined as the average pairwise fraction of reference homologies that are also found in the test alignment (i.e, the alignment program's output). q-score does not directly penalize for aligning positions that are unaligned in the reference, also known as over-alignment [cit] . overalignment can be quantified using the modeler score (here termed m-score), which is the fraction of aligned homologies that are also aligned in the reference [cit] . a higher m-score indicates fewer false homologies, and viseversa. the m-score does not penalize for under-alignment [cit], as the correct alignment of only one position would result in a perfect m-score (i.e., 1). hence, it is necessary to compare both true and false homologies when judging alignment performance."
"as our goal, we wished to detect the original music sources and simultaneously detect their arrangement authority by analyzing hashed audio features. moreover, we are required to detect them by analyzing the raw signal before hashing."
"finally, we look at the debye layer very close to the membrane in fig. 4 . in this region, only the pnp model is able to reproduce the ionic gradients and their large influence on the extracellular potential. naturally, these effects are not captured by either en or lsa. in contrast to the previous comparisons, we here simulated the en model with the same grid as the pnp model, i.e. finely resolving the debye layer, in order to evaluate the potential at the same coordinates. note that such a fine resolution is normally not required for the en model. we see that even with the same spatial resolution, the en model does not reproduce the correct debye layer potential, as the electroneutrality assumption does not hold in the membrane vicinity 1 . however, it converges quickly to the pnp solution and it yields valid results already at a minimum distance of about 20 nm, just outside of the debye layer, where pnp and en solutions coincide."
"however, these gains have come at a cost because secondary structure is time consuming to accurately predict, which prevents these methods from scaling to a large number of sequences. presently none of the alignment programs that use predicted secondary structure can align a thousand or more sequences in a reasonable amount of time [cit] . this inefficiency is due to the need to find and align many sequences that are related to each sequence for which secondary structure is being predicted. using the most accurate secondary structure predictions in sequence alignment therefore indirectly incorporates more sequence information into the alignment process. an alternative to this approach is to directly add more sequences to those being aligned, which has also been shown to substantially improve the accuracy of aligning small sequence sets [cit] . both of these approaches leverage large external databases of sequences that may not provide additional information when the input set is already large or all-encompassing."
"there is a clear trade-off between accuracy and effort, both computational and implementationwise, and it is important to know which model should be used in which situation. the following listing strives to give advice on which model to use depending on the particular case."
"a schematic view of the computational domain is given in fig. 1, consisting of an intracellular domain ω cy and the extracellular space ω es, separated by the membrane interface γ int ."
"to compare empirical benchmarks, secondary structure assignments according to dssp [cit] were downloaded from pfam [cit] for proteins with solved structures. pairs of sequences in each reference set were replaced with their corresponding secondary structure to generate an alignment of secondary structure states. a multiple alignment of n sequences therefore resulted in (n 2 -n)/2 different pairwise alignments. the pairwise secondary structural identity of each of these alignments was calculated and used to compare benchmarks. secondary structure identity was defined as the number of columns with matching secondary structure (8-state dssp) normalized by the maximum number of matches possible. the large number of data points was simplified for plotting by finding the shortest contour line on the kernel density surrounding 75 % of points. the r programming language [cit] was used for all analyses. p-values were calculated using the wilcoxon signed rank test in r [cit] ."
"to address these complicating effects on the extracellular potential, we have previously addressed more general models based on the poisson-nernst-planck (pnp) system of electrodiffusion, which allows to explicitly model ion concentrations and their dynamics [cit] . we could show that in contrast to vc-type models, the full pnp system allows to capture these effects, which have a significant influence on the extracellular potential especially at small membrane distances."
"considering the impressive success of neuron models, it is surprising how little attention the extracellular space (es) has received. given the fact that more and more experimental recordings are performed extracellularly, one is interested in the process of the generation of extracellular potentials -and in models that replicate such recordings. models producing extracellular potentials will necessarily have to include the extracellular space to some degree. in hh-type models, the es is assumed to be isopotential and most commonly set to a grounding potential of 0 v. such an assumption might be valid when one is interested in the intracellular or membrane potentials only, but it is obviously not useful when regarding extracellular potentials."
"audio fingerprinting generates a hash code for every segment based on an audio feature vector. however, the mixed arrangement is ignored in the audio hash. in this report, we proposed to detect an additional arrangement authority by embedding a watermark. to utilize this protocol, we tried to apply the echo hiding watermarking method that is robust against locality sensitive hashing."
"these vc-type models have been refined to represent an inhomogeneous extracellular space [cit] that accounts for effects like frequency filtering [cit] . an interesting technique in this context is the application of inverse methods to these kinds of models, enabling the estimation of current source densities from local field potential (lfp) measurements [cit] ."
"multiple sequence alignment (msa) is a ubiquitous task in biology, and has a wide variety of applications including homology detection [cit], predicting residue couplings [cit], finding evolutionarily important sites [cit], oligonucleotide design [cit], and phylogenetics. a multiple sequence alignment may reveal many aspects about a gene: which regions are constrained, which sites undergo positive selection [cit], and potentially the structure of its gene product [cit] . many of these applications depend on the correct alignment of thousands of diverse sequences. a variety of methods have been developed to provide more accurate alignments [cit], yet many of these approaches are not amenable to aligning thousands of sequences in a reasonable amount of time. furthermore, performance tends to decrease dramatically beyond a certain point as more sequences are added to the input set [cit] . thus, the accurate alignment of large numbers of sequences remains an unsolved challenge that is frequently encountered in modern datasets."
"the main finding of this study is that fast secondary structure predictions can be employed in a scalable manner to counteract the drop-off in accuracy associated with aligning more sequences. this effect can be explained by the fact that structure is more conserved than sequence and therefore remains a reliable predictor even as sequences diverge greatly. secondary structure prediction algorithms exhibit a similar increase in accuracy as more sequences are used in the prediction. for example, accuracy of the gor algorithm increases by 6 % when multiple sequences are used for prediction [cit] . the same logic was applied in this study, as profiles of secondary structure predictions are progressively merged while sequences are aligned along the guide tree, resulting in improved group-level predictions that assist alignment. at the top of the guide tree, where the sequence profiles being merged are highly divergent, the secondary structure probabilities are more accurate because they are based on the entire group's consensus prediction."
"it is unclear which reference benchmark most adequately reflects a typical user's sequences, and the wide diversity of msa applications probably spans most of the alignment scenarios found in benchmarks. sabmark sets cover a narrow range of sequence identities, while oxbench focuses on closely related sequences that are easier to align. due to both alignment quality and breadth of sequence identities, i chose to continue the rest of this study with slightly modified versions of the original prefab and homstrad datasets, called prefab-mod and homstrad-mod (see methods). to supplement the modified benchmarks, i added fulllength sequences belonging to the same pfam family. full-length sequences were used rather than only the shared domain to make the alignments more challenging and to represent a greater variety of potential usage scenarios. oftentimes sequences being aligned have varying lengths because they cover overlapping regions of a gene, or were trimmed differently based on their quality scores at each terminus."
"however some frames were successfully embedded and detected. such an example of correctly detected correlation peak is shown in fig.4 . therefore, further improvement of the embedding and detecting method is clearly essential."
"this incorporates the important assumption that the electrolytes are electroneutral at any given time instance, based on the assumption that any charge excess is relaxing quickly towards the electroneutral equilibrium state. please note that, although the summed charge density i z i ec i is zero, the individual concentrations are allowed to change in space and time by virtue of eq. (1). the constraint eq. (4) can be expressed as a pde by taking the derivative in time and inserting eq. (1), yielding"
-one possible option is to ignore the original sources and fingerprint the mashup as an independent music sources. -the second option is to ignore the mashup and preserve the original fingerprints of the original sources separately.
"the lsh configurations are the same as radhakrishan's settings [cit] : testing music source is selected from sqam (sound quality assessment material) set and 8 tracks are used [cit] . table 1 shows the evaluated-source list. in this report, any source mixing arrangement is not carried out. the configurations for echo hiding are listed in table 2 . note the delay times for watermarks are set as very long as the length of 40 segments (523 ms). such a long delay echo may deteriorate inaudibility, but it is overlooked in this experiment."
"my own attempts to construct a substitution matrix based on amino acid triplets also showed signs of estimation inaccuracy. however, testing this matrix did reveal a small improvement in q-score, albeit far less than that of using secondary structure predictions. the gor (version iv) method employed here uses two matrices of parameters, one based on single residues and the other on pairs of residues, which can be accurately estimated due to their relatively small size. furthermore, reduction to a three-letter (h/e/c) alphabet that reflects an important property of the alignment enables local sequence context to be efficiently harnessed, because the contextual information only needs to be computed once per site and can then be reused under the dynamic programming approach to alignment. in contrast, using large substitution matrices requires re-computing the covariation score at every site, which is very inefficient and is not suitable for large sequence sets [cit] ."
"approximating the axon as a cylinder yields a rather simple geometry in cylinder coordinates, for which implementations exist for all of the regarded models. this enables us to simulate the extracellular action potential (eap) of a single axon, a setup that already provides sufficient complexity to demonstrate the differences of model responses."
"the following programs were compared in this study: default parameters were used for all programs with the exception of muscle and pasta, which required changing the maximum number of iterations. for muscle, \"maxiters\" was change from 16 to 2 for sets of 500 or more sequences as recommended by the developers. for pasta, the parameter \"iter-limit\" was changed to 1 for sets of 500 or more sequences. attempts to use the default value of 3 proved prohibitively time consuming on larger sets. for mafft the \"auto\" option was used to automatically switch between different progressive and iterative strategies based on the number and length of input sequences. timings for all sets were determined on a 2.2 ghz intel core i7 with 8 gb of ram using a single processor. for consistent timing comparisons, pasta and promals were configured to use only one processor."
"if the conductivity is furthermore assumed to be homogeneous, an analytical solution commonly referred to as the line source approximation (lsa) [cit] can be expressed in cylinder coordinates, where the membrane surface is collapsed to a line source. this avoids the need for a numerical solution and is computationally tractable, since one only has to compute it at the points of interest. it has also shown to give quite accurate results at distances larger than about 1 µm from the membrane in an experimental comparison [cit] ."
"the mathematical models considered are given in the following. for the pnp model, the poisson equation is defined on the whole domain. in the case of the electroneutral model (en) model, another internal boundary condition is imposed for the potential, completely excluding ω memb from the computational domain. each equation is allowed its own partition of the boundaries γ ext and γ int into dirichlet and neumann conditions, e.g. the neumann part of the external boundary for the nernst-planck equation is denoted γ np ext,n ."
"in conventional watermarking methods, it is possible to embed and detect robustly against some signal processing attacks, such as mp3 coding, noise adding and resampling. however the hashing process had never been considered."
"another notable fact is that the extracellular space is completely \"passive\" in this model. since it is derived as a special case of the en system, the electroneutrality condition is inherent. additionally, any concentration dynamics have been removed and lumped into a single (but possibly position-dependent) conductivity parameter κ. if κ is scalar and constant in space, the wellknown lsa can be used as an analytical solution, which makes it a very convenient model due to the greatly simplified solution procedure. in any case, the system dynamics are completely determined by the membrane current sources given by eq. (10), which takes the familiar form from the well-known cable equation."
"it has been previously established that the vast majority of information indicating whether to align two positions is contained directly in the amino acid pairing. this has led to the assumption of positional independence that is the primary means for efficient alignment algorithms [cit] . however, the results of this study show that local sequence context can be efficiently harnessed to further improve alignments. gor secondary structure predictions are based solely on local residues, and are therefore an indirect means of incorporating contextual information. previous direct attempts to break the independence assumption have been based on substitution matrices with quadruplets of amino acids [cit] . however, direct approaches have failed to show an improvement in alignment quality, possible due to the extremely large number of parameters required to estimate the substitution matrix of all possible dipeptides (80,200 distinct values). very large datasets such as blocks [cit] are still insufficient to accurately determine the frequency of many amino acid quadruplets [cit] ."
"the main reason for questioning the assumption of a \"passive\" and electroneutral es, however, is the effect of membrane dynamics. the membrane can be regarded as an electrochemical capacitor which attracts clouds of ions on both interfaces of the electrolytic solution. the resulting charge accumulation forms the debye layer, a very thin region around the membrane with steep concentration and potential gradients with a thickness of the order of about 1 nm under physiological conditions. this layer is affecting the potential in the vicinity of the membrane directly through its electrostatic potential and indirectly, through capacitive currents due to a dynamically changing membrane potential, e.g. during an action potential (ap). see [21, chapter 12] for a summary of the underlying biophysical theory."
"to evaluate the robustness against lsh coding, we tried to detect watermarks from lsh coded sequences of the music sources. the testing music source is embedded random payload by echo hiding and converted to the lsh code sequences."
"while vc/lsa are widely used, further research is needed to actually simulate full 3d models with realistic geometries in reasonable time. however, one should refrain from simply using the established models for a given problem and carefully consider if they are applicable to the particular situation, as our results show that the validity of the calculated potential critically depends on the membrane distance."
"in contrast, many structural benchmarks have been built from related rna or protein tertiary structures that have been superimposed to provide an empirical alignment that is free of many of the simplifications of simulated alignments. by this definition residues in the same column of an alignment should occupy the same structural position in space. a major downside of structural benchmarks is that \"gappy\" regions are typically not considered in scoring because they are not superimposable in space [cit] . some downstream applications of multiple sequence alignment may be especially sensitive to false homologies in gappy regions, such as tree building and the detection of positive selection [cit] . nevertheless, structural benchmarks have generally been preferred over simulated benchmarks, resulting in an emphasis on the maximization of true homologies in \"core blocks\" (homologous regions), with less regard for false homologies."
"furthermore, the homogeneous es with a constant scalar conductivity κ allows to use the computationally advantageous analytical lsa for the vc solution. an evaluation comparing analytical lsa and numerical vc solutions confirming the equivalence in this case can be found in [34, chapter 5] ."
"what all of these pnp-type models have in common is their increased computational demand. since existence and uniqueness of analytical solutions of the underlying systems of pdes have only been shown for certain special cases [cit], they have to be solved numerically. this requires domain knowledge for the numerical analysis and its (possibly parallelized) solution, which might be one reason that these rather intricate models have not seen a wide distribution."
"where w t and w f are the matrix length and width of an average block size in the fine spectrogram, respectively. this coarse spectrogram q is produced for every chunk."
"as a further intricacy, the potential eq. (5a) is now defined on the electrolyte domain only, necessitating the addition of internal boundary conditions, given simply as the sum of the membrane currents over each ion species i:"
"recent studies show that the extracellular space should not be regarded as a purely passive ohmic medium, as in classical volume conductor theory. the extracellular modeling community has already recognized the limitations of the established models and recently proposed enhancements to theoretical [cit] and experimental methods [cit] . in this work, we have given an overview over current modeling efforts to include influences of both capacitive membrane dynamics and extracellular diffusion on the extracellular potential."
"the similarity is evaluated in the distance norm between the vectors in the feature domain space. however, in the case that the vector dimension is high, the calculation of the norm also becomes high. therefore, the feature vector is shortened by hashing technique which fulfills both keeping the dimension small and yet providing a high level of accuracy in similarity measurement."
"this form is better suited for finite element implementations. an important modification concerns the membrane boundary conditions. for the concentrations, we replace eq. (3) by"
"msa programs are typically optimized and assessed based on their ability to recreate the alignments in benchmark datasets. in this way, benchmarks determine the objective to which alignment programs strive to attain. there is an ongoing debate over whether simulated, structural, or other types of benchmark are preferable [cit] . simulated alignments are generated by \"evolving\" sequences along a predetermined tree under a model of substitution. therefore, the complete evolutionary history of the sequences is known and the entire alignment can be used as a reference. in typical simulations, the choice of insertion and deletion rates across sites is specified, a substitution matrix is used, covariation between positions is ignored, and there is no selective pressure on the tertiary structure. furthermore, real sequence sets often include spurious (e.g., chimeric [cit] ) sequences, sequencing errors, uneven taxon sampling, rearrangements, and uneven lengths that have largely been neglected in studies relying on simulations."
"sequence pairs in the one gap database [cit] were translated and realigned with the objective of creating a high accuracy unbiased set of aligned sequences with gaps. the realignment procedure, described as follows, did not include a model of gap placement. first groups of sequences were used to create a multiple alignment. the most similar pairs of sequences with different internal gap patterns were then realigned to remove any artifacts from the multiple alignment. pairs with gaps remaining after pairwise alignment were kept, and their gaps were marked to prohibit the reuse of gaps in the same position in other pairs. this process was repeated for each protein family to generate a large set of pairwise alignments with different internal gaps."
"after deducing the models and solution methods, we are now ready to compare the computed results with each other. in the following, we will consider a single axon embedded in an extracellular bath as the simulation setup for a simple reason: both pnp and electroneutral model have not been implemented in a full 3d setup yet due to the large computational demands and the problem of obtaining a computational grid for a complex extracellular geometry."
"as mentioned before, the accuracy of a model does not come without a cost. while the lsa can be implemented easily and solved very fast, both pnp and en have to be solved numerically on a computational grid and therefore require expert knowledge and significant simulation times. detailed comparisons of the discussed models in terms of computation time and efficiency with respect to the linear and nonlinear solvers were disregarded for the sake of this study."
"multiple sequence alignment benchmarks homstrad [cit] multiple alignments were downloaded on february 20 th, 2015 from the website mizuguchilab.org/homstrad. the homstrad alignments were realigned using mustang (v3.2.1) [cit] . all other benchmarks were downloaded as part of the bench (v1.0) collection from www.drive5.com/bench. this collection includes oxbench [cit], prefab (v4.0) [cit], and transitively-consistent alignments from sabmark (v1.65) [cit] in both their original form and realigned with mustang [cit] . these benchmarks were compared (see results), and prefab and homstrad were selected for benchmarking msa programs due to their high quality and breadth of sequence identities. the selected benchmarks required slight modification before they could be used to assess the alignment of large numbers of sequences."
"i next compared decipher to pasta [cit], which is a program intended to extend the accuracy of less-scalable algorithms to large alignments. pasta works by dividing an alignment up into overlapping sub-problems that are each aligned with an accurate strategy, by default mafft's l-ins-i consistency-based approach. these sub-alignments are merged using transitivity, and the process is repeated starting from a new guide tree. interestingly, pasta outperformed decipher on sets of 125 and 250 sequences on homstrad-mod (fig. 5), but was statistically indistinguishable on larger sets (additional file 1: table s2 ). however, decipher substantially outperformed pasta on prefab-mod, and its lead increased as more sequences were aligned. furthermore, pasta showed a large drop in accuracy with increasing alignment size. table 1 shows that decipher's performance diminished the least of all alignment programs as alignment size increased."
"and boundary conditions e is the elementary charge, k b the boltzmann constant, and t is the temperature in k. together with the poisson equation for the electric potential"
"when only two sequences were aligned from each benchmark, the alignment programs all gave similar results, with mafft showing the lowest accuracy. in the sets of 125 sequences, decipher is ranked second behind mafft. for input sets of this size, mafft uses its most accurate consistency-based algorithm (l-ins-i) that is not scalable to larger sequences sets. beyond 125 input sequences, decipher clearly outperforms the other three programs (additional file 1: table s2), and its lead improves as more sequences are aligned (fig. 5) . this reflects the fact that decipher's accuracy stays relatively constant with increasing numbers of sequences (fig. 3), which is partly attributable to its use of secondary structure during alignment. clustal omega, mafft, and decipher all have similar m-scores across the range of input sizes (additional file 1: figure s3 ). muscle had the poorest performance, with substantially worse q-and m-scores for all but the smallest input sequence sets. furthermore, although q-score, total column score (tc-score), and cline shift-score [cit] sometimes give conflicting performance rankings, these three statistics strongly agreed on both benchmarks (additional file 1: figures s4 and s5) . fig. 4 contribution of local sequence context to the cost of opening a gap in the alignment. hydrophobic residues greatly decrease the likelihood of a gap, whereas hydrophilic and \"structure-breaking\" residues increase the likelihood of a gap. in the gap model, positions located within four residues were used to modulate the cost of opening a gap at position zero over-training to a single reference set has been a concern for some alignment programs [cit], although both reference sets used here showed similar results. however, other programs may be better trained on the original benchmarks that are not based on the outputs of the mustang structural alignment program. to verify that decipher was not over-trained to mustang's outputs, i repeated the analysis using the original pre-fab reference pairs, which were aligned independently of mustang. the unmodified prefab reference sequences showed strong secondary structure concordance, and therefore provide a high-quality alternative benchmark. nevertheless, the results (additional file 1: figure s6 ) were very similar for both sets of reference sequences, indicating that decipher's performance was not closely tied to mustang's outputs."
"when comparing performance, input reference sets were generated by randomly selecting a predefined number of supplemental sequences from the pool of available pfam sequences. these supplemental sequences were added to the reference sequences to reach the intended total number of sequences in each input set (between 125 and 4,000). after alignment, the supplemental sequences were removed and the remaining (reference) sequences were tested for alignment accuracy. only one randomly selected set of supplemental sequences was used per alignment size, up to the maximum number of sequences available for each set. the smallest sets of 2 sequences were created by randomly selecting a pair of sequences from each reference set. all alignments were scored using qscore [cit] with optional parameters \"-ignoretestcase -cline -modeler\". these parameters specify that only uppercase letters (core blocks) in the reference alignment are used in scoring, and that qscore should output the cline shift-score [cit] and modeler score (m-score) [cit] ."
"having successfully integrated context-awareness into the decipher software for sequence alignment, i next compared its performance to other state-of-the-art alignment programs. first, i chose to benchmark decipher against three popular programs capable of efficiently aligning thousands of sequences: clustal omega [cit], mafft [cit], and muscle [cit] . these programs are regularly employed in a variety of different studies, and have become the de facto standard for comparison on benchmarks. figure 5 shows the performance of each program relative to decipher for increasing numbers of input sequences. the performance ranking is in strong agreement between the homstrad-mod and prefab-mod benchmarks, yet there is a greater spread between programs on prefabmod because it contains a larger fraction of sequences in or below the twilight zone."
"we now turn to the diffusion layer in fig. 3, which shows deviations between lsa and the other two models. this was to be expected considering our previous theoretical analysis. in this region, the potential is influenced by ionic redistributions caused by the membrane activity. only pnp and en models are able to correctly represent these effects."
"we chose the fully-coupled approach for both pnp and en models. for optimal comparability, we carried out a pnp simulation with explicit membrane flux handling and used the obtained membrane fluxes as boundary conditions for the subsequent en and lsa simulations. this ensures that all three models use exactly the same membrane current sources to calculate the extracellular potential, eliminating any inconsistencies due to numerical errors."
"it has been shown that both pnp and en approaches allow to model various ephaptic phenomena that have been experimentally observed, but could previously not be reproduced in models [cit] [34, chapter 7] ."
"here we assume that there are no ion flows present on the membrane domain ω memb, i.e. we do not explicitly model the microscopic particle flows inside ion channels, but rather resort to the well-established approach of hh-type models and represent ion channels by effective conductances and current densities."
"there is an inherent trade-off between true and false homologies, and the results of this study advocate for the comparison of both in the development and benchmarking of alignment algorithms. while it is common to report q-score and tc-score, these two statistics are strongly correlated. in contrast, q-score and m-score are not linearly related, and beyond a certain optimum one must be lowered to raise the other. analyses of alignment performance have often focused solely on quantifying true positives (i.e., q-score), which has the potential to paint an unbalanced picture of alignment performance. similarly, the choice of alignment benchmark was carefully analyzed in this study. the results showed that not all reference sets are equally well aligned, and therefore benchmarks should be compared in addition to alignment programs [cit] . treating all benchmarks as intrinsically equivalent risks developing algorithms that are trained for the wrong goal."
"in the conventional echo hiding method, the host signal is added slight gained echo on about 10 ms delay in order to maintain inaudibility, and the echo kernels are exchanged as according to the embedding rate. robust hash echo hiding we propose is the same process as the conventional method but it adds slight echoes on about over segment unit (e.g., two segments length delayed echo is added for watermark bit '0' and three segments length for bit '1') and the embedding bit rate is about 1 bps. that is the delay times of echo are large compared to the conventional echo hiding. such a large delay time of echoes distort the audio signal. however we study the robust detection method rather than inaudibility in this report."
"next, i asked whether incorporation of secondary structure improved sequence alignment, and how this scaled with the number of sequences being aligned. averaged across all sizes of sequence sets, incorporation of secondary structure resulted in a 5.3 % improvement in q-score on prefab-mod and 2.1 % on homstradmod. this substantial increase in q-score came at the expense of a 0.4 % decrease in m-score on prefabmod and a 0.3 % decrease on homstrad-mod. therefore, the fraction of homologies that are correctly aligned slightly decreased, while the total number of correctly aligned homologies substantially increased. unsurprisingly, the largest gains were on divergent reference sets where there is the most room for improvement, and essentially no gain was made on references with less than 60 % average distance between pairs (fig. 3b) . secondary structure predictions provided a greater benefit on prefab-mod because a larger fraction of its reference sequences are over 60 % distant."
"the boundary conditions at the internal membrane interfaces deserve special attention. while the poisson eq. (2a) can be defined on the whole domain and therefore does not need any additional boundary conditions, the nernst-planck eq. (1a) is only defined on electrolyte subdomains. the membrane flux condition for species i is"
"as expected, only the pnp model is appropriate for quantitatively calculating the debye layer potential. we note that the theoretical hierarchy of models is represented directly by the subsets of the extracellular regime in which each model yields valid results."
"interestingly, the improvement from incorporating secondary structure increased as more sequences were aligned (fig. 3c) . on the smallest sets of 2 sequences there was a 3.4 % improvement on prefab-mod and 1.2 % on homstrad-mod. on large 4,000 sequence sets the advantage increased to 8.5 % and 3.3 %, respectively. therefore, incorporating secondary structure partially counteracted the decrease in score that is typically observed with larger alignments [cit] . this behavior mirrored that of secondary structure prediction, where accuracy increases as more sequences are used in the calculation [cit] . for this reason, the most accurate secondary structure prediction algorithms make use of multiple alignments. similarly, here the initial secondary structure predictions lack accuracy since they are obtained from single sequences. as more sequences are aligned, these probabilities are averaged to increase their accuracy and better guide the alignment. this is in contrast to primary sequence, where additional sequences inevitably result in more ambiguity, which in part causes a loss of signal that manifest in poor quality alignment of ambiguous profiles."
"next, i calculated log-odds scores for the residues opposing the gap (in the ungapped sequence), and found that these positions displayed a small bias in amino acid content (additional file 1: table s1 ). there was a moderate correlation between the log-odds scores for positions to the left or right of the gap and the residues opposing the gap (r 2 of 0.69 and 0.64, respectively). however, in this case there was no apparent difference between locations within the gapped region. for this reason i chose to simply modulate the gap extension cost based on the average scores for the \"gapped\" residues in a positionindependent manner. altogether, this probabilistic model of opening and extending a gap adjusts the gap penalty within a range of about +/-20 % at each position."
"the hashing process is a mapping process whereby the original signal is transformed by a random matrices set of variables, and their mapped components are fully quantized. because of that, segment-based watermarks are randomized and can't be identified autonomously."
"by using the above mentioned audio fingerprinting techniques, the music's original sources are identified in the hashing domain. however, we should consider arrangements of music source. how should the arranged music be differenciated from the original music sources? particularly, how should we treat mashup music (e.g. one made by concatenating several music sources)?"
"computational models play an important role for the analysis of complex systems like the brain. when applied under the correct assumptions, they allow to obtain results of a system with reduced complexity in order to study the influence of the essential mechanisms. consequently, computational models have established as an important tool next to experiments in neuroscience. a tremendous amount of work has gone into developing models of neurons, pioneered by the work of hodgkin and huxley [cit] for the dynamics of membrane currents and extended by rall [cit], who applied cable theory to account for the tree-like neuronal morphology. others have built upon this work to include complicated geometries, a plethora of different channel types and kinetics, synaptic currents, and more. these models based on the cable equation, which we here refer to as hodgkin-huxley (hh)-type models, are arguably among the most successful models in the natural sciences, demonstrated by the spread of the wellknown simulators for these models, neuron [cit] and genesis [cit] ."
"the hashing processes every segment by using the same hashing matrices set. while the hashed codes are randomized in a segment, the similarity relations between each couple of segments are expected to be preserved in the similarity measurement of hash codes after lsh. therefore, the watermarking embedded in the relationship between time series segments is expected to be preserved. as a such type of watermarking method, the simplest one is the echo hiding method [cit] ."
"however, if the extracellular domain is highly inhomogeneous, one should resort to the more flexible vc model and explicitly include the spatial conductivity distribution into the model, especially when conductivitydependent effects like frequency-filtering are considered [cit] ."
"to expand this model of gap placement based on local sequence context, i next investigated the effect of short sequence patterns. repeats are a major source of length variation in biological sequences [cit] and are commonly found across all branches of life [cit] . repeats have a wide variety of forms, including short microsatellite repeats of a single codon and longer tandem repeats of regions that may evolve through mutation to become mismatched over time [cit] . longer repeats can be aligned with specialized programs [cit] that employ tandem repeat finding algorithms [cit] . short patterns are typically neglected as insignificant by these programs due to their frequent occurrence in sequences. however, chang and benner [cit] found that short dipeptide repeats (e.g., aa) were more common than expected around gaps, potentially offering a means of modulating gap costs. to investigate this effect, i examined the occurrence of different sequence patterns in the one gap database."
"a further complication is given by the question of how to handle the additional system of ordinary differential equations (odes) present in the dynamic hh membrane boundary conditions. again, we have the option to split the calculation of the membrane fluxes from the solution of the main system (explicit calculation) or to include it implicitly in a fully-coupled approach. this choice does not have such a strong impact on the numerical stability, but on another matter. in the en model, when using pure neumann boundary conditions for the potential in at least one subdomain, the solution is only determined up to a constant. this problem can most easily be solved by an implicit handling of membrane fluxes."
"decipher is an r [cit] package with functions for primer design [cit], probe design [cit], and other bioinformatics tasks. in this study the decipher software was extended to include multiple sequence alignment with the function \"alignseqs\", which can align a set of dna, rna, or amino acid sequences. decipher also includes functions for alignment of dna sequences via their translation (\"aligntranslation\"), and the merging of two existing alignments (\"alignprofiles\"). see the additional file 1 text for a complete description of the decipher algorithm. decipher was written in the c and r programming languages, and is available from decipher.cee.wisc.edu or bioconductor [cit] ."
"assessment of over-alignment is one step in the ongoing effort to create more biologically meaningful alignments [cit] . other efforts have focused on specific sequence features that may be present in some alignments but are neglected by most alignment programs. this has resulted in specialist alignment programs for different mutational events, such as long tandem repeats [cit], domain rearrangements [cit], and inversions [cit] . prevalent sequence features, such as short repeats and the local sequence context around insertions and deletions, have been identified as informative, yet are largely ignored by alignment programs [cit] . in contrast, one source of information that has received significant attention is the use of secondary structure to provide a stronger biological basis for the alignment process. those programs that have integrated secondary structure predictions into alignment have shown noteworthy gains in q-score [cit] ."
"in this study, we strive to give an overview over recent progress in modeling the extracellular potential of neurons and to compare the models both theoretically and numerically. we list the assumptions underlying each of the models and the requirements imposed by the respective solution procedures. this results in a trade-off between accuracy and model complexity. we mention common use-cases and, ultimately, give recommendations on which model to use (and which not to use) in each of those cases."
"to create homstrad-mod, columns of the alignment that were in agreement between the original and mustang alignments were kept uppercase to define core blocks. therefore, homstad-mod alignments are identical to those of homstrad in the regions used in scoring. alignments with (i) less than 25 % of their length in core blocks, (ii) a total width of less than 30 sites, or (iii) having greater than 80 % average pairwise identity were removed. benchmarks were supplemented with full-length pfam [cit] sequences downloaded from each set's corresponding pfam family. the matching pfam homologous region was required to be less than three times the width of the respective reference sequences. reference sets with fewer than 100 supplemental sequences were removed. prefab-mod reference pairs were left untouched from the original prefab sequences realigned with mustang. the final benchmarks contained 717 and 399 reference sets in homstrad-mod and prefab-mod, respectively. all benchmarks created for this study are available from decipher.cee.wisc.edu/download.html."
"this should be taken into account for such models based on high-resolution electron microscopy (em) reconstructions [cit], which have become available recently. for these geometries, the en model appears promising, since the mesh generation in 3d is much easier when the debye layer does not have to be resolved."
"both pnp and en models represent coupled systems of pdes, for which no analytical solutions are known, although electrodiffusion systems have received quite some attention in the fields of semiconductor and biomolecule analysis [cit] ."
"as a result of experiment, the averaged detection bit error rate (ber) is 46.2%. all the eight testing stego signals show almost similar detection bit error rates. this is clearly unacceptably high and only slightly below the chance level."
"after lsh coding, each segment is mapped to orthogonal k-dimensional codes. when ∆ segments delayed echo was added, a similar bit is expected to appear every ∆ segments in the hashed codes. in this study, we tried to detect the watermark by finding peaks in the averaged auto-correlation of each k's dimension as shown in fig. 3"
"before lsh coding, the detection process is also the same as that of conventional echo hiding. and its robustness is inherently the same as the conventional method."
"in this study, i began by comparing the accuracy of structural benchmarks that would form the foundation for the rest of the study. next, i investigated whether it was possible to efficiently integrate secondary structure predictions with negligible added time and no additional sequences other than those being aligned. to do this i relied on less accurate, but very fast, predictions made using the gor method [cit] for secondary structure prediction. the gor method provides the probability of a residue being in helix (h), β-sheet (e), or coil (c) conformation based on local sequence context. drawing inspiration from the gor method, i created a model of gap placement that was also based on local sequence context. these features became the basis of a new program for multiple sequence alignment named decipher. finally, i compared decipher's performance with that of other popular alignment programs on high-quality structural benchmarks."
"in the third options, one can detect both the originals and the arrangement as shown in fig.1 . in this concept, the arranger s3 embeds his watermark into the arranged source before broadcasting it. in the authorization checking system, the hashed audio fingerprints are extracted from the uploaded arranged source and the segmental fingerprints identify the original sources s1 and s2 as in conventional fingerprint retrieval. moreover as in conventional watermarking method, the arranger s3's watermark is detected by analyzing the uploaded music signal before the hashing. providing that the watermarking scheme is robust against the hashing, the hashed audio fingerprints preserve the arranger's watermark and the arranger s3's watermark is detected from the hashed fingerprints."
"decipher was neither the slowest nor fastest program benchmarked for aligning each of the sequence sets (fig. 6) . mafft was generally the fastest program, except fig. 5 performance of popular multiple sequence alignment programs relative to decipher on the homstrad-mod (h-mod) and prefab-mod (p-mod) benchmarks. promals [cit] exhibited the best performance on the smallest sets of two sequences. mafft [cit] had the best performance on small input sets of 125 sequences, where it uses a much slower consistency-based strategy. muscle [cit] showed the worst performance on larger sequence sets. decipher's performance relative to other programs improved as more sequences were aligned results for the subset of reference alignments with at least 4,000 reference sequences are shown (297 alignments for homstrad-mod and 201 alignments for prefab-mod) fig. 6 average execution time according to the number of sequences being aligned (note the axis breaks and log-scale). promals [cit] was substantially slower than the other programs that do not rely on a large external database of sequences. mafft [cit] was the fastest program for large sequence sets. pasta was the slowest program tested for aligning large sequence sets, requiring an average of 2.7 h to align 4,000 sequences. a noteworthy speed improvement was obtained with decipher by using multiple processors for the smallest sequence sets where it uses slower, more accurate strategies for alignment. the change in elapsed time is dramatic for mafft and muscle beyond 250 sequences where more efficient strategies were used. pasta was the slowest program, and required an average of 2.7 h to align 4,000 sequences. both clustal omega and de-cipher were able to align 4,000 sequences in about half an hour on average. since guide tree computation is the limiting factor for large sequence sets, parallelization may be useful in such circumstances. for example, decipher was about twice as fast when 8 processors were used (fig. 6 ). decipher's maximal memory use was 2gb when aligning 4,000 sequences."
"the numerical results confirm what has been found theoretically. the extracellular domain can be roughly divided into three partitions: bulk solution, diffusion layer and debye layer. all three models are valid in the bulk solution. only the models that allow for dynamic concentrations yield valid results in the diffusion layer. this should be emphasized particularly, respecting the fact that the lsa model today is widely used without further consideration if it is a valid approximation."
"motivated by the improvement obtained from incorporating local sequence context via secondary structure predictions, i next asked whether the same approach could be applied to gap placement. previous research has revealed that insertions and deletions (indels) are more likely to occur adjacent to certain amino acids [cit] and in exposed coil regions [cit] . for this reason it is common to decrease the cost of opening a gap in hydrophilic stretches [cit], or alternatively to increase the cost in hydrophobic regions [cit] that are likely to be buried in the protein's constrained core. to my knowledge, a more sophisticated model of gap likelihood based on local context has not been applied to sequence alignment. to this end i used the one gap database [cit] to calculate the relative frequency of indel events based on the residues to the left and right of a central gap. this frequency information was then converted into log-odds scores according to the background frequency of each amino acid. figure 4 shows the contribution of nearby amino acids to the likelihood of a gap at position zero. as expected, hydrophobic residues (fmilyw) greatly decrease the likelihood of a gap. hydrophilic and \"structure-breaking\" (e.g., p) residues increase the chance of an adjacent gap, albeit with less of an effect than hydrophobic residues. since the log-odds scores are in the same units as the [cit], with upper-case letters denoting core blocks. the lower alignment shows the same sequences (named by organism) realigned with decipher and colored according to 3-state probabilities predicted by the gor method [cit] . columns of the lower alignment in bold exactly match columns of the upper reference alignment substitution matrix (third-bits), they can be directly applied to modulate gap-opening and gap-closing costs at any position based on its local sequence context (additional file 1: table s1 ). i evaluated different window sizes for including this information, and found that the best window stretched from position -4 to +4 relative to the central gap. hence, the cost of creating a gap at any position is the original gap cost plus a score that is modulated based on the residues to either side of the gap (see additional file 1)."
"the analysis shows that the pnp is valid in all three es regimes, the en model is valid in both diffusion layer and bulk solution, and vc-type models are valid within the bulk solution only."
"variety of end-of-life: end-of-life characteristics play a prominent role at any given stage of a value chain. from a top-down approach, a process that is capable of utilising waste biomass for raw feedstock, also called \"cascading use\" is a valuable, sustainable business model as there will a regular influx of low-cost feedstock, promising a continuous product supply to the market. [cit], 10, 1695 9 of 24 \"bottom-up\" approach, strategic management and utilisation of waste (post-product consumption) is capable of delivering three-fold benefits: environmentally through reduction of waste for treatment and disposal; economically by enabling resource efficiency and through transformation of waste (as low-cost raw material for a secondary industry); and socially through creation of jobs, new value chains and social equity [cit] . to be able to \"catch up\" [cit] eu landfill directive (which aims to phase out landfilling recyclable waste, e.g., bioplastics, paper, glass and bio-waste), we need to identify candidate value chains that generate products that can potentially circularise the value chain. selection of value chains based on the capabilities of the products to demonstrate a variety of end-of life characteristics would be valuable to report via this study."
"to meet the growing data rate demand for mobile data in urban areas, network densification is the best possible solution. the network densification can be achieved by deploying low-power small cells as they maximize the capacity by reusing the same frequency band. the small cells with moving capability can also fulfill the emergency communication requirement by deploying them in disaster hit areas."
"on the other hand, the ratio of the power losses ∆ evaluated with respect to and the power losses ∆ evaluated with regard to the maximum power ( ), can be deduced from the load factor using the formula [cit] :"
"multi-sector application: the ability of a bio-based product and its value chain to cover a range of applications (in different industrial sectors) was identified as an important criterion for value chain selection. undertaking this task for value chains with products that serve a rather smaller demand/specialised demand could make this study highly specific, deviating from the aim of creating a harmonised sustainability framework for horizontal sector application. therefore, focus is placed on value chains and bio-based products that have the potential to be applied in a variety of sectors (e.g., bio-based mulch film catering to agricultural/horticulture industries and other industries like the landscaping industry; versatile fine chemicals that find application as solvents in paints and coating, adhesives and binders, fuel additives and agrochemicals)."
"author contributions: the concept of analysis, methodology, data acquisition and interpretation and development of the manuscript were all jointly undertaken by k.l., l.l. and l.s."
"contributions: motivated by this, we propose a public safety users connection priority-based context-aware resource allocation (ps-cara) scheme. the proposed ps-cara scheme has the following differentiating characteristics: (1) it uses the context information (that is, public safety user priority and their location information) to allocate the resources among users. (2) moreover, ps-cara scheme dynamically adjusts the mpc spectrum access ratio η based on the number of public safety users, which in turn increase the users' throughput. subsequently, we further divided the available resource blocks into three parts, that is, for access links, backhaul links, and sidehaul links. (3) we also consider uplink (ul) power control scheme to reduce the interference among users. (4) moreover, we consider two major factors for user association (a) priority indicator and (b) minimum pathloss factor [cit], and hence it helps ps-cara to reduce the call-blocking probability and interference among the users, and increase the public safety user throughput. we summarized the list of key mathematical symbols in table 1 . user association indicator for user u associated with j-th base station l, f, s pathloss, fading, shadowing, respectively p o signal-to-noise-ratio (sinr) target control parameter α pathloss compensation factor η mobile personal cell spectrum access ratio δ priority indicator ρ load distribution ξ call blocking probability γ sinr"
"for longer lines, starting from a certain line length l 3, the steady-state stability limit starts determining the loadability curve (note that, in compensated lines, this line length can be much lower compared with the base-case of uncompensated lines). thus, the loadability limit is determined by the voltage drop and steady-state stability limits. in this region, both the active and reactive power of the compensator decrease starting from the maximum value of complex power. if the maximum reactive power the synchronous condenser can deliver, q max, is less than this value, the sequence of the limits can change."
"we compare the performance of the proposed ps-cara scheme with the conventional static resource allocation (c-sra) scheme, which ignores the context information during resource allocation."
"note that, assuming different values for the power losses limit ∆ ( ) and the load factor, the joule losses limit could prevail over the voltage drop limit reducing the loadability curve after a certain line length that can be individuated by the methodology described. the same is valid in the case of a more stringent setting of the steady-state stability limit."
"the second round of assessment was initiated with the collation of information and analysis of national policies, bioeconomy initiatives and growth plans established by individual eu member states. this review provides an insight into the bioeconomy strategy adopted by individual states based on their strengths such as natural bio-resources, preference for bioeconomy development, access and development of technological innovations and maturity level. a summarised list of initiatives and action plans associated with each of the eu member states is presented as a part of the supplementary information."
"despite the apparent complexity of the general analytical description, we demonstrate that the practical application of this analytical approach is rather simple. in both case studies, the loadability curves can be easily derived, avoiding the need to formulate the loadability problem in terms of a constrained optimization problem, whose solution implies an adequate procedure of implementation on a digital computer and a major computation burden."
"in particular, bio-based solvents are broadly classified into plant-based alcohols, diols, organic acids, glycols and many more. from an economic perspective, according to the above mentioned report [cit] the global bio-based solvent market was worth roughly 6 [cit] and it is currently projected to grow at a cagr of 7.8%, reaching 9 [cit] . the versatility of bio-based chemicals, particularly bio-based solvents (for example, in pharmaceutical, cosmetics, agriculture, cleaning, printing inks and adhesive applications), and demand/room for innovation and product development, coupled with stringent regulations on hazardous pollutants released from the use of conventional chemicals have fostered increased research interest and financial investment via national programmes and government support. moreover, the feedstock variety that can be used to generate a myriad of bio-based chemicals makes these value chains innovative and techno-economically viable, in addition to their improved environmental performance."
"the complexity of the proposed ps-cara scheme is reduced as compared to the existing schemes because we separately allocate resources for ps and non-ps users, and hence the resource selection competition is separately among ps and non-ps users. therefore, the proposed ps-cara is a sub-optimal scheme that requires a modest complexity, and it has a linear relationship with the number of rbs as compared to the huge number of possible combinations regarding the np-hard problem. since we have a distributed network and each node decision is independent of the other node, this results in less system complexity."
". joule losses are an economic problem rather than a distinct limiting factor in line loadability. thus, any limit concerning joule losses should involve the energy lost in a given time period (for example, one year), whereas the instantaneous power losses corresponding to a certain power transported have scarce or no practical meaning [cit] ."
"a value-chain is defined as a set of interlinked activities that deliver products/services by adding value to bulk material (feedstock). in a bio-based value chain, the feedstocks tend to be biomass drawn from an existing primary production route (e.g., agriculture, forestry and livestock), or of a novel (e.g., microalgae) or secondary origin (e.g., sludge, industrial wastewater and household organic waste). a generalised schematic for an ideally circular bio-based value chain has been presented in figure 2 . value chains, in particular those that valorise secondary resources are designed to turn available organic material into different valuable product, ranging from high-value chemicals to secondaryuse by-products and renewable energy [cit] . pathways that are capable of transforming waste/secondary feedstock into an array of high value products are called integrated biorefineries [cit] . integrated biorefineries contain a \"pre-treatment plant\" that prepares the feedstock for upcoming transformation and refining technologies within the supply chains, before packaging and distribution."
"the five bio-based value chains chosen from the second round of assessment have been adopted to be mapped for full (general) coverage of the resource flows, technology/conversion routes employed, the various stakeholders and the fate of the products or the other waste streams that may result from the value chain. these schematics have been presented in figures 6-9 ."
"escalating environmental and economic pressure to use our resources responsibly and add value to the used material/products in the commercial sphere has helped the development of technology routes and material circularity in nearly every global sector. according to the eu circular economy strategy, the aim of such systems thinking is to \"close the loop by becoming resource efficient through development and establishment of industrial symbiosis, to reduce the pressure on eu's natural capital\" [cit] ."
"as we show in appendix b, the ratio ∆p p(l) can be explicitly evaluated as a function of p and l. from the methodological point of view, however, the problem is to identify a significant criterion for setting a limit to the ratio ∆p p(l) . joule losses are an economic problem rather than a distinct limiting factor in line loadability. thus, any limit concerning joule losses should involve the energy e j lost in a given time period t (for example, one year), whereas the instantaneous power losses corresponding to a certain power transported have scarce or no practical meaning [cit] ."
"this paper proposes a new approach for the analytical description of the various regions of the loadability curves of overhead transmission lines. using the complete line model with distributed parameters and the relevant general formulation, we show how the loadability curves of any actual line can be deduced taking into account the conductor thermal limit, a maximum voltage drop across the line, a maximum amount of joule losses along the line, and a steady-state stability margin."
"a value-chain is defined as a set of interlinked activities that deliver products/services by adding value to bulk material (feedstock). in a bio-based value chain, the feedstocks tend to be biomass drawn from an existing primary production route (e.g., agriculture, forestry and livestock), or of a novel (e.g., microalgae) or secondary origin (e.g., sludge, industrial wastewater and household organic waste). a generalised schematic for an ideally circular bio-based value chain has been presented in figure 2 ."
"limitations: mcda can be a valuable tool, however, there are a few concerns when it is applied to immensely complex systems. when a process system involves social interactions such as producers, regulatory authorities and consumers, it becomes challenging to call one perspective as more important than the other. mcda is also capable of overlooking or under-representing some key factors within a complex value chain. an essential and independently functional element within a bio-based value chain, eco-system services, is one such example. it is essential to be able to apply mcda for smaller scale analysis to be able to predict all possible determinants of a particular chain-stage prior to its broader application. however, this can be time-consuming. input/output (io) analyses, which is a methodology that involves monitoring the sectoral trade data to quantify the complex interactions between the different nodes of a given value chain may also be utilised to study the dynamics of a value chains in real-time. however, its data needs depend on statistical information drawn from datasets published by government and international authorities (un-faostat, eurostat) [cit] and the data may not always be available. besides being data intensive, it may not always be possible to derive data for bio-based value chains based in rural communities, with io methodology."
"since we are also dealing with connection level priority instead of a service level priority model, call blocking probability (cbp) is a good performance metric to judge the quality of the system. the cbp measures the call admission rejection probability because of the limited available resources. the cbp (ξ) of a user u trying to associate with bs can be given as"
"upon collation, analysis and categorisation of these initiatives, the preference of these member states over the choice of feedstock, bio-refining technology, current and desired products/sector development and techno-economic or social optimisation route were identified and ranked. this information was used to calculate weighted scores called \"preference scores\" to specific (feedstock-conversion route-bio-product combinations, based on the preference demonstrated by the eu-collective bioeconomy strategies. these scores, (as a % of total number of strategies), have been presented in table 3 . information on most of the eu-relevant bioeconomy strategies and initiatives collated and analysed as a part of the second round of assessment, is presented in the supplementary section in table s1 . the outcomes of this second round of assessment have also been discussed further under section 3.1."
"hence, the solutions to these problems are not feasible and only a sub-optimal solution can be achieved. to solve these problems, we propose a ps-cara scheme that heuristically solves the constructed optimization problem (p1) in (12) and (p2) in (14) by system-level simulations. by these simulations we found the sub-optimal solution to these problems. the proposed ps-cara scheme's major steps are summarized in the next section."
"the performance of the proposed ps-cara scheme is tested under the ps scenario. the ps-cara scheme is verified for three possible cases: (1) ps-cara: ps and non-ps users without power control, (2) ps-cara: with all users using power control, and (3) ps-cara: ps ue without power control and non-ps ue with power control. the reason for simulating these cases is to check the suitability of the ps-cara scheme under different power control situations, and then to find the best suitable case."
"ps and non-ps users fpc implementation: initially, all users are deployed in a system by allocating full transmission power. this may result in high interference from the neighboring users. to control this situation, fpc is implemented that allocates appropriate power to users by considering their locations, which in turn will reduce the interference."
"when the thermal limit is attained at the sending end, the maximum allowable active power p(l) can be calculated by solving the following two-equations system:"
"in this section, our objective is to maximize the system sum-rate of ps and non-ps users connected with either mbs or mpc by considering the sinr, transmit power, and priority constraints. thus, we propose the sum-rate maximization problem (p1) as"
"the goal of this section is to show the ability of the proposed analytical methodology to derive the loadability characteristic of ohls. calculations are performed for the base-case of uncompensated ohls and for the specific case of shunt compensated radial lines. in the second case, we assume that a synchronous condenser is connected at the receiving end of the ohl, as shown in figure 6 . the limits considered in both cases are:"
since our target is to associate users based on the loading conditions of bss. the total load on k-th mbs and m-th mpc is calculated as
"the first stage of a complex bio-based value chain is biomass availability. [cit] define the quantity of biomass, generated by a confined area of land (country) that is currently used or has the potential to be used as the \"biomass potential\" [cit] . biomass potential may be measured from a number of relevant sustainability-based angles: theoretical, environmental, economic and sustainable. theoretical potential provides an estimation of potential biomass productivity based on the physical characteristics of all available arable land. environmental and economic biomass value chains, in particular those that valorise secondary resources are designed to turn available organic material into different valuable product, ranging from high-value chemicals to secondary-use by-products and renewable energy [cit] . pathways that are capable of transforming waste/secondary feedstock into an array of high value products are called integrated biorefineries [cit] . integrated biorefineries contain a \"pre-treatment plant\" that prepares the feedstock for upcoming transformation and refining technologies within the supply chains, before packaging and distribution."
"\"top-down\" and \"bottom-up\" initiatives: stronger policy-level improvisation via \"top-down\" approaches drive chain-actors to encourage good practice such as producer responsibility (e.g., packaging industry) and deploy \"re-direct used material\" strategies. similarly, \"bottom-up\" approaches, such as altering consumer behaviour via responsible and innovative retail design and practices, must also be devised. this may also include incentives/loyalty schemes for consumers who opt for bio-based products that upon consumption can be recovered and re-introduced into another product."
"\"food vs. non-food bio products\" conflict: we live in an era of a global lack of food security, the supply chain of which alone is highly complex and fraught with unforeseeable risks such as crop failure from climate change or geo-political instability. to add to this, use of agro-food based biomass as the starting feedstock not only puts the bio-based business model at risk but also invites \"food vs. bio-based products\" conflict, undermining the sustainability characteristics of a bio-based value chain. however, substituting primarily food-based feedstock with waste-based feedstock or by-products would incorporate material circularity e.g., starch rich feedstock can be drawn from food retailers and food processing industries by using vegetable peels instead of starch rich crops. this strategy not only encourages waste valorisation but also reduces both the risks involved and the dependence of bio-based industries on the primary food supply chains."
"the second region extends up to the line length at which another limit is attained. therefore, as made above for uncompensated lines, the attainment of another limit must be checked. in this case, the check concerns the attainment of the thermal limit at the sending end at the length . as already said, at this line length the currents at the two line ends are both equal to the thermal limit. analytically, at the following constraints are attained at the same time:"
"equation (13) this procedure converts a limit to the lost energy into a limit to the (instantaneous) power losses ∆p, calculated with regard to the loadability limit p(l). examples of application are shown in section 4."
the second region extends up to the line length at which also the sending end current achieves the thermal limit ( figure 12 ). this length corresponds to point depicted in figure 9 .
"multi-criteria decision analysis (mcda) is a valuable tool for decision making in complex process systems using multiple parameters that influence the embedded processes within a value chain. these parameters can be differently weighed as \"significant factors\" by the various chain-actors. also, incorporating this flexibility into the scope of this assessment, mcda in decision making enables a systematic investigation and transparency in analysis [cit] . the goal of mcda, in general, is to provide an opportunity to explore the knowledge and concerns put forward by the chain-actors, weigh them from an unbiased viewpoint, systematically analyse, identify the most important criteria, and subsequently, make decisions within a complex multi-actor process systems. this study employs two-tier mcda to rank bio-based value chains based on a set of selection criteria, highlighting the significance of their adherence to the principles of circular economy. the outcomes of this analysis highlight the importance of these selection criteria dedicated to highlighting the circularity characteristics of any bio-based value-chain/business model. an elaborated mapping methodology to understand the strengths, weaknesses, opportunities and challenges embedded in a bio-based value chain, attributable to the synthesis of a variety of bio-products, also demonstrating the significance of upstream processes and material use on the downstream activities (mainly post-consumption and end-of-life management) has been presented as a part of this paper. please see figure 4 for a flow diagram that elaborates the mcda methodology employed in this study for value chain selection and mapping."
"according to the second line of equation (1), the modulus of the current changes along the line. in the first region of the loadability curve, characterized by the incidence of the thermal limit, the modulus of i 1 (the current at the sending end of the line) is slightly lower than that of i 2 (the current at the receiving end). the same is valid for the modulus of the current at any point along the line. since the first region concerns lines with limited length, the differences in the current modulus are little (by far less than 1%) and are usually neglected. actually, the thermal limit is attained at the receiving end of the line (see section 4.2 for more insights)."
"a methodical approach for the selection and mapping of the most promising circular bio-based value chain is presented in this paper. the purpose of the methodology expressed in this paper is to having comprehended the synergies between a circular economy and a bioeconomy and its potential for contributing to the global sustainability targets from a number of earlier studies [cit], it is crucial to understand where we stand to take appropriate and smarter \"next steps\". prior to exploring the opportunities that bioeconomy could offer, the probability of it being successfully established must also be systematically investigated."
"the maximum allowable active power, therefore, must satisfy the inequality: the cascade system in figure 5 can be described by the equivalent transmission matrix t:"
"the simulation results depict that the proposed ps-cara scheme results in very low interference for case 2, where power control is applied by considering the location and deployment situation of ps and non-ps users. the results in figure 7 clearly demonstrate that by applying ps-cara with power control, around 70% of total users lie in between the low interference range of −100 dbm to −80 dbm, whereas for the ps-cara without the power control scheme, only 3% of total users are in the specified low-interference range. furthermore, we individually plotted the effect of interference on ps and non-ps users for the three cases described above. from the simulation results shown in figure 8 we can clearly notice that the case 2 ps-cara, with all users using power control, is the best possible solution to reduce the interference. hence, based on these results, we conclude that ps-cara with power control is the best possible option in the public safety scenario as it reduces the interference remarkably."
"the mobile mpcs are modeled in simulations by considering the random-walk mobility model. in figure 5, we plotted mpc moving path by using random-walk mobility model. we considered this model because it is suitable for low speed mobile networks. the other reason for opting for the random-based mobility model is because here the mpc can move randomly without any restrictions. to be more specific, each mpc speed, destination, and direction is chosen randomly without relating other mpc. hence, the random-walk mobility model is considered to model the movement in the public safety scenario. the performance of the ps-cara scheme is considerably improved as compared to the c-sra scheme because of implementing fraction power control scheme. the selection of power control parameter is very important to provide the better performance, and its selection varies for each environment. the main parameters of concern in fractional power control are; sinr target control (p 0 ) parameter and path loss compensation factor (α). the improper selection of these parameters can result in high interference and coverage degradation which in turn will degrade the system sum-rate. thus, we need to select the value which gives us maximum coverage but at the cost of less interference."
"multi-regional supply chain: a multi-regional value chain, besides adding value to a low-value feedstock, also contributes to the economic growth of dependent communities via creation of jobs, development of skills and the knowledge pool of the local communities, leading to improved community wellbeing and social equity. such approaches require a harmonised approach to reporting and communication of information among the embedded chain actors for transparency on practices and traceability of materials. however, such an approach could facilitate eu states with transition economies to establish bioeconomy models, with the needed investment from national funding initiatives [cit] ."
"analytically, considering that ( ) must be intended as known (see figure 3 ), this ratio can be calculated, for any given length, as"
"agricultural waste transformed into green, low-environmental impact insulation material was the conventional technology until the discovery of fossil resources, which gave rise to relatively inexpensive polymers and materials (e.g., polyurethane, mineral wool). however, some environmental and human health concerns are associated with these insulation materials (from long-term release of aerosols and vapour) such as respiratory issues and eye and skin irritation, particularly in the case of foam insulations. natural fibre insulation such as cotton wool and wood fibre boards have been identified to perform similarly to their petro-derived counterparts and are particularly advantageous with regards to complying with any environmental building certification schemes [cit] . bio-based binders and other additives, such as polylactic acids (pla) and polyhydroxyalkanoates (pha) generated from other starch-based value chains, may be utilised in the preparation of these insulation materials. dry lignocellulosic biomass can also be processed into compressed fibres for dashboard panels, geotextiles and animal bedding [cit] ."
"all this considered, we set a limit to the ratio ∆p m p m, where ∆p m is the power losses calculated with regard to the average power p m . finally, using equation (12) and the definition of f c, we can write"
a two-tier multi-criteria decision analysis was undertaken to identify and select the most promising value chains and the outcomes of this assessment have been presented in table 4 .
"bio-plastics, bio-based solvents, bio-lubricants, fabrics and fine chemicals followed by bio-based insulation material were chosen to progress to a second round of assessment. within the second round of assessment, they were subjected to a similar weighted scoring, set against a background of eu-wide bioeconomy initiative. this \"bioeconomy preference score\" is primarily based on the target-feedstock and technology preferences of the bioeconomy initiatives and other relevant sustainability schemes established/planned with an active interest to transform from a linear economy to circular bio-based economy. in terms of feedstock preference, eu member states seemed to possess a clear strategy on utilising feedstock generated locally or nationally with minimal logistics and not demanding an additional stream (land-conversion)/infrastructure for feedstock generation (in other words, use excess and residual biomass). as a result, a majority of the initiatives highlight a feedstock preference in the following order: agricultural (63%), forestry (35%), waste stream (organic waste from domestic and commercial waste) (25%). in terms of initiatives, there are those that either focus on pursuing innovative technology routes or prefer a combined approach to utilising biomass with innovative biomass transformation technologies. preference for bio-based value chains based on the nature and goal of the initiatives assessed as a part of this study was identified and ranked in table 5 . it is evident that there is a greater emphasis on development and exploitation of bio-based chemicals and bio-plastics in the conceived bioeconomy agenda and the existing bio-based infrastructure. there is a huge array of bioplastics, classified under roughly 10 categories, that are prevalent worldwide [cit] . currently, there is a focus on bioplastics that can be synthesised from one important feedstock, starch. this is due to the current availability of a mature and commercial conversion route that provides a feasible solution to existing \"plastic waste management issues\". in addition to this, the multi-functionality of bioplastic under study (pla synthesised from starch), which (in combination with other polymers) may be utilised to create mulch films, disposable cutleries, framing materials and fibres defines its suitability to address the current challenges facing our transition to a fully-functional bioeconomy. similarly, bio-based chemicals such as solvents, lubricants, dyes and pigments offer a greener and relatively low-environmental impact alternatives to their conventional counterparts facing restrictions of use from regulatory bodies such as reach (registration, evaluation, authorisation and restriction of chemicals) and echa (european chemicals agency) [cit] . in addition to their lessened impact in terms of eco and human toxicity, their promising potential to create opportunities for socio-economic growth and reduce dependence on non-renewable resources creates a sustainable pathway for development. the construction sector is one of the biggest contributors of landfill waste in the eu owing to the design flaws in the product. construction materials (particularly contemporary insulation material) are seldom created for any kind of responsible end-of-life management [cit] ."
"where s j is the number of user scheduled during the j-th subframe and u is the total number of user sending the admission request. in this paper, we also minimize the cbp of the users associated with the bs, by considering the user association and priority constraints. thus, cbp minimization problem (p2) can be evaluated as"
"when the thermal limit is attained at the sending end, the maximum allowable active power ( ) can be calculated by solving the following two-equations system:"
"inspired by this, we presented 3gpp lte-a based mobile personal cell (mpc) that follows users to meet their emergency data rate and latency demands. the mpc deployment can meet the public safety low latency and high priority requirements as it can expeditiously establish the communication link among the base station and public safety emergency users. however, mpc resource allocation poses some serious challenges because users frequent movements rapidly changes their location in disaster situations; this results in rapidly fluctuating channel conditions. moreover, in disaster hit areas users also needs priority in connection establishment over the conventional users. so, the conventional static resource allocation schemes designed for low mobility or static environments are not suitable for varying channel conditions [cit] . therefore, mpc requires context-aware resource allocation schemes that can allocate resources among users using context information such as their location and connection establishment priority. moreover, to efficiently manage the context-aware schemes, we need centralized information of the resource allocation."
"gaps in sustainability schemes: from assessing the outcomes of the literature review, it is evident that sustainability schemes for bio-based products (e.g., bioplastics, bio-solvents, bio-based adhesives and binders, enzymes and cosmetics, etc. but not bioenergy) are either still in their infancy or have variable levels of maturity with major sustainability related gaps to cover. some major gaps and limitations include a lack of clear criteria for sustainability/circularity assessment of bio-based products on one hand and on the other hand, an overlap in the existing certification schemes. for example, for the cen standards for bioplastics (cen/tc/249), some of the sustainability criteria such as the determination, declaration and reporting of the bio-based carbon content [cit] are required via the following standards: however, these standards do not explicitly direct the economic operator to take further responsibility to address/quantify the sustainability criteria associated with bioplastics including production derived emissions to air, water and soil or economic and social impacts. a discrete set of standards is under development by the technical committee (cen/tc/411) for bio-based products to report the sustainability aspects of bio-based products [cit] . these standards are responsible for the determination, declaration and reporting of environmental impact assessment (e.g., en16751: bio-based products: sustainability criteria). the scope of en16751 in particular, despite providing guidance on undertaking impact assessment and reporting on bio-based products, covers the stages from feedstock acquisition up to the feedstock \"pre-processing\" phase. lack of guidance on assessment and reporting of environmental burden resulting from \"manufacturing\" to \"end-of-life\" phases, and lack of assessment methodologies and thresholds are some of the major gaps and limitations in these standards."
"for this study, the mapping has been carried out to highlight, in general, probable material, wastes/emissions, conversion/refining routes associated with a given feedstock and end bio-product synthesised from it. these maps do not provide explicit information on coverage of these value chains by specific sustainability schemes/certification programmes as there are diverse products and co-products that could be produced as a part of the value chain. to establish this level of detail, the goal, scope and the product of analysis would have to be established beforehand."
"penetration of non-bio-based value chains: a recent shift in consumer behaviour, challenges to penetrating non-bio-based value chains (existing fossil-based supply chains) owing to consumer perception of \"brand-value\" and relatively cheaper products need to be considered. however, the trend is changing as there is a growing demand among consumers in small businesses owing to the \"trust\" factor that they are mainly passion driven rather than profit-driven [cit] ."
"secondly, each of the selection criteria were allotted weighting factors based on their relevance and significance to the principles of material circularity and bioeconomy transition. the weighting factors are presented in table 2 . the criteria that have been allocated a weighting of 0.2 are those that directly contribute to innovative or under-represented but resource efficient value chains which subsequently have the potential to encourage the establishment of a circular economy. criteria with a weighting of 0.1 (preference within eu member states) has been covered further with an elaborate evaluation under second round assessment. relevant characteristics of each of the bio-based value chains were assessed and reviewed against the weighted selection criteria to finally assign ranks from the first round. the outcomes of the first round of assessment and further discussion on this approach to the identification of promising bio-based value chains have been presented in section 3.1."
"note that, assuming different values for the power losses limit ∆p m p m (l) max and the load factor f c, the joule losses limit could prevail over the voltage drop limit reducing the loadability curve after a certain line length that can be individuated by the methodology described. the same is valid in the case of a more stringent setting of the steady-state stability limit."
"the gradual and moderate reduction of the loadability curve ( figure 11 ) in the second region (114-276 km) is due to the thermal limit and to the progressive reduction of the line power factor: the increasing reactive component of the current, injected for compensation, reduces the active current in the line."
"the solution of equation (27) is graphically represented by the intersection of the circles c1 and c3 illustrated in figure 10 . clearly, the maximum allowable active power is determined by the intersection point of c1 and c3. figure 10 . power circles related to the voltage drop limit (c 1 ), thermal limit at the sending end (c 3 ), and power losses limit (c 4 )."
"it must also be underlined that the power transfer limits of ohls have been studied with reference to both compensated and uncompensated lines in the old paper [cit], at a time when the concept of line loadability had not been developed yet. this paper proposes an analytical representation of the loadability curves of ohls. this is a new approach to characterize the various regions of the loadability curves, completely different from the traditional approach based on numerical analyses. the closed-form expressions derived take into account the constraints related to the conductor thermal limit, permissible voltage drop and joule losses, as well as the steady-state stability margin. the proposed analytical approach allows obtaining an original and simple geometrical tool, based on circular diagrams, whose interceptions show the influence of the different limits. the contribution of this paper can help power designers and system operators in both planning and operation stages of ohls. section 2 illustrates the analytical representation of the loadability curves of uncompensated ohls. section 3 is dedicated to the analysis of radial ohls compensated by means of synchronous condensers connected at the receiving end. this is a much less frequent case, investigated here in order to show the potentiality of the new analytical approach. section 4 has the aim to demonstrate the applicability of the analytical approach and the use of the relationships provided in the previous sections. the application examples developed refer to standard 400 kv single-circuit ohls."
"the performance of the proposed ps-cara scheme is compared with respect to the average (50%) and edge throughput (5%). the simulation results shown in table 4 clearly demonstrate the effect of adding the mpc to the conventional mbs system. the simulation results clearly demonstrate that by deploying mpc, the average and edge throughput increases by around 10.3% and 32.8%, respectively. moreover, we also notice that by deploying a greater number of mpc per mbs, the average and edge throughput of the system increases. the average and edge throughput increases by deploying a greater number of mpc because it reduces the call blocking probability because of having additional resources for the emergency users. however, there is a chance of an increase in co-channel interference, but that is countered by using the ps-cara scheme which has the capability of reducing the interference because of introducing power control."
"value chain maps provide valuable insight into the integrated activities, actors and technology, in addition to the material flow, and have provided a foresight of the scope and qualitative performance potential (in socio-economic and environmental terms) within each of the life cycle stages. inability to acquire real-time information from any changes to the process dynamics may be a limitation. however, for a preliminary assessment, value-chain mapping provides an overall breakdown of the various elements presented above. the methodology prescribed in this paper not only helps understand the embedded complexities and overcome them within bio-based value chains but are also equally applicable to complex non-bio-based supply chains that foresee a transformation by closing their loop to encourage product and process circularity. overall, the suggested method could be used by policy makers, investors, business groups and economists to understand the interdependencies among the various embedded chain-actors, dependence of the chain on material consumption and the various technology routes available for innovative and sustainable production of bio-based products that can potentially replace high-impact fossil-derived products."
"equation (13), as a function of the limit ∆ ( ) and the load factor . therefore, once the load factor and the limit this procedure converts a limit to the lost energy into a limit to the (instantaneous) power losses ∆, calculated with regard to the loadability limit ( ). examples of application are shown in section 4."
"from the methodological point of view, however, the problem is to identify a significant criterion for setting a limit to the ratio ∆ ( )"
"where in (9), the term (r + (1 − η)r) represents that the mbs users (mue) can access some rbs of mpc based on η. if enough free spectrum of mpc is available then η reduces, and non-ps users can also access the freely available mpc spectrum."
"supplementary materials: the following are available online at http://www.mdpi.com/2071-1050/10/6/1695/ s1, table s1 : bioeconomy initiatives and strategies analysed for the second round of the value chain assessment and selection."
"concerning joule losses along the line and considering that this paper deals with the maximum permissible power p(l), a natural approach could be to fix a limit to the power losses ∆p calculated with regard to the loadability limit p(l). better, looking for homogeneity with the percent voltage drop limit, we could fix a limit to the ratio between ∆p and p(l)."
"in view of the commercial, environmental and socio-economic potential identified from the value chains selected from the second round of assessment, they were adopted for an elaborate value-chain mapping in the section 3.2."
"the market for bio-based chemicals in general is worth $6 billion and at a projected annual growth rate of 16.16%, the market is expected to reach $27 [cit] . bio-based chemicals include a broad spectrum of products, which may be classified as commodity chemicals, intermediate chemicals and specialty chemicals, based on their application. commodity chemicals refer to the \"high volume-low value\" products, sourced from biomass (but not restricted to), such as fatty acids, methyl esters and alcohols. intermediate products refer to the refined sugar complexes, basic polymers, pigments/dyes, plant oils and other types of starches. specialty chemicals, synthesised either independently from plant or prepared from intermediate chemicals includes bio-based chemicals such as advanced polymer solvents and other preparations for final formulation in personal care products, pharmaceuticals, paint coatings, additives, domestic/industrial detergents and other applications."
"multi-regional supply chain: a multi-regional value chain, besides adding value to a low-value feedstock, also contributes to the economic growth of dependent communities via creation of jobs, development of skills and the knowledge pool of the local communities, leading to improved community wellbeing and social equity. such approaches require a harmonised approach to reporting and communication of information among the embedded chain actors for transparency on practices and traceability of materials. however, such an approach could facilitate eu states with transition economies to establish bioeconomy models, with the needed investment from national funding initiatives [cit] ."
"as far as the steady-state stability limit p lim is concerned, one can refer to the equivalent system shown in figure 5 . all this considered, we set a limit to the ratio ∆, where ∆ is the power losses calculated with regard to the average power . finally, using equation (12) and the definition of, we can write"
"interconnectedness of value chains: uneven distribution or complete lack of technological readiness for a multi-regional/local value chain to be established is a key hurdle in transformation to a bio-based economy. this requires creation of awareness and a strong social connection between inter-chain stakeholders to encourage technological and operational coherence. the thought-process among the different chain actors (particularly among those involved between the feedstock procurement to packaging stage) has to be parallel with each other. family businesses embed such wider thoughtful thinking and are mostly successful in staying established on a long-term, despite various external shocks, particularly from low-demand, inflation, etc. the key aspect to note is the trust developed with fellow stakeholders and customers with the consistency and quality of product. occasional lack of co-operation due to process-level disparities between the embedded stakeholders of the value chains could be overcome via the establishment of national standards, covering stakeholders embedded in a value chain, with standardised templates for recording, reporting and communicating information."
"before implementation of the ps-cara scheme, it is important to analyze the complexity to prove its suitability. the ps-cara scheme summarized in figure 4, where initially the proposed scheme allocates the dedicated resource to al, sh, and al links. after the environment selection phase, we have iterations that decide which two users share the current rb."
"public perception of bio-based products: [cit], the general public's perception of bio-based products are highly variable [cit] . as much as there is keenness to switch to bio-based products owing to their overall environmental benefits, the sporadic market supply, the expensive nature of such products and sometimes, limited functionality and durability of the bio-based products seem to hinder their acceptance. for members of the public who choose to opt for a more sustainable life, the existing product end-of-life management infrastructure is partially to fully unsuitable to pursue a circular waste management practice, leading to an overall skepticism. therefore, a holistic approach is needed in our transformation to not only a bio-based economy but also to a circular economy."
"the solution of equation (27) is graphically represented by the intersection of the circles c1 and c3 illustrated in figure 10 . clearly, the maximum allowable active power is determined by the intersection point of c1 and c3. figure 9 . intersection of the power circles related to the voltage drop limit (circle c 1 ) and receiving/sending end thermal limit (circles c 2 and c 3 respectively)."
"first round assessment and selection: a dedicated matrix composed of a combination of the preliminary value chains which are to be assessed from the viewpoints of each of these selection criteria was developed as a part of this first-tier analysis. since this study is a part of a project with a wider vision, the expertise of the consortium members in bioeconomy was invited. their recommendations in terms of the performance of the set of preliminary value chains, in combination with the set of selection criteria, presented in figure 5, was obtained and reviewed. scores, in the form of rating (scale of 1-10), were allocated to the recommendations (yes/maybe/no) with justification provided by the consortium members."
"this paper presents the public safety priority-based context-aware resource allocation (ps-cara) scheme for interference reduction in the ps-lte system. this scheme takes care of the context-information and mpc mobility while allocating resources among users. from the simulation results, we found that ps-cara with power control is the best possible solution for ps scenario as we have around 67% less interference as compared to ps-cara without using power control. moreover, we also found that by giving priority to the ps users, the call blocking is reduced significantly as compared with the c-sra scheme. similarly, we notice that by deploying a greater number of mpc in ps scenario, we can increase the mean and edge throughput of the users. in the future, we have a plan to check the performance of the ps-cara scheme by introducing the energy consumption constraint because energy consumption is one of the important constraints in the ps scenario."
"for each value of l (and, thus, for any given line), the two relationships in equation (20) represent the two power circles, c 1 and c 2, shown in figure 8, and expressed in cartesian coordinates (p, q) as"
"the second region extends up to the line length at which another limit is attained. therefore, as made above for uncompensated lines, the attainment of another limit must be checked. in this case, the check concerns the attainment of the thermal limit at the sending end at the length l 2 . as already said, at this line length the currents at the two line ends are both equal to the thermal limit. analytically, at l 2 the following constraints are attained at the same time:"
"can fuse a variety of data set obtained from sensors and also provide the improved ability to adapt in the environment and predict fire in an accurate manner, which has great significance for the safety of human lives as well as property."
"however, from a value chain and bio-product perspective, a majority of current bioeconomy strategies are dedicated to bioenergy and biofuel-based value chains, followed by food and beverage chains. [cit] . however, a surge in integrated biorefineries that synthesise bio-based products other than biofuels is evident from the most recent report compiled and analysed from a stakeholder engagement approach undertaken by the bio-based industrial consortium (bic) and nova institute [cit] . the growth of biomass-cascading biorefineries is also supplemented by the keenness of european bio-based industries to valorise organic-rich bio-waste (mainly agricultural residue and sludge). besides, seizing an opportunity to synthesise value-added products from low-cost feedstock, an unhindered supply of starting material (one of the key barriers to bio-product synthesis), is a promising start for a bio-based business model. [cit], the supply of biomass and waste for consumption within other value chains was predominantly sourced from the agri-food and livestock sector [cit] . though bio-based value chains create opportunities to circularise, some chain-level dynamics influence the environmental, commercial and social practicability of the same by varying degrees. these dynamic factors include biomass supply logistics, feedstock costs (influenced by whether the feedstock is primary or secondary), feedstock treatment requirements and ethical compliance requirements. firstly, it is essential to identify a bio-based value chain with techno-economic potential, and that is environmentally and socio-economically sensible. secondly, when developing a bio-based business model around the identified value chain, it is imperative to anticipate the various interactions among processes, stakeholders and related process-dynamics on the overall performance of the chain. the aim of this paper is to provide a two-fold methodology that helps identification of promising bio-based value chains and to demonstrate how value chain mapping can help stakeholders visualise the various interactions embedded in a value chain. escalating environmental and economic pressure to use our resources responsibly and add value to the used material/products in the commercial sphere has helped the development of technology routes and material circularity, in nearly every global sector."
"the solution of equation (27) is graphically represented by the intersection of the circles c 1 and c 3 illustrated in figure 10 . clearly, the maximum allowable active power is determined by the intersection point p 4 of c 1 and c 3 ."
"equation (32) can also be reduced to a second order algebraic equation in the unknown q, following the same procedure already shown for equation (4) . for the sake of brevity, the relevant equations are not reported here."
"where r u is the allocated rb to user u. moreover, in (7) there is an additional δ u parameter that allows only ps users with high priority to access the mpc spectrum. based on these loads, the total achievable data rate for the users connected with mbs and mpc are evaluated as"
"according to the eu circular economy strategy, the aim of such systems thinking is to \"close the loop by becoming resource efficient through development and establishment of industrial symbiosis, to reduce the pressure on eu's natural capital\" [cit] ."
"the eu preference to develop a manageable and multifunctional bioplastic category for the commercial market stems from the rapid and unsustainable consumption of conventional plastics for a variety of purposes, at a global level [cit] . in addition, the discovery of alarming levels of micro plastics in our food sourced from soil, water and sea, has led to the awareness of the interactions between plastic degradation and the environment (bioaccumulation) [cit] . this is evident in a number of initiatives, listed in the supplementary information, that all target withdrawal from fossil-based resources over the next decade with particular focus on energy and plastic consumption. unlike a decade ago, modern bioplastics are catching up with bio-based solvents in terms of multi-sectoral application (including packaging, agriculture, cosmetics, electronics, construction and automotive) [cit] . evidence of encouragement of bio-based product development and growth can be seen from a plenary meeting of the european parliament that voted in favour of \"biodegradable mulch films\" during the revision of eu fertiliser regulation [cit] and a recent increase in \"big brands\" adopting bio-plastics to appeal to their prominent (high spending power coupled with relatively high environmental awareness) consumer base [cit] ."
"the loadability curves are characterized by various \"regions\", or ranges of length. in the first region, the transmissible power is limited by the conductor thermal limit; in the second region, by the voltage drop limit, ∆v max . further regions concern very long lines, starting from a length of about 300-500 km. these regions are determined by the other constraints referred to the line or power system performance: the steady-state (or angle) stability margin, the voltage stability margin, and/or the maximum allowable joule losses, ∆p max . (in principle, the power losses limit could be more stringent (i.e., it could be attained at lower l) than the voltage drop limit. in actual cases, however, for realistic values of voltage drop and power losses limits, the power losses limit does not prevail (see section 4) .)"
"according to the procedure illustrated in figure 3, the inequality (19) must be checked for each value of l. in this case also, section 4 shows the application of this procedure in a practical case."
"the approach to attaining/creating a circular economy is cascading of material, which may be virgin raw materials, by-products or wastes resulting from any given sector. the concept of cascading and its significance to the establishment and growth of a resource/ [cit], particularly in the eu forest strategy, eu bioeconomy strategy and eu circular economy package [cit] . to understand the state of our transition, transparency on the eu-level biomass potential is essential. bioeconomy, according to the european commission, is a part of the economy that utilises bio-based renewable resources the european commission, is a part of the economy that utilises bio-based renewable resources sourced both from land and sea, processed to produce materials and energy for consumption [cit] . a fully-functional bioeconomy is one of the many pathways that have been identified to attaining a circular economy, both at micro-level (local rural development) and macro-level (nation-wide) [cit] . the principles of both circular economy and bioeconomy are in synergy, in terms of the ultimate goal of attaining a sustainable technological and socio-economic development by decoupling economic growth from resource exhaustion and subsequent environmental degradation [cit] . the two economies share a growing overlap, however, there is currently a need to push circularity down to the consumer level, which is where the largest share of waste is found with no end-of-life valorisation. this has been stated as one of the biggest challenges of the bioeconomy. however it is acknowledged that some sectors of the bioeconomy cannot satisfy the principles of circular economy (e.g., bioenergy and biofuel) because they are considered a dead end route for biomass [cit] . in terms of current targets for sustainable growth, bioeconomy, in combination with circular economy, has the potential to directly contribute to 11 out of the 17 un's sustainability development goals (sdgs) (figure 1 ). the direct contribution of circular and bio-based economy to sustainable consumption and production (sdg 12), reducing our pressure on the environment, air, water and land (sdg 13, 14, 15) is the ultimate aim of the concept. by working in partnership with rural communities and local bio-based infrastructure [cit] (sdg17), utilising the rural knowledge pool, alleviation of poverty (sdg 1 and 2), forging skills among communities (sdg 4) to take an interest in guarding the local ecosystem services encourages the development of sustainable communities (sdg11), in addition to creating jobs and socio-economic opportunities (sdg 8) . use of bioenergy, devising smart strategies and value-chain pathways to lock the chain's ghg emissions, either via carbon capture or soil incorporation of high quality biochar, have been identified as potential means of achieving the ambitious paris climate target [cit] . having comprehended the synergies between a circular economy and a bioeconomy and its potential for contributing to the global sustainability targets from a number of earlier studies [cit], it is crucial to understand where we stand to take appropriate and smarter \"next steps\". prior to exploring the opportunities that bioeconomy could offer, the probability of it being successfully established must also be systematically investigated."
"variety of end-of-life: end-of-life characteristics play a prominent role at any given stage of a value chain. from a top-down approach, a process that is capable of utilising waste biomass for raw feedstock, also called \"cascading use\" is a valuable, sustainable business model as there will a regular influx of low-cost feedstock, promising a continuous product supply to the market. from a \"bottomup\" approach, strategic management and utilisation of waste (post-product consumption) is capable of delivering three-fold benefits: environmentally through reduction of waste for treatment and feedstock variability: the flexibility of bio-based value chains to produce products and by-products from a variety of feedstocks is crucial. biomass, the starting material of any bio-based value chain, is cost-susceptible to both market volatility and also seasonal in nature. dependence on a seasonal feedstock will lead to a seasonal value chain, thereby resulting in seasonal products (and byproducts) which gives bio-based products a less attractive perspective among consumers [cit] ."
"the purpose of this paper is to provide a methodology to assess existing or novel bio-based value chains from key angles that are of significance to our journey to attaining a fully functional bio-based circular economy. due to the limited time and resources, it is essential to distinguish the most promising bio-based business model from the rest, and that is precisely what this paper, with its methodology suggests. eu-based bioeconomy and bio-based value chains are diverse in nature and are not restricted to those value chains that have been considered in this study. the preliminary list of 12 value chains was selected based on their relevance and significance to the bioeconomy, their current activity level/contribution and coverage by various sustainability and certification schemes. these bio-based value chains have been selected to ensure the representation of eu's diverse bio-based value chains in addition to their potential to address the key environmental, techno-economic and socio-economic threats and challenges faced globally. the value chains were selected in two-steps via multi-criteria decision analysis (mcda), wherein the first step, the preliminary value chains were ranked by placing them against a back drop of five key selection criteria in the current context: feedstock variability; eu feedstock preference; variety of end-of-life options; multisector application and multi-regional supply chains. this step led to the identification of eight bio-based value chains which were subjected to a second round of assessment where five out of the eight value chains showed a promising interest/inclination for bioeconomic development. for the selected five bio-based value chains which included starch to bio-plastics, starch to bio mulch films, starch to frame material, cellulose to bio-based solvents, vegetable fats/plant lipids to bio-based lubricants, elaborate value chain maps were developed to demonstrate the highly informative nature of this tool and its crucial role in understanding the complex interactions among the various process stages, associated processes and stakeholders within a more complex value chain."
"from a sectoral perspective, the eu biorefinery map (figure 3 ) represents the prevalence of high numbers of oil and fat based biorefineries dedicated to the production of biofuel and oleochemical products."
"in this study, the initial \"cradle-to-grave\" value chain mapping provides a generalised yet visual schematic of the dynamics including the resource flow and actors integrated within bio-based value-chains that have been chosen via the assessments above. for the selected and finalised list of bio-based value chains, the following chain characteristics are crucial and relevant to visualising their significance to a circular economy. the characteristics are as follows:"
the second region extends up to the line length l 2 at which also the sending end current achieves the thermal limit ( figure 12 ). this length corresponds to point p 3 depicted in figure 9 .
"value chain maps can be laborious and time-consuming to develop, depending on the complexity of the value chain under analysis. the map is only an informative tool for the visualisation of bio-based business models, identification of market opportunities and the scope of the value chain. it may not be able to highlight any changes in the dynamics associated with the factors (chain actors, inputs/outputs and technology routes) presented in the chain."
"from having undertaken the multi-criteria decision analysis, a number of key aspects associated with the bio-based business models were brought into consideration, out of which, only a few general criteria were chosen as the selection criteria. some of the key gaps and challenges that are addressed in this section were drawn based on our analysis and the mapping exercise on the overall. these are some key hurdles faced by existing and new bio-based business models in the current context of our transition to a bio-based economy. these points have been highlighted to encourage holistic systems thinking, starting from the feedstock procurement stage, through product design, which influences the final product cost up to the need for a dedicated end-of life management infrastructure which has an influence on public perception of bio-based products. such systems thinking will encourage streamlining the innovation, processes and delivery of an incredibly complex bio-based value chains. application of systems thinking can be customised to the practices in other non-bio-based sectors thereby facilitating our transition to a fully-functional circular economy."
"in order to determine the coordinates of avoiding the complex solution of the system (24), it is convenient to resort to an iterative procedure similar in principle to those described in figures 1 and 3 . the module of the sending end current can be calculated, for increasing values of, using the left-hand member of the first of (24): the point is identified when the sending end current attains the thermal limit. section 4 shows the application of this procedure in a practical case. this simple analytical procedure (similar in principle to that illustrated in figure 3 ) allows calculation of the second region of the loadability curve."
"value chain maps are a valuable, flexible and convenient tool to develop and analyse the scope and performance potential of a bio-based business model by breaking down the various process dynamics into logistics, sectors of application and embedded stakeholders. the strengths, weaknesses, costs and competition from other value chains in the production of specific commodities can be visualised via value chain maps. the next step in identifying such promising value chains is to understand the chain complexities via such a \"mapping exercise\"."
"supply risks: the seasonal nature of the biomass supply stemming from the cultivation time-span of biomass is an issue. however, further challenges posed to cross-border biomass supply from climate change, diminishing ecosystem services due to human intervention (e.g., intensive farming), variable food/biomass demand, geopolitical instabilities and social ethical concerns also undermine the \"sustainable\" characteristics of a bio-based value chains. it is therefore essential to encourage biorefinery and business models that are built on pre-established climate change resilient but low-demand biomass (e.g., millet and sorghum in africa). from a cost perspective, their low demand reduces biomass costs, from a socio-economic perspective, accessing the knowledge pool of the local rural communities can be mutually beneficial via opportunities for rural development and encourages their interest in maintaining the local ecosystem services."
"feedstock costs: biomass may seem economically unfeasible compared to cheap petroleum feedstocks and their intermediates. the cost of feedstock which in turn is influenced by its supply/demand ratio in the commercial market makes it even more difficult for bio-based business models to increase profit margins and sometimes to breakeven. however, encouragement and development of innovative multi-functional bio refineries that are capable of utilising low-value, low-cost and waste biomass can prove to be an ideal alternative; such biorefineries must also be able to transform such waste feedstock into product designs that can be disassembled during their end-of-life with the product components being re-circularised back into the manufacturing loop. re-incorporation of material can have long-term economic benefits to such business models, in addition maintaining the value of all the first-chain process inputs."
"in order to determine the coordinates of p 3 avoiding the complex solution of the system (24), it is convenient to resort to an iterative procedure similar in principle to those described in figures 1 and 3 . the module of the sending end current can be calculated, for increasing values of l, using the left-hand member of the first of (24): the point p 3 is identified when the sending end current attains the thermal limit. section 4 shows the application of this procedure in a practical case."
"deployment: we propose ps-cara scheme under the sdn-based cloud network architecture because of its capability to centrally manage all the base stations. the number of mpc required per macro bs varies according to the deployment situation or scenario. that is, in the ps scenario a higher number of mpcs are desired to meet the user requirements. therefore, more mpcs are deployed in ps scenario and the resources are allocated by considering the context information of the users."
"it is trivial to remark that, until the voltage drop limit is achieved, reactive compensation is not required and the loadability curve coincides with that of uncompensated lines. hence, the maximum line length for which reactive compensation is not required, is equal to l 1 ."
"when the thermal limit is attained at the sending end, the maximum allowable active power ( ) can be calculated by solving the following two-equations system:"
"keeping this in mind, the succession of the various limits and the relevant analytical description are explained in the following subsections where, for the sake of clarity, the case of unitary power factor is examined first."
"the proposed ps-cara scheme also reduces the call blocking probability of the users as compared with the existing c-sra scheme. the users call blocking probability decreases because of deploying a greater number of mpc in the public safety areas. this in turn reduces the traffic congestion which occurs during the public safety scenario because a higher number of users access the limited spectrum of the mbs at the same time. we compare the simulation results in three different aspects: (1) using only c-sra scheme where we randomly allocate the resources among users, (2) proposed ps-cara scheme without giving priority to public safety users during resource allocation, (3) proposed ps-cara scheme with public safety priority, where public safety users have been given high priority during resource allocation. the simulation results depicted in figure 9 show that the proposed ps-cara scheme with public safety priority outperforms all the other cases without giving priority. hence, by giving priority to ps users the cbp reduces because the connection and resources are provided to users on a priority basis, which in turn results in less congestion for the priority users. figure 9 . call blocking probability of the proposed ps-cara scheme."
"country-based feedstock preference: consistency in raw material supply and chain-productivity is essential for the successful uptake of bio-based products and their associated value chains. the guarantee of a promising flow of feedstock to the facilities can only be ensured through the choice of \"locally sourced\" feedstock. \"locally-sourced\" feedstocks generally have established logistics and reporting procedures, which can communicate their point of origin to the economic operator. moreover, utilisation of such \"locally generated feedstock\" can be associated with positive social impacts from employing decades of skilled cultivation related knowledge from the local rural community and its established infrastructure, catalysing its development with an innovative biorefinery, subsequently reducing the overall cost of value-chain establishment [cit] ."
"on the other hand, the ratio f p of the power losses ∆p m evaluated with respect to p m and the power losses ∆p evaluated with regard to the maximum power p(l), can be deduced from the load factor f c using the formula [cit] :"
"the points of intersection of the circles in equation (21) are p 1 and p 2 in figure 8 . the maximum allowable active power p(l) corresponds to the abscissa of p 1, whose coordinates are obtained by solving equation (21). for this purpose, as is well known from analytical geometry, the radical axis, i.e., the line passing through p 1 and p 2, is described by the equation:"
"the performance of the proposed ps-cara scheme is evaluated by upgrading the system-level simulator designed in matlab [cit] . this simulator is designed by taking care of the assumptions given for the ps scenario [cit] . the key assumptions considered during simulations are: (1) user can associate to one bs at one time, that is either with mbs or mpc, (2) only uplink interference scenario is modeled, (3) frequency division duplexing (fdd) is considered, (4) urban macro scenario is considered because this scenario is compulsory for both general purpose scenario and public safety scenario [cit], and (5) all users and mpc are deployed outside only in the coverage area of mbs. in table 3, we summarized the parameters used to perform the simulations."
"concerning joule losses along the line and considering that this paper deals with the maximum permissible power ( ), a natural approach could be to fix a limit to the power losses ∆ calculated with regard to the loadability limit ( ). better, looking for homogeneity with the percent voltage drop limit, we could fix a limit to the ratio between ∆ and ( )."
"simple and effective as the word relation-based approach is, it cannot make full use of the information of sememe-based linguistic kbs because it disregards the complicated relations between sememes and words as well as relations between different sememes. to address this limitation, we propose sememe embedding-based approach, which learns both sememe and word embeddings jointly."
"when thinking about ways to visualize different kinds of data for various components, some ways of display seem more intuitive than others. as mentioned above, some of the most relevant data to visualize for a thermofluid network are mass flow rates, pressures, enthalpies, and temperatures. often, mass flow rates and temperatures are of special interest. in a non-modelica context, köcher (2000) reported a way of visualizing pressures and temperatures in a district heating network at the nodes. yet, in the system class representation shown in fig. 4, most of the mentioned information concerns the edges rather than the nodes. thus, the way the edges are drawn in a graph plot are a central part of the visualization. in order to prepare that visualization, we use selected data to calculate edge weights and edge colors to be used in the plotting."
"in the future, we will explore the following research directions: (1) in this paper, for simplification, we ignore the rich hierarchy information in hownet and also ignore the fact that a word may have multiple senses. we will extend our models to consider the structure information of sememe and multiple senses of words; (2) in fact, our framework for cross-lingual lexical sememe prediction can be transferred to other cross-lingual tasks. we will explore the effectiveness of our model in these tasks such as cross-lingual information retrieval."
"in our experiments, chinese is source language and english is target language. to learn chinese and english monolingual word embeddings, we extract about 2.0g text from sogou-t 1 and wikipedia 2 respectively. and we use thulac 3 [cit] ) for chinese word segmentation."
"in the task of bilingual lexicon induction, we opt for chinese-english translation lexicon version 3.0 5 to be the gold standard. in the task of word similarity computation, we choose wordsim-240 and wordsim-297 [cit] ) datasets for chinese, and wordsim-353 [cit] and simlex-999 [cit] datasets for english to evaluate the performance of our model. these datasets contain word pairs as well as human-assigned similarity scores. the word vectors are evaluated by ranking the word pairs according to their cosine similarities, and measuring spearman's rank correlation coefficient with the human ratings."
"furthermore, its open design allows for a wide variety of data to be represented by nodes and edges. together with the powerful plotting package matplotlib [cit], networkx can be used to visualize graphs in many ways."
"we evaluate our model by recommending sememes for english words. in hownet, many words have multiple sememes, so that sememe prediction can be regarded as a multi-label classification task. we use mean average precision (map) and f 1 score to evaluate the sememe prediction results. we compare our model that incorporates sememe information with word relation-based approach (named clsp-wr) and our model which jointly trains word and sememe embeddings (named clsp-se) with a baseline method bilex [cit], a bilingual wrl model without incorporation of sememe information. for bilex, we use its trained bilingual word embeddings to predict sememes for the words in target language with our sememe prediction approach. (1) our two models perform much better compared with bilex in all the seed lexicon size settings. it indicates that incorporating sememe information into word embeddings can effectively improve the performance of predicting sememes for target words. the reason is that both of our models make words with similar sememe annotations have similar embeddings, and as a result, we can recommend better sememes for target words according to its related source words."
"to address the issue of the high labor cost of manual annotation, we propose a new task, crosslingual lexical sememe prediction (clsp) which aims to automatically predict lexical sememes for words in other languages. clsp aims to assist in the annotation of linguistic experts. there are two critical challenges for clsp: (1) there is not a consistent one-to-one match between words in different languages. for example, english word \"beautiful\" can refer to chinese words of either \"美丽\" or \"漂亮\". hence, we cannot simply translate hownet into another language. and how to recognize the semantic meaning of a word in other languages becomes a critical problem. (2) since there is a gap between the semantic meanings of words and sememes, we need to build semantic representations for words and sememes to capture the semantic relatedness between them."
"since hownet was published [cit], it has attracted wide attention of researchers. most of related works focus on applying hownet to specific nlp tasks [cit] . [cit] conduct studies of augmenting hownet by recommending sememes for new words. however, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic kbs."
"where w s mt is the source word that w t t matches with, and c(w t t ) is the number of times w t t occurs in the target corpus."
"in this approach, we represent sememes with distributed vectors as well and place them into the same semantic space as words. similar to spse, which learns sememe embeddings by decomposing word-sememe matrix and sememe-sememe matrix, our method utilizes sememe embeddings as regularizers to learn better word embeddings. different from spse, we do not use pre-trained word embeddings. instead, we learn word embeddings and sememe embeddings simultaneously."
"after the graph construction, transformation, and result file handling, the system graph will contain all relevant data for visualization. this data structure concept has proven to be user-friendly and efficient, making all data easily accessible and fast to process. the system representation as described above presents a compromise between the most intuitive design and strictly following the modelica model setup. this compromise may be evaluated for each use case and the graph representation adjusted accordingly. this is possible with moderate manual effort, as the object-oriented and graph-based code structure should be reasonably transparent for the user."
"nevertheless, due to the existence of the latent variable, optimization of the matching process in cross-lingual part poses a challenge. we settle on viterbi em algorithm to address the problem. next, we still take the target-to-source side as an example and give a detailed description of the training process using viterbi em algorithm."
"this will hopefully reduce manual effort in modeling complex system like district energy systems and lead to insights from modeling these systems for real-world applications. to this end, we plan to release the developed python code as an open-source package in the near future. in addition, the modelica component models for the district heating network modeling will be made available through the open source model libraries aixlib 1 and its contributions to the annex 60 library 2, which is a joint effort within the international energy agency's annex 60 programme."
"our method consists of three parts: monolingual word representation learning, cross-lingual word embedding alignment and sememe-based word representation learning. hence, we define the objective function of our method corresponding to the three parts:"
"in any case, the python routine will create a plot following the structure shown in fig. 5 for every time-step in the simulation result file or for a user-selected period within the simulation time limits. the graph drawing will loop over all nodes and edges, plotting them into space a and adjusting their appearance according to data like the node type, edge weight, and edge color stored in the networkx graph data structure. for the line plots, the lines will be drawn from the time-step at the beginning of the visualization until the current time-step for this plot. thus, when the individual static plots are compiled into a video, this will give the impression of a line plot tracking the behavior of the corresponding variable with each time-step."
"again, this paper is limited to show static plots of the visualization for given time-steps. a further application is to compile a video from all the plots to animate the dynamic system behavior. for this demonstration, fig. 9 shows the state of the system near the beginning of the simulation while fig. 10 shows the system closer to the end of the simulation. by varying the line width of the pipe connections according to pre-calculated edge weights that depend on the mass flow rate, it is possible to show the mass flow rates for all of the supply lines' over 100 pipe elements in one single plot. together with the color mapping of water temperatures within the pipe to the color bar given on the upper right, the plots give an impression of the energy flows in the network."
"where α and β control the relative strengths of the two terms. it should be noted that the idea of forcing similar words to have close word embeddings is similar to the state-of-theart retrofitting approach [cit] . however, retrofitting approach cannot be applied here because sememe-based linguistic kbs such as hownet cannot directly provide its needed synonym list."
"the approach presented in this paper aims at producing a visual output, helping to better comprehend and analyze the data produced by simulating complex thermofluid networks in modelica. to this end, we use a postprocessing routine in python. python was chosen as a programming language, in part because of its accessibility through easy syntax, wide use, and being platformindependent. another advantage of using python is the possibility to build on the previous work done in postprocessing modelica results as described in section 1. fig. 1 shows a schematic representation of the approach presented in this paper. the information contained in a modelica model is used to initialize a model graph object. as it is often helpful to make abstractions from the original model design for visualization, this model graph is transformed to a system graph in a subsequent step. after reading the modelica model's simulation results to the system graph, this class can generate a visual output in the form of static plots and video animations. both the python classes for the model and the system extend the class nx.graph from networkx, so that it inherently has all of networkx's well established functionalities for graph handling and analysis. in order to read the modelica result files and process the data, the code uses the modelicares package. this way, the model and system classes can be focused on performing the vi- reading information from a modelica model to the model class in python marks the start for the described process. this information is represented in the graph by placing edges between the nodes. in order to arrive at a more intuitive display of the model structure, especially for complex pipe networks, the model graph is transformed to a system graph. one major change in that transformation is the introduction of network nodes between sub-models. in a further step, pipe models are transformed from individual nodes to edges connecting the network nodes. with pipes serving as connecting elements in real-world systems, this representation may be more user-friendly for the following visualization. the process is described in more detail in section 3."
"sememe-based word representation is intended for improving word embeddings for sememe prediction by introducing the information of sememebased linguistic kbs of source language. in this section, we present two methods of sememe-based word representation."
the single-cell simulation was performed using the ventricular cell model suggested by ten [cit] . the formulation of the i ks current equation was modified to examine the variation with the change in apd.
"in the first example, our model finds the best translated word for handcuffs in chinese ⼿ 铐 \"handcuffs\", whose sememe annotations are exactly the same as those of handcuffs. in addition, the second closest chinese word 镣 铐 \"shackles\" is a synonym for ⼿铐 \"handcuffs\" and also has the same sememe annotations. therefore, our model predicts all the correct sememes successfully. from the prediction results of this example, we notice that our model can accurately predict general sememes like 用具 \"tool\" and ⼈ \"human\", which are supposed to be difficult to predict."
"where f active (x) is defined as active force, and f passive (x) represents passive force. the term f preload is a constant force. as this would induce an initial sarcomere length that is larger than the resting length, f preload corresponds to f passive (sl 0 ). f afterload term is differently used for isotonic contraction and isometric contraction. under the isotonic contraction condition, this term is fixed after the release. on the other hand, under the isometric contraction, this term is used to simulate compliant ends of the muscle as shown eq. (11) . here, x is sarcomere length, and kse denotes the stiffness in units of normalized force per μm."
"we use sememe annotations in hownet for sememe prediction. hownet annotates sememes for 118, 346 chinese words and 104, 025 english words. the number of sememes in total is 1, 983. since some sememes only appear few times in hownet, which are expected to be unimportant, we filter out those low-frequency sememes. specifically, the frequency threshold is 5, and the final number of distinct sememes used in our experiments is 1, 400."
"in this study, we observed apd variation in single-cell and three-dimensional ventricular models. first, we used a cellular electrophysiological simulation to observe the electrical changes due to apd variation and to clarify the relationship between apd and calcium concentration. second, we used a three-dimensional electromechanical simulation to compare cardiac pumping efficiency quantitatively under sinus rhythm conditions."
"when connecting multiple modelica.fluid component models in a pipe network, the fluid flow is driven by pressure differences between connectors. often, models provide a relationship between mass flow rate and the pressure drop between the component's ports. this leads to a network of mass flows between different pressure levels. in many cases, another key aspect of modeling are the thermal properties of the fluid flow and parts of the components. a system model containing information about all these aspects can be very useful to understand the system's dynamic behavior. yet, with increasing system size this amount of data increases at a rate that can make it hard to comprehend and verify simulation results. in these cases, numerical outputs and line plots of individual variables may not be sufficient ways of processing simulation results for the user. thus, the aim of this paper is to present an approach to visualize the information from thermo-fluid system simulations by means of graph drawing and animation."
"as the identifiers for each component's variables may be different, we extend this general component class for every relevant component type and assign it its own class with specific identifiers and in some cases with special functionalities. in the example of 2, three such classes are needed, i.e. the classes boundary, valve, and pipe. as extending the component class requires relatively little effort, we prefer this method over the attempt to have only one component class that tries to manage all different component types and their differences."
"regarding the graph drawing for space a in fig. 5, the user can select different visualization types. most times, this will consist of a 2d view recreating the system graph as illustrated in fig. 4, with the edge weights and colors varying according to the preselected variables. for the future, we will also work on 3d plots, where the value of an additional variable can be visualized by use of a z-axis. this is especially interesting to visualize pressure levels so that mass flows will flow from nodes plotted at greater z-axis levels to those with lesser z-values."
"for the first processing step, the model class includes methods to parse the modelica code of a given file and extract information from its declaration sections as well as from the equation section. these functionalities are of limited scope, however, as they focus only on mapping the model structure into a graph in python. more complex modelica features such as extending and redeclaring are not processed by this simple parser. for the component model declarations, the parser extracts data about the component's class, its instance name as well as the coordinates of its graphical representation, which can be read from the corresponding annotation. at the current stage, this step will process only declarations of components that have been selected in advance. this limitation arises from the fact that later in the process, special subclasses are needed to extract relevant information from the simulation results for each type of component. in the example model, we prepared only for the fluid components to be processed. as a result, sub-models that are not an integral part of the fluid network, like the system model in the lower right corner of fig. 2 and the control inputs for the valve openings are not taken into account to be part of the model graph. if of special interest, a processing of these sub-models could also be implemented into the presented framework. yet, for larger fluid networks, this may compromise the clarity of the visualization."
"the part of the model simulating mechanical contraction characteristics was constructed to mimic cardiac contraction by means of the crossbridge of a myofilament. in fig. 1, xb prer represents prerotated states of the myosin head in relation to binding and contributes to stiffness but does not generate force in the absence of net motion. xb postr denotes a strongly bound myosin head and represents the isomerization to induce strain in the extensible neck region. the force due to the crossbridge can be subdivided into an active force and a passive force. the active force induces the action of the cycling crossbridge, and the passive force induces the complete muscle response with viscoelastic elements. mass prevents instantaneous changes in muscle shortening velocity for quickrelease protocols, whereas a linear elastic element is intended to simulate the effects of compliant end connections that take place in real muscle preparations."
"to represent electrical propagation by means of conduction in three dimensional space, the partial differential equation expressing the electric conduction phenomenon in myocardial tissue and the ordinary differential equation for the electrical wave propagation of the ionic channel were derived [cit] ."
"we argue that the presented approach can be a useful tool in handling the complexity of larger thermo-fluid networks and their dynamic system behavior. on the one hand, the visualization of energy flows and other simulation result data can help modelers to verify their model setups and assumptions. on the other hand, the visualization can be used to inform about relationships and interactions of system components. yet, drawing conclusions from such visualization for the operation and design of actual systems, similar to all aspects of modeling and simulation, requires critical verification of the models used and the results obtained."
the data attached to nodes and edges can subsequently be used to visualize the overall system behavior visualizing simulation results from modelica fluid models using graph drawing in python
"for components that do not correspond to any modelica component directly, in some cases the code will assign the object of a neighboring graph element. for example a network node may thus be attributed an instance of the neighboring pipe object, so that it will return the pipe's mass flow rate when queried for such data. in some cases, like for an edge between two network nodes, there may also not be a neighboring object that directly represents a modelica model component. for this situation, we sometimes prefer not to attach any data to it in order to not give any wrong impression in the visualization. yet, there is the possibility to interpolate some of this data when the user wants it visualized in a certain way."
"1. in the cellular electrophysiological simulation, as apd is shortened owing to the increase in the electrical conductivity of the k + channel, the intracellular ca 2+ concentration decreases. that is, the apd and the sum of the intracellular ca 2+ concentrations showed a positive correlation. 2. the shortened apd reduced the conduction wavelength in the three-dimensional ventricular tissue by shortening the plateau and early repolarization in myocardial cells. 3. in addition, the shortened apd reduced cardiac pumping efficiency by more than 60% compared with the normal group."
"proceedings of the 11 th figure 9 . visualization output for the district heating network at a low-load operation at the beginning of the simulation define the identifiers for different variables in the result file as described in section 3. thus, it is possible to use a pipe component class that retrieves the supply or the return pipes' data at the user's selection. similarly, we use a supply and a building class to handle the data of these components. to demonstrate the visualization approach for this use case, we use a simulation of the district heating network model for a simulation time of 2500 hours with an hourly time-step. starting at the beginning of the year, this illustrates the first part of the year with significant heat loads. following the layout of fig. 5, we use the graph drawing to visualize mass flow rates and the temperatures in the supply lines as well as two line plots. one line plot shows the supply plant's supply and return temperatures and the second line plot shows the ambient outdoor temperature as read from the weather input file."
"regarding the computational performance, processing the data as well as the model and system graphs creates little overhead and takes a few seconds on a standard laptop computer. the time for the plotting will largely depend on the model size, time-step, simulation time, and the required resolution of the output data. therefore, this part of the process can currently take from a few minutes up to 2 hours for a very high-resolution animation of a large district heating system simulation with small time-steps and a duration of 1 year. yet, it is likely that the time this part of the process can be efficiently reduced by parallelization of the plotting."
"(2) clsp-se model achieves better results than clsp-wr model. the reason is that by representing sememes in a latent semantic space, clsp-se model can further capture the relatedness between sememes as well as the relatedness between words and sememes, which is helpful for modeling the representations of those words with similar sememes."
"as the model class processes the modelica code in terms of declaration statements, connections, and their graphical annotations, the missing graphical annotations lead to the return components neither being represented in the model nor in the system graph. the resulting system graph for the district heating network is shown in fig. 8 . nevertheless, the values of the return pipes can be shown in the graph in place of the supply pipes' values, as the return lines are placed at the same locations as the supply lines. for the data handling of each component, we extend the general component class and figure 6 . using graph drawing to visualize system behavior over time"
"this paper presents an approach that uses postprocessing of modelica simulation results and graph drawing in order to better visualize the dynamic behavior of complex thermo-fluid networks than standard line plots of individual result variables. using a graph and attributes for nodes and edges as a data-structure to handle modelica simulation results has proven a feasible concept, as it can mirror the object-oriented structure of the modelica model into the post-processing. this allows for a low-maintenance framework that nevertheless offers flexibility for adjustments and options to tailor the visualization output to the specific aims of the visualization and to the requirements of the used models."
"furthermore, the process of visualizing modelica simulation results introduces methods to parse modelica code and handle information about model structure and behavior in a python-based graph structure. for the future, it will be interesting to use these resources for the automated generation and modification of modelica models. to this end, we are working on a bi-directional work-flow to generate modelica models for district energy systems from different input data with the system graph at the conceptional core. possible input data includes data from geographic information systems (gis) or citygml. in reverse, these models and their results can again be processed by the system graph as described in this paper. thus, the system graph can be used as the foundation in an integrated workflow for model generation as well as result analysis and visualization. we think that such an approach has the potential to address handling the complexity of input and output data of large-scale energy system models, which has been identified as one of the key challenges in modeling such systems [cit] ."
"in this section, we introduce our novel model for clsp. here we define the language with sememe annotations as source language and the language without sememe annotations as target language. the main idea of our model is to learn word embeddings of source and target languages jointly in a unified semantic space, and then predict sememes for words in target language according to the words with similar semantic meanings in source language."
"the functionality of the presented approach was demonstrated for a simple example from the modelica standard library as well as for a real-world application of a district heating system model. in this proof of concept, we used line thickness to visualize mass flow rates from one node to another and line colors to indicate temperature levels. other possible uses include visualizing pipe diameters with line thickness or flow velocities with line colors. also, we limited our graph drawing to 2-dimensional representations of the system, which resembles the diagram view of the corresponding modelica models. in this process, parsing the modelica code for the graphical information in the annotations leads to nodes in the graph with corresponding coordinates. in future work, it will be interesting to visualize certain values in a pseudo-3-dimensional way, where the model representation can stay in the x-and y-axes while simulation result values can be shown on a corresponding z-axis. this is especially promising to visualize pressure levels of supply and return lines for thermo-fluid networks or deviations between simulation results and measurement data."
"the seed lexicon term l seed encourages word embeddings of translation pairs in a seed lexicon d to be close, which can be achieved via a l 2 regularizer:"
"the selection of what values to represent by edge weights and colors is up to the user. the system class contains methods for both calculations, that take as arguments the variable that is to be represented, e.g. temperature or mass flow rate. based on this selection, the edge weight and color will be calculated and attributed to the corresponding edge. in the case of color representation, a relative value between the mini-"
"english word canoeist 锻炼 \"exercise\", ⼈ \"human\", 体育 \"sport\", 事情 \"fact\", 船 \"ship\" 5 nearest chinese words 短跑 \"sprint\" 事情 \"fact\" 锻炼 \"exercise\" 体育 \"sport\" 独⽊⾈ \"canoe\" 船 \"ship\" 皮艇 \"kayak\" 船 \"ship\" 名将 \"sports star\""
"viterbi em algorithm alternates between a viterbi e step and a subsequent m step. the viterbi e step aims to find the most probable matched word pairs given the current parameters. considering the independence, we can seek the match for each word individually:"
"in order to keep the visualization output as flexible as possible, we define a framework for sub-plots using matplotlib's grid structure. fig. 5 illustrates the concept. the only fixed properties are the spaces a and b that serve as placeholders for the network graph drawing and the corresponding color map. in many cases, one can argue that such a graph drawing has advantages over a multitude of standard 2d line plots. yet, we do not want to argue that it is inherently always superior to the clarity and simplicity of a line plot. therefore, any number of line plots can be placed beneath the graph drawing in any number of spaces c, d, and so on. the system class allows the user to name the variables that should be plotted in addition to the graph drawing."
"proceedings of the 11 th by means of graph drawing. in addition to static graph drawings showing the graph's structure, the information contained in the graph drawing can be extended by different means. for this demonstration, we will use the line thickness and color of edge connections to represent mass flow rates and temperatures for every simulation time-step. in section 7 we will point to further possibilities of enriching this data visualization approach in future work. after visualizing the system properties for every timestep, we create a video from the individual plots, which as a final outcome produces an accessible and intuitive way to animate an amount of data for a complex system that would be hard to process for a human user in a standard 2d line plot. in the following sections, we will present the individual steps outlined above in more detail for an example model from the modelica standard library. after that, we will present a use case of a campusscale district heating network to demonstrate the capabilities of the visualization approach in an applied context."
"we will use the model incompressiblefluidnetwork from the modelica standard library's modelica.fluid.examples package to illustrate the process description. the model's diagram view is shown in fig. 2 . this system consists of a piping network with 11 pipes and 3 valves, transporting fluid flows from a source on the figure's left side to a sink on the figure's right side. for reasons of clarity, we will limit the processing of this example to basic functions. the full capability of the presented approach in its current state will be shown in section 6 for the example of a district heating network model."
"a comparison of fig. 9 and fig. 10 illustrates the concept of line plotting in the lower part of the figures. as the line is plotted from the beginning until the current time-step, it gives an impression of monitoring the by visualizing energy flows in the graph drawing, the presented approach can be a useful tool in verifying simulation results. for verification purposes, the main advantage of the graph-drawing based visualization approach over simple line plots is that various system variables are shown together and in context of the system behavior. furthermore, it would be possible to not only display simulation result variables in such a visualization, but to also display deviations from measurement data, if such data is available."
"where c t and c s denote the target and source corpus respectively. here, we simply assume that the matching processes of target words are independent of each other. therefore, we have:"
"in this paper, we introduce a new task of crosslingual sememe prediction. this task is very important because the construction of sememe-based linguistic knowledge bases in various languages is beneficial to better understanding these languages. we propose a simple and effective model for this task, including monolingual word representation learning, cross-lingual word representation alignment and sememe-based word representation learning. experimental results on real-world datasets show that our model achieves consistent and significant improvements compared to baseline method in cross-lingual sememe prediction."
"these studies observed the electrophysiological effects of apd on the k + channel at the single cell level, not in three-dimensional tissue. however, it is critical to observe such electrophysiological phenomena and the mechanical behavior of the heart in three-dimensional tissue, because the electrical stimulus for the ventricles in heterogeneously transferred from cell to cell or tissue to tissue. furthermore, to thoroughly understand cardiac activity, it is necessary to conduct quantitative analysis of the electromechanical behavior of the heart in three-dimensional space."
"in this approach, we obtain word and sememe embeddings in a unified semantic space. the sememe embeddings bear all the information about the relationships between words and sememes, and they inject the information into word embeddings. therefore, the word embeddings are expected to be more suitable for sememe prediction."
"the shortening of apd owing to increased electrical conductivity of a protein channel on myocardial cells likely decreases the wavelength and the pumping efficiency of the ventricles. additionally, it may increase tissue sensitivity to ventricular fibrillation, including reentry, and cause symptoms such as dyspnea and dizziness."
"in this section, we first introduce the dataset used in the experiments and then describe the experimental settings of both baseline method and our model. next, we present the experimental results of different methods on the task of cross-lingual lexical sememe prediction. and then we conduct detailed analysis and exhaustive case studies. following this, we investigate the effect of word frequency on cross-lingual sememe prediction results. finally, we perform further quantitative analysis through two sub-tasks including bilingual lexicon induction and word similarity computation."
"one reason for using the modelica modeling language is the high re-usability of component models from model libraries. in this context, the acausal connections between component models can be used to efficiently assemble larger system models [cit] . for thermo-fluid systems, the modelica.fluid [cit] package includes the concept of stream connectors, which facilitates the modeling of flow networks with possible flow-reversals. in energy systems modeling, e.g. for building or district heating systems, this enables the assembly of large system models from only a limited number of component models like pumps and pipes."
"this change reduced the amount of ventricular work per unit atp consumed by the myofilament during the sinus rhythm in accordance with shortened apd 90 (fig. 7c) . this means that as apd 90 is shortened, energy efficiency of the ventricle during 1 cycle decreased. the energy efficiency at 299 apd was 41, and the efficiency at 39, 34, 28, and 23 is respectively shown at 256 apd, 212 apd, 188 apd, and 171 apd . at 150 apd, this efficiency was the lowest: 18. figure 7d is a graph of stroke volume and the ejection fraction calculated from edv to esv in the pv loop of fig. 6b . the stroke volume was measured by means of the difference between edv and esv in the pv loop, and the ejection fraction was obtained by means of the stroke volume ratio for edv. these data refer to the amount and efficiency of blood that is sent from the heart to the aorta and the pulmonary artery. stroke volume and ejection fractions at 299 apd were measured and found to be 36 ml and 45%, respectively. nonetheless, these values decreased with decreasing apd 90, resulting in 13 ml and 14% at 150 apd . the stroke volume and the ejection fraction of the left ventricle decreased as apd 90 was shortened. the pumping efficiency when apd 90 became 150 ms was 68% lower relative to the value at 299 apd ."
"in case study, we conduct qualitative analysis to explain the effectiveness of our models with detailed cases. we show two examples of crosslingual word sememe prediction, in which we predict sememes for handcuffs and canoeist. fig. 2 shows the embeddings of five closest chinese and english words to handcuffs and canoeist, and the vector of each word is projected down to two dimensions using t-sne [cit] . 6 the largest seed lexicon size is 6000 because that is the maximum number of translation word pairs that we can obtain from the bilingual corpora. table 2 lists top-5 sememes we predict for the two words and the sememes annotated for each word in hownet are in boldface. in the table, we also exhibit the annotated sememes of the five closest chinese words."
"to explore how frequencies of target words affect cross-lingual sememe prediction results, we split the testing set into four subsets according to word frequency and then calculate the sememe prediction map and f 1 score for each subset. the results are shown in (1) the more frequently a target word appears in the corpus, the better its predicted sememes are. it is because high-frequency words normally have better word embeddings, which are crucial to sememe prediction. (2) our models evidently perform better than bilex in different word frequencies, especially in low frequency. it indicates that by considering external information of hownet, our models are more robust and can competently handle sparse scenarios."
"monolingual word representation is responsible for explaining regularities in monolingual corpora of source and target languages. since the two corpora are non-parallel, l mono comprises two monolingual sub-models that are independent of each other:"
"where v m is the membrane potential, t is time,i ion is the sum of all transmembrane ionic currents, i stim is the current due to the external stimulus, and c m is membrane capacitance."
"to tackle these challenges, in this paper, we propose a novel model for clsp, which aims to transfer sememe-based linguistic kbs from source language to target language. our model contains three modules including (1) monolingual word embedding learning which is intended for learning semantic representations of words for source and target languages respectively; (2) cross-lingual word embedding alignment which aims to bridge the gap between the semantic representations of words in two languages; (3) sememe-based word embedding learning whose objective is to incorporate sememe information into word representations. for simplicity, we do not consider the hierarchy information in hownet in this paper."
"before reading simulation data to the graph, we sug- gest converting the model graph to a system graph. this transformation can help to make the visualization more intuitively comprehensible to the user. in this example, we transform the model graph in such a way that the pipes are converted to be edges between network nodes instead of nodes themselves. for this example, we decided to keep the valves as nodes, thus showing both possible pathways of keeping a component type as nodes and converting nodes to edges for the pipes. this could be changed according to the specific application with little effort. fig. 4 shows the result of the conversion, with the network nodes marked in a blue color and the pipes being represented by edges. even though the advantages of this transformation may not be highly significant for this simple example, the use case in section 6 will demonstrate the benefits in the context of a larger pipe network. furthermore, the distinction between model and system graphs allows for more dedicated class definitions with focus on parsing the modelica file for the model class and focus on visualization for the system class."
"in the previous sections, we used a rather academic example to demonstrate the process and functionalities of the presented approach. in this section, we show a use case for which the presented visualization tools were originally developed. we investigate a district heating network that supplies about 120 buildings with heat from one central heating plant. to model this system, we use simplified component models for pipes, the building substations, and the supply. the graphical representation of the system model is shown in fig. 7 . the pipe models calculate a pressure drop depending on the mass flow rate and have a thermal connection to the ground temperature to calculate thermal losses. the building substation models include a control valve, adjusting the mass flow rate according to building heat demand given as a table input. the supply model consists of a simple pump model and an ideal heat source, controlling the network's supply temperature to a set temperature depending on the outdoor air temperature. considering the about 120 buildings, over 200 pipe elements in the supply and return lines, and the loops figure 8 . the district heating network's system graph in both, the district heating network qualifies as a complex thermo-fluid system. using only 2d line plots to visualize mass flow rates and temperatures for the entire system can thus be cumbersome. in this context, the presented visualization approach can help to verify and better understand the system behavior of the model. for the system model, there are two largely identical pipe networks, one for the supply lines from supply plant to the buildings and one for the return lines from buildings to the supply plant. we modeled both these networks, but only used graphical annotations for the modelica code of the supply lines. therefore, the return pipes and their connections are not shown in the diagram view of the modelica model. this leads to a clearer model view, yet makes it even more important to verify the model results in order to ensure that all these connections are correct."
"building on the previous work done by the python developers mentioned above, we set out to present an approach for visualizing the dynamic behavior of complex thermo-fluid networks modeled in modelica by means of a python post-processing."
"here, the monolingual term l mono is designed for learning monolingual word embeddings from nonparallel corpora for source and target languages respectively. the cross-lingual term l cross aims to align cross-lingual word embeddings in a unified semantic space. and l sememe can draw sememe information into word representation learning and conduce to better word embeddings for sememe prediction. in the following subsections, we introduce the three parts in detail."
proceedings of the 11 th figure 5 . schematic view of the visualization grid structure mum and maximum value will be calculated and mapped to a color coding using matplotlib's color-mapping function.
"where integral force represents the sum of the normalized forces integrated by time, and viscosity is the viscous factor in the crossbridge. this equation implies contraction or expansion of the sarcomere (not isosarcometric conditions). in the case of isosarcometric conditions, + c rel dg"
"where ρ x, ρ y, and ρ z denote cell resistance in x, y, and z directions, respectively. s x, s y, and s z represent the ratio of the volume to the surface in x, y, and z directions, respectively. in the model of ten [cit] (1)"
"returning to the illustrative example introduced with fig. 2, we can demonstrate the graph drawing part of the visualization output. unfortunately, as the presented approach directly aims at overcoming the limits of static data plotting, it is hardly possible to show the benefits of an animated visualization in the form of this paper. therefore, we attempt to mitigate this shortcoming in the paper by using the timeline representation given in fig. 6 . in order to avoid distractions, we limited the display to the plain graph drawing for four steps during the simulation time of 200 s."
"on the other hand, the degree of deformation of the ventricles during bcl of 600 ms decreased as apd 90 was shortened, and the graph of the strain change shifted upward. the strain variable was smaller in the epicardium than in the other ventricular tissues. as apd 90 decreased, the difference in strain graphs between the two ventricular tissues and the epicardium increased."
"having set all the graph's nodes according to the relevant model components, the parser returns to the modelica model file to extract the connection statements. for each connection statement involving those component instances that are represented as a node, an edge is added to the graph accordingly. in order to conserve all relevant graphical information from the modelica code, the parser also processes the annotations of the connection statements. if a connection in the modelica code is not drawn directly between two ports as a straight line, the intermediate points given in the annotations are inserted to the graph as separate network nodes. as a result, the graphical representation of the graph will better match the original modelica model. for the example model shown in fig. 2, the model graph is displayed in fig. 3 ."
"we recently developed image-based three-dimensional ventricular electromechanical models, that can quantitatively compare the energy consumed by electrophysiological phenomena during mechanical beating [cit] . in the present study we observed changes in apd according to the electrical conductivity of the k + channel at the single-cell level, and extended these results to three-dimensional ventricular tissue."
"the system graph is created as a data structure and template to visualize the dynamic system behavior. in order to read the simulation result data into this structure, the system class can access top-level system data directly by making use of result handling methods from the package modelicares. for handling result data of the individual components, system calls special component classes. a basic component class defines methods for extracting certain data from the result file for a component in a general way. examples for such methods are get_mass_flow_rate or get_temperature, which return the time-series of mass flow rate or temperature in the component respectively."
"in the second example, accurate chinese translated counterpart for canoeist does not exist, but our model still hits all the three annotated sememes in the top-5 predicted sememes. by observing the most similar chinese words, we can find that although these words do not have the same meaning as canoeist, they are related to canoeist in different aspects. for example, 短跑 \"sprint\" and canoeist are both in the sports domain so that they share the sememes 锻炼 \"exercise\" and 体育 \"sport\". 名将 \"sports star\" has the meaning of sports star and it can provide the sememe ⼈ \"human\" in sememe prediction. furthermore, it is noteworthy that our model predicts 船 \"ship\" due to the nearest chinese words 独 ⽊ ⾈ \"canoe\" and 皮 艇 \"kayak\", whereas 船 \"ship\" is not annotated for canoeist in hownet. it is obvious that 船 \"ship\" is an appropriate sememe for canoeist. since hownet is manually annotated by experts, misannotated words always exist inevitably, which in some cases underestimates our models."
"next, we give a detailed explanation of target-to-source matching, and the source-totarget matching is defined in the same way. we first introduce a latent variable"
"in terms of processing the data, when adding a node to the model graph, a component object is automatically initialized and attached to the respective node. to this end, networkx allows for setting data and objects as attributes to nodes, edges, or the graph itself. in the implemented approach, each object that is attributed to a component's graph representation is also moving from the model into the system graph. as a result, all data regarding the component can be accessed by user-friendly methods like the get_mass_flow_rate mentioned above for each node and edge. thus, the object-oriented approach from the modelica model is followed also in the post-processing by an object-oriented python implementation."
"english word handcuffs 用具 \"tool\", \"police\", \"detain\", ⼈ \"human\", \"guilty\" 5 nearest chinese words ⼿铐 \"handcuffs\" \"guilty\", \"police\", ⼈ \"human\", \"detain\", 用具 \"tool\" 镣铐 \"shackles\" \"guilty\", \"police\", ⼈ \"human\", \"detain\", 用具 \"tool\" 绑 \"tie\" 包扎 \"wrap\" 螺丝⼑ \"screwdriver\" 用具 \"tool\", 放松 \"loosen\", 勒紧 \"tighten\" 绳 \"rope\" 线 \"linear\", 材料 \"material\", 拴连 \"fasten\""
"a simple and intuitive method is to let words with similar sememe annotations tend to have similar word embeddings, which we name word relationbased approach. to begin with, we construct a synonym list from sememe-based linguistic kbs of source language, where we regard words sharing a certain number of sememes as synonyms. next, we force synonyms to have closer word embeddings."
"the three dimensional ventricular simulation was conducted by setting the basic cycle length (bcl) to 600 ms. the change in apd during 1 cycle and the conduction wavelength were measured. where cv is the conduction velocity in myocardial cells. apd 90 means the time point where ventricular cells become excited and 90% repolarized. for calculation of conduction velocity, we specified the node (down) located on the lower side of the ventricular model surface and the node (up) on the upper side of its vertically positioned surface. the conduction velocity was calculated by dividing the distance along the straight line between these two nodes by the difference in the time of the response to a stimulus between the two nodes."
"formally, we let w s i be original word embedding of w s i andŵ s i be its adjusted word embedding. and let syn(w s i ) denote the synonym set of word w s i . then the loss function is:"
"in experiments, we take chinese as source language and english as target language to show the effectiveness of our model. experimental results show that our proposed model could effectively predict lexical sememes for words with different frequencies in other languages. our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages."
"in terms of our cross-lingual sememe prediction task, parallel data-based bilingual wrl methods are unsuitable because most language pairs have no large parallel corpora. besides, unsupervised methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. [cit] to enhance its performance."
"once verified, the visualization can also be used as a tool to better understand system behavior and thus assist in planning of the system operation. in real-world thermo-fluid networks, the exact ways the energy flows take is often not known. especially in district heating networks that include multiple loops and where the pipes are buried in the ground, it can be hard to measure the direction and flow rates of all the pipes. in these cases, as fig. 10 indicates, the visualization can help to identify main routes of energy flows as well as pipe elements with low flow rates. yet, to draw conclusions for the operation of the actual system, efforts must be made to verify such observations in the real-world system, as model assumptions and malfuntions in the actual system can lead to deviations between simulation and real-world operation."
"the heart's blood pumping activity is caused by the repeated contraction and relaxation of the cardiac muscle. contractions are caused by electrical conduction/propagation and the mechanical behavior of myocardial cells [cit] . the action potential (ap) of ventricular myocardial cells leads to the contraction of myofilaments through the transmembrane passage of various electrical ions (especially ca 2+ ions) [cit] . the ap may cause secretion or excretion of these ions, as well as myocardial contractility. changes in ap may cause abnormal transmembrane currents in myocardial cells, which in turn may cause cardiac arrhythmias and other heart diseases."
", but the amplitude in the epicardium was smaller. the amplitude of the atp graph was smaller as apd 90 decreased, and the change in atp consumption at 150 apd during bcl of 600 ms was reduced by 50% as compared to 299 apd ."
"we performed a single cellular electrophysiological simulation by increasing electrical conductivity of the i ks channel, which affects the apd of myocardial cells. the results are presented in fig. 2 . figure 2a -c shows intensity of the i ks current depending on the electrical conductivity of the i ks channel, where a is an \"endocardium cell, \" b is a \"midmyocardium cell, \" and c is an \"epicardium cell. \" in each graph, \"normal\" is the case where the electrical conductivity of the i ks channel is normal. labels \"gks2\" to \"gks10\" indicate that the electrical conductivity of the i ks channel is 2, 4, 6, 8, or 10 times the normal value, respectively. in fig. 2a -c, the current flowing through the i ks channel is increased due to the increased electrical conductivity of the i ks channel. the i ks current, which was slowly delayed at normal electrical conductivity, was rapidly delayed as the electrical conductivity of the i ks channel increased."
"from this, we can see that ϵ serves as a threshold to keep out unreliable matched pairs. the viterbi m step performs maximization as if the latent variable has been observed in the viterbi e step. thus, we can treat the matched pairs as correct translations, and use a l 2 regularizer as well. consequently, the m step computes:"
"cross-lingual word embedding alignment aims to build a unified semantic space for the words in source and target languages. [cit], we align the cross-lingual word embeddings with signals of a seed lexicon and selfmatching. formally, l cross is composed of two terms including alignment by seed lexicon l seed and alignment by matching l match :"
"as for the matching process, it is founded on an assumption that each target word should be matched to a single source word or a special empty word, and vice versa. the goal of the matching process is to find the matched source (target) word for each target (source) word and maximize the matching probabilities for all the matched word pairs. the loss of this part can be formulated as:"
"as a second input to the visualization process, the system class uses methods from modelicares to read data from the result file into python. this data can be selected according to the purpose of the visualization. yet, for analyzing thermo-fluid systems, we will concentrate on the processing of mass flow rates, pressures, enthalpies, and temperatures. in order to handle this data efficiently, networkx allows to attach almost any kind of data and objects to individual nodes and edges. thus, each node and edge representing a model component can hold its relevant information from the modelica result file. as the dynamic behavior of the system is of special interest, each dataset contains the time-series of data for every time-step of the simulation."
"the electrophysiological part of the model is based on the ion model proposed by ten [cit] . this model, which reproduces the conduction phenomenon of aps in myocardial cells, was applied to the electrical conduction equation based on continuum mechanics:"
"for animating the individual plots in a video, we use the lightweight and freeware software images to video [cit] . this software can be called via a command line interface with all settings saved in an xml file. as these steps can be executed from within the python environment, the solution requires no effort from the user. also, the user is free to work directly with the individual plots created or use different software to create a video. still, this part could be improved upon if a python package for creating the video could be used instead for a more integrated process."
"many researchers classify faults by the k-means algorithm. m. laszlo and s. mukherjee [cit] are motivated by the observation that the popular k-means method for clustering is very sensitive to the initial set of centers with which it is seeded. the new method employs a simple chromosomal representation which is easily implemented and yields partitions of better quality. they have demonstrated that their ga works as well as published methods on the small data sets considered in the literature, and that it produces near optimal partitions for data sets of much larger size and dimensionality. jim z.c. lai, tsung-jen huang and yi-ching liaw [cit] present a fast k-means clustering algorithm (fkmcucd) using the displacements of cluster centers to reject unlikely candidates for a data point. the computing time of the proposed algorithm increases linearly with the data dimension d, whereas the computational complexity increases exponentially with the value of d. theoretical analysis shows that the method can reduce the computational complexity of full search by a factor of sf and sf is independent of vector dimension."
a key problem in fault detection and diagnosis is the issue of misclassification due to noises or outliers in the raw data. for this reason a combined approach based on k-means clustering and pnn is presented in this paper.
"in general, fault diagnosis involves a two-step sequential process of symptom extraction and actual diagnosis. many fault diagnosis algorithms have been proposed in the literature [cit], such as statistics, geometric, neural network, expert system, k-means and fuzzy methods, with applications to traffic systems, chemical processes, electrical systems, and rotating machinery [cit] . depending on the information available to classify training, there are two kinds of classification ways. one is supervised learning [cit], such as pnn and svm. the other is unsupervised learning [cit], also called clustering."
data access. data can be downloaded from http://dx.doi.org/10.5281/zenodo.58985. further information including a link to the download site is given on a website hosted by neurodata (http://neurodata.io/wanner16).
"coal mining requires various kinds of machinery. the fault diagnosis of this equipment has a great impact on mine production. the problem of incorrect classification of noisy data by traditional support vector machines is addressed by a proposed probability least squares support vector classification machine (plssvcm) [cit] . the introduction of a probability value gives the classification results from traditional lssvcm a quantitative evaluation to match their qualitative explanation. the plssvcm is applicable to binary and multi-class problems. diagnostic results using roller bearings show that plssvcm performs well with small-scale fault diagnosis problems. it has a perfect generalization property even with uncertain, noisy samples. if the number of samples is larger than large computation, loads will be needed to use the cross-validation method of determining the regularization factor and the width of a gaussian kernel. therefore, future research will focus on the selection of optimal kernel parameters for plssvcm that will improve its accuracy even more."
"cluster pnn is a feed-forward neural network with supervised learning, which uses bayes decision rule and parzen window. in this paper, pnn is used to diagnose faults in each cluster. figure 7 shows the fault types divided by k-means-pnn. in figure 7, each segment contains 300 data and cluster no.1 is classified into three kinds of types by k-means-pnn. label 1 is normal data, label 2 is fault 1, label 3 is fault 2, and label 4 is fault 12. in figure 7, normal data segment contains 300 data, each fault type also contains 300 data, and the total data is 1200. in each fault type segment, the first 160 data are the normal data and the last 140 are the fault data. figure 7 shows that the correct percentage of classification by k-means-pnn is 91.33%."
"pyknossos: synapse annotation mode. synapses can be annotated by three successive clicks to place connected nodes onto the first neuron, the synaptic cleft, and the second neuron. if skeletons of the two neurons are available, the first and the last click connect to existing skeleton nodes. the first click can be omitted if a skeleton node is active. synapses can be assigned to user-defined classes and scored with a confidence level. figure 3 . access of data from web server using pyknossos. pyknossos running on a local machine streams cubed image data from a web server (data store; neurodata) and stores cubes on disk. cubes are generated by neurodata and dynamically loaded into pyknossos. annotations (e.g., skeleton reconstructions) are downloaded from a repository (label store) and may be modified locally."
"some articles add some clustering algorithms before ann classification [cit] . r.eslamloueyan has proposed a duty-oriented hierarchical neural network (dohann) for isolating the faults of a relatively complex process. the concept behind the suggested method is to appoint a specific fault diagnostic agent for a particular set of similar fault patterns. the performance of dohann has been evaluated and compared to some other methods of fault diagnosis through using the test data of te process. according to the results, the do-hann recognizes the process faults much better than snn. training of each network in the dohann is carried out more conveniently because its neural networks are structurally simpler than that of the snn model."
"an indexed key table consisting of all possible keys for a desired key length is published globally. for example if a key of length 32 is required, all possibilities of the keys are generated, tabulated and indexed. the index identifies the key to be selected at any instant of time based on the chaotic value generated."
"where d s,i and y s,i are the ith target and actual outputs corresponding to the sth training pattern, w is a vector composed of all the weights and biases involved in the network, and n is the number of output units. in this scheme, an initial weight vector w 0 is iteratively adapted according to the following recursion to find out an optimal weight vector. the positive constant η is the learning rate."
an ensemble fault diagnosis method based on kmeans and pnn called k-means-pnn is presented in this paper. simulation studies show that the proposed algorithm provides an accepted degree of accuracy in fault classification under different fault conditions and the result is also reliable.
"the proposed work explains a new way of generating the elements in and p arrays and s box. experimental results clearly show that the algorithm generates highly non linear s boxes and p arrays while preserving the same level of security as in blowfish. the encryption quality for text has been measured by means of key sensitivity tests, key space analysis and avalenche effect analysis. the algorithm produces good quality cipher texts by employing chaos theory for generation of p arrays and s boxes used in blowfish at a reduced time complexity based on the experimental results for image encryption, it can be observed that the proposed cipher offers high encryption quality with minimal memory requirement and computational time. also the algorithm offers sufficient resistance towards brute force attack and statistical crypt analysis of original and encrypted images."
"the resistance of any encryption algorithm offered against cryptanalysis is measured by its avalanche effect [cit] i.e., for a small change even in one bit of plain text or key should produce significantly differing cipher texts by many number of bits. the modified blowfish algorithm is experimentally found to exhibit good avalanche effect for plain texts differing by one bit. the results of every round have been tabulated below."
the modified blowfish algorithm is experimented with different seed values for the chaotic equation and the results demonstrate that a perfect non-linear relation ship exists between the plain text and the seed thus providing high resistance towards differential crypt analysis.
data viewing using pyknossos. [cit] 05_skeletonsglomeruli.zip from the location specified above. [cit] 05.csv contains a table that lists the neurite length of each neuron in each glomerulus. the csv-file can be loaded into pyknossos using the neuronlibrary plugin (fig. 4b) . this plugin facilitates various operations such as filtering of neurons by cell type or glomerulus (fig. 5) .
"in order to test the accuracy of the k-means algorithm, the data set of iris flower is used. figure 4 shows that the k-means algorithm divides iris flowers into three species based on iris flowers length of sepal feature. the red is virginica, the blue is setosa, and the green is versicolor."
"this algorithm is proposed to improve the degree of accuracy on fault diagnosis. at the same time, it is a combination of algorithm about k-means and pnn. figure 3 shows the sketch of the algorithm, which is briefly described as follows:"
"1. initialize first the p-array and then the four s-boxes, in order, with a fixed string. this string consists of the hexadecimal digits of pi (less the initial 3). for example:"
data viewing using pyknossos. [cit] 05_skeletonsglomeruli. zip from the location specified above. the file glomeruli.nmx contains the outlines of all glomeruli. use the 'open' command in the 'file' menu of the 'settings' window to load the file in pyknossos. to view glomeruli together with skeletons and image data we recommend to adjust the glomerular region opacity in the 'region' tab of the 'visualization' tab in the 'settings' window. data access. data can be downloaded from http://dx.doi.org/10.5281/zenodo.58985. further information including a link to the download site is given on a website hosted by neurodata (http://neurodata.io/wanner16).
"to prevent the leakage of information to an opponent, it is also advantageous if the cipher image bears little or no statistical similarity to the plain image. an image histogram illustrates how pixels in an image are distributed by graphing the number of pixels at each color intensity level."
"in general, the bp algorithm [cit] includes the forward course and the backward course. in the forward course, a vector is added to the input layer, which is then spread along the network; finally, an output vector is obtained as a response of the input vector, in which the synaptic weights can not be changed. in the backward course, an error signal will be obtained by comparing the output signal with the desired output; the error signal is forward-spread to modify the weight from one output layer to another. the modified network will output the signal that is closer to the desired output. the forward course and the backward course alternate and constantly circulate, and the output will be convergent with the desired output in some states. figure 8 shows that no.1 is classified to three kinds of types by k-means-bp. label 1 is the normal data, label 2 is the fault 1, label 3 is the fault 2, and label 4 is the fault 12. in figure 8, each segment contains 300 data, and the total data is 1200. in each fault type segment, the first 160 data are the normal data and the last 140 are the fault data. figure 8 shows that the correct percentage of k-means-bp is 60%."
"dissection. all animal procedures were approved by the veterinary department of the canton basel-stadt (switzerland). fish were housed and bred in an animal facility under standard conditions. the sample was prepared from a zebrafish larva 4.5 days post fertilization (dpf) that expressed the genetically encoded calcium indicator gcamp5 under the control of the panneuronal elavl3 promoter 41 . the sex is not yet determined at this developmental stage. prior to sample preparation, the larva was paralyzed with mivacron, mounted in low-melting agarose (sigma aldrich a9414), and immersed in standard e3 zebrafish medium 42 (5 mm nacl, 0.17 mm kcl, 0.33 mm cacl 2, 0.33 mm mgso 4, ph 7.4). responses of olfactory bulb neurons to odor stimulation were then measured by multiphoton microscopy as described 43 over a period of approximately 4 h. subsequently, the larva was anesthetized in tricaine methanesulfonate (ms-222) and a small craniotomy was made above the contralateral olfactory bulb using a glass pipette to facilitate the penetration of reagents into the brain. immediately afterwards the larva was transferred into em fixative (table 1) . fixation and staining. the sample was fixed and stained en bloc using an established protocol with minor modifications 21, 22 . the procedure includes successive exposures of the sample to reduced oso 4, thiocarbohydrazide (tch), oso 4, uranyl acetate and lead aspartate. reagents and solutions are described in table 1 . tissue was initially fixed in em fixative for 1 h at room temperature and 1 h on ice. after five washes (always 3 min each) with ice-cold cacodylate buffer, samples were postfixed in redos solution for 1 h on ice, washed 5 times in bidistilled h 2 0, and incubated in fresh tch solution for 20 min at room temperature. in a second postfixation step, samples were incubated in os solution for 30 min at room temperature and washed five times in bidistilled h 2 0. the sample was then incubated in ua solution and left in a refrigerator (4°c) overnight, washed five times in bidistilled h 2 0, incubated at 60°c for 30 min in bidistilled h 2 0, incubated in walton's lead aspartate for 20 min at 60°c, and washed five times in bidistilled h 2 0 at room temperature. the sample was then dehydrated in an ethanol series (20%, 50%, 70%, 90%, 100%, 100%; 5 min each), incubated in 50% ethanol and 50% epon resin for 30 min, and incubated in 100% epon resin for 1 h at room temperature. epon resin was exchanged and samples were again incubated at room temperature for 4-12 h (overnight)."
"there are many methods for fault diagnosis [cit] . usually, fault diagnosis methods are classified into three general categories: quantitative model-based methods, qualitative model-based methods, and process data-based methods [cit] . in the early days, many researchers diagnose faults by the model-based method. however, model-based fdd uses mathematical system models to estimate the system state and parameters, and in general these methods can only solve the problem of low dimensional systems. alternatively, data-driven fdd can deal with high dimensional data, and data dimension reduction techniques [cit] are generally applied to many areas of information."
"2. xor p1 with the first 32 bits of the key, xor p2 with the second 32-bits of the key, and so on for all bits of the key (possibly up to p14). repeatedly cycle through the key bits until the entire p-array has been xored with key bits. (for every short key, there is at least one equivalent longer key; for example, if a is a 64-bit key, then aa, aaa, etc., are equivalent keys.)"
analyzing its strength against statistical attacks proves the robustness of any algorithm. the strength of the proposed algorithm is measured by the histogram analysis of the plain and encrypted images and by the correlation coefficient between the adjacent pixels in both plain and encrypted images.
"the dataset consists of 50 samples from each of three species of iris flowers (iris setosa, iris virginica this data set in cluster analysis is uncommon, since the data set only contains two clusters with rather obvious separation. one of the clusters contains the iris setosa species, while the other cluster contains iris virginica and iris versicolor, and is not separable without the species information fisher used. this makes the data set a good example to explain the difference between supervised and unsupervised techniques in data mining: fishers linear discriminant model can only be obtained when the object species are known."
"pyknossos: orthogonal tracing mode and arbitrary reslices. in the default configuration, pyknossos has five viewports (fig. 4a) . image data is displayed in four viewports: the yx viewport (imaging plane) and three mutually orthogonal viewports of arbitrary orientation. in tracing mode, one of the latter is perpendicular to the current tracing direction. we find that this 'auto-orthogonal' view increases tracing speed and facilitates the identification of branch points and synapses. the reslice views are generated by a tri-linear interpolation procedure to efficiently extract reslices at arbitrary orientations and zoom levels. the fifth viewport displays the skeleton reconstructions."
the histogram of the encrypted image is fairly uniform and significantly different from the respective histograms of the original image and hence does not provide any clue to employ any statistical attack on the proposed image encryption. figure 3 shows the histogram analysis of plain and original images. the histogram analysis shows that the histogram of the cipher image is fairly uniform and is significantly different from the original image. the encryption algorithm has covered up all the characters of the plain image and has complicated the statistical relation ship between the plain image and its ciphered version.
"the signal-to-noise ratio and resolution of image data is clearly sufficient to trace neurites and identify synapses. conductive sample embedding enabled data acquisition in high vacuum. in tests performed with comparable samples, high vacuum conditions increased the signal-to-noise ratio of em images by a factor of three or more compared to low vacuum conditions (14 or 20 pa water pressure) 26, which have been used in most previous sbem applications 6, 18 . no noticeable image distortions were seen throughout most of the dataset because tile sizes were small. occasionally, small local distortions occurred in regions where the density of conductive material is low. however, these distortions do not affect image interpretation. high quality of image alignment throughout the volume was confirmed by orthogonal reslices and specific inspection of stitching boundaries 26 . neuron reconstruction is error-prone and requires error correction procedures to obtain high accuracy. in order to achieve efficient error correction we initially generated three independent reconstructions of each neuron. errors were then identified and corrected by core, a semi-automated method that involves inspection of mismatch points by an expert and local re-annotation around mismatch points 26 . the final reconstruction accuracy was quantified by different measures: (1) recall, a measure for errors caused by missed processes; (2) precision, a measure for errors caused by wrongly traced processes; and (3) the 'relative length error', a measure that takes into account both types of errors 26 . accuracy was evaluated against a ground truth generated by highly redundant reconstruction and expert error correction, and by comparing independent reconstructions to each other. error quantification showed that accuracy was high: typically, the total length of missing or incorrectly annotated processes was o5% of the total neurite length 26 . additional tracing of 'orphan' profiles showed that missed processes were usually short terminal branches, o3.5 μm in length, with few synapses 26 ."
"6. replace p3 and p4 with the output of step (5). 7. continue the process, replacing all entries of the p-array, and then all four s-boxes in order, with the output of the continuously changing blowfish algorithm."
blowfish algorithm [cit] requires 521 encryptions of itself to generate the sub keys namely 18 entries in p arrays and 1024 entries in s boxes. the proposed design replaces the 521 encryptions by adapting chaos functions in order to generate highly non linear and key dependant p arrays and s boxes.
"the algorithm has been implemented in mat lab 6.0 in windows environment with a system configuration of piv processor with 1 gb ram. the proposed algorithm has been tested with various images in usc-sipi repository which is a collection of digitized images primarily to support image processing, image analysis and machine vision. (http://sipi.usc.edu/database/)."
"previous studies have mapped glomeruli in the developing zebrafish ob, reported expression patterns of various marker genes, determined projection patterns of mitral cells, and characterized odor responses of glomeruli and ob neurons 43, [cit] . anatomical information about neurons in the zebrafish ob is, however, incomplete. most anatomical studies focused on mitral cells 43, 49, 50, 57, 58 but only few in types have been characterized in detail 31, 59 . the detailed morphological information contained in our datasets may therefore be exploited to generate a comprehensive atlas of the zebrafish ob that integrates molecular, anatomical and functional information."
"this paper is organized as follows. the second section briefly introduces fault diagnosis strategy based on k-means-pnn. and to validate the performance and effectiveness of the proposed scheme, the fault diagnosis approach using k-means-pnn based on simulation benchmark of te process is illustrated in section iii. finally, concluding remarks are made in section vi."
"future work may generate dense volumetric reconstructions of neurons in the zebrafish ob by combining automated segmentation with skeleton reconstructions 7, 8, [cit] . this approach uses machine learning methods to over-segment em image data in 3d, resulting in 'supervoxels' representing subvolumes of neurons. skeletons are then used to merge supervoxels from the same neurons. the generation of skeletons is typically a bottleneck in this procedure because it involves substantial manual labor. the skeletons provided along with our em image data should therefore greatly facilitate volumetric neuron reconstructions."
"in total, 521 iterations are required to generate all required sub keys. applications can store the sub keys rather than execute this derivation process multiple times."
"manual tracing. skeletons of neurons were traced manually using knossos or pyknossos. tracings of ob cells were initiated from seed points close to somata. when cells showed obvious features of glia such as sheet-like processes, tracings were abandoned. a small number of cells was excluded because their morphology was not neuron-like (usually no or only minor processes) or because they did not extend processes into neuropil regions of the ob. all reconstructions of ob cells with neuron-like morphology were completed and included. starting from seed points, tracers followed neuronal processes by placing successive nodes onto cross-sections of neurites in the original image data, resulting in a skeleton representation of each neuron 27 . most skeletons were reconstructed by a professional tracing service (www.ariadne-service.ch)."
"an s box can be thought of as a miniature substitution cipher. the input to an s box could be a n bit word, but the output can be an m bit word where m and n are not necessarily the same. an s box can be keyed or keyless and linear or non-linear. shannon suggested that all block ciphers should have two important properties namely diffusion and confusion. the idea of diffusion is to hide the relation ship between the plain text and the cipher text, which will frustrate the adversary who uses cipher text statistics to find the plain text. diffusion implies that each symbol in the cipher text is dependant on some or all symbols in the plain text. the idea of confusion is to hide the relation ship between the cipher text and the key, which will frustrate the adversary who uses cipher text to find the key. in other words, if a single bit in the key is changed, most or all bits in the cipher text will also be changed. every iteration makes use of s boxes, p boxes, and other non linear operations in order to provide diffusion and confusion."
"data access. the image data is available under http://doi.org/10.7281/t1ms3qn7 and can be accessed through the neurodata web services (neurodata; http://neurodata.io/wanner16). data can either be viewed interactively through the neurodataviz web interface (http://viz.neurodata.io/ [cit] 05/) or using pyknossos (fig. 3) . neurodata provides various apis to extract and download specific subvolumes ('cutouts') for external analysis. a detailed description of the services is available at http://neurodata.io/wanner16. data viewing in pyknossos. the repository containing pyknossos code (https://github.com/ adwanner/pyknossos) includes a dataset configuration file (in folder datasets/wanner16) to access em image data hosted by neurodata web services from a local pyknossos installation. after installation of pyknossos the dataset can be accessed by loading the configuration file from the menu entry 'datasets' in the 'settings' window. the program automatically fetches the cubed image data around the current location using the online jpeg stack service of neurodata. by default, pyknossos uses one download stream but the user can increase the number of parallel download streams in the 'loader' tab of the 'settings' window. pyknossos uses a hybrid approach for online image data streaming. any downloaded image cube is stored permanently on the local disk in the same folder as the configuration file (fig. 3) . this minimizes the data streaming traffic because each image cube has only to be downloaded once. in addition, the permanent caching enables offline browsing of already visited locations."
"where n i denotes the total number of samples in class c i . if the priori probabilities for each class are the same and the losses associated with making an incorrect decision for each class are the same, the output layer classifies the sample x accordance with the bayes's decision rule based on the outputs of all the summation layer neurons and the final result of output is y ."
the strength of any cryptographic algorithm depends on the size of its key space to make brute force attack infeasible. the values of p arrays and s boxes depend on the initial seed and its related parameters associated with the chaotic equation. the number of initial seed can vary between 0.2 to 0.8 and hence the key space depend upon the number for decimal places of the mantissa that are to be supported by the calculating machine which is approximately infinitely large making brute force attack computationally infeasible. even for the same initial seed with different skip value the p arrays and s boxes generated are completely different and non linear. figure non linear relation ship between s box and random seed samples
"pyknossos: general description. we developed pyknossos, a software package for ergonomic manual skeleton tracing, visualization and annotation of neurons in 3d image datasets (figs 3 and 4). other publicly available applications for similar purposes include knossos (www.knossostool.org) 27, catmaid (http://catmaid.org/) 44 and ilastik (www.ilastik.org) 45 . pyknossos was inspired by knossos but is fully implemented in python, which enables rapid and efficient code development, modification and extension. efficient real-time 3d visualization of large numbers of reconstructed neurons together with raw image data is achieved using the vtk-python bindings in combination with custom image data loading procedures written in c++. the user interface of pyknossos has been designed by close interactions between programmers and users to integrate user feedback into the application as directly as possible. pyknossos is written in python (version 2.7) and runs on various operating systems including linux and windows 7. the main graphical user interface is based on the pyqt4 library. pyknossos can be run as stand-alone executable or directly from script using a python interpreter. all source code and detailed installation instructions can be found on https://github.com/adwanner/pyknossos."
"bp neural network is a kind of feed forward, whole link, and multi-layers network. its characteristics include the association ability, memory ability, self-flexibility, self-learning and so on. it provides a new method to solve the prediction and control problem for the non-linear system [cit] ."
"there has been an increasing interest in fault diagnosis in recent years, as a result of the growing demand for higher performance, efficiency, reliability and safety in control systems. industrial fault may cause inevitable economic losses, seriously environmental pollution and even fatal injuries. quick fault diagnosis method can help avoid abnormal event progression and minimize the quality and productivity offsets."
"for application of the above function for generating p arrays and s boxes in blow fish algorithm, it is proposed that the values yielded by the chaos function are to be converted to appropriate key representations. for this, the following three factors have to be agreed upon by the users."
"our datasets can be further mined to address a wide range of questions in cellular, developmental and systems neuroscience. an obvious next step is to annotate the synapses between reconstructed neurons in order to reconstruct the full wiring diagram, an effort that is underway. exhaustive dense reconstructions of wiring diagrams have so far been achieved only in c. elegans 2, 3 and in representative subvolumes of very few other circuits 5, [cit] 14 . the topological analysis of wiring diagrams is particularly important to understand the function of neuronal circuits whose connectivity cannot be approximated based on topological relationships or other means, as in the ob."
"dataset 2: skeleton reconstructions of neurons in the ob dataset 2 (data citation 2) contains manual skeleton reconstructions and soma outlines of 1,022 neurons in the larval zebrafish ob (101 mb; fig. 2d ). each reconstructed neuron has a unique id and is provided in the native pyknossos file format *.nmx. an nmx-file is a zip-archive container in which annotations and skeletons are saved in individual nml-files, the native knossos file format, which is xml-based 27 . each skeleton consists of 3d nodes (vertices) connected by edges. soma outlines are computed from convex sets of 3d points."
first we divide the te process data into six species by k-means algorithm. the following fault data are obtained by wavelet processing. figure 6 shows the clustering result.
"as each fault diagnosis technique has its disadvantages, the ensemble method makes traditional classification method improve. the ensemble method can solve many complex problems in fault diagnosis area."
"step2. given the set of cluster centers sc p, perform the lloyd iteration to generate the improved set of cluster representatives sc p+1 ."
"where a denotes the dimension of the pattern vector x, δ is the smoothing parameter and x i j is the neuron vector. each neuron in the summation layer computes the maximum likelihood of pattern x being classified into class c)ii by summarizing and averaging the output of all neurons that belong to the same class:"
"currently, bp algorithm is widely used for training neural networks [cit] . supposing a set of p training samples is available, the problem can be characterized as the process of minimizing the following sum squared error:"
"the ob of zebrafish and other species contains glomeruli responding to common odorants, as well as subsets of glomeruli that detect specific odorants with a defined biological function such as pheromones 52, 53 . our datasets provide a unique opportunity to explore whether these classes of glomeruli differ in their cellular composition and microcircuit organization, and to study how these glomeruli interact via intra-bulbar projections. moreover, the high density of our reconstructions allows for a detailed analysis of the sub-glomerular organization of neuronal microcircuits. preliminary observations indicate that individual mitral cells do not always innervate entire glomeruli but can be restricted to distinct subcompartments (fig. 5g ) 60 . it may thus be interesting to examine whether these subcompartments reflect distinct microcircuits associated with the same glomerulus. datasets described in this paper should be particularly valuable to link the morphology of neurons to their ultrastructure. the high resolution of the em data allows for the quantitative analysis of subcellular components such as mitochondria, endoplasmatic reticulum (er), primary cilia and synapses. features of these organelles may then be mapped onto the 3d reconstructions of individual neurons and compared across neurons. a quantitative analysis of synapses, for example, would provide the opportunity to compare the size and other ultrastructural features of synapses within the same neuron, across neurons of the same type, and across neurons of different types. additional analyses may then ask whether synapses of the same neuron vary systematically depending on the identity of the synaptic partner. moreover, it is possible to determine whether mitochondria, er or other organelles are associated preferentially with specific types of synapses. a combined morphological and ultrastructural analysis of neurons has major potential to discover unknown rules governing the organization of neuronal microcircuits 12, 13 . the development of automated procedures for the reconstruction of neurons from volumetric em data is an active field that requires ground truth datasets. such datasets are usually obtained by labor-intensive manual reconstruction and often contain only parts of neurons, which complicates the quantification of reconstruction performance at the level of entire neurons. our skeleton reconstructions may serve as valuable ground truth because they comprise a large number of neurons that were fully reconstructed with high accuracy. in the future, automated reconstruction may also incorporate statistical information about the geometry of neurons. such information needs to be extracted from large, high-resolution morphological datasets such as our skeleton reconstructions. our datasets can therefore serve a resource for the future development of automated reconstruction procedures."
"consolidation of skeletons and error correction. each neuron was initially reconstructed three times by different individuals. multiple tracings were then combined into a 'consolidated' skeleton by core ('convergence by redundancy and experts'), a procedure based on redundancy (multiple independent tracings of each neuron) and focused input by an expert. during the core procedure, errors were identified and corrected. the procedure is described in detail elsewhere 26 . briefly, individual skeletons were resampled to a node density of approximately 100 nm. starting from seeds, nodes of skeletons were iteratively combined into 'cliques' based on their distance and connectedness. the resulting cliques thus represented the agreement between independent tracings and formed the basis of the consolidated skeletons. disagreements between one skeleton and the others resulted in nodes that were not part of a clique. to resolve such disagreements, the point of divergence ('mismatch point') between the skeletons was inspected by a scientific expert. the expert usually recruited two additional tracers to generate additional tracings of local segments around the mismatch point. in rare cases, particularly when errors were obvious, the expert intervened directly and overruled the tracers. a new consolidated skeleton was then computed and the procedure was repeated until all mismatch points were resolved 26 ."
"in addition to the histogram analysis, we have also analyzed the correlation between two vertically adjacent pixels, two horizontally adjacent pixels and two diagonally adjacent pixels in plain image/cipher image respectively. the correlation coefficient is calculated using the formula table 1 that there is negligible correlation between the two adjacent pixels in the cipher image. however, the two adjacent pixels in the plain image are highly correlated."
"only a small amount of the information contained in the image data (dataset 1) and in the skeleton reconstructions (dataset 2) has been extracted and analyzed so far 26 . these datasets are unique because they represent the near-complete 3d ultrastructure of an entire brain region and a very dense morphological reconstruction of the neurons contained in this brain region. to facilitate further annotations and analyses of these datasets we describe basic procedures to access, visualize and organize these datasets."
"installation of pyknossos and data browsing window. 4. browse to the folder 'datasets/wanner16' that was included in the release zip-archive and select the dataset configuration file 'wanner16.conf'. 5. if you are in 'online' mode, i.e. you are connected to the internet and the option 'working offline' in the 'dataset' menu of the 'settings window' is not checked, the hybrid streaming of image dataset cubes should start immediately. 6. load skeletons and glomerular outlines as described above (data records, datasets 2 and 3)."
data access. data can be downloaded from http://dx.doi.org/10.5281/zenodo.58985. further information including a link to the download site is given on a website hosted by neurodata (http://neurodata.io/wanner16).
"data viewing using pyknossos. [cit] 05_skeletonsglomeruli. zip from the location specified above. each nmx-file with the file name pattern neuron_id*.nmx contains the soma outline and reconstructed skeleton of the neuron with the indicated unique id. to load one or multiple of these nmx files, start pyknossos, click on the 'open' command in the 'file' menu of the 'settings' window, and select one or multiple files. multiple skeletons can be loaded simultaneously or sequentially by appending new skeletons to the already loaded ones. load image data as described above (dataset 1) to overlay skeletons onto raw data. various display options for skeletons and somata can be found in the 'skeleton' and 'soma' tab of the 'visualization' tab of the 'settings' window, respectively."
"bemd can decompose the frequency of original image from high to low into a finite quantity of bimfs and a tendency image, of which bimfs satisfy two constraint conditions: first, being symmetry for local value of the original bidimensional signal and with its mean value equivalent to 0; second, its maximum values being positive and its minimum values being negative. bemd is an adaptive analytical method driven by parameter-free data, and is a sifting process. the steps of bemd are as follows:"
"pcnn, proposed by eckhorn [cit], is a simplified neutral network model of sync-pulse principle based on brain neurons of cat. broussard [cit] achieved image fusion with the help of pcnn to improve the recognition rate of target and demonstrated the relationship between the ignition frequency of pcnn neurons and grey scale of image, confirming the feasibility of the application of pcnn to image fusion."
"let rij be the non-ar and arij be an ar, both of which are not the source node. i presents router ri, the end node of one path. and j presents the hop number from source node. for instance, path a-b can be expressed as a-arb1-arb2-rb3. then the paths can be represent as table 1 . the purpose of our algorithm is to find out relations between all the arij. using single-source traceroute and source routing traceroute it is easy to get hop length between any two leaf nodes, based on what we calculate the intersection or connection of two paths and finish topology discovery. and as mentioned before, nodes' degree is used to help finding out the ars that is directly connected to a non-anonymous router."
"1) information entropy (ie). ie refers to the quantity of average information contained in an image. the bigger it is, the more information it contains and the better the fusion effect is."
"the application of m-pcnn is able to process multi-source images. multi-source images can be processed in parallel manner with multiple external stimuli working in the same neuron, which saves a lot of time spent in processing multi-source images and reduces the complexity of compute."
"of which, 1, 2 indicates entropies of 1 and 2 . (7) to carry out inverse transform on n fused m-bimfs and res to obtain the final fuse image."
"in recent years, medical image technology has been widely applied to medical diagnosis; however, single imaging technology is incapable of providing comprehensive diagnosis basis. for example, ct is of very high spatial resolution and can show better bone information but unclear soft tissue information; in contrast, mri is of lower resolution and can show better soft tissue information but fuzzy bone information. therefore, the fusion of two or more medical images with complementary information can provide more comprehensive information for medical diagnosis [cit] ."
"a. network model our algorithm is based on the result of single-source traceroute and assuming that every router supports source routing and every traceroute packet passed through a same path each time, and all the connection has the same bandwidth. here we ignore all hosts and remain only routers to make our algorithm be easy to describe. all other non-anonymous nodes have been merged or been discovered. the network model is as figure 1 (a) shows, in which black circles represent the ars. shows, in which the root and leaves are non-anonymous routers and other nodes are ars. our work is to find out which ars belongs to a router in real network, and which two ars are connected directly."
"of which, bimfj is bimf of j, and res is the final residue. the key problems of bemd are extreme point detection, interpolation method and stopping criteria. the correct selection of extreme points exerts direct influence on the construction of envelope surfaces. the method of morphological operators(morphological reconstruction and watershed) is used to detect extreme points in this paper. at present there are mainly two methods of envelope surface interpolation based on triangulation and radial basis function (rbf) respectively. rbf that has good interpolation effect on discrete points distributing sparsely and irregularly is used to interpolate in this paper. the method of standard deviation is adopted as stopping criteria. the standard deviation of two consecutive h i(j−1) and h ij (i means the bimf of i) in the sifting process is calculated, and the sifting process stops when the standard deviation is less than the defined threshold value which is usually defined as 0.2 ~ 0.3. the standard deviation is defined as follows:"
"in this paper we have addressed the problem of discovering computer network topology that contains anonymous routers. we give an algorithm for performing topology discovery using source routing traceroute method, which is efficient to solve the topology discover problem caused by anonymous routers."
"both situations above exist in real network. considering hardware configuration, our algorithm firstly makes ars an intersection and if the merge conflicts from a knowing condition, we choose to connect two ars then."
"2) correlation coefficient (cc) reflects the degree of correlation of is with f. the bigger cc is, the more information can be obtained by f from is and the better the fusion effect is."
"the purpose of using different decomposition methods in image fusion to obtain different coefficient matrixes or components is to fuse images in different and corresponding frequency range so as to achieve optimum fusion effect. after different images are decomposed by bimf, the corresponding bimf may not be the component of its corresponding frequency. because different images have different distribution of extreme points and different values of extreme points, which makes corresponding bimfs not correspond actually. if bimfs are directly fused, the final fusion result will be influenced by frequencies lacking in correspondence. to solve the above shortcomings of bemd in image fusion, we put forward the definition of m-bimf, i.e. a new bimf formed by adding several bimfs together. since bemd satisfies the decomposition process from high frequency to low frequency, m-bimf is also energy distribution from high frequency to low frequency. for instance, m-1 with maximum frequency is formed by adding the first and second bimfs together; m-2 with the second maximum frequency is formed by adding the third and fourth bimfs together, ···, so (1) can be expressed as:"
"if there are too many decomposed layers, false information may be introduced in the sub-images, and the quality of fused image will be influenced in the end; if there are too few decomposed layers, it will degenerate into the mode of single-scale image fusion, and loss the advantages of multi-scale mode. similarly, if there are too many decomposed layers by bemd, there is energy loss in the image obtained through inverse transform after fusion and the fusion result is not good. the quantity of bimfs should be reduced in fusion when the stopping criteria of decomposition are not altered. when envelope surfaces are constructed and interpolation is done to discrete extreme points, there may be maximum value points nearby minimum value points, and vice versa due to their disperse distribution, and upper and lower envelope surfaces may intersect, which extends the boundary of the image texture. the extension of boundary alters the distribution of latter extreme points, and thus exerts influences on the construction of latter bimfs. the interpolation method does not bring error but makes the original texture information not properly reflected in each bimf. for example, bimf1 in figure 2 (b) does not properly reflect the characteristics of ct. single bimf sometimes does not properly reflect the original texture features of an image, so more reasonable method is needed to process bimf to make it capable of better reflecting the characteristics of the image."
"where, s, are the respective average gray value of is and f. 3) spectral distortion (sdi) is one of metrics for measuring distortion of the spectrum of the image. a lower sdi tends to promise a high quality of image fusion, if the metrics is adopted in this field."
"in order to illustrate the advantages of the method we proposed, a comparison with such existing methods has been carried out as dual channel pcnn, morphological pyramid, and wavelet fusion. figure 3 shows the fusion effects of the four methods. subjectively, fuzzy and edge phenomena are presented in eyes in wavelet fusion, and the fusion result showed new information (background color became bright) outside the brain. the fusion by the morphological pyramid had better contrast but also had false edge. the fusion by dual-channel pcnn had relatively clear edge and texture, while bemd fusion not only had the advantage of dual-channel pcnn, but also had higher contrast and clearer edge. in the extreme case of bemd fusion, all bimfs were added together (with very small res), and the fusion result was the same as that by dual channel pcnn. therefore, the decomposition algorithm in this paper is valid and has better fusion effect than that obtained by direct use of m-pcnn. objective evaluation is shown in table 1 . the experimental results demonstrate that bemd is very effective in the fusion of this kind of images. figure 4 is the fusion results of another group of multi-modal medical images. subjectively, the morphological pyramid had better contrast, but the fused result also had edge problem. there was local fuzziness in wavelet fusion. for dual channel pcnn, the intracranial part was not clear, while the texture obtained after decomposition by bemd was clearer. objective evaluation is shown in table 2 . the experimental data demonstrate the advantages of bemd decomposition method."
traceroute can trace the route (path) that packets across ip network while source routing traceroute allows people to specify some nodes on the packets' route and trace the special path we need. according to the path we can get some information of this network and discover its topology.
in our paper we put this step at first to simplify the following calculation. an actual situation would be more complex so that node degree judgment should be called whenever it was needed.
"the application of pcnn mode to image fusion produces good effects [cit], but these pcnn image fusions have such a common characteristic as the incapability of one pcnn of completing the fusion process of the whole image. in general, at least two pcnns have to be used to fuse multi-source images, which lead to the fact that it is hard for pcnn mode to meet the requirement of time overhead effected by a system with high real-time demand."
"bemd is an adaptive decomposition method of multi-scale images, and of better characteristics than wavelet decomposition. first, bemd is a kind of data-driven decomposition, and does not require predefined wavelet function as wavelet decomposition does; second, bemd is particularly suited to analyze bidimensional non-linear and nonstationary data. m-pcnn overcomes the shortcoming of the original pcnn mode that multiple pcnn modes are needed to calculate ignition matrix respectively, and is highly efficient. in this paper, the definition of m-bimf is proposed through the analysis of characteristics of bimf to repartition bimf so as to make bimf more suitable for image fusion. the experiments demonstrate that m-pcnn image fusion algorithm based on bemd propose in this paper achieves better fusion result."
"in this chapter, bemd is run to decompose multi-modal medical images firstly, and then m-pcnn is used to fuse images. the selection of parameters exerts great influence on m-pcnn, so parameter setup is first to be discussed here. the setup of m-bimf is also an important factor influencing the final fusion result. here the setup of each parameter will be given, and then fused images obtained by our method and other different fusion methods will be compared. finally, quality evaluation of different methods is given."
"m-bimf, a concept put forward based on bemd decomposition in this paper, achieves better results when applied to image fusion. but when an image is used in fusion, how many m-bimfs it should be partitioned into, and each m-bimf should be the sum of which bimfs still requires further research. with the improvement of the decomposition theory and algorithm of bidimensional empirical mode decomposition, bemd will certainly play a greater role in medical image fusion."
"as mentioned before, we also use node's degree to help topology discovery, finding out topology of ars which is directly connect to a non-anonymous router. non-anonymous routers' information can be got by snmp, including their active interface number, from which we will know how many devices are connected to them. like network pattern in figure 1 (a), through snmp we can know router a has one interface. but after the single-source traceroute we find node a has 3 children (figure 1 (b) ). according to the contrast we know arb1, arc1 and ard1 are actually one router, topology becomes as figure 4 shows. the dark gray circle presents a router merged from two or more anonymous nodes."
"the quantity of bimfs of two original images may be different after they are decomposed by classic decomposition of bemd. here, it is required that the mode set the quantity of common m-bimfs of the original images to n. according to the characteristics of images and bimfs obtained after decomposition, m-bimf, representing the process from high frequency to low frequency, can be obtained through calculation. residue (res) represents the average tendency of the image. here the method of entropy weighted and selected coefficient is used to fuse low frequency. the detailed algorithm is as follows:"
"among the following various evalution indexes, m, n represents the size of images respectively; is means the original image; f refers to the fused image."
"(1) to carry out registration on the two medical images i1, i2 to be fused; (2) to perform bemd on the two medical images to be fused; image i1 thus obtains its corresponding k bimfs and res; image i2 obtains its 1 bimf and res; (3) to set the quantity of common m-bimfs to n according to the characteristics of the two images to be fused, and calculate their corresponding m-"
"l is the quantity of m-bimfs. m-bimf meets the definition of bimf; the mean value of m-bimf which is obtained by adding together several bimfs with mean value equivalent to 0 is still zero; besides its maximum value points are positive and minimum value points are negative. in this way an image can be expressed by several m-bimfs. such a m-bimf has larger scale than the original bimf, which reduces the quantity of bimfs and has better texture features than those of the original bimf. another important significance of m-bimf is that corresponding components can better correspond to each other through reasonable definition of m-bimf. and thus better image fusion effect can be achieved."
"bemd is an adaptive decomposition process. namely, the quantity of bimfs can be decomposed into is determined by the data of the image itself, and different images have different quantity of bimfs after decomposition. only images with same decomposed layers can be fused. the general processing method is to set the quantity of bimfs in advance, which thus alters the stopping criteria of the quantity of extreme points of the original bemd decomposition. however, the quantity of bimfs of an image is hard to predefine. if the given quantity of layers is fewer, the decomposed bimfs may not manifest the texture of the image; if the given quantity is more, unexpected termination may be effected to the program, so the stopping criteria of decomposition should not been altered."
new parts are added to the set by using the target segmentation mask (see section 3.3) as well as image properties that maximize the chance of good optical flow estimation. to balance good object coverage and position quality the following score function is used for part sampling:
"in this paper we have proposed a new model for visual tracking. the model is composed of several interacting types of visual sub-models that differ in the level of detail by which they describe the target. we propose to use holistic detailed, holistic coarse and part-based sub-models that mutually interact in localization and cross-sub-model updates by accounting for the potential uncertainty of the visual information. this makes the visual model shift between purely holistic and part-based behavior, depending on the visual uncertainty. [cit] benchmarks where we have shown that the mutual interaction between the sub-models significantly improves the performance of the tracker. we have compared the tracker to the state-of-the-art reference trackers, the results confirm the hypothesis that the proposed tracker outperforms related part-based trackers as well as many other trackers both in accuracy and robustness."
"the visual model also maintains a global color model of the target and the immediate neighborhood described by a foreground and background rgb histograms, i.e., f t and b t . the color model is used to bridge a gap between the detailed holistic and part-based representations by generating segmentation mask used for sampling parts as well as serving as a constraint for conservative updates of the holistic template set."
"t is the position of the i-th part and h (i) is its visual model, a static grayscale histogram. localization. at time-step t, the localization of the target is performed by a three-step matching algorithm. firstly, the displacement of each part v (i) t is estimated by the lucaskanade optical flow. based on forward-backward validation [cit] we partition parts into subset for which the optical flow can be estimated reliably, p t, and the the rest, k t . the state x t is estimated by maximizing the probability of part positions x t conditioned on the measurements y t and estimation from the previous time-stepx t, i.e.,"
"where h(x) is the harris corner score at position x, u (x) is a periodic function 1 that enforces uniform coverage of sampling points in homogeneous regions and α u is a mixing constant. only local maxima of q(x) that are not already covered by existing parts and are inside the segmentation mask are kept and are ordered according to the color similarity likelihood and at most n u new parts are added at every time-step to ensure a gradual adaptation. the partset is initialized at the beginning of sequence in a similar manner, but without a limit on a number of parts."
"the reminder of the paper is organized as follows: section 2 positions our contributions against the related work, section 3 describes the new visual model and its integration into the proposed tracker. experimental evaluation and results are presented and discussed in section 4, followed by a conclusion in section 5."
"furthermore, the template anchors concept in our tracker is very general and can easily be replaced by object-classspecific detectors like face detectors to aid tracking in specific applications like face tracking. our ongoing work is directed towards exploring various possibilities of introducing application-specific priors into the tracker as well as detecting partial occlusions by observing changes in the pertbased model. table 2 . [cit] benchmarks in terms of average overlap and number of failures."
"according to the results, ant is the second most robust tracker. it is outperformed only by the plt tracker. as we can see in figure 6 most of this gain comes from the decreased robustness of ant in case of occlusions. on the other hand the ant tracker outperforms plt in accuracy in case of size and illumination change as well as frames without degradation."
"our main contribution in this paper is introduction of graduated flexibility in parameter estimation of deformable parts trackers. a new visual model composed of several interacting types of visual sub-models that primarily differ in the level of detail by which they describe the target is proposed. the level of detail varies in the type of features used, the number of parameters estimated by each model and the aggressiveness of the adaptation. the sub-models mutually interact in localization and cross-sub-model updates by accounting for the potential uncertainty of the visual information. this makes the visual model shift between purely holistic and part-based behavior, depending on the visual uncertainty ( figure 1 ). to the best of our knowledge this kind of self-constrained graduated estimation of free parameters has not been proposed before. [cit] ."
"in the following, the proposed tracker will be referred to as anchored tracking (ant). [cit], which provide a fully annotated dataset, evaluation protocol and an evaluation toolkit along with the results of a large number of state-of-the-art trackers. the large number of trackers tested makes these benchmarks arguably the largest short-term tracking benchmarks to date. dataset and the evaluation protocol. the datasets consist of 16 [cit] and 25 [cit] manually annotated sequences that contain various challenging visual tracking scenarios such as severe illumination changes, object deformations, abrupt motion changes scale variation, camera motion and occlusion. the sequences were selected from a larger pool from various sources using a clustering methodology that provided a diverse dataset of reasonable size."
"the proposed visual model is formalized as a hierarchically dependent set of the following sub-models: the holistic object templates (section 3.1) representing the holistic detailed description, the global color model (section 3.3) as holistic coarse representation and a deformable part-set (section 3.2) as a local representation of the appearance. the proposed architecture is motivated by the observation that some segments of tracking session can be performed better using less free parameters, e.g. only position of the target, while other segments may require a more detailed description of the state -our visual model presents a mechanism to gracefully shift between these two modes. localization. a tracking iteration starts by initializing the tracker at a location predicted by a motion model estimated by a kalman filter. the object templates are matched to the image, depending on the strength of the match of the best template, it can either provide a detection of the object ( the part-based model is deformed to account for geometrical deformations and the color model generates the object segmentation mask. the resulting object location for the frame can be given either by the best template match (detection) or by the part-set model (otherwise). update. in the update step the part-based model is updated by removing and adding patches using a object segmentation mask generated by the color model and the estimated region of the object. in case of the consensus of the color and part-based models, a new template is considered for addition to the object template set. the color model is updated as well using the generated output region. the output region from the model is also used to update the motion model. since it is clear that the holistic templates strongly influence the part-based and color-based representations, we refer to the our model model as an anchored visual model based on the fact that the templates are only used if they are deemed very reliable and in this case act as anchors to the rest of the model."
"in terms of accuracy, the proposed tracker performs comparably to most reference trackers. [cit] can be attributed to a more competitive set of trackers and to some degree to different annotation format, that uses rotated bounding boxes, which reduces region overlap with axis-aligned bounding boxes, reported by ant. trackers like dgt achieve better accuracy by utilizing computationally expensive segmentation. holistic trackers, like dsst, kcf and samf perform better in accuracy, at a relative difference of about 10%, but fail approximately four times more often. this means that they are also more often reinitialized which consequently corrects the region estimate, resulting in increased final overlap. as seen in figure 5, the ant is ranked similarly in robustness as the part-based dgt. the raw results available in the supplementary material reveal that the dgt in fact fails approximately four times more often, but only on sequences with certain visual degradations. the dgt failures occur in sequences where the assumptions required for efficient color segmentation are violated. the hybrid nature of the proposed visual model in ant is robust to a wide array of visual degradations as seen in figure 6, hence the lower failure count. the ant also outperforms the related lgt tracker in accuracy at relative increase of approximately 10% and significantly outperforms it in robustness. this means that improved accuracy can in fact be attributed to increased robustness and not a trade-off between the two tracking aspects."
dirac-delta positioned at the flow displacement. this decomposition is obtained by assuming that a part is conditionally independent from other parts except immediate neighbors and that visual likelihood of a part is conditionally independent from the other parts. the part visual likelihood is defined as
"benefits of the proposed guiding mechanism. several variations of the ant have been evaluated to test the contributions of each sub-model and the proposed cross-submodel interaction: the ant-d tracker is a tracker that only uses a single anchor template, ant-p uses only parts and segmentation, and ant-dp uses part-set together with the memory system, but the system only acts as a detection mechanism, i.e., it does not guide the update of the part set. the results are shown in table 1 . table 1 . [cit] benchmark for ant tracker derivations. as seen in table 1, the ant-d tracker achieves good accuracy, mainly at the expense of robustness since a single static template cannot properly address the appearance changes. on the other hand ant-p achieves good robustness, but the accuracy is low since the part-based model applies self-supervised updating without external supervision and recovery capability from the template memory system. the ant-dp combines the traits of ant-d and ant-p trackers, and benefits from switching between the detection and part-based tracking. the complete ant tracker improves performance in terms of accuracy and robustness by using anchor templates not only to detect the object, but also to guide the update process of the part-set even if the template detection is not reliable enough for a full detection. in particular, ant improves over the variations ant-d, ant-p and ant-dp in accuracy as well as robustness. the results thus clearly support our hypothesis that the proposed combination of part-based visual model and holistic visual model improves the overall tracking performance."
"comparison to the state-of-the-art. the ant was further compared to several top-performing state-of-the-art trackers [cit] and baseline trackers [cit] . the results of the evaluation are presented in figure 3 as raw a-r plot [cit], sequencenormalized and attribute-normalized ranking plots. the results are summarized in table 2, per-sequence scores are available in the supplementary material. results in figure 3 show that the proposed tracker outperforms all reference trackers by achieving the best accuracy and robustness. it is important to note that some trackers, like fot [cit] and lt-flo [cit], achieve a higher accuracy due to many re-initializations, which is not the case with ant. [cit] challenge, plt, in robustness. both trackers do not fail on any sequence, but plt achieves a lower accuracy in case of deformations and scale changes due to its holistic visual model. as seen in table 2, most of the other holistic trackers, like ivt, struck, and edft are less robust in tracking non-rigid objects, but achieve a higher accuracy in comparison to the part-based lgt and lgt++, which are related to ant. on the other hand, the ant achieves an accuracy comparable to the holistic models and simultaneously outperforms related part-based trackers in robustness. this can be also observed on qualitative examples in figure 4, e.g. in sequence singer where ant better estimates the size of the object despite illumination and scale changes that are challenging for both partbased lgt as well as holistic plt and ccms. this further confirms our hypothesis about retaining the best properties from holistic and part-based trackers."
"of paramount importance to the wtapso is its collective behavior, performance against a wide variety of mathematical problems, and its viability as a tool for making testable hypotheses for electrophysiology studies."
"for these experiments, the number of register outputs was varied between 1, 2 (the baseline), 3 and 4. the modified configurations are indicated by configurations config12, config13 and config14. every extra write port on the scalar register file contributes muxing cost and also increases the encoding space required in the configuration memory. there is also extra overhead associated with forwarding values from the write ports to the read ports. however, much like with increasing the number of inputs, increasing the number of outputs also increases the number of parallel, independent operation chains that can be executed on simd-morph, providing improved utilization and speedup, as shown in figure 19 ."
"we assume that there exists a secure channel between the dealer and every shareholder, so that the shares can be securely distributed to shareholders. moreover, we assume that every player is connected to a common authenticated broadcast channel c, so that any message sent through c can be heard by the other players. the adversaries cannot modify messages sent by an honest player through c, and they cannot prevent honest players from receiving messages from c. note that these assumptions are widely used in existing secretsharing schemes. with these assumptions, we can focus our discussion on the key aspects of pss without digging into the low level of technical details. our purpose is to provide an efficient way to establish additional pairwise secret channels among shareholders without invoking a separate key establishment protocol."
"utilizing the same biophysical ic neuron model, responses to sinusoid-amplitude modulated tones were reconstructed. figure 6 demonstrates reconstructed modulation transfer functions (mtfs). for these examples, inputs were derived from a beta prior and demonstrate that incomplete knowledge of input parameters can still lead to good fits. importantly we are able to fit a wide variety of ic neural response classes seen in the ic [cit] . for these fits, we utilized a mean-square error metric which measures relative distance between the mean of the experimental and model rmtfs, with lower values representing better fits. sources of error most likely arise due to the choice of the beta function prior which may not fully encompass input distributions. with naïve assumptions about the underlying input distribution, wtapso was able to recreate the fig. 5 recreation of in vivo ftcs using a biophysical neuron ic model using wtapso. responses were illicited from sinusoid tones of varying frequencies. the fitness function varies between 0 and 1 with 1 being a better fit. spontaneous responses were not fit, but set to rates typically seen in recordings (data not shown). wtapso is able to recreate a wide variety of responses types and shapes seen in vivo rmtfs of individual ic neurons that were representative of the most common rmtf classes, demonstrating the ability to draw inference even in situations with incomplete information."
"the key difference between this paper and prior work is that instead of proposing a cca architecture, we propose an architecture framework into which many cca architectures fit. this framework provides a clean interface between a processor pipeline and a cca, enabling easy customization of a cca for the expected system workload. we demonstrate how the framework can process a dataflow subgraph to generate cca instruction on the fly, without the costs associated with a trace cache. beyond the architecture framework, we describe the compilation process, by which subgraphs are identified in applications and communicated to the architecture framework."
"experiments have been performed to test the above system. the developed tifinaghe text recognition system has been tested on printed text. the system designed in matlap for the recognition. the image to be tested is captured using a scanner. before the recognition phase a database of 360 images is made. all tests are applied on 124 characters. the system is working fine and showing a good recognition rate (table 8) . it has been noticed that the extracted features of the images produced from segmentation module deviate a lot from the respective results in the training set. it seems that the resolution differences are affecting the geometric moments of the image, making them highly variant. it is expected that the recognition rate of the system can be improved by normalizing the training set as the characters that result after the segmentation phase. the system has been implemented on intel (r) core (tm) 2 duo, cpu t5870 @ 2.00 ghz, with a ram: 2.00 go. the system is still under development."
"computational modeling at all levels of abstraction can be a powerful tool to make experimentally falsifiable hypotheses and guide experimental design. furthermore, network models can make inferences about neural circuits that are difficult or impossible to test in physiology. however, one of the major issues incurred in modeling is the tuning and fitting of model parameters to physiological data. typically, models fit general classes of physiological observations rather than having the ability to fit individual or small sets of observations."
"swarm social networks can largely be thought of as a continuum across two fundamental swarm types: the fully connected and ring topologies. the ring topology ( fig. 1 right) only transmits information between its nearest neighbors. information therefore takes a relatively long time to traverse across all agents of the network [cit] . this may allow for a more thorough searching of the solution space. conversely, the fully connected topology can be thought of as a fully connected graph in which every agent has instantaneous access to information from every other agent (fig. 1 left) . this offers quick convergence but may cause the swarm to become trapped in local minima. most other social networks draw from aspects from the two extremes. the four-corners network, for example, (fig. 1 bottom) is fully connected locally but information is bottle-necked by sparse connectivity of the four networks."
"in this paper, we propose a dynamically changeable simd datapath, referred to as simd-morph, that in the base mode can execute data-parallel code across all the simd lanes. however, for scalar code, the datapath is morphed into a subgraph accelerator similar to the configurable compute accelerator (cca) [cit] . a cca consists of an array of simple functional units interconnected in a feedforward manner. the cca executes dataflow subgraphs identified by the compiler as atomic units. acceleration is provided in two ways. first, instruction-level parallelism is exploited by concurrently executing independent operations in the subgraphs (horizontal compression). second, operation chaining executes dependent the challenges of simd-morph that lead to the central contributions of this work are as follows:"
"memory access support in simd-morph allows for reduced use of register live-outs especially in situations where final values are written to memory arrays. for this reason, the benchmarks here are, to a point, insensitive to the number of outputs. a minimum of 1 or 2 is required to support incrementing loop counter or pointer values. a notable exception to this is, of course, crc, where few hot loops store values out to memory but values are live-out from one loop iteration to the next. figure 20 shows the normalized power consumption of varying the number of write ports to the register file, indicating an approximately linear relationship between the number of ports and power consumed."
"in this paper, we use bivariate polynomials to propose a new type of secret-sharing scheme, called protected secretsharing (pss), in which shareholders can use their shares to achieve two purposes simultaneously: one is to reconstruct the original secret and the other is to establish a shared key between every pair of shareholders. using these shared keys, shareholders can build pairwise secure channels among them to exchange the shares in the secret reconstruction phase. therefore, pss provides an efficient solution to protect the original secret from nonshareholders. our proposed scheme is information theoretically secure, and it can be easily extended to threshold cryptosystems for the same purpose."
"here p b n is the best position of the n th neighbor. sociometrically, this network draws influence from each of its neighbors irrespective of neighbor performance. this network has shown good performance in some cases, but may suffer from premature convergence to local minima. many other social networks have been developed to compromise between the two extremes of social network design, namely the ring and fully connected social networks. here we develop a biophysically inspired network which maintains the relative simplicity of static social networks while performing well on benchmark optimization problems."
"where p n corresponds to the best performing neighborhood agent and p g corresponds to the global leader. finally, the global leader draws influence from the best performing of the local leaders as well as its past best performance (fig. 2d) . this is to ensure continued progress towards viable solutions. updates are given by:"
"a new type of secret-sharing, called protected secret-sharing (pss), has been introduced in this paper. in a pss scheme, the shareholders' shares not only can be used to recover the secret but also can be used to protect the shares against nonshareholders in the secret reconstruction phase. a (, ) pss scheme using a bivariate polynomial is proposed, and we provide security and complexity analysis of the proposed scheme. some possible future works are also discussed in the paper. note that our method is generic enough to be directly applied with threshold cryptosystems for the same purpose."
"the tifinaghe script is used by approximately 20 million people who speak varieties of languages commonly called berber or amazigh. the three main varieties in morocco are known as tarifite, tamazighe, and tachelhite (el ayachi ). in morocco, more than 40% of the population speaks berber. in accordance with recent governmental decisions, the teaching of the berber language, written in the tifinaghe script, will be generalized and compulsory in tifinaghe is an alphabetic writing system. it uses spaces to separate words and makes use of western punctuation. the earliest variety of the berber alphabet is libyan. two forms exist: a western form and an eastern form. the western variety was used along the mediterranean coast from kabylia to morocco and most probably to the canary islands. the eastern variety, old tifinaghe, is also called libyan-berber or old tuareg. [cit] . historically, berber texts did not have a fixed direction. early inscriptions were written horizontally from left to right, from right to left, vertically (bottom to top, top to bottom); boustrophedon directionality was also known. modern-day berber script is most frequently written in horizontal lines from left to right; therefore the bidirectional class for tifinaghe letters is specified as strong left to right. displaying berber texts in other directions can be accomplished by the use of directional over rides or by the use of higher level protocols. the encoding consists of four tifinaghe character subsets: the basic set of the ″ institut royal de la culture amazighe (ircam) ″, the extended ircam set, other neo-tifinaghe letters in"
"effectively utilizing accelerators such as simd-morph requires tool chain support, and so it is important to introduce the compilation strategy used during design space exploration. compiling an application to make use of computation accelerators boils down to two steps: enumerating portions of the application's dataflow graph (dfg) that can be executed on the accelerator, and selecting which portions to accelerate."
"in order to capture biophysically relevant neural behaviors, models often have to encapsulate a wide variety of parameters or include computationally costly hodgkinhuxley modeling. particle swarm optimization (pso) is a swarm intelligence optimization paradigm that models the schooling of fish or flocking of birds to solve complex problems [cit] . pso was initially developed by kennedy and eberhart as a nonlinear optimization solver which models the emergent solving problem capabilities of swarms of relatively simple agents. as its operation is independent of the mathematical program cost function, it has found applications in far reaching fields such as the design of optimal power systems [cit] or the training of neural networks [cit] . pso has also been extended to solving multi-objective optimization problems [cit] ."
"in addition to the invariant moments derived by hu [cit], the modified invariant moments [cit], computed from the shape boundary for each character, and walsh transform [cit] ) are used as features."
"for these experiments, the number of register inputs was varied between 2, 3, 4 (the baseline), 5 and 6 as indicated by the configurations config2, config3, config4 and config5. every extra read port on the scalar register file contributes muxing cost and also increases the encoding space required in the configuration memory. however, increasing the number of inputs also increases the number of parallel, independent operations that can be executed on simd-morph and provide improved utilization and speedup. figure 13 shows the results of this exploration, with the second column showing the data for the baseline 4-input configuration. while there is significant improvement for rc4, most of the benchmarks show a very limited increase in performance; the principle reason for this being that the memory ports allow data to be \"live-in\" from memory arrays, reducing the sensitivity to the amount of live-in register data. the difference observed between the 2-input and 3-input performance of all the benchmarks shown, however, justifies the added cost of adding at least one extra port to a conventional 2-read-port register file. however, because performance saturates when using a 4-read-port register file, this is the preferred configuration. in our experiments, the power consumed by the scalar register file increases approximately linearly with the number of read ports, as shown in figure 14 ."
"where the values 0 x and 0 y are scalars and f is the activation function given by step 3. error back propagation for each example of applied learning base input of the network, we calculate the error at output layers by the"
"note that while the data being encrypted is accessed in a sequential order (line 8), the value being xored with the data is the result of multiple indirect memory accesses to m[y]. this type of pointer-chasing is difficult to perform efficiently on simd accelerators, which are typically designed for contiguous accesses."
"though pso is inspired by swarming biological systems, social networks presented for particle swarm optimization are not often biophysically inspired. here we present a new social network which takes inspiration from winner-takeall (wta) neural coding in visual cortex. seminal work in graph theory has shown that small world, high information throughput networks emerge from normal ring topologies after a relatively small number of non-neighbor connections are made [cit] . these small world networks are of particular interest in the central nervous system. specifically, previous studies have shown the emergence of hierarchical structures and points of influence in connection heavy areas of the human connectome [cit] . in the realm of pso, kennedy has shown that pso problem solving behavior is topology dependent and exhibits small world dependence [cit] . interestingly, full small world inclusion does not necessarily facilitate more robust problem solving behavior, and may even be detrimental in comparison to a more sparsely connected network [cit] ). indeed, our preliminary studies have indicated that simply creating a fully informed network typically exhibited weaker problem solving ability than a ring social network (data not shown). therefore, inclusion of small world shortcuts into swarm topologies must be carefully designed and evaluated."
"some swarm intelligence methodologies seek to address the problem of dimensionality by subdividing the swarms fig. 6 example rmtfs. functions were optimized around synaptic conductance values and input beta distribution priors. wtapso is able to recreate a wide variety of response classes including low-pass, high-pass, band-pass, and all-pass shapes into many sub-swarms to search problem sub-spaces and then pool what was learned to find an optimum solution [cit] . however, many problems, such as biophysical neural models which require the simultaneous tuning of many model parameters, are computationally demanding, and subdividing swarms into miniswarms would unacceptably blow up computation time. wtapso may provide a viable framework for modeling problems such as these because its construction is as simple as defining a new social network and does not require any more computation time than a swarm with comparable number of agents. furthermore, it is able to find solutions in a fewer number of iterations in comparison with other pso methods, meaning that complex models may run in faster time. toolboxes adopting this method may then use ring topologies to solve low dimensional problems with an option for wtapso for high dimensional problems. while other particle swarm paradigms, such as comprehensive learning pso [cit], attempt to mitigate the amount of bad influences updating an individual agent, wtapso does so in a more deterministic fashion and allows for systematic testing of various particle hypotheses. for example, each neighborhood can be initialized to different regions of the solution space and allow the swarm to then run to completion."
"the design of the next generation mobile platforms must address three critical issues: efficiency, programmability and adaptivity. the inherent computational efficiency of 3g solutions is insufficient and must be increased by at least an order of magnitude as shown in figure 1 . straight-forward scaling of 3g solutions by techniques such as increasing the number of cores is part of the solution, but is not enough on its own. programmability provides the opportunity for a single platform to support multiple applications and even multiple standards within each application domain. further, programmability provides faster time to market as hardware and software development can proceed in parallel, the ability to fix bugs and add features after manufacturing, and higher chip volumes as a single platform can support a family of mobile devices. lastly, hardware adaptivity is necessary to maintain efficiency as the core computational characteristics of the applications change. 3g solutions rely heavily on the vast amounts of vector parallelism in wireless signal processing algorithms, but lose most of their efficiency when vector parallelism is unavailable or constrained."
"in practical optimization, metrics such as algorithm performance and time costs are very important. for example, algorithms that have better problem solving ability may not be useful if run times are exceptionally long. for this study, we tested wtapsos ability to solve high dimensional problems, its problem solving ability across a variety of problems, and its runtime and compared these against commonly used social networks designed in-house and in software toolboxes. for control pso groups, we utilize the common ring network, a dynamic variant of the ring network which reshuffles agent positions in the ring after each update, and the four corners social network (fig. 1) . updates for these networks followed a static constriction coefficient method discussed above. we also utilized the common and trelea type 2 networks found in a pre-designed matlab particle swarm optimization toolbox [cit] ."
"secret-sharing schemes, first introduced by shamir [cit] and blakley [cit], are very important techniques to ensure secrecy and availability of sensitive information. moreover, they are widely used as building blocks in various cryptographic protocols, such as threshold cryptosystems, attributebased encryption, and multiparty computation. in a (, ) threshold secret-sharing scheme, the secret is divided into shares so that it can only be recovered with or more than shares, but fewer than shares cannot reveal any information of the secret. in the past few decades, many secret-sharing schemes have been proposed in the literature, and three major approaches can be used to design them: shamir's approach [cit] based on the univariate polynomial, blakely's approach [cit] based on the hyperplane geometry, and mignotte/asmuthbloom approach [cit] based on the chinese remainder theorem (crt)."
"in many document analysis systems, printed material is scanned and stored as an image. it is later retrieved for character and graphics extraction and recognition. during the scanning process, the document may be skewed and the text lines in the image may not be strictly horizontal. the skew may cause problems in text baseline extraction and document layout structure analysis. several methods have been developed by many researchers for skew angle detection. a skew angle is the angle that the text lines of the document image make with the horizontal direction. the skew correction is necessary for the success of many ocr systems. it consists of the extraction of the skew angle θs corresponding to baseline using hough transform. the baseline is considered as the line corresponding to the maximum points in the horizontal projection profile. the skew angle θs is detected by observing high valued cell in the accumulative matrix in the hough transform space. the image is then rotated by θs in the opposite direction so that the scripts become horizontal. fig. 4 (a) and fig. 4 (c) respectively show a text before and after baseline skew correction."
"one consequence of the no free lunch theorem is that there can be no guarantees on optimal performance on every problem an optimization paradigm might encounter. [cit] assertion that the nfl theorem does not negate the need to find better pso social networks, there is a large subset of interesting problems that are high dimensional. as our data have shown, traditional particle swarm social networks and toolboxes often fail at finding adequate solutions to high dimensional problems. the wtapso social network shows improved performance on high dimensional problems. furthermore, our preliminary studies in using this social network on real-world system models has shown excellent performance in parameter tuning in high dimensional problems with run times that are reasonable on a personal pc. the wtapso social network also offers unique abilities to explore solutions spaces of real world problems. for example, the four social networks can be started at vastly different initial conditions that test potential biophysical mechanism hypotheses. the networks swarms can be allowed to run independently to find local optimum solutions then connected to see which physical parameter dominates in a certain operating zone. finally, the swarm can be recursively defined, incorporating more neighborhoods extending below the lower agents which might contribute to better problem solving behavior."
"definition 1 (protected secret-sharing (pss)). in a pss, the received shares by shareholders can be used to serve two purposes simultaneously: (a) reconstruct the original secret and (b) establish pairwise shared keys among shareholders (note that these pairwise shared keys are used to build a secure channel between every pair of shareholders in order to exchange the shares in the secret reconstruction phase. therefore, the reconstructed secret can be protected from any nonshareholder)."
"the no-free-lunch theorem (nfl) [cit] states that given the set of all possible optimization problems, on average no optimization paradigm will perform better than any other paradigm on all possible problems. in other words, there are some problems that will be more difficult for an optimization paradigm to solve. with that in mind, we will show that wtapso can solve a wide variety of nonlinear problems that will be of interest to real systems analysis. table 3 shows the results of wtapso, ring, dynamic ring, and four social networks against the 10 dimensional ackley function, 10 dimensional griewank problem, 10 dimensional rastrigin problem, 30 and 100 dimensional rosenbrock programs, 2 dimensional schaffer f 2 and f 4 functions, 10 dimensional sphere function, and 10 and 20 dimensional styblinsky -tang function. inputs to each function leading to global minimums can be found in table 2 and problem formulations are found in the appendix. on the ackley and griewank 10 dimensional problems, all social networks got relatively good results, but wtapso in both cases averaged closer to the global minimum of 0 on both problems. the 10 dimensional rastrigin problem proved difficult to solve for the three test social networks to solve, but wtapso was able to find values closer to the global minimum of 0. to test the ability of wtapso to solve very high dimensional problems, the 30 and 100 dimensional rosenbrock problems were used. due to the nonlinear nature of the rosenbrock problem, even slight variations in the input can cause explosion of the output. the ring and four corners social networks fail to find satisfactory answers in all both dimensional cases. wtapso was able to find satisfactory answers even in the very difficult 100 dimensional case. in the two dimensional schaffer problems, all social networks performed roughly the same, with the dynamics ring social network finding the exact minimum of 0 on the schaffer n.2 problem and smaller standard deviations in the schaffer n.4 problem."
"note that the proposed pss scheme aims to achieve information theoretical security. hence, both of the above security goals do not rely on any computational assumption."
"to demonstrate the viability of wtapso to solve biophysical neuron model inverse problems, we show response generation from three different auditory feature representations in the ic."
"models of central nervous system neurons and networks often run into the \"curse of dimensionality\", in which even relatively simple optimization problems become difficult to solve when the number of input variables is high. furthermore, in most models, general classes of response profiles are modeled, rather than being able to model individual response profiles. particle swarm optimization has been used in many computational biology models, including fitting somatosensory responses to chirp functions [cit] ) and training recurrent neural networks which model gene regulatory networks [cit] developed an optimization program which allows for the optimization of neuron model parameters to fit time dependent traces. while this is advantageous for tuning models to fit single neuron spike trains, it does not lend itself to the fitting of tuning where model response is dependent on frequency or another non-time based measure."
"note that although bivariate polynomials have been used to design many different types of secret-sharing schemes in the literature, for example, verifiable secret-sharing (vss) [cit], pairwise key distribution [cit], and dynamic secret-sharing [cit], the purpose of this work is different from the previous ones, and the types of employed bivariate polynomials are different as well."
"where  ' i is defined as in equations (18, 19, 20, 21, 22, 23 and 24) (,,,) gxyuv is the kernel function given by the following form:"
"(i) insider adversary. the insider adversary is a legitimate shareholder who owns a share generated by the dealer. an insider adversary may work alone or collude with some other insider adversaries to learn the secret before it is supposed to be reconstructed or to recover invalid secret using fake shares. note that when the secret is reconstructed, we assume that the insider adversaries can learn the secret, but they will not leak the secret to nonshareholders, for example, the outsider adversaries."
"the simplest configuration that allows for the maximum performance from simd-morph has 4 input values, 3 output values and 4 memory ports, using the baseline 1x16 interconnect topology. performance saturation from using more complex configurations is illustrated in figure 21, which shows no change in performance when using more than 4 input values and a negligible performance improvement when using more than 3 output values. this configuration is config13 in table 1 . figure 22 illustrates the performance:power efficiency of each of the configurations used in the design-space exploration . this figure, too, illustrates the optimal efficiency of config13."
"our proposed pss scheme consists of two phases: (i) share generation and distribution by the dealer and (ii) secret reconstruction by shareholders. during the share generation and distribution phase, the dealer selects a random asymmetric bivariate polynomial to generate the shares for each shareholder, and every share consists of two univariate security and communication networks 3 polynomials. these shares are sent to shareholders through the secure channels. during the secret reconstruction phase, each shareholder first uses her share to compute pairwise shared keys with the other shareholders. with these shared keys, pairwise secure channels can be established among the shareholders. after receiving the shares from the other shareholders through these secure channels, each shareholder can recover the original secret without leaking it to any nonshareholder."
"simd-morph was synthesized on a 90nm technology with a targeted clock frequency of 200 mhz using synopsys design compiler and synopsys physical compiler. power consumption was measured using synopsys primetime-px. the chosen baseline assumes a processor with 16-wide simd datapath and a 16-entry scalar register file with three read ports and two write ports, the power consumption of which is 63.48mw. the baseline simd- morph configuration, illustrated in figure 6 is connected to a scalar register file with four read ports and two write ports, the power consumption of which is 71.01mw -a power overhead of 11.9% for the modified components. the efficiency-optimal configuration, config13, with three write ports instead of two in the scalar register file has a power consumption of 71.53mw -a power overhead of 12.7% for the modified components. the power consumption of the scalar datapath and the simd register files is not accounted for here as they are not modified in this work; accounting for the power consumption of these components will reduce this power overhead. the average performance/power efficiency improvement of using the optimal simd-morph configuration is 1.75x."
"the overall performance improvement when using the optimalefficiency configuration of simd-morph is shown in figure 23 . for the benchmarks from the mediabench suite, only the non-simdized portions of the code are considered for acceleration on simdmorph, while for the other benchmarks, the entire application is considered. for this reason, the averages of the two categories are displayed separately. the average speedup for the media applications is 1.4x, while the average speedup for the other applications is 2.6x."
"the data from section 3 indicates that the performance impact of subgraphs with a large number of operations and with long chains of operation is, at best, minimal. in response to this, different simd-morph topologies were explored, varying the interconnection network between the elements and also varying the total number of elements. these are indicated by configurations config9, config10 and config11. in config9, each group of 8 elements does not communicate with the other, but one 4-element ceg within each group still feeds another. the configurations config10 and config11 explore the notion of not using all 16 lanes of the simd datapath but using 8 or 4, respectively, instead. the performance impact of these configurations is shown in figure 17 . the most important observation from this graph is that there is virtually no difference between the 2x8 and 1x8 configuration; i.e. once the maximum depth of subgraphs is halved, having extra units has no added benefit, so one may as well configure 8 the power savings from moving to a 1x16 configuration to a 2x8 configuration is also negligible as seen in figure 18 ."
"in the coming years, the deployment of mobile computers will continue to skyrocket. the prime example today is the smart phone, but in the near future we expect to see the emergence of new classes of such devices. these devices will improve on the smart phone by incorporating advanced functionality such as high-bandwidth internet access, human-centric interfaces with voice recognition, high-definition video coding, and interactive conferencing. while integrating new capabilities is important for attracting customers, battery lifetime and power dissipation remains paramount."
"on average, approximately 85% of the subgraphs have depths of 3 operations or fewer. the performance contribution paints a similar picture as well, with 80% of the performance improvement coming from subgraphs with depths of 3 operations or fewer."
"the anysp architecture tackles the first two issues using a combination of wide-simd execution (64 lanes), efficient data shuffling, and support for common intrinsic functions [cit] . efficiency and programmability are simultaneously garnered by pushing simd execution to new levels, while applying domain specific customizations to the datapath. adaptivity within vectorizable code is also supported by allowing neighboring lanes to conjoin, creating 32 lanes, each supporting a computation depth of two, or overlaying multiple narrow vector computation threads using independent address generation units. researchers at st microelectronics apply similar generalizations to simd datapaths [cit] . however, all of these solutions ignore non-simd-izable computation. such code is relegated to execute on a low performance scalar pipeline provided in the design. for inherently scalar applications such as compression or encryption, high computational rates cannot be sustained. even for highly vectorizable codes, amdahl's law will expose the non-vectorizable portions of the code as the eventual bottleneck."
"the next stage in the tifinaghe character recognition is the feature extraction stage. feature extraction represents the character image by a set of numerical features. these features are used by the classifier to classifier the data. in our work moments and other shape descriptors by hu have been utilized to build the feature space. using nonlinear combinations of geometric moments hu derived a set of invariant moments which has the desired property of being invariant under image translation, scaling and rotation."
"available to large monovalent and divalent cations such as na + and ca 2+ while m1 and m2 are octahedral sites containing a distribution of divalent and trivalent cations of moderate size such as mn 2+, fe 2+ or fe 3+ . more recently, detailed structural analysis of several alluaudites demonstrated that the x2 site has two distinct positions labeled a2 (0, 0, 0) and a'2 (0,~0, 1/4) in a tunnel at (0, 0, z) and x1 has three distinct positions, labeled a1 (1/2, 0, 0), a'1 (0,~1/2, 1/4) and a''1 (x, y, z) in a tunnel at (1/2, 0, z). the general formula of moore was then reformulated as"
"we have shown the ability of wtapso to solve the inverse problem of retrieving physiological inputs from neural response tuning curves. while general input distributions can be utilized, as in the beta distribution in the am tuning model, physiologic inputs will most likely give rise to more accurate fits. thus, when making inferences regarding neural function, one must pay special attention to the region of feasible physiologic inputs, both in feature selection and constraints. while this method does fit individual tuning curves, synaptic inputs are almost certainly not unique mappings, i.e., each response curve could be generated by different sets of inputs. however, this method generates inputs which serve as a basis for parameters which can empirically tested in electrophysiolgy experiments (table 5 ). this method is also problem independent: as long as there is a mathematical model of the neuron under test, wtapso can be used to fit physiological responses."
"particle swarm optimization is a metaheuristic, derivative free optimization paradigm which models flocks of birds or the schooling of fish to solve mathematical problems [cit] . individual agents are relatively simple with complex behavior emerging from carefully designed social networks. we begin with a treatment of agent interactions in canonical pso, before moving to the formulation of wtapso."
"neurons throughout the auditory pathway exhibit tonotopic frequency mappings with different spatial regions of auditory regions characterized by neurons exhibiting maximal firing at a specific frequency, known as the characteristic frequency. we demonstrate wtapso systematically fitting input parameters and distributions to recreate ftcs recorded in vivo (fig. 5) . figure 5a,d demonstrates reconstruction of typical ftc curves with maximum mean rate at its center frequency. wtapso is also able to recreate more challenging tuning curve, such as those with two disjoint peaks in the mean (fig. 5b ). finally, wtapso is able to recreate tuning curves with less well defined shapes, such as in fig. 5c . while the driven region of the curve is well fit, there is some error at the non-driven region, as spontaneous rate is a fixed parameter not subject to optimization in this version of the model. table 4 shows the reconstructed parameters for the fits in fig. 5 . several constraints were put in place to ensure operation in a biophysically relevant region, such as resting membrane potential set between −80 and −50 mv and fully positive conductance values. for the purpose of this work, parameters governing input psth values were not constrained. however, careful attention to these should be paid when drawing physiological relevance from fits."
"previous work has shown that greedy solutions work poorly, particularly when the accelerator is large, like simd-morph is [cit] . for that reason, we leveraged a more thorough compilation approach very similar to previous work [cit] . essentially, the compiler performs an exhaustive search of the design space to enumerate and select the best possible set of subgraphs for acceleration. several pruning heuristics keep the compilation time reasonable for the vast majority of cases, and timeouts prevent corner cases from taking an intractable amount of time. this more thorough compilation strategy ensures that the design space exploration is as accurate as possible."
the central moments are invariant to translation. they can also be normalized to be invariant to scaling change by the following formula. the quantities in equation (6) are called normalized central moments
"drawing from biological neural networks, we propose a new social network for particle swarm optimization. figure 2 displays the overall social network architecture. the social network is built around a dynamic hierarchical structure that allows for performance based promotion among agents. a number of networks in visual and parietal cortex exploit winner-take-all (wta) coding schemes [cit] . in wta coding, input neurons compete for output neuron activation, with the response of the output cell most closely resembling the firing rate and tuning characteristics of the strongest input. physiologically, wta rules are thought to be involved in visual attention and saliency detection throughout the visual system, including object orientation and direction [cit] . visual cortical regions also exhibits small world connectivity [cit] . the social network is built around a dynamic hierarchical structure that allows for performance based promotion among agents. first, a global leader is selected by polling the swarm for the agent who is performing best on the fitness function. agents are then grouped into a lower tier of local neighborhoods. from the neighborhoods, local leaders are selected based upon which agent is performing best on the fitness function from the neighborhood. the hierarchical structure shares characteristics of information transmission seen in ring networks by limiting the speed of update influence between any two agents. the graph analysis software gelphi [cit] ) was used to derive relevant static graph measures for each social network structure (table 1) . interestingly, the average path length, a measure of the distance information travels along a graph, is shorter in wtapso than the ring or dynamic ring topologies, but longer than that in the four corners social network. the cluster coefficient, a measure of the amount of grouping, displays the same characteristic, with wtapso being more clustered than the ring networks, but less so than the four-corners network. these however, while true in the static sense, will be fundamentally altered in dynamic networks when parameters change during updates as connections are dynamically broken and reassembled. hierarchical architecture for the winner-take-all particle swarm social network. a.) the overall topology of the wtapso social network. the global leader is shown as a blue sphere, while local neighborhood leaders and agents are shown as red and black spheres respectively. numbers near agents represent a sample of agent placement in the swarm. by definition, the global leader is always 1, while local leaders represent the best performing agents of a given neighborhood. once the best performing local agent is determined, update connections are broken, represented by dotted black lines, so that only good influences are updating the target agent. b.) a sample update of a neighborhood leader. the neighborhoods are disconnected at update and only the best two performing agents are allowed to update the neighborhood leader. by its rank, the global leader always updates the neighborhood leader. c.) updating a neighborhood agent is very similar to updating a neighborhood leader. only the best two best performing agents are allowed to update the target agent. by virtue of its position, the local leader will always update the target agent, while the next best performing local agent acts as the other update agent. d.) the global leader receives update information from its own previous best performance and the best performing local leader in wtapso, updates between any two agents follow a modified winner-take-all rule where agents compete for influence on the agent under update. to update a neighborhood agent (fig. 2c ), the neighborhood is polled for the two best performing agents with the agent under update excluded from the poll. since the neighborhood leader is by definition the best performer of the neighborhood, it is included as one of the updating agents. the other updater is then the local agent who is currently performing the best on the fitness function. at this point, the neighborhood becomes temporarily disconnected and the agent under update is only updated by the two best performing agents. this works to remove any bad influence from updating a target agent. quantitatively, updates for neighborhood agents are as follows [cit] :"
a control memory is required in order to store the various configuration bits required for simd-morph. the requirements of the different components are broken down as follows:
"our first test compared the performance of wtapso on high dimensional problems compared to other social networks. figure 4 displays the performance of all social networks on the rosenbrock optimization problem between 2 and 10 dimensions. significance was assessed by a twoway anova with tukey honestly-significant-difference (hsd) post tests between groups. data was log transformed before analysis. minimal value was truncated at 10 −30 in order to represent data on a log plot. in lower dimensions (less than 5), traditional social networks can perform better than wtapso. this is not to say, however, that the wtapso performs poorly. [cit], wtapso is providing solutions well within the range of the global minimum. as higher order dimensions are introduced, performance of every social network decreases as expected. after 4 dimensions, wta pso starts to perform as well as matlab pso toolbox common network and outperforms every other network. it is interesting to note that many of the social networks grow worse in problem solving ability as dimensionality increases, but wtapso (and the toolbox networks) seem to plateau and maintain a stable ability to solve problems. subsequent experiments show that even in extremely high dimensional problems (30 − 100 dimensions), wtapsos problem solving ability does decrease somewhat, but remains much more robust than the other social networks (tables 3 and 4) ."
"while many updates schemes have been derived for pso, for this work we will utilize the constriction coefficient method [cit] as it is a widely used update method and insures swarm stability with appropriate parameters selected. in short, updates for the ring topology are calculated as follows:"
"in the character recognition system, the recognition is the last phase which is used to identify the segmented character. where we use two techniques: the neural network [cit] and dynamic programming [cit] ."
"(ii) outsider adversary. the outsider adversary is an attacker who does not own any share generated by the dealer, but she may try to learn the secret that she is unauthorized to access. note that this attack is possible in many existing secret-sharing schemes when the shares are exchanged in an insecure fashion during the secret reconstruction phase."
the position normalization is designed to eliminate unwanted areas and reduce the processing time. for this operation we use the histogram given by the following form:
"the quantity in equation (34) is invariant to a homogenous scaling. theorem 2: suppose c is a smooth curve in the plane and c' is the curve obtained by rotating c an angle  clockwise, then"
"the memory units in simd-morph access memory via a multiported load/store queue. this system helps prevent inter-lock and overlapping accesses in the event of memory aliasing when executing pointer-intensive code. increasing the number of units therefore requires appropriate modifications to the memory system. further, address generation support needs to be added to elements in the simd datapath in order to support base+displacement memory operations. for these experiments, the number of memory units in simd-morph was varied between 0, 1, 2 and 4 (the baseline) units as indicated by the configurations config6, config7, and config8. figure 15 shows the results of this exploration, with the 4th column showing the data for the baseline 4-memory unit configura-"
"factoring in the speedup contributed by each of these subgraphs presents only a slightly different story. in figure 9, each segment shows the % of overall cycles saved came from subgraphs of various sizes. the speedup contribution of 1 to 4-operation subgraphs is 71.5% and that of 5 to 6-operation subgraphs is 26%. this indicates that while there while there are few subgraphs larger than 4 operations in size, their speedup contribution is significant. figures 10 and 11 depths. the subgraph depths for the benchmarks in consideration ranged from 1 operation to 5, although the execution frequency of the 5-deep subgraphs (there is one in each of g721encode and g721decode) was so small that it is not noticeable in these graphs."
"in the majority of existing secret-sharing schemes, it is simply assumed that shares are released by the shareholders in the secret reconstruction phase, and then anyone can reconstruct the secret using these revealed shares. but, in many cases, it is undesirable for nonshareholders to learn the secret. considering the scenario where a famous billionaire sets up the will and shares it among his children using secret-sharing, the children are told that the will should not be read when the billionaire is alive and its contents should be kept strictly private among the family members. however, some paparazzi may want to learn the will after the billionaire passes away to make some head news. in this case, traditional secret-sharing schemes may not provide sufficient protection. to solve this problem, shareholders can use pairwise secure channels to exchange the shares so that the recovered secret is only available to shareholders but not to nonshareholders. if these secure channels are built using cryptographic methods, a shared key is required to be established between every pair of shareholders beforehand. however, employing an additional key establishment protocol may make the secret-sharing schemes significantly more complicated."
"proof: suppose c is a smooth curve in the plane, c' is the curve obtained homogeneously by rescaling the coordinates by a factor r, then"
"in this work we present a new particle swarm social network which draws influence from small world networks found in the visual cortex of the central nervous system. fig. 1 the continuum of social networks. top left: the fullyconnected topology is a fully-connected graph where each agent is directly connected to every other agent. top right: the ring topology slows down information transfer by only allowing connections between an agents nearest neighbors. bottom: many social networks draw from aspects of both fully-connected and ring topologies. the four corners social network is fully-connected locally but sparsely connected between neighborhoods [cit] . specifically, winner-take-all particle swarm optimization (wtapso) is a dynamic, hierarchical topology that utilizes winner-take-all coding strategies found in the visual system which shows improved performance on high-dimensional problems as compared to traditional particle swarm networks (fig. 2) . we begin with a derivation of wtapso, and demonstrate that wtapso. we demonstrate that wtapso is able to fit a wide variety of neural models by showing fits of physiological data from a biophysically accurate computational model of an inferior colliculus (ic) neuron."
"finally, figure 12 shows the distribution of memory operations in the subgraphs. over 90% of the subgraphs use only 2 of the 4 available memory units. however, over 60% of subgraphs have at least one memory operation, indicating that support for memory operations is still important, although it is unlikely that reducing the number of operations supported will have a significant impact on performance."
"where rmse corresponds to the root mean square error, r is the correlation coefficient, and x c and x m are the experimental and model curves respectively. this function is bounded between 0 and 1, with values closer to zero corresponding to a better fit. for am rate tuning, inputs consisted of two-parameter beta-functions which approximate all-pass, band-pass, and low-pass inputs dependent on parameter value. a mean-square error metric was utilized for am tuning responses. spontaneous rates were modeled by an ornstein-uhlenbeck process [cit] ) with mean rates modeled from spontaneous activity recordings in young and aged rats."
"invariant moments derived by hu [cit] were frequently used as features for shape recognition and were shown to be invariant to scaling, translation and rotation (tables 2, 3"
"where p g corresponds to the global leader's best past performance and p l corresponds to the best performing local leader. after updates are complete, agents are re-evaluated against the fitness function and positions within the swarm are updated. if at any point an agent, irrespective of its place in the social network, performs better than the global leader on the fitness function, it is promoted to the global leader position. neighborhoods are then reformed and local leaders elected."
"next we compared the performance of wtapso against a previously established matlab pso toolbox. only the subset of test functions available within the toolbox were used for this analysis. results in table 4 demonstrate that wtapso performs better on most test functions, with the exception of the 10 dimensional sphere problem. this helps to illustrate the no-free-lunch theorem and that there are problems that are easier for some social networks to solve."
"to give a specific example, figure 2 is the critical loop for the rc4 encryption algorithm, the basis of secure sockets layer (ssl) and many other popular streaming, secure protocols."
"our final test of basic algorithm behavior was to determine whether wtapso was able to solve problems faster that other social networks. for this test, to account for differences in cpu time and processing, we utilized iteration count as our time metric. to standardize against problem type, we used only the rosenbrock problem at 2,5,8,10 and 30 dimensions. [cit], swarms were allowed to run until a function value of 100, corresponding to an acceptable solution, was reached or the number of iterations exceeded 10,000, at which point it is assumed that the swarm would not converge onto an adequate solution. figure 6 demonstrates that on the 2 dimensional rosenbrock problem, every swarm was able to reach an adequate solution at a reasonable number of iterations. in 5 dimensions, the ring social networks are able to solve the problem in an adequate time, but the four corners social network can no longer find a solution. as dimensionality increases, only the wtapso is able to solve the problem and does so in a remarkably low number of iterations (100 iterations). this test shows that, independent of cpu processing power, wtapso is able to find solutions in a lower iteration count than the test social networks which in most cases will translate to lower run times."
"\"selecting\" which subgraphs to accelerate is also difficult. typically, the selection problem is formulated to push as much computation as possible onto the accelerators, while ensuring that there is no overlap between subgraphs. that is, given a set of enumerated subgraphs, find the group that covers the largest portion of the dfg while minimizing the number of nodes appearing in multiple subgraphs. this problem is also np-complete and is quite similar to the well known technology mapping problem in vlsi design. clearly, mapping applications to subgraphs is a challenging compilation problem."
"utilizing instruction set extensions to improve the computational efficiency of applications is a well studied field. examples of industry standard domain specific instruction set extensions such as intel's sse or amd's 3dnow! multimedia instructions are commonplace in modern systems. techniques for generating domain specific extensions are typically ad-hoc, where an architect examines a family of target applications and determines what extensions can be expected to provide increased performance."
"in this section, we first prove the correctness and secrecy of the proposed scheme; that is, neither type of adversaries can achieve its objectives based on our assumptions. then, we briefly analyze the complexity of the proposed scheme. proof. to prove this theorem, we first consider the situation that there are no dishonest shareholders. then we justify why less than a portion of 1/3 dishonest shareholders cannot prevent the correct secret from being reconstructed. in step 2 of the secret reconstruction phase, each shareholder uses her share 1 ( ) to compute the lagrange component of the secret as"
"in both the 10 and 20 dimensional variants, wtapso performed better than the other social networks, demonstrating the ability to track changing global minimums on a problem definition. [cit] do provide better results. however, the update functions and use of information were fundamentally different, owing to the difference in observations and further proving the importance of all aspects of the design of a swarm."
"in contrast to domain specific extensions, many techniques for generating application specific instruction set extensions have been proposed [cit] . each of these algorithms provide either exact formulations or heuristics to effectively identify those portions of an application's dataflow graph that can efficiently be implemented in hardware. these techniques are not directly ap- figure 23 : speedup from using simd-morph using the optimal configuration. \"average media\" refers to the average speedup obtained from accelerating the outer loops of mediabench applications while \"average other\" refers to the average speedup obtained from accelerating the entirety of the other applications. the speedup seen in pc1 is 9.5x but is capped at 4x in this graph."
"here we present a new hierarchical particle swarm optimization social network which draws influence from winnertake-all coding found in visual cortical neurons. wtapso shows improved performance in high dimensional test optimization problems and may be valuable in parameter tuning problems of physical models. while many pso social networks are not biophysically inspired, we hope to show that power of drawing influence from real world networks as a way to inspire better problem solving behavior in artificial social networks. futhermore, wtapso serves as a potentially powerful tool for fitting complex neural responses across sensory modalities."
"register data is transferred into the simd datapath via ceg0's 4 inputs directly from the scalar register file. register data is transferred out of the simd datapath via ceg3's outputs. despite the increased complexity of adding connections from the scalar register file to the simd datapath, this method is a better solution than adding extra ports to the simd register file. this is because the code that will require this functionality is not simd-parallel and would normally be executed on the scalar pipeline so adding these connections to the simd register file will require extra cycles to copy data between the two register files, thereby reducing performance. figure 7 shows a representation of lines 6-9 of the crc benchmark shown in figure 3 on the simd-morph hardware. the live-in values in this case are the variables crc_accum, c and the base pointer crc_table. this pointer is used to issue the load from crc_table[i] (line 7 in the code). the shaded nodes are idle and are not used for any computation. the live-out value is the updated value of crc_accum. the mov operations are used to transfer operands from where they are generated to where they are read."
"in the last three decades, many fascinating works about secret-sharing have been proposed in the literature, and different types of secret-sharing schemes can provide different properties. for example, verifiable secret-sharing (vss) scheme [cit] not only allows the shareholders to verify the validity of their received shares in the share generation and distribution phase but also allows the verification of the revealed shares in the secret reconstruction phase. in proactive secret-sharing schemes [cit], shareholders can refresh their shares periodically without the dealer being involved, so that the shares obtained by the adversaries will become obsolete after the shares are updated. moreover, the threshold can be dynamically adjusted when some shareholders join in or leave. in multiple secret-sharing schemes [cit], each shareholder can use her share to recover multiple secrets at different stages. in this paper, we have not considered these additional properties, and the existing secret-sharing schemes have not considered the issue of protecting the secret(s) from nonshareholders. therefore, incorporating the ideas presented in this paper with these different types of secret-sharing schemes will be interesting, and we consider these further investigations as our future works."
"responses were recorded from fischer-344 [cit] . briefly, all surgical procedures used were approved by the purdue university animal care and use committee (pacuc 06-106). auditory brainstem responses were recorded from these animals a few days preceding surgery to ensure all animals had hearing thresholds typical for their age and to ensure there were no abnormal auditory pathologies. surgeries and recordings were performed in a 9 x 9 double-walled acoustic chamber (industrial acoustics corporation). anesthesia was induced in the animals using a mixture of ketamine (vetaket, 80mg/kg for young animals and 60 mg/kg for aged) and medetomidine (dexdomitor, 0.2 mg/kg for young and 0.1 mg/kg for aged animals) administered intra-muscularly via injection. the animals were maintained on oxygen through a manifold. the pulse rate and oxygen saturation were maintained using a pulse-oximeter to ensure normal range. supplementary doses of anesthesia (20 mg/kg of ketamine, 0.05 mg/kg of medetomidine) were administered intra-muscularly as required to maintain areflexia and a surgical plane of anesthesia. an initial dose of dexamethasone and atropine were administered to reduce swelling. constant body temperature was maintained via a heating pad (gaymar) set at 37 degrees c. a central incision was made along midline and calvaria was exposed. a stainless steel headpost was cemented anterior to bregma and secured via bonescrews and a headcap constructed with dental cement. cranioectomy was performed posterior to lambda and 1 mm lateral from midline. the dura matter was kept intact and target structure location was estimated via stereotaxic coordinates and physiological measurements in response to auditory stimuli. recordings were typically made from the right ic."
"to show that wtapso is able to track changing global minimums, the styblinksy-tang function was used, which has a global minimum that changes with problem dimension, defined by:"
"untethered devices perform signal processing as one of their primary computational activities due to their heavy usage of wireless communication as well as rendering of audio and video signals. fourth generation wireless technology (4g) has been proposed by the international telecommunications union to increase the bandwidth to maximum data rates of 100 mbps for high mobility situations and 1 gbps for stationary and low mobility scenarios like internet hot spots [cit] . this translates to an increase in the computational requirements of 10-1000x over previous third generation wireless technologies (3g), with a power envelope that is limited to increasing only 2-5x [cit] . figure 1 presents the demands of 3g and 4g computing in terms of their peak processing throughput requirements and their power budgets. conventional processors cannot meet these requirements; low-power laptop processors, such as the pentium m, operate below 1 mop/mw, while digital signal processors, such as the ti c6x, operate around 10 mops/mw. conversely, 3g wireless protocols, such as w-cdma and 802.11a, require approximately 100 mops/mw. the ibm cell system can provide excellent throughput, but its power consumption makes it infeasible for mobile devices [cit] . research solutions, such as viram [cit] and imag-ine [cit], can achieve the performance requirements for 3g, but exceed the power budgets of mobile terminals. soda improves upon these solutions and delivers a solution for 3g wireless [cit] . companies such as phillips [cit], infineon [cit], arm [cit], and sandbridge [cit] also propose domain-specific systems that can support 3g."
"since (, 0) is a univariate polynomial with degree at most − 1, the secret can be obtained in step 5 through lagrange interpolation as"
"in optimization, many solvers suffer from the curse of dimensionality, where higher dimensional problems become overly difficult to solve. this necessitates the need for specialized high dimensional solvers. further complicating these issues is the fact that many experimental paradigms may not provide access to explicit derivatives, meaning that classical optimization methods cannot be utilized. furthermore, computational neuroscience models are often high dimensional in nature. in this work we present a new social network for particle swarm optimization which mimics winner-take-all connectivity found in visual cortex neural networks. we have demonstrated that wtapso is able to solve very high dimensional problems, outperforming traditional social networks and publicly available particle swarm toolboxes. in this study, we used a relatively simple static constrained coefficient method. it is predicted that allowing for a dynamic constriction coefficient will yield better results as step size can be modified based on distance from a solution. it was also seen that individual agents in wtapso can get stuck in local minima and cannot be drawn out even with well behaving influences. depending on problem complexity, there is some wasted processing spent on agents which are no longer able to find solutions. a way to mitigate this may be the inclusion of a grim-reaper routine, which searches for agents that are both far away from the best performing agent and no longer making significant advances in fitness function. this routine will then remove the agents from the swarm, and thus reduce computational load, or reinitialize the agent based on best performing network and global leaders. the latter may also provide the algorithm better influences as one more agent will now be searching a space that is more likely to contain the local minimum than its previous location, but this remains to be tested."
"two recognition methods based on neural network and dynamic programming has been presented. the system recognition consists of three phases including pre-processing, features extraction and recognition. the pre-processing includes position normalization, baseline skew correction and segmentation. the skew angle is determined by using hough transform. the segmentation process consists of two steps: (a) segmentation of text into lines using horizontal histogram, (b) segmentation of lines into characters using vertical histogram. in features extraction phase a set of 7 invariant moments descriptors, 7 modified invariant moments descriptors and 7 walsh coefficients have been used to represent the numerical features of the character extracted. finally the numerical features are passed to the classifiers to recognize the character. the programs were written using matlap. as mentioned previously, no efficient technique has been found for tifinaghe scripts recognition. this field is of importance for future research."
"\"enumeration\" consists of generating a set of subgraphs from a given dfg, and determining if they can run on an accelerator. generating a set of subgraphs is difficult because the number of possible subgraphs grows exponentially with the size of the dfg. determining if the subgraphs can run on an accelerator, i.e., determining if they perform the same computation, is essentially equivalence checking, which is np-complete. the problem is further complicated if the accelerators perform a superset of the desired computation (e.g., an accelerator for dot-products could also accelerate multiply-accumulates in an application)."
"the rest of paper is organized as follows. in section 2, we review some secret-sharing schemes based on polynomials. in section 3, we present the models for pss, including the system model, the adversary model, and the security goals. our proposed (, ) pss scheme based on bivariate polynomials is introduced in section 4. its security and complexity analysis is described in section 5. finally, we conclude the paper in section 6."
"with f(*) denoting the cdf which a sample is to be drawn from. the random variable x is then a sample from cdf f(*). it should be noted that while this is a traditional method of sampling from an arbitrary probability distribution, slight aliasing is inherent in the method. the model feature space can be defined as necessary to capture relevant aspects of the model. table 3 displays features for model reconstruction in am and frequency tuning models. finally, as pso performance modulates across fitness functions (table 3), careful selection of an appropriate function is important to algorithm performance. in order to preserve shape and absolute rates endemic to frequency tuning curves, a fitness function which accounts for both the correlation between experimental and model curves as well as their relative distance was used [cit] as shown:"
"where p n, p l correspond to the best neighbor position and best neighborhood leader positions respectively. updates of neighborhood leaders draw influence from their local neighborhood as well as the global leader (fig. 2b ). like the agent update, the neighborhood leader's neighborhood is polled for the best performing agent, excluding the neighborhood leader. neighborhood leaders will always draw influence from the global leader as the global leader by definition is the best performing agent of the swarm. similar to neighborhood agents, updates are made as follows:"
"the same problem also arises if secret-sharing schemes are used as building blocks in some other cryptographic protocols. for example, threshold cryptography, first introduced by desmedt [cit], is the application of secret-sharing with public-key algorithms. among various threshold cryptosystems, some are based on elgamal [cit], some are based on rsa [cit], some are based on elliptic curves [cit], and some are based on pairing [cit] . in these protocols, shares are either used to generate a digital signature or used to decrypt a ciphertext. to prevent any nonshareholder from learning the outputs of the protocol, a shared key is also needed between every pair of shareholders. similarly, employing an additional key establishment protocol in threshold cryptosystems can complicate the process significantly."
the  i 's have dynamic values. thus it was found that it was more practical to deal with the logarithm of magnitude of  i . thus the seven moment invariants used in the proposed system are replaced by their logarithm values. for each character issued from the segmentation process the above moment invariant descriptors are calculated and fed to the classifiers. ( 2 03 21 2 12 30 03 21 03 21 2 03 21 2 12 30 12 30 12 30
"similar to rc4, crc has non-contiguous, pointer-chasing memory accesses (line 7). an additional challenge is that crc also has a dependence, crc_accum, that crosses loop iterations thus preventing any significant form of data-level parallelism. like rc4, crc would garner little if any benefit from traditional simd accelerators. both of these applications are very important to many embedded domains, and augmenting a simd datapath to better support them would prove very beneficial. even in applications that are amenable to simd acceleration, often times only their inner-most loops are data-parallel and can be simd-ized. simd datapaths are continually getting wider with moore's law [cit] but as this happens, the non-simd-ized portions of the application will begin to dominate execution time as per amdahl's law. figure 4 illustrates the importance of this trend. this figure shows the percentage of time spent outside inner-most loops, i.e. executing non-simd-izable code for several simd-izable benchmarks from the mediabench suite [cit] . from this figure we can deduce that even in an ideal system with an infinitely-wide simd machine, over 30% of the application's execution time is spent on non-simd-ized code. this limits the speedup of these applications to ≈3x in the best case, and clearly represents an important target for acceleration."
"one approach to accelerating the scalar code is to enhance the capabilities of the scalar pipeline. integrating specialized functional units which more efficiently execute critical portions of an application's dataflow graph with instruction set extensions to utilize the new hardware is a popular approach for designing application specific instruction processors, or asips. several commercial tool chains design asips with instruction set extensions, including arm optimode, and arc architect. however, this approach fails to take advantage of the vast hardware resources already present in the datapath, namely the simd execution units, which are mostly idle when scalar code is executing."
"based on the above analysis, the computational complexities are similar in both schemes. but, compared with shamir's secret-sharing scheme, more information needs to be transmitted and stored by each shareholder in our proposed scheme. the price is paid to achieve an additional property that the recovered secret is not revealed to nonshareholders. this property is desirable in many applications and our proposed scheme achieves it even if the adversaries have unlimited computational power. although including a pairwise key establishment protocol [cit] with shamir's secret-sharing scheme can protect the secret from nonshareholders as well, most pairwise key establishment protocols are computationally secure (not information theoretically secure) and the complexity of key establishment protocol will have a quadratic relationship with the number of shareholders participating in the secret reconstruction phase."
"in this work, benchmarks are generally classified into two categories. the \"media\" benchmarks are the same as those in ure 4 and are from the mediabench benchmark suite. the rest are from the mibench [cit] and netbench [cit] suites. these benchmarks were chosen as they were representative of applications that are normally run on mobile devices. for the media benchmarks, the simd-izable inner loops are not considered as they are assumed to execute on a normally configured simd datapath. the other benchmarks demonstrate limited datalevel parallelism and, therefore, the entirety of the benchmarks are considered for execution under simd-morph. figure 8 shows the distribution of various sizes of subgraphs in each of the applications in increments of 4. the overwhelming majority of subgraphs, 83%, are between 1 and 4 operations. approximately 16% of subgraphs have between 5 and 8 ops and just over 1% of subgraphs have more than 8 operations."
"as embedded devices become more and more pervasive, the types of computation they are required to perform becomes more varied. simd-ization is one of the most common architectural techniques used to improve the efficiency, but it is not effective for many styles of applications, e.g., some types of data encryption and compression, and many forms of error detection and correction."
"in this phase, the proposed ocr system detects individual text lines and then segments lines into characters. the lines of a text are segmented by using the horizontal histogram; we browse from top to bottom until the first line containing at least one black pixel, the line is the beginning of the first line of text, then we continue traverse until a line that contains only white pixels, this line corresponds to the end of the first line of text. with the same way, we continue to detect other text lines (fig.5 a) . the same principle is used in the vertical histogram to detect characters in each line of text ("
in this phase features are extracted from the external contour of the character (fig.6) . in order to differentiate between the characters illustrated in fig.7that
"the power of pso arises from the emergence of intelligent problem solving behavior in the interactions of simple agents in social networks. adjustments to the pso algorithm typically arise in the mathematical representation of agent interactions or social network topologies, such as inertial weight constraints [cit] . metaheuristics, pso specifically, have an inherent tradeoff between swarming and convergence behaviors [cit] . as these are population metrics, they are largely controlled by the architecture of the social network. too little swarming may lead to the swarm prematurely converging to an incorrect local minimum, while too much may become computationally burdensome and may prevent convergence on the optimal solution [cit] . therefore, the study and design of social networks is of prime importance to pso."
"step 5. (loop) loop in step two allows computing the error threshold (0.0001), and the number of terations (50000). after the learning of network and the execution of tifinaghe characters recognition system to recognize a text, we use the euclidian distance to identify characters."
"several important application domains are not amenable to simdization, and even those that are have a significant fraction of non-simd-izable code that is important to accelerate. for these reasons, this paper answers the question, \"how can the standard narrowscalar-pipeline-with-wide-simd-pipeline architectures be modified to better accelerate a wider variety of applications?\""
"to begin, we demonstrate local and global neighborhood behavior of the wtapso algorithm. figure 3 oscillation behavior might also be due to agents changing rank in the social network. however, we cannot say this for certain as leadership changes were not tracked for this test. irrespective, the overall trend for each neighborhood leader is towards the minimum. the global leader is always monotonically decreasing towards the minimum value. notably, there are areas in which the global leader seems to get stuck in local minimums, but network dynamics push it out of these minimums (fig. 3 upper and lower right)."
"where pixel(x, y), is the intensity of the pixel with the coordinates (x, y). in this operation, firstly, we compute the horizontal and vertical histograms, secondly, we scan the horizontal histogram in two directions: from top to bottom and bottom to top respectively until the first meeting of black pixels, finally, we scan the vertical histogram in two directions: from left to right and right to left respectively until the first meeting of black pixels. after obtaining the positions of first black pixels, unwanted areas are eliminated in the image as shown in (fig. 3) ."
"where k s desired output and k z the actual output. in the next, we propagate this error on the hidden layer; the error of each neuron of the hidden layer is given by"
"pre-processing algorithms provides the required data suitable for further processing. in other words, it establishes the link between real word and recognition engine. preprocessing steps consist of digitization of image document and cleaning it (by medium filter for example), converting the gray-scale image into binary image, normalizing the text [cit], detecting and correcting baseline skew [cit], and segmenting [cit] the text into lines and the lines into characters."
"the baseline simd architecture used is shown in figure 5 . this baseline is modified in the manner shown in figure 6 to create the simd-morph architecture. the 16 simd lanes are grouped in 4 configurable execution groups (cegs) of 4 elements each, named ceg0 (lanes 0 to 3) through ceg3 (lanes 12 to 15). the fus in all the lanes are capable of executing the same operations as before but now each ceg has an added memory unit capable of executing scalar loads and stores. each fu may receive its inputs from 8 possible sources: the 4 outputs of the previous ceg, any of the other 3 fus in its row or from an 8-bit constant register (not pictured). in addition, the memory fus may receive loaded data from memory."
"characters are classified according to their computed features by means of artificial neural networks. among the many applications that have been proposed for neural networks, character recognition has been one of the most successful. many neural network architectures have been used in optical character recognition implementation. mlp is usually a common choice. unfortunately, as the number of inputs and outputs grow, the mlp grows quickly and its training becomes very expensive. in addition, it is not easy to come up with a suitable network design and many trail-and-error cycles are required. the neural network (fig.8) the processing of neural network consists of five steps:"
where () i bx is the ith bit in the binary expansion of x (it is equal either 0 or 1). table 6 represents the seven first elements of the vector walsh calculated for one character with four transformations. while table 7
"inputs in the frequency tuning models consisted of peristimulus time histograms whose input shapes were allowed to dynamically vary according to swarm updates. once input psths were formed, a curve-fit was made to approximate an input probability density function. cumulative density functions were then formed. input spike times were drawn from excitatory and inhibitory input cdfs via an inverse transform sampling method [cit] ). briefly, a value u was generated from a uniform distribution. then, a random variable x was found satisfying:"
"modern wireless devices are often equipped with wide simd processors in order to exploit data-level parallelism that is very often present in the media applications that these devices need to execute. however, not all portions of these applications is amenable to simd-izing. further, several other, non-media applications are often run on these devices as well and during their executing only the scalar portion of the processor is used while the simd datapath is left idle. simd-morph is a design which allows the simd datapath to be used in these situations as well. this design modifies a standard 32-bit, 16-wide simd datapath by adding connectivity between the different lanes in order to exploit instruction-level parallelism in sections of code that would normally be executed on the scalar pipeline of a simd processor. the performance benefit of simd-morph is evaluated in two ways: speedup obtained from executing outer loops of applications when the inner loops are easily simd-ized and also the speedup obtained from executing purely sequential code on a substrate that is normally used to execute code with data-level parallelism. the performance impact for these two scenarios is a very impressive 1.4x and 2.6x, respectively."
"to reflect that m out represents a mixture of digestate and electricity in the process matrix we have set that each tonne of food in produces one \"unit\" of m out, where one unit of m out corresponds to 0.55 tonnes of digestate and 0.96kwh. this fact that m out consists of two different types of output is then taken care of in the pricing of m out, discussed further below."
"but can deter such a malicious behavior since the leaking user can be traced and evicted users, we say we are in the disclosure security model. as we will see later 38 in this paper, there is a sensible difference between these two security models."
"to set key k i 's bit length assume that the adversary, in t a seconds, is capable of breaking an s bit key. therefore, the bit length of key k i should be set to: by a leave. since there is no point in protecting the payload after key k i expires, it 7 is enough to protect k i for its life to protect all dependent messages."
"our results suggest that there are at least three possible indirect effects of the rocs policy on the organic waste industry: firstly, by promoting ad plants as a means to recycle organic waste it could drive in-vessel composters out of business. secondly, above a threshold level that is needed to make ad plants viable, increasing rocs does little to promote ad and instead drives the price of organic waste up, effectively subsidising waste producers. the fact that financial incentives do not necessarily benefit those that they are aimed at is seen in other areas of policy, where, for example, financial incentives to help people buy houses can drive house prices up, not make houses more affordable. thirdly, high levels of rocs could result in the price of organic waste becoming positive, meaning that waste producers can sell their waste instead of having to pay for it to be removed. this potentially incentivises waste production rather than waste minimization. hence the beis policy of rocs could inadvertently undermine defra's drive to reduce organic waste. this is a direct result of the differential impact of the rocs policy on composters and ad plants because of the dual role that ad plants have as both waste treatment and energy production plants."
"where s i is the set of users that share key k i . therefore, the lifetime of key k i is the smallest t such that"
"we can make two remarks: first, the bit length of a key of the lkh tree in the disclosure security model is shorter than the bit length of the same key in the no-disclosure security model. indeed, from equations 16 and 17:"
"subsequently, for speed of computation and ease of analysis of the simulation data, the model was re-coded in matlab [cit] . both the netlogo code and the matlab code are available from the surrey data repository."
"designing effective policy to manage social systems is a difficult problem. many systems are complicated and nonlinear, which can result in indirect, unforeseen, effects of a policy. the behaviour may be non-intuitive: the same policy applied in apparently similar scenarios may have different effects due to historical factors. this is known as path dependency. these system issues can be further exacerbated by the fact that different policy levers coming from different policy perspectives are frequently aimed at steering the same system. for example, at the uk government level, the department structure lends itself to policy design in silos, with each department championing their own policies with their own objectives and in competition with each other for financial resources from the treasury."
"our results further highlight the fact that a static view of policy, with a given policy linked to a given outcome is insufficient to predict policy effect reinforces and the need for dynamic techniques such as agent-based modelling."
"the last two decades have seen a significant reduction in the percentage of waste sent to landfill sites due to policy interventions, with more recycling of paper, glass and metal and more biodegradable waste directed to either composting or anaerobic digestion. the principal responsibility for waste policy lies with department for environment, food and rural affairs (defra), but some aspects of environmental governance are also managed from the department for energy and climate change (decc), which was subsumed within the new department of business, energy and industrial strategy (beis) [cit] . reducing the quantity of biodegradable waste sent to landfill and the concomitant development of a market for renewable energy from biodegradable waste requires the coordination of many agencies in both departments [cit], there was little clear evidence of a joint strategy."
each agent develops its own trading network: waste processing agents start by only having a few business 'contacts' and gradually extend their list of contacts with time. this knowledge network is important: an agent can only do business with someone that they know.
"when all keys are set to the maximum length, the total number of bits is: fig. 4 . performance of our model compared with the performance of the classical model, for both the no-disclosure and the disclosure security models, when the number of users goes from 2 12 to 2 24 and the adversary is of type a1 and a2. users' leaves are independently and exponentially distributed."
"abms have been used in some policy areas, for example in economic policy [cit] but are still not widely used in others [cit] . here, we focus on the area of policy for food waste disposal within the uk, but the key message that policy outcomes can be path dependent and that dynamical modelling should be an important part of the planning process, holds more generally."
"it appears that the landfill tax provides a useful driver to encourage waste processing companies to enter the marketplace, but once there is competition for waste, further increases in landfill tax may have little effect."
"in section 3.1, we discuss two general scenarios (i) the impact of ramping up the landfill tax looking ahead, there are various policy options open to defra and beis for incentivising the bio-waste industry. in section 3.2, we demonstrate that, even with only two policies, the outcome can depend on how the policies are applied: the same levels of landfill tax and rocs can result in radically different proportions of ad plants to in-vessel composters depending on the order and the time for which the policies are implemented."
"the above computation can be used for both the auxiliary keys and the group key. note that, if you consider keys k i and k 2i (k i is the father of k 2i in the tree), then"
"there is an interesting side effect of the policy: as with no landfill tax, there are oscillations in the price that lead to oscillations in the number of composters and the amount of waste recycled. the landfill tax tends to exacerbate these oscillations, making their amplitude larger. this is because the landfill tax has a significant influence on the price for organic waste for the first few years. this is the period when most organic waste is still sent to landfill and there is little competition between waste processing companies. during this period, the price for organic waste is lower in the presence of landfill tax than in its absence. a lower price enables waste processing companies to make a higher profit. however, this higher profit means that they have more financial reserves, allowing them to remain in the market for longer when prices start to rise and resulting in prices reaching a greater maximum before crashing."
"this completes our model for choosing the key's bit lengths in the lkh tree. in 9 the following section we present a particularly important case, when users behave 10 independently and their joins and leaves are exponentially distributed. in this case,"
"to summarise, we can recognise that the drive to increase food waste recycling becomes a competition for the survival of the fittest between anaerobic digestion and in-vessel composting, where the 'fittest' is the type of business that is most economically profitable. [cit], as shown in figure 5 . this suggests that ad plants are currently out-competing in-vessel composting. however, since the financial viability of anaerobic digestion is strongly tied to financial incentives determined by policy, this is not a robust situation. in the next section, we demonstrate that a phased removal/reduction of rocs can lead to different scenarios. figure 4 suggests that the relative composition of composters to anaerobic digesters depends on the financial incentives. however, the situation is more complex: the historical state of the market has a strong influence on policy outcome. this is illustrated in figure 6(a) and (b) . this shows the results of a series of simulations where the rocs level is increased in increments of 1p/kwh from zero up to 10p/kwh and then reduced again. for each value of the rocs, the simulations are run for 50 years so that transient behaviour has died away. this enables the underlying equilibrium structure to be determined. the timescales involved may mean that an equilibrium is never reached, but an understanding of the equilibrium structure is nevertheless useful: as we will see below, it is this equilibrium structure that underpins transient behaviour. so for the results shown in figure 6 (a) and (b) the sequence is: start at a rocs level of zero, run the simulation for 50 years and record the outcome. then increase the rocs by 1p/kwh, and starting from the final state of the previous simulation, compute for a further 50 years and record the outcome. this is continued until the rocs level is 10p/kwh. then the process is repeated, but this time stepping down in rocs. this whole process of increasing then decreasing the rocs was repeated ten times. in figure 6 it can be seen that if the rocs incentives are increased gradually from zero, no ad plants occur (cf figure 4 (a) ) until the rocs level is above 5p/kwh; no composters occur if the rocs levels are maintained above 7p/kwh (cf figure 4(c) and (d) ). however, if the rocs are first raised to a high level then reduced, a mixture of ad plants and composters can exist, even for rocs levels below 5p/kwh. so for the same rocs incentive (0 to 6p/kwh) the balance between the ad plants and composters is strongly dependent on the policy history. the reason for the existence of two possible outcomes for the same rocs incentive is that the financial decision to start an ad plant is based on whether it is 'profitable' to do so. the decision on profitability is based on the current market price for organic waste, the cost of any other raw materials, the cost of any loans and the day-to-day running of the plant and the income from selling electricity, all assuming the plant is operating at 80% capacity. the conditions for company start-up are more stringent than those for an established ad plant to survive, since the established plants may be able to operate at greater than 80% capacity and/or have paid off their loans. so on gradually increasing the rocs, no ad plants start up until the rocs level is above approximately 5p/kwh. however, on decreasing the rocs, it is possible for some ad plants to survive at rocs levels below 5p/kwh. no account of future market price/policy expectations are made in the decision to start up, but including market expectations is unlikely to change the fact that the market conditions required for a company to start-up are unlikely to be the same as the market conditions required for a mature company to survive. if the desired policy outcome is to have a sustainable ad industry but with minimal financial incentive, this suggests that this is possible, but only if there is sufficient investment for a long enough initial period to firmly establish the ad industry. this is further illustrated in figure 7(a) and (b) where rocs are introduced at a high level, then reduced after 10 or 30 years respectively. in each case, it appears that a thriving ad industry has been established between year 7 and year 17 when the higher level of incentives is in place. however, with the removal of the incentives in year 17 (figure 7(a) ), the ad industry collapses because they are no longer financially viable. the crash in the ad industry results in a crash in prices with more waste initially diverted to landfill, before the in-vessel composting industry fills the vacuum. the difference after 30 years is that at least some of the ad plants have existed for long enough to pay off their set-up costs and/or are managing to operate at greater than 80% capacity -in the simulations it is assumed that loans to cover set-up costs are repaid over a twenty year period."
"the key graph model requires to build up a balanced tree of auxiliary keys (in the 14 following key tree), where the root is the group key and the leaves are the private 15 keys of the users in m. each user stores the keys on its leaf-root path; for example,"
"the typical effect of increasing landfill tax according to the historical time course (figure 1), thereby increasing the cost of sending organic waste to landfill, is shown in figure 3 (a) and (b). at the start of the simulation, all organic waste goes to landfill. when the landfill tax is introduced and combined with the gate fee, it costs £27 /tonne for waste producers to send organic waste to landfill i.e. a 'price' of −£27 for organic waste. after three years, landfill tax is increased to £10, further increasing the cost of sending waste to landfill. this only leads to a small dip in the price of organic waste since, by this point, much of the waste is being recycled. as more and more organic waste is recycled, the price of organic waste becomes less and less sensitive to the landfill tax. hence overall, the picture with gradually increasing landfill tax is qualitatively similar to that shown for no landfill tax in figure 2 . in the long term, the amount of waste going to landfill and the price for food waste are unchanged. this is because altering the landfill tax has only an indirect impact on the internal economics of the individual composters as it is a tax paid by the landfill operators. there is the same initial linear increase in the number of companies that is seen in the absence of landfill tax, but since the price remains attractive for longer, more companies start up."
"a further research direction would be to extend the proposed model according to 21 different kinds of distributions; however, this is out of the scope of this paper and 22 in the following we will focus on the exponential distribution only."
"the disclosure security model implies that key k i, for all i, has to be protected just for time ki and the bit length of key k i should be set to:"
"finally, there are large oscillations in the price, which then lead to large oscillations in the number of waste processing companies and consequently in the amount of waste going to landfill. the presence of oscillations is because there is inherently some delay in the system: when waste is very low in price, companies make a profit. this accrued wealth then sustains them through a period when prices are unrealistically high. eventually, continued high prices result in some companies consistently making a loss and going out of business. since many of the companies started at a similar time with a similar economic model, once one goes out of business many others follow. essentially, the accrued wealth through the 'good' times enable companies to function for a certain period in an environment which is not economically sustainable. the buffer of the accrued wealth means that there is a delay between economically unsustainable conditions and the companies going out of business."
"the overall daily list of decisions for the agents is set out in table 1 . each of these decisions have to be modelled. so for example, specific rules have to be defined for how waste processing companies decide to start up; who trades 1. agents that produce food waste buy raw materials. 2. the raw materials are processed, generating food waste. 3. existing contracts to supply food waste are fulfilled. 4. new contracts are made and food waste transferred under the terms of these contracts from suppliers to waste processors. 5. prices of food waste are updated by the suppliers of food waste. 6. any food waste that has not gone to food waste processors goes to landfill. 7. waste processors buy any additional materials that they need to process the waste that they have. 8. waste is processed by the waste processors. 9. waste processors sell their outputs. 10. waste processors pay for operating costs and for loans. 11. the profit for each agent is calculated. 12. any agents who have run out of money go bust. 13. new 'contacts' are made. 14. new waste processing companies are formed if it is 'profitable' for them to do so. 15. any contracts that have come to the end of their contract period expire."
"note that the payload is also dependent on key k i, as implied by theorem 2. the 23 worst case scenario happens when a payload message is sent right before k i expires."
"at each time step the agents in the model undertake a series of tasks, as listed in table 1 . within the model, the main role of waste producers is to produce waste which they then transfer either to landfill or to waste processing companies. the role of waste processors is more complicated and we give further details on how they function in 1.1. in 1.2, the values used for model parameters are discussed. there are a number of key decision rules: how waste processing companies decide to start-up; who trades with whom; the pricing of waste, and how different policies are implemented. these are discussed in sections 1.3, 1.4, 1.5, 1.6, respectively."
"the decomposition of biodegradable waste in ad plants produces methane that can either be sold directly as a renewable fuel or burned on-site to generate green electricity. either way, an important income stream for ad plants comes from rocs. the net effect is that the waste policy of promoting the use of ad plants is intimately connected with the energy policy of using rocs, with rocs having a direct impact on the financial viability of ad plants."
"we know that in b r every key k i is dependent on all the keys in the sub-tree of b r 2 rooted at k i . of b r and, by transitivity, on every key in b r (see figure 3) . therefore, we get the"
"-parameter is set to 10 −6 . keys of a pre-defined length, the procedure is further simplified -there is no need 22 to resort to key stretching-, while the advantages being the same."
"keys are used to protect information. so, a key is \"important\" if it is used to 8 protect \"important\" information, that is information that is required to be kept 9 confidential for a longer period with respect to other information managed by the 10 system. here, we formalize this claim. note that, while it is usually clear how to 11 set confidentiality requirements for data (i. e., the payload sent to the multicast 12 group), it is not trivial to understand what requirements should be asked for the 13 keys. indeed, during standard multicast group activities, many users join and 14 leave, and many keys are generated, are used for encrypting either the payload or 15 re-keying messages (that carry other keys) and eventually expire. so, we start from 16 a formal definition of \"importance\" of keys. later, we will show how this notion 17 can be used to build a methodology for choosing the bit length of every key in the 18 lkh tree to guarantee a required degree of confidentiality of multicast data. we say that a multicast system (such as the lkh model) is secure against the 5 brute force attack if no payload message can be decrypted while in its lifetime as 6 a result of a brute-force attack. interestingly, we will show that the \"importance\" 7 (i. e. the bit-length) of a key does not directly depend on its lifetime, but on the 8 lifetime of the information dependent on that key."
"uk food waste amounts to approximately 15 million tonnes a year, of which approximately 7.2 million is from households and 3.9 million from manufacturing from around 7000 food and drink companies. [cit], there were 600,000 tonnes of food waste from the hospitality sector, most of which went to landfill. for the simulations here, we consider only the largest 700 companies that together have a total waste output of approximately 2 million tonnes per annum. the average waste per day per company is 7.8 tonnes, with companies varying from 1 to 14.6 tonnes a day. anaerobic digesters can operate on a small scale, with some viable plants operating on farms primarily fed with slurry. here we focus on the role of large commercial plants for food waste that can process about 50,000 tonnes of food waste/annum, equating to approximately 150 tonnes/day."
"secondly, as more waste processing companies enter the marketplace there is competition for waste, so the average price increases from the initial value of −£20 that is fixed by the gate fee. (note that we have defined the 'price' to be the amount of money received by the waste producer for organic waste in pounds per tonne. hence a negative price means that waste producers have to pay for the removal of their organic waste, rather than being able to sell it.) prices do not increase indefinitely. waste processing companies start out with an initial investment of capital and at each time step spend money on raw materials, operating costs and loan repayment. at the same time they receive an income from the sale of their products. although the initial investment enables companies to operate for the short term even if they make a loss, they cannot keep making a loss without running out of money and consequently going out of business. within the abm simulations, the result is that prices settle about an equilibrium value where on average companies just make a profit. for composters, with the parameters fixed as given in table s1 in the supplementary material, this means that the maximum sustainable 'price' for organic waste is approximately −£12."
"the specific model considered here illustrates the possible impact of landfill tax and rocs on the recycling of food waste. in the model, the agents consist of individual companies that either produce food waste or take food waste in as a raw material and process it; this latter category of agents includes in-vessel composters and ad plants which currently represent the prime methods used for recycling food waste. in-vessel composting refers to a collection of methods where organic waste is recycled in a container or building in which air circulation, temperature and moisture are controlled to ensure that decomposition occurs aerobically. conversely, in ad plants decomposition is designed to occur without access to oxygen, resulting in the production of methane. ad plants are more expensive to both build and run than in-vessel composters."
"there is little market for digestate, so for the current simulations the price for digestate is set at £0/tonne. operating costs are set to be £20/tonne capacity each day this is based on the fact that a typical ad plant employs approximately 0.05 people per tonne each day. in a report by andersons on the economic viability of ad [cit], it is estimated that to build an ad plant costs between £2,500 and £6,000 for every kw of electricity of generating capacity, averaging about £4,000."
"according to a generally accepted classification provided by it is used to encrypt information for some time, and eventually it expires (when 4 the key is not used any more). we will refer to the length of the life of a key as its 5 lifetime."
"the principle behind increasing landfill tax is that high gate fees will promote a market in biowaste around an equilibrium price. ad plants are more expensive to start up and more expensive to run than in-vessel composting. for this reason, our simulations of the effect of changing landfill tax did not lead to the development of an ad industry: competition between in-vessel composters kept the price that waste processors have to pay for waste above the level that makes ad plants economically viable. this position is changed with the introduction of rocs, as is shown in figure 3 again, as waste processing businesses enter the marketplace, competition for waste drives the price up. the price reaches a high, unsustainable maximum before dropping and oscillating about an approximately constant level. during the first seven years, when there is ample waste and no rocs incentive, composters start up. with the introduction of rocs, ad plants also start to appear. however, once the price that waste processors have to pay for waste becomes unsustainably high because of the competition for waste, it is the companies who are 'fittest' that survive. here, the fitness of the company depends crucially on the internal company finances with companies that are more profitable having more chance of survival. in the simulation shown in figure 3 note that, once financial incentives are high enough that it becomes economically viable to run an ad plant, the overall level of the rocs does little to directly benefit the waste industry: competition means that greater financial incentives result in higher prices for waste. so the main beneficiaries of financial incentives become the waste producers, not the waste processors. consequently, higher financial incentives do not result in more ad plants or in more waste being recycled and instead turns waste into a marketable by-product. this could be regarded as reducing the incentive to minimise waste generation, which runs counter to the fundamental principles of waste management."
"our methodology shows an improvement of more than 67.4% in key storage. as 4 for a2, users are required to store 879 bits, thus saving more than the 59.6% on 5 the classical naïve solution., the bit length of key k i should be set to"
"along with concerns over capacity, biodegradable waste regulation has also been driven by growing concerns over climate change. methane, a potent greenhouse gas, is released during the decomposition of biodegradable waste in landfill sites [cit] identifying food waste as a priority [cit], under which anaerobic digestion (ad) was promoted as an efficient use of biodegradable waste."
"claim 1. a multicast system is secure against the brute-force attack if every key 10 k is long enough to resist the attack for the life of all messages dependent on key k. based on the above described result, we can build a methodology for choosing 13 the key length in the lkh tree model. we assume that the payload's lifetime is 14 chosen by the multicast group manager. this is reasonable, since it depends on 15 the confidentiality requirements the manager is willing to enforce on the payload."
"we can now generalize. let b 0 be the empty tree, that is the key tree at time t 0, 24 when the system starts. moreover, let b r be the key tree at time t r, right after the r-th re-keying operation (either a leave or a join). from the above discussion,"
"in-vessel composters range from 10,000 tonnes/annum to 75,000 tonnes/annum, that is from 30 to 200 tonnes/day. in the model, the size of composters is set randomly between 30 and 200 tonnes/day."
"finally, our results illustrate that different policy trajectories can lead to different business outcomes, as seen by our results on path dependency. both the simulations shown in figure 7 started with the same initial market state with all waste going to landfill and price dictated by the landfill tax. in both cases the rocs level was introduced after six years at the level of 10p/kwh and at the end of the simulation was at the level of 2p/kwh. in both cases a thriving ad industry appeared to have been established, but in one case the ad industry crashed, and in the other it did not. the difference between the simulations was the policy path: in one case rocs were reduced after 30 years, and in the other after only 10 years. this illustrates the risks of stopping a costly policy when it appears to have 'worked'. this path dependency is typical of systems which are nonlinear, but is perhaps alarming from a policy design perspective: it confirms the fact that policies need to be well-designed to avoid lock-in to inferior outcomes."
"26 table ii shows the bit length of every key in a lkh tree of 65,536 users when 27 considering the two types of adversaries a1 and a2. focus on a1: the user's 28 private key should be 49 bits long, while the group key can be just 37 bits long. table ii. level-by-level key bit length for a 65,536 user lkh tree, for the two types of adversaries a1 and a2. payload's lifetime is set to 10 hours, users' permanence in the group is 90 days."
"key k i will be changed whenever a user in s i leaves, therefore its lifetime can be computed as the smallest time t such that pr min"
"then, we compute an upper bound on the lifetime of every key in the system and 17 we formally show which messages are dependent on that keys. leveraging the result 18 in claim 1, together with an upper bound to the adversary computational power,"
"landfill tax is paid by landfill site operators to hm customs and revenue on the waste they collect. landfill site operators pass on any change in landfill tax to their customers, so there is a direct correspondence between increases in landfill tax and increases in gate fee. consequently, changes in landfill tax are modelled as changes in the cost of sending food waste to landfill sites."
"clearly, it is important to protect the keys used in the group as well: if the group 17 key, for instance, is compromised, then also the payload is compromised. however,"
"the model shows that, as is to be expected in a market driven system, when resources are abundant prices are low. as they become scarcer, prices increase and reach a dynamic equilibrium value that is based on the need for companies to make a profit to survive and, consequently, on the financial incentives available in the system. however, the power of an abm approach is both the ability to encode decisions at the microscale from which macroscopic patterns of behaviour emerge, and the ability to study the non-equilibrium situation. so, from the microscale of individual waste contracts between waste producers and waste processors emerges a pattern that suggests organic waste recycling could be viewed as a battle for the 'survival of the fittest' with ad plants and in-vessel composters competing for a finite resource (organic waste). policies that give financial incentives can change the 'fitness' of different 'species' of company, with large financial incentives for some companies resulting in their market dominance. the need to study the non-equilibrium system is empha-sised by the decade long timescales required to reach equilibrium, when policy change typically occurs on the timescale of years."
"similarly, also payload has a life. in multicast communications, we assume the 7 payload is generated when it is sent to the group and it expires when its information 8 content is not valuable any more. again, we will refer to the length of the life of 9 a given payload as its lifetime. publicly known -there are many free services that deliver stock quotes with a short 14 delay."
"using an agent-based model we consider a number of different scenarios to examine the impact of landfill tax and the ro policy. this highlights how the impact of policies on the development of an ad industry is path dependent, that is, how the outcome of a policy is dependent both on the nature of the policy and the length of time that a policy is in place. in section 2 we give a short introduction to the agent-based model and the software used to create, run and analyse model output. this is followed by key results and discussion in sections 3 and 4, respectively."
"waste producing and waste processing agents interact with each other through two networks of links, one representing who knows who, 'contacts', and one representing the transfer of food waste from producer to processor as part of a waste 'contract'. these two networks are dynamic: new companies start with few contacts and, over time, gradually make contacts with other companies. this knowledge network is important because a necessary pre-condition for establishing a contract between companies is that companies know about each other. at an extreme, a lack of connectedness results in companies establishing an insufficient volume of contracts and business failure. the choice of contract partner is based on who is in the waste processors contact network and who has waste to dispose of. if there is more than one possible waste supplier, then a contract is formed with the supplier of waste who is offering food waste at the lowest price. since arranging contracts takes time, each day a fixed maximum number of contracts can be formed by any one processor. each waste processing company has a maximum possible operating capacity. contracts are formed for a variable amount of time, dependent on whether the contract is with an ad plant or a composter. this reflects differences between the different business types: composters have considerable flexibility about when and how much waste they can take in and tend to operate on a daily basis. in contrast, ad plants have a less flexible operation, and will tend to have a mixture of short and some longer contracts. consequently, in the abm all contracts for composters are set to be fourteen days long. each contract with and ad plant has a different length, chosen randomly from two weeks to six months."
"now, let us get some real numbers to support our claims. in the specific example above, the lifetime of user u j 's private key k n+j can be computed according to equation 13 :"
"the wider implication is that a fixed low level of financial incentive may not be enough to kick-start the development of a given industry: the financial incentive needs to be high enough for investors to see a viable business plan, and in place long enough that businesses are firmly established and able to compete with other industries that use the same resources."
"in figure 4, we plot, as a function of the number of users, the number of bits 1 to be stored on each user device, according to both the no-disclosure (ndsm) and 2 disclosure security models (dsm) and taking into account the two different types 3 of adversaries (a1 and a2). further, note that the same plot also represents the 4 number of bits the center is required to broadcast during a leave. note that the 5 improvement provided by our methodology tends to increase when the number of 6 users increases for both models and that, when the adversary is of the same type, the 7 disclosure security model allows to save more bits with respect to the no-disclosure 8 one. this is in accordance with intuition: if the payload can be disclosed by evicted 9 users, then it is possible to tune confidentiality to the minimum of the desired 10 period of confidentiality and the practical period during which confidentiality can 11 be effectively preserved, and this helps in reducing the bit length of the keys. confidentiality of the payload to the confidentiality of the keys."
"the lifetime of the user private key. the user private key is individual to each member of the group. it is assigned when the user joins the group and will be used by the center to encrypt individual messages sent to that user until it leaves the group, that is when its private key expires. the life of the user private key is the period of time the user stays in the group. so, to evaluate the lifetime of the user private key, we have to evaluate the period of time between the join and the leave of the typical user in the group. assume that t uj is the random variable telling how long user u j will stay in the group. we compute the lifetime of user u j 's private key k j+n as the smallest t such that"
"hence, from a defra perspective, the main focus has been on reducing the amount of biodegradable waste sent to landfill, initially driven by concerns over capacity and subsequently by concerns over greenhouse gas emission from landfill sites. they have used, and continue to use, landfill tax as one of their main policy instruments."
"thirdly, there is initially an approximately linear increase in the number of composters. in the model, we set how frequently new companies consider whether to start up or not. here set to one per fortnight, companies only set up if it is economically attractive to do so. since initially the price is low, companies set up at a constant rate of one per fortnight, resulting in the linear increase in the number of companies."
"the discipline of developing an abm can itself help clarify relationships and identify dominant factors/interactions that determine behaviour. furthermore, although it may be difficult to find parameters to calibrate the model precisely, the modelling approach means that model outcomes can be tested as to their sensitivity to model assumptions. like micro-economic simulations, our abm simulation and analysis focuses on prices and tonnage and subsequent dynamics, although it could readily be extended to include other effects."
"this might seem counter-intuitive. however, this fact reflects the observation that the shorter is the period of time the information is to be kept secret, the shorter is the key. a second remark is that, in the disclosure security model, the difference in bit length of the keys from level x to level x + 1 is exactly one. this allows to precisely quantify the total bit savings upon the best possible solution with all keys with the same length. indeed, the total number of bits to store the keys of an arbitrary user u j is:"
"at the same time, concern over climate change has changed energy policy. [cit] climate change act [cit] the uk aims to reduce greenhouse gas emissions by at least 80% [cit] . recognising that an important element of greenhouse gas emissions comes from the reliance on fossil fuels, the renewable obligation (ro) [cit] . the ro requires energy producers to provide a particular share of their energy from renewable sources to their customers. to support this, electricity generators receive renewable obligation certificates (rocs) for 'green electricity', that is electricity produced from renewable sources. rocs give electricity generators a price for green electricity that is above the wholesale market price for electricity. rocs are reviewed every four years, with different technologies being allocated a different level of rocs per mwh of electricity generated [cit] according to their technological and economic development. as a consequence, rocs received for individual renewable energy technologies can change stepwise over time."
"in this paper we illustrate how agent-based modelling can be used for scenario planning and enable the user to investigate how policy decisions made at a macro level impact on behaviour at the micro level, which in turn produces emergent macro level outcomes. in contrast to other modelling approaches, agent-based models (abms) are not expressed in terms of variables, functions or equations [cit] . instead the building blocks of an abm are autonomous heterogeneous entities (so-called agents) that interact with each other and with an artificial environment [cit] . for example, agents could be households, consumers, companies, workers or even whole nations. repeated interactions among these agents over time induce ceaselessly changing microeconomic patterns (e.g. production and consumption levels). the goal of abms is to properly describe complex systems and to infer and analyse their aggregate propertiesin a bottom-up perspective -from the interactions and behaviours of micro entities [cit] ."
"at each time step, waste processors receive/pay money for their inputs, receive money for their outputs and pay operating costs. they may also have to make payments associated with loans. since it can be expensive to set up a waste processing plant, particularly in the case of ad plants we assume that in order to start-up, waste processors have to take out a loan. suppose that the price/tonne for each input/output is given by £p i where"
"ad plants benefit from the ro policy through the biogas that they produce. this benefit is either in the form of a guaranteed payment based on the price per mwh of green electricity generated or from the market price of biogas. in the model, for simplicity, we assume that all ad plants use the biogas that they produce to generate electricity and receive a price for the electricity that is directly dependent on the rocs they receive."
"during the model development phase, the model was coded in an open source agent-based modelling package called netlogo [cit] (version 5.0.5). the netlogo environment has the advantage that it is easy to construct interactive models, where the user can vary parameters using buttons or sliders and consequent model behaviour can be observed, in real time, in a variety of graphical forms."
"dependent on k 2 and k 3, k 2 is dependent on k 4 and k 5, and k 5 is dependent on k 11 ."
"(15) now, we split our analysis for the two different security models, and for each of 24 these, we consider two types of adversaries a1 and a2."
"theorem 1. after the r th re-keying, every key k in t r is dependent on all the 14 keys in the sub-tree of t r rooted at k."
"policy design is challenging: almost inevitably policies that are designed to have one primary effect have secondary, perhaps unintended, consequences. here, our purpose is to illustrate the benefits of using an abm as a thinking/scenario planning tool to aid the development of intuition [cit] . specifically, we consider how the interaction between in-vessel composting, a low-cost technology (that still demands considerable process control for quality output), and anaerobic digestion, could result in a market for organic waste."
"in this case, the life of the payload exceeds the life of the key. by claim 1, we 25 conclude that key k i has to be protected for time ki +, where ki is k i 's lifetime,"
"most of the simulations shown below start with no waste processing agents and with all organic waste going to landfill. waste processing agents start up if it is 'profitable' for them to do so, and go out of business if they go bankrupt. each day, each agent runs through a sequence of steps that mimics the day-today decision processes of the companies concerned. for example, producers of food waste generate waste and send their waste to composters, ad plants or to landfill, whereas, waste processing companies take in food waste, process it and sell the products, debts are paid and money received for sale of the outputs."
"p2p networks connect users all over the world without considering either international borders or culture, thereby a single content sharing may cross many jurisdictions. these networks are usually connoted with copyright infringement because it is estimated that, over the course of a month, 96.3% of users of bittorrent portals have downloaded at least one infringing content [cit] . it is not feasible to enforce the exclusive rights provided by copyright law, if present, in every single jurisdiction while ensuring that they do not conflict with the rights and interests of users. as so, in an attempt to mitigate the impact of copyright infringement, rightholders are now trying to block websites such as the pirate bay instead of trying to bring end users to court [cit] because, in general, it is more efficient and less onerous to prove that the owners of such websites profit from the copyright infringement. therefore, such websites are subject to indirect liability as they induce, contribute to or fail to prevent direct copyright infringement. nevertheless, this does not mean that sharing copyright infringing contents is legal. on the contrary, users may be subject to civil and potentially criminal liability [cit] . the extent of such liability depends on the applicable legal framework and on the intent of such acts: a user may also be subject to indirect liability."
peer selection. trackers may narrow the advertised lists of peers to increase the amount of block requests sent to malicious peers and thereby increase the amount of blocks disclosed to them.
"the function externalpackagewrapper would then contain the script which is normally executed using the external software package. this can be made general purpose by passing parameters using the setting cfg.decoding.train.classification.model_parameters of tdt. the second approach for creating an interface to external toolboxes takes place at the level of the transformation of outputs. this might be useful if a user would like to complete the entire decoding analysis including the generation of output in an external toolbox. in this case, the user creates a placeholder for the classifier which merely passes all data and classification parameters inside the model variable as the optional output decoding_out.opt (e.g., using the toolbox function \"passdata\" as a classifier). then, the interface to the external package is created as follows:"
"reference to this paper should be made as follows: xhafa, f., ko lodziej, j., barolli l., kolici, v, miho, r. and takizawa, m. (xxxx) 'evaluation of hybrid algorithms for independent batch scheduling in computational grids', int. j. web and grid services, vol. x, no. x, pp.xxx-xxx. biographical notes: fatos xhafa holds a permanent position of professor titular at the departament of llenguatges i sistemes informàtics, universitat politècnica de catalunya, upc (spain). he was a visiting professor at the department of computer science and information systems, birkbeck, university of london, uk (2009 /2010 ) and a research associate at college of information science and technology, drexel university, philadelphia, usa (2004 usa ( /2005 . his research interests include parallel and distributed algorithms, combinatorial optimization, approximation and meta-heuristics, networking and distributed computing, grid and p2p computing. he has widely published in peer reviewed international journals, conferences/workshops, book chapters and edited books and proceedings in the field and is editor in chief of the international journal of space-based and situated computing, and of international journal of grid and utility computing, inderscience pubs. he is also actively participating in the organization of several international conferences in the field. [cit], 3pgcic 2011, [cit] -2010 . dr ko lodziej is managing editor of ijssc journal and serves as a eb member and guest editor of several peer-reviewed international journals. [cit], respectively. [cit]"
"the remainder of this section is organized as follows. first, we describe how peer arrivals are generated from real peer arrival traces. then, we characterize the simulation setup for both p2p models. lastly, we define the use cases considered. 6 the source code for cidrarchy is available at https://gitlab.inesctec.pt/pmms/ cidrarchy ."
"execution sequence. the execution sequence refers to the computing medium, being sequential or distributed. in the sequential case the meta-heuristics flows are run sequentially while in parallel setting the meta-heuristics flows are run in parallel in a networked computing environment."
"scaling is the process of adjusting the range of data which enters the classifier. this can be done to bring data to a range which improves the computational efficiency of the classifier (for example libsvm recommends scaling all data to be between 0 and 1). it can, however, also be used to change the relative contribution of individual features or individual samples or to remove the influence of the mean spatial pattern [cit] which might affect classification performance. scaling is also known as normalization, but we prefer the term scaling to distinguish it from another meaning of the term \"normalization\" which is commonly used in the mri community to refer to spatial warping of images. typically, row scaling is used, i.e., scaling across samples within a given feature. although scaling can theoretically improve decoding performance, for some data sets it may not have any influence [cit] . practically, scaling often has little or no influence on decoding performance when beta images or ztransformed data are passed, because this data already represents a scaled form of the raw images which is scaled relative to each run, rather than to all training data. however, scaling may still speed-up classification."
"the mistrustful p2p model provides deterministic protection of user content interests against passive attacks of any size, and against active attacks of a size up to c . passive attacks are defeated by avoiding block advertisement. active attacks are thwarted by constraining the amount of blocks implicitly disclosed to an attacker of a size up to c to be at most m, and by downloading at least m blocks per cover content. the provided protection is deterministic because contents are only downloaded if the privacy requirements are met. if the size of an attacker exceeds c, then the provided protection may become probabilistic: the attacker may be able to know if the peer fully downloaded a content (genuine), and thus know that the user has interest in that content. however, our model still provides deterministic protection of user content interests against all other attackers of a size up to c ."
"in the traditional use cases, despite considering an optimistic model that has global knowledge of blocks availability/rarity and that is able to take more advantage of seeders, around 20% of time is still spent on backoff when used with less popular contents. this highlights the importance of the number of simultaneous peers on the overall performance, which is amplified by our model when considering protection against larger attackers, given that peers need to wait for other peers to join before being able to complete the download (temporary unavailability of peers)."
"computational grids are parallel in nature. remote users can connect to the grid systems and independently submit tasks or applications to the system which should be scheduled for execution in grid nodes. it is in this context where arises the independent task scheduling, in which there are no dependencies among the tasks. within this context, depending on user requirements one can consider the immediate mode (tasks or applications are considered for allocation as soon as they enter the system) or batch mode (tasks or applications are grouped into batches and scheduled). the later case of batch mode frequently arises for the case of periodic submissions."
"we may use terminology unfamiliar to some readers. rather than explaining all terms in the text, we summarize the most important ones in table 1 [cit] ). although we provide examples written for users who want to carry out within-subject classification, most of these examples also hold for between-subject classification. curse of dimensionality within machine learning the fact that classifier performance, i.e., the predictive power of a classifier drops when the number of features (e.g., voxels) becomes much larger than the number of samples (e.g., brain images)"
"according to akamai's state of the internet q3 2016 report [cit], the global average peak connection speed, which is considered to be more representative of the internet connection capacity [cit], is 37.2 mb/s. therefore, we consider a star network topology with a central node mimicking an isp, and with homogeneous leaf nodes connecting to it through asymmetric links: 30 mbit/s downlink, 3 mbit/s uplink, and 1 ms latency (2 ms delay between peers) to avoid any latency issues."
owned data chunk missing data chunk a seeder has all data chunks and just shares them; a leecher is a peer still downloading missing data chunks and sharing the data chunks it has already downloaded.
"tdt allows a number of different settings: either all data are scaled in advance (in tdt: \"all\"), which is only valid when scaling carries no information about class membership that influences test data, or scaling is carried out on training data only and these estimated scaling parameters are then applied to the test data (in tdt: \"across\"). the typically used scaling methods which have also been implemented in tdt are min0-max1 scaling or z-transformation. min-max scaling scales all data to a range of 0 and 1, while z-transformation transforms data by removing the mean and dividing by the standard deviation. in addition to scaling data to a specified range, cut-off values can be provided for outlier reduction [cit] . with this setting, all values larger than the upper cut-off are reduced to this limit, and all values smaller than the lower cut-off are set to this value. in tdt, these approaches can be combined with outlier reduction."
"the modular structure and use of encapsulation for different subroutines make the toolbox relatively easy to understand. at the same time, this structure is less error prone and reduces programming effort from the side of developers. for users, the existence of decoding design matrices (see below) and the possibility of extensive logging make the internal processes of the toolbox more transparent, which also reduces the probability of mistakes."
"the extent of user liability and the strength of plausible deniability depend on the main motivation to use the system, and on the type of contents that are distributed by the p2p file sharing system: if the system is mostly used to distribute illegal content, it is less plausible that the user intended to share legit contents when using it. therefore, it is important to endow the content interest disguise scheme with the means to distinguish legit from illegal contents in order to ensure that the main motivation to use the system comes mostly, if not completely, from legit content sharing."
"the core of the toolbox is training and testing the classifier. a classifier is typically first built from data where class membership is known (\"train\" classifier); then, the ability of the classifier to generalize to unseen data is evaluated (\"test\" or \"validate\" classifier). this separation is important to limit the impact of overfitting the classifier to the data and to have an unbiased estimate of the classifier's generalization performance. the training and test cycle is hardcoded to make sure that training and test data are truly kept separate in each cycle (unless users create their own extension to overcome this separation, if required). tdt is equipped with a number of external classifiers which can be selected by setting the cfg variable. these classifiers belong to the packages libsvm [cit] and liblinear [cit] and include l1-and l2-norm regularized support vector machines and logistic regression. in addition, a correlation classifier [cit] ) has been implemented as part of the toolbox. multiple classifiers have been compared across different data sets, with variable results [cit] . typically, the l2-norm support vector machine (svm) performs quite well, which is why it is the default classifier in tdt. the options of the classifiers can be set using the cfg variable. tdt can also be used for purposes other than classification, for example to conduct a simple correlation of two data sets. finally, at the stage of training and testing, advanced users can create an interface to completely external toolboxes that include other optimization methods (see how to extend the toolbox). all input can be passed as part of the settings in the cfg and all output can be stored for later processing. example call:"
"books have been written about feature selection [cit] which is a method in machine learning that refers to the reduction of the dimensionality of data, with the aim of finding the most relevant features and for improving classification performance. feature selection has been classified into three general categories: filters, wrappers, and embedded methods [cit] . filters are methods that use univariate or multivariate statistics based on the data to rank features and select them based on their rank. examples are the f-statistic, the weights of a classifier, or in brain imaging external masks that provide a ranking, such as external functional localizer images. wrappers are methods that iteratively include or exclude features, based on some optimization criterion specific to the feature selection method. examples are sequential forward selection or sequential backward elimination. finally, embedded methods are methods where the selection of features becomes part of the classification problem. examples are lasso [cit] or l1-regularized support vector machines [cit] where unimportant voxels receive a weight of zero, which eliminates their contribution. recursive feature elimination is another popular method and is sometimes referred to as a wrapper method, although the final feature set and classification can depend on the previous steps. for that reason-and to better distinguish it from other sequential methods-it can also be referred to as an embedded method. in tdt, feature selection has been implemented with a number of filter methods and with the embedded method of recursive feature elimination. implemented filter methods are the f-statistic, the mann-whitney u-statistic, classifier weights, and external masks, with the additional option of supplying a separate mask for each cross-validation fold. for determining the optimal feature set, nested cross-validation can be performed. it should be noted, however, that similar to parameter selection the utility of feature selection is limited when the number of samples is very small. in that case, nested cross-validation can optimize the number of features to the idiosyncrasies of the data, and it might be better not to perform feature selection at all. in addition, feature selection can be computationally highly expensive and can thus dramatically slow down classification, in particular when recursive feature elimination is used with nested cross-validation and a large number of steps. as has been mentioned for parameter selection, researchers should be cautious not to try out a number of different feature selection methods and choose one that produces the \"best\" final result. often researchers might be interested in implementing multiple feature selection steps. in the current version of tdt, up to two feature selection steps are possible. more sophisticated feature selection methods can be added to tdt, e.g., by including them directly into custom classification routines."
"a denial-of-service attack aims either at shutting down the entire system or at making one or more contents unavailable. it may target trackers to prevent peers from knowing each other and therefore disable content sharing, or target seeders to prevent new blocks from being introduced to harden or even preclude content download."
the simulator is run 30 times for each scenario and computational results for makespan and flowtime are averaged. standard deviation (at 95% confidence inter- parameter value evolution steps 20 * nb tasks population size 4 * (log 2 (nb tasks) − 1) intermediate population size (population size)/3 cross probability 1.0 mutation probability 0.4 table 4 parameter values of ts.
"peers avoid advertising what they download by requesting from other peers random chunks, thus defeating passive attacks of any size; the block download process and the messages exchanged are described in section 6.2 . attackers have then to engage in the content sharing because the chunks owned or missing are only implicitly disclosed to other peers while sharing. active attacks, of a size up to c, are defeated by constraining the amount of chunks disclosed to any set of c peers to be, at most, m : an attacker is not able to distinguish cover contents from genuine ones because at least m chunks are downloaded of each content. as so, user content interests are deterministically hidden. for legitimate users, legal liability is prevented by not requiring peers to relay traffic on behalf of other peers, by never fully downloading cover contents (their data is never accessible), and by avoiding both proof of access and proof of download for any attack of a size up to c . the protection of user content interests and the user liability are discussed in more detail in section 7, which also describes the countermeasures employed against common attacks."
"we have used the hypersim-g simulator (see fig. 4 ), a grid simulator, to evaluate our ga-ts hybrid algorithm. hypersim-g extends hyper-sim simulation package, an open source, general-purpose discrete event simulation library developed in c++."
"due to the modular structure and the simplicity of the code, users can extend tdt with little effort to incorporate other methods as well: other software packages can be used instead of spm, new classifiers can be introduced, other means of feature selection can be applied, and even complete toolboxes-e.g., other matlab decoding toolboxes-can be interfaced to run in the tdt framework."
"for a general decoding analysis, let us for the moment treat tdt as a black box ( figure 1a ). the user feeds in a set of brain images that he wants to classify and receives a single classification accuracy or an information map as output. what is needed in addition is the configuration variable cfg which carries all information necessary to conduct a specific decoding analysis. most parameters in cfg are defaults that are set automatically; parameters are only set manually when they should be changed. in addition, the spm-interface can automatically extract the decoding-relevant information from the spm.mat file which is generated as part of a standard general linear model (glm) in spm. this information is then automatically added to cfg. this interface can be used if users classify not individual brain images, but use parameter estimates of single trials or runs generated by spm (e.g., beta maps, see table 1 ). assuming that a user created the spm.mat file with unique names for all regressors (e.g., \"left\" and \"right\" for regressors related to button presses) and would like to carry out a \"standard\" leave-one-run-out cross validation scheme with two categories, a complete decoding analysis in tdt can be executed in only one line of code (we will explain the meaning of this below). in short, the example call"
"in tdt, at the decoding-level both binomial testing and permutation testing have been implemented. however, binomial testing should only be used when training and test data are not reused, i.e., when no cross-validation and no bootstrapping is performed. a permutation test is conducted in three steps: first, the same decoding design is set up using a permutation scheme where data from different chunks is kept separate. second, the permutations are conducted which can be run in parallel to speed-up processing. third, the permutation test is conducted. results can be reported in matlab or written to brain images. for the second-level, users are advised to use dedicated third-party software such as spss, r, or matlab which is specialized for statistical analyses and allows testing for basic assumptions of the tests. for users who do not want to export their results to other packages and are confident that the assumptions of their tests are met, tdt offers a set of basic functions that can conduct classical t-tests and f-tests."
"tdt can be downloaded from http://www.bccn-berlin.de/tdt. the toolbox code is open source, but is licensed as copyright software under the terms of the gnu general public license (free software foundation). in addition, the toolbox comes with thirdparty software (libsvm, liblinear, newton-svm, http:// research.cs.wisc.edu/dmi/svm/nsvm/), each with their respective copyright. tdt has been tested under windows, linux, and os x and works both on 32 and 64 bit systems. [cit] b or above and requires no additional matlab toolboxes. it works \"out-of-the-box\" with an installed version of spm2, spm5, spm8, or spm12b. in addition, we provide an example data set that can be downloaded from the toolbox website."
the attacker creates a large number of pseudonymous identities (sybil) or multiple malicious peers may coordinate their efforts (collusion) to increase the amount of blocks disclosed by other peers to either prove content download or determine user content interests. the size of the attack is defined by the number of unique peers (not known to be related).
"for the evaluation of the ga-ts hybrid algorithm we used the hypersim-g grid simulator . we considered three grid scenarios: small, medium and large sizes with 32 hosts / 521 machines, 64 hosts / 1024 machines,"
"tdt offers a user-friendly, yet powerful and flexible framework for the multivariate analysis of brain imaging data. the toolbox has many advantages in terms of structure, transparency, speed and error management. it comes with an interface to the common brain data analysis software spm which should make it particularly easy to apply to existing data. beginners can start using the toolbox with one single line of code, which increasingly can be extended to exploit the full functionality of tdt. in addition, the toolbox can easily be extended for more general purpose use which allows adding new classifiers, feature selection methods, or even complete external software packages for multivariate data analysis. we hope that tdt-through its simplicity and flexibility-will encourage a much broader application of machine learning methods in the analysis of functional imaging data."
"the legal and ethical framework described the main legal and ethical challenges brought by the advances in computer technology, focusing on the legal and privacy dimensions of p2p file sharing. the multitude and complexity of copyright laws across the globe make it impossible to define clear boundaries regarding user liability. still, it is our belief that the user will not be subject to any legal liability for, unknowingly and unwillingly, downloading an illegal content if the main motivation to use a p2p system is to share legit contents, and it cannot be proven that the user had access to the content data. thus, any privacy-preserving p2p system should ensure that the main motivation for its adoption is to share legit contents in order to minimize the extent of potential user liability."
"a statistical model that incorporates analysis of variance, linear regression, and related parametric tests into a common framework. in brain imaging, the glm is commonly used to explain each voxel's time series separately with multiple linear regressors each representing conditions of interest or nuisance variables [cit] . the term mass-univariate refers to the fact that the glm is calculated for each voxel individually hyperplane a plane in more than 3d. typically, the term separating hyperplane is used which separates the space spanned by different features (e.g., voxels) in two subspaces and defines the decision boundary of the classifier. each part of the voxel space is in this way assigned to one of two classes"
"in order to ensure that the peer arrival rates are realistic, we gathered the peer arrival traces of several contents and then we used an exponential function to generate the peer inter-arrival times and change the mean arrival rate every 10 min (nonhomogeneous poisson process). the traces were gathered by monitoring a widely used tracker ( open.demonii.com ), and provide the number of first time peer arrivals over 10 min intervals since content publication up to 21 days. we did not use the gathered peer arrival traces directly because trackers, per request, provide a list of, at most, 200 peers currently in a swarm but not the time instant of their arrival. therefore, we sent a request to the tracker every 400 milliseconds and considered that a peer has arrived for the first time at the time interval it got firstly listed."
"the multitude and complexity of copyright laws across the globe make it impossible to define clear boundaries regarding user legal liability. still, the user should not be held legally liable for, unknowingly and unwillingly, downloading an illegal content if the main motivation to use a p2p system is to share legit contents, and it cannot be proven that the user had access to the content data."
"the description provided for each privacy-enhancing p2p system focus on the trust requirements, on the protection against both passive and active attacks aiming at identifying user content interests, and on the potential legal liability of users."
"the evaluation of our model was conducted through simulation, and, given that simulations are only as good as their models, they were carried out using the ns-3 discrete-event network simulator [cit], which provides realistic models of the network stack and its protocols. still, the simulation of large-scale p2p networks using accurate models generates a very large number of events so the time required to run simulations is large."
"methods that reduce the number of features (e.g., voxels). in our terminology, selecting regions of interest or running a searchlight analysis are not parts of feature selection. in addition, we treat methods separately that reduce the dimensionality, but give up the voxel-to-voxel mapping (e.g., pca). within tdt, we refer to methods that change the voxel-to-voxel mapping as feature transformation methods"
"pollution attacks are thwarted by employing asymmetric cryptography to sign checksums of each block in order to verify its integrity. the publisher may, e.g., distribute the public key and the checksums along with content description or metadata."
"in sum, the mistrustful sharing building block reinforces the content interest disguise because the distinction between genuine and cover contents is hardened by not disclosing what peers download or miss, and a larger set of cover contents can be used as they do not need to be fully downloaded. it prevents legitimate users from being held liable due to cover content and misleading content downloads. cover contents are never fully downloaded to guarantee that the user has never access to their content data. misleading contents may be fully downloaded, but there is no proof of access or proof of download for any attack of a size up to c ."
"finally, for completeness we would like to mention additional mvpa software packages for fmri data: searchmight (http://www.princeton.edu/∼fpereira/searchmight/) which is a dedicated and fast software package for simple searchlight analyses with different classifiers 3, 3dsvm (http://afni.nimh. nih.gov/pub/dist/doc/program_help/3dsvm.html) which is part of the software package afni; probid (http://www.kcl.ac. uk/iop/depts/neuroimaging/research/imaginganalysis/software/ probid.aspx) which is specialized for group comparisons, mania [cit], and the brain decoder toolbox (http://www.cns.atr.jp/dni/download/brain-decoder-toolbox/)."
"processing tasks that arrive to grid systems in batch mode is one of the most common scenarios in grid systems. it arises due to the independent submissions of tasks by many user geographically distributed. the problem also arises in massive processing where tasks spawned by an application do not have dependencies among them. the batch grid scheduling essentially includes the phases below, and are shown in fig. 1. 1. gather the information on available resources (machine pool) 2. gather the information on pending jobs (job pool) 3. make a batch and compute a planning for that batch 4. allocate jobs 5. monitor job execution (failed jobs are re-scheduled again, entering pool job)."
"we have implemented the steady state version of gas for the purpose of this work. in steady state gas, a few good individuals of population are selected and crossed. then, the worst individuals of the population are replaced by the newly generated descendants; the rest of the individuals of the population survive and pass to the next generation (see alg. 1)."
"we are thus considering the case of hybridization of two meta-heuristics running in sequential environment. we have considered a low level hybridization and the coercive control strategy. roughly, our hybrid algorithm runs the ga as the main algorithm and calls ts to improve individuals of the population."
"in the following we will describe tdt in more detail. we will use the style of a tutorial which is intended to ease readability of this section. we will introduce tdt at three different levels of complexity, starting with a rough description of the toolbox to provide an initial overview of the basic structure of tdt. this section should be sufficient for users just getting started with decoding, or those who would like to conduct a simple generic decoding analysis. second, we give a more detailed explanation of how to expand on this standard decoding analysis for users at an intermediate level who want to tailor the analysis to their needs. third, for advanced users we expand on more detailed optional settings of the toolbox, including parameter selection or feature selection. after these three steps, we will briefly demonstrate how users who want to exploit the full capabilities of the toolbox can extend the toolbox to new classifiers, new feature selection methods, or even complete machine learning packages."
"being a participant of the system, we consider that an attacker is able to engage in the content sharing as a commoner or as a seeder, to be a tracker, and to publish contents with misleading description. also, we consider that an attacker may coordinate a large number of peers that collude with each other (collusion attack) or assume multiple pseudonymous identities (sybil attack), which is equivalent to a larger colluding group."
"in the disguise use cases, we considered a 50% increase of the peer arrival rate due to cover downloads and that those additional peers only download and share 50% of the blocks required to complete the download. as for all other peers, they leave immediately once they download the content. the results obtained indicate that cover downloads improve the performance of our model on all use cases. still, this is just an estimation that highly depends on the scheme being used to select cover contents and how much of each to download in order to disguise user content interests."
"tdt has been developed from an spm perspective and works with current spm versions (downwards compatible to spm2), but can also be used outside of spm. tdt can directly read all decoding-relevant information from spm design matrices and in that way simplifies the process of setting up standard decoding analyses. for example, a complete leave-one-run-out cross-validation decoding analysis can be executed using one line of code, by providing the path to the analysis and the names of the regressors that should be classified (see below)."
"the block selection mechanism is used by commoners to determine which block is to be offered to a requesting peer. it plays an important role on how the blocks end up distributed across the network, affecting the probability of peers obtaining useful blocks. this mechanism ensures that no uploaded block is offered twice to the same peer, and determines when requests should be refused. the request refusal may be due to the lack of useful blocks to offer, due to resource or privacy constraints, or due to content interest disguise strategies."
"laws are formally adopted rules that mandate or prohibit a certain behavior, created by the members of a society to balance the individual rights to self-determination against the needs of the society as a whole, and, ideally, are drawn from ethics, which define what is considered right or wrong, i.e., the socially acceptable behaviors. the key difference between laws and ethics is that the former carry the authority of a governing body, usually a nation [cit] . in turn, ethics are based on cultural mores, beliefs, values and principles, which reflect the unique existential experiences that we accumulate as individuals as well as societies and, supported on institutions, provide long-term stable rules that are made obvious. thus, these rules can be seen as refractions of the common world awareness that give rise to different experiences and interpretations: multicultural ethics [cit] ."
"the mistrustful p2p file sharing model hides user content interests through content interest disguise in order to provide plausible deniability to the user. it has no trust requirements to enable content sharing in large groups of untrusted peers, and prevents user liability in case of legitimate usage while enabling timely content downloads. the provided protection enables the user to configure, per content, the required trade-off between privacy and performance by setting the size c of the largest colluding group to be protected against, and the minimum amount m of blocks that need to be downloaded in order to ensure that genuine and cover contents are indistinguishable to any attacker of a size up to c . we discussed mistrustful p2p model's legal and ethical framework, demonstrated its feasibility for more use cases, provided a security analysis, compared it against a traditional p2p file sharing model, and improved its main mechanisms."
"simulation 2 consisted of two whole-brain analyses that were run on the same simulated data set used for simulation 1. in simulation 2a, we used all data to generate a weight map, i.e., we did not separate data in training and test sets. a weight map indicates the contribution of each voxel to the classification which reflects a combination of signal enhancement and noise suppression. as can be seen in figure 4b (left panel), the results are similar to the difference in means of the original data, because noise was spatially uncorrelated. simulation 2b used the same leave-one-run-out cross-validation scheme as in the searchlight analysis to achieve one classification accuracy for the entire brain. the analysis consisted of an additional nested cross-validation where we conducted recursive feature elimination to identify the set of voxels that is optimally suited for carrying out the classification task. classification performance was at ceiling (100 % accuracy). figure 4b (right panel) shows how often each voxel is chosen in any of the six cross-validation steps for recursive feature elimination, again clearly favoring signal voxels over noise voxels."
"although the most basic one-line-of-code use of the toolbox already allows running a multitude of analyses with practically no programming skills, users may want to adjust a number of settings. in this section, we explain the major steps that are required in each decoding analysis ( figure 1b), and in the next section optional steps are explained. for each of these required steps there are default settings in tdt, but the user may wish to adjust them. in general, each user creates a short script which contains (1) all the settings of the cfg variable and (2) this example automatically extracts regressor names from the spm model, then accumulates all relevant information from the design related to the regressors of interest (\"left\" and \"right\"), creates a leave-one-run-out cross-validation design, and executes the decoding analysis using default settings. important basic settings that otherwise use these default settings are explained in the next four paragraphs. core i7-2820qm"
"the most typical output for a decoding analysis is the mean cross-validated accuracy value. however, a number of other types of output are quite common and potentially useful. for example, it can be of interest to look at the classification accuracy for each class separately to assess classifier bias. a classification accuracy of 75% should be interpreted very differently when either class is decoded with 75% accuracy, or when one is decoded with 100% accuracy and the other with 50% (leading to 75% on average for a balanced number of samples in each class). in case of differently sized classes, balanced accuracy or d-prime can give indices of performance that take into account the different size of the groups. additionally, the area under the receiveroperator-characteristic curve (auc) which uses the distance of a classification output to the decision boundary can provide results about the information content using a graded rather than a binary response."
"the content download process of the mistrustful p2p model, depicted in fig. 5, consists in the following steps: (1) the peer registers at the tracker to join the swarms of both genuine and cover contents in order to disguise the content interests of the user, without disclosing its role; (2) it requests a list of peers (a subset) in the swarm from the tracker, which is not aware of each peer's role; (3) it selects peers from that list that cope with the user privacy requirements (eligible peers); (4) it requests random blocks from those peers in order to complete its download; (5) the erasure coding mechanism is used by a seeder to generate new blocks, and by a commoner to retrieve the content data after fully downloading a content. the disclosure constraint mechanism determines if a block request can be sent to a peer or accepted by a given commoner. the block selection mechanism determines which block is to be shared, if any; the same block is never offered twice to the same peer. the peer selection mechanism selects an eligible peer to which a block request can be sent. the request backoff mechanism defines the delay between block requests."
"during the past three decades, meta-heuristics have been among most studied approaches to efficiently solve combinatorial optimization problems. families of meta-heuristics such as local search methods, population-based methods and biologically inspired methods were developed for most computationally hard problems. more recently, attention has been shifted to the design and implementation of high level algorithms that combine heuristics methods. these algorithms, known as hybrid algorithms, or hybrid meta-heuristics for the case of meta-heuristics being hybridized, aim to explore the existing synergies among stand-alone heuristics methods in order to achieve even more efficient and robust algorithms. based on this premise, many optimization frameworks have been proposed in the field."
"let us consider for example a genetic algorithm. how to hybridize with local search methods? we could use the ga as a main algorithm and call local search methods along the flow. where to hybridize? local search methods can be used at different places along the ga flow: to generate some of the individuals of the first population and thus introduce more diversity among individuals, to implement some genetic operators (e.g. mutation), improving offsprings by local search, etc."
"to add new classifiers, two functions need to be provided, one for training and one for testing the classifier. if the new classifier should be called \"newclassifier,\" the training method needs to be saved as newclassifier_train.m and is called by"
"we would also like to mention a toolbox specifically created for carrying out representational similarity analysis [cit] which can serve as a standard for this type of analysis. the capabilities of tdt for representational similarity analysis at the current stage are still much more basic than those of the rsa toolbox. however, tdt is in our experience faster than the rsa toolbox, both for searchlight analyses and for creating correlation matrices. users who would like to benefit from the speed of tdt, but use all functionalities of the rsa toolbox might consider using both toolboxes. alternatively, appropriate extensions can be made (see how to extend the toolbox) to include many functionalities of the rsa toolbox. we are planning to include more representational similarity analysis methods in future versions of tdt."
"we assume that an attacker might be any entity that participates in the system -a publisher, a tracker, a regular peer, or a group of colluding peers. external entities monitoring all traffic of a peer, such as isps or governments, are out of the scope of this work."
"one main issue in designing hybrid algorithms is the high degree of arbitrariness, that is, the many ways one can choose to combine different resolution methods. said in other words, the questions are how to hybridize, where to hybridize? the how refers to the way we \"modify\"/\"combine\" some parts of a meta-heuristics by using other meta-heuristics resulting in a new control flow. the where refers to the fact that hybridization can take place at different phases of the meta-heuristics, starting with the computation of the initial solutions up to the modification of some procedures of the original heuristic method by using procedures of other heuristic methods. for instance, such procedures are neighborhood exploration or genetic operators."
"this section is structured as follows. section 6.1 provides an overview of our model and its main building blocks -content interest disguise and mistrustful sharing -to best describe how the problem we aim to solve is addressed. section 6.2 discusses the peer roles and their sharing behavior, the content sharing process, and the role of mistrustful sharing on it. the attack model considered is defined in section 6.3 . sections 6.4 through 6.8 characterize the instantiations of each one of the mechanisms used on the evaluation of our model, whose main focus is on its feasibility rather than on its overall performance."
"in simulation 3, we ran a region-of-interest (roi) analysis with two rois, where one roi covered one third of the signal region while the other covered the remaining two thirds ( figure 4c, left panel) . again, we used a leave-one-run out crossvalidation scheme. we then continuously varied the amount of signal that was added to the noise. as can be seen in figure 4c (right panel), the decoding accuracy gradually decreased with decreasing snr until it reached chance-level when no signal is present. in addition, the accuracy in the smaller roi was generally lower than that in the larger roi, reflecting the reduced amount of signal present in that region."
"in tdt, parameter selection is currently implemented as grid search, where all parameter combinations are combined. depending on the problem, there might be smarter optimization approaches, but in our experience grid search works well for optimizing few parameters, in particular when only few samples are available. more advanced users can also create their own functions to tailor parameter selection to their specific problem."
"several privacy-enhancing p2p systems have been proposed in the literature for a wide range of applications, providing different degrees of privacy to users and employing various techniques, the majority of which provides privacy preservation through anonymity or through plausible deniability. freenet [cit] and tor [cit] are probably the most prominent anonymity solutions for, respectively, anonymous content distribution networks and lowlatency anonymity. despite their merits, anonymity systems tend to introduce more overhead, and their users may be held indirectly liable. for instance, tor defaults to a path length of four (300% network overhead), which lowers the throughput and increases the average latency [cit], and a tor relay node (core router) may relay traffic of misbehaving peers, which makes the last one on the path (exit node) to appear to be the originator of such traffic. systems such as oneswarm [cit], which rely on trust links to provide privacy, are not considered given that they are not suitable for untrusted p2p networks. herein, we depict the privacy-enhancing p2p systems providing plausible deniability and designed specifically for p2p file sharing."
"for the first three categories we define 24 use cases. for the last category we define the same use cases except those for collusion size variants, totaling 12 use cases. we consider the content size to be either 100 mib or 800 mib, the number of seeders to be either 1 or 64, and three video traces to compare different degrees of popularity: a more popular (mp), a popular (p), and a less popular (lp) contents. over the first 48 h, the total number of peer arrivals for mp, p, and lp traces are respectively 7580 0, 2270 0, and 340 0 (approximately). except for the last category, collusion size is either 1 or 31. table 3 summarizes the use cases and their categories."
"as discussed in section 6.3, both sybil and collusion attacks of a size up to c are thwarted by not advertising what peers download or miss (avoiding block advertisement), and by employing the disclosure constraint mechanism. avoiding block advertisement forces attackers to engage in the content sharing in order to gather information about the blocks owned by peers, increasing the resources required to launch such attacks. the disclosure constraint mechanism limits the amount of information that a single colluding group can gather. this protection may be extended by enabling the user to treat multiple peers using the same public ip address, such as those behind nat, or related public ip addresses, such as those of a single organization, as pseudonymous identities of a single entity."
"messages, but which may severely compromise results. for that reason, we wanted to make tdt particularly easy to use, where all important settings are made at the beginning. we believe that minimal programming ability is necessary, with the only requirement being that a user is able to modify existing matlab scripts that define the settings for the decoding analysis. this simplicity also allows users to see at a glance what decoding analysis they are performing."
"commoner never discloses the reason behind refusal. fig. 3 summarizes the peer roles and their sharing behavior. the block download process on the mistrustful p2p model differs considerably from the one on other p2p file sharing systems, given that peers do not advertise what they download. this process is summarized in fig. 4, where peer a is the requester and peer b is the provider, and works as follows. peer a requests a random block from peer b, i.e., without providing the id of an intended block. peer b then either refuses the request by simply ignoring it, and thus not disclosing the reason behind refusal, or replies with the id of the block it is willing to share, which must be different from any other that may have been previously disclosed between them (both as requester and as provider). if peer b has offered a block, peer a sends a reply message either accepting the offered block, in order to start its download, or canceling the block request, in order to avoid unnecessary network overhead. unless the block request has been canceled, peer b sends the block it has offered to peer a. the possible outcomes of a block request are further discussed in section 6.6 ."
"human neuroscientists are interested in understanding the function of the human brain and nervous system. for that purpose, they have developed numerous methods that directly or indirectly measure the activity of the nervous system at work. one of these methods is functional magnetic resonance imaging (fmri) which measures brain activity indirectly through the blood oxygen level dependent (bold) response [cit] . conventionally, the focus of fmri has been to perform massunivariate analyses, i.e., to analyze the recorded data time courses of each fmri brain voxel (for meg/eeg each sensor/electrode) separately, for example with the general linear model (glm, e.g., [cit] ) ."
"this section provides a security analysis of the mistrustful p2p model. we start by describing the common attacks in p2p systems, and present the countermeasures employed. then, we discuss the identification of user content interests, the proof of full content download, and user legal liability."
"the developers of the princeton mvpa toolbox seem to have shifted their focus to pymvpa [cit],b), a highly versatile programming environment that allows for a large variety of decoding analyses. the key advantages of this toolbox are the high level of sophistication, the active user community and the fact that the toolbox does not require third-party software. while tdt requires the third-party software matlab and might not be as versatile in its core functions, pymvpa also requires more advanced programming skills and for searchlight analyses is much slower than tdt. in addition, matlab is still widespread in the neuroimaging community, and running decoding analyses on spm results from python is not straightforward. finally, tdt can also be easily extended if users need additional functionality, which also requires only little programming skills (see example 3:"
"a legitimate usage of a p2p system based on our model may only result in the download of an illegal content either due to inadvertently considering an illegal cover content as legal or due to a misleading description. for the first case, given that the user is never granted access to the content data because cover contents are never fully downloaded, the user is not subject to any legal liability. for the second case, as long as actual attacker is not underestimated, the user is also not subject to legal liability because it cannot be proven that the user had access to the content data. even if a misleading content download can be proven by an underestimated attacker, it is our belief that, as long as the percentage of such misleading contents remains low, it is plausible that the user may have been driven to unknowingly and unwillingly download that content."
"in the context of this work, we define privacy preservation as the concealment of user content interests. we aim at developing a privacy-preserving p2p file sharing model that:"
"the scope of action of the mistrustful sharing mechanisms entails mostly steps (3), (4) and (5) . their role on the content sharing is illustrated in fig. 6 . the erasure coding mechanism enables a commoner to retrieve the content data after fully downloading the coded content, and enables a seeder to generate a new block for each incoming request. the disclosure constraint mechanism determines if a block request can be sent to a peer or accepted by a given commoner. the block selection mechanism is used by the contacted commoner to determine which block is to be offered to the requesting peer, if any. the request backoff mechanism determines the delay between block requests of a commoner. the peer selection mechanism selects the peer to which the block request is sent."
"a few other toolboxes have been created that can be used to carry out multivariate decoding analyses on fmri data. the key advantages of tdt have been listed in the introduction. in general, it is difficult to tell to which degree other toolboxes offer similar advantages, for example where they might be comparably transparent, fast, easy to use, or versatile. more objective measures that have been used for comparison purposes have partly underestimated the ability of other toolboxes [cit] because the toolboxes might be more elaborate than described on the toolbox websites. rather than thoroughly describing the advantages and disadvantages of all existing toolboxes, we will mention the most widely used and in our view most promising alternative toolboxes and try to elucidate some degree of comparison to tdt. the princeton mvpa toolbox (http://code.google.com/p/ princeton-mvpa-toolbox/) is a rather versatile tool with a number of classifiers, basic feature selection capabilities and scaling and the possibility to run searchlight analyses, but it requires a more advanced level of programming skills and to our knowledge is not further developed. another advantage of this toolbox is an active user community. tdt on the other hand offers more analysis methods, can be easily extended and due to the multilevel approach is probably easier for getting started. the spm interface of tdt is particularly well suited for users of spm."
"the instantiation provided for each mechanism, described in sections 6.4 through 6.8, is the one used to evaluate our model in section 8 . due to the large number of variables and factors at play, we consider the impact of those that are expected to change more often -c and m, content size, peer arrival rate, and number of seeders -, and of cover downloads on the average download bitrate and on the download completion ratio. for the sake of clarity and tractability, other factors and variables such as intercontent relations, incentives to share, parallel chunk requests, and internet connection heterogeneity are not considered. the evaluation aims at demonstrating the feasibility of our model rather than at optimizing its overall performance, i.e., it aims at demonstrating that peers are able to timely download contents without advertising what they download."
"peers, per content, can take one of two roles depending on their privacy requirements and the way they contribute to the file sharing: seeder -peer having a content that wants to share, and willing to forgo its privacy -, or commoner -peer willing to participate in the content sharing if its privacy requirements can be met. a seeder may be the author or a party interested in publishing a content, and therefore does not require the concealment of its content interests. it generates a unique block (erasure coded chunk) for each request it receives, and only refuses to serve block requests if it has no resources available. on the other hand, a commoner does not generate new blocks, only shares them if its interests remain hidden, and only has access to the content data after fully downloading the content. a commoner keeps track of the blocks it shares with other peers both for privacy protection and to avoid offering a block twice to the same peer. it may refuse to serve block requests if it has no useful blocks to offer, due to resource or privacy constraints, or due to content interest disguise strategies. the"
"dealing with dynamic nature of grid systems. one important requirements in grid systems is to address the dynamics of the systems, such as machine failure, task failure, etc., due to which, depending on scheduling policy, tasks must be migrated to other nodes on the system or should be re-scheduled. batch scheduling has the limitation that tasks are not scheduled for processing as soon as they enter the system. however, we cope with the dynamics of grid system by using batch processing in very short intervals of time. that is, we keep the time among two successive batches very small (e.g. less than 100 seconds) so that changes in the system are unlikely to happen in such short time. on the other hand, using the batch processing has the advantage that an optimized planning of tasks to machines can be computed as compared to immediate mode processing."
"decoding step an iteration of a decoding analysis which is part of the cycle of evaluating the classifier. when cross-validation is used, decoding step refers to a cross-validation fold"
"will perform a cross-validated leave-one-run-out searchlight decoding analysis between the regressors \"left\" and \"right,\" where beta_dir is the path where the spm glm results are stored, output_dir is the folder where the decoding results will be saved, and 4 is the radius of a spherical searchlight in voxels (see next paragraph for a detailed explanation). this analysis will yield a map of accuracies that can be inspected with spm or other thirdparty software and which should uncover brain regions involved in encoding the motor response."
"the multitude and complexity of copyright laws across the globe make it impossible to define clear boundaries regarding user liability. still, it is our belief that the users of p2p systems will not be held legally liable for, unknowingly and unwillingly, downloading an illegal content (direct liability) or contribute to its download (indirect liability) if the main motivation to use the p2p system is to share legit contents, and it cannot be proven that the user had access, either in part or fully, to the content data. the user does not benefit directly from the download of an illegal content that he has no access to, and is also unable to determine that the content is unexpectedly illegal before having access to its data. therefore, it is plausible that he either has been mislead into downloading it or had no intention in assisting a misbehaving peer infringing copyright law."
"the two main goals of the evaluation are to show that peers are able to timely download contents without advertising what they download, and to estimate the impact of privacy preservation on the average download bitrate; it is not our goal to optimize the overall performance of our model. to do so, we simulated the content sharing to evaluate the ratio of peers that are able to complete their downloads and the average download bitrate. the results obtained were compared against those of a traditional p2p file sharing model. as referred in section 4, a content is considered to have been timely downloaded if the average download bitrate is within the same order of magnitude of traditional p2p file sharing models."
"traditional p2p file sharing systems focus on performance and scalability, disregarding any privacy and legal issues that may arise from their use. in this section we provide an overview of these systems, and present their main privacy and legal issues. we use bittorrent as an example given that it is the most prominent of such systems. we start by providing a brief definition of common bittorrent terminology -hash, content, chunk, torrent file, peer, seeder, leecher, swarm, and tracker -, describe its content sharing process, and end presenting the main privacy and legal issues."
"content interest disguise, as in other privacy-preserving p2p systems, hides user content interests through plausible deniability. the user content interests are hidden by downloading both contents that the user is interested in (genuine) and additional contents of no interest to the user (cover), as long as they cannot be distinguished. registering at a tracker and joining a swarm no longer represents interest in that content, and user content interests can no longer be identified by monitoring just a small fraction of the network [cit] . a content interest disguise scheme selects the set of cover contents, how much of each one to download, and may impose constraints to the content sharing in order to hide user content interests. the evaluation of the mistrustful p2p model is centered on the feasibility of its novelty, the mistrustful sharing building block, and thereby the proposal of a content interest disguise scheme is out of the scope of this work."
"ated such that the block requests sent to the same peer are, on average, 50 ms apart and such that, on average, m / c block requests can be sent to each peer during content download. m / c represents the configured protection and k τ the minimum time required to download a given content. for the swarm component, we consid-"
"the toolbox can be interfaced with complete external machine learning packages, for example, the powerful shogun toolbox [cit] . this might be useful for users who want to use the general workflow of tdt, e.g., the data handling, cross-validation facilities and searchlight routines, but do not wish to implement a large number of external algorithms for use in tdt. the same general procedure can be applied to use highly sophisticated processing streams that do not directly fit into the tdt framework. there are two approaches for creating an interface to an external toolbox. the first takes place at the level of the classifier. essentially, an interface is created where all steps of the decoding analysis-if requested, even including cross-validation-are carried out outside of tdt. the training function of a classifier acts as a placeholder to pass data to the testing function. the interface function would look as follows:"
"when we developed tdt, we initially used it as a tool for conducting searchlight analyses on preprocessed data in order to create individual searchlight accuracy maps that could then be used as input to group-level analyses. later, the toolbox was extended to become more general purpose, adding whole-brain and roi analyses and more and more machine-learning related utilities. our focus for tdt was on specifically creating a tool that provides users with the means to conduct multivariate decoding. most decoding analyses are carried out on preprocessed data (e.g., spatially realigned or temporally detrended data), but preprocessing is not a part of tdt, as numerous software packages exist that have been created for that purpose, each with their own benefits and drawbacks. for example, detrending data or scaling time-series are important steps for single image decoding, but this can be done with popular software packages including spm, fsl, or afni or directly in matlab with specialized toolboxes. the same applies to visualization of results: third-party software can be used for that purpose, for example mricron [cit] to visualize searchlight accuracy maps or weight maps."
"in this paper we have presented an evaluation of two hybrid algorithms, namely ga(ts) and ga-ts, for the problem of independent batch scheduling in grid systems under hierarchic and simultaneous optimization models. in the ga(ts), the flow of the hybrid algorithm was that of a genetic algorithm (ga) in which tabu search (ts) was used to make locally improvements of new individuals of the population. in ga-ts, the ga and ts were executed in a sequence-like manner, first the ga, next ts on the best output solution of ga. in both versions, the independent batch scheduling was formulated as a bi-objective optimization problem and the hierarchic optimization mode, with makespan as primary objective and flowtime as secondary objective, and simultaneous optimization using a weighted sum of the objectives as a single objective function, were considered. the proposed algorithms have been experimentally evaluated using grid simulator and the results are compared with the results achieved by both ga and ts as stand-alone heuristic schedulers. for the evaluation we have considered different sizes of the problem instance (number of tasks and number of machines in the grid system). the experimental study showed that hybrid versions performed best for the optimization of the makespan while stand alone versions of meta-heuristics performed better for the flowtime."
"the level of hybridization expresses the dependencies among the flows of the considered heuristics. high level hybrid heuristics are loosely coupled and low level hybrid heuristics are strongly coupled, as described next."
"in sum, the basic ethical imperatives are that a person should not, knowingly and willingly, cooperate in or contribute to the wrongdoing of another, and that the human intellectual creativity needs to be encouraged and stimulated in order to promote 3 in jurisprudence, a natural person is a human being, as opposed to a legally generated juridical person."
"this section describes the common attacks in p2p file sharing and the countermeasures employed, which are summarized in table 2, focusing on the context of our work. the attack model considered assumes that an attacker can be any entity participating in the system -a publisher, a tracker, a seeder or a commoner -or a group of those entities in collusion. external entities, such as isps and governments, are out of the scope of this work."
"for the first empirical analysis, we ran a roi decoding in ventral temporal cortex. our goal was to investigate the classification accuracy and the confusion of the different categories. for that purpose, we used the masks provided with the haxby data set to focus our analysis on ventral temporal cortex. the beta images were then submitted to a leave-one-run-out cross-validation scheme, using a one-vs.-one multiclass svm classification approach 2 . the output was specified to reveal a confusion matrix comparing the frequency of the predicted label with the true label. this approach was repeated for all six subjects and the resulting confusion matrices were averaged. the result is shown in figure 5a . as can be seen, each class could be classified very well, with only little confusion between the classes. the results indicate two clusters with larger confusion: images of faces and cats were confused more often than other categories, and images of inanimate objects were confused more often. this could indicate that in ventral visual cortex, faces and cats as well as different inanimate objects are processed more similarly to each other."
mization modes are considered for the bi-objective scheduling problem. the evaluation is done using different grid scenarios generated by a grid simulator. the computational results showed that the hybrid algorithms outperforms both the ga and ts for makespan parameter but not for the flowtime parameter.
forgery and repetition. these attacks try to either tamper with the data being transmitted or to retransmit authentic data previously captured in order to achieve their goals.
"at the application level, dos attacks are prevented by blacklisting attackers; at the network level, the mistrustful p2p model presents the same vulnerabilities as any other p2p file sharing system."
"peers communicate using tcp new reno with a maximum transmission unit (mtu) of 1500 bytes, maximum segment size (mss) of 1460 bytes, and with nagle's algorithm [cit] disabled. peers are provided with a list of all other peers currently in the swarm, request one block at a time, accept one request at a time, always attempt to complete the download in a single session, and leave immediately after completing the download. version 3.23 of ns-3 was used. simulations were run for the first 48 h of content sharing because the most demanding stage, flash crowd, usually ends within the first 36 h. thus, the simulation fully encompasses flash crowd stage and ends in steady-state stage."
"for the sake of clarity and tractability, peers were selected randomly in order to avoid adding an additional factor into the evaluation. non-uniform selection of peers is expected to impact on how the blocks end up distributed across the network, especially when considering heterogeneous internet connections: peers with more available bandwidth are able to replicate more rapidly the blocks they own."
"in strongly coupled hybridization the combined meta-heuristics interchange their inner procedures, resulting in a low level of hybridization. the level of hybridization expresses the degree of interaction among the meta-heuristic components in the hybrid structure. in this case, usually one of the heuristics is the main algorithm, which during its flow calls other heuristics procedures. for instance, we can run ga and then additionally to crossover and mutation, we can apply ts for improving the newly generated solutions. the notation m h 1 (m h 2 ) is used in this case to express that m h 1 is the main algorithm and m h 2 is subordinated to that (ga(ts), in our case)."
"in the last two decades, the advances in computer technology have drastically changed the media landscape. the widespread availability of digital media and broadband internet connections enabled consumers to become also producers and distributors of creative work, something that in the past was mostly limited to professional parties. peer-to-peer (p2p) networks endowed individuals with the means to easily and efficiently distribute digital media over the internet, and are extensively used for largescale file sharing due to their decentralized and scalable nature. nevertheless, as more information flows through these networks, users are becoming increasingly concerned about their privacy. the p2p architecture enables solutions that enhance privacy, such as freenet [cit], nymble [cit], and tor [cit], but user legal liability issues may be raised as it also facilitates unauthorized distribution and reproduction of copyrighted material [cit] ."
"the protection provided by the mistrustful p2p model is deterministic as long as the size of the actual attacker does not exceed c . therefore, herein we discuss the necessary conditions for an underestimated attacker of size c + δ to be able to prove content download, where δ is the difference between the actual and the configured sizes of the largest attacker."
"grid computing has emerged as a wide area distributed paradigm for solving the large-scale problems in science, engineering, etc., known as the family of escience grid-enabled applications [cit] as well for advanced services [cit] . computational grid involves the combination of many computing resources into a network for the execution of computational tasks. the resources are distributed across multiple organizations, administrative domains having their own access, usage policies and local schedulers. the tasks scheduling and the effective management of the resources in such systems are complex and therefore, demands sophisticated tools for analyzing the algorithms performances before applying them to the real systems [cit] ."
"we assume that attackers, being participants of the system, cannot forge the source ip address of packets (ip address spoofing). as so, they are not able to forge nor repeat packets because peers communicate directly. even considering ip address spoofing, as long as key exchange and distribution mechanisms are employed, which are out of the scope of this work, the integrity of packets can be ensured and the addition of a sequence number prevents repetition attacks."
"value number of iterations nb tasks * nb machines max. tabu status 1.5 * nb machnies number of repetitions before activating intensification/ 4 * ln(nb tasks) * diversification ln(nb machines) number of iterations per intensification/diversification log 2 (nb tasks) number of iterations max tabu status/2− for aspiration criteria − log 2 (max tabu status) val) is also reported. the results for makespan and flowtime are given in tables 5 and 6, for ga, ts, ga(ts) and ga-ts in both hierarchic and simultaneous optimization mode. as can be seen from table 7, for makespan value the ga(ts) algorithm (in its simultaneous version) performed better than ga, ts and ga-ts for all but large size instances. on the other hand, from table 8, we can see that ts (in its simultaneous version) performed better than all other methods for flowtime value."
"tdt comes with a number of methods which allow for executing most commonly used analysis pipelines \"out of the box\" (figure 1 ). these include feature scaling, feature transformation (e.g., principal component analysis), parameter selection, and feature selection as steps prior to decoding. the decoding analysis itself has two-class and multiclass classification implemented, using a number of often-used classifiers including support vector classifiers, logistic regression, and correlation classifiers [cit] . additionally, support vector regression is part of the toolbox. the toolbox has been tested and runs under windows, linux, and mac os."
"simulator's configuration for the dynamic case in the dynamic case the numbers of tasks and machines can vary over time according to the probability distributions specified in the simulator. more precisely, one the one hand new tasks can enter the system, and on the other, machines can leave the grid system, which provokes the tasks assigned to those machines to be re-scheduled. the configuration of simulator is presented in table 2 ."
"the request backoff mechanism determines the delay between block requests to help maximizing the amount of useful blocks that can be obtained from the available block requests in the shortest time frame. the mechanism identifies the set of peers to which block requests can be sent (eligible peers), and determines for how long no block requests should be sent. therefore, as the former is a direct result of individual peer behavior and the latter depends on the swarm behavior, we define the backoff time as a two-dimensional variable that has per peer and per swarm components. the peer backoff component provides the delay to return a peer to the set of eligible peers; the swarm backoff component provides the delay until the next block request. the actual backoff time is randomly generated within the interval [0, b ], where b is the calculated backoff time."
"then, we discuss the results obtained for all baseline use cases to assess the impact of collusion size, content size, number of seeders, and content popularity, which are the variables that are expected to change more often. we end by comparing the use cases of baseline, overhead, disguise and traditional categories to, respectively, evaluate the impact of minimum network disguise overhead, estimate the impact of cover downloads (additional peer arrivals and partial downloads), and to assess the impact of the provided protection as a whole. fig. 8 provides a comparison of the average download bitrate achieved by the traditional p2p model and by the mistrustful p2p model, which is set for minimum protection -baseline use cases for single peer attacks -given that traditional p2p systems do not provide any privacy protection. the results obtained by both models are equivalent, despite considering an optimistic model for representing traditional p2p systems, and show that peers are able to timely download contents without advertising what they download. the performance difference is negligible when considering single seeder use cases, and more noticeable for some use cases when considering 64 seeders. the benefit provided by the larger number of seeders seems to be dependent on the ratio between seeders and regular peers because it fades as the number of simultaneous peers increases, be it due to higher peer arrival rate or larger content size that requires peers to stay longer to complete their download. this correlation with the peer arrival rate is more evident for the 800 mib content using the popular peer arrival trace (center right). table 4 provides the number and ratio of downloads completed for all 84 use cases, and shows that peers are able to complete their downloads: the download completion ratio is always above 96%. this ratio is below 100% for all use cases because there is always a set of peers that is unable to complete the download: peers arriving when the time left to end the simulation is less than the time required to complete the download. thus, the 800 mib use cases achieve a lower download completion ratio than their 100 mib counterparts. the download completion ratio is identical for all categories, but the number of downloads is different for the disguise category because the peers used to emulate cover downloads (one third of all peers) are not considered as they only download partially the content (50%). the lowest download completion ratio is achieved for the popular (p) peer arrival trace, in particular for 800 mib content size, due to an increase on the peer arrival rate near the end of the simulation (see fig. 8 )."
"the cultural differences, despite some ethical standards being universal (e.g., murder and theft), make it difficult to define what is ethical or not. studies have shown that the perspective on ethical practices of individuals regarding the use of computer technology differs with their nationality. asian traditions of collective ownership conflict with western protection of intellectual property, and many of the ways the former use software is considered software piracy by the latter [cit] . these differing perspectives are also evident on the control over the internet content and on the surveillance made by governments: they radically differ between countries [cit] . according to a report [cit] from the reporters without borders, the great firewall of china is getting \"taller\", the united kingdom is the \"world champion of surveillance\", and \"nsa 2 symbolizes intelligence services' abuses\"; [cit] ."
"for a group analysis, this procedure is repeated for every participant, and the resulting accuracy maps can then be spatially normalized and submitted to standard statistical analysis procedures in spm or any other preferred software package (figure 2 bottom)."
"a content download is usually broken into three stages: flash crowd, steady-state, and end phase [cit] . the flash crowd is the most demanding stage because there is a sudden burst of peer arrivals, which largely surpass the peer departures. the steady-state stage is characterized by an equilibrium between arrivals and departures. the end phase stage comprehends the end of life of a content where there are fewer arrivals than departures. therefore, an ordinary poisson arrival process is not able to capture the peer arrival dynamics because the mean peer arrival rate changes over time."
"the hybridization scheme is shown in figure 2 . it should be noted that in the hybridization scheme in figure 2, instead of replacing the mutation procedure of gas by the ts procedure, we have added a new function to the ga population class (namely apply tabusearch) for applying the ts. this new function could be applied to any individual of the current population, however, this is computationally costly. in our case, given that we want to run the grid scheduler in short times, the apply tabusearch is applied with small probability a . in fact, this parameter can well be used to tune the convergence of the ga since ts usually provides substantial improvements to individuals."
"the peer selection mechanism aims at selecting the eligible peer that has the highest probability of providing a useful block in less time. the set of eligible peers is constrained both by the disclosure constraint and request backoff mechanisms. the former provides, for a given content, the set of peers to which no further block requests can be sent to; the latter provides, for the same content, the set of peers that are ineligible for the moment, and when can the next block request be sent to each one of them."
"the quality of a schedule can be measured using several optimization criteria, such as minimizing the makespan (that is, the finishing time of the latest task), the flowtime (i.e., the sum of finalization times of all the tasks), the completion time of tasks in every machine (closely related to makespan), which formally can be defined as follows:"
"progress. the extent of these incentives depends on the cultural background as the well-being of the society may be incentive enough (collective ownership) or further incentives may be required (intellectual property rights). p2p networks introduce new challenges to the private copying levy system, and the legislation on this subject is expected to change in the near future to address them. users of p2p systems should not be subject to any civil and criminal liability as a result of legitimate usage of the system if the main motivation to use it is to share legit contents, and they have no access to the content data. the intent is important to determine the extent of user liability, especially if his actions directly or indirectly caused provable harm."
"although the number of publications using multivariate decoding methods has been rising continuously, it still seems to be common for researchers to program their own multivariate data analysis pipeline from scratch 1 . this is not only very timeconsuming and redundant, it is also very error-prone. indeed, if the same piece of code is adapted for different decoding analyses, changes that were done for previous analyses might be overlooked in later analyses or might affect other analysis steps which can produce invalid results. just as problematic-and not uncommon even for experts-is the unintentional wrong use of decoding methods, for example circular analyses [cit] ). in addition, researchers might hesitate to conduct decoding analyses, either because of their limited understanding of machine learning methods for functional neuroimaging [cit] or because of the effort involved in learning to apply a new analysis method. for that reason, researchers getting started with decoding would strongly benefit from an easy-to-use implementation of multivariate methods. at the same time, they would ideally keep using the same software for simple analyses that they would later use for more sophisticated approaches once they have gained sufficient knowledge."
"a powerful feature of tdt is the use of decoding design matrices (figure 3) . a decoding design matrix determines the exact structure of a cross-validation design, i.e., which data belongs to which class and which data is training and test data in each decoding step (also called cross-validation fold or iteration). to determine which samples should remain together (e.g., because they are in one run and the analysis is a leave-one-run-out cross-validation), data can be assigned to chunks which serve as categorical numerical labels. although in many cases, the number of chunks determines the number of decoding steps, more complex designs (e.g., bootstrapping designs, figure 3c ) can have more steps than chunks. finally, multiple decoding analyses can be carried out in one design, a feature which might be particularly useful if data needs to be analyzed separately and combined, or if crossclassification is applied to a number of different data sets saving repeated training on the same data. a number of existing functions allow creating a multitude of different designs in advance, and additional design functions can be created by more advanced users (see figure 3 for some example designs using different functions). after creating the design, the design inspection can be used to visualize if the analysis is carried out in the correct manner. example call (design created and visualized prior to a decoding analysis as part of the script):"
"has no trust requirements. the user must be able to download a content without having to trust anyone. therefore, the privacypreserving p2p system must enable content sharing in large groups of untrusted peers (untrusted p2p networks)."
"we demonstrated the feasibility of the mistrustful p2p model through simulation, using ns-3, and evaluated the impact of privacy preservation on the average download bitrate, and download completion ratio, considering that peers leave immediately after finishing the download. in the majority of the use cases considered, the average overall download bitrate is close to the one of the traditional p2p model. with the mistrustful p2p model, peers have no need to suddenly terminate or remove downloads because the provided protection does not depend on the time a peer keeps sharing a content."
"bitblender [cit] provides plausible deniability by introducing relay peers that simply proxy requests on behalf of other peers. peers willing to act as relay peers can register at a central node called blender, and, once requested, will join a p2p swarm in a probabilistic way so that they cannot be distinguished from regular peers. the joining probability of relay peers is defined by the blender, when asking registered peers to join a p2p swarm, so that the set of relay peers remains unknown while having the cardinality requested by the tracker. it enables the identification of regular and relay peers through download progress tracking, and provides a trivial proof of content download because peers advertise what they download. thereby, bitblender provides protection against passive attacks, but it is vulnerable to active attacks using download progress tracking. it requires users to trust both the tracker and the blender . its users may be subject to legal liability for downloading misleading contents, and for relaying traffic of misbehaving peers. swarmscreen [cit] provides plausible deniability by obscuring user content interests through content interest disguise. the devised scheme, which consists in \"adding a small percentage (between 25% and 50%) of additional random connections that are statistically indistinguishable from natural ones\", thwarts guilt-byassociation attacks, that is, attacks in which the user content interests can be inferred with high certainty just by classifying peers based on the behavior of the communities they participate in. swarmscreen presents no trust requirements, but its attack model only considers passive attacks. it is vulnerable to active attacks because contents can be distinguished through download progress tracking. peers communicate through direct links, but users may be subject to legal liability due to misleading content download."
"classifiers are essentially functions that describe a separation boundary (i.e., hyperplane, see table 1 ) of multiple classes in voxel space. newly predicted samples are mapped relative to this function and typically receive an output larger or smaller than 0 that denotes the distance to the separating function, where positive values denote membership to one class and negative values membership to the other class. for linear classifiers, so called decision values can be used to indicate the distance of a particular sample to the separating hyperplane. using the predicted labels, the actual labels and the decision values, a number of metrics can be calculated to evaluate the performance of a classifier."
"the security analysis presented the countermeasures employed against common p2p file sharing attacks, and discussed the identification of user content interests, proof of full content download, and user legal liability. the protection provided by the mistrustful p2p model is deterministic, and therefore only an underestimated attacker (of a size exceeding c ) may be able to determine user content interests, which requires proof of full content download. nevertheless, the size of an underestimated attacker should be significantly higher than c before it is able to prove content download."
"the main privacy issues of traditional p2p systems are that they publicly disclose user content interests, and provide a proof that the user is able to access a content in part or entirely; the former discloses an intention while the latter provides a proof of its realization. a peer only downloads the contents that a user is interested in, therefore, by registering at a tracker and joining swarms, the user content interests are being publicly disclosed. peers provide a proof that the user is able to access a content in part or entirely by advertising which chunks they own, and entirely by notifying the tracker upon download completion."
"tdt comes with a number of processing approaches, but of course not all methods have been implemented. researchers might want to use their preferred machine learning algorithm or feature selection method for their decoding analysis. other users might wish to use tdt only as a wrapper tool for their own machine learning package. data can also be passed to tdt, which in principle extends tdt to other modalities such as eeg or meg decoding. in this section, we will use three examples to illustrate how tdt can be extended."
"other measures have received interest as well. for instance, weight maps are useful to provide information about the contribution of each voxel to the classification and are often used in whole-brain classification settings. importantly, weight maps cannot be interpreted as reflecting the neural substrate relating to a task or another variable that is classified. researchers interested in interpreting weights beyond the classifier alone can convert the weight maps to patterns which then show the contribution of each voxel for the representation of the classes under study [cit] . in addition, representational similarity analysis which exploits the correlation similarity structure of voxels and different distance metrics can be used to illustrate the representational distance of different conditions [cit] . all approaches described above have been implemented in tdt."
"we hope that the above illustration of tdt demonstrated the simplicity and general utility of this toolbox for multivariate analysis of functional mri data. in addition, the extensive error checking helps prevent many programming errors and should guide users in how to quickly resolve them. we did not go into detail about an additional important feature of tdt that proved helpful to prevent errors: the visualization of the decoding design. it shows users at a glance if the cross-validation design they intended to set-up is indeed the one that is computed and if the input data is indeed the data they wanted to use. this further facilitates the prevention of unwanted or erroneous analyses. to help in getting started, tdt comes with an example dataset an example analysis scripts."
"the main legal issue of traditional p2p systems is that, unknowingly and unwillingly, a user may be held liable for copyright infringement due to illegal content download. if a content is published with a misleading description or if the resource used to obtain the hash of the content is insidious, the user will only become aware of that fact after accessing the content data in part or fully. nevertheless, for contents that can be accessed in part before fully downloading them, which is usually the case of multimedia contents, the user may be infringing copyright law after downloading a single or a few chunks. the copyright infringement can be triv- 4 a content is available if it can be fully downloaded. ially proved because peers advertise which chunks they own and notify the tracker upon download completion."
a block request has five possible outcomes: (1) refusal -the request is refused by the contacted peer; (2) cancellation -the request is canceled by the requester (duplicate block); (3) acceptance
"the bittorrent anonymity marketplace [cit] follows swarmscreen's approach to provide plausible deniability. it does not present any trust requirements, and peers also communicate through direct links. however, in order to protect against both passive and active attacks, all contents are fully downloaded to make them indistinguishable, given that an attacker is able to fully track download progress: peers advertise what they own or miss. the authors define k-anonymity as the privacy protection level obtained from fully downloading k contents. thus, since it introduces high network overhead, it either prevents downloads from timely completing or constrains the achievable level of privacy protection. users may be subject to legal liability due to the download of misleading contents."
"more recently, these mass-univariate analysis methods have been complemented by multivariate pattern analysis (mvpa) which refers to a collection of multivariate methods of brain data analysis that incorporate multiple dependent variables at the same time [cit] . one of the most popular multivariate methods of brain data analysis is generally referred to as multivariate decoding which describes the mapping of multiple dependent variables to one or multiple independent variables, and which contrasts with multivariate encoding describing the opposite mapping [cit] . the popularity of multivariate methods of brain data analysis stems from three facts: first, multivariate methods offer increased sensitivity in detecting statistical dependence between cognitive variables and (patterns of) brain activity, because these methods can combine information across multiple voxels and exploit their covariance. put simply, multivariate methods make it easier to detect existing differences between brain signals. second, multivariate methods allow for greater specificity in finding a statistical dependence between measured brain data and multiple categorical responses. although a number of brain regions show overall changes in activity selective for specific stimulus categories (e.g., [cit] ), stimuli can be encoded in a more distributed manner [cit] . multivariate methods can be used to target distributed representations of stimuli, cognitive variables, or other variables of interest. this effectively enables researchers to ask more specific questions regarding neuronal representations, for example about the representation of individual face exemplars [cit] . third, multivariate decoding methods allow functional neuroimaging to be used for accurate prediction of cognitive and mental states in the form of a \"neuronal marker.\" for example, they might be used to support the diagnosis of neurological disorders beyond regions typically detected by radiologists [cit], predict the conversion from mild cognitive impairment to alzheimer's disease [cit], interrogate cognitive states from people that cannot communicate [cit], or support non-invasive brain-computer interfaces for motor control in paralyzed patients [cit] ."
"for the second empirical analysis, we ran a multiclass searchlight analysis on subject 1 from the data set, with a leave-one-run out cross-validation scheme and a searchlight radius of 4 voxels."
enables timely downloads. the user should be able to download a content in due time. the average download bitrate must be within the same order of magnitude of traditional p2p systems that do not preserve the privacy of users.
"for support vector machines, the area of space between two classes, of which the center is typically the separating hyperplane. svms have the goal of maximizing the margin between two classes searchlight analysis one of the three most common types of decoding analysis conducted, the two other being whole brain decoding and region-of-interest decoding. searchlight decoding typically creates a map of classification accuracies that can be interpreted as the local information content around each voxel [cit] support vector machine a type of classifier that maximizes the margin between two different classes [cit] weight vector determines the contribution of each feature to the final classifier function. for most classifiers, the weight vector cannot be directly interpreted as reflecting the classified variable, because it is a filter that extracts a class signal while at the same time it suppresses correlated noise. using the covariance of the data, the weight vector can be converted into an interpretable pattern vector [cit]"
"the selected peers offer useful blocks, those blocks get transferred; otherwise, the requests are canceled and no transfer occurs. the list of peers may be updated during content download, and steps (3), (4) and (5) are repeated until download completion (genuine contents) or until at least m blocks have been transferred (cover contents). there is no notification from the commoner upon download completion."
"finding the maximum intersection between the set of blocks disclosed to any set of c peers is an np-hard problem [cit], thus we devised a conservative yet efficient algorithm to evaluate dynamically the number of blocks that can still be shared with a peer. the algorithm is divided into two main functions: one to update the counter of blocks disclosed to a peer ( algorithm 1 -update blocks disclosed), and the other to determine the number of blocks that can still be disclosed to a peer ( algorithm 2 -blocks to disclose left). the variables commoners and blocksdisclosed are respectively an array sorted by the number of blocks disclosed, and the maximum number of blocks disclosed to any set of c peers."
"the identification of genuine and cover contents may only be possible if the size of the actual attacker exceeds the size c of the largest colluding group considered (underestimated attacker). still, the identification of a content as cover, per se, only enables an attacker to reduce the set of contents that the user may be interested in, given that the user may have no interest in any of them. thereby, an underestimated attacker still has to prove content download in order to determine user content interests, as long as the actual amount of blocks downloaded per cover content can take any value between m (inclusive) and k (exclusive)."
"for a neuroscientist, it is important to statistically evaluate the results generated by a classification analysis: is the observed result statistically significantly different from chance? although at a first glance a decoding accuracy close to chance may seem disappointing, it is indeed quite commonly observed and not necessarily a problem. in neuroscience, typically the goal is not to maximize classification performance, but to demonstrate that information is present in the brain signals used for classification. this contrasts with the use in machine learning where the primary goal is to maximize classification performance and accuracies only slightly above chance would not be considered very useful. binomially distributed, which is why binomial tests should not be used to statistically evaluate cross-validation results [cit] . as an alternative, permutation testing can be used. however, for searchlight analyses this can become very time consuming. in addition, caution is warranted when using permutation approaches to make sure that these tests are not overly liberal [cit], for example because they break up statistical dependencies at the wrong level (i.e., [cit] ) . these problems relate only to statistical analyses at the \"decoding-level.\" an alternative and common approach is to combine multiple decoding results and conduct a \"second-level\" statistical analysis. this is the case when evaluating whether the mean accuracy across a group of subjects is different from chance. for these purposes, classical parametric tests such as the student's t-test can be used."
"this section describes all remaining classification-related methods that can be carried out using a decoding analysis in tdt ( figure 1c) . these include feature scaling, selection of model parameters in nested cross-validation, transformations of feature space, and feature selection. none of these steps are strictly necessary for a decoding analysis, but they can in principle help improving the results of a decoding analysis."
"peer selection attacks are avoided by registering at multiple unrelated trackers and requesting them lists of peers in a swarm. as so, through comparison, misbehaving trackers can be identified and blacklisted. nevertheless, this attack does not increase the amount of information that a single colluding group of a size up to c can gather."
"to allow a first look inside the \"black box,\" we will now explain the inner functions of this example call (figure 1) . prior to running a decoding analysis, tdt assumes that data have been preprocessed appropriately (figure 2 top) . standard preprocessing includes spatial realignment, possibly slice timing correction and detrending, and in some cases also spatial normalization and smoothing (although these latter steps are less common for preprocessing of decoding analyses). as mentioned above, rather than running a decoding analysis on individual images, it has become quite common to use single trial estimates of data [cit] or even estimates that combine multiple trials within one run [cit] . this process can improve classifier performance (mourão [cit], 2007 [cit] ), and combining multiple trials can lead to higher classification accuracies [cit] and slightly improved power [cit] . these estimates would typically be generated in the common glm framework and are stored as so called beta images. for simplicity, in the following we will assume that one beta image per condition per run is generated (e.g., one for \"left\" for run 1, one for \"right\" for run 1, one for \"left\" for run 2, one for \"right\" for run 2, etc.). in this case, the above example call simply gets the beta images from beta_dir representing the conditions \"left\" and \"right\" and will extract the run numbers. the brain mask which spm automatically creates during model estimation contained in beta_dir is automatically used to reduce the analysis to voxels inside the brain. this information is sufficient to carry out a decoding analysis with a leave-one-run-out cross-validation scheme (see table 1 for terminology), and in this example, a so-called searchlight analysis is executed with a radius of 4 voxels, using a support vector machine [cit] as a classifier. on a dell precision m4600 intel @ 2.30 ghz, 64 bit windows 7 with matlab (2011a), this analysis takes roughly 3 min for around 120,000 searchlights."
"in addition, tdt contains many checks to prevent erroneous analyses. in general, it is always possible that unwanted errors occur, and of course researchers always have to double-check if their analysis yields meaningful results. the implemented checks facilitate the process of detecting and eliminating errors. among others, they ensure independence of training and test data, that all data has the same realignment parameters, and more generally whether the selected options are incomplete or would generate errors. other initial checks help in saving time and frustration of the user: each decoding analysis creates a log-file which can be used to recover possible sources of errors. these features have been implemented based on the experience of the authors as well as through user experience and will continuously be improved."
"hides user content interests. the user must be able to hide his content interests from any participant in the system: trackers, regular peers, or groups of colluding peers. protection against external entities monitoring all the user's traffic, such as isps or governments, is out of the scope of this work."
"to demonstrate that tdt works correctly, we validated the toolbox by running a number of analyses on simulated and real data. these analyses are not supposed to provide an overview over all capabilities of the toolbox. rather, they are used to illustrate some of the functionalities of tdt."
"traditional p2p file sharing systems focus on performance and scalability, disregarding any privacy or legal issues that may arise from their use. these systems take advantage of the large number of interconnected peers 1, and their idle resources, to more efficiently distribute contents at the cost of requiring peers to publicly advertise what they download, making it trivial to identify user content interests. this problem is further aggravated by the fact that peers form interest-based communities, and every single connection presents an opportunity for a malicious peer to passively obtain additional information that may enable the identification of user content interests, with high certainty, by monitoring just a small fraction of the network [cit] ."
"peers are identified by their public ip addresses because we consider that ip addresses provide a more flexible resource testing that can be made harder to acquire than other resources such as human time, network bandwidth, computational power or storage capacity, which cannot be related. for privacy protection purposes, a set of peers whose ip addresses are considered to be related can be treated as a single peer (ip address aggregation), or multiple peers sharing a single ip address can be treated individually by considering the (ip, port) pair as the identifier (ip address multiplexing). the user is then able to configure how ip addresses are treated for privacy protection purposes, providing the flexibility to go as low as treating all (ip, port) pairs as unique identities, e.g. all peers behind nat 5, up to treating all ip addresses of a given entity, colluding entities, city, country or any other set of ip addresses as a 5 network address translation. single peer. this also enables the user to configure the set of rules that are applied to peers using anonymous systems such as tor, which can be ip address aggregation rules, ip address multiplexing rules or any combination of both."
"we are interested in evaluating the impact of content size, peer arrival rate, number of seeders, c and m, and cover downloads on the average download bitrate and on the download completion ratio. for the sake of clarity and tractability, we aim at reducing the impact of other variables on the overall performance, including those referred in the literature [cit] such as session length, peer lifetime (time between first arrival and last departure), downtime, uptime, lingering time (additional time a peer lingers in the system after download completion), and inter-content relations. we consider that peers have homogeneous internet connections, do not perform simultaneous chunk requests, always attempt to complete the download in a single session, and leave immediately after completing the download (worst case). also, we consider a single genuine content download in order to enable fair performance comparison with traditional p2p systems, and thus cover downloads are emulated by increasing the peer arrivals and by having those peers to download only a fraction of the content."
tabu search (ts) has shown its effectiveness in a broad range of combinatorial optimization problems and distinguishes for its flexibility in exploiting domain/problem knowledge (see alg. 2).
"several privacy-preserving p2p systems have been proposed, but, given that there is a trade-off between privacy and performance, they consider different attack models and employ different techniques for privacy preservation. the majority of the solutions employ either techniques to provide anonymity or techniques to provide plausible deniability. techniques to provide anonymity, such as onion routing [cit] and information slicing [cit], are usually stronger but have lower performance. techniques to provide plausible deniability, such as request relaying -peers relay requests to create uncertainty about communicating endpoints -and content interest disguise -peers download additional contents to hide their real interests -, are usually weaker but have better performance. despite their differences, all p2p file sharing systems share one common issue: they require peers to advertise, either fully or partially, what they download. lacking alternatives, users have adopted anonymity systems for p2p file sharing [cit], misunderstanding the privacy guarantees provided by such systems [cit], in particular when relaying traffic of insecure applications [cit] . anonymity systems provide a channel to anonymously transmit messages, but the user's identity may be disclosed by the content of that messages, which are the sole responsibility of the application."
"fig. 4. messages exchanged during block download process. peer a requests a random block from peer b, which either refuses the request or offers a block with id x . if the block is accepted by peer a, it will get transferred; otherwise, no transfer occurs to avoid unnecessary network overhead."
"ing the probability of retrieving useful blocks, in particular during the flash crowd stage because the few useful blocks are being uploaded mostly by seeders. given that all contents are divided into 64 blocks, increasing the content size augments this effect as seeders will take more time to share the blocks required. this effect is noticeable for the most popular content in fig. 11 but not for the less popular content because the main bottleneck with the former is the unavailability of blocks and not the peer unavailability as with the latter. as shown by fig. 12, apart from the beginning, the average backoff time ratio is low throughout the content sharing of the more popular content while it is high throughout the fig. 12 . average ratio of backoff time over one hour periods for a 800 mib (top) and a 100 mib (bottom) contents using, respectively, a more popular ( mp ) and a less popular ( lp ) peer arrival traces as input. each plot depicts each use case categorybaseline, overhead, disguise, and traditional -for a single seeder and collusion attacks of, at most, 31 peers (except traditional ). the peer arrival rate is represented by a dotted gray line with a y-scale on the right. content sharing of the less popular one. therefore, we can conclude that the impact of minimum network disguise overhead, as for collusion size, depends on the number of seeders and simultaneous peers. its impact dilutes as the number of simultaneous peers increases."
"a hash is an alphanumeric string used to identify and to verify the integrity of the data being transferred, usually an sha-1 digest (hexadecimal string). a content is composed of one or more files, identified by the hash of its data, and partitioned into several pieces. a chunk is one of those data pieces, and a torrent file provides the content's metadata. a peer is a node sharing chunks, which, for a given content, can be either a seeder -a peer that has all data chunks and is just sharing them -or a leecher -a peer still downloading the content and sharing the chunks it has already downloaded. fig. 1 depicts the peer roles and their sharing behavior. a swarm is the set of peers sharing the same content (peers form interest-based groups to share contents), and is usually identified through the hash of the content. a tracker is a central node that provides lists of peers in swarms by keeping track of which peers are sharing which contents and their role (seeder or leecher) on each content."
"for the design of our hybrid approach we consider two well-known meta-heuristics: genetic algorithms (gas) and tabu search (ts). both gas and ts have been developed for the independent task scheduling in and in sequential setting. we have considered the steady-state ga in this work. the choice of these two meta-heuristics is based on the following observations. first, grid schedulers should be very fast in order to adapt to dynamic nature of computational grids. therefore, a fast convergence of the main algorithm is preferable in this case, which can be achieved through a good trade-off between exploration and exploitation of the search. second, in order to achieve high quality planning in a very short time, it is suggestive to combine the exploration of the solution space by a population of individuals with the exploitation of neighbourhoods of solutions through local search. in such case, gas and ts are among the best representatives of population based and local search methods, respectively."
"the term feature transformation is not commonly used in the brain decoding community, but it is known in the machine learning community as a set of methods that change the meaning of each individual feature [cit] . we use this term to refer to a collection of methods that is used to change the space of data and possibly reduce the dimensionality of this changed space, as well. principal component analysis (pca) is one example. we prefer to treat such methods separately from feature selection (see below), because they lead to a change in the mapping of features. methods that we refer to as feature selection (below) select a subset of features (e.g., voxels) of the input. in contrast, in feature transformation a data point refers to a non-linear transformation applied to the input, which essentially creates new features. for example, the outcome of a principle component analysis (pca) is no longer separate single voxel data, but a linear combination of the original voxel data. this strict semantic separation helps to avoid confusion, makes the results of feature selection easier to interpret, facilitates the addition of new methods, and facilitates the use of multi-step feature selection (e.g., first running pca and transforming voxels to principle component space, and then running feature selection on a subset of principal components). other examples of feature transformation would encompass independent component analysis (ica), fourier transformation, or pre-calculated mappings that can for example be used to align data spaces between subjects, known as hyperalignment [cit] . currently, only pca is implemented in tdt, but other methods can easily be added. the transformation can be estimated on training data only and applied to test data in each cross-validation step (\"across\"). alternatively, it can be estimated on both training and test data (\"all\") when it is ensured that this does not lead to nonindependence [cit] ). this should not be the case as long as no label information is used for transformations (which is not done in pca). if feature transformation is also used for dimensionality reduction, the user can specify either the number of to be selected features or can specify a critical value in the score of the method that needs to be exceeded for a transformed feature to be of interest (e.g., for pca the percent variance explained). example call:"
"a regularization method which influences the complexity of a classification model. in most cases, an l2-norm is used which minimizes length of the weight vector. the l1-norm can be used to minimize the sum of the absolute weights, typically resulting in a sparser model (i.e., less features will contribute to the classification)"
"in this work we are interested in scheduling of independent tasks to grid resources. the formal definition of the problem is based on the definition of the expected time to compute (etc) matrix in which et c[j][m] indicates an estimation of the completion of task j in resource m. in fact, one possible way to compute the entries et c[j][m] is to divide the workload of task j by the computing capacity of resource m. under the etc matrix model, the scheduling problem specification is as follows:"
simulator's configuration for the static case in the static grid the number of tasks and machines are kept constant during the execution of the simulator. it is thus assumed that the system starts with a certain number of tasks and machines and that there are no machine drops from the system. the configuration of simulator is defined by parameters presented in table 1 .
"traditional p2p systems take advantage of the large number of interconnected peers, and their idle resources, to more efficiently distribute contents; their main performance metric is the average download time. e.g., bittorrent protocol employs several mechanisms for chunk and peer selection, such as rarest first mechanism -locally rarest chunks are downloaded first -, and optimistic unchoking -periodically select a random peer -aiming at constantly improving the download bitrate [cit] . the distinction between seeders and leechers is used to estimate relative download times (through their ratio), and also to quickly assess the content availability 4, which is assured if at least a single seeder is present. the steps required to download a given content, depicted in fig. 2, can be summarized as follows: (1) a peer willing to download a given content registers at the tracker and joins the swarm; (2) it requests a list of peers (a subset) in the swarm from the tracker; (3) it selects peers from that list to obtain missing chunks in order to complete its download; (4) peers synchronize the chunks they own and miss to determine if any missing chunks can be transferred; (5) if selected peers own missing chunks, those chunks get transferred; (6) upon download completion, the peer notifies the tracker to be known as a seeder for that content."
"we consider 84 use cases to evaluate how content size, popularity (overall peer arrival rate), number of seeders, collusion size c, minimum network disguise overhead m, and cover downloads affect average download bitrate and download completion ratio; by use case we mean an evaluation using a distinct set of values for the variables in study."
"herein, we address the two main goals described in the previous section: (1) to show that peers are able to timely download contents without advertising what they download, and (2) to estimate the impact of privacy preservation on the average download bitrate and on the download completion ratio. the main performance metric considered by p2p file sharing users is the average download time or the average download bitrate, which are two sides of the same coin. the remainder of this section is structured as follows. in order to demonstrate the feasibility of block advertisement avoidance, we start by comparing our model against the traditional p2p model for the case of minimum protection, and by detailing the download completion ratio for each single use case."
"the mistrustful p2p model [cit] provides plausible deniability through content interest disguise, as in other privacy-preserving solutions, but it has no trust requirements, prevents user legal liability in case of legitimate usage, and ensures deterministic protection of user content interests against attacks of a size up to a configured level. it is built on the basis of mistrusting all the entities participating in the p2p network, hence its name, and therefore users are not required to establish trust links in order to participate in the content sharing. the mistrustful p2p model enables each user to set the required trade-off between privacy and performance by configuring, per content, the size of the largest group of colluding peers to be protected against, and the minimum network overhead of content interest disguise (minimum network disguise overhead)."
"a very recent development under matlab is cosmomvpa (http://cosmomvpa.org/) which is still in its very early stage. the general structure of the toolbox is quite similar to tdt. rather than creating a cfg in the beginning as is the case for tdt, each option is passed along with the multiple subfunctions that are called. the toolbox offers a range of classifiers, allows volumetric and surface-based searchlight analysis and has a basic interface to spm and other brain analysis software packages. the toolbox is easy to use with some intermediate level programming knowledge in matlab. however, cosmomvpa currently does not offer decoding designs, potentially leading to overlooked mistakes in decoding analyses. due to its novelty, the error management of the toolbox and additional methods such as feature selection and parameter selection are still quite basic. passing parameters along the use of subfunctions also invites mistakes on the side of the user. however, it is possible that these current limitations will be changed in future versions of the toolbox."
"the remaining three mechanisms -block selection, request backoff, and peer selection -are defined to enable the evaluation of our model, and to ensure that contents are timely downloaded. the block selection mechanism determines which block is to be offered to a given requesting peer, affecting the distribution of blocks among peers and therefore the probability of retrieving a useful block (innovative block). the request backoff mechanism determines the delay between block requests aiming at maximizing the amount of useful blocks that can be obtained from the available block requests in the shortest time frame. with the same aim, the peer selection mechanism selects a peer to which a block request will be sent."
"algorithm 1 receives as input the id of the peer to which one additional block was disclosed. if none has yet been disclosed, a new entry is created; otherwise, the entry is updated and, if needed, some elements are swapped to keep the array sorted. in each case, if the updated entry is on one of the top c positions, the maximum number of blocks disclosed is updated. algorithm 1 has linear time complexity. algorithm 2 also receives as input the id of the peer. if the configured privacy requirements are not met (invalid), no blocks can be disclosed to any peer. if they are met, left contains the number of blocks that can still be disclosed, ensuring that at least one block can be disclosed to each one of the top c peers; at most, m − ( c − 1 ) blocks can be disclosed to a single peer. left needs to be updated if there are already at least c peers and the peer referred by id is outside of that set. algorithm 2 runs in logarithmic time."
"one of the motivations for developing hybrid meta-heuristics is that of coping in practice with dynamic and large instances from real-world problems, that is, a clear practical relevance. in this context, we are interested to see how hybridization could help in efficiently solving the scheduling problem in computational grids. the problem is very complex due to the large-scale, heterogeneous and dynamic nature of grid systems. additionally, the problem can be formulated for different modes such as immediate and batch mode and is multi-objective in its general formulation."
"all baseline use cases are depicted in fig. 9 to assess the impact of collusion size, content size, number of seeders, and content popularity. a collusion size of up to 31 peers requires a peer to contact, at least, 32 unique peers to be able to complete the download. thereby, when considering 64 seeders, the results are identical for both a collusion of 1 and of 31 because it is guaranteed that there are always enough peers available. however, in the use cases considering a single seeder and a collusion of 31, mainly for smaller and less popular contents as there are less simultaneous peers, the correlation between the average download bitrate and the peer arrival rate is evident. as the number of simultaneous peers increases, be it due to higher peer arrival rate or larger content size, the performance gap between collusion of 1 and collusion of 31 for single seeder use cases fades until it becomes negligible. thus, to compare the performance of baseline category with all others, we focus on single seeder and collusion of 31 use cases as they provide the worst case and enable a better assessment of the impact of each variable. for the sake of clarity over completeness, we only provide plots for the edge use cases: 10 0 mib and 80 0 mib contents using, respectively, the less popular and the more popular peer arrival traces. figs. 10, 11, and 12 provide the results obtained in each use case category -baseline, overhead, disguise, and traditional -for, respectively, the ratio of block requests sent to seeders out of all block requests, the average download bitrate, and the average ratio 10 . average ratio of requests sent to seeders over one hour periods for a 800 mib (top) and a 100 mib (bottom) contents using respectively a more popular ( mp ) and a less popular ( lp ) peer arrival traces as input. each plot depicts each use case category -baseline, overhead, disguise, and traditional -for a single seeder and collusion attacks of, at most, 31 peers (except traditional ). the peer arrival rate is represented by a dotted gray line with a y-scale on the right. of time spent on backoff (average backoff time ratio). as shown by fig. 10, the main reason for the performance gap between our model, for all use cases using the less popular peer arrival trace, and the traditional p2p model is the exploitation of the seeder. unlike the traditional p2p model that favors requests to seeders, our model treats all peers alike and, on average, shares no more than m / c blocks with each individual peer, including the seeder. for this reason, our model presents a low ratio of block requests sent to seeders, which is not subject to significant variations."
"is the number of active peers. the swarm backoff time is a function of both block transfer time and the number of active peers, which can be obtained from the tracker(s). given that the evaluation assumes no simultaneous block requests, there is no minimum delay required between consecutive requests, which can be, at most, the time required to download a single block."
"the reasons behind seeking privacy may be various, such as to avoid user profiling, tracking and data mining, to privately download legal contents that may be embarrassing or objectionable from a political, religious or social point-of-view, or to download unethical, illegal or incriminating contents without being subject to any liability. the current state of computer technology enables organizations and individuals to create databases of personal information that were previously impossible to set up, and swap this information, sell it or use it in any other way as a commodity [cit] : organizations and individuals are able to collect information about users, combine facts from separate sources, and merge them to create such databases of personal information. users may feel exposed or embarrassed if it becomes public that they had access to contents such as to help dealing with alcohol and drug abuse, to help dealing with anger management, or considered heretical or profane. even accessing illegal or incriminating contents may have an ethical motivation because laws are, ideally, drawn from ethics, but that is not always the case: e.g., autocratic regimes consider illegal to share or even access contents that they consider inappropriate or potentially harmful."
"the mistrustful p2p model is built on the concept of mistrusting all the entities participating in the p2p network, hence its name, and therefore users are not required to establish any trust links in order to participate in the content sharing. it relies on two main building blocks -content interest disguise and mistrustful sharing -and aims at hiding user content interests through plausible deniability, in untrusted p2p networks, while overcoming the main limitations of akin p2p file sharing systems. these limitations can be summarized as follows: (1) peers are required to advertise what they download enabling passive attacks; (2) protection against active attacks is only achieved by introducing either trust requirements or considerable network overhead; (3) the privacy protection against both passive and active attacks is probabilistic; (4) legitimate users may be held legally liable for, unknowingly and unwillingly, downloading or relaying traffic of illegal contents."
"tdt is open-source (see license statement), with extensively commented code, comprehensive variable names, and an execution structure that follows along a main function with several auxiliary functions. we did not want to provide a black-box where a user cannot follow the steps that are carried out in a decoding analysis."
"originally, we created tdt to simplify setting up decoding analyses and prevent the unintentional introduction of programming errors or unnoticed changes in settings that do not elicit error 1 to support this observation numerically, we conducted a google scholar search where we intended to limit our search to articles using multivariate decoding methods and which explicitly mention the software package they use for regular analyses. these articles should also mention the mvpa toolbox used. for the search, we used the combination \"fmri and ('multivariate pattern analysis' or 'multivoxel pattern analysis') and (fsl or afni or freesurfer or spm).\" this yielded 659 [cit] . of these, only 103 (15.6 %) contained the strings \"pronto,\" \"pymvpa,\" \"mvpa toolbox\" or \"3dsvm,\" which refer to previous mvpa-related software packages (see comparison to existing packages of multivariate data analysis). 132 (20.0 %) contained the terms \"in-house,\" \"custom-written,\" \"custommade,\" \"custom-built\" or \"custom matlab.\" neither term was contained in 442 (67.0%) articles."
"as future work, we intend to do the following. further characterize the mistrustful p2p model by considering more variables such as the number of blocks into which a content is divided. improve the block selection mechanism by updating the weights based on both incoming and outgoing block requests. propose a content interest disguise scheme that aims at minimizing the amount of cover traffic required for a given protection level by varying the value of m per cover content while preserving the disguise of the genuine downloads. evaluate the mistrustful p2p model for parallel downloads and heterogeneous internet connections. in such scenarios, evaluate the integration of variables such as the average block transfer time of each peer, the internet connection bandwidth and latency deviations, and the number of concurrent downloads into the backoff time equations. provide protection against link monitoring by encrypting communications between peers to extend the attack model in order to also consider isps and governments, which requires key exchange and distribution mechanisms. design an incentive mechanism to stimulate sharing."
"the first refers to the number of methods to hybridize. in principle, there is no limitations on how many heuristics should be hybridized, however, most proposed approaches in literature consider two-three heuristics. then, one has to provide the concrete methods to hybridize by choosing from the available heuristics for the problem (exact, local search, population-based, etc.) finally, and most importantly, one has to design the flow of the new hybrid algorithm. for this, the crucial point is the level of hybridization, which refers to the degree of coupling between the meta-heuristics, the execution sequence and the control strategy."
"in our future work we plan to implement multi-objective versions of metaheuristics such as multi-objective gas (mogas), to compute the pareto front, which would serve as a basis for a decision taking during the scheduling phase in grid systems."
"prevents user liability. the user shall not be subject to any liability as a result of legitimate usage of the privacy-preserving p2p system. the user shall be protected against actions of misbehaving nodes, or from the download of contents published with misleading description (misleading contents), which may drive users to unknowingly and unwillingly download unethical or illegal contents."
"here we present the decoding toolbox (tdt) that offers a flexible yet easy to use framework for decoding analyses in matlab (mathworks, natick, ma). tdt offers an interface with spm (statistical parametric mapping, http://www.fil.ion.ucl.ac. uk/spm/), the most common fmri data analysis software, but can also be extended to other fmri data analysis packages. tdt can be used by any researcher with basic knowledge in fmri data analysis and minimal programming skills, while at the same time allowing a large amount of flexibility in the application of complex decoding approaches for more experienced researchers. we briefly summarize the key features of the toolbox that should make its use attractive."
"the mistrustful p2p model reinforces plausible deniability by (1) not advertising what peers own or miss in order to defeat passive attacks of any size, and by (2) constraining the amount of blocks implicitly disclosed to other peers while sharing in order to thwart active attacks of a size up to c . unlike in other p2p systems, peers are not required to relay traffic on behalf of other peers, and therefore users are also not subject to indirect liability. proof of access to content data and proof of full content download are avoided through (1), (2), and by encoding contents in a way that only enables decoding after full download, preventing direct legal liability. attackers have then to engage in content sharing to know which blocks a peer owns, increasing significantly the computational resources required to launch attacks."
"all solutions require peers to advertise, either fully or partially, what they own or miss. an attacker may exploit download progress tracking to obtain a proof that the user is able to access a content either entirely or in part, and to narrow or even void plausible deniability. for unethical or illegal contents that can be accessed in part before fully downloading them, which is usually the case of multimedia contents, an attacker may obtain a proof of such access. as so, the user may be held legally liable for, unknowingly and unwillingly, downloading a single or a few chunks, be it due to traffic relaying or due to misleading content description. thereby, a legitimate usage of these systems may hold the user liable for copyright infringement."
"the remainder of this paper is structured as follows. section 2 depicts the main legal and ethical challenges at stake along with user liability considerations. section 3 briefly describes traditional p2p file sharing systems. section 4 characterizes the problem we aim to solve. the related work is presented in section 5 . section 6 describes the latest version of the mistrustful p2p model. sections 7 and 8 provide, respectively, the security analysis and the evaluation of our model. the results and discussion are presented in section 9 . section 10 draws the main conclusions."
"in this paper, we systematically investigated the applicability of several posterior probability estimation techniques in the context of terrain classification based on temporal coherence. we exploited temporal coherence using a bayes filter approach which takes several recent terrain class predictions into account. depending on the choice of the classifier and the distance, a robot has to navigate over a certain terrain type before a terrain transition occurs, the classification performance increased by up to 6.9%. this number denotes the increase of classification performance related to a classification approach based on individual observations only. we showed that the bayes filtering approach was nearly always superior to the single-observation approach with the only exception of the knn classifier at a travel distance of less than or equal to 0.6 m. the significantly best experimental results were obtained using a combined support vector machine and k-nearest neighbor approach which has not been employed in the domain of terrain classification so far. further investigation revealed that the various classifiers did not only differ in classification performance but also in the confidence of erroneous predictions. in the context of bayesian filtering this is an important issue since a decrease in this confidence results in a decreased influence of wrong predictions on the final classification."
"as quality measure we used the true positive rate (tpr). it is the ratio (measured in per cent) of the number of correct predictions for which the predicted class x t equals the actual classx t and the number of instances contained in the test set. we derived the prediction performance using 5-fold cross validation and averaging the true positive rate over all five folds. table 2 shows the results for the proposed classifiers when using single observations (so) and bayes-filtered posterior probabilities of recent predictions (ab). note that the true positive rate for the single observation-based approach differs between varying experiments. this is due to the model evaluation procedure introduced in the previous section which selects a varying test set for each travel distance. related to both the so and the ab approach, the svm-knn technique yields the best prediction performance, followed by svm, mlp, knn, and pnn classification ( fig. 2(a) ). the differences in the true positive rates of the applied classifiers proved to be statistically significant, using a two-tailed t-test at a significance level of 1%. the combined support vector machine and k-nearest-neighbor approach benefits from the reduced training set resulting in another configuration of the separating hyperplane. this hyperplane results in a higher generalization as compared to the one established by the svm approach which uses all training patters at once. the classification performance of each classifier is also reflected in the increase of the true positive rate obtained when using the adaptive bayes technique in comparison with the single observation approach (fig. 2(b) ): the larger the true positive rate for a given classifier, the larger the benefit of using temporally-filtered predictions."
"crucial health informatics is essential in order to manage the corresponding supply of services and is a fundamental requirement which would include the potential available level of capacity of elective care service supply, as well as the flow of patients into outpatient services on a weekly basis 10 . mobile health applications are expected to generate a revenue of us$23 [cit], an increase from us$ 15.3 [cit] 11 and these applications spans the spectrum of medical and operational services at medical organizations 12, 4, 13, 14 . in addition to the applications provided by vendors as components of complete solutions offers, 165,000 mobile health apps are also currently available on major commercial application stores, with about 46.000 reviewed apps 15 . one major use of smartphones apps is in outpatient clinics, where the focus over the last decade has been on the effective, uninfluential factors on waiting times, preparing for uncertainties and risks, better support for individuals who are dealing with workload, and dealing with variation in the demand and supply in the provision of elective care 10 . the prevalence of basic phones services, short messages service 'sms' have been used in supporting services such as appointment bookings, which reduced the fail to attend cases from 23.4% to about 14.2%"
"classifier selection should be handled with care since each approach has different characteristics. knns and pnns belong to the class of lazy learning techniques. that is, all computations are delayed until a prediction query is requested. on the one hand, this renders a time-demanding training phase unnecessary which is advantageous if the underlying phenomenon changes frequently. on the other hand, all patterns have to be available at run-time which might pose a problem if storage is limited. given that the acquired training set consists of n samples, storage requirements are o(n · d), where d is dimensionality of a training instance. furthermore, if the calculating capacity is constrained in the recall phase, the desired prediction frequency might not be accomplished due to a large set of training patterns. for example, when using the knn classifier, a naïve approach involves o(n) distance calculations to determine the k-nearest-neighbors. although accelerating data structures like m-trees [cit] exist, high-dimensional nearest-neighbor search is known to be a non-trivial task suffering from the curse of dimensionality. mlp and svm classifiers typically provide compact models, resulting in a fast prediction performance. model training, however, is computationally much more demanding since both methods iteratively try to minimize a given error function. the time spent on choosing a classifier with a good generalization behavior is significantly increased by the model selection process which has to consider a sufficiently large set of candidate model parameter settings. the svm-knn approach is characterized by an involved model selection and testing phase. since a class prediction also requires the determination of the k-nearestneighbor set to a given query, the training set has to be present at run-time. we note, however, that this approach still guarantees predictions performed in real-time. hence, we included svm-knn classification in our investigations. table 1 summarizes the key characteristics for the proposed classifiers: the respective model parameter(s) which yielded the best generalization and model selection times along with the number of tested model candidates. for the respective best classification model we further present the training time using data contained in one fold of a 5-fold cross validation scheme, the average testing time for a single query, and storage requirements (measured in kb). we performed all run-time analyses on a pentium d 3.0 ghz desktop pc. for the storage considerations, we represented each floating point number as double, each 8 bytes in size. table 1 the respective model parameter(s), the number of considered candidates during model selection, model selection and training times, prediction complexity, and storage requirements of the proposed classifiers."
"13 . presently with the advancement in hardware and software developed for mobile business applications and wireless network, it is expected that there will be 500 [cit] . in another survey it was suggested that 75% of web users in the uk search for health information, and more importantly, 90% would like to use an online gp appointment booking service and a service allowing them to ask a clinician's questions 4 . in saudi arabia, where the case study was undertaken, 84% of consumers find technology important in managing their health, and 40% use mobile health apps 9 . vitrucare for example is an application that reduced clinical practice contacts in outpatient's departments and a&e and reduced the acute admissions contacts, which declined by 53% after implementation, reducing cost in this case from £97,534 to £42,752 4 and other applications with similar positive results include apps such as ginger.io."
"in today's world, there are over 7.6 billion mobile connections (4.7 billion subscribers), with more than 1 billon 4g connection in 150 countries, with an estimated global subscribers base to reach 5.6 billion by the end of the decade, covering over 70% of the world's population 1 . according to the independent website of an analytics company statcounter, mobile internet usage worldwide has already surpassed the pc, and particularly in saudi arabia where the case study took place, mobile and tablet internet usage accounts for 60% [cit] 2 . additionally, benefits of mobile applications are recognised in reports such as the king's fund report, in which, in a list of technologies which are going to revolutionise healthcare, the smartphone was ranked number one 3 . cost reduction, improving outcomes and proactive and targeted care is achieved with help of smartphones applications such as real-time patient monitoring and analytics 4 . improving elective care and outpatient clinic's is vital for the future for various reason including financial cost reduction. in the uk for example, elective care accounts for about 18% of providers' total annual expenditure, and if outpatient is included, reaches 30%. in acute specialist trusts, it represents 34% of the activity, 23% in acute teaching trusts and 21% in district general hospitals 5 respectively. the selected case study outlined in this paper is in a well-recognised hospital is saudi arabia, that is representative of the situation in the ministry of health (moh) and it is well ranked medically but lags in the provision of integrated information tools. the kingdom of saudi arabia (ksa) has a land area of approximately 2.15 million km 2, with a population of 29.5 million people and has 256 hospitals with 49.000 beds in government hospital and 14.000 beds in the private hospital sector 6 . the saudi health care system ranks above many other international healthcare systems such as canada, australia, and new zealand with a ranking of 26th among 190 of the world's health systems according to the world health organization. however, there is increasing concern about the underutilization of electronic health systems in spite of the ministry of health (moh) allocating about us$ 1.1 billion for the development of electronic services 7, 8 . due to the large landmass of the country, and the resultant dispersed distribution nature of health care services and health professionals across the geographical areas in ksa 8, the demand is high for electronic medical services and elective health including outpatient clinics 9 . patients can experience long waiting lists and a dearth of services issues, particularly in groups such as the elderly, adolescents and people with special needs especially in rural areas where many people have difficulty to access healthcare facilities 8 . the remainder of this paper is structured as follows, in section 2, we present the concept of mobile health and application, with an outline of the potential integration between the proposed framework and the moh framework. section 3, outlines the proposed framework, followed by the case study application in section 4. section 5 shows the results which are discussed and analysed using the balanced scorecard (bsc), followed by the conclusion outlined in section 6."
"as a further contribution we examined the proposed classifiers with respect to their limiting factors such as storage requirements, prediction times, model generation times, and model selection times. the results provide criteria for choosing an appropriate classifier for a variety of hardware configurations."
"to not constrain the model to work at a certain driving speed, we varied the speed between 0.2, 0.4, and 0.6 m/s. in total, the dataset consists of 7635 patterns, corresponding to approximately 1.5 hours of robot navigation. we performed individual terrain classifications using vibration data acquired during 1.28 s of robot travel. for two consecutive segments we permit an overlap of 28 samples to achieve a prediction frequency of 1 hz. the combination of terrain class predictions was realized by our adaptive bayes filter approach introduced in sect. 2."
"in today's world, there are over 7.6 billion mobile connections (4.7 billion subscribers), with more than 1 billon 4g connection in 150 countries, with an estimated global subscribers base to reach 5.6 billion by the end of the decade, covering over 70% of the world's population 1 . according to the independent website of an analytics company statcounter, mobile internet usage worldwide has already surpassed the pc, and particularly in saudi"
"in our experiments, an xsens mti altitude and heading reference system was mounted on an aluminum plate on top of our rwi atrv-jr outdoor robot to measure vibration signals in left-right direction at 100 hz. during data acquisition, the robot navigated over five different terrains ( fig. 1) : indoor pvc floor, asphalt, gravel, grass, and clay (the surface of a boule court)."
"in general, the reasonings in jaśkowski-fitch notation can have nested subreasonings, i.e. ones that are enclosed within boxes created by quantifier rules. the graph model presented here does not take into account this structure. however, the np-hardness proofs presented in this paper also apply to the appropriate generalization, which includes the nested structure, as they are its special case."
"it is desirable to have a \"push-button\" tool that automatically finds a suitable topological sortings τ of a natural deduction proof so that they have the largest number of \"then\" τ -steps but we show here that this goal is non-trivial since the problem is np-complete."
"in cpn, the steady-state of random neural networks running on network routers decides network paths. random neural networks (rnns) [cit] b; [cit] ) are trained continuously by enhancing network flows with packets that monitor performance. these packets store information about the network state as they cross the network from a source to a destination, and get updated at each hop in the path. in general, cpn blindly trusts the information carried by packets, but a network attack could result in some routers falling under the control of a malicious user, and as a consequence packets may be subject to tampering. the problem is aggravated as routing decisions in cpn are distributed. in this paper, we examine potential weaknesses of cpn and the use of rnns in relation to confidentiality, integrity and availability, and identify possible solutions that could enhance the resilience of cpn to representative security threats."
"the header of the cpn packets has been modified to allow the packets to gather network information according to the specified qos goal. therefore, as packets travel the network, they store qos data (such as timestamps, counters, etc.) in a special data storage area of the packet header known as the cognitive map (cm) [cit] . when a packet arrives at its destination, an acknowledgement (ack) packet is generated which stores the route taken by the original packet, and the measurements it collected during its journey. the ack will then return along the reverse route. at each hop the ack visits, it deposits information in a special short-term memory store called mailbox. when it finally reaches the source, the ack establishes the route that the dps will follow."
"thus then thesis by a4, vectsp_1:def 10; end; figure 6 : a linearisation of the proof presented in fig. 4 . this linearisation maximizes the number of \"then\" τ -steps."
"goal: develop a regression model explaining why a particular prediction was made at the level of specific examples/samples strategy: inverse regression and sufficient reduction in the \"abundant\" feature setting."
the following short analysis of this property shows that it is impossible to apply several proof readability criteria at the same time and that they are sometimes contradictory. a commonly accepted solution in such circumstances is to impose a hierarchy on the optimized properties (this additional property and several others are briefly described in section 7) .
"in this paper, we focus on another, still underdeveloped approach. models of cognitive perception of read material stress that locality of reference is a significant factor in the process of understanding. this can be summarized with the behaghel's first law that can be stated as follows, elements that belong close together intellectually will also be placed close together [cit] . this law of local reference is also recognized in modern scientific literature concerning human perception [cit] . the significance of the locality of reference is exploited in the current study. we focus on a particular proof step in a proof script. the step uses some information that was derived before in the proof. this information can be located somewhere far away in the proof or within a close neighborhood of the step. with behaghel's law in mind, we assume that a step where at least part of information it requires is available in the directly preceding step is more comprehensive than a step in which all information is far away in the proof (as a significant part of assumptions can still be in the, so called, working memory, see e.g. [8, chapter 1] )."
"the locality of reference principle is the basis for a method of proof distillation that is presented in this paper. since the steps that refer to the preceding step are perceived as more comprehensive, the procedure transforms the proof scripts so that the number of steps that violate the requirement is minimal. the transformation consists of two actions. the first one translates a linear representation of a proof script to a representation in the form of a directed acyclic graph and the second one chooses a different linearisation of the proof so that the number of proof steps that are, according to the presented principles, difficult to comprehend is minimal."
"to be precise, the mizar system will not accept many of the reasonings given by the construction in the proof. the reason is that the real system has a restriction on the number of references that can be used in the justification of a single step. in the official mizar distribution this limit is 25, and this corresponds to the limit for the in-degree of every vertex in abstract proof graphs. this restriction is only used to bound the time of correctness verification of a single step and so far there was no need to increase this limit."
"the usual solution to ensure network confidentially is to encrypt end-to-end communications, and this approach can be applied also to cpn. in the context of a cpn implementation, at least two alternatives are possible. data encryption could occur before data is passed to cpn for transmission and after cpn delivers the data ( figure 3 depicts the transfer time that was measured for files of the indicated length, with or without encryption. the source and destination nodes (which implemented the encryption and decryption process) run with a clock rate of 2.8 and 2.4 ghz respectively. the path connecting the source and destination was unique to ensure obtaining consistent results for all tests, and it consisted of 5 hops. the encryption was realised via secure sockets (so the programming was external to the cpn module). the results indicate that the encryption and decryption process add about one additional second to the total end-to-end transfer time. this additional delay to the file transfer is the penalty caused by the extra computation that was needed."
"hijacked routers behaved maliciously by directing smart packets to the worst possible next hop as an attempt to increase the end-to-end flow cost or to prevent route discovery. because cpn can easily isolate those routers, the difference of using the aforementioned goal functions to train the rnn was very small. nevertheless, we did observe slightly higher loss (10 −4 ) ratio when only cost was used. during the experiments, a number of core routers were assumed to be under the control of an attacker. routers will drop a percentage of all the incoming normal traffic. figure 8 depicts the measured ratio between the packet delivery rates for the case when the rnns were trained using a cost-packet loss goal vs. the case of rnns only trained with a cost-based goal. it can be observed that as the number of hijacked routers increased, the comparative advantage of using a combined cost-loss goal diminished from about 20% to 5%."
"1. introduction 1.1. motivations. the readability of formal proofs has significantly influenced development of formalization. the idea of formalization was recognized by david hilbert in his program of formalization of mathematics -research in his program led, in particular, to the concept of the universal turing machine. hilbert's project in his most idealistic form collapsed due to gödel's incompleteness theorems. in spite of them, a group of mathematicians under a single pen name nicolas bourbaki tried to formalize an exposition of modern advanced mathematics using first-order logic. this project was also abandoned because the resulting formal proofs turned out to be too obscure and the formalization in such a detailed form started to be perceived as hardly readable [cit] and as a result impractical. moreover, checking the correctness of formal proofs turned out to be as difficult as the reading comprehension of these proofs. not only bourbaki's project was faced with the readability problems. the earlier efforts of whitehead and russell in the formalization of mathematics had the same problems. the formal definitions and proofs that they created were deemed cryptic as the approach to mathematics they used was extremely rigorous and precise. the formalization has become feasible only after increasing availability of computers. these problems are described in more detail by zammit [cit] ."
"step 2. we describe here a property of arcs in g(p(g, e), π) whose both tail and head cross different gadgets. let p 1, p 2 be an arc of g(p(g, e), π) such that"
"a failure detection element was proposed to improve the performance of cpn in terms of packet delays and packet loss during failures. with this enhancement, at each rnn and for each neuron i, the timestamp of the last sp and the last ack that used it, are stored. if no ack has been received after sending the last sp then the link is considered \"under failure\" and the neuron corresponding to this link is considered \"expired\". expired neurons do not participate in the calculation of the excitatory probabilities and the subsequent decisions of the rnn. the enhanced cpn was also compared with the open shortest path first (ospf) routing protocol and was shown to perform better in such situations."
"another way to impose a restriction on the distance between premises and their uses is to minimize the maximal distance between an arbitrary premise and its use. this distance for the reasoning presented in fig. 6 is equal to 7 (the distance is reached between lines 2 and 16 for the label a3). the smallest restriction of maximal distance in this abstract proof graph is equal to 5 and is obtained in 128 cases, but optimal values of previously mentioned parameters are not always possible to reach among them. more precisely, the number of \"then\" τ -steps is at most 12 (the optimal number is 12), the number of reference arcs between any two different maximal τ -reasoning paths is at least 7 (the optimal number is 5), the sum of distances between premises and their uses is at least 39 (the optimal number is 38), and the number of labels in a reasoning is at least 6 (the optimal number is 3)."
"one of the most popular methods to make it possible for computers to assist in formal reasoning is based on bringing the formal mathematical language to the informal one by introduction to the formal language idioms that stem from informal mathematical practice. attribute that appears in the statement of a proof step. it is worth to note that the mml script authors often use the same symbol to express different things, e.g., * and + are defined or redefined for different notions in mml more than 130 and 160 times, respectively. therefore, it comes as no surprise that in scripts with a more complex environment, even the authors have a problem in recognizing the symbol variant they used."
"availability refers to the security requirement for information and network services to be available to their legitimate users, and may be affected if intermediate routers behave erratically (the case of hijacked nodes running attacker's software) or stop working, possibly due to a denial of service attack. availability in cpn has been investigated previously in terms of the propagation of worms that disable nodes [cit], denial of service attacks [cit] and misbehaving routers [cit] . interestingly, as we will see, the self-adapting behaviour that is natural in cpn constitutes and inherent strength of the algorithm against all these threats."
"in this paper we consider several problems arising in the course of improving readability of proof scripts. we concentrated on two methods of improving proof readability based on behaghel's first law. for these two methods we have found a common, unexplored problem ahp concerning graph partitions. this problem is similar to the problem of finding a partition of dag into at most k node-disjoint paths, which is solvable in polynomial time [cit] . with this in mind we can observe that the requirement of acyclicity imposed on the members of the partition is essential. precisely this is the difference here."
"(i) minimization of the largest distance between a premise and its use measured as the number of steps between these two steps in a linear representation of a proof script, (ii) or minimization of the sum of all distances between every premise and its use."
authors that cooperate on a common formalization analyze each other's achievements (e.g. over 40% of mml proof scripts have more that one author). therefore it comes as no surprise that formal systems develop in many directions that improve their readability.
"the dag p(g, e). in the construction of the digraph p(g, e) we use a one-to-one function e, but its choice does not affect the properties of the considered digraph."
"at each node a specific rnn, that has as many neurons as the possible outgoing links, provides the sp with the routing decision in the form of an output link. this output link corresponds to the most excited neuron, and since the rnn has a unique solution for any set of weights and input variables, this choice is also unique. the learning process used with rnn is reinforcement learning, which uses the observed outcome of a decision to reward or punish the routing decision, so that future decisions are more likely to improve or maintain the desired qos goal."
"until now, we have concentrated on the number of steps that refer to the preceding step. maximization of sequences of this kind is not the only strategy to improve readability. the topological sorting presented in fig. 6 has additional important features that are not consequence of behaghel's first law, but are related to readability. in particular, it is one of two linearizations that have the smallest sum of all distances between premises and their uses, equal to 38 (the second topological sorting with this property contains 11 \"then\" τsteps). additionally, 4 labels have been used in this reasoning, but it is not the smallest number of labels in existing linearisation. if we swap lines 6 and 2, then the label a2 is not necessary. among all sortings of the abstract proof graph in fig. 5 the smallest possible number of labels is 3. this is achieved in 8 cases. however, none of the cases minimizes the sum of all distances since the sum there is at best 41. notably there are, among the 8 cases, ones that contain exactly 12 \"then\" τ -steps, which is the maximal possible number for the abstract proof graph. this illustrates that there are proofs for which all plausible readability optimalization criteria cannot be met at once."
"more specifically, cpn uses three types of packets; smart packets (sp) for discovery, source routed dumb packets (dp) to carry the payload and acknowledgement (ack) packets to bring back information that has been discovered by either sps or dps. this information is used in each node to train random neural networks (rnns) [cit] b) and produce routing decisions. at each network node sps are routed according to the measured experiences of previous packets with the same qos goals and the same destination. in order to explore all possible routes and account for sudden network changes, each sp might make a random routing decision instead of the one calculated by the rnn, with a small probability (usually 5 − 10%)."
"smart packets were sent as a 0.1 fraction of normal packets to maintain path adaptation if needed in the case of a change in network conditions. at each hop, smart packet used a rnn to decide the next hop except for 10% of the decisions, which used a random neighbour selection that helped to facilitate network exploration and the discovery of alternative paths to destinations. because of the use of virtualized resources in our tests, we opted for introducing smart packet routing based on router costs, which were randomly selected at the beginning of the experiment, in the range 1-100. for any given source-destination pair, smart packets had in general the task of finding the path with the minimum cost (the sum of the router costs involved in a path) or, if requested, the path with the minimum combination of cost and packet loss. the penalty value p was fixed to 100 units."
"deliberately degrade or altogether deny legitimate network service. there is therefore an increased need for network stability and reliability which has led to the growth of autonomic networks that use qos-driven approaches for greater stability and reliability in communications. the cognitive packet network (cpn) that was introduced by has been shown to provide good network adaptation properties under varying network conditions and user requirements. its performance has been investigated extensively and for a variety of performance metrics, but most studies have considered operating conditions where there is no malicious intent or security threats limited to availability attacks through worm propagation or network denial-of-service [cit] ."
"here we investigate potential weaknesses of cpn falling under the triad of common security aspects: confidentiality, integrity and availability [cit] and propose possible solutions. public and shared networks offer greater security challenges than private and dedicated networks. we will make no assumptions about the type of deployment, so both the discussion and results will be applicable to both cases. however, the discussion will be centered only on network weaknesses and will exclude particular issues at endpoints."
"experiential tools that try to realize the second approach were created by pąk for the mizar system [cit] . the impact of these tools, which are based on greedy algorithms, has been already recognized by the community of mizar authors [cit] . additionally, these techniques of proof script modification are taken into consideration in other systems as an effective way to refactor machine proofs [cit] . however, techniques that try to optimise proof readability criteria in most cases correspond to problems for which complexity is still an open problem."
". its expectation is y. assuming that the errors of all sufficient reductions are independent, the problem to find the optimal weights for the convex combination with the minimal variance can be formulated as"
"the inherent self-adaptive properties of the cognitive packet network constitute a significant advantage for helping it withstand attacks against its availability. its selfadaptation becomes even more effective after introducing packet losses into the goal formulation for training the distributed random neural networks that are used in the routing algorithm. a denial of service defence mechanism with a given set of detection probabilities becomes more effective if applied in conjunction with the dynamic routing of cpn. yet, similarly to the internet protocol, cpn was designed based on trust. by trusting the information carried by packets and that no node could be compromised, cpn becomes vulnerable to confidentiality and integrity attacks. it offers, by default, little support to ensure end-to-end confidential delivery of data or integrity of packets, which is of crucial importance given that it relies on real-time packets' information to effectively setup and maintain paths. here, we have proposed extensions to the cpn protocol to explicitly address these weaknesses via the introduction of encryption and digital signatures. we have detailed the technical implementation of these mechanisms and argued that the benefits from strengthening the confidentiality and integrity, along with the availability of information transmitted through cpn, outweigh the disadvantage of the associated delays that are incurred."
"let us focus on a statement of the theory of fields that the inverse of a product of two elements of the field is equal to the product of their inverses. note that symbols 1.f, 0.f represent the additive identity element and the multiplicative identity element of the field f, respectively. note additionally that the system mizar uses only ascii characters. operations such as \" −1 \", \"·\" are represented in mizar as \" and *, respectively, and vectsp_1, group_1, rlvect_1 are identifiers of mizar articles [cit] . the proof script presented in fig. 4 does not comply well to the behaghel's first law. only one step (line 17) refers to the preceding step, the other steps in the reasoning refer to steps placed far away in the proof script. analyzing the possible topological sortings of the corresponding proof graph (see fig. 5 ), 359598 cases, we get that the greatest number of \"then\" τ -steps is reached in 16 linearizations and is equal to 12. hence the smallest number of maximal τ -reasoning paths is equal to 5. in 6 cases of these 16, we have also the smallest number of reference arcs between any two different maximal τ -reasoning paths, which is equal to 5. one of these 6 cases is presented in fig. 6 ."
"these \"consider\" steps introduce new constants. each step that uses such a constant is a head of a non-reference arc, the tail of which is in the corresponding \"consider\" step. for instance consider x be set such that ai: φ(x) by aj ;"
"according to the above theorem, ahp is np-complete. additionally, vertices of the constructed digraph p(g, e) have restricted in-degree and out-degree by 2. hence we finally have the following theorem."
"we suggest two possible implementation approaches. in the first case, we differentiate core routers from edge routers (those connected to the end users), with edge routers signing and verifying signatures for each flow. in a second case, all routers will participate in the signature verification process. the advantage of the former case is speed whereas the second provides greater resilience, as attacked packet could be removed earlier from the network. another difference between the two is the method of distribution of the public key. in the first case only edges will be required to know the public key of edge routers. in the latter case, all router will have to have access to those keys."
the notion of topological sorting is difficult to manipulate and makes various formulations obscure. therefore it is useful to express these problems in terms of acyclic partitions. following borowiecki and mihók [cit] we use the following notation. let p be a graph property.
"to be more precise, the sum of the number of \"then\" τ -steps and the number of maximal τ -reasoning paths is constant and equal to the number of reasoning steps. hence the number of \"then\" τ -steps is maximal if and only if the number of all maximal τ -reasoning paths is minimal. this means we can equivalently formulate many definitions in terms of the number of \"then\" τ -steps and in terms of τ -reasoning paths. in addition, we would like the flow of information between any two different maximal τ -reasoning paths to be minimal. this is equivalent to the property that the number of internal τ -arcs, i.e. the reference arcs whose head and tail belong to the same maximal τ -reasoning path, is maximal. it is true since the sum of all reference arcs between any two different maximal τ -reasoning paths plus the number of internal τ -arcs is constant and equal to the number of reference arcs."
"1.2. related works. the experience of big proof formalization developments shows that formalized results are often used as libraries, which requires reading proof scripts [cit] . but there are many more reasons that force formalization authors to read existing proof scripts. new users often use the available library as an in-depth tutorial [cit] . experienced users adapt or modify existing proofs to obtain stronger theorems [cit] (actually this is mentioned in page 3 of an unpublished preliminary version of the article [cit] )."
"we prove that this subsequence is a cycle of the graph v, e \\ f which contradicts the fact f is a feedback arc set of g."
"to experimentally evaluate this case, we have deployed a cpn testbed and conducted a number of trials and observations assuming that some of the core routers were compromised by an attacker, and therefore, had an abnormal behaviour."
these non-reference arcs are represented in the reasoning with the help of a special kind of steps in the mizar system. this special kind of steps in the general case has the shape:
"desired: relevance of individual covariates at the level of specific samples for a given regression task. challenge (\"chicken-or-egg problem\"): relevance/confidence score to individual covariates x j should condition the estimate based on knowledge of all other (uncorrupted) covariates x"
"we show in theorem 7.1 below that the problem 5th mil is np-complete. however, the subfamily of abstract proof graphs considered in the proof of the theorem cannot be realized by proof scripts in mml due to an additional syntax restriction of mizar. in the mizar system a step of reasoning which introduces variables satisfying a statement which is used further in the script (see (6.2), the variable x satisfies φ(x)) must be decorated with a label (ai). this condition can be expressed in terms of graphs as follows: every vertex that is both a tail of some reference arc and a tail of some non-reference arc corresponds to a proof step with a label. as a result the minimalization must take the following form."
"integrity refers to the security requirement for information and systems not to be altered without the authorisation of their legitimate owners. as mentioned previously, cpn does not normally encrypt packets, but when done as described earlier, it could also help to (at least partially) address integrity issues given that any alteration of a packet's payload by an attacker will trigger decryption errors at the destination. unfortunately, not only the users' data could be the target of an attacker, but also the nodes' identity (by changing values in the packet header) and, in the particular case of cpn, network status data and paths (by altering the cognitive map). attacks to the cpn header or cognitive map can directly affect the ability of the cpn algorithm and the rnn to effectively respond to network and user behaviour changes, and can also affect the quality of the routing decisions. by changing the packet header or cognitive map, which stores real observations of the qos that could be achieved by the network, a malicious user could impact the rnn training process, and therefore tweak the routing decisions to cause unexpected behaviour."
"in section 2 we introduce the notion of an abstract model of natural deduction proofs and we discuss selected methods to improve readability. in section 3 we restrict our attention to two problems of improving the legibility of natural deduction proofs that have origins in behaghel's law. then in section 4 we formalize these problems in terms of the acyclic partitions and in section 5 we show that these problems are np-complete, since minimum feedback arc set problem can be transformed to them. in section 6 we present a family of abstract proof graphs for which there exist proofs written in the mizar system that have structures described by these graphs. section 7 contains a brief overview of other problems associated with improving proof legibility and the study of their complexity. finally, section 8 concludes the paper and discusses the future work."
"a confidentiality attack could take place either at hijacked routers or links. in the former case, an attacker could have gained access to a network node on the path from the victim's flow and forward a copy of the traffic content to a collection point. in the latter case, packet sniffers could have been installed on a network link to eavesdrop incoming network traffic. an interesting point to note is that the self-adapting path behaviour of cpn could partially prevent a confidential attack given that not all packets for a given communication may need to pass through the same set of links and nodes unlike traditional networks. a path adaptation may deviate packets away from the hijacked routers and links. however, there is no way to ensure this will happen given that it will be very difficult to detect the packet sniffing process."
"the abstract notion of proof we deal with here is based on natural deduction created by s. jaśkowski and f. b. fitch [cit] . as an illustration, let us consider an example written in this notation, which is presented in fig. 1 . in the same way as m. hurt and m. ryan [cit],"
"thus then thesis by a14, vectsp_1:def 10; end; this example illustrates the significance of \"then\" construction in the process of improving proof scripts legibility. this construction in natural way focuses the reader's attention on sequences of \"then\" τ -steps. note that every sequence of steps that has a label connected only to the last step of this sequence (as is the case of the sequence of \"then\" τ -steps starting in step 3 and ending in step 13 in fig. 6 ) contains additional information for readers. naturally, the previous unlabelled steps have only local significance, i.e. each of them is used exclusively to justify the directly following step. this is a sign for readers that they can \"encapsulate\" the piece of reasoning and abstract away from its content in many situations."
transformation of vc to fas has been considered by r. m. karp [cit] . using this transformation we can prove that fas is np-complete even for digraphs in which all vertices have in-degree or out-degree equal to 1.
"other ways for this to happen is by changing the actual information that is stored in the mailboxes of the nodes, by altering the rnn weights, or even by replacing the rnn-based algorithm with an arbitrary one. attackers could gain control of one or more nodes in the network and easily change the rnn algorithm behaviour, either directly or indirectly through manipulation of the mailbox entries."
"in section 2 a special kind of dag was proposed to be a simplified model of natural deduction proofs. the considered method describes in a simple way a method of construction of the abstract proof graph for every reasoning that does not contain nested subreasonings. this demonstrates that the model is sound. it is natural now to give an argument that it is also complete, which means in this case that for every abstract proof graph there exists a reasoning whose structure is that graph. now we show that this question has a positive answer."
"an orthogonal approach to improving the legibility of formal proof scripts is based on popular practices that solve the problem of illegible long reasoning in informal mathematical practice. this approach is generally based on two methods. the first one consists in finding (often less important) fragments of reasoning and then extracting them in the form of a lemma, or encapsulating them at deeper levels of nested proofs. the second ones tries to reorganize the order of independent steps focusing mainly on premises used to justify proof steps. the first experiments with the first method were carried out generally to reduce the size of mechanically generated proofs [cit] . it is generally known that theorem provers often justify the same goal and in exactly the same way several times. therefore, it is not surprising that the effort to extract similar proofs as lemmas is often undertaken. however, such modifications are generally carried out without the emphasis on the legibility of obtained scripts. only the initial studies on properties of reasoning passages that in the reader's opinion determine fragments that deserve to be called a lemma were conducted [cit] . additionally, there were considered dependencies between such properties and the complication level of statements that describe reasoning in selected passages."
"other three methods of improving proof readability are also solved but their solution was much simpler. these three methods generalize the previously described and solved graph problems. additionally, many approximate solutions of these problems have been already found that can be adapted to the proof script case."
"dynamic feature weights for three tasks: ambient temperature prediction (left), age estimation (middle) on lifespan database [cit], and ad classification (right). ad classification accuracy of 86.17% by simple thresholding of continuous prediction by air comparing to svm+pca (80%-85%) [cit] . air provides a way to determine, at test time, which features are most important to the prediction. our results are competitive, which demonstrates that we achieve this capability without sacrificing accuracy."
"confidentiality is breached when information, held or transmitted through the network, is disclosed to unauthorised individuals or systems. ensuring total confidentiality in a network is normally very difficult given that packets need regular handling by routers that are managed by a third party, and outside the sender and recipient's domains. in that sense, a cpn is as vulnerable to confidentiality attacks as other types of networks. up to now there has not been any security mechanism in cpn that could guarantee a confidential end-to-end delivery of users' data."
the next step in the process of improving proof readability should be finding the algorithms that approximate the problem ahp as well as the algorithms that approximate problems 1st mil and 2nd mil. it is also important to find heuristics that quickly give satisfactory approximate solutions to all five mil problems. this should enable the possibility to make a decision which methods are more appropriate in particular cases.
"naturally every sequence of \"then\" τ -steps determines a linear fragment of reasoning in the proof script that we call a τ -reasoning path. more precisely, let us fix a directed path improving legibility of natural deduction proofs is not trivial 7 5"
"as an illustration of \"then\" τ -steps and τ -reasoning paths, let us consider a short quasi proof script written in the mizar style. a theorem in the mizar style has the form:"
"additionally, p is said to be a maximal τ e 1 -path if and only if v(p ) is not included in any other set of vertices of a τ e 1 -path. let"
"and call it a partition of g determined by τ with respect to e 1 . for subsets v 1, v 2 of v we use the following notation:"
"represents a cycle of the dag g(p(g, e), π), but g(p(g, e), π) is acyclic since π is an h ( * ) -partition of p(g, e), a contradiction. this finishes the proof, that f is a a feedback arc set of g."
"to illustrate the problem, consider a communication flow from nodes a to b as depicted in figure 4 . a normal communication flow will include packets listing in their cognitive map the path a, c, b (step 1). if node c becomes under the control of an attacker, it could rewrite the cognitive map of the packets and forward them to an arbitrary node d (step 2). the purpose could be either to gain access to the packets' contents or to simply affect the quality-of-service of packets (e.g., increase their endto-end latency). after being intercepted by node d, the packets could continue their normal path (step 3). as usual, the destination will create acknowledgements and sent them on the reverse path (step 4) until they reach the attacker's nodes d and c (step 5). node c could then rewrite the cognitive map back to the original (step 6) to conceal the attack from the source. several variations of this kind of attack could occur, for example, rather than forwarding packets to node d, node c could simply write untrue values in the cognitive map metrics perhaps to indicate better qos metrics on paths passing through c so that packets become easily available to the attacker. the key cpn weakness in these cases is in having absolute trust in the distributed cognitive map handling. to prevent such kinds of integrity attacks to the cpn header and cognitive map, we propose introducing digital signatures both at the source (for normal packets) and destination (for acknowledgements). asymmetric cryptography is a suitable alternative for this task given that it removes the need of secret key distribution. under this scheme, cpn routers will be required to have both a private and public key."
"proof development in formalized mathematical frameworks is similar to program development. in both cases the content is created in an artificial language that has precise meaning executed by a computer system. therefore it comes as no surprise that both activities have much in common. since readability of programs has a significant impact on their maintainability [cit] it is strongly expected that readability of proof scripts has a significant impact on proof maintainability. indeed the maintenance tasks take place in current formal proof development, e.g. novice users of the formal proof development tools often follow the existing formalizations, and the formalizations are subject to refinement for instance when a generalized version of a theorem is to be proved. this concerns especially systems such as isabelle/isar [cit] or mizar [cit] where the proof script language is close to the natural language."
"the signature generation and verification process is depicted in figure 5 . the source (end-node or edge router) will generate a message digest of the cpn normal (dumb) packet header and partial cognitive map (including entries that are not expected to change). the message digest will be then encrypted using the source's private key and inserted into the extended cpn header. the extended packet is then sent over the cpn using its regular packet forwarding mechanism. at the receiving end (or intermediate hop, depending on the scheme being used), the signature will be regenerated from the receiving header and cognitive map, and compared with the signature in the packet. if they do not match, something in the header of the cognitive map must have changed on the way, thus indicating the packet has been tampered and should be discarded. the signature generation and verification will add additional delay to a normal endto-end packet transmission. measurements of these additional delays are depicted in figure 6 for packets of different length. we have observed that signature generation took around 6.5 ms, signature verification was faster at 3.6 ms, and that the packet length has little impact on the execution time of the processes. the measurements were obtained on a 2.8 ghz machine."
"in order to systematically test the generalization capability of the model, the results are validated using a 4-fold crossvalidation scheme. it is important to note that randomly dividing the full dataset into training and test sets would yield biased results, since the test set would very likely contain images from the training scenario. to ensure an independent and thorough validation, the set is divided such that the model is trained with data from a group of three scenarios, and evaluated on the remaining fourth scenario ( table 3) this guarantees that the test scenario is independent of the scenarios used for training the model. this is done for the four set combinations, and the final accuracy is the average of the results from each fold. fig. 4 depicts the four scenarios and the corresponding radar images after each patch has been classified."
"while different models can be trained for each scenario location to deal with this phenomenon, the complexity of such a solution is considerable and requires a very large data set to enable the different models to account for the variability in diverse scenarios and hence to prevent overfitting."
"brain-machine interfaces (bmis) have the potential to dramatically improve the quality of life for individuals with motor paralysis [cit] . the activity of neurons in motor and sensory areas of the brain have been used to control the movement of computer cursors and robotic arms [cit] . there is an ongoing debate, however, about the best sources for these control signals [cit], and the best method for transforming cortical 4 author to whom any correspondence should be addressed."
"each session began by quantifying the cell's responses during an isometric, eight-target wrist torque-tracking task in the flexion-extension and radial-ulnar plane (figure 1). isolated cell activity was discriminated on-line using templatematching software (alpha omega msd). in subsequent brain-control trials, cell activity drove cursor movement in one dimension. inter-spike intervals were averaged over a 0.5 s sliding window to create a continuous signal for cursor position. if the cell was directionally tuned, the rate targets were aligned with its preferred direction. for untuned cells, either the left or right screen position was arbitrarily chosen to represent high discharge rates for visual feedback."
"where y i is the intensity of output pixel y in output channel i. spectral unmixing using unsupervised learning at its core, spectral unmixing is the task of decomposing mixed multichannel images into spectral signatures and abundances of each signature in each pixel [cit] :"
"here our goal was to determine the degree of volitional control over arbitrary neurons recorded from primary motor and sensory cortices. in contrast to previous work, we also explored the monkeys' ability to sustain discharge rates within high-and low-rate targets, and quantified improvements across days when recording stability permitted. we found that monkeys can learn to control nearly all neurons tested, with success largely independent of the strength of directional tuning observed during movement. thus, relatively direct connections between cortical neurons and bmi controls or muscle stimulation may be a useful complementary strategy to decoding of movement intention, especially after paralysis."
"in conclusion, we presented a blind and flexible tool for fluorescence image spectral unmixing-lumos. both experimental and synthetic results demonstrated its ability to robustly separate mixed fluorophores in terms of the quality of results and ability to converge in a variety of scenarios. the lumos method has also greatly expanded the fluorophore options beyond the number limit of detectors and excitation lasers. these qualities make lumos a simple, general, and reliable spectral unmixing approach to quickly apply to any fluorescence images. last but not least, an optimal strategy for spectral unmixing should always combine image processing algorithms with careful dye selections and rigorous image acquisitions. lumos can be coupled with spectral imaging or other hardware designs to yield excellent multi-color imaging results, and will offer new avenues for understanding the complex biological organizations."
"differential evolution is a strategy that optimizes a dilemma by iteratively trying to enhance an individual solution with regard to a specified gauge of excellence. de algorithm is used for multidimensional real-valued functions but it does not put together the ascent of the problem being optimized, which means de does not have need of that the optimization problem to be differentiable as is mandatory for traditional optimization methods such as gradient descent and quasinewton techniques. de algorithm optimizes a problem by considering a population of candidate solutions and generating new contestant solutions by combining existing ones according to its straightforward formulae, and then memorizing whichever candidate solution has the superlative score or fitness on the optimization problem at hand. thus in this manner the optimization problem is treated as a black box that simply makes available a gauge of quality specified a candidate solution and the gradient is for that reason not considered necessary. de has a number of strategies based on method of selecting the objective vector, number of difference vectors used and the crossover type [cit] . here in this paper de/rand/1/bin scheme is used where de stands for differential evolution, 'rand' indicates that the target vector is preferred haphazardly, '1' is for number of differential vectors and 'bin' notation is for binomial crossover. the popularity of differential evolution is due to its applicability to a wider class of problems and ease of implementation. differential evolution has properties of evolutionary algorithms as well as swarm intelligence. the detailed description of de is as follows:"
"memetic algorithms (ma) characterize one of the most up to date mounting fields to do research in evolutionary computation. the name ma is now a day commonly used as coactions of evolutionary or several population-based algorithms with separate individual learning or local development events for problem search. sometimes ma is moreover specified in the text as baldwinian evolutionary algorithms (ea), lamarckian eas, genetic local search strategy or cultural algorithms. the theory of \"universal darwinism\" [cit] to make available a amalgamate structure governing the evolution of any intricate system. in particular, \"universal darwinism\" put forward that development is not restricted to biological systems; i.e., it is not limited to the tapered circumstance of the genes, but it is applicable to almost all multifarious system that show evidence of the principles of inheritance, disparity and assortment, thus satisfying the individuality of an evolving system. for instance, the new science of memetics symbolizes the mind-universe corresponds to genetics in way of life progression that stretches transversely the fields of biology, psychology and cognition, which has fascinated significant concentration in the last two decades. the term \"meme\" [cit] as \"the basic unit of cultural transmission, or imitation\", and in the english dictionary of oxford as \"an element of culture that may be considered to be passed on by non-genetic means\"."
"the image patch is fed to the network input layer and the x coordinate of the patch centroid is injected as an additional node at the input to the fc layers. since different sensor setups can yield different absolute values of the spatial coordinates (e.g., due to the different relative position of the sensor w.r.t. the row of cars), the patch coordinates are normalized to the maximum value of the lateral coverage (typically 10 m) to achieve regularization."
"deep learning techniques have undoubtedly been playing a major role in multiple applications within the field of machine learning in recent years. one of the key characteristics of these techniques is their ability to learn features from the data with limited or no human intervention, as opposed to other machine learning techniques [cit] ."
"to test the performance of lumos on a sample with more fluorophores than detectors, we first imaged mixed beads with 5 different fluorophores: ly (light yellow dye from spherotech inc.), fitc, pe, purple (purple dye from spherotech inc.), and apc ( fig 4a) . the theoretical emission spectra are shown in fig 4c. simultaneous two-photon excitations at 800nm (maitai laser) and 1100nm (insightx3 laser) were used to excite all fluorophores [cit] . because of the significant emission spectra overlaps of ly and fitc in the green channel, pe and purple in the red channel, and pe, purple and apc in the far-red channel, the raw images collected by the 4 detectors ( fig 4a) showed many beads appearing in more than one channel (examples are indicated by white arrows in fig 4a) . the spectral signatures of those fluorophores ( fig 4d) were consistent with the emission spectra information in each channel, which demonstrated the uniqueness of each fluorophore's intensity distribution across the 4 detectors. we therefore applied lumos with 6 clusters to the raw 5-color beads images. the algorithm generated 6 new images in which 1 image included all background pixels and the other 5 images each represented one single fluorophore. we removed the background to get the clean unmixed outputs ( fig 4b) . the algorithm performed well to fully separate out the 5-color beads with individual beads belonging only to a single output channel."
"unlike linear unmixing [cit], one of the major assumptions of the lumos algorithm is that one pixel is uniquely labeled with one fluorophore, which is advantageous in the way that it provides unambiguous results especially in biological imaging (examples in figs 3-5 ). however, in biology, one structure is often labeled with more than one fluorophores for colocalization studies. the structures with colocalized labeling will exhibit a distinct spectral signature, which is usually the combination of, but is different from, the individual fluorophore's spectrum. by leveraging this, lumos is able to treat the colocalized fluorophores as an additional cluster, and separate out the pixels with colocalization."
"to prevent the clustering algorithm from being biased by signal intensity differences and variations between channels, the brightness and contrast of input data were normalized to be relatively spherical distributions before clustering. normalization also makes k-means initialize with better centroid choices and run faster with fewer iterations to converge [cit] . therefore, clustering was performed on z-scores where the z-score is the number of standard deviations away from the mean a signal. this can be expressed for a given pixel x as:"
"the results show that the modified cnn can be trained with data from a few controlled training scenarios and that it operates with very high classification accuracy when deployed in a new location, without the need for scenariospecific training data."
"over the past decade, a wide variety of high-performance fluorophores have been developed [cit] . these reagents exhibit a broad range of physical and spectral properties [cit], are capable of targeting proteins or peptides in living or fixed cells [cit], and can also be used as indicators of biological dynamics [cit] . combining two or more fluorescent probes offers significantly a higher level of information [cit], but may also lead to signal crossover [cit] . current spectral unmixing tools solve this problem to some extent, but their applicability is usually limited. in this paper, we suggested and experimentally examined an approach by using k-means clustering based unsupervised machine learning as a more flexible alternative to separating mixed images blindly."
"to further compensate for the data set size, we introduce data augmentation to improve generalization. this consists of introducing transformations to the input images to artificially enlarge the size of the training set. in order for the transformed images to have the same properties as the real test data from new scenarios, the generative process for the radar images must be taken into account. since the antenna array presents a symmetry axis in the yaw direction, the point spread function in the range-azimuth plane is symmetric in the azimuth dimension (but not in the range dimension). we enforce symmetry invariance on the classifier by flipping a random number of images w.r.t. their vertical axes during training. in addition, the sign of the coordinate c x of the respective image is changed to make the transformation consistent. while the number of images is constant, the training set is virtually enlarged by performing this transformation on a random subset of 50% of each batch and letting the model train for a larger number of epochs. this ensures that enough variations are observed so that the classifier can learn robust symmetric invariant features."
"single neuron activity recorded from within the cortex is one promising control signal. the activity of these cells can be volitionally modulated when monkeys or humans are provided with the visual feedback of the discharge rate [cit] . we have recently demonstrated that single neurons can be used to control functional electrical stimulation (fes) delivered to paralyzed muscles [cit] . monkeys learned to modulate cell activity to control fes and restore simple movements to an otherwise paralyzed wrist. after brief practice periods, the monkeys controlled the activity of each cell regardless of the strength of directional tuning; this cell activity was then displayed as the movement of a cursor in one dimension, with targets presented to elicit high or low discharge rates. this pre-central cell increased discharge rate with practice at brain control as the high-rate target (blue shading) was gradually moved further from the baseline. the center figure shows the average discharge rate while holding each randomly presented target for 1 s. histograms show average cell activity around acquisition of each target. the shaded bars on all histograms denote the target hold period (target appears before shading), and the horizontal lines are the baseline discharge rate."
"after brief periods of practice, monkeys could volitionally control the activity of all 246 neurons tested in the present study. there was only a slight trend for higher brain-control performance using directionally-tuned neurons. this was probably due to the fact that visual feedback was presented in the preferred direction for cells with directional tuning, whereas the visual feedback direction was randomly selected for untuned neurons. the use of untuned neurons in braincomputer interfaces could dramatically expand the useful population of randomly sampled neurons, since about twothirds of neurons were untuned in this and previous studies [cit] ."
"similar but more complicated clustering based methods have been introduced and developed in the field of satellite imaging [cit] . remote sensing image unmixing is similar to fluorescence image unmixing in many ways, and many unmixing ideas commonly used for microscopy imaging were initially introduced in remote sensing [cit] . the ultimate goal of both imaging modalities' unmixing is to decompose the spectral signature of mixed signals into a set of endmembers and corresponding abundances [cit] . however, the uniqueness of fluorescence microscopy makes its spectral unmixing task different from remote sensing. first and foremost, the number and type of fluorophores (endmembers) are known in advance in microscopy, which offers a great advantage and simplicity of using clustering algorithms such as k-means for fluorescence image unmixing. most of the time, the first step of remote sensing image unmixing is to determine endmember [cit], and many of the advanced unmixing algorithms have been focused on how to better estimate the number and characteristics of endmembers, such as adaptive possibilistic clustering [cit] and neural network autoencoder [cit] . second, due to the chemical mixtures of landscape objects, the abundance of one pixel from a satellite image normally comprises fractions of each endmembers, thus remote sensing image unmixing methods output abundances for each pixel as fractions of different chemical components [cit] . however, in fluorescence microscopy, biologists usually assume a distinct labeling of a structure by one specific fluorophore, unless colocalized labeling was designed. the goal of fluorescence image unmixing is more towards unambiguously distinguishing each labeled structure rather than decomposing each pixel into many different chemical components. therefore, using classification based hard clustering, such as lumos, by assuming one pixel per fluorophore is more appropriate in the field of fluorescence imaging and the results of which are more interpretable for biologists. third, remote sensing images have hundreds of spectral bands which is usually much more than the number of endmembers, making linear algebra based unmixing methods, such as linear unmixing, nmf, and deconvolution, better suited [cit] . because fluorescence microscopes have much fewer detectors (usually �4), many unmixing methods applied for remote sensing are insufficient for fluorescence imaging with potentially more fluorophores than detectors. in considerations of those features of fluorescence imaging, we applied k-means clustering as a simple, easy-to-use, and flexible method for microscopy image unmixing."
"autofluorescence is another a common but usually undesired signal in fluorescence microscopy in which regions with no label are fluorescent, often with higher intensity and broader emission spectrum than individual fluorophores [cit] . autofluorescence can come from some extracellular components or some cell types [cit] . non-negative matrix factorization (nmf) is one spectral unmixing method that has been successfully applied for autofluorescence removal [cit] . we here also demonstrated the unmixing performance of lumos when autofluorescence exists. in fig 6a, the apcs in the sample were not stained but were autofluorescent. similar to background, autofluorescence can be treated as an additional cluster if it exhibits a distinct spectral signature among all the fluorophores in the sample (fig 6c) . lumos was able to detect and remove autofluorescence in the image (fig 6b) . however, if the emission spectrum of autofluorescence is similar to other fluorophores in the image, the autofluorescence may be hard to separate out, so additional detection channels may be helpful to unmix the images in such cases."
prolonged practice with a fixed transform from neural or muscle activity to bmi control clearly enhances the performance [cit] . here we observed substantial improvement with practice within a session and across days for neurons with stable recordings. it seems likely that longer sustained practice times than were used here would lead to even greater control than we have documented.
"radar technology is a viable option here, not only because of its reliability under unfavorable light or environmental conditions, but also for its capability to detect and/or track objects and persons while protecting privacy and personal data. with the evolution of industry standard packages and higher monolithic integration, critical features of mm-wave circuits, such as cost, size and power consumption, have been consistently reduced in recent years, leading to highly integrated and mature radar components."
"reduced phototoxicity [cit] . the majority of in-vivo 2plsm studies so far have relied on single or dual color imaging which highly limits the cell populations and physiological components that can be studied at one time [cit] . to identify and characterize complex biological mechanisms, multiple cell types or intracellular processes need to be visualized simultaneously. adapting 2plsm for simultaneous multi-fluorophore detection has presented a challenge due to the widely overlapping two-photon absorption spectra of commonly used fluorescent markers [cit] as well as the high expense of incorporating multiple two-photon laser lines. imaging specimens with a greater number of fluorescent labels is usually confronted with the bleedthrough or cross-talk of fluorescence emissions. these spectral mixing artifacts often complicate the interpretation of experimental results with ambiguous discriminations, particularly if colocalization of fluorophores is under investigation or quantitative measurements are necessary. therefore, a reliable and clean separation of different fluorescence labels is required for analysis and quantifications, and a flexible approach to overcome the hardware limitations on the number of fluorophores that can be simultaneously imaged is desired."
"targets were randomly presented to evoke either highor low-discharge rates of the cell. targets remained on the screen until the monkey maintained the discharge rate within each target for at least 1 s (longer in some experimentssee the results section). after a brief (1.5 s) reward period, the next target was presented. visual feedback of the cell discharge rate was initially normalized to the maximum discharge rate observed during the wrist tracking task. this was subsequently increased to examine higher cell discharge rates. discharge rate targets spanned 30% of the screen area, which encompassed discharge rates of 0 pps through the maximum rate observed."
"the need for a large set of labeled data is an important constraint on the classification of radar signals by means of data-driven models. a large training set is needed to learn meaningful features that can be generalized properly to new test data. while several standardized datasets with up to millions of labeled data are available for classifying natural images, application-specific data for radar image classification is quite restricted. as a consequence, network parameters are prone to overfitting to the training set, thus making the learnt features hardly generalizable to new test data."
"the peak performance was quantified by the maximum number of targets acquired during a 2 min period. the peak performance was compared among conditions and to the performance during the initial 2 min of practice. the performance was quantified as the number of targets acquired in a 2 min period for several reasons related to the experimental design. first, targets remained on the screen until satisfied. therefore, no metrics about the number of correct trials could be calculated. second, the task difficulty was incremented at 5-10 min intervals to extend the range of cell control. thus, 2 min struck the balance between permitting time for learning of the new task constraint (e.g. higher discharge rate or longer hold time), and assessment of stable performance. the 2 min period was chosen to provide a conservative estimate of sustained performance, and reduce the effect of random fluctuations in cell rate."
"the non-parametric rank sum test was used for all comparisons because at least one data set for each comparison failed the lilliefors test for normality. unless otherwise noted, all reported values are means ± standard deviation (sd)."
"in summary, monkeys can readily learn to control the activity of motor and sensory cortex neurons when provided with the visual feedback of the discharge rate."
"monkeys were required to maintain discharge rate within each target for 1.0 s for all cells tested. they were able to maintain discharge rate for an average of 2.1 ± 0.7 s for 25 cells tested with longer target times. with six of these cells, monkeys were able to maintain discharge rate for 3.0 s, the longest time tested (figure 7). cell discharge rate could be maintained well above or below baseline rates for extended periods up to 3.0 s for cells in both the pre-central cortex ( figure 7(b) ) and post-central cortex ( figure 7(a) )."
"the installation for parking monitoring incorporates a 77 ghz radar sensor with a downfire orientation, and a wifi access point to transfer the data to the remote back-end. for the development and test phases, an ip webcam is installed to obtain optical images of the parking scenario and establish the ground-truth data in terms of the occupancy of each parking spot. a schematic representation of a parking scenario is depicted in fig. 1 along with an image of the sensor installed in a pilot installation with the camera. the sensor is placed perpendicular to the row of cars at a distance d, a height h, and with pitch angle α in the elevation plane."
"strength of directional tuning was calculated for cells during the initial torque-tracking task using the vector method [cit] . vectors (v i ) were constructed in each of the eight peripheral target directions (θ i ) with magnitudes proportional to the average cell rate (r i ) during the target hold period [cit] . these vectors were then summed and normalized to produce a resultant vector (v) with magnitude between 0 and 1. note that if normalizing to the average or baseline firing rate, the discharge rate during each target must be individually divided by the baseline firing rate before calculating the vector in each target direction. if the vectors in each target direction are later normalized to an average firing rate the result will vary depending on the average firing rate. nearly identical results to those reported below were obtained when the maximum z-score between targets was calculated as an estimate of directional tuning."
" choose a objective vector x i1 (g) from the population such that i and i 1 are poles apart.  all over again, haphazardly pick two individuals, x i2 and x i3, from the population such that i, i 1, i 2 and i 3 all are distinct to each other.  after that the objective vector is mutated for calculating the tryout vector in the following manner:"
"parking monitoring is one of the major building blocks of such a solution, in which a network of sensors gathers information -to provide a real-time map of public space usagein order to efficiently reroute traffic and reduce congestion."
"from the point of view of the physical sensors for parkingspace detection, the problem has been addressed in the past by means of various types of technologies, like ultrasonic and infrared sensors (normally restricted to indoor parking lots due to their sensitivity to weather conditions) and video-based systems [cit] ."
"where x c is the raw intensity of pixel x in channel c,x c is the scaled intensity of pixel x in channel c and c is the set of all input channels. this step is not always desirable, as in some cases pixels with the same intensity ratios but different raw intensities may actually represent different structures."
"given the limitations of purely data-driven approaches when the size of the available dataset is reduced, recent developments in the field of deep learning show that a considerable improvement can be achieved in the performance of a cnn for a particular purpose by incorporating application-specific domain knowledge into the model to compensate for the limitations on the training data [cit] . the idea is to leverage the domain knowledge to improve models by introducing strong priors into the network. this reduces the search space in the optimization process and enhances the performance for the application at hand with limited data."
"we observed that the firing rates of isolated neurons in the post-central primary somatosensory cortex (s1) could be controlled equally well compared to neurons in the precentral primary motor cortex (m1). recordings during active movements have shown that most s1 neurons have, in addition to afferent input from peripheral receptors, central input that activates them prior to the onset of muscle activity [cit] and before active movements [cit] . carmena and colleagues found that an offline decoder could use s1 neurons to predict arm movement, velocity and grip force, but less effectively than from m1 neurons (figure 2 [cit] )."
"to demonstrate the flexibility of lumos to unmix and analyze images with colocalized labels, cd28 virus labeled with cerulean or yfp was used to transduce t cells either separately or together. the t cells were then mixed with non-labeled antigen-presenting cells (apcs) to form conjugations [cit] . cerulean or yfp was recruited and concentrated at the t-cell and apc contact sites. when t cells were transduced by cerulean or yfp virus separately, the cerulean and yfp were detected by the cfp and yfp channels respectively without bleedthrough (s2 fig) . when t cells were transduced by the mix of cerulean and yfp viruses, some t cells expressed both cerulean and yfp, while some only expressed one of them ( fig 6a) . lumos was able to separate the raw images into cerulean-only, yfp-only, and cerulean+yfp colocalized channels ( fig 6b), by identifying distinct spectral signatures ( fig 6c) . the calculated mander's colocalization coefficients were 44.2% (m cerulean ) and 38.2% (m yfp ) [cit] . in addition, although apcs were not labeled, they showed some autofluorescence in the raw images ( fig 6a indicated by white arrows, and s2 fig) . similar as background noise (s3d fig), autofluorescence was also identified and separated out by lumos ( fig 6b) . the 3d unmixing results were shown in s3 movie."
"incorporating relatively direct connections from individual neurons to bmi controls may also be a useful complementary strategy to decoding movement parameters from large ensembles of neurons [cit] . while the majority of bmi studies decode population activity [cit], recent work has demonstrated that a similar performance can be achieved after decoder weights are randomized and monkeys are provided with sufficient practice time with the same decoder and cell population [cit] )."
"the implications of k-means clustering are usually limited by the difficulties in choosing an optimal number of clusters, \"k\" [cit] . however, in the case of fluorescence microscopy, \"k\" is known and determined by the number of fluorochromes used, making k-means clustering a well-suited method for spectral unmixing. usually, the \"k\" is set to be the total number of fluorophores plus one (considering the background noise) (examples in figs 3-5 ). when special circumstances happen, options are available to optimize the \"k\" to tailor lumos for different cases. for example, when there are known colocalization labeling or autofluorescence structures (fig 6), additional clusters could be added by considering colocalization and autofluorescence as distinct \"fluorophores\". when applying lumos, carefully examining the image data to better determine \"k\" in advance may improve the unmixing results."
"to obtain longer duration cell recordings, monkey l was re-implanted with a chronic electrode array over the left motor cortex. the array of 12 independently movable microwires is fully described elsewhere . this array provided stable recordings from the same isolated cell for the duration of an experimental session for all cells, and across multiple days for 36 cells, [cit] ."
"inspired by both darwinian philosophy of ordinary evolution and dawkins' conception of a meme, the term \"memetic algorithm\" (ma) was first given by moscato in his technical report [cit] memetic algorithms are the area under discussion of intense scientific research and have been fruitfully applied to a massive amount of real-world problems. even though many people make use of techniques intimately associated to memetic algorithms, unconventional names such as hybrid genetic algorithms are also in employment. moreover, many people name their memetic techniques as genetic algorithms. the prevalent use of this misnomer hampers the measurement of the total amount of applications. mas are a course group of stochastic global search heuristics in which evolutionary algorithms-based approaches are pooled with problemspecific solvers. after that might be implemented as local search heuristics techniques, approximation algorithms or, sometimes, even precise methods. the hybridization is preordained to either speed up the discovery of good solutions, for which evolution alone would take too long to discover, or to attain solutions that would otherwise be out-ofthe-way by evolution or a local method alone. as the great preponderance of memetic algorithms use heuristic local searches rather than precise methods. it is tacit that the evolutionary search provides for an extensive exploration of the search space while the local search by some means zoomin on the sink of magnetism of talented solutions researchers have used memetic algorithms to embark upon many conventional np problems. to mention some of them: bin packing problem, generalized assignment problem, graph partitioning, max independent set problem, minimal graph coloring, multidimensional knapsack, quadratic assignment problem, set cover problem and travelling salesman problem."
"signal-to-noise ratio. all spectral unmixing methods require a good image quality. lumos is a pixel-based method which makes it susceptible to any noise detected at the same time with real signals. therefore, we tested the performance of lumos for unmixing images with different snrs (fig 7, right) . the cluster size ratio and number of fluorophores were fixed at 0.2 and 8, while f1 score of the smallest cluster was evaluated at different snrs. the simulated data showed that lumos was very robust when the snr was above 2. for images with snrs around that level or lower, lumos will likely have low performance on the raw data. even with ideal spectral signatures, any pixel-level unmixing techniques such as lumos will fail when the observed spectral signature is contaminated by high noise. in cases where the image to be unmixed is prohibitively noisy, denoising pre-processing techniques or an unmixing method that can take spatial information into account may be desired. spectral unmixing using unsupervised learning"
"the networks are trained using backpropagation to compute the gradient of the cost function w.r.t the network parameters, and mini-batch stochastic gradient descent with a learning rate of 0.001 and a batch size of 32. the training time for each network is roughly 6 hours. it is worth mentioning at this point that, despite the considerable computing resources and time required for training the network, evaluating the classification scores for a single image requires a single forward pass through the network and can easily be performed in real time. table 6 shows the classification accuracy of the tested networks. the best scores are obtained on network b (2 fc layers) with spatial priors. this configuration yields an average improvement of at least 1.4% over the rest of the tested networks. this improvement is consistent in all scenarios."
"mas have been demonstrated very triumphant transversely an extensive range of problem domains. more topical applications take account of (but are not limited to) artificial neural network training, [cit] pattern recognition, [cit] motion planning in robotic, [cit] beam orientation, [cit] circuit design, [cit] electric service restoration, [cit] expert systems for medical field, [cit] scheduling on single machine, [cit] automatic timetabling, [cit] manpower scheduling, [cit] nurse rostering and function optimization [cit] allocation of processor, [cit] maintenance scheduling (for instance, of an electric distribution network), [cit] multidimensional knapsack problem by e [cit] vlsi design, [cit] clustering of gene expression profiles, [cit] feature/gene selection or"
"most spectral unmixing tools [cit] cannot distinguish background noise from real signals, while background removal is usually an essential prerequisite before unmixing to remove any signal not originating from the targeting signals [cit] . usually, if significant background noise exists, a simple math subtraction with a specific pixel threshold measured from nonstructure background is performed, which can have the undesirable effect of removing real signals. the lumos method does not rely on a fixed numerical background subtraction, but rather the background is treated as a separate cluster with a spectral signature different from different beads separated from the 4 detection channels by the lumos and the last image is the composite showing all five beads as clearly separated objects. (c) theoretical emission spectra of the 5 fluorophores. ly and purple spectra were obtained from spherotech, and fitc, pe and apc were obtained from online spectraviewer. there were significant overlaps of all the 5 fluorophores. (d) the relative intensity of the pixels of each separated fluorophore in the 4 channels. each fluorophore was represented with a unique spectral signature. background pixels formed one additional cluster with low pixel intensities in all the channels."
"in the present work, we use a modified convolutional neural network (cnn) on a patch level to evaluate the parking occupancy state. this architecture considers the mimo radarimage properties by including spatial context to extract robust features with improved performance. this paper is structured as follows: a quick overview of cnn applications in radar and their limitations is provided in section ii. the parking scenario and the radar sensor are described in section iii, and section iv presents the modified cnn architecture with spatial priors. experimental results obtained with a pilot installation deployed in multiple scenarios are discussed in section v."
"the activity of a single motor cortex cell was recorded using either acute (monkeys i and l) or chronically implanted (monkey l) electrodes. sterile surgeries were performed with isoflurane anesthesia (1-1.5% in 50:50 o 2 :n 2 o). all surgeries were followed by a program of analgesics (buprenorphine 0.15 mg kg −1 im and ketoprofen 5 mg kg −1 po) and antibiotics (cephalexin 25 mg kg −1 po). each animal was implanted with a cranial recording chamber centered over the left hand and wrist area of the motor cortex at stereotaxic coordinates a: 13 mm, l: 18 mm [cit] . postmortem examinations confirmed that recording sites were in primary motor or somatosensory (s1) cortices, in areas related to the hand and wrist. for s1 recordings, electrode penetrations were made on the convexity of the postcentral gyrus, targeting area one. receptive fields on the hand and forearm were identified for some s1 neurons included here, and all recording sites were within several millimeters of these neurons."
"cnns have also been applied recently in the radar field for automatic target recognition in synthetic aperture radar [cit], spectrum sensing [cit] and micro-doppler (md) signature classification for applications like human activities [cit], or hand gesture recognition [cit] ."
"to rule out the possibility that the monkey learned a strategy of generally increasing cortical activity in order to control the isolated neuron, the activity of cells recorded simultaneously with the neuron participating in brain control were examined. for the 55 cell pairs recorded, the firing rates of the two recorded cells were generally uncorrelated. the average cross-correlation between the firing rates of the two recorded cells was only 0.04 ± 0.08 (range: −0.12 to 0.29). monkeys were also carefully observed for general body movements (such as leg movements) known to broadly increase cortical activity."
"cluster size. as lumos is a k-means clustering based method, the algorithm assumes similar amount of data points in each cluster, and can disregard small but real clusters in order to minimize the total loss function [cit] . this may be problematic when one fluorophore expressing structure is represented by significantly fewer pixels than the other structures, in which case the algorithm will misclassify the pixels belonging to a more abundant fluorophore to the minor structure, leading to an unmixing failure. therefore, we first tested the robustness of the algorithm by changing the size of one fluorophore expressing structure while keeping the size of the rest of structures fixed. the number of fluorophores and snr were fixed at 8 and 10 respectively. f1 score was used as the evaluation metric as it can detect when the algorithm starts to erroneously combine fluorophores. the f1 score of the smallest cluster was used because the smallest cluster is inherently the most difficult for lumos to recognize and represents the worst-case scenario. performance was monitored by setting the threshold for successfully unmixed samples at an f1 score of 0.9 or higher on the smallest cluster. the f1 score for the smallest cluster dropped off sharply when decreasing the cluster size ratio to below 0.01 (fig 7, left), because at the tipping point, one part of a larger cluster was merged with the smallest cluster as the algorithm prioritized the improvements to other dominant clusters. this happened to all of the pixels in a small cluster at once so the drop off in accuracy was sudden. this can make lumos vulnerable when one fluorophore is expressed in much smaller structures than the rest."
"there are also limitations of our algorithm, especially when unique circumstances are associated with the imaging data. as demonstrated in the simulations, our approach may cease to be useful when it misclassifies a significant portion of the pixels belonging to a fluorophore of interest. this can occur when there are relatively unbalanced structure sizes, significantly overlapping emission spectra, and a low snr. additionally, although considering the information of nearby pixels by using a median filter, lumos still does not take any spatial information at biological structure level into account so its clustering ability is limited to classifying individual pixels rather than whole structures as some other methods attempt [cit], and may fail when two fluorophores have very similar signatures (s4 fig) . in one paper [cit], total variation regularization was combined with sparse regression to consider spatial-contextual information during remote sensing image unmixing. sparse regression (more commonly used for remote sensing data) requires a known spectral library which is hard to obtain for biological microscopy, and is not required by lumos. another spatial-spectral unmixing algorithm was proposed and successfully applied for biological microscopy imaging by using dictionary learning to separate spectrally close but morphologically different structures [cit] . however, singlestained reference images were required to learn the morphological information and generate the dictionary. these reference images can be time-consuming to collect and sample specific. future improvements to lumos may introduce a spatial regularizer [cit] or a morphology dictionary [cit] to further enhance the robustness of the algorithm, while still maintaining the advantage of the blindness of k-means. lumos specifically assumes the abundance of each fluorophores is binary at pixel level, which produces unambiguous classification of individual fluorophores. if there is colocalization at structure scale, for example one structure labeled with more than one fluorophore, the colocalization group can be treated as an additional cluster to be separated and analyzed ( fig 6) . however, implicit in our unmixing algorithm is the assumption that a pixel represents an exclusive single label without considering nano-scale colocalization due to the imaging spatial resolution limitations. this assumption is valid for spatially well-dispersed fluorescent structures relative to the imaging resolution, but may not hold when two labeled structures are contacting or too close to each other. we expect future improvements by adding the options of fuzzy clustering [cit] or overlapping k-means [cit] to extend the flexibility of lumos when there are nano-scale colocalization considerations."
"the two-photon excitation spectrum of a fluorophore is usually broader than the one-photon spectra and may have multiple peaks [cit], making it possible to just use one or two two-photon laser lines to excite multiple fluorophores simultaneously, which is both time and cost efficient. on the other hand, simultaneous excitation also leads to the issue of channel cross-talk which limits the number of detection channels to usually less than 4 for two-photon microscopy. this makes the ability to image more fluorophores than detectors crucial for many applications. as the lumos method has no intrinsic requirement that the number of channels be at least equal to number of fluorophores, we next ascertained the limit of our method by imaging more colors simultaneously without modifying the imaging hardware."
"in the proposed architecture, the convolutional layers of the cnn extract local features from the images, while the coordinates contain global information. since these two sources of information are combined in the fc layers, several versions of the architecture with variations in the fc layers are tested in order to evaluate the effect of introducing spatial coordinates. two main variations are evaluated using 1 and 2 fc layers, which in turn are tested when introducing spatial priors. table 5 summarizes the main features of the tested networks."
"from the point of view of model complexity, the architectures with 2 fc layers (net b) deliver higher classification accuracies than those with a single fc layer (net a). this is because the additional complexity enhances the capability to learn more complex non-linear combinations of the features obtained in the convolutional layers. however, an arbitrary increase in the complexity of the model entails a higher probability of overfitting. by introducing spatial priors, the model is forced to learn robust features considering both local and global information, thus achieving better performance when evaluated in unseen scenarios. the generalization capability of the network with spatial priors is clearly evident in fig. 5, which shows the convergence of the cost function on the test and training sets of network b, with and without spatial priors. the error curves on the test set show in both cases a typical pattern consistent with the phenomenon of overfitting, making the test error rise from around epoch 70. at this stage, the model starts learning specific patterns in the training set that do not generalize properly to the examples in the test set. however, when the network is trained with spatial information, the test error curve evolves closer to the training curve. this improved performance is due to the fact that the model, when incorporating global context, is capable of learning more general and robust features that apply not only to the training set but also to new test scenarios."
"once the algorithm converges, a new output image is created with k channels where each channel belongs to one cluster. in the output image, a pixel x assigned to one channel c is given the value of the highest intensity of that pixel among all the c input channels, and any pixel not belonging to channel c is assigned a value of 0:"
"two male macaca nemestrina monkeys participated in the experiments (4-5 years old, weight 4.5-6.5 kg). all procedures were approved by the university of washington institutional animal care and use committee."
"while these generic mechanisms are very effective in terms of generalization, the approach can be improved further by modifying the network to include domain-specific knowledge to extract robust features and enhance the model, as will be discussed in section iv."
"therefore, to improve the flexibility and applicability of multi-channel fluorescence imaging spectral unmixing, we looked for methods that do not need spectra information and are not restricted by the number of detection channels. unsupervised learning is a class of machine learning techniques that find patterns directly from unlabeled data [cit] . by taking advantage of the ability of unsupervised learning algorithms to automatically \"learn\" to identify features from raw images, we here investigated clustering based unsupervised learning in blindly unmixing channels of multi-color 2plsm images: learning unsupervised means of spectra (lumos). similar clustering methods have been applied for spectral unmixing in the remote sensing field [cit], but never to fluorescence microscopy. by assuming the discrete labeling of biological structures, our model uses k-means clustering to \"learn\" the relationships between pixels from the raw image, and search for their intensity patterns to re-classify each pixel into a unique fluorophore group [cit] . we emphasize that lumos requires neither the knowledge of emission spectra nor a greater or equal number of detection channels than fluorophores, which highly expands the capability of two-photon imaging. we have successfully demonstrated the ability of lumos to cleanly separate out up to 6 fluorophores in biological samples imaged by a 2plsm system with only 4 detectors. synthetic results demonstrated the accuracy and power of lumos in separating more fluorophores under the challenging conditions of unbalanced structure size and low signal-to-noise ratio (snr). the method can be easily translated to images acquired by other fluorescence imaging modalities such as confocal to create a clean representation of the fluorophores in the sample for quantitative analysis."
"the classifier trained with hog descriptors, the standard deviation of the power and the coordinates as features delivers the best classification accuracy in the experiments. good classification accuracy is also obtained in some scenarios by including the mean backscattered power as a feature. the results, nonetheless, suggest that the model's generalization capability with this feature is poor. this is due to the fact that the performance in some scenarios is particularly weak, because of the variability of such magnitude for different radar sensor configurations."
"a tryout vector is generated by the de mutation operator for each individual of the in progress population. for generating the tryout vector, an objective vector is mutated with a biased differential. a progeny is fashioned in the crossover operation using the recently generated tryout vector. if g is the index for generation counter, the mutation operator for generating a trial vector v i (g) from the parent vector x i (g) is defined as follows:"
"convolutional neural networks (cnns) are architectures based on this idea. they are widely used in image recognition systems to interpret complex representations from large sets of training data, through a combination of hierarchical features of growing complexity representing several levels of abstraction [cit] . although these concepts were proposed many years ago, the availability of large sets of data coupled with greater computing power in recent years has made them the model of choice not only in vision tasks but also in other fields, such as natural language processing [cit], drug discovery [cit], and many more."
"while optical systems have been widely studied because of the reduced cost of video technology and high resolution capability, their deployment is rather restricted due to privacy concerns, as they can contravene local regulations."
"in the present study monkeys maintained the discharge rate of each cell within targets for at least 1 s, and for up to 3 s for some cells. previous studies of operant conditioning of single cell activity rewarded bursts of cell activity [cit], or the transient reduction of cell activity for low-rate conditioning [cit] . while these early studies provided the first demonstration that cortical neurons could be volitionally controlled, they did not reinforce precise temporal control of cell activity. here we demonstrate that monkeys can match rate targets both above and below baseline levels for prolonged periods (figure 7). the monkeys modulated the cortical neuron discharge rate much more during the brain-control task compared to the wrist step-tracking task, despite smaller wrist torques during brain control. increased cell modulation during initial brain-control sessions has been reported in a previous study using population decoding from large ensembles of neurons [cit] . these authors attributed increased cell rates to the learning of the new task, because cell modulation was not explained by kinematic or velocity modulation, and decreased slightly with practice. in our case, greater cell modulation was explicitly rewarded and led to increased task performance in brain control. the fact that higher cell rates could be achieved without greater wrist torque is consistent with the use of cortical activity as a control signal after paralysis [cit] and the dissociation of cell activity and movement observed in bci studies [cit] ."
"pressure vessel design (f 9 ): the problem of minimizing total cost of the material, forming and welding of a cylindrical vessel [cit] . in case of pressure vessel design generally four design variables are considered: shell thickness (x 1 ), spherical head thickness (x 2 ), radius of cylindrical shell (x 3 ) and shell length (x 4 ). simple mathematical representation of this problem is as follow:"
"while classical machine learning approaches rely on highly specific domain knowledge to manually design and extract features, deep learning techniques are particularly suited in the context of classification of radar signals because of the ability of automatically learning features from data that, in some cases, a human might find difficult to interpret (as opposed to natural images, for example), and where manual feature engineering requires plenty of expertise and training [cit] ."
the above equation makes sure that the population's average fitness does not get worse. the pseudo-code for differential evolutionary strategy is described in algorithm 1 [cit] .
baseline cell rates were obtained during 1 min periods in which the monkey viewed a blank screen immediately following the first 10 min period of cell control of the cursor.
"however, the generative process of a range-azimuth radar image is different. as is well known, a linear array exhibits different imaging characteristics as a function of the scanning angle. this is due to the reduction in the effective aperture when the array is scanned off boresight [cit] . as a consequence, the pattern beamwidth broadens in large scanning angles, thus receiving less backscattered power from a scatterer in that location than in the near-boresight region. in addition, due to non-idealities of the array that cannot be fully compensated for after calibration, the relative positions of the side-lobes and their amplitudes vary depending on the steering angle. fig. 2 illustrates this phenomenon scanned with the parking radar sensor, showing the relative power received by a corner reflector from different angles. table 1 provides quantitative data of the half-power beamwidth (hpbw), relative received power w.r.t to boresight and sidelobe level."
the proposed changes in original de algorithm control the search process adaptively and provide more chances to explore the large search space and exploit the better solution in more efficient way. this change in de tries to balance intensification and diversification of search space. the detailed memetic search in de (msde) outlined in algorithm 3 as follow:
"initialize the control parameters, f and cr (f (scale factor) and cr (crossover probability)). generate and initialize the population, p(0), of np individuals (p is the population vector) while stopping criteria not meet do"
"when the sensor is being installed, the ground-truth coordinates are mapped to the radar image using a corner reflector at each parking location within the area of coverage (normal parking and traffic continues undisrupted during this procedure). this allows us to locate the area patches that correspond to each parking space and their respective coordinates. each patch is then individually evaluated in the cnn to determine its occupancy state."
"instead, we consider this phenomenon when designing the cnn architecture to generate a unified model by combining local features that appear in a given patch image and global information with the position of the patch within the image [cit] . for this purpose, the position of the region of interest is introduced as an additional feature in the last layers of the network, so that the model can make informed decisions, considering not only the features extracted in the convolutional layers, but also the relative position of the patch to be classified within the scenario."
"where s i is the centroid of cluster s i . unlike other applications where k is difficult to define and requires tuning to optimize, in our case, the number of clusters k is simply the total number of fluorophores plus 1 more cluster that represents the background. the cluster centroid resulting from this approach can be interpreted as the spectral signatures of each fluorophores. these are the spectral means alluded to in the name learning unsupervised means of spectra (lumos). the algorithm partitions the data into k clusters using eq3 as a loss function. k-means approximates the solution to minimize the loss function by assigning data points to the class to whose centroid they are closest, and iteratively updating the centroid. fig 1 details the steps taken in lumos. there are several algorithms for initializing the cluster centroids and we implemented the k-means++ initialization algorithm for its speed and convergence properties [cit] . briefly, k-means++ chooses the first cluster centroid at random from the input data points, and each subsequent cluster centroid is selected from the remaining data points with the probability inversely related to the distance from the closest appointed centroid. the algorithm converges when clusters do not change following one iteration. the maximum number of iterations allowed per replicate, max_iter, was set to 100 to limit run time. the iterative algorithm was applied num_replicates times and the replicate with the lowest cost was used in accordance with the loss function given in eq3. all the unmixing performed in this paper used 10 replicates. the values of num_replicates and max_iter can be tuned, with more replicates and iterations yielding higher quality results but longer runtime."
"in this application, the key challenge lies in analyzing such images in order to detect and classify vacant parking spaces. this is done by designing machine learning models that are capable of learning general features from the radar images that apply to a wide variety of scenarios."
"however, in order to extract general and robust features, deep learning algorithms require a large amount of labeled training data. this is a formidable constraint in this application when deploying the system at scale, due to the effort needed to acquire labeled data. while a very robust model can be trained for a particular location, the challenge is to create scalable models -trained with a data set from a few controlled scenarios -that can be deployed in new potential locations, without scenario-specific training data. the effort is focused therefore on generating such universal models with the aim of reducing the recurring costs of training specific models for each location."
"the cnns are tested and validated with a set of radar images acquired during a two-month experimental campaign. the pilot installation described in section ii is deployed at 4 different scenarios with different sensor setups to account for the potential variability between scenarios. the parameters for each setup are shown in table 2 . for each scenario, four tests were run on different days and at different times of day. between 90 and 120 min were recorded each day at 6 frames per minute. each scenario contains roughly 16000 images; these were manually labeled according to the scenario occupancy state."
"our major findings are that (1) monkeys can learn to use arbitrarily chosen single cells in both the pre-and post-central cortex to control the movements of a computer cursor, (2) the degree of control improves with practice within a session and across days, (3) control is only slightly related to cell directional tuning during wrist movement, and (4) rates of some cells could be maintained above or below baseline for several seconds. these findings can be compared to other bmi studies and previous studies examining volitional control of single neurons."
"depending on the content of the input image, it may be appropriate to group together pixels with different net intensities but similar ratios of intensities in different z-planes. this could be necessary in fluorescence microscopy, and especially 2plsm, in which there usually are signal intensity differences across imaging depths. this can be accounted for by dividing the intensity of a pixel x in each channel c by the overall sum of that pixel intensities across all the channels:"
"to prevent the model from overfitting (i.e., learning particular features of the training set that do not generalize properly to new test data), cnns often include mechanisms to improve generalization. for example, a regularization term is normally added to the cost function to penalize large weight values, and dropout layers can be included to randomly omit part of the learned connections, thus forcing the network to learn general robust features [cit] ."
"spectral unmixing using unsupervised learning spectral unmixing using unsupervised learning fluorophore expressing signals, so that background noise can be separated (s3 fig) and removed from the sample signals ( figs 3d, 4d, 5e and 6b) . therefore, the outputs of lumos are cleaned in the way that they are both spectral unmixed and background removed."
"a canonical cnn architecture for image classification assumes that the input-data generation process is homogeneous in the 2-d space, and that the properties of the image to be classified remain constant regardless of the relative position of the region of interest within the frame. this assumption holds in general for natural images, except for minor rotations and scale variations, which the cnn can partially account for due to the property of equivariance to some transformations."
"estimate the fitness, f(x i (g)); generate the trial vector, v i (g) by using the mutation operator and incorporate algorithm 2 as per the following equation;"
"mathematical results of msde with experimental setting as per section 5.2 are discussed in table 2 . table 2 show the relationship of results based on mean function value (mfv), standard deviation (sd), mean error (me), average function evaluations (afe) and success rate (sr). table 2 shows that a good number of the times msde outperforms in terms of efficiency (with less number of function evaluations) and reliability as compare to other considered algorithms. the proposed algorithm all the time improves afe. it is due to balancing between exploration and exploitation of search space. table 3 contains summary of table 2 outcomes. in table 3, '+' indicates that the msde is better than the well thought-out algorithms and '-' indicates that the algorithm is not better or the divergence is very small. the last row of table 3, establishes the superiority of msde over de."
"number of fluorophores. the natural questions that follow from the analysis are: what is the maximum number of fluorophores that can be separated, and what is the extent of spectral overlap that can be successfully unmixed. to address these questions, we challenged the algorithm by increasing the number of fluorophores until it failed (fig 7, middle) . the cluster size and snr were held constant at 0.2 and 10 respectively, and f1 score of the smallest cluster was measured. all fluorophores were assumed: 1) to be effectively excited, 2) to have the same shape and intensity scale of emission spectra with a tail into the longer wavelength, 3) to have emission peaks evenly distributed between 420nm and 685nm (fig 2a) . to mimic the variations in real-world imaging, the spectra of pixels belonging to one fluorophore were randomly shifted with a standard deviation of 10nm (s4a fig). the imaging hardware was assumed to be the same as our system. lumos's performance was very stable until the number of fluorophores reached 12 (fig 7, middle) . at this point, the mean emission peaks were 37nm apart and there was 72% emission spectra overlap. we also tested the performance of lumos on synthetic images of two fluorophores with varying differences in emission peaks (s4b fig). the peak of the lower wavelength fluorophore was fixed while the peak of the higher wavelength fluorophore was varied to evaluate performance at different peak distances. depending on where in the range of detectors they fell, the peaks of two fluorophores could be 10-15nm apart and the fluorophores could still be separated by lumos. this 10-15nm peak distance represents an 88-92% overlap in ideal emission spectra, which is very close to the standard deviation of 10nm with which each pixel's individual emission peak was sampled (s4a fig) . this variance in emission spectra from pixel to pixel is a key limiting factor in how similar the emission spectra of two fluorophores can be while maintaining separability with lumos. in real-world cases, the fluorochromes in a biology sample will not be as ideal as the simulated scenario. careful selections of dyes with relatively separated emission spectra are always desired to gain the best unmixing results."
"the hybridization of some local search techniques in de may diminish the possibility of skipping factual solution. in hybridized search algorithms, the global search capability of the main algorithm explore the search space, trying to identify the most promising search space regions while the local search part scrutinizes the surroundings of some initial solution, exploiting it in this way. therefore, steps sizes play an important role in exploiting the identified region and these step sizes can be controlled by incorporating a local search algorithm with the global search algorithm. differential evolution (de) has come into sight as one of the high-speed, robust, and well-organized global search heuristics of in progress significance. over and over again real world provides some very complex optimization problems that cannot be easily dealt with existing classical mathematical optimization methods. if the user is not very susceptible about the exact solution of the problem in hand then nature-inspired algorithms may be used to solve these kinds of problems. it is shown in this paper that the natureinspired algorithms have been gaining to a great extent of recognition now a day due to the fact that numerous realworld optimization problems have turn out to be progressively more large, multifarious and self-motivated. the size and"
"the images rendered by the radar sensor rely on the angular resolution achieved by a large virtual aperture based on the multiple-input multiple-output (mimo) radar concept and the range resolution, obtained with a broadband frequency modulated continuous wave (fmcw) [cit] ."
"first, we started with a simple case in which there was same number of fluorophores as imaging channels. bpae cells with nuclei stained with dapi, f-actin labeled with alexafluor488 (af488), and mitochondria labeled with mitotracker red were imaged using 780nm laser [cit] to excite all three fluorophores ( fig 3a) . due to the long tail of the dapi emission spectrum (fig 3c), f-actin signals in the green channel were contaminated by the nuclei signals ( fig 3a) . dapi had strong signals in both blue and green channels, while af488 and mitotracker red were distinct in green and red channels respectively. therefore, each fluorophore had a unique distribution of intensity across channels-\"spectral signature\", calculated as the intensity of the pixels in one lumos cluster detected by each channel in the raw image ( fig 3d) . lumos was able to group pixels with similar spectral signatures into the same cluster and re-assign each pixel into the correct fluorophore cluster. as only the blue and green channels had bleed-through issues, we applied lumos unmixing only on these two channel images, and produced 3 output channels (dapi, af488, and background). after the unmixing procedure, the spectral overlap of the dapi and af488 was corrected, and the unmixed images now represent the abundance of each of the fluorophores ( fig 3b, the 3d unmixing results were shown in s1 movie)."
"a typical cnn architecture is composed of several layers that combine convolutional filters and non-linear activation functions to extract local features from the input, normally followed by pooling layers to reduce the dimensionality of the feature maps. when multiple layers are stacked, more complex patterns can be formed by combining features detected in previous layers. typically, in classification tasks, the upper layers take the form of fully connected (fc) layers that learn non-linear combinations of the high-level features extracted in the previous layers."
"for the colocalization experiments, cd28-deficient, do11.10 t cells were retrovirally transduced with cd28 fused at the c terminus to yfp or to cerulean either separately or together. t cells were then mixed with stably transfected antigen-presenting cells (apcs) expressing mhc class ii, icam-1, and cd80 that were or were not preloaded with 2.0 μg/ml ova peptide for 1 hour at 37˚ [cit] for 20 sec. the pellet was incubated at 37˚c for 10 min, resuspended and plated on poly-l-lysine coated cover slips for imaging [cit] ."
"population-based optimization algorithms find near-optimal solutions to the easier said than done optimization problems by inspiration from nature or natural entities. a widespread characteristic of each and every one population-based algorithm is that the population consisting of potential solutions to the problem is tailored by applying some operators on the solutions depending on the information of their fitness. hence, the population is moved towards better solution areas of the search space. two essential classes of population-based optimization algorithms are evolutionary algorithms [cit] and swarm intelligence-based algorithms [cit] . although genetic algorithm (ga) [cit], genetic programming (gp) [cit], evolution strategy (es) [cit] and evolutionary programming (ep) [cit] are popular evolutionary algorithms, de comes underneath the class of evolutionary algorithms. among an assortment of eas, differential evolution (de), which characterized by the diverse mutation operator and antagonism strategy from the other eas, has shown immense promise in many numerical benchmark problems as well as real-world optimization problems. differential evolution (de) is a stochastic, population-based search strategy anticipated by storn and price [cit] . while de shares similarities with other evolutionary algorithms (ea), it differs considerably in the sense that distance and direction information from the current population is used to steer the search progression. in de algorithm, all solutions have an equal opportunity of being preferred as parents, i.e. selection does not depend on their fitness values. in de, each new solution fashioned competes with its parent and the superior one wins the contest. de intensification and diversification capabilities depend on the two processes, that is to say mutation process and crossover process. in these two processes, intensification and diversification are evenhanded using the fine tuning of two parameters that is to say scale factor 'f' and crossover probability 'cr'. in de the child vector is generated by applying the mutation and crossover operation. in mutation operation, a trial vector is generated with the help of the objective vector and two erratically preferred individuals. the perturbation in objective vector depends on f and the difference between the randomly selected individuals. further, a crossover operation is applied between the objective vector and parent vector for generating the child vector using crossover probability (cr). so, it is clear that the discrepancy in the child vector from the parent vector depends on the values of f and cr. this discrepancy is measured as a step size for the candidate solution/parent vector. large step size may results in skipping of actual solutions that is due to large value of cr and f while if these values are low then the step size will be small and performance degrades."
"there are a couple of functions for the selection operator: first it selects the individual for the mutation operation to generate the trial vector and second, it selects the most excellent, between the parent and the offspring based on their fitness value for the next generation. if fitness of parent is superior than the offspring then parent is selected otherwise offspring is selected:"
"accompanying this trend is a paradigm shift from modeldriven to data-driven approaches, where highly complex features can be learned automatically, provided enough data are available."
"considering the application of parking space detection by classifying radar images, the necessity for such a large amount of labeled training data, limits the scalability and therefore the viability of the solution. on the one hand, because the cost of labeled data in this context is expensive in terms of time and labor, and on the other, because labeling data from real scenarios requires optical sensors. these sensors might be difficult to deploy in certain countries, as they may not comply with local privacy regulations."
"commonly used dyes differ not only in their emission spectra but also their excitation spectra. the differences in excitation efficiency offers additional features for lumos to better separate out more fluorophores. in the next example, we used sequential scan by alternating two-photon excitations at 840nm (maitai laser) and 1050nm (insightx3 laser) to visualize 6 compartments with distinct labels in one single cell (colorful cell). human embryonic kidney cells (hek293) were transiently transfected with a plasmid that encodes differentially localized fluorescent proteins. the cells express tagbfp in nucleus, cerulean in cell membrane, azami-green in mitochondria, citrine in golgi body, mcherry in endoplasmic reticulum (er), and irfp670 in peroxisome ( fig 5a) . cerulean, azamigreen and citrine all have significant emissions in the green channel ( fig 5b), but they are excited at different efficiencies under 840nm [cit], making it possible to distinguish them with the spectral signatures by collecting the green channel twice with the two excitations ( fig 5e) . the 2plsm excitation/emission setup is shown in fig 5b. all the organelles were ambiguously mixed in the raw images especially in the green, red and far-red channels ( fig 5c) . we assigned 7 clusters to the lumos algorithm to separate out the 6 fluorophores and background from the original 5-channel images. the algorithm reliably separated the raw data into 6 components that corresponded to the 6 organelles ( fig 5d) based on their shapes and locations inside the cell by comparing to the cell structure schematic ( fig 5a) . the 3d unmixing results were shown in s2 movie."
"the monkey's hand remained in the isometric wrist torque transducer throughout the experiment, permitting wrist torque to be measured during brain-control sessions. the monkeys produced much less wrist torque on average during brain control compared to wrist tracking (see the results section and figure 3(c))"
"in the context of smart cities, radar sensors are becoming more competitive than other technologies, not only in terms of cost, but also for the added value of including multiple features, like parking detection, measuring traffic load and velocity, detecting pedestrians and cyclists, etc. the concept of a radar-based parking monitoring system consists of a network of radar sensors, deployed in urban elements, like lampposts or buildings, that monitor different coverage areas of a parking scenario. the data are transmitted to a remote back-end, where they are processed and presented to the end-user in the form of an occupancy map of the parking scenario (fig. 1) ."
"the performance improves with practice, and is generally unrelated to the strength of direction tuning during movement. these findings demonstrate that arbitrary single cortical neurons, regardless of their strength of directional tuning, are capable of controlling cursor movements in one dimension. it is possible that such direct connections from cell activity to neuroprosthetic devices may be useful in controlling artificial stimulation delivered to paralyzed muscles [cit], or for controlling robotic or prosthetic limbs."
"a modified cnn with spatial priors is presented for classifying radar images on a patch level using a mimo-fmcw radar for detecting empty parking spaces. the properties of the radar images are considered in the network design, which includes the spatial coordinates of the patches to account for the loss of array directivity when scanning off-boresight. this allows the model to learn robust features that combine local and global information. a systematic validation analysis on four scenarios shows that the spatial information yields better generalization performance. the hypothesis is first tested with an svm classifier with handcrafted features, and then with the modified cnn architecture."
"lastly, we sought to test the limitations of lumos spectral unmixing by understanding the smallest structure size which can be detected, the maximum number of fluorophores the algorithm can separate, and the minimal quality of the input image that is required. as it is impractical to prepare a real-world biological sample with arbitrarily many fluorophores and precisely control both the size of a stained structure and the image snr, we used synthetic images with those conditions computationally manipulated (fig 2a-2c) . the synthetic data also provides us a ground truth to evaluate the performance of the algorithm."
"to prove this, we first introduce some auxiliary mappings. let l be a set of labels, and let t be time. we introduce the function"
"the it field of \"channel, antenna\" co-evolved with other fields. according to fig. 10a, \"channel, antenna\" [cit], and ca8, ca10 and ca11 seem like to associate with it field. as shown in fig. 10b, ca1 \"space time\" and ca2 \"multipleinput multiple-output\" were direct successors because of the lowest dissimilarity (0.413 and 0.407) among cas; in other words, they have the highest similarity and were almost the same fields of the cited it field \"channel, antenna.\" consequently, the co-evolving cas with high dissimilarity and high correlation were ca7, ca8, ca9, ca10 and ca11, which belonged to the field of it."
"will be selected as new proxy node and will be activated by the last proxy, while the other nodes in nt( ) will be in the idle state. after selecting as proxy node, will be tracked by nt( ). unlike glt, in the predictionbased protocols [cit], the previous active nodes predict the activation zone in which not all nodes can detect the target but only a subset of them. on the other hand, in the nonprediction-based protocols which define a schedule or a plan for nodes activation and/or sleeping, following the object trajectory, some of the nodes are woken up to execute the sensing and the communication tasks for a scheduled time, and then they change their states to the sleep state. both of prediction-based protocols and nonprediction-based protocols may contain some prediction errors that may lead to more energy dissipation. although a recovery mechanism is provided [cit], the recovery mechanism consumes much energy. in glt, there is no need for recovery mechanism since the nt is not prediction based."
"thus, ca3, ca5, ca6, and ca8 highly co-evolved with the it field; in particular, a co-evolution between the cited it field and ca8 fits the background of heterogeneous co-evolutions like publications about fuzzy set applied to images of vessel tracking and segmentation [cit] . additionally, ca3, ca5, and ca6 contributed to the resurgence of the cited it field."
"here, we assume that before any decision to switch is taken, the response tasks to switch to has been selected according to the scores given by matrix a (see section 4). we first consider the switching cost, given the best scored task to switch to. the cost to switch from the current to a new task breaks down into a reconfiguration and an interference cost with respect to a specific task. to these costs we may add a second reconfiguration cost if we need to return to the current task."
"this research extracted pairs of co-evolving academic fields by using a large amount of bibliographic data related to information technology. after the 28 it fields in \"computer science\" and \"information science and library science\" were identified with a citation analysis for the top 1% of academic papers, academic papers citing 28 it fields were retrieved. the dissimilarity and correlation between the citing fields and cited fields were computed. some pairs of it fields and citing fields were scrutinized, and there is an issue that pairs with high correlation included both co-evolutions and the same fields between the citing fields and cited fields. thus, this research regarded pairs with both high dissimilarity and high correlation to be co-evolving pairs. furthermore, using the time series charts, the background of the co-evolving academic field contributing to resurgence in it field was confirmed by literature survey."
"the robot model provides a representation of both the hardware and software components and of the processes activated by each component. for example, the slam software module is modeled as a component, whereas the mapping and localization functionalities are represented as processes managed by the slam component. table vi lists the main components and processes of the robot system model. here, by repair c k we denote the k-th robot component and by π j,c k the j-th process managed by the k-th component."
the overall robot control provides bidirectional communication interfaces with the human user. it further allows the human operator to switch between several operational modalities that lie between autonomous and teleoperated modes during the execution of a task.
"in addition, the indexes msrt, mrst, and mma highlight that the system is more flexible than both cram-em and dago-pomdp. the first column of table v shows that the robot chooses to switch only in the 45% and 60% of cases in which the other two models decided to replan. on the other hand, the values of the indexes in the second column of table v show that the task selected, when switching is the choice, are different; this is due to the lack of a stimulus-response matrix learned from humans in the other two models. therefore, replanning in both cram-em and dago-pomdp is wired into the behavior and not learned. finally, the values of the irp index in the third column of table v show the ability of the task switching model to be conservative with respect to the task at hand."
"as shown in fig. 1, this research uses three types of relations: citation, correlation, and dissimilarity. citations are consciously connected by authors, indicating causality. unlike qualitative analysis (such as interviews), citation analysis shows co-evolutions objectively, since causality is strengthened by many citations. co-evolution requires two fields to be growing simultaneously. resurgence is judged by a rise after a fall. thus, correlations measure concurrency of rises and falls of timelines. dissimilarities indicate that two fields are different."
the numbers of the top 1% of papers can be counted in each cluster according to each year. correlations between citing papers and cited papers can be computed with the time-series numbers. the equation of correlation from the microsoft excel function is expressed as the follow:
"if the stimulus occurs, the robot has to choose a possible response (e.g., going back to the last position where it was connected), or it can inhibit the stimulus and continue its task even though it is disconnected from the operator. therefore, there are two issues that need to be modeled:"
"returns a sequence of actions that governs the processes that can be activated in s 0 to perform some task involving the component c. robot components are both hardware devices and software modules such as engines, dialog, vision, mapping, planning, path planning, and so on, all handling several processes and needing to be regulated not only by transition and precondition axioms, but also by both temporal and spatial relations. to handle relations between processes, a parallelization of situations into several timelines has been introduced [cit], and each timeline assesses the evolution of all the processes of a component. a timeline is a special situation taking into account only the start and end actions of the processes of a component. parallel situations are handled by a set of situations called bags of timelines. time constraints are based on the definition of the greatest start time and least end time, thus bounding a process activity within a start and end time, up to a fixed time t. the greatest start time is defined as:"
(2) the mechanism generating processes to execute a chosen task. (3) the constraints between processes working in parallel. (4) the representation of the robot's current task state.
"since the early work of jersild [cit], several studies in neuroscience and cognitive science have led to a better understanding of many of the variables affecting task switching in the context of cognitive control (we refer the reader to monsell [cit] and the citations therein). [cit] s, as, for example the ata schema [cit], the principles of goal-directed behaviors in newell [cit] . (for a review on the earliest architectures in the framework of the task switching paradigm, see [cit] .)"
"on the other hand, the static setting of the environment allows us to make repeatable trials. in this setting, 20 trials were performed. table iv reports the results obtained from measuring the time indexes msrt, mrst, and mma. the msrt index of the task switching model is lower than the indexes of the other models. the main reason is that the stimulus model anticipates the identification of the stimuli (see section 3). instead, both cram-em and dago-pomdp require that the amplitude of the signal associated with a stimulus reaches a certain threshold in order to detect the stimulus. both cram-em and dago-pomdp perform a state space search to generate new plans in the replanning phase, whereas the task switching model use the payoff model to select the best choice for a new task (see section 6), so no state space search is required. therefore, the mrst time index of the proposed model is the lowest."
"the task switching model demonstrated better performance in terms of time needed to accomplish the mission. in fact, the model can compromise between brave and conservative behavior, which is not allowed in a purely reactive framework. the proposed model is able to inhibit the stimulus, and thus balance continuous changes by preserving focus on the task at hand if switching is too demanding. this behavior outperforms the purely reactive behavior of both cram-em and dago-pomdp. the swrpc, swrpd, and irp indexes reported in table v validate these considerations, also showing that the robot not only behaves more rationally but also more rapidly in accomplishing a mission."
"(3) energy balancing mechanism. when there is a notification message (i.e., insert, delete) to be sent from to its associated groups (i.e., activation zone * ( )), will not send it to all nodes in * ( ) separately. however, sends the notifications to its children in nt( ). additionally, the needed energy per notification will be distributed among all the nodes in * ( ). to clarify the importance of the notification tree, we use the first-order model [cit] to evaluate the consumed energy by each node during intercommunications. the first-order model is explained in (see [cit] for symbols used in the equations)"
"for example, the equation (20) says that to start the process of approaching a certain location, the path planner process needs to be active, which in turns requires the mapping to be active:"
"(22) note that and are not necessarily minimal explanations (see marquis [cit] and cialdea and pirri [cit] for the minimality problem in computing first-order explanations), but we might assume here that these explanations nevertheless minimize the action sequences."
(1) identify the task that is a possible response to the stimulus; (2) decide whether to go on with the current task or to switch to the identified response task.
"comparing the proposed framework with two other approaches to robot replanning, both robot flexibility and performance, in terms of time and tasks choices, are improved. it follows that robot usability also is improved since user stress can be reduced by limiting the workload that continuous monitoring of robot activities demands of the operator."
"a set of robot processes defines a task. this set of tasks forms the robot task library. for example, the task to overcome an obstacle requires that all the processes managed by the 3d laser scan component are active, that the 3d mapping process is running, and that the process adapting robot morphology to the terrain provides the correct positions for the flippers. table vii lists the set of tasks designed in agreement with the available robot functionalities. here, by τ i we denote the i-th task in the task library. the right column in table vii lists all the processes that need to be activated for the robot to perform the corresponding task."
"to understand the complications intervening in this decision, we briefly introduce the representation of processes in the framework of their control, execution, and monitoring, together with the reasoning mechanism required to infer the facts that need to be appraised before eventually switching to a new task. namely, switching requires us to infer an appropriate explanation for enabling all the preconditions for activating the response task. in this section, we introduce the following concepts to address this problem:"
"in living organisms, stimulus transduction elicits the stimulus reflex through a receptor. in a robot, this transduction is conveyed by its active processes. consider the robot motion system, which is made of several components, each of which is driven by processes controlling velocity, steering, obstacle avoidance, terrain adaptation, tracking and so on. these processes collect information from the environment and from the robot's internal states. these processes elaborate the collected information and update it, eventually returning it to the robot's inner states to manage its current activities. in other words, the interaction with environment configures a space of information allocating to each process a quantum of information that is transduced to a stimulus at specific peaks of the information manifold. a stimulus induces a request with the call for a response. a response might appeal to robot components and their processes not yet active at the very moment at which the stimulus is triggered."
"this research provided a methodology that used correlation, dissimilarity, and citation to find co-evolutions, but it still required checking time-series trends to find resurgence. fig. 11 mapping of dissimilarity and correlation for the discovery of co-evolution future works should address a methodology to find past resurgence and forecast resurgence candidates. in addition, in order to use co-evolution effectively, the durability or scale of co-evolutions had better to be measured. the discovery of co-evolution and resurgence will provide evidence for reviving industries through it. table 1 data specifications due to the eventual shortage of computer performance, the cosine similarities of the two strike-through records cannot be calculated after citing articles to c1-1 and c1-2 are clustered the above times do not include the time to try and error for identifying suitable size and category of datasets before practically analyzing. in the column of the dataset, the numbers of c (cluster) correspond to c# (cluster id) in table 2 . in the column of the citation type, dc + cc + bc uses direct citations, co-citations, and bibliographic couplings. dc + cc uses direct citations and co-citations. it takes time to retrieve data from the service within our access right without a dos (denial of service) attack"
"tf is a score that shows that high-frequency words are important. icf in a cluster and idf in a document are scores that show that high-frequency words in other clusters and documents are not important. icf uses a comparison of words between clusters in a dataset (instead of a comparison of words between documents). here, clusters mean one type of academic fields, which are different from the definition by web of science. the reason to use icf instead of idf is to target the mass such as clusters for observing the rise and fall of each specialty. the bibliographic data of some academic papers do not include abstract, though they have semantic connections via citations. old academic papers do not include abstract in its bibliographic data. these academic papers without abstract cannot contribute to this analysis when idf analyze individual academic papers. however, icf enables academic papers without abstract to be counted in timeseries trends. figure 6 shows the concept of tf-icf, which is compared with tf-idf. with the keywords based on scores of tf-icf, the computation to identify topics of clusters is provided."
"the stimulus-response framework introduces new emerging methodologies that build cognitive robot control via learning and perception, and we expect that it will spread to several applications that require cognitive control to resolve the continuous struggle a robot has to face in a real-world environment."
"academic papers have citations, so networks can be constructed by connecting citations. there are three types of citations: direct citations, co-citations, and bibliographic couplings, as shown in fig. 5 [cit] ."
"robot control is defined by a declarative temporal model of its activities and a planning engine. the declarative temporal model is specified in the temporal flexible situation calculus (tfsc) [cit] so that the main components and processes of the robot system reported in table vi are represented by cause-effect relationships as well as by the temporal and spatial constraints among the processes [cit], as discussed in section 5. planning is composed of two main logical modules: the plan generator and the execution monitoring. the plan generator relies on a library of prolog scripts that designates the set of tasks (see table vii ) that the robot can perform according to the specified processes, their temporal constraints (compatibilities), and their preconditions. execution-monitoring is a continuous process ensuring that both the set of action sequences generated by the plan generator according to the tfsc model and the current state of the domain knowledge are consistently executed. the execution monitoring embeds the model of task switching to regulate the behavior of the robot in responses to the incoming stimuli. both the tfsc model and the planning engine are implemented in eclipse prolog [cit] ], which optimally combines the power of a constraint solver (for time and compatibility constraints) with inference in order to generate the set of action sequences and also to enable continuous updates from incoming new knowledge by using finite regression [cit] ."
"the contributions of this research were as follows: (1) to provide evidence of co-evolution and resurgence in the past and (2) to propose a methodology to find co-evolution. to know unexpected collaboration between heterogeneous fields, it is better that co-evolving fields are combinations of it field and other than it field. in fact, there was a phenomenon that co-evolution by insiders and an outsider of it field support the resurgence in the field of \"channel, antenna.\" at the resurgence of \"fuzzy set,\" \"vessel tracking and segmentation\" became one of co-evolved fields. it is found that co-evolutions contributing to resurgence were identified."
"the trends of the switching cost for a task are established with respect to each of its processes and for each of the response tasks to the stimuli of its process yields; therefore, it is defined for each pair τ i, τ j . once the trends are obtained, the switching cost is approximated by its frequencies with respect to time."
"the system of this article is still an ongoing project and there are some issues under resolving. one of the main issues is the energy dissipation in nt tree, which is affected by the overlapped regions. we are investigating to combine the geometric and machine deep learning to identify the overlapped regions of wsn networks in order to clarify the overlapped regions for each node in the network. in this paper, the grouping based location tracking (glt) algorithm is proposed, which is based on the grouping hierarchy structure (ghs) [cit] . glt algorithm ensures a high performance by the support of nt tree and hst tree to aggregate and report the date to the sink node using the minimum number of hops considering the energy balancing. the data reporting process of glt is totally based on the reporting algorithm hst. on the other hand, the nt tree is managing the notifications between the nodes to avoid the duplication messages and data redundancy."
"an analogous definition is given for leastend, the least end time for an action, thus characterizing the t inside which a process is certainly active or certainly not active. given this definition, a time relation such as during is defined as follows:"
"in this section, we introduce what we define as a robot stimulus and how we estimate the presence of a stimulus given the information gathered by a process."
"however, in order to build networks of cas, direct citation and co-citation were used. the types of citations were selected to complete the analyses in a reasonable computation time, incorporating as many citations as possible, depending on the server capacity. the number fig. 3 the trend in the top 1% of academic papers in \"computer science\" or \"information science and library science\" 1 3 of cas became too large to be computed as a result of the actual collection of cas' bibliographic data. for example, when a small size of cas (including around 10,000 papers) was computed actually to know the scale, the number of edges according to direct citation and co-citation was about five times greater than the number of direct citations; the number of edges according to the three types of citations was about 100 times greater than the number of direct citations and co-citations. the gap of 100 times the number of edges led to three times the computation time and one-fifth of the number of clusters. thus, the usage of bibliographic coupling (based on papers citing cas) was discarded, and direct citation and co-citation were used for cas, as shown in table 1 in the \"appendix\". since there have been no previous cases in which over 300,000 records were analyzed with three types of citations, the cas were analyzed without bibliographic coupling to avoid a server crash and irrational time consumptions."
"in glt, the packets include data routing, building hst, building nt, activation, and switchoperation. figure 15 shows the number of packets for glt, which is grouping hierarchy structure based [cit], and the number of packets for different clustering-based protocols (i.e., caics [cit], dlc [cit], and mhc [cit] ). in glt, the number of control packets including hst packets and nt packets does not change per time. on the other hand, the number of activation packets, data routing packets, and switch-operation packets will change per time according to the mobility pattern of the target. in glt, the location of sink affects the number of switch-operation packets, while it has no impact on the number of other control packets. when the sink is located in the center of the interested field, the lifetime of the network is getting longer, and the number of switch-operation packets is reduced. in contrast, when the sink's location is getting closer to the sides of the interested field, network lifetime is getting shorter and as a result the number of the switch-operation packets will increase in order to prolong the network's lifetime as long as possible."
"as in section 3, the modes x are collected by gradient ascent. the modes return the best prediction of the bivariate x. where the probability of x is below a threshold or where the probability is high but the estimated switching cost (given time t) is beyond the effort the robot system can afford, then we introduce a time failure. these failures, that can be collected for each pair τ i, τ j, are best explained with an example."
"tree. the number of control packets (i.e., insert and delete packets) depends on the size of nt tree, while the size of nt depends on the number of groups (the maximum covered regions in the field). similarly, the number of groups is strongly related to the deployment scheme of nodes. the greater the size of nt is, the greater the number of control packets will be and the more the energy consumption will be. as shown in figures 11(a) and 11(b), the number of groups affects the size of nt. when the number of groups is getting greater, the size of nt will be smaller. the size of nt plays an essential role that affects directly the number of control packets. to show this, we used different number of targets and different speeds (figures 11(e) and 11(f) ). when the speed is getting higher, the number of control packets will be increased (see figures 11(c) and 11(d)). figure 12 shows the evaluation of hst during data routing process. the performance of hst is affected by the number of groups in the network, the size of the group, and the deployment scheme of nodes. the evaluation shows that net 4 has the worst performance in terms of energy consumption, delay, and shortest path selection. however, net 4 shows the best performance in terms of energy consumption during the activating of nodes for tracking. hst shows that the number of groups in the network affects the average number of hops and delay. the greater the number of groups, the shorter the path to the sink and the lesser the energy consumption (see figure 8 )."
"it is important to note that for this model to predict accurately the occurrence of a stimulus with the outcome value y, given the features z of the specific process yields, it is required that the outliers introduced during data gathering are kept under control before the estimation of the parameters β takes place. in fact, in addition to the difficulty of obtaining comparable trials, the training set t is extremely noisy due to operators mistakes made while a trial is running."
"the rest of this article is organized as follows: \"methodology\" section describes the steps of this research and the technological factors. \"results and discussion\" section indicates the results and provides a discussion. \"conclusion\" section summarizes this research."
"constructed alpha signals were again combined with random pink noise signals at a specified snr. this time, 259 each composite alpha signal was replicated 9 times, and combined with an independently sampled pink noise 260 signal. this yielded a dataset of 9 synthetic 'channels', each comprised of identical alpha signals embedded 261 within stochastically varying background noise. this enabled us to examine how our algorithm's channel 262 exclusion and averaging procedures performed under varying levels of snr and peak dispersal."
"as shown in fig. 8b, ca1-ca10 are divided into two groups by dissimilarity: fields related to decision-making (ca1-ca3, ca5-ca7) and medical fields (ca8 and ca9). the former group had a lower dissimilarity (0.531-0.764) than the latter group (1.000), so the former group contained the successors of the cited it field. the latter addressed blood vessels in medicine and the applications of the cited it field. ca10 was unique, belonging to computer science and mathematics, but having high dissimilarity (1.000)."
"for all experiments, we recruited 20 operators between 26 and 32 years of age, in good physical conditions. each operator performed up to 10 trials using the task switching interface (see figure 6 (c)) and choosing a task from the task library described in section a.3, in appendix a."
"an interactive interface serves in dataset gathering (see figure 11) . the layout has four principal panels: (1) the stimulus panel, (2) the robot state panel, (3) the task state panel, and (4) the control panel. the stimulus panel includes vertical tabs listing robot components. each tab is composed of a set of frames, each related to a component process and displaying the trend of the process and its yields. an additional button allows the operator to manually notify the robot of the occurrence of a stimulus. the operator can disable a frame via a check-box button. by selecting a tab in the stimulus panel, the operator can observe the state of each robot component, as well as the yields of the processes of that component. the robot state panel is composed of both a text box reporting the task the robot is currently accomplishing and a table listing all the running processes required to perform that task. the operator can indicate which task the robot has to perform via the tasks buttons displayed in the task panel. the control panel has two horizontal tabs: (1) the robot steering tab and (2) the flipper configuration tab. in the first tab, a number of control commands are listed for steering the robot. the latter serves to change flipper configuration and lock/unlock the differential drive to manually control the robot locomotion system. in addition, the main window reports both reconfiguration and interference costs. three radio buttons set the interface for three different user interaction modalities: (1) passive mode, (2) active mode, and (3) acquisition mode. the passive mode allows the operator to manually notify the occurrence of the stimuli by observing the yields of the processes. the active mode enables a mixed robot-human initiative. in this modality, the interface displays the data yielded by the active processes and notifies the operator of the occurrence of a stimulus in real-time. accordingly, the operator can select the task the robot has to perform or allow the robot to continue its current task. the acquisition mode gathers the active processes and shows the robot choice if a stimulus occurs. in this mode, the interface notifies the human operator of the occurrence of a stimulus, the choices taken by the robot to respond, and the task chosen; in this mode, the operator can store his or her choice for further analysis."
"consider the following events applied to a robot, not necessarily in parallel: the battery is getting low, the wifi has a very weak or absent signal, saliency is detected in a sequence of frames, the memory is surcharged, the communication with the operator is clogged, a request pops up on the interface, and someone is talking at the dialog box. a robot has to deal with hundreds of events like these. these events are stimuli, in the sense that they trigger some alert on the information process transduced by an executive process of an active robot component such as, for the example, the battery, the wifi, the attention system, the mapping system, the engines, the dialog system, the interface, or the obstacle avoidance system. three concepts are at the core of stimulus modeling: (1) the information flow caused by an active robot process, (2) the states and events inducing a stimulus, and (3) the rule by which a stimulus is detected to be so:"
"these considerations prove that the task switching model, by exploiting both highlevel reasoning and learning from operators, effectively harmonizes robot choices with context requests by conjugating a rational behavior with a flexible one."
"the performance of the stimulus model is evaluated by the roc curve of both the true and false stimuli over a varying time range, with the stimulus occurring in the range."
"in contrast to sophisticated surveillance technologies such as radars, which are in fact reliable, robust, and accurate but expensive, wsn enable a cheap technology which do not rely on any centralized infrastructure [cit] . wsn is a collection of nodes, which are of low cost, low power, and small size, carrying out the task of sensing, performing simple data processing, and communicating wirelessly over a short distance. the node contains three main subsystems: (1) the sensing subsystem contains one or more physical sensor devices and one or more analog-to-digital converters as well as the multiplexing mechanism to share them. (2) the processing subsystem executes instructions pertaining to sensing, communication, and self-organization. ( 3) the communication subsystem contains the transmitter and receiver for sending or receiving information [cit] ."
"the ability to selectively respond to several stimuli and to inhibit inappropriate urges by focusing on the task at hand are well-known in humans as shifting and inhibition executive functions, respectively [cit] . the neuroscience theory of control uses inhibition to explain many cognitive activities, especially to explain how an individual presented with several stimuli responds selectively and is able to resist inappropriate urges [cit] ."
"each process yields a quantum of information with characteristic features. this quantum of information is called the yield of the process. for example, the process monitoring the temperature of the robot engine outputs, over time, the servo-motors temperature. the information carried by the processes regulates the robot's behaviors. for example, the engine temperature could reach a critical level and damage the robot servo-motors. the critical engine temperature level, as well as the loss of the wifi connection between the robot and the remote command post, are stimuli, and they are identified from the process yields. table viii lists a description for each robot stimuli. note that each process can trigger a single stimulus; therefore, the number and types of stimuli are determined by the number and types of processes and these, in turn, are determined by the number and types of robot components. in the table, the term y π j,c k denotes the stimulus triggered by the yield of the j-th process managed by the k-th component c k ."
"(3) lifetime prolonging mechanism. from glt structure, each node may have two parents; one parent for nt and the other for hst. the routing mechanism is based on hst tree and nt tree; two of them are working together in order to maximize the lifetime of the network. although the main task of nt is to avoid the data redundancy and duplication, it plays an important role in maximizing the lifetime of network. when the hst parent of the reporter node has a lower power (i.e., lower than the threshold value), the reporter node should switch its parent according to the switch-operation. the switch-operation of the parent is managed by nt, which is entirely based on the remaining energy of the nodes in nt. the node with the greatest energy will be selected as parent for the reporter node. for example, in figure 4, nt will switch the routing path from node 10 to node 20 if node 8 is busy or it has a lower battery power. the hst is the default tree for reporting the data to the sink; however, it should be updated according to energy distribution and consumption requirements. the energy threshold value for the nodes in nt will be reduced per switch-operation. with the assistance of nt, the nodes figure 4 : hierarchical spanning tree, assuming that node 15 is the sink node."
"in the recent years, one mission of bibliometrics or scientometrics becomes to provide evidence for decision making about science, technology, research, development, and innovation management (u.s. [cit] . this research tries to expand the utilization of bibliometric or scientometric approaches. traditionally, experts such as researchers or funders have sought for investment targets by the means of qualitative investigation like interview, workshop and literature review. meanwhile, it is difficult for experts to catch up with everything even in their specialty because a large amount of knowledge has been stored every year. thus, experts need to be supported by the computational methodologies in order to overview their specialty and capture the cutting-edge science and technology. because the principles and guidelines for evidence-based policy-making have been enacted [cit] and data science has been required in technology and innovation management, the demand for evidence for policy and management is high."
"the number of groups and the number of deployment patterns in ghs affect the communication overhead. we have tried to find a closed form for g( ) but we failed due to the complicated combinations of g( ). nevertheless, one contribution of this work is finding the number of deployment patterns. the readers are very welcome to solve the issue of g( ); in the future, we will try to find closed form for g( )."
"when has a notification to be sent to * ( ), without using nt, it will dissipate the energy of according to (2) . is the total energy needed to send the notifications message to all nodes in * ( ). in (2), is consumed by node :"
"only recently has the need to model the stimulus-response pattern leading to task switching and the cognitive functions of shifting and inhibition become a hot topic in cognitive robotics [cit] . the earliest studies were carried out within brain-actuated interaction [cit] ], mechatronics [cit] ], behavior learning [cit], navigation [cit], and planning [cit] . recently, several studies posited the need to model task switching to cope with adaptivity and ecological behaviors in a dynamic environment [cit] ."
"while citations, co-occurrences of keywords, collaborations between authors or institutions by co-authoring are available for the measure of co-evolution, this research focuses on co-evolution between sciences (which are industrial seeds) with citations. to identify resurgence, the rise and fall of the field time-series chart was used."
"with these motivations, we propose glt, a model to detect movements of a target using grouping hierarchy structure (ghs) [cit], which does not fall into existing tracking categories and is, to the best of our knowledge, the first of its kind. glt is a two-tier tracking and reporting tree. the first tier is the nt. this tier is designed to achieve three goals. (1) activation mechanism: it notifies the nodes in the notification tree to wake up and collect the data. (2) data cleaning mechanism: when the object is detected in an overlapped region, nt prevents the redundant sensed data during data aggregation. (3) energy balancing mechanism: the nodes in the nt consume approximately the same amount of energy. on the other hand, the second tier of glt is hst (hierarchical spanning tree). hst is designed to work as two mechanisms: data reporting mechanism and lifetime prolonging mechanism. hst and nt are working together to achieve their best performance in terms of network lifetime, tracking accuracy, traffic reduction, and transmission delay reduction."
"the performance of the model is measured on a testset, collected as follows. once a task is chosen, the operators manually label the yields of the running processes upon the occurrence of a stimulus. the yields, labeled by the operators, were compared with the results of the stimuli classification provided by the model (section 3). both correct classifications and mismatches are noted, considering the reference time interval [init, end] centered at the time the operator notified occurrences of stimuli. figure 7 shows the roc curve of the stimulus model with respect to the size of t. the trend reports the percentage of true positives versus false positives for every considered time range. then, −2, 2 is taken as the reference time interval to evaluate the correctness of the stimulus model, due to the tradeoff between the false and true positives."
"tracking. in glt, the tracking quality is based on nt. the quality of tracking is based on the size of nt and its coverage around the current proxy node. therefore, to guarantee the tracking process continuously, all the nodes in nt will be in the idle state in order to be ready for tracking. the size of nt tree is shown in figure 11(b) . in figure 13, we deployed 20 objects in the network topology shown in figure 10 (a). the speed of each object is 5 m/second. the simulation time is 60 seconds. the nt tree ensures a very high performance in avoiding data redundancy such that the reported messages to the sink node will not be duplicated; and at any given time, there will be one proxy node for each object. for example, as shown in figure 13(a), in the time interval [1-6 s], target 1 will be tracked by node 18 even if the object is crossing an overlapping region. in the case of the object appearance for the first time in an overlapped area, there will be one duplicated message. for example, as shown in figure 13 (a) in second 25, the packet is reported by node 84 and node 87. after reporting the first packet, both nodes will discover that they have the same notification message record in their nmt (see figure 3) . at this time, node 84 (with higher energy) will be selected as the proxy for object 1, while node 87 will send a delete notification message to its nt tree. figure 13 (b) illustrates the trajectory of mobile target 2."
"the flowchart of the proposed robot stimulus-response framework is illustrated in figure 1 . the lower layer represents a robot task under execution. the next level illustrates the stimuli and the response models, described in sections 3 and 4. the level enclosed between inference and knowledge delineates the work flow linking the decision level to the current execution level. the upper level provides a schema of the system representation in the formal language of situation calculus, where the mapping of the stream of information into the domain of reasoning takes place. these levels are described in sections 5 and 6. each active robot process yields a quantum of information with characteristic features; this quantum of information is called the yield of an active process. the features z of the yield are used by the stimulus model to learn a function h β (z) (section 3). this function establishes whether or not a stimulus occurred during the specific process execution."
"the proposed method improves both autonomy and flexibility in robot control. furthermore, according to the experiments done to verify the coherence and rationality of robot behaviors with respect to operator behavior while teleoperating the robot, the approach proves to be very promising for improving the proactive interactions with the environment required by a robot relating to people and their needs."
matrix a is initially built using a number of experiments and further completed by estimating the unknown values for the stimulus-response pairs with the regularized low-rank matrix factorization method [cit] ].
"a process π is active between a start π and an end π action. to activate a start or end action, a special predicate poss tells the robot whether the action can be done given the current situation s and time t, for example:"
"the generalizability of score matrix a is measured according to the convergence of the mean square error (mse) with respect to two different values of rank d of the matrices q and p, factorizing a (section 4). the dataset for both stimuli and responses to them is gathered by operators in the field as they execute tasks chosen from the task library via the task switching interface set to active mode (see section a.5 in appendix a). the operator responds to a stimulus by either switching or continuinginhibiting the stimulus. a k-fold cross-validation technique on the gathered dataset is used to assess the generalizability of the model."
"the evolution of it also has revived the gaming, printing, and movie industries through new types of digital platforms that have enabled the delivery of content to customers through various pieces of digital equipment. [cit] demonstrated how the co-evolution of it and the gaming industry has enabled new ways of bringing video games to the market-developing digital gaming platforms that radically change traditional business models in the industry. in the field of robotics, which is related to it, robot manipulation of electronic engineering was mainstream in the beginning of the 21st century. however, in bigdog [cit] and surgery robot [cit], traditional mechanical engineering technologies (such as hydraulic pressure and air pressure) are being used to improve small robot manipulation. [cit], a story of experience that traditional technologies overcame the issue of small robot manipulations was obtained. in these ways, the resurgence of a declining industry can be observed due to the co-evolution of it with that industry."
"for the precautions and limitations when the methodology was used, (1) more pairs of co-evolution were found when the lower threshold of correlation and dissimilarity was set."
"eog channels were removed following artifact rejection, and remaining channels were downsampled to 250 hz 221 in preparation for spectral analysis. datasets exceeding 120 s were trimmed to this duration in order to reduce 222 variability in the quantity of data analysed per participant."
"visual inspection of channel spectra confirmed the 292 absence of any consistent alpha peak. the cog was 293 however estimated for one of these individuals. across the sample, rather than intraindividual differences between recordings (see figure 3b ). these data 301 are therefore in accord with previous reports of the iaf's high temporal stability (at least within the same 302 recording session) and interindividual variability (at least in the context of eyes-closed resting-state eeg)."
"according to fig. 9b, ca6 and ca9 have weak correlations (0.200 and 0.219), which were relatively higher than the others and had high dissimilarities (0.936 and 1.000) with the it field \"simulation optimization.\" thus, ca6 and ca9 relatively co-evolved with the cited it field within a dataset in fig. 9b, but the score does not indicate co-evolution in fig. 11 . this is a limitation due to differences in the data. apart from the other cases, the it field \"simulation optimization\" was cited by non-it fields (such as particle physics and chemistry), and it included interdisciplinary collaborations. it was considered that interdisciplinary collaboration has a high potential for heterogeneous co-evolution, but correlations became relatively low."
"(2) data reporting mechanism using hst. in glt, nt aggregates the data and the proxy node will start the forwarding process. any node in the path from the proxy to the sink forwards the collected data to its parent in hst or its parent in nt. the parent in hst is the default. if the energy of the parent in hst is greater than the threshold value, then the parent in hst is selected. otherwise, the parent in nt is selected (see algorithm 1)."
"the cost of reconfiguration and interference amounts to computing the cost of these statements. we define the cost considering first the terms of the language; namely, variables and functions of any arity. we exclude from the cost evaluation the terms mentioning variables of sort situation and variables of sort action: for example do(a, s) is excluded, but not do(a(x), s 0 ) since s 0 is a constant of sort situation and the action term a is not a variable. let η be a term of the language; the cost of η is inductively given by the function ν, defined as follows:"
"dissimilarity enabled the exclusion of the same fields between citing fields and cited fields for the purpose of finding co-evolutions. thus, mappings of dissimilarity and correlation give the following interpretations:"
"we are now at a point in the formalization of the stimulus-response where we can illustrate how a decision can be taken about whether to respond to a stimulus. to illustrate the decision problem, we continue with the wifi disconnection example."
"wireless communications and mobile computing upon three main parameters: the residual energy of node (i.e., the node with more energy wins), the distance to object (i.e., the nearest node wins), and the status of the node (i.e., busy or free)."
"given a process π, we have introduced the function h β that takes as input the yield of a process and determines whether a stimulus is triggered or not, returning a value 1 if this is the case and a value 0 otherwise. if a stimulus occurs, it is necessary to score all possible response tasks before any decision can be taken."
"we are now ready to determine the cost for inferring the explanations needed to activate a preferred task τ j in terms of the cost of the preconditions and constraints for τ j, at time t, according to equation (22) and in terms of discontinuing a number of processes occurring in the current task τ i . given equation (22), the cost of inference for switching is: fig. 3 . the switching costs for the task τ 12, data retrieval (see table vii ). the up-rows indicate the processes the task is assembled from and their yields. in the left column are the tasks that obtained the maximum switching score in score matrix a. the gray cells indicate the average normalized switching cost to the preferred task, indicated in the left column."
"the it field of \"simulation optimization\" has latent co-evolutions-though it has low correlations with cas. at step 1 of the methodology, the \"simulation optimization\" has a high potential for resurgence; [cit] .0, and the \"optimization algorithm\" [cit] s. both \"simulation optimization\" and \"optimization algorithm\" include the word \"optimization.\" figure 9a illustrates trends in the cited it field of \"simulation optimization\" and the citing fields as cas. [cit] s co-occurred with the trend in ca6 of \"model order reduction,\" [cit] matched the trend in ca9 \"electron nucleus\" [cit] )."
"in object tracking sensor networks, power consumption is a key issue, which directly affects network lifetime. the power can be conserved in object tracking sensor networks by switching a sensor node to sleep when there is no object in the node's sensing range and turn on a sensor node when an object is to enter the node's sensing range [cit] . energy conservation can be achieved using different methods: prediction and nonprediction [cit] . in prediction-based 2 wireless communications and mobile computing tracking models [cit], the nodes are woken up on demand following the target path. the previous active nodes predict the activation zone in which not all the nodes can effectively detect the target but only a subset of them. on the other hand, in nonprediction tracking, the nodes are periodically put to sleep in order to conserve energy and they wake up after a period of time to monitor the sensing region and collect the object movement information [cit] . nevertheless, prediction and nonprediction strategies have shortcomings such as missing target trajectory and duplicated packet reporting and the predictions may not be entirely correct. this leads to loss of object, and large amount of energy may be wasted in recovering the object. additionally, in object tracking sensor networks, the data aggregation model, nodes activation model, and data reporting model play an essential role that could maximize the lifetime of network [cit] . those models should be designed to reduce the communication overhead in the network taking in to consideration the hardware limitations of nodes."
a process yield and its stimulus value are both linked to the signature l of the action language l by the mapping e. the domain of l and of the processes' yields are linked by the interpretation i of l.
"in this section, we explain the second tier of glt, which is called hierarchical spanning tree (hst). the data reporting mechanism and the lifetime prolonging mechanism are the two main objectives of this tier. the first tier achieves data collection process. however, the length of the first tier is not enough to reach the sink due to the limited height of nt. on the contrary, the height of hst expands to reach all nodes in the network. nevertheless, the nt tree plays an important role in the data reporting mechanism; nt excludes the nodes, which have lower energy than threshold; this is an important role, which enhances the lifetime prolonging mechanism of hst. we first show how to construct the hst; second, we explain the reporting mechanism using hst. finally, we show how hst and nt cooperate to achieve network lifetime prolonging."
"the experiments for filling a are short-term missions performed by operators who are well informed about the task library, the processes, and the stimuli that the robot can handle. each operator is assigned a task from the task library (see table vii in appendix a, as described in section 7). during the trial, a stimulus is triggered for an active process π j . upon the occurrence of the stimulus, the operator selects the task τ i that he or she considers the most appropriate to respond to the presented stimulus from among those reported in table vii (appendix a) using a graphical user interface (see figure 11 in appendix a). note that, during the experiment, the operator teleoperates the robot. the experiment terminates when the operator signifies his or her task choice. data from each trial, reporting the occurrence of the pairs τ i, y π j, are collected, and the corresponding entries a ij of matrix a (see table ix and table x ) are filled. when operators did not select the corresponding stimulus-task pair, a dash sign is added."
"on the web of science, citing academic papers is called \"citing articles.\" citing articles (cas) is retrieved based on citations to the top 1% of academic papers, as shown in fig. 4 . in fig. 4, squares represent datasets, and blue circles indicate clusters. while the methodology [cit] ) was proposed to retrieve cas after cited articles were analyzed, the retrieval of cas' bibliographic data was not performed for the second layer of cas (cas of cas) in this research. consequently, 1,512,545 academic papers were retrieved."
"the academic landscape system provides a feature keyword list of respective clusters, which can be computed by the means of a mutant \"tf-icf (term frequency-inverse cluster fig. 4 relations between the top 1% of papers and cas frequency)\" of tf-idf (term frequency-inverse document frequency). the equation is the following:"
"to linked mastoids. each dataset was then trimmed to retain only the eog and the nine centro-posterior detection routines were then applied to identify regions of channel data (segmented into 2 s epochs) that 218 contained excessive deviations in the frequency domain (frequency range: 15-40 hz, spectral threshold: 10 db)."
"the performance of the model of task switching with respect to both cram-em and dago-pomdp replanning approaches was evaluated by measuring the following indexes: (1) the time average of reaction to stimuli (msrt); (2) the time average of response to stimuli (mrst); the average time of mission accomplishment (mma); (4) the ratio between the number of switches performed by the task switching model and the number of replanning instances, where the response to stimuli provided by all the models is the same (swrpc); (5) the ratio between the number of switches and the number of replanning instances, where responses are different from each other (swrpd); and, finally, (6) the ratio between the number of times the task switching model inhibited the stimulus and the number of times both cram-em and dago-pomdp decided to replan (irp)."
"our approach is completely novel, and, to our knowledge, no other approach to robot cognitive control has developed these ideas. the proposed method is supported by a full implementation in an advanced robot platform endowed with multiple abilities to perform several tasks, in which the interplay with operators plays a crucial role for both learning and evaluating performance. our approach is inspired by the wide psychophysical and psychological literature on human patterns in stimulus-response and task-switching, although we have provided an integrated model that is tailored for a robot system and is replicable for other robotic platforms."
"perhaps there is an invariant stimulus for the invariant response, after all. many sorts of higher order variables of energy may exist, only awaiting mathematical description. they will have to be described in appropriate terms, of course, not as simple functions of frequency and amount. we must not confuse a stimulus with the elements used for its analysis. [cit] as a matter of fact, we do not yet have a model of robot stimuli, and most researchers accept that any sensory input from the environment or from the robot's internal states can be considered a stimulus. this simplification is of no help, as the wide literature on human and animal stimuli witnesses. if any sensory input can be considered a stimulus, how it would be possible for a robot to discern between a wall and a victim lying on the ground, between the light from a fire and the light of a lamp, between a scream and repeated noise, and so on? not all inputs require a response, but a stimulus does."
"we collect the time to process failure by establishing the switching cost between τ i, τ j from the modes x of f x (x) as in the following set, where sc is switching cost, and is the threshold for the effort the robot can afford:"
"the problem of shifting and inhibition induced by stimuli was modeled by a switching cost, obtained by conjugating a cost to reconfigure the robot state for the new goal and the cost to resolve interference with the current activities set. finally, the decision problem culminates in the definition of the payoff for switching, which requires us to consider the risk and effort in computing the cost. we used time to failure to combine the whole effort and predict the reliability of all the decision components to calculate the payoff for switching."
"as per the preliminary analysis, we compared the accuracy of sgf-generated paf estimates against those 264 produced by the lm procedure. for the latter, the optimisation function was applied to the mean psd and were therefore excluded from the paf analysis."
"(2) although fig. 11 was overviewed in multiple fields, thresholds should be designedly set in individual fields as shown in the \"case 2: simulation optimization\" section. (3) causalities were guaranteed with the usage of citation-even if their rise and fall in time-series trends did not occur simultaneously. (4) the gap of timelines to find co-evolution was not considered in this research."
"the robotic platform is designed for harsh, unmanned environments. these are largescale environments where robot missions last for long time periods and several events and stimuli burden the robot's exploration and reporting activities."
"as a final point of comparison with previous findings, we examined the relation between age and iaf ( figure 310 3b). both estimators showed a similar trend towards reduced iaf as age increases beyond the fourth decade."
", and a set of constraint formulas is indicated by c. note that constraints are special formulas because only the definiens (namely, the left hand side of the definitions) are expressed in the language l. different approaches for defining constraints in a first-order language have been proposed [cit] . a definition for each time constraint between processes, according to linear interval-based time [cit] ], is provided in finzi and pirri [cit] ."
"obviously, we expect to attribute the switching cost to r s . however, if the switching cost is computed before any decision is taken, then this is like attributing the cost to both r s and r i . in fact, by its definition, computing the switching cost is the same as abducing the set of explanations needed to activate the new task and deactivating the current state; thus, it follows that this inference process ends up weighting the decision well before the decision is taken."
"1. fields of the top 1% of academic papers in \"computer science\" or \"information science and library science\" were identified by the means of clustering citation analysis. 2. fields of the academic papers citing the top 1% of academic papers were identified through citation analysis."
we used gauss-markov mobility model [cit] . it was designed to adapt to different levels of randomness via one tuning parameter. gauss-markov mobility model generates the next position depending on the previous position and conserving the speed and direction.
"in this article, we address the problem of providing a representation of what a stimulus is for a robot and how it affects the robot's behavior and its tasks under execution. consider that whereas for a living organism pain is a stimulus that has a direct effect on its survival awareness, for a robot, a similar stimulus could be the lack of a wifi connection or battery exhaustion, both indicating a loss of vitality. therefore, in providing such a representation, we answer the question of how to model the stimulus reflex within the robot's structure and language, and we explore how the robot learns the best stimulus-response according to the context of its current task."
"msrt measures the average time elapsed between the occurrence of a stimulus and its identification. for cram-em, this time corresponds to the average time elapsed between the occurrence of the stimulus and the detection of the corresponding event. for dago-pomdp, this is the average time elapsed between the occurrence of the stimulus and the decision to rebuild the plan in reaction to it. mrst represents, on average, the time needed to select the best response to the stimulus (e.g., shifting or continuing). for both replanning approaches, this index denotes the average time spent for replanning. to measure these indexes, the experiments were set up as follows. the robot is instructed to autonomously approach an object whose position is known on the given 3d map. the time estimated to accomplish the task is about 20 min (this is a large area). the task ends when either the estimated time expires or the robot reaches all the targets. the occurrence of a stimulus on wifi signal quality and on battery power level is simulated after 6 min. and after 12 min. from the beginning of the task, respectively. the signal associated with a stimulus is kept up for 2 min. the pulse of the signal has a trapezoidal profile. the choice of a reduced number of stimuli is motivated by the fact that we want to bound the domain of both different approaches to replanning. this guarantees the applicability of the two approaches to replanning. both the duration and the profile of the pulse associated with a stimulus have been chosen to test the degree of proactivity of the stimulus-response, but also to allow both cram-em and dago-pomdp to infer plans that avoid stuck conditions for the robot."
"the major contributions of this paper include the following: (1) notification tree: we proposed nt to wake up the nodes in the activation zone in order to reduce the overhead of the proxy node. moreover, nt reduces the communication overhead during data aggregation. (2) hierarchical spanning tree: to reduce the communication overhead from the source node (aggregation node) to the sink, we proposed hst, which is based on ghs [cit] . (3) glt: it is a combination of nt and hst to ensure tracking accuracy and to minimize the data reporting delay. (4) find a closed form for the number of deployment patterns for nodes. the remainder of this paper is organized as follows. in section 2, the related work is presented. grouping structure, nt, and hst are explained in section 3. in section 4, we have given an analysis for ghs and nt; moreover, we have provided a mathematical model for grouping of ghs, which is designed to compute the average communication cost and network deployment patterns. the simulation results are shown in section 5. finally, section 6 concludes this work."
"in order to build a network of the top 1% academic papers, direct citations, co-citations and bibliographic couplings were used. the reason for using these three types of citations simultaneously was to compensate for the number of direct citations of only 1% of academic papers."
"this research explores co-evolved fields with it fields in a half-open attitude, which means that the analysis does not restrict the number of fields on one side of the pairs of co-evolved fields though the fields on the other side is fixed in the it fields. a previous study [cit] ) has limited the fields to those within the author's knowledge; this limitation is rational as it removes noise and completes analyses. however, finding unexpected pairs is an advantage of big data analysis, and unexpected pairs will be able to further stimulate industrial development. this research takes the risk of not finding good co-evolved pairs, but exploratory-seeking good co-evolved pairs in all fields is worth a challenge."
we have thus provided a model to represent the stimuli and for learning when such stimuli occur. we have also introduced the principle that a large region of nonstimuli for each process yield can be identified around the most typical values of the yield to simplify noisy data collection. note that this concept of typical values for the process yield grants the possibility of estimating h β with quite few trials.
"in this work, we proposed a model for robot stimuli and stimulus-response that can significantly improve robot control in general, and particularly in the robot's interaction with the environment. the proposed approach indicates a new way to address robot cognitive control by showing that interaction with the environment can be proactive if stimuli are taken into account not individually but as a whole system whose specific aspects can be learned by interacting with human users. this gained flexibility in robot control can increase the usability of the robot, especially for operators who have to deal with it in extreme environments like first-responder training areas or even post-disaster environments."
"tracking mobile object using wsn is a hard problem, since the nodes have limited power and computational capability. in recent years, tracking mobile objects in wsn has gained a great deal of attention. it is classified into three categories: tree-based tracking [cit], cluster-based tracking [cit] 21], and prediction-based tracking [cit] ."
"in this section, we show how to score the relation stimulus response, which amounts to scoring the task that would eventually be chosen to respond to the triggered stimulus. here, a task is a set of processes; see table ii for an excerpt of the structures linking robot components, processes, and stimuli, and the more detailed tables vi, vii, and viii in appendix a. therefore, while a task is being executed, a stimulus can be triggered from any of the active processes of the task. to address each case, given a limited number of tasks for a robot, we introduce a stimulus-responses matrix a that scores each pair: the score matrix a is formed by two classes of entities, which we refer to as tasks (rows) and stimuli (columns), respectively (see table vii and table viii in appendix a). note that a task is built out of many processes, yet each process can yield a single stimulus; namely, there is a bijection between each process and the stimulus it yields."
"the empirical prediction quality is typically low (see figure 2 (a)). note that, as expected, the mean squared error (mse) considerably decreases with a higher rank d of the matrix a and, consequently, the empirical prediction quality gets higher (see figure 2 an example of the score matrix a filled by the trials values for the implemented task library with their processes and stimuli is illustrated in tables ix and x, in appendix a. the completed stimulus-response matrix, with the whole score set completed, is shown in tables xi and xii in the appendix a."
"location tracking has many applications in our daily life in different areas, for example, military intrusion detection, vehicular networks, and habitat monitoring [cit] . these applications must periodically collect the sensory data (locations, velocities, and trajectories) and use it to reconstruct the overall status of the monitored area through data aggregation. when the sensor nodes detect an event, they record it and rely on a distributed routing protocol to send the relevant information toward a base station or sink. the target events must be detected with an acceptable degree of accuracy. in addition, the event report should be received by the sink in a very short time, especially for time-critical events [cit] ."
"since the noise is mainly introduced by the operator in the difficult task of deciding whether an observation is or not a stimulus, we have devised a method of rejecting most of the outliers that severely affect parameter estimation. indeed, we exploit some prior knowledge about stimuli/nonstimuli for each process, and we use this to define a function that is consistent with the typical behavior of the process and that rejects wrong data in the training set. the proposed function maps to a rejection region all those outcomes that are classified as stimuli but whose range of yields should in principle assign them to a nonstimuli classification since stimuli are extraordinary events. the function defining the rejection region is:"
"to this end, we generate a number of action sequences for as many time lapses within a time range bounded by 120 minutes. for each of these samples, the cost given in equation (25) is computed. an example of the averages of the normalized switching cost for task τ 12 -namely, data retrieval-is given in figure 3 with respect to each of the response tasks to the stimuli that can be triggered by each of the processes composing the execution of task τ 12 ."
"the citation network was divided into clusters by using the academic landscape system [cit] with an unsupervised classification, the newman method [cit], and it was implemented on a website [cit] . the utilization of citations has the merit that the issue of homonyms and synonyms is solved in the form of citations, which use the collective knowledge of experts."
"in the case of ca4 in the \"case 1: fuzzy set\" section, ca1 in the \"case 2: simulation optimization\" section, and ca1 and ca2 in the \"case 3: channel, antenna\" section, the cas that have strong correlations with their citing it fields mean the same fields with the citing fields-but not the co-evolved fields. some cases in this research had successful pairs of co-evolution, but most of the pairs were not co-evolved. in fig. 11, [cit] (as shown in table 1 in the \"appendix\") are plotted, and the mapping indicates relations between dissimilarity and correlation. however, cas of c1-1, c1-2, and c2-1 were excluded due to memory shortage and the failure of accesses to the web of science."
"we define three states for each node: active, idle, and sleep. in active state, the node collects the sensory data and reports it to the aggregation node, while in idle state, the node gets ready to track object and waits for an activation from the currently selected proxy node. in the sleep state, no sensory data could be collected. we assume that the nodes support two types of messages (i.e., control packets): (1) insert control packet: this control packet is sent from the currently selected proxy node (the node that detects the object). the main role of this control packet is to let the receiver nodes change their state from sleep to idle and be ready for the object when the object crosses their sensing ranges. the other role of this control packet is to let all nodes in the activation zone know that there is a proxy node that has been selected, and they should not be activated until the proxy node send them delete packet. (2) delete control packet: this control packet is sent from the proxy node to let the nodes in its nt know that the object is out of its range currently, and one of nt nodes should be selected as a new proxy node. all the receiver nodes should change their state from idle to sleep excluding the new selected proxy node."
co-citation bibliographic coupling the scores of similarities were relative values in the dataset because word diversity in each field was different. the author's source code of similarity was provided in https ://githu b.com/shino iwami /a_jyu20 17_06/blob/maste r/110_keywo rd_simil arity .py.
"in fig. 7, if the it field \"fuzzy set\" had subsequent it fields of \"rough set\" and \"fuzzy control,\" it was expected to have resurgence in addition to co-evolutions with citing fields. according to fig. 8a, the cited it field had two peaks [cit] ), and the latter peak followed the increases of ca3, ca4, and ca6. ca2 started to grow with the cited it field."
"where −1 and −1 are the speed and direction, respectively; −1, −1, and, are the old and the new positions of the target, respectively. the th position, direction, and speed are calculated using gauss-markov model from the ( − 1)th position, direction, and speed. and are calculated by"
"tables ix and x, read side by side, provide an example of the score matrix a introduced in section 4. each entry a ij represents the score given by a stimulus y π j,c k, triggered by the yield of the j-th process, managed by the k-th component, to a task τ i . this matrix is filled with data gathered from the experiments described in section 4 using the graphical user interface described in section a.5. dashes denote those stimulus-task pairs that the operators never selected during the experiments. tables xi and xii, read side by side, form the estimated score matrix a."
"we also introduced a connection between stimuli and reasoning level by defining a boundary shared by the two levels via the mapping e that plays the role of transduction, lifting the terms from the sample space of features, where yields are sampled, to the terms of the language. thus, logical inference provides the grounds for the causal and constraint relations among processes carrying the information driven by stimuli. the logical structure is also the framework within which both processes and the robot's mental states can be defined, and it is where the reconfiguration and interference costs can be fully interpreted."
"this involved creating alpha signals that were comprised of a set of neighbouring frequency components 251 from different channels. we did this by sampling an 'actual'/'measured' alpha frequency per channel from 252 a truncated gaussian distribution centered at the randomly sampled target f α (selected as for the single 253 component simulation) for each simulated (sub)component (targets chosen uniformly from the standard alpha 254 band, as above). the tails of the gaussian were truncated ± 2.5 hz from its mean/target frequency. alpha 255 signals were thus constructed by creating a weighted average of frequencies within this distribution; in other 256 words, a gaussian blur was applied to the frequency-domain signal in order to generate a mixture of alpha 257 waves in the time domain."
"a robot interacting with people and with the environment is subject to several stimulus events, like any living organism. how these stimuli are to be represented and how a response should be modeled has not yet received the necessary attention, even though robot autonomy, beyond laboratory experiments, relies on an intelligent response to these stimuli."
"2 therefore, to both correctly define the two functions and to avoid burdening the decision, the switching cost can be replaced by a prediction of it. to predict the switching cost, as defined in equation (25), two steps are needed:"
"this expression can be extended to three tasks in sequence. finally, the reliability of the combination of the two tasks, taken in sequence, and the switching cost estimation, taken in parallel with the two tasks, is defined as follows:"
"with the advancement of information technology (it), new industries have been developed, and old industries have been replaced. for example, the music industry has shifted from physical music recordings to downloading or streaming; this was due to it advancements. in the music industry, sales of old media have declined. furthermore, users have begun to participate in new attitudes toward music activities such as streaming music and crowdsourcing by using it; sales related to live music have started to rise again after declining [cit] . in the music industry, the music rankings like billboard began in america at the end of the 19th [cit] it became the ranking of 1 3 sales based on the number of plays [cit] ). also, [cit], sony in japan launched walkman with the concept of \"carrying music and feel free to enjoy.\" at that time, it was music mobile with cassette tape, but now products evolved into digital products such as ipod [cit] )."
"in case of prediction-based tracking [cit], the next location of mobile object is calculated using distributed structure or using the historical data or the information of object. during each defined time step, only some nodes near the predicted location are activated while the other nodes stay in sleep mode to save energy. different approaches with different types of measurements have been used in target tracking systems, such as angle of arrival (aoa), received signal strength (rss), time of arrival (toa), and the time difference of arrival (tdoa). additionally, variational filter (vf), kalman filter, extended kalman filters (ekf) [cit], distributed kalman filter, particle filter, and data mining [cit] are the most commonly used models to predict the next location of object."
"the findings from the results were as follows: (1) the it fields \"fuzzy set,\" \"simulation optimization,\" and \"channel, antenna\" experienced a resurgence, and their co-evolved academic fields were identified from their trend charts. (2) some pairs, such as \"fuzzy set\" and \"vessel tracking and segmentation,\" with high correlation and high dissimilarity were extracted as co-evolutions between different fields."
"the distributed working memory structure interconnects the tfsc model and the planning engine with the task switching engine and with the robot functionalities that are integrated in the ros operating system (ros) [cit] ] (see figure 10 ). the structure is distributed over a collection of ros services, organized as a set of addressable memory registers. in addition to the usual reading and writing operations, the structure provides additional operations like memory dumping and update acknowledgment operations, suitable to check both consistency and persistence of the state of the memory. the working memory system also takes care of mapping the internal state of the robot into a representation suitable for planning and reasoning about tasks."
"the first item is dealt with by filling a score matrix a whose values are estimated via factorization, as described in section 4. on the other hand, the decision to either shift from the current task to a new task in so responding to the stimulus, or to inhibit the stimulus, is based on the payoff of switching. this is computed considering the risk of continuing the current task, without taking into account the stimulus, and the effort required to fulfill the stimulus. in turn, the effort is computed considering two costs: (1) the cost to reconfigure the current robot state to the new state that switching would lead to and (2) the cost to resolve the interference due to the interruption of the current task. these costs are computed by considering the preconditions and effects of each action involved in both the processes to be interrupted and in the ones to be activated (section 6). to bind the information yielded by a process to the domain of reasoning, a special functional e is used, mapping the process terms of the representation language to the corresponding values of both the yields and the stimuli at execution time (section 5)."
"i think the central question is the following. is a stimulus that which does activate a sense organ or that which can activate a sense organ? some writers imply that a stimulus not currently exciting receptors is not a stimulus at all. others imply that a stimulus need not excite receptors to be called such. [cit] gibson's implicit question is about the possibility of giving a structure, a definition, to the relationship between objects and stimuli and between stimulus and response:"
"having obtained the switching cost and knowing the response task for the triggered stimulus, we can compute the payoff. here, the payoff is simply the culmination of the stimulus-response problem and it is, indeed, the payoff for switching. let r s be a function specifying the choice of switching and r i the analogous function for inhibition. then the payoff for switching is simply r s − r i ."
"the robot platform c bluebotics, named absolem, has a central body where the electronics is located and has two bogies on the sides. each bogie is made of a central track for locomotion and two active flippers on both ends to extend the climbing capabilities. a breakable passive differential system allows the rotation of the bogies around the body. several sensors are installed on the platform, among them a rotating 2d laser scanner, an omnidirectional camera with a 360-degree field of view, an imu/gps, and a panoramic microphone."
"therefore, by discovering co-evolution and resurgence of science, this research intends to contribute to industrial redevelopment. the purpose of this research is to establish a methodology for finding academic fields that have co-evolved with other fields by the means of bibliometric methodology. in this research, information technology is selected as the comprehensively explored scientific field. although previous research [cit] ) showed a resurgence of the music industry by co-evolution in well-known pairs of music and it on the basis of revenue data, this research aims to find unknown field combinations with co-evolutions and resurgences."
"after that, the topics of the clusters are manually identified with keywords, titles, abstracts, and journal names. the merit of clustering of citation networks is that analysts do not need inclusive knowledge of the field's topics. therefore, classification can be executed before analysts understand the contents of large amounts of data. the merit of the newman method [cit] ) is that the number of fields is decided automatically by computation in order to maximize modularity; newman defined \"modularity\" as an indicator that occur at about 0.3 or more in communities, for example citation network. in the actual utilization of results after clustering (since several minor clusters with less than 10 papers are generated), the tool's users determine the threshold of the cluster nodes for the adopted clusters-depending on the purpose."
"most of the cited works are quite recent, showing increasing interest in the stimulusresponse framework for coping with adaptivity and ecological behaviors in an interactive environment. further examples of these studies exist [cit] . the major problem to be resolved, as noted in wawerla and vaughan [cit], is the switching decision. experts on task switching claim that this decision incurs a switching cost. this cost results from the interplay between the resources needed to reconfigure a mental state and the resources needed to resolve interference with the current state [cit] ]. we model this cost by considering both the reconfiguration of the mental state and the interference with the current state. the reconfiguration fig. 1 . work flow of the proposed stimulus-response framework. from bottom to top: the lowest layer indicates a currently active task being executed. the active processes feed the process yields, collecting information from them; the stimulus model selects from the yields those which are stimuli. the stimulusresponse model scores the response tasks. the decision is taken by evaluating the payoff of switching to a task suggested by the stimulus-response matrix. in the middle panel, between knowledge and inference, execution monitoring takes care of the actual task execution and its updating, both affecting the mental states; namely, the decision of whether to consent to a response to the stimulus. in the upper panel, a first-order logical formalism is used to model the robot processes, with action preconditions and effects affecting activation costs and motivating causal constraints. this closes the loop between stimulus activation, reasoning, planning, and decision. and interference costs are defined in terms of the amount of processes that need to be inferred in order to initiate a new task and their congruency with the current task. the decision is made based on a payoff that is computed by considering the cost of updating the current robot state and the risk of continuing with the task under execution. the stimulus-response model illustrated in these pages indicates a new direction in robot cognitive control, and the tests and experiments presented here prove that this is a promising direction."
"let w be an indicator matrix of the same size as a in which 0's correspond to missing elements of a and 1's correspond to observed data; then, the problem can be reformulated as:"
"in summary, co-evolutions were indicated as shown in the upper-right and lower-right part of fig. 11 . the thresholds were decided based on the relations in the set, so they should be studied in the future. here, the following seven pairs (among 146 pairs when the fields of co-evolution were checked) definitely co-evolved: the three co-evolutions from \"channel, antenna\" and the three co-evolutions from \"fuzzy set\" in fig. 11, as listed in particular, with regard to \"channel, antenna\" having co-evolutions along the well-known history about mobile phones as described in the \"case 3: channel, antenna\" section, coevolutions can be extracted definitely even among the other fields. [fields citing to \"simulation optimization\"] ca1: function minimization ca2: phylogenetics, bayesian evolutionary analysis ca3: biomolecules ca4: global optimization, scheduling ca5: simulation of chemical reaction ca6: model order reduction ca7: heat exchanger network ca8: qspr (quantitative structure property relationship) ca9: electron nucleus ca10: petri net ca11: particle physics, neutrino ca12: pharmacokinetics ca13: copositive programming, noisy channel ca14: bilevel programming ca15: learning automata ca16: location management ca17: stochastic programming, dual dynamic programming ca18: schedulability ca19: conservation and spatial planning for fishery and marine ca20: clustering for software modularization (*bold underlined fields co-evolved with the cited it field.)"
"is the maximal ellipsoid surface, enclosing the q-th confidence region, it follows that these regions correspond to the most typical behavior of the yield of the process π under consideration. hence, these are rejection regions for stimuli-regions where it is most unlikely that a stimulus can occur. therefore, we are finally arrived at the definition of rejection region rejregion:"
"in this article, we introduce a preliminary framework to model robot processes, their yields, the stimuli occurrences, and the decision underlying the response to the stimuli. the advantage of this approach is that robot control does not need to be designed a priori but instead can be drawn by its interactions with users who teach the robot when a stimulus occurs and what the possible alternatives are for handling the stimuli. these aspects are not dealt with in any other control methods, including planning, whether reactive, proactive, or declarative. indeed, a robot that has learned a stimulus-response strategy from several humans will most certainly be more usable than a robot that either has no stimuli at all or has no strategy to respond to stimuli. we also take context into account in order to establish a response cost, and we exploit a theory of actions that models how tasks are chosen and how switching to a new task can occur as a result of a stimulus-response."
"the size of the network (i.e., the number of nodes in the network) has an impact on the transmission delay and power consumption in all four approaches (i.e., glt, caics, dlc, and mhc). in case of glt, the larger the size of the network, the greater the number of groups. thus, the size of nt and hst will be greater. on the other hand, in cluster-based networks, the number of cluster members will be increased in each cluster, which leads to more nodes involved in sensing and tracking. the average energy consumption and average transmission delay of data gathering versus the nodes number are shown in figures 19 and 20, respectively."
"to identify the it fields, the bibliographic data of 14,438 academic papers were retrieved from the web of science; the data were provided by clarivate analytics (as of october 10, 2017) and are shown in fig. 3 . the academic papers were those of the top 1% of academic papers-based on their number of citations in the research areas of \"computer science\" or \"information science and library science.\" here, the top 1% of academic papers was defined as the sum of each top 1% [cit] . this was because older papers obtain more citations; thus, only old papers would have been included in the top 1% of academic papers for the entire period if this method had not been used. table 1 in the \"appendix\" shows the data of the top 1% of academic papers and the citing academic papers."
"here, dissimilarity is defined as a reciprocal of cosine similarity. cosine similarity is based on the ochiai index or the ochiai coefficient, which have both been suggested for measuring the geographical distribution of fish [cit] . the cosine similarity between a cluster a and another cluster b was computed as shown in eq."
"in step 7, if the current item is satisfied with our preset rules which are different from the waterman-byers condition, we will add this current candidate item into the current stem. one new item is added each time. meanwhile, the total score needs to be adjusted. a backtracking technique is used here, which is a bottom-up algorithm."
"through clustering algorithm, this node initializes the firewall and ids logs through hadoop and builds a comprehensive and accurate data source for the subsequent chapters of this article, based on the prediction of log files."
"sometimes we could not successfully predict the secondary structure of an rna because \"our knowledge of the contributions of various rna motifs to the total free energy of rna structures is still incomplete\" [cit] . due to the limitation of this kind of knowledge, we could not give all the thermodynamic parameters concerning free energy. thus it could affect our prediction negatively. for example, when we predicted the microrna hsa-mir-196a, we failed to achieve the proposed structure of 5'uuag3' 3'agcc5'"
"where, m is the number of sub-windows, which is related to time characteristics of the learning task. ω is and ω ã is are weight parameters, which represent the time correlation between two points to be predicted in the data set. if the two points to be predicted are close to each other in time characteristics (for example less than the threshold θ, they belong to the same sub-window and the kernel function has a larger weight. the values of ω is and ω ã is are related to the level of the window and the radius of the window. in general, in the same sub-window, ω is should be greater than ω ã is ."
"mfold. is that saying that our prediction is correct and the one in the database is wrong? let us see our scores. according to our strategy, the score for the hairpin of (b) (in fig. 12 ) is 5.0 while the score for the hairpin of (a) (in fig. 12 ) is 5.15. so, we choose (b) as the structure of the hairpin based on our current scoring function. in order to improve our prediction algorithm we have to refine the scoring function."
"in the model, the whole situation of the network is divided into four first-class indicator situations: threat situation, fragile situation, stable situation and disaster situation [cit] . each first-class indicator situation is described by several secondary indicators. we use the t-s fuzzy neural network (fnn [cit] ) method, which makes secondary indicators as input and first-class indicator situation as output to get the threat situation, fragile situation, stable situation, and disaster situation, respectively. finally, the analytic hierarchy process (ahp) is used to decide the relative weight of each first-class indicator situation, thus the whole situation of the network is obtained [cit] ."
"finally, the pso-sequence kernel support vector machine is used to deal with the value of the whole situation; thus, we get the prediction results of the future state of the network. situation prediction of network security can help network administrators have a good understanding of network status. for network attacks, administrators can release network security warning timely."
"1) we propose an improved network security situation awareness model for iot based on pso-time series kernel support vector machine. 2) we employ a sequence kernel support vector machine and the particle swarm optimization to optimize related parameters, improving the accuracy of network security situation prediction."
we implement our micrornafold by a new recursive algorithm instead of waterman-byers algorithm [cit] . we solve the problem by a backtracking method. 13. obtain a global optimal solution from many possible sub-optimal structures
the pso-time series kernel function support vector machine and pso-gauss kernel function support vector machine were used to predict the network security situation in a certain week of june. the results were shown in fig. 5 .
"with the popularity of the internet of things and the rapid development of cloud computing, security issues become increasingly prominent. security vulnerabilities and security incidents are increasing notably. network security incidents have occurred occasionally such as network worms, hackers dragging databases, 0-day exposure, and privacy data leakage. network security is becoming the focus of many nations, enterprises, and individuals. [cit], which indicated that the government had put network security on the national strategic position [cit] ."
"the rest of this paper is organized as follows. first, we survey related work in section 2. then, we introduce the design of network security situation prediction of iot in section 3. we describe the system implementation and report evaluation results in section 4. finally, we conclude our work in section 5."
"the modified kernel function judges if the two points to be predicted are in the same window by a window function. then, we get the modified value and function."
"as shown in fig. 5, the bottom-up (bu) algorithm is introduced by using an example. we starts with a lone pair depicted in fig. 5a . then we repeatedly select a valid item as an mncm and add this mncm to the current structure until we construct a complete structure (see fig. 5b1 )."
"there is an in-depth study of the existed network security situation prediction achievement in this paper. for the characteristic of situational factors which are randomness, time-sequence, and complexity, we propose a network security situation prediction method based on psosequence kernel support vector machine. a modification function which is suitable for time series data is constructed to revise the kernel function of traditional support vector machine. then the sequence kernel support vector machine is obtained and the particle swarm optimization is used to optimize related parameters. by building an experimental environment and using the obtained values of the situation, it is verified that the method in this paper is feasible and effective. simulation results show that the method in this paper has high accuracy for the prediction of the network security situation, thus it can give network administrators useful help in making timely and effective decisions. in the future development of internet of things technology, the network situational awareness prediction method proposed in this paper can be applied to many scenarios, such as the communication field, cloud computing field and smart city construction field. i hope the research results presented in this paper can contribute to the development of internet of things network security. in the next step, the focus will be on the situation visualization research of network security."
"(3) the length of a lone pair ranges from 4 to a half of the length of the given sequence. this constraint is based on our experimental experience and pre-mirna structure. in fact, the hairpin loop of a pre-mirna may be very long and it contains far more than 6 nucleotides."
"the experimental results show that pso-time series kernel support vector machine is better than the pso-gauss kernel support vector machine in network security situation prediction. and during the week, the network security situation value of weekend was higher than normal, so network administrators should strengthen the network protection in time."
"the experimental results show that micrornafold outperforms the current leading prediction tools in terms of accuracies on matthews coefficient ratio and specificity. we consider the structure with the lowest score as the best structure. the predictive power of micrornafold was evaluated in terms of input sequence lengths as well. our model obtains the best performance when the length of the input sequence is average. in addition, we conducted experiments to assess the prediction performance with the different hairpin loop lengths. it seems that the model is ideal when the hairpin loop is of average length. if domain knowledge could be incorporated into our model it would improve the prediction a lot. knowledge of secondary structure will provide enough structural constraints to the building of three-dimensional structure. in the future, we can consider building a 3d structure model based on the secondary structure prediction."
"2) the 1x1 internal loops: due to the fact that we could not get all the needed parameters under this category, we just give some estimated values except for those publicly available data."
ω 2s is 0.4 and ω ã 1s is 0.3. the relative error of a certain week in june between the actual value of the network situation and the predictive value of two kinds of forecasting methods are shown in table 9 .
"the concept of situational awareness originated from the military field which requires an understanding of the strengths and weaknesses of the enemy in order to make the right decisions on the battlefield. [cit], endsley [cit] gave the definition of situational awareness for the first time, which was the perception of environmental elements with respect to time and/or space, the comprehension of their meaning, and the projection of their status after some variables had changed. [cit], bass [cit] first proposed the concept of cyberspace situation awareness, which aimed at using the sa for the network management and network security to improve the cognitive ability of administrators to shorten the decision time."
"network security situation prediction is closely related to time. but the gauss kernel function cannot reflect the time correlation. by fusing the gauss kernel function with temporal correlation, we can improve the traditional support vector machine. in order to fuse the gauss kernel function with temporal correlation, the definition of the window, modified kernel function, and time sequence kernel function is given."
"on the basis of different levels, different information sources, and different needs, this paper proposed a network security situation prediction model based on sequence kernel support vector machine as shown in fig. 1 ."
"in this section, firstly, we demonstrate the use of mncms for rna secondary structure prediction by showing how it arises as a natural extension of the recently developed ncms."
"3) the 2x2 internal loops: we merge a¢u/u¢a cases and g¢u/u¢g cases together. we construct the table according to the publicly available data, and in other cases, just give the estimated value 2.8."
"utilizing the characteristic of situational factors that are randomness, time-sequence, and complexity, an improved network security situation awareness model for iot is proposed in this paper. considering the close relationship between the network security situation and time, we agree that situation prediction of network security is a kind of time series forecasting problem in essence [cit] . a modification function is constructed that is suitable for time series data to revise the kernel function of traditional support vector machine. the sequence kernel support vector machine is obtained and the particle swarm optimization method is used to optimize related parameters. it proves that the method is feasible by collecting the boundary data of a university campus network. finally, a comparison with the pso-svm is made to prove the effectiveness of this method in improving the accuracy of network security situation prediction."
"(1) base pairs, (2) helix closing base pairs, (3) hairpin lengths, (4) bulge loop lengths [cit], (5) internal loop lengths, (6) internal loop asymmetry, (6) terminal mismatch interactions, and (7) dangling end."
"in order to solve this problem, we need to refine scoring function or incorporate auxiliary information. based on the statistical and theoretical analysis of the experimental data, we may incorporate biological constraints to help the prediction [cit] ."
"network security situational awareness system for predicting equipment log of this article is based on the hadoop big data processing platform, in order to verify the hadoop platform to handle large amounts of log time performance, in this article, the experiment will be treated as a traditional single log spent time comparing with hadoop cluster processing time spent, dealt with different levels of the log data; the time it takes is shown in table 10 . table 10 shows that when the log data level is less than 50,000, the single-machine processing capability is better than the processing power of the hadoop cluster. but as the growth of the log magnitude cluster around the time grows smaller, the rise in single machine processing time spent is almost in a straight line, and the increase in the number of nodes in the cluster processing efficiency is also higher. the efficient operation makes the log quantity of network security device more and more obvious, and the quiet of the single-machine processing mode is more and more prominent. therefore, the design of this paper is based on the big data platform to deal with the security log system has strong practical significance."
"the organization of the paper is as follows: in section ii, we introduce our micrornafold model, a global optimal algorithm based on bottom-up local optimal solutions, and some metrics used in our study. the experiments are carried out and the results are presented in section iii."
"during our study, we found that some of the best structures did not come from the first structure whose score is minimal. for example, according to the mirbase database, the structure of amemir-317 should be (b) in fig. 11 . but the results from micrornafold and mfold both showed that the proposed structure should be (a) in fig. 11 . fig. 12 displays the different hairpins between the database and our prediction approach. we can see that the hairpin which we obtained by our method is the same as the one obtained by"
"we compared the performance of micrornafold with the other two leading methods: one adopts the probabilistic-based strategy, and the other chooses the free energy minimization strategy. for benchmarking experiments, we used mc-fold [cit], and mfold (http://bioinfo.hku.hk/pise/mfold.html) [cit], with default parameters for each program. all benchmarks were conducted on intel-based servers running a gnu/linux operating system. whenever a program returned multiple possible structures (e.g., mfold and mc-fold), we chose the structure with the minimum score."
"the internet of things (iot) is a new technology rapidly developed in recent years. with the development of communication technology, iot devices have made good development in smart cities, wireless sensing, cloud computing, and many other fields. however, with the popularity of internet of things devices, security and privacy issues have become increasingly prominent [cit] . due to increasingly serious problems of iot network security, network security situation awareness of iot comes into being and gradually becomes the focus of the network security field. by assessing the operating status of the network in real-time and promptly predicting the problems before the security incidents occur, the network security situation awareness can help the administrators make the right decisions [cit] ."
a prediction method combined with the quantitative and qualitative of network security situation based on the cloud was proposed by lei xuan [cit] . the future situation was predicted by combining the current trend with the prediction rules mining from history evolvement data.
"secondly, we present micrornafold, which is the hybrid model of traditional energy-based scoring schemes and mncm structures. finally, we introduce a global optimal algorithm which is based on the bottom-up local optimal solutions in our micrornafold model."
pseudoviewer to view our structures [cit] . our best solution is from the first one among several hundreds of sorted possible structures (see fig. 6 ).
"the network security situation prediction process based on pso-time sequence kernel function support vector machine is shown in fig. 2 . in order to verify the reasonability of the method in this paper, the related data of a campus network are collected as the experiment data. the topological structure of the campus iot network is shown in fig. 3 . experiment raw data are the attack information by snort, data flow information by netflow, vulnerability information by nessus and asset performance information by sigar. the rich data source provides a reliable guarantee for the simulation experiment."
"(1) the currently available leading prediction tools are designed for general rna structure prediction, which do not consider much the features of the microrna secondary structures."
the first layer of the window weight parameters was that ω 1s is 0.6 and ω ã 1s is 0.4. the second layer window weight parameters were as follows:
(2) the second unpaired pair is considered as the first mismatched pair of traditional minimal free energy algorithm. the second pair of a lone pair mncm is the first mismatched pair of the hairpin loop according to the traditional thermodynamics-based models.
"we evaluated the predictive power of micrornafold by using known secondary structures of non-coding rna taken from the mirbase database [cit] . our testing data set comes from arabidopsis thaliana, brassica napus, saccharum officinarum, homo sapiens, gallus gallus, glycine max, apis mellifera, drosophila melanogaster, and drosophilla pseudoobscura. the sequence lengths of the testing data set range from 59 to 188. we implemented the micrornafold by using ansi c code and the program was ran on a linux-based machine. we used"
"although we have obtained encouraging results compared to other prediction approaches, there are still some issues that need to be discussed in detail. the first thing that we would like to mention is the auxiliary information. as we know, all the parameters and understanding of rna secondary structure come from experimental results. experimental results and the related analysis based on the experimental facts may help us design a more accurate model and prediction algorithm. how to get this knowledge is still a challenge for us. the second thing is the scoring strategy. during our testing phase, we found some proposed structures from the database could not be generated from our results based on the current scoring function."
". fig. 10 shows the comparison of the predicted structure by micrornafold with the structure proposed by the database. according to our current scoring function, the sum of the two parts"
"for the prediction model of time series kernel support vector machine, the embedding dimension was set as fig. 2 the network security situation prediction process based on pso-time sequence kernel function support vector machine seven by trial and error. that is using the previous week's data to predict the network security situation in the coming day. the prediction model is the time sequence kernel support vector machine optimized by the particle swarm. the parameters of the particle swarm are shown in table 7 ."
"(2) while the currently available leading prediction tools achieve good accuracies on true positive cases, their accuracies on matthews coefficient ratio [cit] are relatively low."
"a complex network-based network security situation prediction mechanism was proposed by li [cit] . using the model of markov, we can not only trace the dynamic behavior of the numerical fluctuations in the security situation but also predict security state effectively."
"as the gauss kernel function is better than other kernel functions, this paper uses the gauss kernel function as the kernel function of support vector machine. gauss kernel function is defined as follows:"
"at this moment, the stack pointer is at the beginning. we consider the beginning as the buttom and the lone pair as the top or head. we backtrack to the previous mncm and rebuild the next possible structure (see fig. 5b2 ). when we compare (b2) to (b1), we notice that the shadowed part is modified. when we go deep toward the lone pair, we can construct the structures shown in fig. 5bi and in fig. 5bn . the local optimal structure with the minimum score among the candidates (b1, b2, ..., bi, ..., bn) is chosen. based on the different lone pairs, we obtain many different local optimal structures. the global optimal solution is obtained by applying insertionsort algorithm."
where l is the level of the window. the choice of the number of window layer and the radius of the sub-window should improve the support vector machine prediction ability greatly.
"in this study, we acquired 360 data from march 1, 2015, to may 31 (90 days and 4 times daily samples) from the university as the training data. according to the steps and algorithms of 3.1, we obtained the value of the whole situation of the network. then, the model of pso-sequence kernel support vector machine was trained by the obtained values of the network. the 120 data which was from june 1, 2015, to june 30 (30 days and 4 times daily samples) were acquired as the test data."
"3' a g + fig. 10 . prediction of a specific structure. (a) is the sequence of the structure, (b) is the predicted structure by micrornafold, and (c) is the proposed structure by the database."
support vector machine (svm) is a general and effective machine learning method based on statistical learning theory [cit] . it has many obvious advantages in the study of complex nonlinear prediction. the regression function of network security situation prediction based on support vector machine is as follows:
"where a i and α ã i are lagrange multipliers. according to the kkt condition, the support vector machine prediction problem can be solved by solving the dual problem in formula (2), that is"
"in order to shorten our parameter tables and simplify our model, we merge canonical base pairs and terminal mismatches into one category: base pair. in fact we just consider mismatches as non-canonical base pairs."
"in order to verify the feasibility and effectiveness of pso-time series kernel support vector machine, we compared the predictive value of pso-time series kernel support vector machine with the actual security situation value and the predictive value of pso-gaussian kernel support vector machine."
"(1) the first nucleotide and the last nucleotide in a lone pair must be watson-crick base pair or wobble base pair (g¢u or u¢g). we propose a new term (see fig. 1b ) as an example, the pair u¢a is interface1 and the pair g¢c is interface2. in order to effectively employ our algorithm, we assume that all the interfaces should be a canonical base pair."
"where fp is the number of false positive cases, fn is the number of false negative cases, and tp is the number of true positive cases."
"p best is the optimal position of the particle, g best is the optimal position of population, k is iteration, c 1 andc 2 are learning factors, ω is inertia weight, γ 1 and γ 2 are the random numbers between 0 and 1."
"we propose a new microrna secondary structure prediction method based on modified october 24, 2008 draft ncms (mncms), which makes use of thermodynamics-based scoring function, implemented as a computer program: micrornafold. micrornafold employs a bottom-up algorithm to compute many local optimal solutions. the global optimal solution is produced by sorting these local optimal solutions. our experimental results show that our algorithm is very efficient in predicting"
"network attack is usually in a variety of network security equipment in the log traces of attack [cit], according to the above design, log aggregation rules and attribute characteristics to aggregation of attack mode, increase the count, origid, and mode three attributes. graphs of input and output details are shown in table 5 and 6."
